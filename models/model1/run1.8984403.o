Slurm job ID: 8984403
argfile: scripts/runargs/run1.csv
args: Namespace(name='model1', outdir='out/model_training/model1', training_data='data/model_training_data_3', validation_data='data/model_training_data_3', nsims_training=1000, nsims_validation=200, num_epochs=100, batch_size=50, ndims=2, nsigs=2, ncells=100, dt=0.1, signal_function='jump', phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus'], metric_final_act='None', metric_layer_normalize=False, infer_noise=True, sigma=0.01, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method='constant', init_metric_bias_args=[0.0], loss='mcd', continuation=None, optimizer='rms', learning_rate=0.001, momentum=0.9, weight_decay=0.0, infer_metric=False, plot=True, use_gpu=True, dtype='float32', seed=1, timestamp=False)
Using seed: 1
EPOCH 1:
	batch 100 loss: 0.8353654654324054
LOSS [train: 0.8353654654324054] [valid: 0.2727077931165695] TIME [epoch: 47.9 sec]
Saving model.
EPOCH 2:
	batch 100 loss: 0.24832584135234356
LOSS [train: 0.24832584135234356] [valid: 0.20709193572402002] TIME [epoch: 12 sec]
Saving model.
EPOCH 3:
	batch 100 loss: 0.14586008943617343
LOSS [train: 0.14586008943617343] [valid: 0.15785592347383498] TIME [epoch: 12 sec]
Saving model.
EPOCH 4:
	batch 100 loss: 0.11040920864790678
LOSS [train: 0.11040920864790678] [valid: 0.08741967044770718] TIME [epoch: 12.1 sec]
Saving model.
EPOCH 5:
	batch 100 loss: 0.06378998512402177
LOSS [train: 0.06378998512402177] [valid: 0.03749386896379292] TIME [epoch: 12 sec]
Saving model.
EPOCH 6:
	batch 100 loss: 0.032534966976381835
LOSS [train: 0.032534966976381835] [valid: 0.017341567343100904] TIME [epoch: 12 sec]
Saving model.
EPOCH 7:
	batch 100 loss: 0.02886750755831599
LOSS [train: 0.02886750755831599] [valid: 0.025033220974728466] TIME [epoch: 12.1 sec]
EPOCH 8:
	batch 100 loss: 0.02015543255954981
LOSS [train: 0.02015543255954981] [valid: 0.012237104831729084] TIME [epoch: 12 sec]
Saving model.
EPOCH 9:
	batch 100 loss: 0.021249670470133424
LOSS [train: 0.021249670470133424] [valid: 0.030072564166039227] TIME [epoch: 12.1 sec]
EPOCH 10:
	batch 100 loss: 0.02043117821216583
LOSS [train: 0.02043117821216583] [valid: 0.01781922490336001] TIME [epoch: 12 sec]
EPOCH 11:
	batch 100 loss: 0.0185924157127738
LOSS [train: 0.0185924157127738] [valid: 0.016118905041366816] TIME [epoch: 12 sec]
EPOCH 12:
	batch 100 loss: 0.018574139438569545
LOSS [train: 0.018574139438569545] [valid: 0.015628461446613073] TIME [epoch: 12 sec]
EPOCH 13:
	batch 100 loss: 0.017473025280050933
LOSS [train: 0.017473025280050933] [valid: 0.011377461394295097] TIME [epoch: 12 sec]
Saving model.
EPOCH 14:
	batch 100 loss: 0.016468779034912586
LOSS [train: 0.016468779034912586] [valid: 0.021497020311653615] TIME [epoch: 12 sec]
EPOCH 15:
	batch 100 loss: 0.0182092925067991
LOSS [train: 0.0182092925067991] [valid: 0.02582797231152654] TIME [epoch: 12 sec]
EPOCH 16:
	batch 100 loss: 0.019343573972582816
LOSS [train: 0.019343573972582816] [valid: 0.014466093084774912] TIME [epoch: 11.9 sec]
EPOCH 17:
	batch 100 loss: 0.0223546572541818
LOSS [train: 0.0223546572541818] [valid: 0.01446467856876552] TIME [epoch: 12.1 sec]
EPOCH 18:
	batch 100 loss: 0.01764890837483108
LOSS [train: 0.01764890837483108] [valid: 0.02219541370868683] TIME [epoch: 11.9 sec]
EPOCH 19:
	batch 100 loss: 0.016559756374917925
LOSS [train: 0.016559756374917925] [valid: 0.01730517568066716] TIME [epoch: 12 sec]
EPOCH 20:
	batch 100 loss: 0.01996665442362428
LOSS [train: 0.01996665442362428] [valid: 0.01935678026638925] TIME [epoch: 11.9 sec]
EPOCH 21:
	batch 100 loss: 0.019884128952398897
LOSS [train: 0.019884128952398897] [valid: 0.011120508285239339] TIME [epoch: 12 sec]
Saving model.
EPOCH 22:
	batch 100 loss: 0.021116492408327758
LOSS [train: 0.021116492408327758] [valid: 0.01798596903681755] TIME [epoch: 12 sec]
EPOCH 23:
	batch 100 loss: 0.022453649123199283
LOSS [train: 0.022453649123199283] [valid: 0.01470348599832505] TIME [epoch: 12.1 sec]
EPOCH 24:
	batch 100 loss: 0.023546813796274366
LOSS [train: 0.023546813796274366] [valid: 0.022301322454586624] TIME [epoch: 12 sec]
EPOCH 25:
	batch 100 loss: 0.022251544669270516
LOSS [train: 0.022251544669270516] [valid: 0.02473052809946239] TIME [epoch: 12 sec]
EPOCH 26:
	batch 100 loss: 0.02471394174732268
LOSS [train: 0.02471394174732268] [valid: 0.015945376083254813] TIME [epoch: 12 sec]
EPOCH 27:
	batch 100 loss: 0.02104443413671106
LOSS [train: 0.02104443413671106] [valid: 0.013670261763036252] TIME [epoch: 12 sec]
EPOCH 28:
	batch 100 loss: 0.022479498605243863
LOSS [train: 0.022479498605243863] [valid: 0.02397024049423635] TIME [epoch: 11.9 sec]
EPOCH 29:
	batch 100 loss: 0.019978387849405407
LOSS [train: 0.019978387849405407] [valid: 0.021313863852992655] TIME [epoch: 12.1 sec]
EPOCH 30:
	batch 100 loss: 0.019097937331534923
LOSS [train: 0.019097937331534923] [valid: 0.026530265668407084] TIME [epoch: 12.1 sec]
EPOCH 31:
	batch 100 loss: 0.019482593731954694
LOSS [train: 0.019482593731954694] [valid: 0.04224999751895666] TIME [epoch: 12.2 sec]
EPOCH 32:
	batch 100 loss: 0.01848877891898155
LOSS [train: 0.01848877891898155] [valid: 0.023535696649923922] TIME [epoch: 12.1 sec]
EPOCH 33:
	batch 100 loss: 0.016434632246382534
LOSS [train: 0.016434632246382534] [valid: 0.015048918966203927] TIME [epoch: 12 sec]
EPOCH 34:
	batch 100 loss: 0.019685747153125703
LOSS [train: 0.019685747153125703] [valid: 0.01825985247269273] TIME [epoch: 12 sec]
EPOCH 35:
	batch 100 loss: 0.016287266626022756
LOSS [train: 0.016287266626022756] [valid: 0.021458874410018324] TIME [epoch: 12 sec]
EPOCH 36:
	batch 100 loss: 0.017353970790281892
LOSS [train: 0.017353970790281892] [valid: 0.026211399910971524] TIME [epoch: 12.1 sec]
EPOCH 37:
	batch 100 loss: 0.021037780549377202
LOSS [train: 0.021037780549377202] [valid: 0.019257322838529944] TIME [epoch: 11.9 sec]
EPOCH 38:
	batch 100 loss: 0.015917366296052934
LOSS [train: 0.015917366296052934] [valid: 0.01588476258330047] TIME [epoch: 11.9 sec]
EPOCH 39:
	batch 100 loss: 0.01823268990498036
LOSS [train: 0.01823268990498036] [valid: 0.010962505801580846] TIME [epoch: 11.9 sec]
Saving model.
EPOCH 40:
	batch 100 loss: 0.019950194917619228
LOSS [train: 0.019950194917619228] [valid: 0.01623418740928173] TIME [epoch: 11.9 sec]
EPOCH 41:
	batch 100 loss: 0.017609673985280096
LOSS [train: 0.017609673985280096] [valid: 0.017380588129162787] TIME [epoch: 11.9 sec]
EPOCH 42:
	batch 100 loss: 0.019491307316347958
LOSS [train: 0.019491307316347958] [valid: 0.029409983241930605] TIME [epoch: 12 sec]
EPOCH 43:
	batch 100 loss: 0.022118764808401466
LOSS [train: 0.022118764808401466] [valid: 0.0141377039719373] TIME [epoch: 11.9 sec]
EPOCH 44:
	batch 100 loss: 0.01839660821016878
LOSS [train: 0.01839660821016878] [valid: 0.009879748639650642] TIME [epoch: 11.9 sec]
Saving model.
EPOCH 45:
	batch 100 loss: 0.017749819280579684
LOSS [train: 0.017749819280579684] [valid: 0.019253474986180664] TIME [epoch: 11.9 sec]
EPOCH 46:
	batch 100 loss: 0.01591225346084684
LOSS [train: 0.01591225346084684] [valid: 0.012889099470339716] TIME [epoch: 11.9 sec]
EPOCH 47:
	batch 100 loss: 1.3717695038067177
LOSS [train: 1.3717695038067177] [valid: 5.175092828273773] TIME [epoch: 12 sec]
EPOCH 48:
	batch 100 loss: 5.98695645570755
LOSS [train: 5.98695645570755] [valid: 3.5111936688423158] TIME [epoch: 12 sec]
EPOCH 49:
	batch 100 loss: 44.571005036830904
LOSS [train: 44.571005036830904] [valid: 62.93097610473633] TIME [epoch: 11.9 sec]
EPOCH 50:
	batch 100 loss: 66.70619392871856
LOSS [train: 66.70619392871856] [valid: 162.4723129272461] TIME [epoch: 11.9 sec]
EPOCH 51:
	batch 100 loss: 44.25849662303924
LOSS [train: 44.25849662303924] [valid: 14.354463386535645] TIME [epoch: 11.9 sec]
EPOCH 52:
	batch 100 loss: 134.53452424049377
LOSS [train: 134.53452424049377] [valid: 315.93417129516604] TIME [epoch: 11.9 sec]
EPOCH 53:
	batch 100 loss: 375.904421787262
LOSS [train: 375.904421787262] [valid: 529.6090629577636] TIME [epoch: 12 sec]
EPOCH 54:
	batch 100 loss: 128.7313920736313
LOSS [train: 128.7313920736313] [valid: 75.4687671661377] TIME [epoch: 11.9 sec]
EPOCH 55:
	batch 100 loss: 30.205522944927214
LOSS [train: 30.205522944927214] [valid: 1.4444567680358886] TIME [epoch: 12.2 sec]
EPOCH 56:
	batch 100 loss: 0.3388578458502889
LOSS [train: 0.3388578458502889] [valid: 0.0399528524838388] TIME [epoch: 11.9 sec]
EPOCH 57:
	batch 100 loss: 2.8376284925080837
LOSS [train: 2.8376284925080837] [valid: 12.031807851791381] TIME [epoch: 11.9 sec]
EPOCH 58:
	batch 100 loss: 15.419995498657226
LOSS [train: 15.419995498657226] [valid: 31.39547824859619] TIME [epoch: 12 sec]
EPOCH 59:
	batch 100 loss: 182.51089332580565
LOSS [train: 182.51089332580565] [valid: 227.9027130126953] TIME [epoch: 12 sec]
EPOCH 60:
	batch 100 loss: 479.5051511383057
LOSS [train: 479.5051511383057] [valid: 429.79564361572267] TIME [epoch: 11.9 sec]
EPOCH 61:
	batch 100 loss: 3417.844095458984
LOSS [train: 3417.844095458984] [valid: 6296.5009765625] TIME [epoch: 12 sec]
EPOCH 62:
	batch 100 loss: 4718.976886901855
LOSS [train: 4718.976886901855] [valid: 5902.101171875] TIME [epoch: 12 sec]
EPOCH 63:
	batch 100 loss: 2037.1578817749023
LOSS [train: 2037.1578817749023] [valid: 2100.9962951660154] TIME [epoch: 12 sec]
EPOCH 64:
	batch 100 loss: 819.9891012287139
LOSS [train: 819.9891012287139] [valid: 106.1420108795166] TIME [epoch: 11.9 sec]
EPOCH 65:
	batch 100 loss: 19345.52179851532
LOSS [train: 19345.52179851532] [valid: 19256.244647216798] TIME [epoch: 12 sec]
EPOCH 66:
	batch 100 loss: 6669.739614257813
LOSS [train: 6669.739614257813] [valid: 905.0039031982421] TIME [epoch: 12 sec]
EPOCH 67:
	batch 100 loss: 310.84363079071045
LOSS [train: 310.84363079071045] [valid: 338.5472808837891] TIME [epoch: 12 sec]
EPOCH 68:
	batch 100 loss: 430.22466012954715
LOSS [train: 430.22466012954715] [valid: 227.05217437744142] TIME [epoch: 12.1 sec]
EPOCH 69:
	batch 100 loss: 188.82970837831496
LOSS [train: 188.82970837831496] [valid: 694.3019317626953] TIME [epoch: 12 sec]
EPOCH 70:
	batch 100 loss: 163.23815861940383
LOSS [train: 163.23815861940383] [valid: 47.838583374023436] TIME [epoch: 12 sec]
EPOCH 71:
	batch 100 loss: 10.58710101723671
LOSS [train: 10.58710101723671] [valid: 2.0945265114307405] TIME [epoch: 12 sec]
EPOCH 72:
	batch 100 loss: 2.916933934688568
LOSS [train: 2.916933934688568] [valid: 2.941219222545624] TIME [epoch: 12 sec]
EPOCH 73:
	batch 100 loss: 13.773670222759247
LOSS [train: 13.773670222759247] [valid: 16.776433849334715] TIME [epoch: 11.9 sec]
EPOCH 74:
	batch 100 loss: 7.418816437721253
LOSS [train: 7.418816437721253] [valid: 0.8928053230047226] TIME [epoch: 12 sec]
EPOCH 75:
	batch 100 loss: 3.6244671696424486
LOSS [train: 3.6244671696424486] [valid: 4.60049546957016] TIME [epoch: 12 sec]
EPOCH 76:
	batch 100 loss: 20.532403559684752
LOSS [train: 20.532403559684752] [valid: 15.500812435150147] TIME [epoch: 12 sec]
EPOCH 77:
	batch 100 loss: 53.40798555850983
LOSS [train: 53.40798555850983] [valid: 498.2599838256836] TIME [epoch: 12 sec]
EPOCH 78:
	batch 100 loss: 450.5876094317436
LOSS [train: 450.5876094317436] [valid: 553.0355285644531] TIME [epoch: 12 sec]
EPOCH 79:
	batch 100 loss: 658.2593660736084
LOSS [train: 658.2593660736084] [valid: 1130.8220642089843] TIME [epoch: 12 sec]
EPOCH 80:
	batch 100 loss: 2346.792348022461
LOSS [train: 2346.792348022461] [valid: 2380.935791015625] TIME [epoch: 12 sec]
EPOCH 81:
	batch 100 loss: 437.9402645111084
LOSS [train: 437.9402645111084] [valid: 296.13338165283204] TIME [epoch: 12.1 sec]
EPOCH 82:
	batch 100 loss: 133.70829639434814
LOSS [train: 133.70829639434814] [valid: 82.63140296936035] TIME [epoch: 12 sec]
EPOCH 83:
	batch 100 loss: 249.0401287460327
LOSS [train: 249.0401287460327] [valid: 346.6606071472168] TIME [epoch: 12 sec]
EPOCH 84:
	batch 100 loss: 78.35364491939545
LOSS [train: 78.35364491939545] [valid: 5.469052457809449] TIME [epoch: 12 sec]
EPOCH 85:
	batch 100 loss: 213.03902077674866
LOSS [train: 213.03902077674866] [valid: 514.8061096191407] TIME [epoch: 12 sec]
EPOCH 86:
	batch 100 loss: 105.73896900475025
LOSS [train: 105.73896900475025] [valid: 0.732868318259716] TIME [epoch: 12.1 sec]
EPOCH 87:
	batch 100 loss: 1.0339451069571077
LOSS [train: 1.0339451069571077] [valid: 0.06417387197725474] TIME [epoch: 12 sec]
EPOCH 88:
	batch 100 loss: 0.7848031714186072
LOSS [train: 0.7848031714186072] [valid: 0.9529277682304382] TIME [epoch: 12.1 sec]
EPOCH 89:
	batch 100 loss: 1.5659425848722457
LOSS [train: 1.5659425848722457] [valid: 1.754595971107483] TIME [epoch: 12.1 sec]
EPOCH 90:
	batch 100 loss: 1.5351939970254898
LOSS [train: 1.5351939970254898] [valid: 1.3650399446487427] TIME [epoch: 12 sec]
EPOCH 91:
	batch 100 loss: 1.3207951200008392
LOSS [train: 1.3207951200008392] [valid: 0.5663189753890038] TIME [epoch: 12 sec]
EPOCH 92:
	batch 100 loss: 4.995614100694656
LOSS [train: 4.995614100694656] [valid: 15.374654054641724] TIME [epoch: 12 sec]
EPOCH 93:
	batch 100 loss: 5.683557179570198
LOSS [train: 5.683557179570198] [valid: 0.9687282264232635] TIME [epoch: 12 sec]
EPOCH 94:
	batch 100 loss: 15.12456183552742
LOSS [train: 15.12456183552742] [valid: 4.9514113664627075] TIME [epoch: 12.1 sec]
EPOCH 95:
	batch 100 loss: 4.2504390168190005
LOSS [train: 4.2504390168190005] [valid: 1.2820549309253693] TIME [epoch: 12 sec]
EPOCH 96:
	batch 100 loss: 4.027534765042365
LOSS [train: 4.027534765042365] [valid: 31.393639087677002] TIME [epoch: 11.9 sec]
EPOCH 97:
	batch 100 loss: 48.089183740615844
LOSS [train: 48.089183740615844] [valid: 59.927988815307614] TIME [epoch: 12 sec]
EPOCH 98:
	batch 100 loss: 41.15017950057983
LOSS [train: 41.15017950057983] [valid: 9.298525834083557] TIME [epoch: 12 sec]
EPOCH 99:
	batch 100 loss: 5.336509811878204
LOSS [train: 5.336509811878204] [valid: 2.0244142532348635] TIME [epoch: 11.9 sec]
EPOCH 100:
	batch 100 loss: 0.3610245196148753
LOSS [train: 0.3610245196148753] [valid: 0.050091815367341044] TIME [epoch: 12 sec]
Finished training in 1267.585 seconds.
