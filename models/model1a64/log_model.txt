__annotations__ : {'phi_nn': <class 'equinox._module.Module'>, 'tilt_nn': <class 'equinox._module.Module'>, 'logsigma': <class 'jax.Array'>, 'metric_nn': <class 'equinox._module.Module'>, 'ndim': <class 'int'>, 'nsig': <class 'int'>, 'ncells': <class 'int'>, 'signal_type': <class 'str'>, 'nsigparams': <class 'int'>, 'sigma_init': <class 'float'>, 'solver': <class 'str'>, 'dt0': <class 'float'>, 'sample_cells': <class 'bool'>, 'infer_metric': <class 'bool'>, 'include_phi_bias': <class 'bool'>, 'include_tilt_bias': <class 'bool'>, 'include_metric_bias': <class 'bool'>}
__doc__ : "PLNN(key, ndim, nsig, ncells, signal_type='jump', nsigparams=5, sigma_init=0.01, solver='euler', dt0=0.01, sample_cells=True, infer_metric=True, include_phi_bias=True, include_tilt_bias=False, include_metric_bias=True, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts='softplus', phi_final_act=None, phi_layer_normalize=False, tilt_hidden_dims=[], tilt_hidden_acts=None, tilt_final_act=None, tilt_layer_normalize=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts='softplus', metric_final_act=None, metric_layer_normalize=False, dtype=<class 'jax.numpy.float32'>)"
__module__ : 'plnn.models.model'
dt0 : 0.1
include_metric_bias : True
include_phi_bias : True
include_tilt_bias : False
infer_metric : False
logsigma : Array(-4.60517019, dtype=float64)
metric_nn : Sequential(
  layers=(
    Linear(
      weight=f64[8,2],
      bias=f64[8],
      in_features=2,
      out_features=8,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f64[8,8],
      bias=f64[8],
      in_features=8,
      out_features=8,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f64[8,8],
      bias=f64[8],
      in_features=8,
      out_features=8,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f64[8,8],
      bias=f64[8],
      in_features=8,
      out_features=8,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f64[3,8],
      bias=f64[3],
      in_features=8,
      out_features=3,
      use_bias=True
    )
  )
)
ncells : 100
ndim : 2
nsig : 2
nsigparams : 5
phi_nn : Sequential(
  layers=(
    Linear(
      weight=f64[16,2],
      bias=f64[16],
      in_features=2,
      out_features=16,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f64[32,16],
      bias=f64[32],
      in_features=16,
      out_features=32,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f64[32,32],
      bias=f64[32],
      in_features=32,
      out_features=32,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f64[16,32],
      bias=f64[16],
      in_features=32,
      out_features=16,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f64[1,16],
      bias=f64[1],
      in_features=16,
      out_features=1,
      use_bias=True
    )
  )
)
sample_cells : True
sigma_init : 0.01
signal_type : 'jump'
solver : 'euler'
tilt_nn : Sequential(
  layers=(
    Linear(
      weight=f64[2,2],
      bias=None,
      in_features=2,
      out_features=2,
      use_bias=False
    ),
  )
)
