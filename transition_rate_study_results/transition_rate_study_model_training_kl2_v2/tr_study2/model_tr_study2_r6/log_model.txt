__annotations__ : {'phi_nn': <class 'equinox._module.Module'>, 'tilt_nn': <class 'equinox._module.Module'>, 'logsigma': <class 'jax.Array'>, 'metric_nn': <class 'equinox._module.Module'>, 'ndims': <class 'int'>, 'nparams': <class 'int'>, 'nsigs': <class 'int'>, 'ncells': <class 'int'>, 'signal_type': <class 'str'>, 'nsigparams': <class 'int'>, 'sigma_init': <class 'float'>, 'solver': <class 'str'>, 'dt0': <class 'float'>, 'confine': <class 'bool'>, 'sample_cells': <class 'bool'>, 'infer_metric': <class 'bool'>, 'include_phi_bias': <class 'bool'>, 'include_tilt_bias': <class 'bool'>, 'include_metric_bias': <class 'bool'>}
__doc__ : "PLNN(key, ndims, nparams, nsigs, ncells, signal_type='jump', nsigparams=3, sigma_init=0.01, solver='euler', dt0=0.01, confine=False, sample_cells=True, infer_metric=True, include_phi_bias=True, include_tilt_bias=False, include_metric_bias=True, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts='softplus', phi_final_act=None, phi_layer_normalize=False, tilt_hidden_dims=[], tilt_hidden_acts=None, tilt_final_act=None, tilt_layer_normalize=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts='softplus', metric_final_act=None, metric_layer_normalize=False, dtype=<class 'jax.numpy.float32'>)"
__module__ : 'plnn.models.model'
confine : True
dt0 : 0.01
include_metric_bias : True
include_phi_bias : True
include_tilt_bias : False
infer_metric : False
logsigma : Array(-2.9957323, dtype=float32)
metric_nn : Sequential(
  layers=(
    Linear(
      weight=f32[8,2],
      bias=f32[8],
      in_features=2,
      out_features=8,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f32[8,8],
      bias=f32[8],
      in_features=8,
      out_features=8,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f32[8,8],
      bias=f32[8],
      in_features=8,
      out_features=8,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f32[8,8],
      bias=f32[8],
      in_features=8,
      out_features=8,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f32[3,8],
      bias=f32[3],
      in_features=8,
      out_features=3,
      use_bias=True
    )
  )
)
ncells : 200
ndims : 2
nparams : 2
nsigparams : 4
nsigs : 2
phi_nn : Sequential(
  layers=(
    Linear(
      weight=f32[16,2],
      bias=f32[16],
      in_features=2,
      out_features=16,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f32[32,16],
      bias=f32[32],
      in_features=16,
      out_features=32,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f32[32,32],
      bias=f32[32],
      in_features=32,
      out_features=32,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f32[16,32],
      bias=f32[16],
      in_features=32,
      out_features=16,
      use_bias=True
    ),
    Lambda(fn=<wrapped function softplus>),
    Linear(
      weight=f32[1,16],
      bias=f32[1],
      in_features=16,
      out_features=1,
      use_bias=True
    )
  )
)
sample_cells : True
sigma_init : 0.05
signal_type : 'sigmoid'
solver : 'heun'
tilt_nn : Sequential(
  layers=(
    Linear(
      weight=f32[2,2],
      bias=None,
      in_features=2,
      out_features=2,
      use_bias=False
    ),
  )
)
