Args:
Namespace(name='model_phi1_1a_v_mmd', outdir='out/model_training/model_phi1_1a_v_mmd', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=500, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3390870363

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.652520692767635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.652520692767635 | validation: 5.8709990850673925]
	TIME [epoch: 117 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.120535330079509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.120535330079509 | validation: 5.777851365007221]
	TIME [epoch: 13.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.912091460603829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.912091460603829 | validation: 5.789073840493429]
	TIME [epoch: 13.6 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9052820739315575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9052820739315575 | validation: 5.6319172869775525]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.667247117609357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.667247117609357 | validation: 5.188513180350853]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.311279693026078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.311279693026078 | validation: 4.93077771191642]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.017743144009666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.017743144009666 | validation: 4.668268616003305]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4798184882745895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4798184882745895 | validation: 3.429611209606871]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9835068752722833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9835068752722833 | validation: 3.1381624280087266]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7396826529267715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7396826529267715 | validation: 2.8814789126561253]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.540496125300322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.540496125300322 | validation: 2.618139364807014]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5828281354547853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5828281354547853 | validation: 2.4785312572908897]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.337095511754581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.337095511754581 | validation: 2.453274624967053]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.30539905472573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.30539905472573 | validation: 2.401448247451369]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.294621541686617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.294621541686617 | validation: 2.3720732526207677]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2881292939378777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2881292939378777 | validation: 2.350473312660512]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2922955912109164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2922955912109164 | validation: 2.3792878679281646]
	TIME [epoch: 13.6 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2841357029851856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2841357029851856 | validation: 2.3352902925262713]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.264782715883885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.264782715883885 | validation: 2.349031957819393]
	TIME [epoch: 13.5 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.260080760232376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.260080760232376 | validation: 2.416870810519662]
	TIME [epoch: 13.6 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.271877370944887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.271877370944887 | validation: 2.4123618087034284]
	TIME [epoch: 13.5 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.273173059209098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.273173059209098 | validation: 2.339834995735335]
	TIME [epoch: 13.5 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1976532036351544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1976532036351544 | validation: 2.3913430519031396]
	TIME [epoch: 13.6 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.22898637012218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.22898637012218 | validation: 2.4267623161961023]
	TIME [epoch: 13.5 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2628955269327204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2628955269327204 | validation: 2.3743674170235716]
	TIME [epoch: 13.5 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1276373859880575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1276373859880575 | validation: 2.334686780822051]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.124709218255866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.124709218255866 | validation: 2.286869672455676]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1187194621545107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1187194621545107 | validation: 2.16384310299595]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.999479172791277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.999479172791277 | validation: 2.0707893157454076]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.760343641410002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.760343641410002 | validation: 1.7937679388606922]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8499102579197761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8499102579197761 | validation: 2.0365209516737095]
	TIME [epoch: 13.6 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8029464057044409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8029464057044409 | validation: 1.5293705550641794]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4233052814507976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4233052814507976 | validation: 1.3843781201484062]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5656875412870594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5656875412870594 | validation: 1.7204461581678308]
	TIME [epoch: 13.6 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5213670869242732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5213670869242732 | validation: 1.451132562007246]
	TIME [epoch: 13.5 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2932351792812262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2932351792812262 | validation: 1.404842804436443]
	TIME [epoch: 13.6 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4211237337367446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4211237337367446 | validation: 1.24060648579947]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2561808301738364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2561808301738364 | validation: 1.0695967301855323]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1457821687028011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1457821687028011 | validation: 0.9734316760066389]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4153535403577737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4153535403577737 | validation: 1.2490255343703682]
	TIME [epoch: 13.5 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2964016390015713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2964016390015713 | validation: 1.0397119452302583]
	TIME [epoch: 13.5 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9072711056300627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9072711056300627 | validation: 1.0804894403567846]
	TIME [epoch: 13.6 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8686555091377062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8686555091377062 | validation: 0.792990033827385]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8403549969539217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8403549969539217 | validation: 1.7525690767873148]
	TIME [epoch: 13.6 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1066339087949202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1066339087949202 | validation: 0.7719204790391038]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8939529906287802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8939529906287802 | validation: 1.2025495982958052]
	TIME [epoch: 13.5 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9587908653803857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9587908653803857 | validation: 1.3744963986544645]
	TIME [epoch: 13.6 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0309479513866817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0309479513866817 | validation: 0.605039684388048]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6679216298867552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6679216298867552 | validation: 0.5918549245270093]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6602065348977295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6602065348977295 | validation: 0.7889932967268318]
	TIME [epoch: 13.6 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7179418754476098		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.7179418754476098 | validation: 0.5638709800146535]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5265672916150923		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.5265672916150923 | validation: 1.3448479568197795]
	TIME [epoch: 13.6 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6567092035906372		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.6567092035906372 | validation: 0.514015914072666]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6544051102548935		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.6544051102548935 | validation: 0.5449855469153948]
	TIME [epoch: 13.5 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6186875097788489		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.6186875097788489 | validation: 0.6341368304875952]
	TIME [epoch: 13.6 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5366501872590411		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.5366501872590411 | validation: 1.0042037177435434]
	TIME [epoch: 13.5 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9928422708627542		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.9928422708627542 | validation: 0.5753723683830484]
	TIME [epoch: 13.5 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8447803797356774		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.8447803797356774 | validation: 0.5829367387991835]
	TIME [epoch: 13.6 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4593913757859069		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.4593913757859069 | validation: 0.3782359190027147]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4073301642942256		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.4073301642942256 | validation: 0.39369639251899846]
	TIME [epoch: 13.6 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42752084116894673		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.42752084116894673 | validation: 0.9155702511638688]
	TIME [epoch: 13.6 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.808722698152362		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.808722698152362 | validation: 0.7685886795439164]
	TIME [epoch: 13.5 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6551795587833685		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.6551795587833685 | validation: 0.42726816037982684]
	TIME [epoch: 13.6 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43325873431598094		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.43325873431598094 | validation: 0.38181591956044747]
	TIME [epoch: 13.5 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36326151990171		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.36326151990171 | validation: 0.47332270172790825]
	TIME [epoch: 13.5 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4243953312658618		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.4243953312658618 | validation: 0.5215604654854153]
	TIME [epoch: 13.6 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5714008483235938		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.5714008483235938 | validation: 0.9662890573232239]
	TIME [epoch: 13.5 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6964161153712001		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.6964161153712001 | validation: 0.4750587843951809]
	TIME [epoch: 13.5 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.529065644044634		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.529065644044634 | validation: 0.5294255057195694]
	TIME [epoch: 13.6 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38384308823568225		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.38384308823568225 | validation: 0.3564528572467901]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3725206779464817		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.3725206779464817 | validation: 0.3785234486785366]
	TIME [epoch: 13.6 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34773472810895867		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.34773472810895867 | validation: 0.4333381565397254]
	TIME [epoch: 13.6 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5261715716904303		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.5261715716904303 | validation: 0.5063705107148195]
	TIME [epoch: 13.5 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5160530457547385		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.5160530457547385 | validation: 0.4442201462787925]
	TIME [epoch: 13.6 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4118256107809478		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.4118256107809478 | validation: 0.3464446045692411]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4501239497969918		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.4501239497969918 | validation: 0.4412362359505283]
	TIME [epoch: 13.5 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4044996091518079		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.4044996091518079 | validation: 0.43093515430676893]
	TIME [epoch: 13.6 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36025961570335063		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.36025961570335063 | validation: 0.34515829424112154]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32760211625851227		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.32760211625851227 | validation: 0.536797671756949]
	TIME [epoch: 13.5 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36553674352114907		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.36553674352114907 | validation: 0.571878201006609]
	TIME [epoch: 13.6 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39368425596192524		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.39368425596192524 | validation: 0.4441030021826162]
	TIME [epoch: 13.5 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42391789932733115		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.42391789932733115 | validation: 0.39829608899918134]
	TIME [epoch: 13.6 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34197237471670167		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.34197237471670167 | validation: 0.5233078022562363]
	TIME [epoch: 13.6 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4000777127630868		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.4000777127630868 | validation: 0.35763849720356367]
	TIME [epoch: 13.5 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30181916489233074		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.30181916489233074 | validation: 0.4695793553064538]
	TIME [epoch: 13.6 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3751324323175712		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.3751324323175712 | validation: 0.3964779012128866]
	TIME [epoch: 13.5 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2941475228174634		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.2941475228174634 | validation: 0.3366050792742509]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3133872995466173		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.3133872995466173 | validation: 0.2710915256661802]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28645945596807143		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.28645945596807143 | validation: 0.3159772829129037]
	TIME [epoch: 13.5 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.364172774943465		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.364172774943465 | validation: 0.35460499467278317]
	TIME [epoch: 13.6 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35961610539508737		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.35961610539508737 | validation: 0.27786227115939427]
	TIME [epoch: 13.6 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31716468318064706		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.31716468318064706 | validation: 0.5378990571846428]
	TIME [epoch: 13.5 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40398302854291246		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.40398302854291246 | validation: 0.3143865000488702]
	TIME [epoch: 13.6 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2645783475847216		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.2645783475847216 | validation: 0.22726021147210496]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2689171553906678		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.2689171553906678 | validation: 0.2762648477882388]
	TIME [epoch: 13.5 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2700211344175269		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.2700211344175269 | validation: 0.272676064286355]
	TIME [epoch: 13.6 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30340577584499406		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.30340577584499406 | validation: 0.24478699507939666]
	TIME [epoch: 13.6 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24785443342792582		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.24785443342792582 | validation: 0.27905044767325227]
	TIME [epoch: 13.5 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30375417160100493		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.30375417160100493 | validation: 0.2599891321103899]
	TIME [epoch: 13.6 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.289832134080954		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.289832134080954 | validation: 0.2426218495633084]
	TIME [epoch: 13.5 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24669332830861063		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.24669332830861063 | validation: 0.2260442406249451]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3095450309869926		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.3095450309869926 | validation: 0.4340352416980272]
	TIME [epoch: 13.6 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3595648707856244		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.3595648707856244 | validation: 0.3071440487878095]
	TIME [epoch: 13.5 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2573247496392731		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.2573247496392731 | validation: 0.22904756752483404]
	TIME [epoch: 13.6 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.220488825390927		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.220488825390927 | validation: 0.22042780617100333]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2355207650871354		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.2355207650871354 | validation: 0.3037014588748062]
	TIME [epoch: 13.5 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25366046745495924		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.25366046745495924 | validation: 0.2689147793507471]
	TIME [epoch: 13.6 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2937496929487153		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.2937496929487153 | validation: 0.23122369567919443]
	TIME [epoch: 13.5 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23715954511527343		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.23715954511527343 | validation: 0.27457382257827934]
	TIME [epoch: 13.5 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21598801134243223		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.21598801134243223 | validation: 0.1795295660006058]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23575964195001994		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.23575964195001994 | validation: 0.27997682391729317]
	TIME [epoch: 13.5 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27216560576760873		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.27216560576760873 | validation: 0.27511911510241593]
	TIME [epoch: 13.6 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24260646503240604		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.24260646503240604 | validation: 0.1634891660166482]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.174901609368168		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.174901609368168 | validation: 0.23191186653848728]
	TIME [epoch: 13.5 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23081830719351112		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.23081830719351112 | validation: 0.178580544307601]
	TIME [epoch: 13.6 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2116427079016938		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.2116427079016938 | validation: 0.13546350635600568]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16380598689658898		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.16380598689658898 | validation: 0.16043635781827398]
	TIME [epoch: 13.5 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21955007240487623		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.21955007240487623 | validation: 0.158046451447223]
	TIME [epoch: 13.6 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2141767188889247		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.2141767188889247 | validation: 0.253403494278736]
	TIME [epoch: 13.5 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23501815254814312		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.23501815254814312 | validation: 0.19322254143007944]
	TIME [epoch: 13.5 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23432515419224403		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.23432515419224403 | validation: 0.20639376273784807]
	TIME [epoch: 13.6 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2006978991275885		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.2006978991275885 | validation: 0.13412340753957624]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18567171046222633		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.18567171046222633 | validation: 0.12228384525657869]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16142891318429703		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.16142891318429703 | validation: 0.261139673758838]
	TIME [epoch: 13.5 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21143119266711952		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.21143119266711952 | validation: 0.12153941324312026]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1483560529505849		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.1483560529505849 | validation: 0.1570071492418088]
	TIME [epoch: 13.6 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24433945838651588		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.24433945838651588 | validation: 0.1536458659342206]
	TIME [epoch: 13.5 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1765777883486374		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.1765777883486374 | validation: 0.18094165165303297]
	TIME [epoch: 13.5 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15370767696697987		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.15370767696697987 | validation: 0.16246413487389738]
	TIME [epoch: 13.6 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20254686207322067		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.20254686207322067 | validation: 0.17827355191093397]
	TIME [epoch: 13.5 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17203769876961225		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.17203769876961225 | validation: 0.13551859021859847]
	TIME [epoch: 13.5 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18478828344828963		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.18478828344828963 | validation: 0.1665210661944414]
	TIME [epoch: 13.6 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.182551384218177		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.182551384218177 | validation: 0.14890943600506215]
	TIME [epoch: 13.5 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1709831351773181		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.1709831351773181 | validation: 0.1776308756602618]
	TIME [epoch: 13.6 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15497730523645847		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.15497730523645847 | validation: 0.15072304082718094]
	TIME [epoch: 13.5 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20180794932377175		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.20180794932377175 | validation: 0.1222338547311643]
	TIME [epoch: 13.5 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15801932943557082		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.15801932943557082 | validation: 0.26118219721439456]
	TIME [epoch: 13.6 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17376909096010126		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.17376909096010126 | validation: 0.0952348765144479]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1391475710931172		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.1391475710931172 | validation: 0.18880811854444618]
	TIME [epoch: 13.5 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18418345798724825		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.18418345798724825 | validation: 0.09155110463529209]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1355354084041672		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.1355354084041672 | validation: 0.14941559285277595]
	TIME [epoch: 13.5 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1443691465076676		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.1443691465076676 | validation: 0.1399585398520441]
	TIME [epoch: 13.6 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19516479213246246		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.19516479213246246 | validation: 0.1058933064345669]
	TIME [epoch: 13.6 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14506762688124886		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.14506762688124886 | validation: 0.17820614490781295]
	TIME [epoch: 13.5 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14131805725963711		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.14131805725963711 | validation: 0.07386716347939694]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09931810677969727		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.09931810677969727 | validation: 0.14114559082254352]
	TIME [epoch: 13.5 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17682574881798596		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.17682574881798596 | validation: 0.10545183888472424]
	TIME [epoch: 13.5 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10271421719094218		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.10271421719094218 | validation: 0.07636524039578682]
	TIME [epoch: 13.6 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1204468223478509		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.1204468223478509 | validation: 0.2120032265931271]
	TIME [epoch: 13.5 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.186713854679937		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.186713854679937 | validation: 0.10696335023652721]
	TIME [epoch: 13.5 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10679939743050026		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.10679939743050026 | validation: 0.07122368295487304]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12936644842225659		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.12936644842225659 | validation: 0.3436770009383907]
	TIME [epoch: 13.5 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21698467340783661		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.21698467340783661 | validation: 0.09566589706597178]
	TIME [epoch: 13.6 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10993375416095107		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.10993375416095107 | validation: 0.0838967877291126]
	TIME [epoch: 13.5 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09864404442492974		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.09864404442492974 | validation: 0.06398104382303649]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1027553431291087		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.1027553431291087 | validation: 0.1903699167652575]
	TIME [epoch: 13.6 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13401937846098133		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.13401937846098133 | validation: 0.08248740946119675]
	TIME [epoch: 13.5 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1023725182089245		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.1023725182089245 | validation: 0.1708168171937723]
	TIME [epoch: 13.5 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1525654729547118		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.1525654729547118 | validation: 0.08889959924267296]
	TIME [epoch: 13.6 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11051211326299151		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.11051211326299151 | validation: 0.10762173735791636]
	TIME [epoch: 13.5 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11620531463529543		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.11620531463529543 | validation: 0.10162948677090133]
	TIME [epoch: 13.5 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15545094682961472		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.15545094682961472 | validation: 0.09566205393810301]
	TIME [epoch: 13.6 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09792251858236878		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.09792251858236878 | validation: 0.12483132559404567]
	TIME [epoch: 13.5 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10771171585784466		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.10771171585784466 | validation: 0.05455892190674879]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07807712428271628		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.07807712428271628 | validation: 0.07105554483622294]
	TIME [epoch: 13.5 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09321807165408867		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.09321807165408867 | validation: 0.13001959705729182]
	TIME [epoch: 13.5 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14983573186715549		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.14983573186715549 | validation: 0.06182495335864255]
	TIME [epoch: 13.6 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1016096057676332		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.1016096057676332 | validation: 0.13607027239888533]
	TIME [epoch: 13.5 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11854314532243809		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.11854314532243809 | validation: 0.13930623362541977]
	TIME [epoch: 13.5 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0975464209122186		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.0975464209122186 | validation: 0.0690758444053747]
	TIME [epoch: 13.6 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09590248972495194		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.09590248972495194 | validation: 0.10767577401178574]
	TIME [epoch: 13.5 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13266048594518762		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.13266048594518762 | validation: 0.07580422559464377]
	TIME [epoch: 13.5 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0763388623852421		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.0763388623852421 | validation: 0.06385810390595972]
	TIME [epoch: 13.6 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06393543554890617		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.06393543554890617 | validation: 0.04350073460494805]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08607990754567779		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.08607990754567779 | validation: 0.30693541798053814]
	TIME [epoch: 13.6 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17566865611286575		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.17566865611286575 | validation: 0.07712129414872601]
	TIME [epoch: 13.5 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07255486334921822		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.07255486334921822 | validation: 0.059349010012958725]
	TIME [epoch: 13.5 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08431925261315443		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.08431925261315443 | validation: 0.14318839511834217]
	TIME [epoch: 13.6 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09377756335793079		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.09377756335793079 | validation: 0.045261920508225674]
	TIME [epoch: 13.5 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06530670248231144		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.06530670248231144 | validation: 0.05929632811099113]
	TIME [epoch: 13.5 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14210915622730186		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.14210915622730186 | validation: 0.0779796225518286]
	TIME [epoch: 13.6 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08531628038142879		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.08531628038142879 | validation: 0.09562535352122456]
	TIME [epoch: 13.5 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07950197417924593		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.07950197417924593 | validation: 0.0986786061539558]
	TIME [epoch: 13.5 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1223906679181839		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.1223906679181839 | validation: 0.050356316065315154]
	TIME [epoch: 13.6 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06626208332282944		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.06626208332282944 | validation: 0.08335031008251798]
	TIME [epoch: 13.5 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059984588009624154		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.059984588009624154 | validation: 0.10099689050473734]
	TIME [epoch: 13.6 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12488576088815193		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.12488576088815193 | validation: 0.05955599630858863]
	TIME [epoch: 13.5 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06208586958672645		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.06208586958672645 | validation: 0.05689168229509679]
	TIME [epoch: 13.5 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0629575908274468		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.0629575908274468 | validation: 0.11655535020361585]
	TIME [epoch: 13.6 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09743819367772896		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.09743819367772896 | validation: 0.047130643501990026]
	TIME [epoch: 13.5 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07691234217439598		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.07691234217439598 | validation: 0.1091238550572052]
	TIME [epoch: 13.5 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09901673434986508		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.09901673434986508 | validation: 0.04713389604128933]
	TIME [epoch: 13.6 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05297367028729288		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.05297367028729288 | validation: 0.048693726049887136]
	TIME [epoch: 13.5 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0954678100818884		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.0954678100818884 | validation: 0.09127968958291323]
	TIME [epoch: 13.5 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.087280823319265		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.087280823319265 | validation: 0.06149019181798486]
	TIME [epoch: 13.6 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06973206131208559		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.06973206131208559 | validation: 0.07996485990896077]
	TIME [epoch: 13.5 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07472701435728775		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.07472701435728775 | validation: 0.038255351782845365]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04844888223915512		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.04844888223915512 | validation: 0.05664264621944963]
	TIME [epoch: 13.5 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07543267928628618		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.07543267928628618 | validation: 0.1898101325716689]
	TIME [epoch: 13.5 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0945684485691132		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.0945684485691132 | validation: 0.06858859920426266]
	TIME [epoch: 13.6 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06489803084723186		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.06489803084723186 | validation: 0.14400954860101972]
	TIME [epoch: 13.5 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11706092589919112		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.11706092589919112 | validation: 0.03206011337075622]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045754679426191124		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.045754679426191124 | validation: 0.05246212516251592]
	TIME [epoch: 13.6 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08746784555617587		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.08746784555617587 | validation: 0.07656786029465745]
	TIME [epoch: 13.5 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05106829100064063		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.05106829100064063 | validation: 0.051761212615402355]
	TIME [epoch: 13.6 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06576552199227778		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.06576552199227778 | validation: 0.10237089543871605]
	TIME [epoch: 13.6 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06967994248674908		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.06967994248674908 | validation: 0.04218088184886427]
	TIME [epoch: 13.5 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05256336482510557		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.05256336482510557 | validation: 0.07169124429603443]
	TIME [epoch: 13.6 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08874122908889624		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.08874122908889624 | validation: 0.041058380470276684]
	TIME [epoch: 13.5 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04272922164198295		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.04272922164198295 | validation: 0.04529081908909595]
	TIME [epoch: 13.5 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.071903026370714		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.071903026370714 | validation: 0.12196109090342876]
	TIME [epoch: 13.6 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07716996339696834		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.07716996339696834 | validation: 0.035664879825121426]
	TIME [epoch: 13.5 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055049150331361665		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.055049150331361665 | validation: 0.08413126500989859]
	TIME [epoch: 13.5 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08105342600067486		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.08105342600067486 | validation: 0.03915553001872296]
	TIME [epoch: 13.6 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034568901202492244		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.034568901202492244 | validation: 0.031541671832713836]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06298773447836832		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.06298773447836832 | validation: 0.15694696068957276]
	TIME [epoch: 13.5 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07951780384410431		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.07951780384410431 | validation: 0.03521284013656068]
	TIME [epoch: 13.6 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05687168949910488		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.05687168949910488 | validation: 0.08330658380811173]
	TIME [epoch: 13.5 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07985584693905606		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.07985584693905606 | validation: 0.05604469389360432]
	TIME [epoch: 13.6 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03859556925033644		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.03859556925033644 | validation: 0.02878233567162692]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049696685070470176		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.049696685070470176 | validation: 0.12767022473554207]
	TIME [epoch: 13.5 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07280890226459957		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.07280890226459957 | validation: 0.053035521565825805]
	TIME [epoch: 13.6 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052136128490182426		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.052136128490182426 | validation: 0.05553229156388294]
	TIME [epoch: 13.5 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0573996209465243		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.0573996209465243 | validation: 0.0644784467797464]
	TIME [epoch: 13.5 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0478722738943158		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.0478722738943158 | validation: 0.03599970649092956]
	TIME [epoch: 13.6 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07964549844306265		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.07964549844306265 | validation: 0.05573508934459603]
	TIME [epoch: 13.5 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056527587811181314		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.056527587811181314 | validation: 0.06188756849695859]
	TIME [epoch: 13.6 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05123842398438436		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.05123842398438436 | validation: 0.04042369216237738]
	TIME [epoch: 13.6 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034112340660749835		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.034112340660749835 | validation: 0.03329546964392901]
	TIME [epoch: 13.5 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0333007039685237		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.0333007039685237 | validation: 0.06564464100304447]
	TIME [epoch: 13.6 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09319104460614244		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.09319104460614244 | validation: 0.04563890749835299]
	TIME [epoch: 13.5 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.063645509516942		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.063645509516942 | validation: 0.08074204032582247]
	TIME [epoch: 13.5 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07549814284513418		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.07549814284513418 | validation: 0.02336089909107065]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0319737397409106		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.0319737397409106 | validation: 0.0436721325126512]
	TIME [epoch: 13.5 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05477755449780004		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.05477755449780004 | validation: 0.029997728141533418]
	TIME [epoch: 13.5 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032221371672437556		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.032221371672437556 | validation: 0.02378234589975939]
	TIME [epoch: 13.6 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057679431874502654		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.057679431874502654 | validation: 0.1091799671970142]
	TIME [epoch: 13.5 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058340915998751415		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.058340915998751415 | validation: 0.03714539005075762]
	TIME [epoch: 13.6 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03523809846694988		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.03523809846694988 | validation: 0.03359100708637548]
	TIME [epoch: 13.5 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05649527426091625		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.05649527426091625 | validation: 0.060101340620348914]
	TIME [epoch: 13.5 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05055258318431144		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.05055258318431144 | validation: 0.027048913891269222]
	TIME [epoch: 13.6 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028009239979204827		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.028009239979204827 | validation: 0.03407255659624307]
	TIME [epoch: 13.5 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04034224195600154		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.04034224195600154 | validation: 0.09423061520445286]
	TIME [epoch: 13.5 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06841082108525812		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.06841082108525812 | validation: 0.035898222245292474]
	TIME [epoch: 13.6 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041455393705022436		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.041455393705022436 | validation: 0.046233510303000845]
	TIME [epoch: 13.5 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0629989774338821		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.0629989774338821 | validation: 0.0407391203633161]
	TIME [epoch: 13.5 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0473778418825927		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.0473778418825927 | validation: 0.050218782269498566]
	TIME [epoch: 13.6 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04189817384851679		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.04189817384851679 | validation: 0.03809294838622124]
	TIME [epoch: 13.5 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05182201155300225		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.05182201155300225 | validation: 0.02707496327868382]
	TIME [epoch: 13.5 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03217506535392253		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.03217506535392253 | validation: 0.039985101654698596]
	TIME [epoch: 13.6 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041836453275746095		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.041836453275746095 | validation: 0.02259581404273452]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03944563179248403		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.03944563179248403 | validation: 0.09854852241529732]
	TIME [epoch: 13.6 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060607750991850326		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.060607750991850326 | validation: 0.02361028434106055]
	TIME [epoch: 13.5 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03752612927071615		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.03752612927071615 | validation: 0.035345792607638996]
	TIME [epoch: 13.5 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02933655057474465		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.02933655057474465 | validation: 0.06018385824540881]
	TIME [epoch: 13.6 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059554291316024716		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.059554291316024716 | validation: 0.020653312390677296]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033980944617838835		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.033980944617838835 | validation: 0.048935534911940706]
	TIME [epoch: 13.5 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034057474253282795		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.034057474253282795 | validation: 0.02533942871910287]
	TIME [epoch: 13.5 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021464350463812606		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.021464350463812606 | validation: 0.036401192889192656]
	TIME [epoch: 13.5 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09149973025140246		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.09149973025140246 | validation: 0.07649371863066448]
	TIME [epoch: 13.5 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06084633259370223		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.06084633259370223 | validation: 0.03575328134908495]
	TIME [epoch: 13.5 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029009598692661894		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.029009598692661894 | validation: 0.03070700534215249]
	TIME [epoch: 13.5 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04748455902739356		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.04748455902739356 | validation: 0.06851245744033099]
	TIME [epoch: 13.6 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04380213338459956		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.04380213338459956 | validation: 0.02133907297504073]
	TIME [epoch: 13.5 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02710753431774658		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.02710753431774658 | validation: 0.05824725636509583]
	TIME [epoch: 13.5 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04209394582112448		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.04209394582112448 | validation: 0.024403960683489302]
	TIME [epoch: 13.6 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028085323161828636		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.028085323161828636 | validation: 0.041954016443752046]
	TIME [epoch: 13.5 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041492365319951124		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.041492365319951124 | validation: 0.06866486544912342]
	TIME [epoch: 13.5 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04556542040294609		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.04556542040294609 | validation: 0.020960399617993834]
	TIME [epoch: 13.6 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02378067986607725		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.02378067986607725 | validation: 0.02228751405414746]
	TIME [epoch: 13.5 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046168867161594386		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.046168867161594386 | validation: 0.040702795735998124]
	TIME [epoch: 13.5 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02435750127843675		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.02435750127843675 | validation: 0.02371875404058587]
	TIME [epoch: 13.5 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036099254884265214		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.036099254884265214 | validation: 0.0319126271741498]
	TIME [epoch: 13.5 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04966241548038838		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.04966241548038838 | validation: 0.02454348156736736]
	TIME [epoch: 13.6 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04164837787770024		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.04164837787770024 | validation: 0.037621567937684924]
	TIME [epoch: 13.5 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028866932740849782		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.028866932740849782 | validation: 0.0392087059527277]
	TIME [epoch: 13.5 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05643133850258367		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.05643133850258367 | validation: 0.019237415401034918]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024252037749596556		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.024252037749596556 | validation: 0.029273767923163593]
	TIME [epoch: 13.5 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035088571903269285		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.035088571903269285 | validation: 0.030388314640874332]
	TIME [epoch: 13.5 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026639648414214085		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.026639648414214085 | validation: 0.03698449606399445]
	TIME [epoch: 13.6 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03778348477279382		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.03778348477279382 | validation: 0.026827856312634978]
	TIME [epoch: 13.5 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03178641971881242		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.03178641971881242 | validation: 0.028834818978787563]
	TIME [epoch: 13.5 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03391909959609064		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.03391909959609064 | validation: 0.03768975938925483]
	TIME [epoch: 13.5 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03562821573019061		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.03562821573019061 | validation: 0.027358730102126937]
	TIME [epoch: 13.5 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01910064567912978		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.01910064567912978 | validation: 0.0187508437050128]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04740583950391476		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.04740583950391476 | validation: 0.040389342569439185]
	TIME [epoch: 13.5 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03723392502486758		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.03723392502486758 | validation: 0.03409042072427055]
	TIME [epoch: 13.5 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033309206334579726		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.033309206334579726 | validation: 0.024211905804688367]
	TIME [epoch: 13.6 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03128912180605046		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.03128912180605046 | validation: 0.026971893045438292]
	TIME [epoch: 13.5 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035295085856141065		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.035295085856141065 | validation: 0.05473799679030428]
	TIME [epoch: 13.5 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027553258096516143		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.027553258096516143 | validation: 0.0187752779125568]
	TIME [epoch: 13.6 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029764660276303076		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.029764660276303076 | validation: 0.05591433080019945]
	TIME [epoch: 13.5 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0372851034219713		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.0372851034219713 | validation: 0.02394614564295198]
	TIME [epoch: 13.6 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026651472424834656		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.026651472424834656 | validation: 0.029745584791982856]
	TIME [epoch: 13.5 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03662018960866801		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.03662018960866801 | validation: 0.031875081466456553]
	TIME [epoch: 13.5 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022261815950749324		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.022261815950749324 | validation: 0.03300913046971992]
	TIME [epoch: 13.6 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04074025072581803		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.04074025072581803 | validation: 0.017065416606701654]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02040604904111853		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.02040604904111853 | validation: 0.025254679394507824]
	TIME [epoch: 13.5 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041525535680959756		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.041525535680959756 | validation: 0.018335447760804882]
	TIME [epoch: 13.6 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01702756809659213		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.01702756809659213 | validation: 0.02345222777951354]
	TIME [epoch: 13.5 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034392126998773906		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.034392126998773906 | validation: 0.02941903522922061]
	TIME [epoch: 13.5 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02298043007445743		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.02298043007445743 | validation: 0.0218001333205449]
	TIME [epoch: 13.5 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019261072445177656		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.019261072445177656 | validation: 0.05206611404988399]
	TIME [epoch: 13.5 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04074856142458991		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.04074856142458991 | validation: 0.015263743463200814]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027176484565582814		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.027176484565582814 | validation: 0.0646515660304262]
	TIME [epoch: 13.5 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03162809132069698		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.03162809132069698 | validation: 0.025550922629308193]
	TIME [epoch: 13.5 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029380755889213357		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.029380755889213357 | validation: 0.04324891800382628]
	TIME [epoch: 13.6 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027440608235813062		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.027440608235813062 | validation: 0.021113562643015914]
	TIME [epoch: 13.5 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026218847747404826		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.026218847747404826 | validation: 0.0356535532133912]
	TIME [epoch: 13.5 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024701251102079446		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.024701251102079446 | validation: 0.028200278255890346]
	TIME [epoch: 13.6 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020989902570018197		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.020989902570018197 | validation: 0.021339360165416286]
	TIME [epoch: 13.5 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03730142460855345		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.03730142460855345 | validation: 0.04532191566947658]
	TIME [epoch: 13.5 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026343813003560035		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.026343813003560035 | validation: 0.027688397498194323]
	TIME [epoch: 13.5 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022084962097414264		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.022084962097414264 | validation: 0.023967025049297083]
	TIME [epoch: 13.5 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03000005436818902		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.03000005436818902 | validation: 0.026037436819282566]
	TIME [epoch: 13.6 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029213116905567042		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.029213116905567042 | validation: 0.018662240628807218]
	TIME [epoch: 13.5 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012955210058104887		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.012955210058104887 | validation: 0.01660424398469482]
	TIME [epoch: 13.5 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04585739428199061		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.04585739428199061 | validation: 0.037722056954441134]
	TIME [epoch: 13.6 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024542389582001507		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.024542389582001507 | validation: 0.022956842936854172]
	TIME [epoch: 13.5 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017382899110634547		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.017382899110634547 | validation: 0.02416486352080821]
	TIME [epoch: 13.5 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02430946261019435		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.02430946261019435 | validation: 0.012203510852386742]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025759044332938234		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.025759044332938234 | validation: 0.06134464382723817]
	TIME [epoch: 13.5 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02942467680289753		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.02942467680289753 | validation: 0.01970112973971257]
	TIME [epoch: 13.5 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015612528069582292		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.015612528069582292 | validation: 0.014789725389439103]
	TIME [epoch: 13.5 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025510921003947765		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.025510921003947765 | validation: 0.04858325740889893]
	TIME [epoch: 13.5 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035234388811577706		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.035234388811577706 | validation: 0.023889112526017782]
	TIME [epoch: 13.5 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01631170789624427		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.01631170789624427 | validation: 0.022020857635047877]
	TIME [epoch: 13.5 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027892639347709997		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.027892639347709997 | validation: 0.031043267709713822]
	TIME [epoch: 13.5 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021232182803237257		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.021232182803237257 | validation: 0.014562794432128984]
	TIME [epoch: 13.6 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031141185964437337		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.031141185964437337 | validation: 0.0634593694226902]
	TIME [epoch: 13.5 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03000345235531717		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.03000345235531717 | validation: 0.031814128604238684]
	TIME [epoch: 13.5 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020104207532456467		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.020104207532456467 | validation: 0.011777034241713188]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013052123106510164		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.013052123106510164 | validation: 0.04541409093149423]
	TIME [epoch: 13.5 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052586256596395056		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.052586256596395056 | validation: 0.06232571128805851]
	TIME [epoch: 13.5 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029277225812681947		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.029277225812681947 | validation: 0.012199255674711841]
	TIME [epoch: 13.5 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0132379074271597		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.0132379074271597 | validation: 0.015573319459013007]
	TIME [epoch: 13.5 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021846813165660083		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.021846813165660083 | validation: 0.031218133013415004]
	TIME [epoch: 13.5 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02432138813046151		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.02432138813046151 | validation: 0.012234223528108261]
	TIME [epoch: 13.5 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023981072105392208		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.023981072105392208 | validation: 0.023867248331336684]
	TIME [epoch: 13.5 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026289383505250324		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.026289383505250324 | validation: 0.02077254917090993]
	TIME [epoch: 13.6 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01788728304081541		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.01788728304081541 | validation: 0.018400712402841697]
	TIME [epoch: 13.5 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024075465888404583		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.024075465888404583 | validation: 0.016257534719182367]
	TIME [epoch: 13.5 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02354478183834688		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.02354478183834688 | validation: 0.021621900950753704]
	TIME [epoch: 13.6 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02086716402107805		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.02086716402107805 | validation: 0.021966041434499275]
	TIME [epoch: 13.5 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02008818420915385		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.02008818420915385 | validation: 0.02825524771906454]
	TIME [epoch: 13.5 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03362623997659084		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.03362623997659084 | validation: 0.01613026459135687]
	TIME [epoch: 13.6 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020320939108918513		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.020320939108918513 | validation: 0.02432784655634234]
	TIME [epoch: 13.5 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015330026700357654		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.015330026700357654 | validation: 0.019846656787878062]
	TIME [epoch: 13.6 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01910160427118398		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.01910160427118398 | validation: 0.015247391340023323]
	TIME [epoch: 13.6 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023306710066817397		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.023306710066817397 | validation: 0.044157825635627215]
	TIME [epoch: 13.5 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02598111744126071		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.02598111744126071 | validation: 0.013663401568709968]
	TIME [epoch: 13.6 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01151849936128356		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.01151849936128356 | validation: 0.014373627389638464]
	TIME [epoch: 13.5 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021165997531941344		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.021165997531941344 | validation: 0.03983424702841859]
	TIME [epoch: 13.5 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04058984144332271		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.04058984144332271 | validation: 0.024550018139360818]
	TIME [epoch: 13.6 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02426133962647587		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.02426133962647587 | validation: 0.014352024969434158]
	TIME [epoch: 13.5 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015000031759665893		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.015000031759665893 | validation: 0.014773269596686168]
	TIME [epoch: 13.5 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012706926640040138		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.012706926640040138 | validation: 0.01299325947991039]
	TIME [epoch: 13.6 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017844752936985137		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.017844752936985137 | validation: 0.047174173973818366]
	TIME [epoch: 13.5 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036759026055240326		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.036759026055240326 | validation: 0.01462314824249171]
	TIME [epoch: 13.5 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012901416301943447		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.012901416301943447 | validation: 0.02492429348066491]
	TIME [epoch: 13.6 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017400716983893956		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.017400716983893956 | validation: 0.01388565023549155]
	TIME [epoch: 13.5 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026733101359923565		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.026733101359923565 | validation: 0.0331670646769298]
	TIME [epoch: 13.5 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01985097718557495		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.01985097718557495 | validation: 0.012479464854058694]
	TIME [epoch: 13.5 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011244589976101735		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.011244589976101735 | validation: 0.01776713711336226]
	TIME [epoch: 13.5 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029248224000623376		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.029248224000623376 | validation: 0.014565520764666346]
	TIME [epoch: 13.6 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011211392944874277		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.011211392944874277 | validation: 0.025216094558395234]
	TIME [epoch: 13.5 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015582439684426973		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.015582439684426973 | validation: 0.012869260601619834]
	TIME [epoch: 13.5 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023909191123250434		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.023909191123250434 | validation: 0.043543884125127594]
	TIME [epoch: 13.5 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02194953297056812		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.02194953297056812 | validation: 0.012828337728043007]
	TIME [epoch: 13.5 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017228681802553224		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.017228681802553224 | validation: 0.011482341097708865]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015385222424328378		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.015385222424328378 | validation: 0.016692705353937715]
	TIME [epoch: 13.5 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02279806546480242		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.02279806546480242 | validation: 0.011856018175391708]
	TIME [epoch: 13.5 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011837526471140076		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.011837526471140076 | validation: 0.01440537968068789]
	TIME [epoch: 13.6 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02176997891689037		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.02176997891689037 | validation: 0.045160355454186674]
	TIME [epoch: 13.5 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02223172609194719		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.02223172609194719 | validation: 0.010520937597001678]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_375.pth
	Model improved!!!
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013492104489488229		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.013492104489488229 | validation: 0.028051716262095933]
	TIME [epoch: 13.6 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019922507464211224		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.019922507464211224 | validation: 0.03505224896228047]
	TIME [epoch: 13.5 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02006550570543769		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.02006550570543769 | validation: 0.015357107100261332]
	TIME [epoch: 13.5 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013977644020861496		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.013977644020861496 | validation: 0.01524884109307769]
	TIME [epoch: 13.5 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014289054570686594		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.014289054570686594 | validation: 0.03585755647126278]
	TIME [epoch: 13.5 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028025109555093998		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.028025109555093998 | validation: 0.024245005789442556]
	TIME [epoch: 13.5 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01592205400770714		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.01592205400770714 | validation: 0.011601403057022633]
	TIME [epoch: 13.5 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01063337901833837		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.01063337901833837 | validation: 0.015523748096008608]
	TIME [epoch: 13.5 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015884517828710844		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.015884517828710844 | validation: 0.03605577533963887]
	TIME [epoch: 13.5 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028636871326746204		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.028636871326746204 | validation: 0.012511240825331081]
	TIME [epoch: 13.5 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00934855187454124		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.00934855187454124 | validation: 0.0103497756918423]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009438317682809763		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.009438317682809763 | validation: 0.022466450413924942]
	TIME [epoch: 13.6 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03450978952469999		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.03450978952469999 | validation: 0.020152463808348682]
	TIME [epoch: 13.5 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012514063812552509		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.012514063812552509 | validation: 0.010745640383587062]
	TIME [epoch: 13.5 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008567170270327768		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.008567170270327768 | validation: 0.008220311709264114]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029860383224823782		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.029860383224823782 | validation: 0.01776778285445553]
	TIME [epoch: 13.5 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01686538308020922		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.01686538308020922 | validation: 0.01244940183536804]
	TIME [epoch: 13.5 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014907943350532707		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.014907943350532707 | validation: 0.017964083991928167]
	TIME [epoch: 13.5 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01271360249882476		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.01271360249882476 | validation: 0.02227351979469607]
	TIME [epoch: 13.5 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021587900081810395		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.021587900081810395 | validation: 0.01045942748185168]
	TIME [epoch: 13.6 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01445097713134974		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.01445097713134974 | validation: 0.01713830982464405]
	TIME [epoch: 13.5 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016076525466369507		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.016076525466369507 | validation: 0.021028530062747713]
	TIME [epoch: 13.5 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015095590439589609		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.015095590439589609 | validation: 0.012258315993312667]
	TIME [epoch: 13.6 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011492049209502017		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.011492049209502017 | validation: 0.010179827003597142]
	TIME [epoch: 13.5 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008575709355118637		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.008575709355118637 | validation: 0.012592544002831121]
	TIME [epoch: 13.5 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032699106215855964		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.032699106215855964 | validation: 0.01674450214123281]
	TIME [epoch: 13.6 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014866761264767152		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.014866761264767152 | validation: 0.008898446201453657]
	TIME [epoch: 13.5 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012775881378799963		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.012775881378799963 | validation: 0.01823632588946633]
	TIME [epoch: 13.6 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013404523362648635		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.013404523362648635 | validation: 0.008919961828500132]
	TIME [epoch: 13.5 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01684299549597195		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.01684299549597195 | validation: 0.03385127692821661]
	TIME [epoch: 13.5 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017167635371768717		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.017167635371768717 | validation: 0.012642521992564808]
	TIME [epoch: 13.6 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01615346942620627		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.01615346942620627 | validation: 0.01577336327574974]
	TIME [epoch: 13.5 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012062577554950445		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.012062577554950445 | validation: 0.010686916393634133]
	TIME [epoch: 13.5 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01503764579947205		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.01503764579947205 | validation: 0.017978410452046425]
	TIME [epoch: 13.6 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011863146640910812		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.011863146640910812 | validation: 0.010392473961078404]
	TIME [epoch: 13.5 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016611124058812043		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.016611124058812043 | validation: 0.01339769341395469]
	TIME [epoch: 13.5 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020061591820724412		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.020061591820724412 | validation: 0.011361075100207077]
	TIME [epoch: 13.6 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010874988138558047		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.010874988138558047 | validation: 0.008076122724156722]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01333089721382796		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.01333089721382796 | validation: 0.01895246494335602]
	TIME [epoch: 13.6 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013900448101551648		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.013900448101551648 | validation: 0.010107559942277071]
	TIME [epoch: 13.5 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01094592520893154		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.01094592520893154 | validation: 0.013939394246548964]
	TIME [epoch: 13.5 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022862752664920605		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.022862752664920605 | validation: 0.00912345198752354]
	TIME [epoch: 13.6 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009148712307202581		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.009148712307202581 | validation: 0.008681449576533504]
	TIME [epoch: 13.5 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007987178087011685		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.007987178087011685 | validation: 0.009975631531343338]
	TIME [epoch: 13.5 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015886413478704325		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.015886413478704325 | validation: 0.0514324119839388]
	TIME [epoch: 13.6 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025582139520298773		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.025582139520298773 | validation: 0.019666508362823007]
	TIME [epoch: 13.5 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0107664359803084		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.0107664359803084 | validation: 0.010165488603014125]
	TIME [epoch: 13.5 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01076437793772489		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.01076437793772489 | validation: 0.012433441085577451]
	TIME [epoch: 13.6 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02067569808900414		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.02067569808900414 | validation: 0.015872073941324442]
	TIME [epoch: 13.5 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010273767443477995		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.010273767443477995 | validation: 0.0075718630319482425]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009940692294690944		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.009940692294690944 | validation: 0.023252401063524327]
	TIME [epoch: 13.5 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01970708086277579		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.01970708086277579 | validation: 0.015414018261731601]
	TIME [epoch: 13.5 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010636844659934934		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.010636844659934934 | validation: 0.007481828294644891]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008681483556240996		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.008681483556240996 | validation: 0.021005112349323153]
	TIME [epoch: 13.5 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022350358523390172		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.022350358523390172 | validation: 0.010495172380847566]
	TIME [epoch: 13.5 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01000921312660695		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.01000921312660695 | validation: 0.007456179478294333]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008411356643633693		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.008411356643633693 | validation: 0.021295308827841662]
	TIME [epoch: 13.5 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02163731936948728		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.02163731936948728 | validation: 0.015173118616301419]
	TIME [epoch: 13.5 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011564626489077343		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.011564626489077343 | validation: 0.014515991636833349]
	TIME [epoch: 13.5 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017309909038429903		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.017309909038429903 | validation: 0.018673189805674412]
	TIME [epoch: 13.5 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013185275630466058		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.013185275630466058 | validation: 0.01167336654564376]
	TIME [epoch: 13.5 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010411148748875108		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.010411148748875108 | validation: 0.009880156044013997]
	TIME [epoch: 13.5 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016576597032445316		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.016576597032445316 | validation: 0.015806873127540838]
	TIME [epoch: 13.5 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011680542672811772		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.011680542672811772 | validation: 0.016550425613320714]
	TIME [epoch: 13.5 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010744201923047121		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.010744201923047121 | validation: 0.009513639875379858]
	TIME [epoch: 13.5 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011171494442169699		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.011171494442169699 | validation: 0.014371068250955129]
	TIME [epoch: 13.5 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0192596774280631		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.0192596774280631 | validation: 0.007610412773638699]
	TIME [epoch: 13.6 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009731021124004771		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.009731021124004771 | validation: 0.010076972722444316]
	TIME [epoch: 13.5 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009681473951063144		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.009681473951063144 | validation: 0.01687663977431248]
	TIME [epoch: 13.5 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018389667026532348		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.018389667026532348 | validation: 0.008313248007862937]
	TIME [epoch: 13.5 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007624424793375684		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.007624424793375684 | validation: 0.008680873306372232]
	TIME [epoch: 13.5 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013616907383495365		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.013616907383495365 | validation: 0.025525548915536975]
	TIME [epoch: 13.5 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013340004760235955		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.013340004760235955 | validation: 0.013613276425816225]
	TIME [epoch: 13.5 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009732468986960615		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.009732468986960615 | validation: 0.009289324002726682]
	TIME [epoch: 13.5 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009638943371684451		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.009638943371684451 | validation: 0.008715375500054611]
	TIME [epoch: 13.6 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020450488849107502		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.020450488849107502 | validation: 0.014797566360655785]
	TIME [epoch: 13.5 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014876638835765526		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.014876638835765526 | validation: 0.009201047109490853]
	TIME [epoch: 13.5 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008952299397594399		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.008952299397594399 | validation: 0.013171386407835758]
	TIME [epoch: 13.5 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01018553787732603		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.01018553787732603 | validation: 0.010179316963403564]
	TIME [epoch: 13.5 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012897472695733252		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.012897472695733252 | validation: 0.014786061106506855]
	TIME [epoch: 13.5 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010279142631239276		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.010279142631239276 | validation: 0.00936685869317784]
	TIME [epoch: 13.5 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01054057058148888		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.01054057058148888 | validation: 0.011317466271146711]
	TIME [epoch: 13.5 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011281212335127943		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.011281212335127943 | validation: 0.011046875715269962]
	TIME [epoch: 13.5 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008990546218243639		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.008990546218243639 | validation: 0.010824839999253608]
	TIME [epoch: 13.5 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017528846628716288		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.017528846628716288 | validation: 0.013353207544516492]
	TIME [epoch: 13.5 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012418096567318118		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.012418096567318118 | validation: 0.008382118048806177]
	TIME [epoch: 13.5 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008849278583278804		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.008849278583278804 | validation: 0.013311093898787565]
	TIME [epoch: 13.5 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015478421058189659		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.015478421058189659 | validation: 0.010639548436437362]
	TIME [epoch: 13.5 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008698196063611044		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.008698196063611044 | validation: 0.007336438042873705]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011829614639695767		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.011829614639695767 | validation: 0.013921169238108143]
	TIME [epoch: 13.5 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012957869551007778		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.012957869551007778 | validation: 0.008799754249147123]
	TIME [epoch: 13.5 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008137444984259591		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.008137444984259591 | validation: 0.011765226815676095]
	TIME [epoch: 13.6 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011609097857124905		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.011609097857124905 | validation: 0.008909460829922906]
	TIME [epoch: 13.5 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009259446749229617		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.009259446749229617 | validation: 0.007905509946833445]
	TIME [epoch: 13.5 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01583556168372058		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.01583556168372058 | validation: 0.018424884527202798]
	TIME [epoch: 13.5 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010144140130979527		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.010144140130979527 | validation: 0.015547552915169387]
	TIME [epoch: 13.5 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016977240324374997		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.016977240324374997 | validation: 0.007081522394487039]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008674286083604875		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.008674286083604875 | validation: 0.006546980574016953]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_473.pth
	Model improved!!!
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006793550201199559		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.006793550201199559 | validation: 0.0071634882122001906]
	TIME [epoch: 13.5 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019308318147561107		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.019308318147561107 | validation: 0.01482338212682124]
	TIME [epoch: 13.6 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00981667938576541		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.00981667938576541 | validation: 0.007033213162390603]
	TIME [epoch: 13.5 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006801631496455497		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.006801631496455497 | validation: 0.006543535942590513]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013091350457841445		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.013091350457841445 | validation: 0.01381324579215688]
	TIME [epoch: 13.6 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011317348772572337		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.011317348772572337 | validation: 0.009464799956412712]
	TIME [epoch: 13.5 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007637558909862056		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.007637558909862056 | validation: 0.006248675146922584]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011526928166479173		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.011526928166479173 | validation: 0.022939906659392703]
	TIME [epoch: 13.5 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0128944611024975		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.0128944611024975 | validation: 0.008953700567523825]
	TIME [epoch: 13.5 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009340503216366494		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.009340503216366494 | validation: 0.012337984854710504]
	TIME [epoch: 13.5 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010610885979391253		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.010610885979391253 | validation: 0.012260430245260738]
	TIME [epoch: 13.5 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010422400150144399		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.010422400150144399 | validation: 0.007349032219434012]
	TIME [epoch: 13.5 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00971427280312714		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.00971427280312714 | validation: 0.013117890362026482]
	TIME [epoch: 13.6 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011262270193834535		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.011262270193834535 | validation: 0.008475114396943963]
	TIME [epoch: 13.5 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006962808067516499		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.006962808067516499 | validation: 0.007363431202832546]
	TIME [epoch: 13.5 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012730813122794338		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.012730813122794338 | validation: 0.02675703132487479]
	TIME [epoch: 13.6 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013669390016628234		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.013669390016628234 | validation: 0.00911304047221828]
	TIME [epoch: 13.5 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00777374421798953		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.00777374421798953 | validation: 0.010882488434029502]
	TIME [epoch: 13.5 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012413878692353704		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.012413878692353704 | validation: 0.007612422657157386]
	TIME [epoch: 13.5 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0070896930192916145		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.0070896930192916145 | validation: 0.007549343361178861]
	TIME [epoch: 13.5 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011699554591409965		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.011699554591409965 | validation: 0.01808080791986573]
	TIME [epoch: 13.6 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01189184499200152		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.01189184499200152 | validation: 0.013206821494753804]
	TIME [epoch: 13.5 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010210321864386305		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.010210321864386305 | validation: 0.015060812502124675]
	TIME [epoch: 13.5 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011006217465920561		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.011006217465920561 | validation: 0.007174266031074756]
	TIME [epoch: 13.5 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006387596512568356		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.006387596512568356 | validation: 0.010937974317356443]
	TIME [epoch: 13.5 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013703545622730039		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.013703545622730039 | validation: 0.007562339170912258]
	TIME [epoch: 13.5 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00851774162639879		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.00851774162639879 | validation: 0.012294435721795722]
	TIME [epoch: 13.5 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03466122107265945		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.03466122107265945 | validation: 0.013623709280286445]
	TIME [epoch: 13.5 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011249491047936589		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.011249491047936589 | validation: 0.007467252698498889]
	TIME [epoch: 13.5 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007313733378519643		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.007313733378519643 | validation: 0.006114373715040728]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_503.pth
	Model improved!!!
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007736598726670943		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.007736598726670943 | validation: 0.006466223972634799]
	TIME [epoch: 13.5 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010516458255438033		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.010516458255438033 | validation: 0.014670615703937207]
	TIME [epoch: 13.5 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01086555562398725		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.01086555562398725 | validation: 0.012224908081898998]
	TIME [epoch: 13.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009304183510170374		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.009304183510170374 | validation: 0.007047525464921576]
	TIME [epoch: 13.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006075205839627524		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.006075205839627524 | validation: 0.0091000302695584]
	TIME [epoch: 13.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0124792254011001		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.0124792254011001 | validation: 0.009620728202702614]
	TIME [epoch: 13.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008832928808683683		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.008832928808683683 | validation: 0.008167208152605299]
	TIME [epoch: 13.5 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008932374616417733		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.008932374616417733 | validation: 0.008777786440402547]
	TIME [epoch: 13.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008438857316463079		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.008438857316463079 | validation: 0.010647647768040454]
	TIME [epoch: 13.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011029496320107761		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.011029496320107761 | validation: 0.006158809415836642]
	TIME [epoch: 13.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008379524155486068		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.008379524155486068 | validation: 0.007438801942252845]
	TIME [epoch: 13.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01017519055967163		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.01017519055967163 | validation: 0.010662358718405568]
	TIME [epoch: 13.5 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008988027510940327		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.008988027510940327 | validation: 0.009155702321699585]
	TIME [epoch: 13.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008149544725806842		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.008149544725806842 | validation: 0.007924140970201348]
	TIME [epoch: 13.5 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008474473368235721		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.008474473368235721 | validation: 0.013219928663089386]
	TIME [epoch: 13.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008266437890479986		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.008266437890479986 | validation: 0.0058278747146355715]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005812273897884733		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.005812273897884733 | validation: 0.006522381788844182]
	TIME [epoch: 13.5 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013348579350743418		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.013348579350743418 | validation: 0.012979410414765056]
	TIME [epoch: 13.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010279743890818533		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.010279743890818533 | validation: 0.005690514673223046]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_522.pth
	Model improved!!!
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00650829805981578		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.00650829805981578 | validation: 0.006939265447615382]
	TIME [epoch: 13.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008292372291299156		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.008292372291299156 | validation: 0.007797661331927115]
	TIME [epoch: 13.5 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007181568240194892		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.007181568240194892 | validation: 0.00574274325197146]
	TIME [epoch: 13.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010547709016073773		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.010547709016073773 | validation: 0.014444059546960989]
	TIME [epoch: 13.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010116740590989787		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.010116740590989787 | validation: 0.00993259249960022]
	TIME [epoch: 13.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007355255601495435		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.007355255601495435 | validation: 0.006762779665204643]
	TIME [epoch: 13.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006807638015260976		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.006807638015260976 | validation: 0.007274930373116106]
	TIME [epoch: 13.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00818136393318988		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.00818136393318988 | validation: 0.023189887065944055]
	TIME [epoch: 13.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01366546112553914		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.01366546112553914 | validation: 0.006663594666287392]
	TIME [epoch: 13.5 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006097283475694213		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.006097283475694213 | validation: 0.0062809161654242005]
	TIME [epoch: 13.5 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006503707004550388		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.006503707004550388 | validation: 0.010137494793121808]
	TIME [epoch: 13.5 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010087735672586445		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.010087735672586445 | validation: 0.009101335442636829]
	TIME [epoch: 13.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00973342442304292		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.00973342442304292 | validation: 0.006761712929629224]
	TIME [epoch: 13.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006461628652479779		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.006461628652479779 | validation: 0.005928624189289289]
	TIME [epoch: 13.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008985409662798783		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.008985409662798783 | validation: 0.019503827258599005]
	TIME [epoch: 13.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015691563304018948		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.015691563304018948 | validation: 0.012017951622174316]
	TIME [epoch: 13.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008134990601033742		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.008134990601033742 | validation: 0.00854411635209391]
	TIME [epoch: 13.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006386670176410231		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.006386670176410231 | validation: 0.010081569173445827]
	TIME [epoch: 13.5 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010296172349988161		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.010296172349988161 | validation: 0.008473458817083583]
	TIME [epoch: 13.6 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006641538867150917		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.006641538867150917 | validation: 0.009792869148699684]
	TIME [epoch: 13.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008418878214958767		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.008418878214958767 | validation: 0.009261081652370138]
	TIME [epoch: 13.5 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0082218641239534		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.0082218641239534 | validation: 0.007920539550579817]
	TIME [epoch: 13.5 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007665520835228796		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.007665520835228796 | validation: 0.006855194273667053]
	TIME [epoch: 13.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008435184306019113		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.008435184306019113 | validation: 0.02361845282538886]
	TIME [epoch: 13.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01601807835133635		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.01601807835133635 | validation: 0.008050530062692081]
	TIME [epoch: 13.5 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006354735305165654		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.006354735305165654 | validation: 0.006207193752039355]
	TIME [epoch: 13.5 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006494160030831779		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.006494160030831779 | validation: 0.007622033564015392]
	TIME [epoch: 13.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008209509096392991		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.008209509096392991 | validation: 0.005426665615976489]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005078868597164386		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.005078868597164386 | validation: 0.006153753013948136]
	TIME [epoch: 13.5 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009854114793063865		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.009854114793063865 | validation: 0.00971035986474798]
	TIME [epoch: 13.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006578977514771868		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.006578977514771868 | validation: 0.006104320582638056]
	TIME [epoch: 13.5 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055938394419089285		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.0055938394419089285 | validation: 0.006301255725777087]
	TIME [epoch: 13.5 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00786752009826083		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.00786752009826083 | validation: 0.008909782105241022]
	TIME [epoch: 13.5 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008166542053880895		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.008166542053880895 | validation: 0.005977698583977784]
	TIME [epoch: 13.5 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01111602881001568		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.01111602881001568 | validation: 0.0048591257543272575]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_557.pth
	Model improved!!!
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006533694046634921		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.006533694046634921 | validation: 0.009328118368395684]
	TIME [epoch: 13.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00853705539186507		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.00853705539186507 | validation: 0.005805077525695386]
	TIME [epoch: 13.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006884074725867095		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.006884074725867095 | validation: 0.013754320017308055]
	TIME [epoch: 13.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008301909873513468		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.008301909873513468 | validation: 0.0050712574336701545]
	TIME [epoch: 13.5 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007703914175032695		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.007703914175032695 | validation: 0.007659822611490288]
	TIME [epoch: 13.5 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006517364283093596		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.006517364283093596 | validation: 0.006435874470672309]
	TIME [epoch: 13.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005851524905090174		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.005851524905090174 | validation: 0.0067309421122952705]
	TIME [epoch: 13.5 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010463318689541588		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.010463318689541588 | validation: 0.006711598036290229]
	TIME [epoch: 13.5 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0069494579899101955		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.0069494579899101955 | validation: 0.007206572801924476]
	TIME [epoch: 13.5 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005698175018527298		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.005698175018527298 | validation: 0.006876778955186894]
	TIME [epoch: 13.5 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006627829693428035		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.006627829693428035 | validation: 0.008857792797273006]
	TIME [epoch: 13.5 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008226967925684833		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.008226967925684833 | validation: 0.011003313358420417]
	TIME [epoch: 13.5 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007033263741704337		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.007033263741704337 | validation: 0.006727162502911344]
	TIME [epoch: 13.5 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005973277924995838		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.005973277924995838 | validation: 0.00780090111083814]
	TIME [epoch: 13.5 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00961339753134968		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.00961339753134968 | validation: 0.0070819134758892364]
	TIME [epoch: 13.5 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005595317985386748		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.005595317985386748 | validation: 0.008453047441771075]
	TIME [epoch: 13.5 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0070579094732764695		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.0070579094732764695 | validation: 0.017035177322452744]
	TIME [epoch: 13.5 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008328219739428459		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.008328219739428459 | validation: 0.012172754930805545]
	TIME [epoch: 13.5 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009211964881423585		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.009211964881423585 | validation: 0.005243131362561838]
	TIME [epoch: 13.5 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005606422455415222		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.005606422455415222 | validation: 0.006599029936655682]
	TIME [epoch: 13.5 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007623304173644608		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.007623304173644608 | validation: 0.008430173461582442]
	TIME [epoch: 13.5 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076913940469784964		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.0076913940469784964 | validation: 0.006636079399700811]
	TIME [epoch: 13.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007168563125564693		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.007168563125564693 | validation: 0.007535404025559802]
	TIME [epoch: 13.5 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006214786581706861		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.006214786581706861 | validation: 0.008597470628578555]
	TIME [epoch: 13.5 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005594626731955184		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.005594626731955184 | validation: 0.005202805083320068]
	TIME [epoch: 13.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008256515342723297		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.008256515342723297 | validation: 0.010418913567624546]
	TIME [epoch: 13.5 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00694288431448476		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.00694288431448476 | validation: 0.005167790099187669]
	TIME [epoch: 13.5 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006301117852010162		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.006301117852010162 | validation: 0.014981428109451338]
	TIME [epoch: 13.6 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008573969754562588		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.008573969754562588 | validation: 0.006086164749011181]
	TIME [epoch: 13.5 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006601684546598774		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.006601684546598774 | validation: 0.005182003694362743]
	TIME [epoch: 13.5 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005483384292399892		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.005483384292399892 | validation: 0.00969078479884991]
	TIME [epoch: 13.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008636185909098527		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.008636185909098527 | validation: 0.004804501023627611]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005419274356001383		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.005419274356001383 | validation: 0.008400399166793204]
	TIME [epoch: 13.6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005474128112083593		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.005474128112083593 | validation: 0.006475077569666298]
	TIME [epoch: 13.5 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008283579629159777		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.008283579629159777 | validation: 0.00504838734365326]
	TIME [epoch: 13.5 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005478620278458466		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.005478620278458466 | validation: 0.005857075197577295]
	TIME [epoch: 13.6 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008050722386609765		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.008050722386609765 | validation: 0.011966550851548272]
	TIME [epoch: 13.5 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007113307380838749		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.007113307380838749 | validation: 0.0049069285877164645]
	TIME [epoch: 13.5 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005450865099881001		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.005450865099881001 | validation: 0.007287519198460315]
	TIME [epoch: 13.6 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007751073234531022		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.007751073234531022 | validation: 0.004569663281952052]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00664130162170478		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.00664130162170478 | validation: 0.011607448707058234]
	TIME [epoch: 13.5 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00816784914986007		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.00816784914986007 | validation: 0.008670505795651207]
	TIME [epoch: 13.5 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006883668664780708		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.006883668664780708 | validation: 0.007955544557675994]
	TIME [epoch: 13.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057955364172314824		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.0057955364172314824 | validation: 0.007191455196350907]
	TIME [epoch: 13.5 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0073540964017004004		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.0073540964017004004 | validation: 0.008441405583018705]
	TIME [epoch: 13.5 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005437511217949938		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.005437511217949938 | validation: 0.005068779783948334]
	TIME [epoch: 13.5 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005340608561100096		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.005340608561100096 | validation: 0.01057443703600881]
	TIME [epoch: 13.6 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008324169577288687		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.008324169577288687 | validation: 0.005347703147521877]
	TIME [epoch: 13.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009515975643484916		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.009515975643484916 | validation: 0.005015765176851322]
	TIME [epoch: 13.5 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00709255263570822		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.00709255263570822 | validation: 0.005752664742662468]
	TIME [epoch: 13.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005049932461638532		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.005049932461638532 | validation: 0.0050570756940714525]
	TIME [epoch: 13.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00634240775444519		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.00634240775444519 | validation: 0.013529575562554179]
	TIME [epoch: 13.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006965862907412856		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.006965862907412856 | validation: 0.00569293154143458]
	TIME [epoch: 13.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004765328295309982		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.004765328295309982 | validation: 0.006756054006674101]
	TIME [epoch: 13.5 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008120124302155998		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.008120124302155998 | validation: 0.007890796960197179]
	TIME [epoch: 13.5 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053695477438027465		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.0053695477438027465 | validation: 0.005178936011366765]
	TIME [epoch: 13.5 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005327758711487745		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.005327758711487745 | validation: 0.006949787697003627]
	TIME [epoch: 13.5 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005553842337067145		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.005553842337067145 | validation: 0.007704497947753434]
	TIME [epoch: 13.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00841481929448163		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.00841481929448163 | validation: 0.00560717935293768]
	TIME [epoch: 13.5 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005870363469030246		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.005870363469030246 | validation: 0.00592405308383801]
	TIME [epoch: 13.5 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005738193996981843		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.005738193996981843 | validation: 0.005377197158099796]
	TIME [epoch: 13.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007078602472105075		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.007078602472105075 | validation: 0.007093086119222384]
	TIME [epoch: 13.5 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005821016009782357		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.005821016009782357 | validation: 0.005335738264060077]
	TIME [epoch: 13.5 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005843945901271042		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.005843945901271042 | validation: 0.00723436421463523]
	TIME [epoch: 13.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0063120430988427975		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.0063120430988427975 | validation: 0.005829810373602392]
	TIME [epoch: 13.5 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005977329232237942		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.005977329232237942 | validation: 0.006759189170807589]
	TIME [epoch: 13.5 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007305877912790281		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.007305877912790281 | validation: 0.0066081693926577515]
	TIME [epoch: 13.5 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005041095350161069		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.005041095350161069 | validation: 0.00525305206337659]
	TIME [epoch: 13.5 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006270836401239896		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.006270836401239896 | validation: 0.005763972533807577]
	TIME [epoch: 13.5 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005289222205044469		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.005289222205044469 | validation: 0.005734843065820625]
	TIME [epoch: 13.5 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007900610105762365		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.007900610105762365 | validation: 0.005873265865724264]
	TIME [epoch: 13.5 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00557159523350423		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.00557159523350423 | validation: 0.009369528501291064]
	TIME [epoch: 13.6 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006011103412607599		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.006011103412607599 | validation: 0.004604364390902907]
	TIME [epoch: 13.5 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005068464863183662		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.005068464863183662 | validation: 0.0052262805612819895]
	TIME [epoch: 13.5 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004916092820789361		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.004916092820789361 | validation: 0.006918715041142361]
	TIME [epoch: 13.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007165074621428071		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.007165074621428071 | validation: 0.004692532316562574]
	TIME [epoch: 13.5 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004971889820468313		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.004971889820468313 | validation: 0.005678385232206495]
	TIME [epoch: 13.5 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00630020543516891		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.00630020543516891 | validation: 0.005375710784776536]
	TIME [epoch: 13.5 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004693460885369818		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.004693460885369818 | validation: 0.0069841982395757]
	TIME [epoch: 13.5 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006897327035787269		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.006897327035787269 | validation: 0.004204806985281234]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_637.pth
	Model improved!!!
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004677520476672623		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.004677520476672623 | validation: 0.008440036648488781]
	TIME [epoch: 13.5 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006907506483575396		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.006907506483575396 | validation: 0.007596799814135004]
	TIME [epoch: 13.5 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005658891727692356		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.005658891727692356 | validation: 0.005432134831907596]
	TIME [epoch: 13.6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004260852197159748		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.004260852197159748 | validation: 0.004936935334760424]
	TIME [epoch: 13.5 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007125374720457486		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.007125374720457486 | validation: 0.004484563749037851]
	TIME [epoch: 13.5 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005660743733113845		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.005660743733113845 | validation: 0.008015054783428443]
	TIME [epoch: 13.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006724533542832423		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.006724533542832423 | validation: 0.006149404921538997]
	TIME [epoch: 13.5 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004819653886630025		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.004819653886630025 | validation: 0.005969526081719582]
	TIME [epoch: 13.5 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00579644950587656		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.00579644950587656 | validation: 0.006310309100012374]
	TIME [epoch: 13.5 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006052287410808053		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.006052287410808053 | validation: 0.0057267559353301745]
	TIME [epoch: 13.5 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051484854377923835		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.0051484854377923835 | validation: 0.005391043825282141]
	TIME [epoch: 13.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004974760173821868		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.004974760173821868 | validation: 0.005234914896582342]
	TIME [epoch: 13.5 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005231422518583534		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.005231422518583534 | validation: 0.004721811056590718]
	TIME [epoch: 13.5 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005548019415210182		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.005548019415210182 | validation: 0.0067022795925946385]
	TIME [epoch: 13.6 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006243716673973281		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.006243716673973281 | validation: 0.005981357667005709]
	TIME [epoch: 13.5 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006012453087418762		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.006012453087418762 | validation: 0.004775648323800001]
	TIME [epoch: 13.5 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005032373388068943		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.005032373388068943 | validation: 0.00715771086444351]
	TIME [epoch: 13.6 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057085014087485605		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.0057085014087485605 | validation: 0.004435424423261789]
	TIME [epoch: 13.5 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003757537982806694		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.003757537982806694 | validation: 0.005622561431051099]
	TIME [epoch: 13.5 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005272303409935962		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.005272303409935962 | validation: 0.005811109222064944]
	TIME [epoch: 13.5 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007626927563422561		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.007626927563422561 | validation: 0.00444450049845629]
	TIME [epoch: 13.5 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0046847287956870935		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.0046847287956870935 | validation: 0.005353490799997191]
	TIME [epoch: 13.5 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004011161152275423		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.004011161152275423 | validation: 0.004399142963204297]
	TIME [epoch: 13.5 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004610577146337365		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.004610577146337365 | validation: 0.005339685612025186]
	TIME [epoch: 13.5 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005480571896455955		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.005480571896455955 | validation: 0.006258197050290383]
	TIME [epoch: 13.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048451025976587615		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.0048451025976587615 | validation: 0.0046197583672050475]
	TIME [epoch: 13.5 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051611904906094346		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.0051611904906094346 | validation: 0.004216124393870776]
	TIME [epoch: 13.5 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005703785343506529		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.005703785343506529 | validation: 0.007381481056525638]
	TIME [epoch: 13.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004917648234403025		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.004917648234403025 | validation: 0.004898020590496605]
	TIME [epoch: 13.5 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004369754165089946		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.004369754165089946 | validation: 0.004353414339595199]
	TIME [epoch: 13.5 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0077035353323430585		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.0077035353323430585 | validation: 0.006702487786276922]
	TIME [epoch: 13.5 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005615338110543893		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.005615338110543893 | validation: 0.006830417626301205]
	TIME [epoch: 13.5 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006083539556914534		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.006083539556914534 | validation: 0.005715693420032739]
	TIME [epoch: 13.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004386806625006993		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.004386806625006993 | validation: 0.005974097768074386]
	TIME [epoch: 13.5 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004850772234584956		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.004850772234584956 | validation: 0.0056904836920630576]
	TIME [epoch: 13.5 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004906113903416507		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.004906113903416507 | validation: 0.004797677883214842]
	TIME [epoch: 13.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007426521365616865		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.007426521365616865 | validation: 0.004642528139269322]
	TIME [epoch: 13.5 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004417016790867391		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.004417016790867391 | validation: 0.00405526372930038]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004280197658896946		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.004280197658896946 | validation: 0.00461700418267641]
	TIME [epoch: 13.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004899813202549928		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.004899813202549928 | validation: 0.00998534859226988]
	TIME [epoch: 13.5 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007006601032083788		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.007006601032083788 | validation: 0.005403351281582994]
	TIME [epoch: 13.5 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004392470930401944		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.004392470930401944 | validation: 0.003620652835275984]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_679.pth
	Model improved!!!
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005035785696715311		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.005035785696715311 | validation: 0.00512385024916652]
	TIME [epoch: 13.5 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004590928895893322		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.004590928895893322 | validation: 0.006785100501433534]
	TIME [epoch: 13.6 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006236111166081552		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.006236111166081552 | validation: 0.00554897967733296]
	TIME [epoch: 13.5 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004734913366923118		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.004734913366923118 | validation: 0.005673836137063154]
	TIME [epoch: 13.5 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004634551569909322		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.004634551569909322 | validation: 0.003998769416244245]
	TIME [epoch: 13.6 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005606700236793432		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.005606700236793432 | validation: 0.006794310123912666]
	TIME [epoch: 13.5 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005084555361234159		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.005084555361234159 | validation: 0.004125770551625187]
	TIME [epoch: 13.5 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004028191133801442		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.004028191133801442 | validation: 0.0037278219991848396]
	TIME [epoch: 13.6 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004081274736089448		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.004081274736089448 | validation: 0.007574117192592888]
	TIME [epoch: 13.5 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005024795696836678		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.005024795696836678 | validation: 0.004240739801610532]
	TIME [epoch: 13.5 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004286542381074185		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.004286542381074185 | validation: 0.003993029516145177]
	TIME [epoch: 13.5 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004555248713477938		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.004555248713477938 | validation: 0.004446571765470032]
	TIME [epoch: 13.5 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004543069637775313		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.004543069637775313 | validation: 0.009651280596938868]
	TIME [epoch: 13.6 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007204414523140714		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.007204414523140714 | validation: 0.004635716543888331]
	TIME [epoch: 13.5 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00448289046959588		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.00448289046959588 | validation: 0.003985088801764942]
	TIME [epoch: 13.5 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004307855030999387		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.004307855030999387 | validation: 0.004780153755676365]
	TIME [epoch: 13.6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005233643396980209		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.005233643396980209 | validation: 0.004709272179475028]
	TIME [epoch: 13.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040550254756654015		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.0040550254756654015 | validation: 0.004220415403298471]
	TIME [epoch: 13.5 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004069161539533678		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.004069161539533678 | validation: 0.004037364022781294]
	TIME [epoch: 13.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007502620106090598		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.007502620106090598 | validation: 0.005952443274597405]
	TIME [epoch: 13.5 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004480735427862573		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.004480735427862573 | validation: 0.004138575598637468]
	TIME [epoch: 13.5 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039798611625993475		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.0039798611625993475 | validation: 0.0051640116311936594]
	TIME [epoch: 13.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004990211711855029		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.004990211711855029 | validation: 0.006433444941303478]
	TIME [epoch: 13.5 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004757988015312887		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.004757988015312887 | validation: 0.005925820826850943]
	TIME [epoch: 13.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00517830079145361		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.00517830079145361 | validation: 0.005253272406583291]
	TIME [epoch: 13.5 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003908398237453389		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.003908398237453389 | validation: 0.004470569369260052]
	TIME [epoch: 13.5 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038185258213200004		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.0038185258213200004 | validation: 0.0049430334673602125]
	TIME [epoch: 13.6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005592863529206447		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.005592863529206447 | validation: 0.0044584865837713795]
	TIME [epoch: 13.5 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041073368297275155		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.0041073368297275155 | validation: 0.003699481844105448]
	TIME [epoch: 13.5 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038712704791506406		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.0038712704791506406 | validation: 0.005789532774204189]
	TIME [epoch: 13.6 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004564578003562931		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.004564578003562931 | validation: 0.004734141691552287]
	TIME [epoch: 13.5 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004879886671883918		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.004879886671883918 | validation: 0.0035795283608106437]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_711.pth
	Model improved!!!
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003957944613413381		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.003957944613413381 | validation: 0.0050653874475212225]
	TIME [epoch: 13.6 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00440090889232595		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.00440090889232595 | validation: 0.005151800431680616]
	TIME [epoch: 13.5 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004161707745707467		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.004161707745707467 | validation: 0.004163719949221489]
	TIME [epoch: 13.6 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004125528397432136		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.004125528397432136 | validation: 0.003912075199500784]
	TIME [epoch: 13.5 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00458163474504575		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.00458163474504575 | validation: 0.004724399453690376]
	TIME [epoch: 13.5 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004027741431565059		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.004027741431565059 | validation: 0.005336803060968038]
	TIME [epoch: 13.6 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005651718313328898		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.005651718313328898 | validation: 0.004084812130596186]
	TIME [epoch: 13.5 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004008513380953262		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.004008513380953262 | validation: 0.003932023671044737]
	TIME [epoch: 13.5 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003978373886162161		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.003978373886162161 | validation: 0.0052414816987386564]
	TIME [epoch: 13.6 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045691143508309095		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.0045691143508309095 | validation: 0.004563767493168595]
	TIME [epoch: 13.5 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005532781931434006		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.005532781931434006 | validation: 0.004764603880232643]
	TIME [epoch: 13.5 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004062569035537273		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.004062569035537273 | validation: 0.004844824844864114]
	TIME [epoch: 13.5 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003842717892084937		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.003842717892084937 | validation: 0.005460968546399737]
	TIME [epoch: 13.5 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005295568571975072		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.005295568571975072 | validation: 0.004649288445570977]
	TIME [epoch: 13.6 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004052903309558697		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.004052903309558697 | validation: 0.004022177502886377]
	TIME [epoch: 13.5 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004684765931020801		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.004684765931020801 | validation: 0.005093111860279025]
	TIME [epoch: 13.5 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004052087622811019		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.004052087622811019 | validation: 0.004632717740420434]
	TIME [epoch: 13.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047090579438141674		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.0047090579438141674 | validation: 0.005544293683680191]
	TIME [epoch: 13.5 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00400452322791606		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.00400452322791606 | validation: 0.003463536805012238]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_730.pth
	Model improved!!!
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033098413041763993		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.0033098413041763993 | validation: 0.005563829423347965]
	TIME [epoch: 13.6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004982252849198965		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.004982252849198965 | validation: 0.0068365981877098565]
	TIME [epoch: 13.5 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005600600852001359		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.005600600852001359 | validation: 0.005113935083966691]
	TIME [epoch: 13.5 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042929097656465515		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.0042929097656465515 | validation: 0.004799533282856736]
	TIME [epoch: 13.5 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035304963405067813		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.0035304963405067813 | validation: 0.0035209670917846084]
	TIME [epoch: 13.5 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044503369325388275		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.0044503369325388275 | validation: 0.0046461475312695195]
	TIME [epoch: 13.6 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004147513112712262		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.004147513112712262 | validation: 0.004133157961715054]
	TIME [epoch: 13.5 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004754931452955583		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.004754931452955583 | validation: 0.004002488623735652]
	TIME [epoch: 13.5 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037819721356315604		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.0037819721356315604 | validation: 0.003510536257920472]
	TIME [epoch: 13.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003836456337401671		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.003836456337401671 | validation: 0.007815086512536557]
	TIME [epoch: 13.5 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004970903428932692		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.004970903428932692 | validation: 0.005189300057320402]
	TIME [epoch: 13.5 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003960299268846574		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.003960299268846574 | validation: 0.004185231261833389]
	TIME [epoch: 13.6 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037297843367461287		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.0037297843367461287 | validation: 0.005061611244972273]
	TIME [epoch: 13.5 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004733231093256005		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.004733231093256005 | validation: 0.00393488382712129]
	TIME [epoch: 13.5 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004005760799500044		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.004005760799500044 | validation: 0.00436849928897246]
	TIME [epoch: 13.5 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004244911127848396		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.004244911127848396 | validation: 0.004154148891827832]
	TIME [epoch: 13.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035795576982383935		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.0035795576982383935 | validation: 0.003953363272465699]
	TIME [epoch: 13.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042737403804254965		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.0042737403804254965 | validation: 0.0033465091406272144]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00388029802603512		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.00388029802603512 | validation: 0.003935510066492418]
	TIME [epoch: 13.5 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036829809993051297		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.0036829809993051297 | validation: 0.004265958478950095]
	TIME [epoch: 13.6 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004017106455336095		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.004017106455336095 | validation: 0.0038335895140184425]
	TIME [epoch: 13.5 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034363008333547144		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.0034363008333547144 | validation: 0.0038779250586181905]
	TIME [epoch: 13.5 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004278924661080657		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.004278924661080657 | validation: 0.004019499600759497]
	TIME [epoch: 13.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004175480448760822		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.004175480448760822 | validation: 0.005264032103145283]
	TIME [epoch: 13.5 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037620543331612034		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.0037620543331612034 | validation: 0.0032986561227491247]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038055388898193135		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.0038055388898193135 | validation: 0.005234225690071601]
	TIME [epoch: 13.5 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003947666162581681		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.003947666162581681 | validation: 0.004107906175126307]
	TIME [epoch: 13.5 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032730762702185153		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.0032730762702185153 | validation: 0.004493229699524762]
	TIME [epoch: 13.5 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003904554451100853		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.003904554451100853 | validation: 0.0037145175371688546]
	TIME [epoch: 13.5 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003541198863505266		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.003541198863505266 | validation: 0.004139733126648792]
	TIME [epoch: 13.5 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042593091450752575		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.0042593091450752575 | validation: 0.004292799061239487]
	TIME [epoch: 13.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032238804258977504		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.0032238804258977504 | validation: 0.0034587890014677655]
	TIME [epoch: 13.5 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004146421037979342		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.004146421037979342 | validation: 0.00392826980125832]
	TIME [epoch: 13.5 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004137451434654915		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.004137451434654915 | validation: 0.004024204213922154]
	TIME [epoch: 13.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003489939173061044		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.003489939173061044 | validation: 0.003205491750187882]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_765.pth
	Model improved!!!
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032075969954852377		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.0032075969954852377 | validation: 0.005256047404162274]
	TIME [epoch: 13.5 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004288564187661351		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.004288564187661351 | validation: 0.0048746501412297]
	TIME [epoch: 13.5 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004864260325359131		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.004864260325359131 | validation: 0.003568222221549032]
	TIME [epoch: 13.5 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035072232829129963		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.0035072232829129963 | validation: 0.004172690290503722]
	TIME [epoch: 13.5 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035111953530558146		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.0035111953530558146 | validation: 0.0036731138982452685]
	TIME [epoch: 13.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032997038582913674		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.0032997038582913674 | validation: 0.0037938424355232667]
	TIME [epoch: 13.5 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003936671284170246		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.003936671284170246 | validation: 0.004936444954728133]
	TIME [epoch: 13.5 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003698521391402904		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.003698521391402904 | validation: 0.00405073872428037]
	TIME [epoch: 13.5 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004152807396886612		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.004152807396886612 | validation: 0.0038967809740712064]
	TIME [epoch: 13.5 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003285374634066214		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.003285374634066214 | validation: 0.0031313955979611254]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_775.pth
	Model improved!!!
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035647291917842287		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.0035647291917842287 | validation: 0.003986474048372871]
	TIME [epoch: 13.5 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034910180708557117		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.0034910180708557117 | validation: 0.0035595853805773323]
	TIME [epoch: 13.5 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043869174818646495		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.0043869174818646495 | validation: 0.0034901379622095594]
	TIME [epoch: 13.5 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035048424609713446		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.0035048424609713446 | validation: 0.003427335064327075]
	TIME [epoch: 13.5 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003231976189448687		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.003231976189448687 | validation: 0.00421644952679581]
	TIME [epoch: 13.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037426730669646966		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.0037426730669646966 | validation: 0.0039055906172600343]
	TIME [epoch: 13.5 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034948786341410945		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.0034948786341410945 | validation: 0.004697460273052019]
	TIME [epoch: 13.5 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005864906521186469		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.005864906521186469 | validation: 0.004044520723176746]
	TIME [epoch: 13.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036932754775853534		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.0036932754775853534 | validation: 0.0046346377869904785]
	TIME [epoch: 13.5 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003557204275800202		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.003557204275800202 | validation: 0.0030313810573611233]
	TIME [epoch: 13.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003591557095530149		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.003591557095530149 | validation: 0.0034432258619451067]
	TIME [epoch: 13.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003465106710523367		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.003465106710523367 | validation: 0.0034695737766722717]
	TIME [epoch: 13.6 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003831393320682456		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.003831393320682456 | validation: 0.004235010239080942]
	TIME [epoch: 13.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003513841873230881		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.003513841873230881 | validation: 0.0028311581032066982]
	TIME [epoch: 13.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd_20240516_135440/states/model_phi1_1a_v_mmd_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003266843560756889		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.003266843560756889 | validation: 0.005437720450104841]
	TIME [epoch: 13.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004155135401926121		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.004155135401926121 | validation: 0.004997211662746399]
	TIME [epoch: 13.6 sec]
EPOCH 792/2000:
	Training over batches...
