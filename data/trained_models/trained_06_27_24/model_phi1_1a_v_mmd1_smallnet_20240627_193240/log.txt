Args:
Namespace(name='model_phi1_1a_v_mmd1_smallnet', outdir='out/model_training/model_phi1_1a_v_mmd1_smallnet', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[8, 16, 8], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 385468495

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.651641413400542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.651641413400542 | validation: 5.664424294054932]
	TIME [epoch: 118 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.966203670136019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.966203670136019 | validation: 5.707144985118649]
	TIME [epoch: 7.89 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8807026474522734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8807026474522734 | validation: 4.4735924367442]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.298906077918659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.298906077918659 | validation: 4.1688802372968]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.054446506325681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.054446506325681 | validation: 4.201940778211084]
	TIME [epoch: 7.88 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.937600254079647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.937600254079647 | validation: 3.8432744120753224]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.935140454314457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.935140454314457 | validation: 4.009402333465188]
	TIME [epoch: 7.83 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.63929399089468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.63929399089468 | validation: 3.6018487326690725]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.420857947285403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.420857947285403 | validation: 3.3968183075485876]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1422338374817373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1422338374817373 | validation: 3.346351235354151]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9547043853760897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9547043853760897 | validation: 2.813770450365409]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.832651537560822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.832651537560822 | validation: 2.6953557105810377]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4598006418177976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4598006418177976 | validation: 2.7408028396568467]
	TIME [epoch: 7.83 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3141333013723244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3141333013723244 | validation: 2.3787287402917436]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8871571891428276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8871571891428276 | validation: 2.113658291308928]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7806852239699575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7806852239699575 | validation: 2.2510860625751787]
	TIME [epoch: 7.77 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.657432536685644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.657432536685644 | validation: 1.9726638111151225]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4919446865901265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4919446865901265 | validation: 2.2954553359465732]
	TIME [epoch: 7.82 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6031762623373167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6031762623373167 | validation: 1.915818230674836]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4673335785930885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4673335785930885 | validation: 1.825268329750545]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4441642287047523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4441642287047523 | validation: 1.7335725622172746]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3340528264149503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3340528264149503 | validation: 1.690448229892753]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2736680666781328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2736680666781328 | validation: 0.9476815897195028]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7705627157125174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7705627157125174 | validation: 0.7423293141872045]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6017566493493267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6017566493493267 | validation: 0.567315452073562]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.233163686112495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.233163686112495 | validation: 0.8025379063019641]
	TIME [epoch: 7.77 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.743074097526423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.743074097526423 | validation: 0.9114337307373322]
	TIME [epoch: 7.77 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7656869039499461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7656869039499461 | validation: 0.707982070768975]
	TIME [epoch: 7.81 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5819955205024531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5819955205024531 | validation: 0.5003942988973792]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.038018852062349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.038018852062349 | validation: 3.0717670836012063]
	TIME [epoch: 7.8 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.342070599227355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.342070599227355 | validation: 1.9239712426722952]
	TIME [epoch: 7.81 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.197832223440674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.197832223440674 | validation: 1.256057287697559]
	TIME [epoch: 7.86 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8849190595416319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8849190595416319 | validation: 0.6918305392574475]
	TIME [epoch: 7.83 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5683438018090838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5683438018090838 | validation: 0.7978576649867225]
	TIME [epoch: 7.82 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6683868997015455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6683868997015455 | validation: 0.36866601519592335]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.581671862683085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.581671862683085 | validation: 1.726674404949815]
	TIME [epoch: 7.82 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2044650621018023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2044650621018023 | validation: 0.7323102404345858]
	TIME [epoch: 7.87 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5667682932091955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5667682932091955 | validation: 0.5376640169663289]
	TIME [epoch: 7.82 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5005042159768567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5005042159768567 | validation: 0.5487864950884688]
	TIME [epoch: 7.81 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5490434874320222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5490434874320222 | validation: 0.4127387113585873]
	TIME [epoch: 7.83 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5551857708326718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5551857708326718 | validation: 0.6778052169160766]
	TIME [epoch: 7.83 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5124922017884042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5124922017884042 | validation: 0.32302421702585593]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5732284416298248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5732284416298248 | validation: 1.0046542017702995]
	TIME [epoch: 7.83 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5185440873775684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5185440873775684 | validation: 1.739364128243421]
	TIME [epoch: 7.83 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0767348825062464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0767348825062464 | validation: 0.5052789636152356]
	TIME [epoch: 7.83 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42588147621164496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42588147621164496 | validation: 0.28205634048316175]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34518802306767377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34518802306767377 | validation: 0.41253329462817345]
	TIME [epoch: 7.86 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.596702519583352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.596702519583352 | validation: 0.6600409478185096]
	TIME [epoch: 7.83 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5196794904455915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5196794904455915 | validation: 0.3320776033610985]
	TIME [epoch: 7.83 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31600590961298447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31600590961298447 | validation: 0.33283664121762213]
	TIME [epoch: 7.84 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4626916166745661		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.4626916166745661 | validation: 0.554885737879874]
	TIME [epoch: 7.88 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4193609427006107		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.4193609427006107 | validation: 0.31549529114726527]
	TIME [epoch: 7.85 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6653247288757155		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.6653247288757155 | validation: 0.7678973663900913]
	TIME [epoch: 7.83 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46676188931993073		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.46676188931993073 | validation: 0.3397977067212953]
	TIME [epoch: 7.83 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3558088097880188		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.3558088097880188 | validation: 0.385874570581468]
	TIME [epoch: 7.84 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3898302049219281		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.3898302049219281 | validation: 0.37411899260109494]
	TIME [epoch: 7.89 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31241476128739876		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.31241476128739876 | validation: 0.5204281332860087]
	TIME [epoch: 7.85 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5189038500336711		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.5189038500336711 | validation: 0.38192071607580136]
	TIME [epoch: 7.83 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41553312470267617		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.41553312470267617 | validation: 0.4386794480850342]
	TIME [epoch: 7.83 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3846094260119549		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.3846094260119549 | validation: 0.6957294889965924]
	TIME [epoch: 7.86 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39823368207018095		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.39823368207018095 | validation: 0.23080354223284477]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2781253416720506		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.2781253416720506 | validation: 0.3637417372421543]
	TIME [epoch: 7.84 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33615224602785104		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.33615224602785104 | validation: 0.3524223158035964]
	TIME [epoch: 7.85 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4729947304943434		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.4729947304943434 | validation: 0.6557296491353322]
	TIME [epoch: 7.84 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4076740177663098		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.4076740177663098 | validation: 0.2132001094260474]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28734815183668205		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.28734815183668205 | validation: 0.2971007128582752]
	TIME [epoch: 7.87 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3123840617526926		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.3123840617526926 | validation: 0.34584440395925453]
	TIME [epoch: 7.85 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3835272451127104		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.3835272451127104 | validation: 0.46908752015716165]
	TIME [epoch: 7.85 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35889463460931315		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.35889463460931315 | validation: 0.24925852807376916]
	TIME [epoch: 7.85 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2698097622797475		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.2698097622797475 | validation: 0.4625215235964645]
	TIME [epoch: 7.9 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3702098110814372		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.3702098110814372 | validation: 0.3338504680844637]
	TIME [epoch: 7.85 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38940861758589457		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.38940861758589457 | validation: 0.29816088835858334]
	TIME [epoch: 7.84 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23894133343322377		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.23894133343322377 | validation: 0.24156291437762]
	TIME [epoch: 7.85 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4335744409228842		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.4335744409228842 | validation: 0.397327106418492]
	TIME [epoch: 7.85 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27619107348893884		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.27619107348893884 | validation: 0.2810027808409711]
	TIME [epoch: 7.9 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30384981972267033		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.30384981972267033 | validation: 0.2673729411920565]
	TIME [epoch: 7.85 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21498791074460522		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.21498791074460522 | validation: 0.6363069450883998]
	TIME [epoch: 7.85 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4385681083732669		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.4385681083732669 | validation: 0.26604768917058025]
	TIME [epoch: 7.85 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2803904047316709		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.2803904047316709 | validation: 0.9503964747241072]
	TIME [epoch: 7.87 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5920769555068873		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.5920769555068873 | validation: 0.2854294436560123]
	TIME [epoch: 7.9 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23678931572042478		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.23678931572042478 | validation: 0.15725696162417285]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18059711985076563		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.18059711985076563 | validation: 0.16459059593010633]
	TIME [epoch: 7.84 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2437980976834376		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.2437980976834376 | validation: 0.2483314424264259]
	TIME [epoch: 8.11 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25866251294294246		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.25866251294294246 | validation: 0.38556799025874594]
	TIME [epoch: 7.84 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25448012824609384		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.25448012824609384 | validation: 0.20381652530501027]
	TIME [epoch: 7.77 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3298730662359054		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.3298730662359054 | validation: 0.5636717560208013]
	TIME [epoch: 7.74 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3498965779876798		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.3498965779876798 | validation: 0.3102909252926267]
	TIME [epoch: 7.77 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2720926025435139		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.2720926025435139 | validation: 0.18581629261500435]
	TIME [epoch: 7.76 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1955565724981508		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.1955565724981508 | validation: 0.3733131404891985]
	TIME [epoch: 7.81 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24319227245257552		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.24319227245257552 | validation: 0.18568579809442376]
	TIME [epoch: 7.79 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2236864161235098		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.2236864161235098 | validation: 0.4854036892522322]
	TIME [epoch: 7.77 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33765201289963936		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.33765201289963936 | validation: 0.22498105122160372]
	TIME [epoch: 7.78 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36128215561623583		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.36128215561623583 | validation: 0.21795280598609637]
	TIME [epoch: 7.76 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20998615187748396		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.20998615187748396 | validation: 0.1797964839505479]
	TIME [epoch: 7.81 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14514790665473482		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.14514790665473482 | validation: 0.1537407827571358]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14258511879621272		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.14258511879621272 | validation: 0.40141803513986196]
	TIME [epoch: 7.81 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28506057806397533		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.28506057806397533 | validation: 0.17504899198894372]
	TIME [epoch: 7.81 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2900822094842673		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.2900822094842673 | validation: 0.20772066072476253]
	TIME [epoch: 7.83 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21512192944967531		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.21512192944967531 | validation: 0.11417121586406138]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1707056758522013		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.1707056758522013 | validation: 0.21416003151573076]
	TIME [epoch: 7.81 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25174086806921225		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.25174086806921225 | validation: 0.18089647161639022]
	TIME [epoch: 7.84 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21277311120280662		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.21277311120280662 | validation: 0.13442086560997824]
	TIME [epoch: 7.84 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18973213163305136		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.18973213163305136 | validation: 0.3002169888382158]
	TIME [epoch: 7.88 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24208919817575447		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.24208919817575447 | validation: 0.11430294965820556]
	TIME [epoch: 7.84 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1477050957149566		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.1477050957149566 | validation: 0.2034298133175572]
	TIME [epoch: 7.84 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17335103843692923		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.17335103843692923 | validation: 0.3148111049701493]
	TIME [epoch: 7.83 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31870188410919087		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.31870188410919087 | validation: 0.2116316139581434]
	TIME [epoch: 7.84 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23425102354738853		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.23425102354738853 | validation: 0.1366586033912992]
	TIME [epoch: 7.89 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13249577729643866		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.13249577729643866 | validation: 0.16513197793966344]
	TIME [epoch: 7.83 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26451038172599284		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.26451038172599284 | validation: 0.20381488189480312]
	TIME [epoch: 7.83 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14072496627186523		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.14072496627186523 | validation: 0.17578520073070136]
	TIME [epoch: 7.83 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1626744210873185		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.1626744210873185 | validation: 0.14162639528367682]
	TIME [epoch: 7.85 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.160399905150886		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.160399905150886 | validation: 0.19190587521712094]
	TIME [epoch: 7.87 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16464537256695919		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.16464537256695919 | validation: 0.12917243264186787]
	TIME [epoch: 7.82 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1876296460710054		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.1876296460710054 | validation: 0.36022880113127964]
	TIME [epoch: 7.83 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2774019174877851		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.2774019174877851 | validation: 0.22426595595359178]
	TIME [epoch: 7.83 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23769944251492284		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.23769944251492284 | validation: 0.10825391283764263]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13694897283077168		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.13694897283077168 | validation: 0.1572898543748965]
	TIME [epoch: 7.86 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1441336933240337		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.1441336933240337 | validation: 0.10720685463156232]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10745930322212252		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.10745930322212252 | validation: 0.12534133597262365]
	TIME [epoch: 7.85 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3029061958187589		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.3029061958187589 | validation: 0.27817436693712827]
	TIME [epoch: 7.85 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1561068018550859		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.1561068018550859 | validation: 0.0889609881945724]
	TIME [epoch: 7.89 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08851554343979273		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.08851554343979273 | validation: 0.08006216857967853]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1371113275322992		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.1371113275322992 | validation: 0.41618306939805305]
	TIME [epoch: 7.85 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2321973733349606		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.2321973733349606 | validation: 0.22816321404173956]
	TIME [epoch: 7.84 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13280806334867093		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.13280806334867093 | validation: 0.13813027649752058]
	TIME [epoch: 7.87 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09564181090583138		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.09564181090583138 | validation: 0.06288405235919624]
	TIME [epoch: 7.89 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09565768702787568		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.09565768702787568 | validation: 0.12401067923004408]
	TIME [epoch: 7.78 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10578731011405337		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.10578731011405337 | validation: 0.1295389876012434]
	TIME [epoch: 7.78 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23783443364613635		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.23783443364613635 | validation: 0.10755453578274396]
	TIME [epoch: 7.78 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12542948548445176		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.12542948548445176 | validation: 0.08245309423263278]
	TIME [epoch: 7.82 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12650596210027587		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.12650596210027587 | validation: 0.20305369959889277]
	TIME [epoch: 7.8 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1535861516028054		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.1535861516028054 | validation: 0.11838209924171236]
	TIME [epoch: 7.79 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10400475305792885		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.10400475305792885 | validation: 0.27141094066735816]
	TIME [epoch: 7.78 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2034537424360525		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.2034537424360525 | validation: 0.12198388312216388]
	TIME [epoch: 7.77 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13365810687483082		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.13365810687483082 | validation: 0.10322905711638616]
	TIME [epoch: 7.81 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10521653030406053		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.10521653030406053 | validation: 0.07763953965519774]
	TIME [epoch: 7.79 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11832513452469551		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.11832513452469551 | validation: 0.07786553820347146]
	TIME [epoch: 7.77 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1072293084865716		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.1072293084865716 | validation: 0.07793703901608995]
	TIME [epoch: 7.79 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13667228365082212		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.13667228365082212 | validation: 0.07399610146468977]
	TIME [epoch: 7.79 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13196739420596593		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.13196739420596593 | validation: 0.18622232548822515]
	TIME [epoch: 7.83 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13012346111244158		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.13012346111244158 | validation: 0.07585505695672377]
	TIME [epoch: 7.79 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10966827430471479		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.10966827430471479 | validation: 0.13388531024960398]
	TIME [epoch: 7.79 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.123188561778866		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.123188561778866 | validation: 0.16063056852962926]
	TIME [epoch: 7.79 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12981455717805374		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.12981455717805374 | validation: 0.10010921138040682]
	TIME [epoch: 7.82 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10485196713338552		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.10485196713338552 | validation: 0.09844464345930468]
	TIME [epoch: 7.83 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09769107038835867		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.09769107038835867 | validation: 0.0573595727764597]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058307722587521094		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.058307722587521094 | validation: 0.0962756673859834]
	TIME [epoch: 7.82 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12443630319952742		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.12443630319952742 | validation: 0.17519423834712117]
	TIME [epoch: 7.82 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15328281967530413		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.15328281967530413 | validation: 0.06304887851544105]
	TIME [epoch: 7.88 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06851964517057392		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.06851964517057392 | validation: 0.05429523423697856]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10743425866326464		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.10743425866326464 | validation: 0.09962373703106275]
	TIME [epoch: 7.83 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15101269678796142		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.15101269678796142 | validation: 0.11601909141092587]
	TIME [epoch: 7.83 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07553328675489454		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.07553328675489454 | validation: 0.057566882224955276]
	TIME [epoch: 7.83 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10195971935472045		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.10195971935472045 | validation: 0.0822557203100054]
	TIME [epoch: 7.88 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0963736802342001		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.0963736802342001 | validation: 0.08851855140040082]
	TIME [epoch: 7.84 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09578973263794179		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.09578973263794179 | validation: 0.06162322750578392]
	TIME [epoch: 7.84 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1002008401257285		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.1002008401257285 | validation: 0.08838840793281348]
	TIME [epoch: 7.83 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07118219008622063		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.07118219008622063 | validation: 0.07962549140230865]
	TIME [epoch: 7.85 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13537499214933313		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.13537499214933313 | validation: 0.07796098011200825]
	TIME [epoch: 7.89 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12190031278198657		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.12190031278198657 | validation: 0.19039415397070114]
	TIME [epoch: 7.84 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14323417223073723		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.14323417223073723 | validation: 0.09214164585215198]
	TIME [epoch: 7.83 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09858739032947739		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.09858739032947739 | validation: 0.12958855001747355]
	TIME [epoch: 7.85 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10925622163626224		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.10925622163626224 | validation: 0.056549588317422995]
	TIME [epoch: 7.85 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08045042668196878		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.08045042668196878 | validation: 0.059692493812961656]
	TIME [epoch: 7.88 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060587483696280475		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.060587483696280475 | validation: 0.05693343134549879]
	TIME [epoch: 7.82 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05242373418982904		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.05242373418982904 | validation: 0.07785134449694533]
	TIME [epoch: 7.84 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10135300402333366		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.10135300402333366 | validation: 0.09077612295037821]
	TIME [epoch: 7.84 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06691871741933442		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.06691871741933442 | validation: 0.0625623859802101]
	TIME [epoch: 7.88 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08946710194234238		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.08946710194234238 | validation: 0.12365893137032208]
	TIME [epoch: 7.85 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09767197814794139		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.09767197814794139 | validation: 0.04285371871718652]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07960351732097637		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.07960351732097637 | validation: 0.09501931067341482]
	TIME [epoch: 7.84 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11001278560608205		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.11001278560608205 | validation: 0.073080090309431]
	TIME [epoch: 7.85 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06630689259566873		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.06630689259566873 | validation: 0.03629043131296278]
	TIME [epoch: 7.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07837548651534795		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.07837548651534795 | validation: 0.08009388109583268]
	TIME [epoch: 7.85 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059758606874955725		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.059758606874955725 | validation: 0.10938594067931424]
	TIME [epoch: 7.84 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07994287544539222		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.07994287544539222 | validation: 0.049887030033210246]
	TIME [epoch: 7.85 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12746991491265242		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.12746991491265242 | validation: 0.06515505477292975]
	TIME [epoch: 7.86 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11789198806828059		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.11789198806828059 | validation: 0.04302380636041152]
	TIME [epoch: 7.88 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0540502885490504		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.0540502885490504 | validation: 0.05378059855277263]
	TIME [epoch: 7.85 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053327240972336994		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.053327240972336994 | validation: 0.09747866429257196]
	TIME [epoch: 7.85 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07968608913314		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.07968608913314 | validation: 0.04778903837188305]
	TIME [epoch: 7.85 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07162351576239005		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.07162351576239005 | validation: 0.0471279431175322]
	TIME [epoch: 7.89 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054262212398719825		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.054262212398719825 | validation: 0.05786290571498524]
	TIME [epoch: 7.87 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07335655149522552		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.07335655149522552 | validation: 0.10382808028987361]
	TIME [epoch: 7.85 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06859791073496514		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.06859791073496514 | validation: 0.04856005512159946]
	TIME [epoch: 7.85 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07214665475728628		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.07214665475728628 | validation: 0.11129266196899627]
	TIME [epoch: 7.85 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07665080046848016		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.07665080046848016 | validation: 0.08563628598455511]
	TIME [epoch: 7.9 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05571052384832896		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.05571052384832896 | validation: 0.06597290077780679]
	TIME [epoch: 7.86 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09070578417720132		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.09070578417720132 | validation: 0.05923542486733353]
	TIME [epoch: 7.85 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10113801448784004		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.10113801448784004 | validation: 0.04345935092877572]
	TIME [epoch: 7.84 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05056815014659993		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.05056815014659993 | validation: 0.037157904684697915]
	TIME [epoch: 7.85 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045179802031245046		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.045179802031245046 | validation: 0.04208106780432347]
	TIME [epoch: 7.9 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05470445907648008		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.05470445907648008 | validation: 0.048630634228936945]
	TIME [epoch: 7.85 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055351544699114096		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.055351544699114096 | validation: 0.03679874878688498]
	TIME [epoch: 7.85 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03361204245008244		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.03361204245008244 | validation: 0.0391444826218877]
	TIME [epoch: 7.84 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07414969590788584		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.07414969590788584 | validation: 0.023894884188590322]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08059113611773501		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.08059113611773501 | validation: 0.0674323918922488]
	TIME [epoch: 7.9 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06055245762275243		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.06055245762275243 | validation: 0.04796193993254377]
	TIME [epoch: 7.85 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03999219275166891		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.03999219275166891 | validation: 0.05420857507703064]
	TIME [epoch: 7.85 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09489987295751798		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.09489987295751798 | validation: 0.07506808989401238]
	TIME [epoch: 7.84 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06045801207394593		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.06045801207394593 | validation: 0.060172232886953954]
	TIME [epoch: 8.16 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04123092304067387		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.04123092304067387 | validation: 0.032115660797004626]
	TIME [epoch: 7.78 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05931689620735813		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.05931689620735813 | validation: 0.07289254390418637]
	TIME [epoch: 7.77 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07603288329811683		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.07603288329811683 | validation: 0.0272260185797892]
	TIME [epoch: 7.77 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056304287571453025		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.056304287571453025 | validation: 0.06174171436711197]
	TIME [epoch: 7.8 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08789951485592629		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.08789951485592629 | validation: 0.05339383728154111]
	TIME [epoch: 7.83 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05315345065036273		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.05315345065036273 | validation: 0.029106550583900007]
	TIME [epoch: 7.79 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04532532540163792		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.04532532540163792 | validation: 0.03186425765920984]
	TIME [epoch: 7.78 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04962568142546024		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.04962568142546024 | validation: 0.06718714002466827]
	TIME [epoch: 7.77 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05056885959329467		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.05056885959329467 | validation: 0.02087298296431372]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03330514551112439		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.03330514551112439 | validation: 0.03489260815209386]
	TIME [epoch: 7.84 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05726633920018216		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.05726633920018216 | validation: 0.027468154229566683]
	TIME [epoch: 7.79 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051876015016552535		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.051876015016552535 | validation: 0.09286359187259613]
	TIME [epoch: 7.79 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09212684614875087		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.09212684614875087 | validation: 0.054948931839318635]
	TIME [epoch: 7.78 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052728958748678466		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.052728958748678466 | validation: 0.04781555430890208]
	TIME [epoch: 7.82 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040606028432028304		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.040606028432028304 | validation: 0.031974395294606786]
	TIME [epoch: 7.83 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054447816949972694		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.054447816949972694 | validation: 0.02301408938717797]
	TIME [epoch: 7.8 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043361775769441634		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.043361775769441634 | validation: 0.07273156965507838]
	TIME [epoch: 7.8 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05163476795100492		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.05163476795100492 | validation: 0.08220270123413007]
	TIME [epoch: 7.79 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09347173061270712		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.09347173061270712 | validation: 0.05939430963231607]
	TIME [epoch: 7.86 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04401741963869012		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.04401741963869012 | validation: 0.030947714292103412]
	TIME [epoch: 7.83 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03308930416333895		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.03308930416333895 | validation: 0.05855005787668521]
	TIME [epoch: 7.8 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05265133613915434		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.05265133613915434 | validation: 0.09444551551481838]
	TIME [epoch: 7.81 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11293896404750492		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.11293896404750492 | validation: 0.04990097946646223]
	TIME [epoch: 7.81 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04357816126504212		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.04357816126504212 | validation: 0.05014118639726864]
	TIME [epoch: 7.87 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039048071694962355		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.039048071694962355 | validation: 0.05769617938402542]
	TIME [epoch: 7.82 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047518796298860876		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.047518796298860876 | validation: 0.03037750484523845]
	TIME [epoch: 7.83 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025694302576224286		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.025694302576224286 | validation: 0.04755850830513228]
	TIME [epoch: 7.81 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06300142303413325		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.06300142303413325 | validation: 0.048136156289943624]
	TIME [epoch: 7.82 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04478238723499593		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.04478238723499593 | validation: 0.027651390454606094]
	TIME [epoch: 7.87 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036086405788830135		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.036086405788830135 | validation: 0.028767890954984193]
	TIME [epoch: 7.81 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041277162436630446		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.041277162436630446 | validation: 0.0397871786055566]
	TIME [epoch: 7.83 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04419317264321616		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.04419317264321616 | validation: 0.048770972397009935]
	TIME [epoch: 7.82 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045887976465224446		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.045887976465224446 | validation: 0.034362964877609314]
	TIME [epoch: 7.85 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062122527524970196		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.062122527524970196 | validation: 0.07494859889110969]
	TIME [epoch: 7.88 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04026610172509327		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.04026610172509327 | validation: 0.034746614542884156]
	TIME [epoch: 7.84 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054802059678028356		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.054802059678028356 | validation: 0.03539159861797909]
	TIME [epoch: 7.83 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03465031892881934		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.03465031892881934 | validation: 0.033945312263459926]
	TIME [epoch: 7.85 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062228504828965414		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.062228504828965414 | validation: 0.032674732959529225]
	TIME [epoch: 7.88 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04211506685569343		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.04211506685569343 | validation: 0.03337268936379198]
	TIME [epoch: 7.85 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023855480181029853		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.023855480181029853 | validation: 0.02621445077037631]
	TIME [epoch: 7.84 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04377597519972168		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.04377597519972168 | validation: 0.03204056736518865]
	TIME [epoch: 7.84 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025036853635678865		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.025036853635678865 | validation: 0.02336633365734546]
	TIME [epoch: 7.85 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032445404591560915		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.032445404591560915 | validation: 0.026921180572596994]
	TIME [epoch: 7.89 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058712786185107314		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.058712786185107314 | validation: 0.01947586217372859]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02533976938520171		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.02533976938520171 | validation: 0.05576966050484237]
	TIME [epoch: 7.83 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0435613024700894		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.0435613024700894 | validation: 0.020666058687869995]
	TIME [epoch: 7.85 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03745797987005319		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.03745797987005319 | validation: 0.04182355033517544]
	TIME [epoch: 7.85 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03937564651385412		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.03937564651385412 | validation: 0.043300811393164294]
	TIME [epoch: 7.9 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02727960768530777		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.02727960768530777 | validation: 0.03387427860232526]
	TIME [epoch: 7.83 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049841788571628576		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.049841788571628576 | validation: 0.16791225008801375]
	TIME [epoch: 7.84 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14183320303999408		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.14183320303999408 | validation: 0.055762584800503334]
	TIME [epoch: 7.83 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036297699160140744		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.036297699160140744 | validation: 0.030492989402602172]
	TIME [epoch: 7.85 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038037240086261324		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.038037240086261324 | validation: 0.02043228416592782]
	TIME [epoch: 7.88 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028425225983887376		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.028425225983887376 | validation: 0.028704944289407452]
	TIME [epoch: 7.84 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0309801054423915		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.0309801054423915 | validation: 0.042067035941313675]
	TIME [epoch: 7.84 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040728346112358094		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.040728346112358094 | validation: 0.018263008437361183]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07419379542894913		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.07419379542894913 | validation: 0.05610168294768893]
	TIME [epoch: 7.89 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040073012903963486		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.040073012903963486 | validation: 0.04346992176100932]
	TIME [epoch: 7.85 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04899734358586247		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.04899734358586247 | validation: 0.014665775949441935]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018220445646157046		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.018220445646157046 | validation: 0.03263190903438566]
	TIME [epoch: 7.85 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03790653263022416		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.03790653263022416 | validation: 0.012528863279619214]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026832519832452568		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.026832519832452568 | validation: 0.036734546099101185]
	TIME [epoch: 7.9 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02780969721498157		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.02780969721498157 | validation: 0.03776358455927206]
	TIME [epoch: 7.84 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037084223288793475		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.037084223288793475 | validation: 0.019836310446474877]
	TIME [epoch: 7.85 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01582306277495795		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.01582306277495795 | validation: 0.01770309860374259]
	TIME [epoch: 7.85 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03414334795185412		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.03414334795185412 | validation: 0.036291044947649675]
	TIME [epoch: 7.87 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0511922309248636		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.0511922309248636 | validation: 0.014130740613144952]
	TIME [epoch: 7.9 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021440397613997814		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.021440397613997814 | validation: 0.025957895331676507]
	TIME [epoch: 7.84 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02994518991000971		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.02994518991000971 | validation: 0.02831667102775829]
	TIME [epoch: 7.84 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02800367785980567		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.02800367785980567 | validation: 0.022609037557850972]
	TIME [epoch: 7.84 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028733837193445674		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.028733837193445674 | validation: 0.022288029178452688]
	TIME [epoch: 7.89 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023423871422669322		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.023423871422669322 | validation: 0.027849333299134187]
	TIME [epoch: 7.86 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029536232923294808		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.029536232923294808 | validation: 0.04544629941231625]
	TIME [epoch: 7.85 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048683202863876354		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.048683202863876354 | validation: 0.02120644248937213]
	TIME [epoch: 7.84 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02097563143293812		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.02097563143293812 | validation: 0.017172667277003957]
	TIME [epoch: 7.85 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01700141627732699		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.01700141627732699 | validation: 0.022341644119938026]
	TIME [epoch: 7.89 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1466798058746138		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.1466798058746138 | validation: 0.2129664614042338]
	TIME [epoch: 7.85 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.141029219325072		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.141029219325072 | validation: 0.15283750348545205]
	TIME [epoch: 7.84 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12300146491486717		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.12300146491486717 | validation: 0.1021250375273853]
	TIME [epoch: 7.84 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06364009653885307		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.06364009653885307 | validation: 0.032729568125946476]
	TIME [epoch: 7.84 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023595854647532548		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.023595854647532548 | validation: 0.05491833830055045]
	TIME [epoch: 7.89 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026438840737329133		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.026438840737329133 | validation: 0.022790780881877644]
	TIME [epoch: 7.84 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024402229962225488		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.024402229962225488 | validation: 0.01321204829788868]
	TIME [epoch: 7.85 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016821025591382786		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.016821025591382786 | validation: 0.020248152943624063]
	TIME [epoch: 7.83 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029691517308972563		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.029691517308972563 | validation: 0.028608348561175036]
	TIME [epoch: 7.84 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03659249481205942		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.03659249481205942 | validation: 0.03787701755703868]
	TIME [epoch: 7.88 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03727962265516121		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.03727962265516121 | validation: 0.020274321662412772]
	TIME [epoch: 7.83 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0167331016509359		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.0167331016509359 | validation: 0.012463700208174649]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019531595096228406		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.019531595096228406 | validation: 0.048381278051130905]
	TIME [epoch: 7.85 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025796722990396863		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.025796722990396863 | validation: 0.02038335314268619]
	TIME [epoch: 7.88 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01997835340048014		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.01997835340048014 | validation: 0.018370539390150806]
	TIME [epoch: 7.85 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01922910039231866		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.01922910039231866 | validation: 0.015087378548673149]
	TIME [epoch: 7.84 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05135093856877394		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.05135093856877394 | validation: 0.035378823873090215]
	TIME [epoch: 7.84 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031472539242845876		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.031472539242845876 | validation: 0.012321097851164805]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01814564003368342		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.01814564003368342 | validation: 0.008806535108049348]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014468056226767006		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.014468056226767006 | validation: 0.019494131834834994]
	TIME [epoch: 7.81 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032207353511091746		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.032207353511091746 | validation: 0.04046499561792781]
	TIME [epoch: 7.81 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033560331853108485		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.033560331853108485 | validation: 0.016579178611535128]
	TIME [epoch: 7.8 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015684576208468772		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.015684576208468772 | validation: 0.008807735184312288]
	TIME [epoch: 7.86 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013403235007828002		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.013403235007828002 | validation: 0.020085556238329423]
	TIME [epoch: 7.85 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023448069965682716		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.023448069965682716 | validation: 0.020031372734657254]
	TIME [epoch: 7.83 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012211308553596834		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.012211308553596834 | validation: 0.019403314077605885]
	TIME [epoch: 7.83 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03874009399064037		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.03874009399064037 | validation: 0.07022706464603964]
	TIME [epoch: 7.82 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03312332125600694		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.03312332125600694 | validation: 0.010447492777257844]
	TIME [epoch: 7.87 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012718387637455069		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.012718387637455069 | validation: 0.021506652742831887]
	TIME [epoch: 7.84 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02609910085677297		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.02609910085677297 | validation: 0.01927370219174803]
	TIME [epoch: 7.83 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023791925967320077		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.023791925967320077 | validation: 0.018044926314792264]
	TIME [epoch: 7.83 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015001407226378605		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.015001407226378605 | validation: 0.01622320701168929]
	TIME [epoch: 7.83 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07514912309125085		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.07514912309125085 | validation: 0.14184769463043156]
	TIME [epoch: 7.89 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10511493839547453		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.10511493839547453 | validation: 0.029654963205483498]
	TIME [epoch: 7.84 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026536652277382188		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.026536652277382188 | validation: 0.021700329627561785]
	TIME [epoch: 7.83 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013008008331446554		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.013008008331446554 | validation: 0.011294672897800529]
	TIME [epoch: 7.82 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010467855404070494		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.010467855404070494 | validation: 0.012906806430509659]
	TIME [epoch: 7.83 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017091635718000455		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.017091635718000455 | validation: 0.02268845812424753]
	TIME [epoch: 7.88 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027000186913610603		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.027000186913610603 | validation: 0.016546762118203186]
	TIME [epoch: 7.83 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01497675082657463		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.01497675082657463 | validation: 0.01469482936206306]
	TIME [epoch: 7.83 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018628784294300177		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.018628784294300177 | validation: 0.0373879779337011]
	TIME [epoch: 7.83 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020055504470197556		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.020055504470197556 | validation: 0.028091340488327475]
	TIME [epoch: 7.84 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019686364948229593		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.019686364948229593 | validation: 0.01217668114373138]
	TIME [epoch: 7.88 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011933376850726853		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.011933376850726853 | validation: 0.029678093410889488]
	TIME [epoch: 7.82 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036431771623214634		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.036431771623214634 | validation: 0.0474538588388545]
	TIME [epoch: 7.83 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024213993497620677		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.024213993497620677 | validation: 0.022961405944426472]
	TIME [epoch: 7.84 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012800725861651137		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.012800725861651137 | validation: 0.018015206807139934]
	TIME [epoch: 7.87 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01888477620438662		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.01888477620438662 | validation: 0.011818329021149425]
	TIME [epoch: 7.84 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013404725164094543		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.013404725164094543 | validation: 0.03491535956531329]
	TIME [epoch: 7.83 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03482141742036699		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.03482141742036699 | validation: 0.02545226560839023]
	TIME [epoch: 7.83 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017732460102979884		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.017732460102979884 | validation: 0.01386447112734154]
	TIME [epoch: 7.83 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021389704587052437		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.021389704587052437 | validation: 0.024848373708740316]
	TIME [epoch: 7.88 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01497879400472899		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.01497879400472899 | validation: 0.016555760791866672]
	TIME [epoch: 7.84 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022774844549651556		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.022774844549651556 | validation: 0.012179207878452721]
	TIME [epoch: 7.83 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01005259752013677		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.01005259752013677 | validation: 0.009921805262416754]
	TIME [epoch: 7.83 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015356504282479115		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.015356504282479115 | validation: 0.0186862384372255]
	TIME [epoch: 7.84 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02892726973697606		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.02892726973697606 | validation: 0.016073698163399236]
	TIME [epoch: 7.89 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011907216137273444		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.011907216137273444 | validation: 0.0083128730938082]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01125657245171579		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.01125657245171579 | validation: 0.02021612135592565]
	TIME [epoch: 7.83 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026983858026441744		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.026983858026441744 | validation: 0.021333442926619817]
	TIME [epoch: 7.84 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0169866985090032		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.0169866985090032 | validation: 0.024465858766537115]
	TIME [epoch: 7.87 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017312995054171696		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.017312995054171696 | validation: 0.012211713812696694]
	TIME [epoch: 7.87 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010949966639522116		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.010949966639522116 | validation: 0.025821648992500574]
	TIME [epoch: 7.84 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02241901855921976		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.02241901855921976 | validation: 0.01070524226115971]
	TIME [epoch: 7.83 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022862878948498772		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.022862878948498772 | validation: 0.03484405416386896]
	TIME [epoch: 7.84 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019421337878334333		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.019421337878334333 | validation: 0.01341958203658445]
	TIME [epoch: 7.88 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013535862030601182		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.013535862030601182 | validation: 0.01715919983293977]
	TIME [epoch: 7.86 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014820046678307986		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.014820046678307986 | validation: 0.013224192103775354]
	TIME [epoch: 7.85 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0110716794211476		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.0110716794211476 | validation: 0.011389324665811027]
	TIME [epoch: 7.84 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020798012100127848		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.020798012100127848 | validation: 0.03682079475988363]
	TIME [epoch: 7.84 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026533770310813908		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.026533770310813908 | validation: 0.042791294807342665]
	TIME [epoch: 7.89 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020063837521322386		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.020063837521322386 | validation: 0.011873458462956708]
	TIME [epoch: 7.84 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016124030225929828		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.016124030225929828 | validation: 0.012624685600348112]
	TIME [epoch: 7.84 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010468251307691367		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.010468251307691367 | validation: 0.01532468885633815]
	TIME [epoch: 7.84 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017210495176525214		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.017210495176525214 | validation: 0.014195521588219722]
	TIME [epoch: 7.84 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013695865009545593		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.013695865009545593 | validation: 0.011739628801959955]
	TIME [epoch: 7.89 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01389483707035199		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.01389483707035199 | validation: 0.017485755494741892]
	TIME [epoch: 7.84 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014347605851577159		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.014347605851577159 | validation: 0.022362404518274412]
	TIME [epoch: 7.84 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029633354961127196		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.029633354961127196 | validation: 0.017290334757758527]
	TIME [epoch: 7.84 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01235501046917608		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.01235501046917608 | validation: 0.013073739769778004]
	TIME [epoch: 7.85 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02121592191199095		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.02121592191199095 | validation: 0.03256833400454416]
	TIME [epoch: 7.88 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015122774181433523		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.015122774181433523 | validation: 0.014504447861982302]
	TIME [epoch: 7.85 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015830529765227967		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.015830529765227967 | validation: 0.020443047506262416]
	TIME [epoch: 7.85 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012286811200659649		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.012286811200659649 | validation: 0.014961296017646522]
	TIME [epoch: 7.84 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01799742719259505		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.01799742719259505 | validation: 0.016514585574362972]
	TIME [epoch: 7.89 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01702498552500134		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.01702498552500134 | validation: 0.011886269816821362]
	TIME [epoch: 7.85 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038340489228308836		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.038340489228308836 | validation: 0.04503985340080441]
	TIME [epoch: 7.85 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024047540159890603		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.024047540159890603 | validation: 0.014671124445785829]
	TIME [epoch: 7.84 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009267222317328037		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.009267222317328037 | validation: 0.009944886487437962]
	TIME [epoch: 7.84 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02317444166779061		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.02317444166779061 | validation: 0.020030577429842395]
	TIME [epoch: 7.9 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01365008226646304		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.01365008226646304 | validation: 0.014831219026144649]
	TIME [epoch: 7.85 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0154235881276636		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.0154235881276636 | validation: 0.02168325529396601]
	TIME [epoch: 7.84 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017099222509298222		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.017099222509298222 | validation: 0.009462425474299806]
	TIME [epoch: 7.85 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010004615243027176		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.010004615243027176 | validation: 0.01751469838219745]
	TIME [epoch: 7.85 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017453585058328544		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.017453585058328544 | validation: 0.012904785507017277]
	TIME [epoch: 7.89 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016582499878223377		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.016582499878223377 | validation: 0.012393916877288054]
	TIME [epoch: 7.85 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011143589451540635		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.011143589451540635 | validation: 0.014416194736282167]
	TIME [epoch: 7.84 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018282657880235673		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.018282657880235673 | validation: 0.008017694109901765]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009868226045486393		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.009868226045486393 | validation: 0.01589416291200847]
	TIME [epoch: 7.86 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015139340013039855		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.015139340013039855 | validation: 0.012858860191295386]
	TIME [epoch: 7.88 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016957096353844264		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.016957096353844264 | validation: 0.02192093857665586]
	TIME [epoch: 7.84 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015823763113160687		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.015823763113160687 | validation: 0.012354779757363894]
	TIME [epoch: 7.85 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013250614564296366		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.013250614564296366 | validation: 0.013730604790131419]
	TIME [epoch: 7.84 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011755607800214908		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.011755607800214908 | validation: 0.018710173310693204]
	TIME [epoch: 7.89 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01812038470171175		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.01812038470171175 | validation: 0.008735887679412531]
	TIME [epoch: 7.85 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012236275664002441		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.012236275664002441 | validation: 0.015194603532424621]
	TIME [epoch: 7.85 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016957595453705315		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.016957595453705315 | validation: 0.01910710181666429]
	TIME [epoch: 7.84 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019506558676009384		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.019506558676009384 | validation: 0.024292727233113275]
	TIME [epoch: 7.84 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037134036078083944		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.037134036078083944 | validation: 0.020583662547080307]
	TIME [epoch: 7.89 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01643599720145687		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.01643599720145687 | validation: 0.011484176465973786]
	TIME [epoch: 7.85 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009482020988226883		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.009482020988226883 | validation: 0.009451329838630307]
	TIME [epoch: 7.84 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012009240074042351		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.012009240074042351 | validation: 0.01413730708641733]
	TIME [epoch: 7.85 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055240806987647106		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.055240806987647106 | validation: 0.12827582951916597]
	TIME [epoch: 7.85 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08388901398423722		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.08388901398423722 | validation: 0.0380110700111522]
	TIME [epoch: 7.89 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02021300920594053		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.02021300920594053 | validation: 0.014346612090298241]
	TIME [epoch: 7.85 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013497185576670585		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.013497185576670585 | validation: 0.015282315658704186]
	TIME [epoch: 7.85 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009486140177550853		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.009486140177550853 | validation: 0.010051763459303264]
	TIME [epoch: 7.83 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010510728894495445		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.010510728894495445 | validation: 0.02297443020244635]
	TIME [epoch: 7.88 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014565061548964099		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.014565061548964099 | validation: 0.011293954847099196]
	TIME [epoch: 7.87 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010651937895964525		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.010651937895964525 | validation: 0.011995059816178813]
	TIME [epoch: 7.84 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014276129656733944		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.014276129656733944 | validation: 0.01683242061166599]
	TIME [epoch: 7.85 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01299088034676196		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.01299088034676196 | validation: 0.011858639835926357]
	TIME [epoch: 7.84 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011112564275292596		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.011112564275292596 | validation: 0.014442214083289016]
	TIME [epoch: 7.9 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015429127321418337		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.015429127321418337 | validation: 0.024083651049106654]
	TIME [epoch: 7.85 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014371579761705399		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.014371579761705399 | validation: 0.010374589394520848]
	TIME [epoch: 7.84 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012106422272113795		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.012106422272113795 | validation: 0.01104458225044184]
	TIME [epoch: 7.84 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009602044706524544		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.009602044706524544 | validation: 0.009373084968145064]
	TIME [epoch: 7.84 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013537614635508528		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.013537614635508528 | validation: 0.01882478830117816]
	TIME [epoch: 7.89 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013115802042683485		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.013115802042683485 | validation: 0.018375700248739434]
	TIME [epoch: 7.85 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01795928911823813		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.01795928911823813 | validation: 0.013893079159350495]
	TIME [epoch: 7.85 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009234546518164332		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.009234546518164332 | validation: 0.009051621324672052]
	TIME [epoch: 7.84 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018042465834989366		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.018042465834989366 | validation: 0.056743227493660696]
	TIME [epoch: 7.86 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02176612722536604		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.02176612722536604 | validation: 0.010121072288939114]
	TIME [epoch: 7.88 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008696270001648607		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.008696270001648607 | validation: 0.008577740764272337]
	TIME [epoch: 7.85 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007600507492938419		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.007600507492938419 | validation: 0.009072095872704309]
	TIME [epoch: 7.84 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014983410417568352		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.014983410417568352 | validation: 0.024878245400314046]
	TIME [epoch: 7.83 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017831892374342618		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.017831892374342618 | validation: 0.015509515409358243]
	TIME [epoch: 7.87 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009751106653402081		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.009751106653402081 | validation: 0.00930217593365718]
	TIME [epoch: 7.88 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009886043070500542		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.009886043070500542 | validation: 0.019205591142468628]
	TIME [epoch: 7.85 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014313206379821515		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.014313206379821515 | validation: 0.007225194061427617]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009936658011952978		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.009936658011952978 | validation: 0.01607996149379368]
	TIME [epoch: 7.83 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018590156305390772		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.018590156305390772 | validation: 0.021544115826385105]
	TIME [epoch: 7.88 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01327046659785693		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.01327046659785693 | validation: 0.008911916457853894]
	TIME [epoch: 7.85 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008601531448363153		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.008601531448363153 | validation: 0.016953822146162728]
	TIME [epoch: 7.85 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011934334570992437		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.011934334570992437 | validation: 0.010770458841217079]
	TIME [epoch: 7.85 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009264943362206266		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.009264943362206266 | validation: 0.02401795276767886]
	TIME [epoch: 7.84 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014907225502397212		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.014907225502397212 | validation: 0.015754494070462404]
	TIME [epoch: 7.9 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010920497990290589		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.010920497990290589 | validation: 0.013172569907946696]
	TIME [epoch: 7.84 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014084616023732148		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.014084616023732148 | validation: 0.009745866609450921]
	TIME [epoch: 7.85 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008981038580986459		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.008981038580986459 | validation: 0.009102898882248897]
	TIME [epoch: 7.85 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01883964078085345		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.01883964078085345 | validation: 0.009816713054086762]
	TIME [epoch: 7.86 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010440567084834101		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.010440567084834101 | validation: 0.009815184356759987]
	TIME [epoch: 7.89 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010217715832678203		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.010217715832678203 | validation: 0.010677812339389707]
	TIME [epoch: 7.85 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01158143818114015		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.01158143818114015 | validation: 0.00956120030514359]
	TIME [epoch: 7.85 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009559342824348841		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.009559342824348841 | validation: 0.010988224021422233]
	TIME [epoch: 7.84 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010535319465525755		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.010535319465525755 | validation: 0.016511935089151518]
	TIME [epoch: 7.9 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01530523153827092		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.01530523153827092 | validation: 0.007818346483817497]
	TIME [epoch: 7.86 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009822127277752881		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.009822127277752881 | validation: 0.010759178028633572]
	TIME [epoch: 7.84 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010770597343945683		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.010770597343945683 | validation: 0.01944097004473064]
	TIME [epoch: 7.83 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015754267208544337		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.015754267208544337 | validation: 0.015033858626379393]
	TIME [epoch: 7.84 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011245773527533557		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.011245773527533557 | validation: 0.011815157490922068]
	TIME [epoch: 7.89 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013698387755124268		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.013698387755124268 | validation: 0.008860925753891186]
	TIME [epoch: 7.85 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007165510363331414		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.007165510363331414 | validation: 0.013182513452811774]
	TIME [epoch: 7.84 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010048951323814288		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.010048951323814288 | validation: 0.010205866022320152]
	TIME [epoch: 7.84 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013045249099479166		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.013045249099479166 | validation: 0.016374269973566485]
	TIME [epoch: 7.85 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013086256059723274		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.013086256059723274 | validation: 0.00811043309181144]
	TIME [epoch: 7.9 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007427671585924811		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.007427671585924811 | validation: 0.009761186602213447]
	TIME [epoch: 7.85 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007304342075065672		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.007304342075065672 | validation: 0.010304791743253924]
	TIME [epoch: 7.84 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016229743378676297		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.016229743378676297 | validation: 0.014850071517161658]
	TIME [epoch: 7.85 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014866030108720925		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.014866030108720925 | validation: 0.009485734104206854]
	TIME [epoch: 7.85 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008180968850391547		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.008180968850391547 | validation: 0.009556488694723418]
	TIME [epoch: 7.89 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017385907822902866		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.017385907822902866 | validation: 0.01737138648553737]
	TIME [epoch: 7.84 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012151363005559412		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.012151363005559412 | validation: 0.008104488766137419]
	TIME [epoch: 7.85 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009153840248792927		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.009153840248792927 | validation: 0.01930175911141616]
	TIME [epoch: 7.84 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011004614779134805		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.011004614779134805 | validation: 0.008246767851051436]
	TIME [epoch: 7.87 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00849655436374101		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.00849655436374101 | validation: 0.014627566529668222]
	TIME [epoch: 7.86 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011612644901416634		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.011612644901416634 | validation: 0.011908378099385709]
	TIME [epoch: 7.84 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008564767731364945		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.008564767731364945 | validation: 0.01000517671989768]
	TIME [epoch: 7.84 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019618363502834177		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.019618363502834177 | validation: 0.01976263426594389]
	TIME [epoch: 7.85 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01232442381787608		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.01232442381787608 | validation: 0.01210093103075654]
	TIME [epoch: 7.89 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014048023446465654		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.014048023446465654 | validation: 0.008045128754424072]
	TIME [epoch: 7.86 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007118845188524608		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.007118845188524608 | validation: 0.009016853764208698]
	TIME [epoch: 7.85 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008767352221812977		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.008767352221812977 | validation: 0.010620201377317007]
	TIME [epoch: 7.84 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010173959296595998		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.010173959296595998 | validation: 0.022070546877381608]
	TIME [epoch: 7.84 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012245271003862704		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.012245271003862704 | validation: 0.009898726843037896]
	TIME [epoch: 7.9 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007912961900922132		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.007912961900922132 | validation: 0.010266334047772378]
	TIME [epoch: 7.84 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011213157140378951		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.011213157140378951 | validation: 0.01072016201834091]
	TIME [epoch: 7.85 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008723009817230959		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.008723009817230959 | validation: 0.011967222515770079]
	TIME [epoch: 7.84 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013848155273699966		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.013848155273699966 | validation: 0.01297751608734369]
	TIME [epoch: 7.85 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01014346259668139		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.01014346259668139 | validation: 0.01220928350826629]
	TIME [epoch: 7.89 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008058920338068532		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.008058920338068532 | validation: 0.00961434593110423]
	TIME [epoch: 7.85 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011010712714172134		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.011010712714172134 | validation: 0.006867709151267925]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007093833230551818		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.007093833230551818 | validation: 0.010536218286714003]
	TIME [epoch: 7.81 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011647819026553518		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.011647819026553518 | validation: 0.013321692788661896]
	TIME [epoch: 7.85 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008030310878977933		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.008030310878977933 | validation: 0.007442152063342944]
	TIME [epoch: 7.81 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008092465013299492		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.008092465013299492 | validation: 0.012178728890465918]
	TIME [epoch: 7.79 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011689958541156526		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.011689958541156526 | validation: 0.011151678260150073]
	TIME [epoch: 7.8 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01270294659953872		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.01270294659953872 | validation: 0.015732130412282906]
	TIME [epoch: 7.81 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009909894615361452		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.009909894615361452 | validation: 0.0074071199078490114]
	TIME [epoch: 7.84 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007111765192634267		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.007111765192634267 | validation: 0.014327918272963112]
	TIME [epoch: 7.8 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010959200315985653		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.010959200315985653 | validation: 0.008804508135196181]
	TIME [epoch: 7.79 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007405291668261175		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.007405291668261175 | validation: 0.013961722673445386]
	TIME [epoch: 7.79 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013065965195606523		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.013065965195606523 | validation: 0.009490431456918382]
	TIME [epoch: 7.79 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009224287947305187		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.009224287947305187 | validation: 0.010655406496072242]
	TIME [epoch: 7.85 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00969908622578718		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.00969908622578718 | validation: 0.010365118993452997]
	TIME [epoch: 7.79 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007704409410020461		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.007704409410020461 | validation: 0.008553845019476758]
	TIME [epoch: 7.8 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011381503790161038		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.011381503790161038 | validation: 0.006886105600682364]
	TIME [epoch: 7.8 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006833775604272545		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.006833775604272545 | validation: 0.008021997248533495]
	TIME [epoch: 7.81 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0066449909803831374		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.0066449909803831374 | validation: 0.01678828913023154]
	TIME [epoch: 7.85 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01636057764493185		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.01636057764493185 | validation: 0.010831228661010162]
	TIME [epoch: 7.8 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008871055828090911		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.008871055828090911 | validation: 0.007723495949024949]
	TIME [epoch: 7.8 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007343809691178133		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.007343809691178133 | validation: 0.00884870699977786]
	TIME [epoch: 7.79 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008193828475988771		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.008193828475988771 | validation: 0.009339157940739222]
	TIME [epoch: 7.81 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010547489445432491		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.010547489445432491 | validation: 0.00868354576340535]
	TIME [epoch: 7.83 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00793501561852587		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.00793501561852587 | validation: 0.009906232930205355]
	TIME [epoch: 7.8 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010309885156295894		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.010309885156295894 | validation: 0.007384818651319012]
	TIME [epoch: 7.8 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00680222733870827		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.00680222733870827 | validation: 0.008237537351948286]
	TIME [epoch: 7.8 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007857058017267108		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.007857058017267108 | validation: 0.009024323962492371]
	TIME [epoch: 7.85 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01399276355830235		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.01399276355830235 | validation: 0.03719905557065198]
	TIME [epoch: 7.81 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014349067187734562		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.014349067187734562 | validation: 0.008377617806649324]
	TIME [epoch: 7.82 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00738410714679429		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.00738410714679429 | validation: 0.008721041031349469]
	TIME [epoch: 7.8 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008650795528291868		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.008650795528291868 | validation: 0.01019733464426589]
	TIME [epoch: 7.79 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009199278628689599		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.009199278628689599 | validation: 0.008906027848515491]
	TIME [epoch: 7.85 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0069709259185900445		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.0069709259185900445 | validation: 0.042730402861635444]
	TIME [epoch: 7.81 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016876653342089692		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.016876653342089692 | validation: 0.01075425631680282]
	TIME [epoch: 7.8 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009901509945274329		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.009901509945274329 | validation: 0.01780796593712093]
	TIME [epoch: 7.81 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008975033290242018		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.008975033290242018 | validation: 0.007297060180922067]
	TIME [epoch: 7.82 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005925723405351677		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.005925723405351677 | validation: 0.007546674952031927]
	TIME [epoch: 7.85 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00713824460494164		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.00713824460494164 | validation: 0.017465370967409827]
	TIME [epoch: 7.81 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01252130365793748		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.01252130365793748 | validation: 0.008060064928187232]
	TIME [epoch: 7.81 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006291087509981327		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.006291087509981327 | validation: 0.009135806084740666]
	TIME [epoch: 7.82 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008177975292927256		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.008177975292927256 | validation: 0.01339016489168823]
	TIME [epoch: 7.84 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010806260979704987		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.010806260979704987 | validation: 0.015234131857630942]
	TIME [epoch: 7.85 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011464903142349372		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.011464903142349372 | validation: 0.009275238375597076]
	TIME [epoch: 7.83 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007206455564461862		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.007206455564461862 | validation: 0.02376995257247652]
	TIME [epoch: 7.83 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014950753667635283		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.014950753667635283 | validation: 0.009233359692882569]
	TIME [epoch: 7.82 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007651654726311567		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.007651654726311567 | validation: 0.00828288709239906]
	TIME [epoch: 7.87 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006239737397810528		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.006239737397810528 | validation: 0.01501187734671763]
	TIME [epoch: 7.83 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013303125616340788		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.013303125616340788 | validation: 0.010177535996202748]
	TIME [epoch: 7.81 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006931381926791252		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.006931381926791252 | validation: 0.006823856071670867]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006117195523463257		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.006117195523463257 | validation: 0.0068626139504261285]
	TIME [epoch: 7.83 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00826244436048638		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.00826244436048638 | validation: 0.008404344612035557]
	TIME [epoch: 7.87 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010789160072073499		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.010789160072073499 | validation: 0.008463399695892668]
	TIME [epoch: 7.83 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009584237576074978		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.009584237576074978 | validation: 0.008558012392289973]
	TIME [epoch: 7.83 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01000066336128461		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.01000066336128461 | validation: 0.011454248252221902]
	TIME [epoch: 7.83 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007707931519135024		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.007707931519135024 | validation: 0.008937339136402185]
	TIME [epoch: 7.84 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007440600929210227		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.007440600929210227 | validation: 0.008168085728999894]
	TIME [epoch: 7.86 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006404733984681828		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.006404733984681828 | validation: 0.009133555887428991]
	TIME [epoch: 7.83 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01045906490386996		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.01045906490386996 | validation: 0.013622728733110233]
	TIME [epoch: 7.83 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008656957420712836		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.008656957420712836 | validation: 0.006880684332662526]
	TIME [epoch: 7.83 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006171706901579051		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.006171706901579051 | validation: 0.007202202101755933]
	TIME [epoch: 7.85 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006616797060150434		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.006616797060150434 | validation: 0.01155573469757978]
	TIME [epoch: 7.85 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012074302789689198		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.012074302789689198 | validation: 0.008251895796241587]
	TIME [epoch: 7.83 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008176708755354034		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.008176708755354034 | validation: 0.006728522288658801]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_532.pth
	Model improved!!!
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00874676156544774		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.00874676156544774 | validation: 0.01222154341654039]
	TIME [epoch: 7.84 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008434389136816338		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.008434389136816338 | validation: 0.006991668499491486]
	TIME [epoch: 7.89 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006042585261258253		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.006042585261258253 | validation: 0.016936256269436553]
	TIME [epoch: 7.83 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009839470071780829		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.009839470071780829 | validation: 0.007394088453740058]
	TIME [epoch: 7.84 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007221347547706274		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.007221347547706274 | validation: 0.008343127160493579]
	TIME [epoch: 7.83 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007839406193358753		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.007839406193358753 | validation: 0.006791610728188768]
	TIME [epoch: 7.84 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007845554304282475		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.007845554304282475 | validation: 0.010009279057449636]
	TIME [epoch: 7.89 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007374508719116072		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.007374508719116072 | validation: 0.007338428505260777]
	TIME [epoch: 7.85 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010211807380036813		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.010211807380036813 | validation: 0.009381566169609836]
	TIME [epoch: 7.84 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009984494864820156		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.009984494864820156 | validation: 0.014063045240704182]
	TIME [epoch: 7.84 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007665970230658393		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.007665970230658393 | validation: 0.008655137959491677]
	TIME [epoch: 7.85 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006269144579512394		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.006269144579512394 | validation: 0.011369167469454732]
	TIME [epoch: 7.89 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00950754435826196		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.00950754435826196 | validation: 0.008051945296755418]
	TIME [epoch: 7.84 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00599525082407988		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.00599525082407988 | validation: 0.0068257693182764345]
	TIME [epoch: 7.84 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007470585226289404		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.007470585226289404 | validation: 0.008398277209211848]
	TIME [epoch: 7.84 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006968759820313136		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.006968759820313136 | validation: 0.008556111983736367]
	TIME [epoch: 7.88 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011548753347388167		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.011548753347388167 | validation: 0.012981451514275467]
	TIME [epoch: 7.84 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008468454155129363		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.008468454155129363 | validation: 0.007784478868913466]
	TIME [epoch: 7.85 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006052330293551003		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.006052330293551003 | validation: 0.007924124399785671]
	TIME [epoch: 7.84 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007832851533053119		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.007832851533053119 | validation: 0.008395010988362546]
	TIME [epoch: 7.84 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00797647579957584		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.00797647579957584 | validation: 0.010921496103781852]
	TIME [epoch: 7.89 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009686705404726357		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.009686705404726357 | validation: 0.008395924801382553]
	TIME [epoch: 7.86 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009147718659694987		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.009147718659694987 | validation: 0.007356927321231694]
	TIME [epoch: 7.84 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0063086050639361855		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.0063086050639361855 | validation: 0.008769416316587004]
	TIME [epoch: 7.85 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00797227232463912		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.00797227232463912 | validation: 0.007943147839659302]
	TIME [epoch: 7.84 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005865301915926058		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.005865301915926058 | validation: 0.0069620795521940235]
	TIME [epoch: 7.9 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006839036688223105		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.006839036688223105 | validation: 0.0072551321898779355]
	TIME [epoch: 7.85 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00689673088174453		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.00689673088174453 | validation: 0.010641443976462727]
	TIME [epoch: 7.84 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011069176606918798		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.011069176606918798 | validation: 0.0071184221894064256]
	TIME [epoch: 7.84 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008792387154677573		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.008792387154677573 | validation: 0.00973870809779518]
	TIME [epoch: 7.86 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00754419267148923		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.00754419267148923 | validation: 0.013986836895086757]
	TIME [epoch: 7.89 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008779363519193721		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.008779363519193721 | validation: 0.007994333561662546]
	TIME [epoch: 7.84 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006962793496999794		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.006962793496999794 | validation: 0.00872658747610177]
	TIME [epoch: 7.84 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006277753643741623		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.006277753643741623 | validation: 0.011479855712593134]
	TIME [epoch: 7.84 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007130304270304644		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.007130304270304644 | validation: 0.011318896884907692]
	TIME [epoch: 7.87 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008267214530153026		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.008267214530153026 | validation: 0.00872102517471136]
	TIME [epoch: 7.86 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00854670003540517		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.00854670003540517 | validation: 0.009680618410370583]
	TIME [epoch: 7.83 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007928331086793033		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.007928331086793033 | validation: 0.008515736932590312]
	TIME [epoch: 7.84 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006825572216967497		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.006825572216967497 | validation: 0.006512007239228129]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006475979009700372		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.006475979009700372 | validation: 0.009239509573246733]
	TIME [epoch: 7.88 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009886421165473752		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.009886421165473752 | validation: 0.0069911006085023085]
	TIME [epoch: 7.85 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006442627732939644		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.006442627732939644 | validation: 0.008216328569395465]
	TIME [epoch: 7.84 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006510155475728558		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.006510155475728558 | validation: 0.0073455435594050045]
	TIME [epoch: 7.84 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006491755829010553		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.006491755829010553 | validation: 0.0070763031090573195]
	TIME [epoch: 7.84 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008020375850405489		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.008020375850405489 | validation: 0.010537004156576879]
	TIME [epoch: 7.88 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007774309500746926		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.007774309500746926 | validation: 0.007709422304145614]
	TIME [epoch: 7.84 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00656699627873193		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.00656699627873193 | validation: 0.00905447757064926]
	TIME [epoch: 7.84 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007793103858036811		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.007793103858036811 | validation: 0.006660402248954761]
	TIME [epoch: 7.85 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006221863377814334		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.006221863377814334 | validation: 0.007727767787636775]
	TIME [epoch: 7.85 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006881176746394189		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.006881176746394189 | validation: 0.009100237659984002]
	TIME [epoch: 7.89 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007797032753930216		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.007797032753930216 | validation: 0.008412641745223833]
	TIME [epoch: 7.83 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006270712441619215		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.006270712441619215 | validation: 0.006267241860748915]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0058811895010535825		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.0058811895010535825 | validation: 0.009191185721811594]
	TIME [epoch: 7.83 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008017203075559509		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.008017203075559509 | validation: 0.010405442610219542]
	TIME [epoch: 7.89 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022069204006144243		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.022069204006144243 | validation: 0.010156921376936384]
	TIME [epoch: 7.85 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00748318885227987		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.00748318885227987 | validation: 0.0061202756828535095]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_588.pth
	Model improved!!!
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006066099236220115		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.006066099236220115 | validation: 0.00818875380797916]
	TIME [epoch: 7.83 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0061083161115580795		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.0061083161115580795 | validation: 0.006588437658412275]
	TIME [epoch: 7.85 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006173121970381823		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.006173121970381823 | validation: 0.007636897370432477]
	TIME [epoch: 7.89 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006035030513679643		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.006035030513679643 | validation: 0.007183812769130476]
	TIME [epoch: 7.84 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007637895873983268		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.007637895873983268 | validation: 0.006249233115415318]
	TIME [epoch: 7.85 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005306415491900115		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.005306415491900115 | validation: 0.00847074623002174]
	TIME [epoch: 7.84 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006153609854400245		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.006153609854400245 | validation: 0.011941775540936728]
	TIME [epoch: 7.85 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009283565822175667		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.009283565822175667 | validation: 0.008123109772223296]
	TIME [epoch: 7.87 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005863654432287544		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.005863654432287544 | validation: 0.009188770871190754]
	TIME [epoch: 7.84 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006012381651458294		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.006012381651458294 | validation: 0.006335516841994415]
	TIME [epoch: 7.84 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006896414998850228		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.006896414998850228 | validation: 0.006775576013552682]
	TIME [epoch: 7.84 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00706688691152102		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.00706688691152102 | validation: 0.007373930017544307]
	TIME [epoch: 7.87 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005488583760897441		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.005488583760897441 | validation: 0.007110117711221167]
	TIME [epoch: 7.84 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006224549408068913		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.006224549408068913 | validation: 0.00719526339186586]
	TIME [epoch: 7.81 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0074828300263616926		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.0074828300263616926 | validation: 0.008514927906252282]
	TIME [epoch: 7.81 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012646026886197531		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.012646026886197531 | validation: 0.007965544561077023]
	TIME [epoch: 7.8 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007042440575662218		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.007042440575662218 | validation: 0.005744423033366206]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_605.pth
	Model improved!!!
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00538353157843925		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.00538353157843925 | validation: 0.0072993738566239814]
	TIME [epoch: 7.83 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006696036788673089		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.006696036788673089 | validation: 0.0071080082290969696]
	TIME [epoch: 7.83 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005998311343495668		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.005998311343495668 | validation: 0.008692316679259883]
	TIME [epoch: 7.83 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006442253393895769		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.006442253393895769 | validation: 0.006516026051496558]
	TIME [epoch: 7.85 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060637750128323615		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.0060637750128323615 | validation: 0.006551776770409399]
	TIME [epoch: 7.87 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00631136266935611		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.00631136266935611 | validation: 0.00882959278201537]
	TIME [epoch: 7.83 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005446524692460117		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.005446524692460117 | validation: 0.006718670699981184]
	TIME [epoch: 7.82 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00616590485250795		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.00616590485250795 | validation: 0.009618409411089594]
	TIME [epoch: 7.82 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006995716181239989		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.006995716181239989 | validation: 0.0078066858503774515]
	TIME [epoch: 7.86 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006179953098858536		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.006179953098858536 | validation: 0.008309130054530881]
	TIME [epoch: 7.86 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007219867587381121		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.007219867587381121 | validation: 0.008745711743068329]
	TIME [epoch: 7.83 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00711479566547999		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.00711479566547999 | validation: 0.0074165909817306055]
	TIME [epoch: 7.83 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006188653531532763		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.006188653531532763 | validation: 0.006205422923392623]
	TIME [epoch: 7.84 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00669338916244955		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.00669338916244955 | validation: 0.0069539654095927]
	TIME [epoch: 7.88 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005954741910513553		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.005954741910513553 | validation: 0.007674834934295988]
	TIME [epoch: 7.84 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006511900659599867		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.006511900659599867 | validation: 0.0066138291340585124]
	TIME [epoch: 7.84 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01085604158957268		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.01085604158957268 | validation: 0.017015749695003277]
	TIME [epoch: 7.83 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009054241873237745		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.009054241873237745 | validation: 0.008461279636145444]
	TIME [epoch: 7.83 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005420975316391969		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.005420975316391969 | validation: 0.006458064117488797]
	TIME [epoch: 7.88 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0058435732252449385		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.0058435732252449385 | validation: 0.007333613487088912]
	TIME [epoch: 7.84 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007298128360244129		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.007298128360244129 | validation: 0.007163343928503013]
	TIME [epoch: 7.83 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005987706000676965		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.005987706000676965 | validation: 0.006281306743383547]
	TIME [epoch: 7.83 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006226863726561509		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.006226863726561509 | validation: 0.00727226113121106]
	TIME [epoch: 7.84 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005798251532618563		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.005798251532618563 | validation: 0.009624245504887205]
	TIME [epoch: 7.88 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006586765649651929		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.006586765649651929 | validation: 0.007365294739462133]
	TIME [epoch: 7.84 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057590352855251764		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.0057590352855251764 | validation: 0.006573126533313201]
	TIME [epoch: 7.84 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006497673554599884		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.006497673554599884 | validation: 0.007902732761171014]
	TIME [epoch: 7.83 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006721803241710696		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.006721803241710696 | validation: 0.006153951975995652]
	TIME [epoch: 7.86 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005639833200240302		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.005639833200240302 | validation: 0.008150610359707837]
	TIME [epoch: 7.86 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060880230982864394		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.0060880230982864394 | validation: 0.007980195888013259]
	TIME [epoch: 7.84 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00620094616210614		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.00620094616210614 | validation: 0.00655662964542693]
	TIME [epoch: 7.84 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053621572380959295		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.0053621572380959295 | validation: 0.007303897245231165]
	TIME [epoch: 7.83 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0066350207968452896		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.0066350207968452896 | validation: 0.009146216057570239]
	TIME [epoch: 7.88 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007132420010049527		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.007132420010049527 | validation: 0.007205156182922593]
	TIME [epoch: 7.84 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052651276227137275		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.0052651276227137275 | validation: 0.007177020896334905]
	TIME [epoch: 7.84 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006774378898572901		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.006774378898572901 | validation: 0.007540479479751586]
	TIME [epoch: 7.83 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005821921484768842		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.005821921484768842 | validation: 0.006422528361770718]
	TIME [epoch: 7.84 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005957760398456237		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.005957760398456237 | validation: 0.008413884995833586]
	TIME [epoch: 7.89 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005697234360648966		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.005697234360648966 | validation: 0.005965064280272029]
	TIME [epoch: 7.84 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006039530369434205		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.006039530369434205 | validation: 0.0071034577695494655]
	TIME [epoch: 7.84 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056546501787486225		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.0056546501787486225 | validation: 0.0062156224709194125]
	TIME [epoch: 7.84 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005984165395670325		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.005984165395670325 | validation: 0.006233008861192415]
	TIME [epoch: 7.85 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005136334329348968		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.005136334329348968 | validation: 0.005769285779268439]
	TIME [epoch: 7.88 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005824320018735193		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.005824320018735193 | validation: 0.006944859621297291]
	TIME [epoch: 7.84 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00672821039756208		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.00672821039756208 | validation: 0.006508014281518933]
	TIME [epoch: 7.83 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054686199241657345		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.0054686199241657345 | validation: 0.005493524988878085]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055166573350131955		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.0055166573350131955 | validation: 0.007272082452380023]
	TIME [epoch: 7.86 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005445575335778658		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.005445575335778658 | validation: 0.006156607654729073]
	TIME [epoch: 7.86 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005722187717449345		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.005722187717449345 | validation: 0.005837735320367739]
	TIME [epoch: 7.83 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006676326505678416		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.006676326505678416 | validation: 0.006604218347869736]
	TIME [epoch: 7.83 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005271978412382962		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.005271978412382962 | validation: 0.007114349181140252]
	TIME [epoch: 7.83 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053412181104061434		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.0053412181104061434 | validation: 0.006436523246152973]
	TIME [epoch: 7.88 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005373288615341125		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.005373288615341125 | validation: 0.009469637938737644]
	TIME [epoch: 7.85 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006557185623944266		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.006557185623944266 | validation: 0.00720712920912545]
	TIME [epoch: 7.83 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009612483033424823		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.009612483033424823 | validation: 0.008363821784173555]
	TIME [epoch: 7.83 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005771113682506654		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.005771113682506654 | validation: 0.006800885440403228]
	TIME [epoch: 7.84 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057878396882070195		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.0057878396882070195 | validation: 0.007929136665478495]
	TIME [epoch: 7.89 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053074953629492745		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.0053074953629492745 | validation: 0.006103317840865957]
	TIME [epoch: 7.83 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005156341679976277		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.005156341679976277 | validation: 0.00624313373358624]
	TIME [epoch: 7.84 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00513606762435652		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.00513606762435652 | validation: 0.00607157895734195]
	TIME [epoch: 7.84 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005512208075925283		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.005512208075925283 | validation: 0.005866641365291612]
	TIME [epoch: 7.85 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060721000572391114		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.0060721000572391114 | validation: 0.007993079212794633]
	TIME [epoch: 7.88 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006857548522451318		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.006857548522451318 | validation: 0.007253058486897888]
	TIME [epoch: 7.84 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00573069378952455		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.00573069378952455 | validation: 0.0061644930272442755]
	TIME [epoch: 7.84 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005002073447752543		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.005002073447752543 | validation: 0.0057990272631657305]
	TIME [epoch: 7.86 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005597683952231786		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.005597683952231786 | validation: 0.0056460316233895285]
	TIME [epoch: 7.86 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006188434981260355		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.006188434981260355 | validation: 0.007378757239030172]
	TIME [epoch: 7.87 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006424721479601535		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.006424721479601535 | validation: 0.0054147955049493384]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_673.pth
	Model improved!!!
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005204420464388251		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.005204420464388251 | validation: 0.006101499746094738]
	TIME [epoch: 7.84 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0058695199741395625		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.0058695199741395625 | validation: 0.006170950044044327]
	TIME [epoch: 7.84 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005011093633675516		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.005011093633675516 | validation: 0.00765847841976108]
	TIME [epoch: 7.89 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0062106968276338295		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.0062106968276338295 | validation: 0.008154138041833778]
	TIME [epoch: 7.85 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006348635394045468		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.006348635394045468 | validation: 0.006020585778217513]
	TIME [epoch: 7.84 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057702914851096455		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.0057702914851096455 | validation: 0.0055710083593569215]
	TIME [epoch: 7.84 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005309285014300694		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.005309285014300694 | validation: 0.006286395147665311]
	TIME [epoch: 7.84 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004998093793569913		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.004998093793569913 | validation: 0.0067144087645312715]
	TIME [epoch: 7.9 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005660467154995376		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.005660467154995376 | validation: 0.006245371630819143]
	TIME [epoch: 7.84 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005642529251432487		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.005642529251432487 | validation: 0.0058477477798055]
	TIME [epoch: 7.84 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00536707074461901		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.00536707074461901 | validation: 0.006108949141323268]
	TIME [epoch: 7.84 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005954772375090581		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.005954772375090581 | validation: 0.006022710733267097]
	TIME [epoch: 7.85 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005784149463841575		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.005784149463841575 | validation: 0.005544511495119852]
	TIME [epoch: 7.88 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005228471262303874		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.005228471262303874 | validation: 0.006065704428064452]
	TIME [epoch: 7.84 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007509132608675565		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.007509132608675565 | validation: 0.007203806892990262]
	TIME [epoch: 7.84 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005330223447452057		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.005330223447452057 | validation: 0.0055239719226018984]
	TIME [epoch: 7.83 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005916821928472804		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.005916821928472804 | validation: 0.006317241690975622]
	TIME [epoch: 7.86 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054136324703295685		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.0054136324703295685 | validation: 0.005976257566822124]
	TIME [epoch: 7.86 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006422197482990614		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.006422197482990614 | validation: 0.0069425679228302594]
	TIME [epoch: 7.84 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005662282202781218		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.005662282202781218 | validation: 0.00591304437741376]
	TIME [epoch: 7.84 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0046223039646266		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.0046223039646266 | validation: 0.005604779473952458]
	TIME [epoch: 7.84 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055603909576207354		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.0055603909576207354 | validation: 0.005736254935603091]
	TIME [epoch: 7.89 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006228964455646084		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.006228964455646084 | validation: 0.006600832576763802]
	TIME [epoch: 7.84 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005549404074908311		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.005549404074908311 | validation: 0.004795015427848996]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_697.pth
	Model improved!!!
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004871281383632865		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.004871281383632865 | validation: 0.006824105206297393]
	TIME [epoch: 7.84 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012113505749010241		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.012113505749010241 | validation: 0.0114736518091373]
	TIME [epoch: 7.84 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007759403316566257		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.007759403316566257 | validation: 0.006322313726494686]
	TIME [epoch: 7.89 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005198041357844464		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.005198041357844464 | validation: 0.00688842094046859]
	TIME [epoch: 7.84 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005245724526772919		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.005245724526772919 | validation: 0.005712104036997973]
	TIME [epoch: 7.84 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005406639543100172		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.005406639543100172 | validation: 0.0058203084002167735]
	TIME [epoch: 7.85 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004784240939388963		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.004784240939388963 | validation: 0.0076221219265946855]
	TIME [epoch: 7.87 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006620551094600349		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.006620551094600349 | validation: 0.005058473760759374]
	TIME [epoch: 7.87 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004814124919266911		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.004814124919266911 | validation: 0.00638133630878084]
	TIME [epoch: 7.84 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004606494042095366		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.004606494042095366 | validation: 0.006613218564654733]
	TIME [epoch: 7.84 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005733238828227044		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.005733238828227044 | validation: 0.006234455460432617]
	TIME [epoch: 7.84 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004759457332188405		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.004759457332188405 | validation: 0.006251576560685696]
	TIME [epoch: 7.88 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005412104203699859		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.005412104203699859 | validation: 0.0062772491020118]
	TIME [epoch: 7.85 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005504603147824031		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.005504603147824031 | validation: 0.005970154955680967]
	TIME [epoch: 7.84 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00591491666144311		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.00591491666144311 | validation: 0.005627738876367604]
	TIME [epoch: 7.84 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005145165304558458		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.005145165304558458 | validation: 0.008083657733434203]
	TIME [epoch: 7.84 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0062553624773981145		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.0062553624773981145 | validation: 0.006574816650381253]
	TIME [epoch: 7.89 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004936559686836632		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.004936559686836632 | validation: 0.006225279376491241]
	TIME [epoch: 7.84 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00555347904964865		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.00555347904964865 | validation: 0.0076284591741289254]
	TIME [epoch: 7.84 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006056847870350965		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.006056847870350965 | validation: 0.006256614843781044]
	TIME [epoch: 7.85 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005037042871259249		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.005037042871259249 | validation: 0.0072694415154015285]
	TIME [epoch: 7.85 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005664723166384243		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.005664723166384243 | validation: 0.006736598440585198]
	TIME [epoch: 7.88 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004920400726518104		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.004920400726518104 | validation: 0.0057295588391403154]
	TIME [epoch: 7.84 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004798008584405212		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.004798008584405212 | validation: 0.005605431949683815]
	TIME [epoch: 7.85 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005843739863731141		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.005843739863731141 | validation: 0.00700125864609002]
	TIME [epoch: 7.85 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00686787818517938		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.00686787818517938 | validation: 0.0056059554897475135]
	TIME [epoch: 7.87 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005757037316571158		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.005757037316571158 | validation: 0.00598171147031209]
	TIME [epoch: 7.86 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00497680086497417		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.00497680086497417 | validation: 0.006973762969290805]
	TIME [epoch: 7.84 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005337514862502256		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.005337514862502256 | validation: 0.007762698295995037]
	TIME [epoch: 7.85 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006513850886086284		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.006513850886086284 | validation: 0.0066298256729795165]
	TIME [epoch: 7.84 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005176433663930298		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.005176433663930298 | validation: 0.006392158634474994]
	TIME [epoch: 7.88 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005121708135115938		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.005121708135115938 | validation: 0.004951253987057073]
	TIME [epoch: 7.85 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005659306681968249		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.005659306681968249 | validation: 0.007013304168754578]
	TIME [epoch: 7.84 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005050588935629569		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.005050588935629569 | validation: 0.005259951337699611]
	TIME [epoch: 7.84 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004570292463135955		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.004570292463135955 | validation: 0.005497074271456111]
	TIME [epoch: 7.84 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005825737305482468		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.005825737305482468 | validation: 0.007050470779321791]
	TIME [epoch: 7.89 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005086796264525232		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.005086796264525232 | validation: 0.006357543973964378]
	TIME [epoch: 7.85 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005508802551197878		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.005508802551197878 | validation: 0.006260850595904069]
	TIME [epoch: 7.83 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006209136986660169		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.006209136986660169 | validation: 0.015569069343947]
	TIME [epoch: 7.84 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00952290690457232		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.00952290690457232 | validation: 0.006883801205861767]
	TIME [epoch: 7.85 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005382453882805122		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.005382453882805122 | validation: 0.0052301419301839505]
	TIME [epoch: 7.88 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004586099175264875		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.004586099175264875 | validation: 0.006382917812241855]
	TIME [epoch: 7.84 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004640098991506697		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.004640098991506697 | validation: 0.005686068990092166]
	TIME [epoch: 7.84 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005486442784568103		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.005486442784568103 | validation: 0.0066370003981342535]
	TIME [epoch: 7.84 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005083711095941904		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.005083711095941904 | validation: 0.00614751387627001]
	TIME [epoch: 7.86 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004616336406213687		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.004616336406213687 | validation: 0.005022881594591029]
	TIME [epoch: 7.87 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0046943331978756535		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.0046943331978756535 | validation: 0.0068125018686233385]
	TIME [epoch: 7.84 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005862440080981305		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.005862440080981305 | validation: 0.006594190595680826]
	TIME [epoch: 7.84 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004626869669779129		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.004626869669779129 | validation: 0.0064722300112274415]
	TIME [epoch: 7.83 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005014398014800849		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.005014398014800849 | validation: 0.006515502377102659]
	TIME [epoch: 7.89 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004931491083781848		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.004931491083781848 | validation: 0.005766441113972721]
	TIME [epoch: 7.86 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004916540007475106		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.004916540007475106 | validation: 0.006846095743551002]
	TIME [epoch: 7.84 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005619963358823759		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.005619963358823759 | validation: 0.005574223264719862]
	TIME [epoch: 7.84 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004795235698217343		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.004795235698217343 | validation: 0.006210857186230592]
	TIME [epoch: 7.84 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005417138415864001		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.005417138415864001 | validation: 0.005976521188867981]
	TIME [epoch: 7.89 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004819896117795539		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.004819896117795539 | validation: 0.005389747991628201]
	TIME [epoch: 7.85 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004750719643830144		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.004750719643830144 | validation: 0.0061595325814641135]
	TIME [epoch: 7.84 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004939765366596149		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.004939765366596149 | validation: 0.005290133134212492]
	TIME [epoch: 7.84 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049515491125304055		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.0049515491125304055 | validation: 0.005775341021466896]
	TIME [epoch: 7.85 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005671294221878686		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.005671294221878686 | validation: 0.007413924409658091]
	TIME [epoch: 7.88 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005805324971517217		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.005805324971517217 | validation: 0.005752562675396648]
	TIME [epoch: 7.84 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050800006319648445		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.0050800006319648445 | validation: 0.005574750260311845]
	TIME [epoch: 7.84 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004390307887039857		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.004390307887039857 | validation: 0.005647760483147461]
	TIME [epoch: 7.84 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005355527898369657		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.005355527898369657 | validation: 0.005999226369868566]
	TIME [epoch: 7.87 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006455281115241575		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.006455281115241575 | validation: 0.005584189755599272]
	TIME [epoch: 7.87 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048425838960809795		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.0048425838960809795 | validation: 0.005417036453033676]
	TIME [epoch: 7.84 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043274607583132325		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.0043274607583132325 | validation: 0.005733339486629116]
	TIME [epoch: 7.84 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050302661679104636		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.0050302661679104636 | validation: 0.006256891516613763]
	TIME [epoch: 7.84 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004715395573708065		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.004715395573708065 | validation: 0.0055416259336897936]
	TIME [epoch: 7.88 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00471903124783116		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.00471903124783116 | validation: 0.0054344279370838215]
	TIME [epoch: 7.85 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004574265491924173		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.004574265491924173 | validation: 0.005764935643462987]
	TIME [epoch: 7.84 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00449255938926665		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.00449255938926665 | validation: 0.006386707606705989]
	TIME [epoch: 7.84 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00536149367151036		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.00536149367151036 | validation: 0.00573355957590416]
	TIME [epoch: 7.84 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004586260499441743		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.004586260499441743 | validation: 0.005864978319492562]
	TIME [epoch: 7.89 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005281295088958578		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.005281295088958578 | validation: 0.006744063419257021]
	TIME [epoch: 7.85 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004634245887035159		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.004634245887035159 | validation: 0.006627858292986246]
	TIME [epoch: 7.84 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004802579551865435		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.004802579551865435 | validation: 0.006811011106186943]
	TIME [epoch: 7.83 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004708816108547472		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.004708816108547472 | validation: 0.00558527281718913]
	TIME [epoch: 7.85 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004466157491346024		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.004466157491346024 | validation: 0.005454247875501584]
	TIME [epoch: 7.88 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004800679851708437		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.004800679851708437 | validation: 0.006346327218353511]
	TIME [epoch: 7.84 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004615412499610797		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.004615412499610797 | validation: 0.005685163383520077]
	TIME [epoch: 7.84 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048417979489212126		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.0048417979489212126 | validation: 0.005885671887092756]
	TIME [epoch: 7.84 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042169019735192655		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.0042169019735192655 | validation: 0.005480638636991121]
	TIME [epoch: 7.85 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005250154517186568		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.005250154517186568 | validation: 0.006034132131678208]
	TIME [epoch: 7.88 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005733950901379482		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.005733950901379482 | validation: 0.006514600728307854]
	TIME [epoch: 7.85 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044598426555754215		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.0044598426555754215 | validation: 0.00473673739229426]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_783.pth
	Model improved!!!
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004310665094531294		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.004310665094531294 | validation: 0.005359596121724573]
	TIME [epoch: 7.83 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004811287219890574		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.004811287219890574 | validation: 0.005603661353542411]
	TIME [epoch: 7.88 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004301045733639444		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.004301045733639444 | validation: 0.006026709136533836]
	TIME [epoch: 7.85 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00437788255565184		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.00437788255565184 | validation: 0.006693632854599181]
	TIME [epoch: 7.84 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004772673075755492		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.004772673075755492 | validation: 0.005770237496222578]
	TIME [epoch: 7.84 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004368962189547379		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.004368962189547379 | validation: 0.0068108783819245435]
	TIME [epoch: 7.83 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004743676863559493		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.004743676863559493 | validation: 0.005788564586915256]
	TIME [epoch: 7.88 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004620808148928181		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.004620808148928181 | validation: 0.004944593477003733]
	TIME [epoch: 7.85 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004949755949213461		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.004949755949213461 | validation: 0.0060936746223052]
	TIME [epoch: 7.84 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004658278892529396		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.004658278892529396 | validation: 0.005717480100497425]
	TIME [epoch: 7.83 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004731519207683724		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.004731519207683724 | validation: 0.00647593459557996]
	TIME [epoch: 7.84 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004257672232334727		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.004257672232334727 | validation: 0.0051246638319758665]
	TIME [epoch: 7.88 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004825754653377107		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.004825754653377107 | validation: 0.005655875658815603]
	TIME [epoch: 7.84 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042627794536167244		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.0042627794536167244 | validation: 0.005263869313138712]
	TIME [epoch: 7.84 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005084796816856842		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.005084796816856842 | validation: 0.006109663588682892]
	TIME [epoch: 7.84 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004987534039281626		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.004987534039281626 | validation: 0.006370922373822949]
	TIME [epoch: 7.87 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005267362839441722		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.005267362839441722 | validation: 0.005295292936270084]
	TIME [epoch: 7.86 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004871825262807092		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.004871825262807092 | validation: 0.004935013175604884]
	TIME [epoch: 7.83 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004529298991855489		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.004529298991855489 | validation: 0.005332933925278169]
	TIME [epoch: 7.84 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004629124793603445		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.004629124793603445 | validation: 0.00499439670904543]
	TIME [epoch: 7.82 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044626719149337725		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.0044626719149337725 | validation: 0.005386140239752129]
	TIME [epoch: 7.88 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004462726219528207		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.004462726219528207 | validation: 0.0052289152270084144]
	TIME [epoch: 7.83 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00525763523504732		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.00525763523504732 | validation: 0.005524111120191056]
	TIME [epoch: 7.83 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004668176974778531		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.004668176974778531 | validation: 0.005889894733058249]
	TIME [epoch: 7.84 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004729715297228128		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.004729715297228128 | validation: 0.006534543688302971]
	TIME [epoch: 7.83 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00510076277854973		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.00510076277854973 | validation: 0.005497314093270581]
	TIME [epoch: 7.88 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004655305110675037		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.004655305110675037 | validation: 0.005522502772781047]
	TIME [epoch: 7.83 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00440092871238729		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.00440092871238729 | validation: 0.005851195819753437]
	TIME [epoch: 7.83 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005523303636623593		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.005523303636623593 | validation: 0.006320213061300601]
	TIME [epoch: 7.82 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004183343463020118		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.004183343463020118 | validation: 0.0045873247689605]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_813.pth
	Model improved!!!
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004734840836760296		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.004734840836760296 | validation: 0.005429192791611621]
	TIME [epoch: 7.89 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004435272456025718		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.004435272456025718 | validation: 0.005547850380341176]
	TIME [epoch: 7.84 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004574835887262642		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.004574835887262642 | validation: 0.005251952210796778]
	TIME [epoch: 7.85 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004856879788126435		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.004856879788126435 | validation: 0.005399470958896103]
	TIME [epoch: 7.85 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004187559356816466		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.004187559356816466 | validation: 0.006066163899372058]
	TIME [epoch: 7.89 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055683244701325695		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.0055683244701325695 | validation: 0.006289574072012338]
	TIME [epoch: 7.86 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004212378091427936		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.004212378091427936 | validation: 0.006164171972288405]
	TIME [epoch: 7.85 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004348922214768317		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.004348922214768317 | validation: 0.006853805480531114]
	TIME [epoch: 7.85 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0059166766406007175		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.0059166766406007175 | validation: 0.0060302778372958955]
	TIME [epoch: 7.85 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00416458767881412		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.00416458767881412 | validation: 0.004691546389762323]
	TIME [epoch: 7.89 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00402691456817883		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.00402691456817883 | validation: 0.005159696297288094]
	TIME [epoch: 7.85 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004396056847037143		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.004396056847037143 | validation: 0.005830726156467252]
	TIME [epoch: 7.84 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004270826352468055		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.004270826352468055 | validation: 0.0050527635947250306]
	TIME [epoch: 7.85 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005053750211958768		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.005053750211958768 | validation: 0.005733509080991955]
	TIME [epoch: 7.86 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043189110693586715		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.0043189110693586715 | validation: 0.00689795202151915]
	TIME [epoch: 7.88 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004411558930545501		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.004411558930545501 | validation: 0.0051264280594862975]
	TIME [epoch: 7.85 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004372734883745467		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.004372734883745467 | validation: 0.006325795893894593]
	TIME [epoch: 7.85 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004235277713455715		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.004235277713455715 | validation: 0.004696845068148363]
	TIME [epoch: 7.84 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041932660620315155		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.0041932660620315155 | validation: 0.005065934719328261]
	TIME [epoch: 7.87 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00448579024923754		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.00448579024923754 | validation: 0.005559586574923259]
	TIME [epoch: 7.87 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004515402807211102		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.004515402807211102 | validation: 0.005956101686405082]
	TIME [epoch: 7.84 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044309659675692645		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.0044309659675692645 | validation: 0.0052226711482462876]
	TIME [epoch: 7.85 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004442164133042412		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.004442164133042412 | validation: 0.0055187219479659625]
	TIME [epoch: 7.85 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004676015215699606		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.004676015215699606 | validation: 0.005200355428217728]
	TIME [epoch: 7.89 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004219125187584676		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.004219125187584676 | validation: 0.004550155759547253]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_838.pth
	Model improved!!!
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004833706121032667		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.004833706121032667 | validation: 0.005111646721424464]
	TIME [epoch: 7.85 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004814407856650739		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.004814407856650739 | validation: 0.005093519841985513]
	TIME [epoch: 7.85 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004171670013114796		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.004171670013114796 | validation: 0.004600581580784164]
	TIME [epoch: 7.84 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043067754656241014		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.0043067754656241014 | validation: 0.004893433318691115]
	TIME [epoch: 7.9 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004425480556671294		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.004425480556671294 | validation: 0.004529412615129302]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_843.pth
	Model improved!!!
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004426564754649136		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.004426564754649136 | validation: 0.005800672989270575]
	TIME [epoch: 7.85 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004561056873949001		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.004561056873949001 | validation: 0.005410919824984116]
	TIME [epoch: 7.84 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004207298632192518		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.004207298632192518 | validation: 0.006354784952536333]
	TIME [epoch: 7.86 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00467062131341244		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.00467062131341244 | validation: 0.0057288288247228265]
	TIME [epoch: 7.89 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044274648230353784		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.0044274648230353784 | validation: 0.0058952388934153666]
	TIME [epoch: 7.85 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004486640863759803		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.004486640863759803 | validation: 0.004813485099160268]
	TIME [epoch: 7.85 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004024953873959033		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.004024953873959033 | validation: 0.00480239786216324]
	TIME [epoch: 7.84 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038631570013892957		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.0038631570013892957 | validation: 0.005896760318757657]
	TIME [epoch: 7.89 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043687057646909526		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.0043687057646909526 | validation: 0.005337337484822691]
	TIME [epoch: 7.86 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004356088743693925		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.004356088743693925 | validation: 0.006460802024670934]
	TIME [epoch: 7.85 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004927656964404928		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.004927656964404928 | validation: 0.005733043379486514]
	TIME [epoch: 7.85 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004004873314343598		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.004004873314343598 | validation: 0.005371617024387565]
	TIME [epoch: 7.84 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042269267199814475		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.0042269267199814475 | validation: 0.004997129135128592]
	TIME [epoch: 7.89 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004088055033821411		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.004088055033821411 | validation: 0.0059953198026913256]
	TIME [epoch: 7.85 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0046140909916737105		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.0046140909916737105 | validation: 0.006023709670844861]
	TIME [epoch: 7.85 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004236419140641979		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.004236419140641979 | validation: 0.005488144179699785]
	TIME [epoch: 7.85 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042756351327096955		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.0042756351327096955 | validation: 0.005028876345257462]
	TIME [epoch: 7.85 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004034806783633686		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.004034806783633686 | validation: 0.0046225703597133835]
	TIME [epoch: 7.89 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004285234555328709		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.004285234555328709 | validation: 0.004953191002879337]
	TIME [epoch: 7.85 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044521194718771615		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.0044521194718771615 | validation: 0.004484826727151121]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_863.pth
	Model improved!!!
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004024707036039949		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.004024707036039949 | validation: 0.005371004541385165]
	TIME [epoch: 7.85 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004303632780489624		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.004303632780489624 | validation: 0.004985859485727478]
	TIME [epoch: 7.88 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041785466184126804		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.0041785466184126804 | validation: 0.005826433752285836]
	TIME [epoch: 7.87 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041658382909415615		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.0041658382909415615 | validation: 0.006457259309349508]
	TIME [epoch: 7.84 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004567848357029073		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.004567848357029073 | validation: 0.004734389559799923]
	TIME [epoch: 7.85 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004176005347416322		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.004176005347416322 | validation: 0.00573051405350558]
	TIME [epoch: 7.85 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00429074079716848		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.00429074079716848 | validation: 0.004712100477934469]
	TIME [epoch: 7.89 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040920589131633375		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.0040920589131633375 | validation: 0.0048475261158603]
	TIME [epoch: 7.86 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047570734478645845		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.0047570734478645845 | validation: 0.006695420617706408]
	TIME [epoch: 7.84 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004448262579080299		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.004448262579080299 | validation: 0.0054596182898662485]
	TIME [epoch: 7.85 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045178231776480705		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.0045178231776480705 | validation: 0.0052310364495138]
	TIME [epoch: 7.85 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003930413919001185		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.003930413919001185 | validation: 0.00503209125005002]
	TIME [epoch: 7.9 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004148434083313316		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.004148434083313316 | validation: 0.005412434800574496]
	TIME [epoch: 7.85 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039569966316263415		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.0039569966316263415 | validation: 0.004591218584923366]
	TIME [epoch: 7.85 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004317640605077485		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.004317640605077485 | validation: 0.005961821793526588]
	TIME [epoch: 7.84 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004519610257752041		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.004519610257752041 | validation: 0.005100000474120438]
	TIME [epoch: 7.86 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004409348981434878		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.004409348981434878 | validation: 0.005287249743500537]
	TIME [epoch: 7.89 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004460329276691182		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.004460329276691182 | validation: 0.0048097298215795704]
	TIME [epoch: 7.85 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003970911394571593		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.003970911394571593 | validation: 0.0053771878847827595]
	TIME [epoch: 7.85 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038578049513767992		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.0038578049513767992 | validation: 0.00523258772798499]
	TIME [epoch: 7.85 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004150923737441755		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.004150923737441755 | validation: 0.006128033836071021]
	TIME [epoch: 7.88 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004227406233206543		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.004227406233206543 | validation: 0.004952922324278873]
	TIME [epoch: 7.87 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004507142529483242		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.004507142529483242 | validation: 0.005917532710616021]
	TIME [epoch: 7.85 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006027552680157264		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.006027552680157264 | validation: 0.004815008987591194]
	TIME [epoch: 7.85 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039025388825765623		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.0039025388825765623 | validation: 0.005324873039819115]
	TIME [epoch: 7.85 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003995705828876663		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.003995705828876663 | validation: 0.005290693070476308]
	TIME [epoch: 7.88 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004137562846540494		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.004137562846540494 | validation: 0.005094092303358954]
	TIME [epoch: 7.86 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038155609458860526		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.0038155609458860526 | validation: 0.005273831463814965]
	TIME [epoch: 7.85 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003983895194955341		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.003983895194955341 | validation: 0.005022888359483007]
	TIME [epoch: 7.85 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004691089718645833		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.004691089718645833 | validation: 0.004837336364495477]
	TIME [epoch: 7.85 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003881718914910668		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.003881718914910668 | validation: 0.004610188720681654]
	TIME [epoch: 7.9 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048606630827498845		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.0048606630827498845 | validation: 0.004960447340929431]
	TIME [epoch: 7.84 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050835626126762195		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.0050835626126762195 | validation: 0.006823665804252054]
	TIME [epoch: 7.85 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004505469268029421		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.004505469268029421 | validation: 0.005278489542891774]
	TIME [epoch: 7.85 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004133815611638762		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.004133815611638762 | validation: 0.004559839523190904]
	TIME [epoch: 7.86 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003971859299782989		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.003971859299782989 | validation: 0.0054421021619035]
	TIME [epoch: 7.89 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004078822277934052		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.004078822277934052 | validation: 0.004803384944426674]
	TIME [epoch: 7.84 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003912894921404424		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.003912894921404424 | validation: 0.0042224184311142905]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_901.pth
	Model improved!!!
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004246258864711413		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.004246258864711413 | validation: 0.004726403455439684]
	TIME [epoch: 7.85 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004164012087585225		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.004164012087585225 | validation: 0.004852914797831076]
	TIME [epoch: 7.9 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003919855347960419		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.003919855347960419 | validation: 0.0061626989842311995]
	TIME [epoch: 7.86 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004478127111117672		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.004478127111117672 | validation: 0.004889554574869539]
	TIME [epoch: 7.86 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003931027563794513		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.003931027563794513 | validation: 0.004726902028783527]
	TIME [epoch: 7.86 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004198511676738824		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.004198511676738824 | validation: 0.0058953559606295764]
	TIME [epoch: 7.86 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004077572342127181		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.004077572342127181 | validation: 0.00497580800855307]
	TIME [epoch: 7.92 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004671050064184221		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.004671050064184221 | validation: 0.006046198768812742]
	TIME [epoch: 7.86 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003912052828859496		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.003912052828859496 | validation: 0.004270599785403962]
	TIME [epoch: 7.86 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004085786538886757		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.004085786538886757 | validation: 0.0047880152167388106]
	TIME [epoch: 7.85 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003996450973473666		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.003996450973473666 | validation: 0.005519671430797459]
	TIME [epoch: 7.87 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003944105334026586		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.003944105334026586 | validation: 0.0048858040232652635]
	TIME [epoch: 7.9 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00421463620590044		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.00421463620590044 | validation: 0.005053709931605107]
	TIME [epoch: 7.85 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042116378240848054		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.0042116378240848054 | validation: 0.0049301997093111425]
	TIME [epoch: 7.86 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044837751944586755		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.0044837751944586755 | validation: 0.00482184191544128]
	TIME [epoch: 7.86 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038689882202284434		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.0038689882202284434 | validation: 0.004840195177569371]
	TIME [epoch: 7.88 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038075898409581136		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.0038075898409581136 | validation: 0.004920328928438279]
	TIME [epoch: 7.86 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041090960238899625		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.0041090960238899625 | validation: 0.004835619525941151]
	TIME [epoch: 7.83 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004023778761476455		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.004023778761476455 | validation: 0.00545628109208234]
	TIME [epoch: 7.83 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004388916092573119		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.004388916092573119 | validation: 0.004718417018894146]
	TIME [epoch: 7.84 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004242447989404458		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.004242447989404458 | validation: 0.004978212559963525]
	TIME [epoch: 7.88 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003984542198899397		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.003984542198899397 | validation: 0.00514647375505031]
	TIME [epoch: 7.84 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004263355428498911		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.004263355428498911 | validation: 0.004975934468702436]
	TIME [epoch: 7.84 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004448564801595723		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.004448564801595723 | validation: 0.005434499568261037]
	TIME [epoch: 7.83 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038641264413046468		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.0038641264413046468 | validation: 0.005481697834610772]
	TIME [epoch: 7.83 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004131651627517739		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.004131651627517739 | validation: 0.004850558988252743]
	TIME [epoch: 7.88 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004147253132345398		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.004147253132345398 | validation: 0.005085585629824875]
	TIME [epoch: 7.83 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037391677142678587		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.0037391677142678587 | validation: 0.005227577251594265]
	TIME [epoch: 7.83 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039281752441853845		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.0039281752441853845 | validation: 0.004849497549331286]
	TIME [epoch: 7.83 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00417981872162061		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.00417981872162061 | validation: 0.005193653431283758]
	TIME [epoch: 7.84 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004209669837329101		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.004209669837329101 | validation: 0.004697829728983396]
	TIME [epoch: 7.87 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003940297216457233		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.003940297216457233 | validation: 0.004598634078751639]
	TIME [epoch: 7.84 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00391145752004726		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.00391145752004726 | validation: 0.004736680956171742]
	TIME [epoch: 7.84 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005541101920497859		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.005541101920497859 | validation: 0.0063749946713224405]
	TIME [epoch: 7.83 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004293099733021944		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.004293099733021944 | validation: 0.005048463934406736]
	TIME [epoch: 7.86 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044022788948671464		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.0044022788948671464 | validation: 0.005607117723080257]
	TIME [epoch: 7.85 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038831101043515044		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.0038831101043515044 | validation: 0.0049001940498089]
	TIME [epoch: 7.83 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004106662491057317		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.004106662491057317 | validation: 0.005077291643375787]
	TIME [epoch: 7.83 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004161291432246475		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.004161291432246475 | validation: 0.00488465197705203]
	TIME [epoch: 7.83 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037411743958884556		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.0037411743958884556 | validation: 0.0046620599333006855]
	TIME [epoch: 7.87 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039372349789756475		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.0039372349789756475 | validation: 0.005138257416705991]
	TIME [epoch: 7.83 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039071203272500864		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.0039071203272500864 | validation: 0.004880053728775884]
	TIME [epoch: 7.83 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004064686628397941		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.004064686628397941 | validation: 0.004208893777137651]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_944.pth
	Model improved!!!
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038774495495866143		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.0038774495495866143 | validation: 0.004747584029250042]
	TIME [epoch: 7.82 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004189536218556997		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.004189536218556997 | validation: 0.005304324859424336]
	TIME [epoch: 7.87 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004230692530211419		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.004230692530211419 | validation: 0.004220107901416895]
	TIME [epoch: 7.82 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00366171304738926		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.00366171304738926 | validation: 0.005541495018984594]
	TIME [epoch: 7.82 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003746155868718056		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.003746155868718056 | validation: 0.005422257811357998]
	TIME [epoch: 7.82 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037165776345219267		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.0037165776345219267 | validation: 0.005390812607095875]
	TIME [epoch: 7.83 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037885789986283965		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.0037885789986283965 | validation: 0.00519415799130042]
	TIME [epoch: 7.86 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004005315777515455		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.004005315777515455 | validation: 0.005017601626177732]
	TIME [epoch: 7.82 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003904052335447577		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.003904052335447577 | validation: 0.004408771669327181]
	TIME [epoch: 7.82 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003984730914063848		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.003984730914063848 | validation: 0.004952266295081487]
	TIME [epoch: 7.82 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003968457714187201		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.003968457714187201 | validation: 0.004979909519266383]
	TIME [epoch: 7.87 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041318388120614715		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.0041318388120614715 | validation: 0.004639547177548708]
	TIME [epoch: 7.83 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00396236737750499		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.00396236737750499 | validation: 0.005243651272902747]
	TIME [epoch: 7.82 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037190448038523713		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.0037190448038523713 | validation: 0.005175744517406942]
	TIME [epoch: 7.82 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037817123569577333		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.0037817123569577333 | validation: 0.0045212652178989775]
	TIME [epoch: 7.82 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003996805941466095		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.003996805941466095 | validation: 0.00451008866594446]
	TIME [epoch: 7.87 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003913854786705967		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.003913854786705967 | validation: 0.004392651342875012]
	TIME [epoch: 7.83 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034417604652502458		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.0034417604652502458 | validation: 0.004641311246204753]
	TIME [epoch: 7.82 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004060102863945874		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.004060102863945874 | validation: 0.004021200321568718]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_963.pth
	Model improved!!!
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037418013292909455		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.0037418013292909455 | validation: 0.004341576843229626]
	TIME [epoch: 7.83 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003925340738520576		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.003925340738520576 | validation: 0.0051293354722578965]
	TIME [epoch: 7.86 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004083681841598943		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.004083681841598943 | validation: 0.005326525628068865]
	TIME [epoch: 7.82 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038659610258189067		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.0038659610258189067 | validation: 0.004917295887077231]
	TIME [epoch: 7.82 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037008884175112966		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.0037008884175112966 | validation: 0.0046686025629275076]
	TIME [epoch: 7.82 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003556634069991972		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.003556634069991972 | validation: 0.005122640357199558]
	TIME [epoch: 7.83 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003810667905813482		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.003810667905813482 | validation: 0.005183193309046327]
	TIME [epoch: 7.86 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004138073826505296		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.004138073826505296 | validation: 0.005429125507069385]
	TIME [epoch: 7.82 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036264116232662164		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.0036264116232662164 | validation: 0.004873613592364021]
	TIME [epoch: 7.82 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003976947150000167		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.003976947150000167 | validation: 0.004829394621386509]
	TIME [epoch: 7.82 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004111239832113979		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.004111239832113979 | validation: 0.00513707186028429]
	TIME [epoch: 7.86 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037009293882927013		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.0037009293882927013 | validation: 0.004794796382822056]
	TIME [epoch: 7.83 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003762457903807424		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.003762457903807424 | validation: 0.004752738108186135]
	TIME [epoch: 7.82 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004288660065652077		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.004288660065652077 | validation: 0.004485202136960189]
	TIME [epoch: 7.82 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004315251793560634		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.004315251793560634 | validation: 0.004883951189025683]
	TIME [epoch: 7.82 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003978274901773889		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.003978274901773889 | validation: 0.004595395720337583]
	TIME [epoch: 7.87 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035536048099740615		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.0035536048099740615 | validation: 0.005382305563614678]
	TIME [epoch: 7.83 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003938465830652498		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.003938465830652498 | validation: 0.004466357700079415]
	TIME [epoch: 7.82 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039380726369634585		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.0039380726369634585 | validation: 0.005147591642163754]
	TIME [epoch: 7.82 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004028061768645429		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.004028061768645429 | validation: 0.005145882766652865]
	TIME [epoch: 7.82 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004319676087211486		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.004319676087211486 | validation: 0.004184544333134204]
	TIME [epoch: 7.87 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003858162685568898		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.003858162685568898 | validation: 0.004957283930665328]
	TIME [epoch: 7.82 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003697785681971772		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.003697785681971772 | validation: 0.004379804929733043]
	TIME [epoch: 7.82 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003683618963369068		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.003683618963369068 | validation: 0.004911782076003002]
	TIME [epoch: 7.82 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003698712261652931		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.003698712261652931 | validation: 0.005149614823084155]
	TIME [epoch: 7.83 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003754707792663901		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.003754707792663901 | validation: 0.004703549948342129]
	TIME [epoch: 7.86 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003959367701532841		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.003959367701532841 | validation: 0.004760987813753423]
	TIME [epoch: 7.82 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004221803458389909		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.004221803458389909 | validation: 0.004572686088419194]
	TIME [epoch: 7.82 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036992670226122177		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.0036992670226122177 | validation: 0.005034753447679767]
	TIME [epoch: 7.82 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003824481599275892		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.003824481599275892 | validation: 0.004620545712590147]
	TIME [epoch: 7.86 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003797908310963484		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.003797908310963484 | validation: 0.004635822157411657]
	TIME [epoch: 7.83 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036725436137761283		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.0036725436137761283 | validation: 0.004776394820761787]
	TIME [epoch: 7.82 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036486066460020613		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.0036486066460020613 | validation: 0.004220438648777379]
	TIME [epoch: 7.82 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036769451044889015		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.0036769451044889015 | validation: 0.004576538980728576]
	TIME [epoch: 7.82 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043350340096880765		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.0043350340096880765 | validation: 0.004507370819888765]
	TIME [epoch: 7.86 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004232891649983438		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.004232891649983438 | validation: 0.0047628740663274455]
	TIME [epoch: 7.83 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004089448270923393		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.004089448270923393 | validation: 0.004959766578953443]
	TIME [epoch: 7.82 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039274169808396165		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.0039274169808396165 | validation: 0.005026465150750782]
	TIME [epoch: 7.82 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037783572639259813		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.0037783572639259813 | validation: 0.0043265358897851885]
	TIME [epoch: 7.82 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00367777618170602		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.00367777618170602 | validation: 0.004377368294337483]
	TIME [epoch: 7.86 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038552227819650523		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.0038552227819650523 | validation: 0.004598822219349964]
	TIME [epoch: 7.82 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003708192444555867		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.003708192444555867 | validation: 0.005295124194310628]
	TIME [epoch: 7.82 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037678583208595098		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.0037678583208595098 | validation: 0.004631079781011165]
	TIME [epoch: 7.82 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037457946572676372		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.0037457946572676372 | validation: 0.0043466113205418]
	TIME [epoch: 7.83 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003662334641350972		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.003662334641350972 | validation: 0.004834039449156391]
	TIME [epoch: 7.86 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038716861125773095		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.0038716861125773095 | validation: 0.004575163940989339]
	TIME [epoch: 7.82 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004120187940769522		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.004120187940769522 | validation: 0.005078951968938065]
	TIME [epoch: 7.82 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003897876088781503		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.003897876088781503 | validation: 0.004671621465947726]
	TIME [epoch: 7.82 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035771620651348255		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.0035771620651348255 | validation: 0.00413458443965517]
	TIME [epoch: 7.86 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003608331054410716		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.003608331054410716 | validation: 0.004571861094693615]
	TIME [epoch: 7.83 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003901383763909859		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.003901383763909859 | validation: 0.005012197703822777]
	TIME [epoch: 7.82 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036911573668605164		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.0036911573668605164 | validation: 0.00484966244426601]
	TIME [epoch: 7.82 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038059930959688567		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.0038059930959688567 | validation: 0.004659437110607845]
	TIME [epoch: 7.82 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003746756875994123		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.003746756875994123 | validation: 0.004884928171692397]
	TIME [epoch: 7.86 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00412671414566059		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.00412671414566059 | validation: 0.004702083451917867]
	TIME [epoch: 7.82 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034639391696729546		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.0034639391696729546 | validation: 0.003957062797915524]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_1019.pth
	Model improved!!!
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00362566404077103		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.00362566404077103 | validation: 0.004168028275440011]
	TIME [epoch: 7.82 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034721697820193067		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.0034721697820193067 | validation: 0.005476056539575891]
	TIME [epoch: 7.82 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004428986662757643		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.004428986662757643 | validation: 0.004948279671073421]
	TIME [epoch: 7.86 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037065839317871516		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.0037065839317871516 | validation: 0.004701584127180144]
	TIME [epoch: 7.82 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00372402281282649		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.00372402281282649 | validation: 0.004576823976901109]
	TIME [epoch: 7.82 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037018218452760567		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.0037018218452760567 | validation: 0.005602185806717508]
	TIME [epoch: 7.82 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003844758797357328		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.003844758797357328 | validation: 0.004820475632983278]
	TIME [epoch: 7.83 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036238402063987537		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.0036238402063987537 | validation: 0.004420266280954509]
	TIME [epoch: 7.86 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038505329404201058		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.0038505329404201058 | validation: 0.004877525863998884]
	TIME [epoch: 7.82 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036753245803526438		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.0036753245803526438 | validation: 0.004009578937412663]
	TIME [epoch: 7.82 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037089252068464897		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.0037089252068464897 | validation: 0.004177402167933413]
	TIME [epoch: 7.82 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036752538346749783		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.0036752538346749783 | validation: 0.004616908727226705]
	TIME [epoch: 7.86 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035945023223426945		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.0035945023223426945 | validation: 0.004902861486554947]
	TIME [epoch: 7.83 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036858627438119047		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.0036858627438119047 | validation: 0.005503014595045285]
	TIME [epoch: 7.81 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003708612885121328		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.003708612885121328 | validation: 0.004185749282266029]
	TIME [epoch: 7.81 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036232111737450816		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.0036232111737450816 | validation: 0.005083343964767249]
	TIME [epoch: 7.82 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035629248283751518		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.0035629248283751518 | validation: 0.004386444374029707]
	TIME [epoch: 7.86 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037052922677935076		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.0037052922677935076 | validation: 0.004720997785104326]
	TIME [epoch: 7.82 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003907176130266964		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.003907176130266964 | validation: 0.004018215551826718]
	TIME [epoch: 7.82 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004053404392687118		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.004053404392687118 | validation: 0.005231843908370726]
	TIME [epoch: 7.82 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037675442253723842		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.0037675442253723842 | validation: 0.004485781929854344]
	TIME [epoch: 7.82 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036472620556223445		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.0036472620556223445 | validation: 0.00406531707548498]
	TIME [epoch: 7.86 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036867048910358847		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.0036867048910358847 | validation: 0.005541418545953387]
	TIME [epoch: 7.82 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038111854143720644		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.0038111854143720644 | validation: 0.004479748027349531]
	TIME [epoch: 7.82 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036439129251811184		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.0036439129251811184 | validation: 0.00425436634021418]
	TIME [epoch: 7.82 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003529462749123736		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.003529462749123736 | validation: 0.00467801832791047]
	TIME [epoch: 7.83 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003868017851607591		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.003868017851607591 | validation: 0.0047364857365481865]
	TIME [epoch: 7.86 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036671500804980935		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.0036671500804980935 | validation: 0.0038868882730609916]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_1047.pth
	Model improved!!!
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038146035031765392		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.0038146035031765392 | validation: 0.004193967322031065]
	TIME [epoch: 7.82 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036850717462534243		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.0036850717462534243 | validation: 0.004662571151595936]
	TIME [epoch: 7.82 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003661765483575403		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.003661765483575403 | validation: 0.004779834535129594]
	TIME [epoch: 7.86 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036490371786151886		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.0036490371786151886 | validation: 0.004250994614718861]
	TIME [epoch: 7.83 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033865162990975176		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.0033865162990975176 | validation: 0.00438714733281608]
	TIME [epoch: 7.82 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038415845662021562		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.0038415845662021562 | validation: 0.004633102613752871]
	TIME [epoch: 7.82 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038168125792189455		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.0038168125792189455 | validation: 0.004795235155898566]
	TIME [epoch: 7.82 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00388431543357669		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.00388431543357669 | validation: 0.0040906564731983]
	TIME [epoch: 7.86 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003560122300637679		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.003560122300637679 | validation: 0.004342875738015334]
	TIME [epoch: 7.82 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035384082660369445		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.0035384082660369445 | validation: 0.005771248106735478]
	TIME [epoch: 7.82 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037724687127479836		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.0037724687127479836 | validation: 0.0050328052960265145]
	TIME [epoch: 7.82 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003807601902548197		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.003807601902548197 | validation: 0.004435589069351127]
	TIME [epoch: 7.82 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003393681322791584		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.003393681322791584 | validation: 0.0039460898282105755]
	TIME [epoch: 7.85 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003676261151780826		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.003676261151780826 | validation: 0.004685868251948949]
	TIME [epoch: 7.82 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038867432651504682		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.0038867432651504682 | validation: 0.004679174434225933]
	TIME [epoch: 7.82 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003535854984757085		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.003535854984757085 | validation: 0.004135895946788797]
	TIME [epoch: 7.82 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038532394857049767		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.0038532394857049767 | validation: 0.004860013831725987]
	TIME [epoch: 7.83 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042179790648954356		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.0042179790648954356 | validation: 0.00624436158169508]
	TIME [epoch: 7.86 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038643038622670064		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.0038643038622670064 | validation: 0.0046845384037437245]
	TIME [epoch: 7.82 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034285161040000472		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.0034285161040000472 | validation: 0.004682901986507641]
	TIME [epoch: 7.82 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035273862559890143		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.0035273862559890143 | validation: 0.004313501597620037]
	TIME [epoch: 7.81 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003782650508680733		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.003782650508680733 | validation: 0.0051096621497841435]
	TIME [epoch: 7.86 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003812830318285828		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.003812830318285828 | validation: 0.004679536574280551]
	TIME [epoch: 7.83 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003493039069746298		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.003493039069746298 | validation: 0.004418092863259106]
	TIME [epoch: 7.82 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035544664897390068		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.0035544664897390068 | validation: 0.004337153593738281]
	TIME [epoch: 7.82 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039419377355098565		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.0039419377355098565 | validation: 0.006197802624868231]
	TIME [epoch: 7.82 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004573932351991406		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.004573932351991406 | validation: 0.004709356933997813]
	TIME [epoch: 7.86 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003508530871189919		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.003508530871189919 | validation: 0.004139380778920388]
	TIME [epoch: 7.82 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035778255593107875		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.0035778255593107875 | validation: 0.004922558009967524]
	TIME [epoch: 7.82 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003582851147790111		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.003582851147790111 | validation: 0.004781347263286047]
	TIME [epoch: 7.82 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003738330162761754		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.003738330162761754 | validation: 0.004400182007642405]
	TIME [epoch: 7.82 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035707655100705544		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.0035707655100705544 | validation: 0.004490023929371358]
	TIME [epoch: 7.86 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036193159429332824		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.0036193159429332824 | validation: 0.0045175820166983745]
	TIME [epoch: 7.82 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00358187644068946		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.00358187644068946 | validation: 0.004644951445363341]
	TIME [epoch: 7.82 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003844998963841487		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.003844998963841487 | validation: 0.004714396911349927]
	TIME [epoch: 7.82 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036961892204033925		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.0036961892204033925 | validation: 0.00468995486008522]
	TIME [epoch: 7.83 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035530662228189878		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.0035530662228189878 | validation: 0.004654992377796748]
	TIME [epoch: 7.86 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036870549825208853		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.0036870549825208853 | validation: 0.0059069149369688325]
	TIME [epoch: 7.82 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003583627881930318		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.003583627881930318 | validation: 0.004172380422428727]
	TIME [epoch: 7.82 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035248787591565913		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.0035248787591565913 | validation: 0.004495149886093066]
	TIME [epoch: 7.82 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00371128048193415		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.00371128048193415 | validation: 0.004643111192789429]
	TIME [epoch: 7.86 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033577969130449393		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.0033577969130449393 | validation: 0.004811964750770601]
	TIME [epoch: 7.83 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034930421774971823		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.0034930421774971823 | validation: 0.004418058859283108]
	TIME [epoch: 7.82 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035980770228867466		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.0035980770228867466 | validation: 0.004212800648638638]
	TIME [epoch: 7.82 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035431866473651626		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.0035431866473651626 | validation: 0.0040976368197668065]
	TIME [epoch: 7.82 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004040250017357376		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.004040250017357376 | validation: 0.0039364393929249945]
	TIME [epoch: 7.86 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004209493762704521		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.004209493762704521 | validation: 0.005053024480958977]
	TIME [epoch: 7.82 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003772769087060601		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.003772769087060601 | validation: 0.0042366813707907]
	TIME [epoch: 7.82 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00359266134559819		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.00359266134559819 | validation: 0.0043991018490604356]
	TIME [epoch: 7.82 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031982309529512702		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.0031982309529512702 | validation: 0.004900284897416568]
	TIME [epoch: 7.82 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036030100876093805		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.0036030100876093805 | validation: 0.004805942635862014]
	TIME [epoch: 7.87 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003415491403667475		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.003415491403667475 | validation: 0.004919775187065289]
	TIME [epoch: 7.82 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034900730231582676		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.0034900730231582676 | validation: 0.004384989937685424]
	TIME [epoch: 7.82 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032466788718645556		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.0032466788718645556 | validation: 0.0045015773890156405]
	TIME [epoch: 7.82 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003409111270064836		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.003409111270064836 | validation: 0.004766604250950235]
	TIME [epoch: 7.83 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033360462148926445		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.0033360462148926445 | validation: 0.004479597870082304]
	TIME [epoch: 7.86 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033763408545691206		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.0033763408545691206 | validation: 0.00466907362259048]
	TIME [epoch: 7.82 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003298610055483478		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.003298610055483478 | validation: 0.0041369302658522105]
	TIME [epoch: 7.82 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003725296625188848		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.003725296625188848 | validation: 0.004458112439513044]
	TIME [epoch: 7.82 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003511228756758214		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.003511228756758214 | validation: 0.004163158931016122]
	TIME [epoch: 7.86 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035054835317587683		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.0035054835317587683 | validation: 0.0043241010625371945]
	TIME [epoch: 7.83 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003705920738357541		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.003705920738357541 | validation: 0.0047784241101775544]
	TIME [epoch: 7.82 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036643922141230373		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.0036643922141230373 | validation: 0.004763279768320241]
	TIME [epoch: 7.82 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035046242024767593		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.0035046242024767593 | validation: 0.004609945292884955]
	TIME [epoch: 7.82 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035783898583529993		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.0035783898583529993 | validation: 0.004103843443324515]
	TIME [epoch: 7.86 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003421117507122556		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.003421117507122556 | validation: 0.005639104302133356]
	TIME [epoch: 7.82 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003764141381341468		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.003764141381341468 | validation: 0.0047451826015594]
	TIME [epoch: 7.82 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003594617088384141		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.003594617088384141 | validation: 0.004579361728230856]
	TIME [epoch: 7.82 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035459144020534078		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.0035459144020534078 | validation: 0.004536054015139259]
	TIME [epoch: 7.82 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003345810861771803		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.003345810861771803 | validation: 0.004835753779008469]
	TIME [epoch: 7.86 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032958465952943675		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.0032958465952943675 | validation: 0.004751451369046068]
	TIME [epoch: 7.82 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003615087576208859		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.003615087576208859 | validation: 0.004495249782879553]
	TIME [epoch: 7.82 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035605807105368002		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.0035605807105368002 | validation: 0.004157812097739227]
	TIME [epoch: 7.82 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003503473941715642		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.003503473941715642 | validation: 0.004372319712445967]
	TIME [epoch: 7.83 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035733426075333506		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.0035733426075333506 | validation: 0.004476391677335755]
	TIME [epoch: 7.86 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033784567328572976		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.0033784567328572976 | validation: 0.004091912631460231]
	TIME [epoch: 7.82 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032806881172611413		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.0032806881172611413 | validation: 0.004337124427641692]
	TIME [epoch: 7.82 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003275064884328825		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.003275064884328825 | validation: 0.004280169504486051]
	TIME [epoch: 7.82 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035638252719110276		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.0035638252719110276 | validation: 0.0049401349114839305]
	TIME [epoch: 7.86 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003482692689919067		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.003482692689919067 | validation: 0.004748017460284762]
	TIME [epoch: 7.83 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036765258510840335		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.0036765258510840335 | validation: 0.004941253339283525]
	TIME [epoch: 7.82 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036664155793474183		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.0036664155793474183 | validation: 0.004346233578293325]
	TIME [epoch: 7.82 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031733022162690515		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.0031733022162690515 | validation: 0.004515796408142466]
	TIME [epoch: 7.82 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038013448587331344		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.0038013448587331344 | validation: 0.004903509553169271]
	TIME [epoch: 7.86 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035274526811573583		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.0035274526811573583 | validation: 0.004746705577393463]
	TIME [epoch: 7.83 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003524501614429038		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.003524501614429038 | validation: 0.004121809779569248]
	TIME [epoch: 7.82 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003351255627737034		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.003351255627737034 | validation: 0.004814880137446848]
	TIME [epoch: 7.82 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037337386212857185		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.0037337386212857185 | validation: 0.004695203593061153]
	TIME [epoch: 7.82 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035580364855972656		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.0035580364855972656 | validation: 0.0044691773770028625]
	TIME [epoch: 7.87 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034243340666060954		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.0034243340666060954 | validation: 0.004233465957821935]
	TIME [epoch: 7.82 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00360031353223366		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.00360031353223366 | validation: 0.003910143055148805]
	TIME [epoch: 7.82 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034441959028727563		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.0034441959028727563 | validation: 0.004117114129313375]
	TIME [epoch: 7.82 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003676760248250569		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.003676760248250569 | validation: 0.004216043659474391]
	TIME [epoch: 7.83 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033358785037492207		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.0033358785037492207 | validation: 0.004905860567096484]
	TIME [epoch: 7.86 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003571398868433561		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.003571398868433561 | validation: 0.004315213549551265]
	TIME [epoch: 7.82 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034490030371264243		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.0034490030371264243 | validation: 0.004925679872854299]
	TIME [epoch: 7.82 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003435899638215232		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.003435899638215232 | validation: 0.00428118333912075]
	TIME [epoch: 7.82 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033502684618375517		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.0033502684618375517 | validation: 0.004594106576146136]
	TIME [epoch: 7.85 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032890954051132265		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.0032890954051132265 | validation: 0.0037304362778906325]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_1146.pth
	Model improved!!!
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033159380354375895		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.0033159380354375895 | validation: 0.004398571547915705]
	TIME [epoch: 7.84 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003583275148892592		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.003583275148892592 | validation: 0.004697903555596797]
	TIME [epoch: 7.83 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035982599690044786		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.0035982599690044786 | validation: 0.004833920967337293]
	TIME [epoch: 7.83 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003545144711950006		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.003545144711950006 | validation: 0.004189751119693961]
	TIME [epoch: 7.88 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003466223101270387		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.003466223101270387 | validation: 0.0048627582172249524]
	TIME [epoch: 7.83 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003580424401830765		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.003580424401830765 | validation: 0.004478550659444854]
	TIME [epoch: 7.83 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035203325305307883		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.0035203325305307883 | validation: 0.004268421954422496]
	TIME [epoch: 7.83 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035307234698180607		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.0035307234698180607 | validation: 0.004483739716448133]
	TIME [epoch: 7.83 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003408086627498956		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.003408086627498956 | validation: 0.004544414588330953]
	TIME [epoch: 7.87 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033056587661602532		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.0033056587661602532 | validation: 0.004534224976322676]
	TIME [epoch: 7.83 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035568147370608942		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.0035568147370608942 | validation: 0.004754728662543101]
	TIME [epoch: 7.83 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003446608409697639		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.003446608409697639 | validation: 0.004133951348631548]
	TIME [epoch: 7.83 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003583572005638616		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.003583572005638616 | validation: 0.004447662946477448]
	TIME [epoch: 7.84 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003420336642768272		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.003420336642768272 | validation: 0.004485946790115291]
	TIME [epoch: 7.87 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035059926236423538		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.0035059926236423538 | validation: 0.004204305964036625]
	TIME [epoch: 7.83 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003380302414792798		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.003380302414792798 | validation: 0.004339190797158933]
	TIME [epoch: 7.83 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032811064432290695		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.0032811064432290695 | validation: 0.004321682890037459]
	TIME [epoch: 7.83 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035542137011271646		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.0035542137011271646 | validation: 0.004494361765155899]
	TIME [epoch: 7.87 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036477230920040496		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.0036477230920040496 | validation: 0.004605781448224878]
	TIME [epoch: 7.84 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035059715193733482		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.0035059715193733482 | validation: 0.004266945623337043]
	TIME [epoch: 7.83 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034025701410786142		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.0034025701410786142 | validation: 0.005036422209483391]
	TIME [epoch: 7.83 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036834896873348425		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.0036834896873348425 | validation: 0.005470274190997712]
	TIME [epoch: 7.83 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033563538207957064		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 0.0033563538207957064 | validation: 0.0037997679377263552]
	TIME [epoch: 7.88 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034806669313833306		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.0034806669313833306 | validation: 0.004579051127663147]
	TIME [epoch: 7.83 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032925057342556956		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.0032925057342556956 | validation: 0.00472019829320658]
	TIME [epoch: 7.83 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003509392243185674		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.003509392243185674 | validation: 0.004790883902370166]
	TIME [epoch: 7.83 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003361399708087222		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.003361399708087222 | validation: 0.00408867729601165]
	TIME [epoch: 7.83 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036314893414403477		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.0036314893414403477 | validation: 0.0044453203308590285]
	TIME [epoch: 7.88 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033528333238873626		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.0033528333238873626 | validation: 0.004151969271390718]
	TIME [epoch: 7.83 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034248101294732086		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.0034248101294732086 | validation: 0.0038936280115221696]
	TIME [epoch: 7.83 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003480635710587714		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.003480635710587714 | validation: 0.005242511897876524]
	TIME [epoch: 7.83 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033303518773842317		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.0033303518773842317 | validation: 0.004288473619873296]
	TIME [epoch: 7.84 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036884415662230795		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.0036884415662230795 | validation: 0.004673851737077732]
	TIME [epoch: 7.87 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035492595642143097		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.0035492595642143097 | validation: 0.00466498779625181]
	TIME [epoch: 7.83 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034162653016288347		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.0034162653016288347 | validation: 0.004896580045860789]
	TIME [epoch: 7.83 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003557384305037754		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.003557384305037754 | validation: 0.004419213936378995]
	TIME [epoch: 7.83 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033984779687411286		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.0033984779687411286 | validation: 0.004472303165951617]
	TIME [epoch: 7.88 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032882892445528157		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.0032882892445528157 | validation: 0.004882749080873438]
	TIME [epoch: 7.84 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034083040742449236		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.0034083040742449236 | validation: 0.004708967405676711]
	TIME [epoch: 7.83 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003313825939322401		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.003313825939322401 | validation: 0.003990448614472718]
	TIME [epoch: 7.83 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033375446189246863		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.0033375446189246863 | validation: 0.004369336742229104]
	TIME [epoch: 7.83 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003525830841401628		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.003525830841401628 | validation: 0.004238803548523766]
	TIME [epoch: 7.88 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003445433939339733		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.003445433939339733 | validation: 0.004423171106138448]
	TIME [epoch: 7.83 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00349790401809588		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.00349790401809588 | validation: 0.004555827252303489]
	TIME [epoch: 7.83 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003457761389374263		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.003457761389374263 | validation: 0.004210383571908788]
	TIME [epoch: 7.83 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032797825290095594		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.0032797825290095594 | validation: 0.00446134226091566]
	TIME [epoch: 7.83 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032635181363116026		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.0032635181363116026 | validation: 0.004452947143662356]
	TIME [epoch: 7.88 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033701825770556483		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.0033701825770556483 | validation: 0.005047833886782037]
	TIME [epoch: 7.83 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003595099966466295		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.003595099966466295 | validation: 0.004584476472820585]
	TIME [epoch: 7.83 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036377696565820417		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.0036377696565820417 | validation: 0.0043184305487355065]
	TIME [epoch: 7.83 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00335955486002392		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.00335955486002392 | validation: 0.004717904072983685]
	TIME [epoch: 7.85 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003313019345296225		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.003313019345296225 | validation: 0.004414673875907117]
	TIME [epoch: 7.87 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003293797757684564		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.003293797757684564 | validation: 0.0045533610156209235]
	TIME [epoch: 7.83 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00336890825733606		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.00336890825733606 | validation: 0.004500609177140126]
	TIME [epoch: 7.83 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033379961298608827		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.0033379961298608827 | validation: 0.004098914730871753]
	TIME [epoch: 7.83 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003399530756924613		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.003399530756924613 | validation: 0.0050997200068660185]
	TIME [epoch: 7.87 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003471887250266341		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.003471887250266341 | validation: 0.004484756256803011]
	TIME [epoch: 7.84 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035402643671254706		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.0035402643671254706 | validation: 0.004906796198957118]
	TIME [epoch: 7.83 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034200727342806065		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.0034200727342806065 | validation: 0.004337187944489059]
	TIME [epoch: 7.82 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003497465826053576		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.003497465826053576 | validation: 0.003919323514195685]
	TIME [epoch: 7.83 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003205102355592983		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.003205102355592983 | validation: 0.0042251397847694]
	TIME [epoch: 7.87 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033775396066296607		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.0033775396066296607 | validation: 0.003484932489607055]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_1208.pth
	Model improved!!!
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003496271918482172		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.003496271918482172 | validation: 0.0043752233279033565]
	TIME [epoch: 7.82 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003191739699588461		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.003191739699588461 | validation: 0.00412264563977097]
	TIME [epoch: 7.82 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003336737002888145		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.003336737002888145 | validation: 0.004492255215824657]
	TIME [epoch: 7.83 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003492011358694764		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.003492011358694764 | validation: 0.004418466494778634]
	TIME [epoch: 7.86 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003364495508606138		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.003364495508606138 | validation: 0.004557651348942973]
	TIME [epoch: 7.82 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034823965093674527		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.0034823965093674527 | validation: 0.003948193633172389]
	TIME [epoch: 7.82 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031299025637874117		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.0031299025637874117 | validation: 0.0042021100158889935]
	TIME [epoch: 7.82 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003484791727379706		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.003484791727379706 | validation: 0.005055903470849975]
	TIME [epoch: 7.85 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035840309284619646		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.0035840309284619646 | validation: 0.004019577218702767]
	TIME [epoch: 7.84 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036494728622077244		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.0036494728622077244 | validation: 0.004161650697501644]
	TIME [epoch: 7.82 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003437940171642975		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.003437940171642975 | validation: 0.0038820043319127027]
	TIME [epoch: 7.82 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031736018232190835		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.0031736018232190835 | validation: 0.0037513920631690083]
	TIME [epoch: 7.82 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034999486612916544		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.0034999486612916544 | validation: 0.004545464957520601]
	TIME [epoch: 7.87 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033674473009218317		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.0033674473009218317 | validation: 0.004088146353486229]
	TIME [epoch: 7.83 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033862995215679023		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.0033862995215679023 | validation: 0.004267376200830864]
	TIME [epoch: 7.82 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003350064431837989		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.003350064431837989 | validation: 0.004372691060702026]
	TIME [epoch: 7.82 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033024360385856153		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.0033024360385856153 | validation: 0.003876647153042519]
	TIME [epoch: 7.83 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031946992802420987		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.0031946992802420987 | validation: 0.004224604876363356]
	TIME [epoch: 7.87 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003215268777563206		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.003215268777563206 | validation: 0.004412066008453196]
	TIME [epoch: 7.82 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032752141864959786		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.0032752141864959786 | validation: 0.005116766779983433]
	TIME [epoch: 7.82 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003573683693211388		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.003573683693211388 | validation: 0.004326351772347197]
	TIME [epoch: 7.82 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035795329477156393		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.0035795329477156393 | validation: 0.004335738770666784]
	TIME [epoch: 7.84 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033878229810699995		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.0033878229810699995 | validation: 0.0045077312148449605]
	TIME [epoch: 7.86 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033252424707601283		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.0033252424707601283 | validation: 0.004517818538207366]
	TIME [epoch: 7.82 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033506892437918786		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.0033506892437918786 | validation: 0.004347213123066645]
	TIME [epoch: 7.82 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031214777862188874		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.0031214777862188874 | validation: 0.004293026126803379]
	TIME [epoch: 7.82 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003096640906300215		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.003096640906300215 | validation: 0.004495467324798349]
	TIME [epoch: 7.85 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003329761101779007		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.003329761101779007 | validation: 0.00385119834900263]
	TIME [epoch: 7.85 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035320279475626205		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.0035320279475626205 | validation: 0.004334429444898426]
	TIME [epoch: 7.82 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035213998628229196		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.0035213998628229196 | validation: 0.0046468236678251585]
	TIME [epoch: 7.82 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00329228805225268		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.00329228805225268 | validation: 0.005189051324163026]
	TIME [epoch: 7.82 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003460536254644956		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.003460536254644956 | validation: 0.004107271985369101]
	TIME [epoch: 7.87 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003197288093993785		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.003197288093993785 | validation: 0.004190783505048954]
	TIME [epoch: 7.83 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032712743435039347		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.0032712743435039347 | validation: 0.004116334913554756]
	TIME [epoch: 7.82 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032212858298180363		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.0032212858298180363 | validation: 0.0038158321365950026]
	TIME [epoch: 7.82 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003112746505966061		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.003112746505966061 | validation: 0.004349299214453118]
	TIME [epoch: 7.82 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032107330682925703		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.0032107330682925703 | validation: 0.003766178524061312]
	TIME [epoch: 7.88 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033937738179125616		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.0033937738179125616 | validation: 0.00458196620552949]
	TIME [epoch: 7.83 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033933724771283017		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.0033933724771283017 | validation: 0.0045099068564722206]
	TIME [epoch: 7.82 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034670451648361996		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.0034670451648361996 | validation: 0.004705104250474612]
	TIME [epoch: 7.82 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033379924835294414		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.0033379924835294414 | validation: 0.004769068068241487]
	TIME [epoch: 7.84 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034443969603512154		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.0034443969603512154 | validation: 0.003916745965451061]
	TIME [epoch: 7.86 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033485872866343093		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.0033485872866343093 | validation: 0.0041131935426503095]
	TIME [epoch: 7.83 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032265079714841488		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.0032265079714841488 | validation: 0.004856598107557726]
	TIME [epoch: 7.82 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031729731283647333		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.0031729731283647333 | validation: 0.004300524635370347]
	TIME [epoch: 7.82 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003312200248061937		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.003312200248061937 | validation: 0.004501661146729759]
	TIME [epoch: 7.85 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033514485928081037		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.0033514485928081037 | validation: 0.0041832790877955]
	TIME [epoch: 7.85 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003208439996409453		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.003208439996409453 | validation: 0.003895495014012868]
	TIME [epoch: 7.82 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033610291073433492		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.0033610291073433492 | validation: 0.004098371906188765]
	TIME [epoch: 7.82 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032169009501614135		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.0032169009501614135 | validation: 0.004497660714583178]
	TIME [epoch: 7.82 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034484395540909972		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.0034484395540909972 | validation: 0.0039635218624492355]
	TIME [epoch: 7.87 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035486377660836847		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.0035486377660836847 | validation: 0.004307121702140256]
	TIME [epoch: 7.83 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003320682890851717		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.003320682890851717 | validation: 0.004076975177176389]
	TIME [epoch: 7.82 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003317291904606785		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.003317291904606785 | validation: 0.004354173359826637]
	TIME [epoch: 7.82 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003256132239022596		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.003256132239022596 | validation: 0.004263302320814145]
	TIME [epoch: 7.82 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00329709975744448		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.00329709975744448 | validation: 0.0038596363686965608]
	TIME [epoch: 7.87 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035382949155202924		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.0035382949155202924 | validation: 0.003943727829392044]
	TIME [epoch: 7.82 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035836162948740076		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.0035836162948740076 | validation: 0.004262080776922316]
	TIME [epoch: 7.82 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032853527677574176		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.0032853527677574176 | validation: 0.004269637370242324]
	TIME [epoch: 7.82 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003586357330471748		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.003586357330471748 | validation: 0.004050000824574705]
	TIME [epoch: 7.83 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035417568495901683		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.0035417568495901683 | validation: 0.004041346024661877]
	TIME [epoch: 7.87 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034032335523135077		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.0034032335523135077 | validation: 0.004265332732537089]
	TIME [epoch: 7.82 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032149103161745966		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.0032149103161745966 | validation: 0.004355552809781687]
	TIME [epoch: 7.82 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003399540043490001		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.003399540043490001 | validation: 0.004236641816754544]
	TIME [epoch: 7.82 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003326426959091287		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.003326426959091287 | validation: 0.004457617008239953]
	TIME [epoch: 7.85 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003212026814567135		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.003212026814567135 | validation: 0.004360552315989761]
	TIME [epoch: 7.85 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035208335893855683		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.0035208335893855683 | validation: 0.003782792706977414]
	TIME [epoch: 7.82 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003196231585853875		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.003196231585853875 | validation: 0.004383342794596271]
	TIME [epoch: 7.82 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003158527153402404		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.003158527153402404 | validation: 0.005005662583870304]
	TIME [epoch: 7.82 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032833944288782222		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.0032833944288782222 | validation: 0.00437926668759679]
	TIME [epoch: 7.87 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035190933353996213		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.0035190933353996213 | validation: 0.004494461155324872]
	TIME [epoch: 7.83 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035967991501926052		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.0035967991501926052 | validation: 0.004206302723474337]
	TIME [epoch: 7.82 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003264838846338515		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.003264838846338515 | validation: 0.004389715236838676]
	TIME [epoch: 7.82 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032749562962364007		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.0032749562962364007 | validation: 0.004517141144812683]
	TIME [epoch: 7.82 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030905976346422594		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.0030905976346422594 | validation: 0.00383126295269909]
	TIME [epoch: 7.87 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031755774109296993		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.0031755774109296993 | validation: 0.004076628781627228]
	TIME [epoch: 7.82 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030956121645191		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.0030956121645191 | validation: 0.004169910256827624]
	TIME [epoch: 7.82 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003168324161572262		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.003168324161572262 | validation: 0.003920220781882329]
	TIME [epoch: 7.82 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033603525808682446		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.0033603525808682446 | validation: 0.004317836973079745]
	TIME [epoch: 7.82 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032578786585928297		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.0032578786585928297 | validation: 0.004146246436985726]
	TIME [epoch: 7.87 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033723242880007373		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.0033723242880007373 | validation: 0.003755280376886584]
	TIME [epoch: 7.82 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033356129003705532		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.0033356129003705532 | validation: 0.004178755380108297]
	TIME [epoch: 7.82 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031029654696910396		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.0031029654696910396 | validation: 0.004164096929804945]
	TIME [epoch: 7.82 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003423706459498565		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.003423706459498565 | validation: 0.00357976033997384]
	TIME [epoch: 7.83 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032299561121808284		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.0032299561121808284 | validation: 0.004988710465151008]
	TIME [epoch: 7.86 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003398678705850312		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.003398678705850312 | validation: 0.004022426712310043]
	TIME [epoch: 7.82 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031033990164848384		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.0031033990164848384 | validation: 0.00477201239020441]
	TIME [epoch: 7.82 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003303702578676974		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.003303702578676974 | validation: 0.004731824113048055]
	TIME [epoch: 7.82 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003455296246523308		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.003455296246523308 | validation: 0.0044334693970107635]
	TIME [epoch: 7.86 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003044410549439528		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.003044410549439528 | validation: 0.004492848538192346]
	TIME [epoch: 7.83 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003222199859320249		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.003222199859320249 | validation: 0.004365055938714135]
	TIME [epoch: 7.82 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031945176570043043		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.0031945176570043043 | validation: 0.004320532768011082]
	TIME [epoch: 7.82 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003196736602022782		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.003196736602022782 | validation: 0.004500220927951853]
	TIME [epoch: 7.82 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003311687013638432		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.003311687013638432 | validation: 0.004086996322029571]
	TIME [epoch: 7.87 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032191799975773813		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.0032191799975773813 | validation: 0.004468454427086096]
	TIME [epoch: 7.82 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003093310655643858		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.003093310655643858 | validation: 0.004136512129857654]
	TIME [epoch: 7.82 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034676692441872213		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.0034676692441872213 | validation: 0.00384575390594311]
	TIME [epoch: 7.82 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031016884977747867		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.0031016884977747867 | validation: 0.004786956538748899]
	TIME [epoch: 7.83 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003359086178902398		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.003359086178902398 | validation: 0.0042315059639031925]
	TIME [epoch: 7.86 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031230598883997653		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.0031230598883997653 | validation: 0.0035353983360552938]
	TIME [epoch: 7.82 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031987449627382974		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.0031987449627382974 | validation: 0.0048015422049768166]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_smallnet_20240627_193240/states/model_phi1_1a_v_mmd1_smallnet_1309.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 10483.440 seconds.
