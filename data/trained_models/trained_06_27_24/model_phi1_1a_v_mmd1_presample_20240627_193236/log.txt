Args:
Namespace(name='model_phi1_1a_v_mmd1_presample', outdir='out/model_training/model_phi1_1a_v_mmd1_presample', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=200, model_do_sample=False, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1273697132

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.809695746583315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.809695746583315 | validation: 6.105374770123784]
	TIME [epoch: 26.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.755068237881916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.755068237881916 | validation: 4.875644715555552]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.051292624021928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.051292624021928 | validation: 4.179539588036707]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.654005619088591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.654005619088591 | validation: 4.090217459544865]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3757539095719107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3757539095719107 | validation: 4.002627675306334]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.068457191308565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.068457191308565 | validation: 3.9423555704905726]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0944930572320617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0944930572320617 | validation: 3.8103095456058766]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8060445295456233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8060445295456233 | validation: 3.2340037049364097]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.465880382557886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.465880382557886 | validation: 3.6861490312623246]
	TIME [epoch: 8.59 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4607768068163858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4607768068163858 | validation: 3.306964665293257]
	TIME [epoch: 8.57 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.152303627379597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.152303627379597 | validation: 2.6659611217775607]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8731986470988087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8731986470988087 | validation: 2.3825987597700555]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7874697153218813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7874697153218813 | validation: 2.2431301485010735]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4434304422759128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4434304422759128 | validation: 2.5032720421211505]
	TIME [epoch: 8.58 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6128393055070183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6128393055070183 | validation: 2.6023580179657673]
	TIME [epoch: 8.59 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5727954484989903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5727954484989903 | validation: 2.370149633012903]
	TIME [epoch: 8.59 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4654334290269864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4654334290269864 | validation: 2.1939997846619312]
	TIME [epoch: 8.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4303730544306932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4303730544306932 | validation: 2.155069973194536]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.368390072751885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.368390072751885 | validation: 2.1140404219203446]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4953057253896556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4953057253896556 | validation: 2.2657480927245333]
	TIME [epoch: 8.57 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.218006131922967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.218006131922967 | validation: 3.948813625318235]
	TIME [epoch: 8.58 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.909507649433259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.909507649433259 | validation: 2.239246817831135]
	TIME [epoch: 8.57 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5906150024135943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5906150024135943 | validation: 2.086323417983793]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3514980906326446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3514980906326446 | validation: 2.120377199572631]
	TIME [epoch: 8.57 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3439672226030148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3439672226030148 | validation: 2.0509160923107523]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3639627498018934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3639627498018934 | validation: 2.1384058415478915]
	TIME [epoch: 8.83 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.388352521818449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.388352521818449 | validation: 1.999240869461707]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3448829071663693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3448829071663693 | validation: 2.0938482004214873]
	TIME [epoch: 8.59 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.421370612963957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.421370612963957 | validation: 2.1114059155207467]
	TIME [epoch: 8.61 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.57727345081271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.57727345081271 | validation: 2.0147372763308007]
	TIME [epoch: 8.6 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4234876322739118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4234876322739118 | validation: 2.030866698425827]
	TIME [epoch: 8.6 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3817651561076212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3817651561076212 | validation: 2.235183079873236]
	TIME [epoch: 8.58 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4720368391030374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4720368391030374 | validation: 2.1608475247303245]
	TIME [epoch: 8.6 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5199406526864268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5199406526864268 | validation: 1.9663303572463016]
	TIME [epoch: 8.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4258062271914935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4258062271914935 | validation: 2.002197552263131]
	TIME [epoch: 8.59 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5011526220016234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5011526220016234 | validation: 1.9402340868832413]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3117587749459063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3117587749459063 | validation: 2.265592688927276]
	TIME [epoch: 8.57 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5447555064783978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5447555064783978 | validation: 1.991295958714098]
	TIME [epoch: 8.58 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3857390796002613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3857390796002613 | validation: 1.9645325932248827]
	TIME [epoch: 8.57 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.509145175810523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.509145175810523 | validation: 2.1785366063584277]
	TIME [epoch: 8.57 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3430624708189027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3430624708189027 | validation: 1.9218720968156853]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.375540406858353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.375540406858353 | validation: 2.0315919383589938]
	TIME [epoch: 8.57 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4598925704314623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4598925704314623 | validation: 1.983755242510302]
	TIME [epoch: 8.56 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4419210817365444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4419210817365444 | validation: 1.9016643309616663]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.391350228257396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.391350228257396 | validation: 1.8745278506630996]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4188234510916224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4188234510916224 | validation: 1.8647197297839293]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3357193823620457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3357193823620457 | validation: 2.1803272507715628]
	TIME [epoch: 8.6 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3312592327455832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3312592327455832 | validation: 2.0121367050680465]
	TIME [epoch: 8.58 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7863361684921615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7863361684921615 | validation: 2.0596219749096156]
	TIME [epoch: 8.58 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3479461475481767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3479461475481767 | validation: 1.8141043544491784]
	TIME [epoch: 8.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3323219730165037		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.3323219730165037 | validation: 1.801823491766604]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3556314239196257		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.3556314239196257 | validation: 1.855540626814423]
	TIME [epoch: 8.56 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1947795740050822		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.1947795740050822 | validation: 1.8381806257239197]
	TIME [epoch: 8.57 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2247710723742125		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.2247710723742125 | validation: 1.8437507411191656]
	TIME [epoch: 8.6 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2759305739806561		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.2759305739806561 | validation: 1.7998235426613407]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9632623688644708		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.9632623688644708 | validation: 4.487769236837873]
	TIME [epoch: 8.57 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.372001223290333		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 2.372001223290333 | validation: 2.149528529430678]
	TIME [epoch: 8.57 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4269756351981324		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.4269756351981324 | validation: 1.8775111934991324]
	TIME [epoch: 8.58 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2226273719948682		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.2226273719948682 | validation: 1.8675102124856693]
	TIME [epoch: 8.57 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1723961144123718		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.1723961144123718 | validation: 1.7877510019008547]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1271177142672801		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 1.1271177142672801 | validation: 1.774887593058593]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.136248912437727		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 1.136248912437727 | validation: 1.66984961817818]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1425221677591448		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 1.1425221677591448 | validation: 1.5249916845570222]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0953736874335502		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 1.0953736874335502 | validation: 1.1932077343900764]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1183654415426616		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 1.1183654415426616 | validation: 1.2720450796248386]
	TIME [epoch: 8.58 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8655008580932012		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.8655008580932012 | validation: 0.9866381424468731]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7661861054886983		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.7661861054886983 | validation: 0.9520089943110601]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6844562075079432		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.6844562075079432 | validation: 1.2312914557702506]
	TIME [epoch: 8.57 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8172559480922935		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.8172559480922935 | validation: 0.7619685639472156]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6428728633774784		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.6428728633774784 | validation: 1.0560329202584136]
	TIME [epoch: 8.59 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6740161692736946		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.6740161692736946 | validation: 0.6873933788258766]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6068440167830031		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.6068440167830031 | validation: 0.7751876750057209]
	TIME [epoch: 8.58 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5474783149513979		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.5474783149513979 | validation: 0.5258847559017163]
	TIME [epoch: 8.84 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.559132138904251		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.559132138904251 | validation: 0.816913592572458]
	TIME [epoch: 8.59 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5107663475311914		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.5107663475311914 | validation: 0.4860162172908275]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5167983060296046		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.5167983060296046 | validation: 0.6088294604346995]
	TIME [epoch: 8.58 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5365803435660474		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.5365803435660474 | validation: 0.5370098938817521]
	TIME [epoch: 8.58 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4817651583263348		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.4817651583263348 | validation: 0.6277169825629276]
	TIME [epoch: 8.6 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5373445763394844		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.5373445763394844 | validation: 0.4903462177158233]
	TIME [epoch: 8.59 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5034161962832464		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.5034161962832464 | validation: 0.7381293238864095]
	TIME [epoch: 8.58 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5211730372776427		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.5211730372776427 | validation: 0.4365358541702704]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4561761700293921		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.4561761700293921 | validation: 0.5417347316843464]
	TIME [epoch: 8.59 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4168848052320153		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.4168848052320153 | validation: 0.4797381817385069]
	TIME [epoch: 8.58 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46534547753542643		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.46534547753542643 | validation: 0.5850029009130852]
	TIME [epoch: 8.58 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4850563158751764		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.4850563158751764 | validation: 0.7056369205576734]
	TIME [epoch: 8.58 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41801600975554787		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.41801600975554787 | validation: 0.4099304534165509]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4472116783901406		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.4472116783901406 | validation: 0.44329672578659896]
	TIME [epoch: 8.58 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43006276314086944		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.43006276314086944 | validation: 0.3747229935652795]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4398751717799891		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.4398751717799891 | validation: 0.38409415600370367]
	TIME [epoch: 8.57 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36454037243482385		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.36454037243482385 | validation: 0.7320010146513752]
	TIME [epoch: 8.57 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.432639751298928		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.432639751298928 | validation: 0.33570117481239137]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4007768544944593		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.4007768544944593 | validation: 0.7111491535989762]
	TIME [epoch: 8.58 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39615920435520824		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.39615920435520824 | validation: 0.5395148699082328]
	TIME [epoch: 8.58 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3647519717634413		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.3647519717634413 | validation: 0.5501074792113972]
	TIME [epoch: 8.58 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3297624995852332		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.3297624995852332 | validation: 0.44753056729180274]
	TIME [epoch: 8.58 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41166154589073833		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.41166154589073833 | validation: 0.33380171997869834]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3581261341833406		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.3581261341833406 | validation: 0.5114686644999801]
	TIME [epoch: 8.59 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3677125684343685		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.3677125684343685 | validation: 0.3751633221028474]
	TIME [epoch: 8.57 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4182831427132442		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.4182831427132442 | validation: 0.37552451383567786]
	TIME [epoch: 8.59 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3697864514240081		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.3697864514240081 | validation: 0.3526051413318232]
	TIME [epoch: 8.59 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31615984167202893		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.31615984167202893 | validation: 0.32805758134072693]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30675847080749274		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.30675847080749274 | validation: 0.6092403649640574]
	TIME [epoch: 8.58 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40083780271364344		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.40083780271364344 | validation: 0.3526571315979628]
	TIME [epoch: 8.6 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32953534242197396		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.32953534242197396 | validation: 0.34178693975058927]
	TIME [epoch: 8.57 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.340421677961465		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.340421677961465 | validation: 0.6465207516880322]
	TIME [epoch: 8.59 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3995919238188376		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.3995919238188376 | validation: 0.34919225279107335]
	TIME [epoch: 8.58 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29681080032188256		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.29681080032188256 | validation: 0.34601049183635757]
	TIME [epoch: 8.59 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28074498948848564		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.28074498948848564 | validation: 0.4825853377908058]
	TIME [epoch: 8.59 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40831862147873743		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.40831862147873743 | validation: 0.35112773284017373]
	TIME [epoch: 8.58 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.269893331735843		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.269893331735843 | validation: 0.31885946586026215]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31732094512241643		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.31732094512241643 | validation: 0.29976784338070805]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35063256692186745		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.35063256692186745 | validation: 0.39703783072499277]
	TIME [epoch: 8.58 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2931506803222096		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.2931506803222096 | validation: 0.41702371247129866]
	TIME [epoch: 8.57 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2922589771195994		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.2922589771195994 | validation: 0.27918998065801043]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.286363594524336		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.286363594524336 | validation: 0.4748220304205538]
	TIME [epoch: 8.58 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35447161564401597		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.35447161564401597 | validation: 0.41811756969450475]
	TIME [epoch: 8.58 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3610466035176667		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.3610466035176667 | validation: 0.28959341913662545]
	TIME [epoch: 8.57 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.257543001981303		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.257543001981303 | validation: 0.23964913711226513]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.278301777371464		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.278301777371464 | validation: 0.47095379113887265]
	TIME [epoch: 8.58 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3982014944205801		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.3982014944205801 | validation: 0.41917365926941036]
	TIME [epoch: 8.57 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2664268617139341		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.2664268617139341 | validation: 0.21750985128077246]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2850954637489775		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.2850954637489775 | validation: 0.48301671673350144]
	TIME [epoch: 8.59 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39796619817059714		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.39796619817059714 | validation: 0.26303087691068316]
	TIME [epoch: 8.58 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27302572756510296		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.27302572756510296 | validation: 0.26398776599791973]
	TIME [epoch: 8.59 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28969712596405434		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.28969712596405434 | validation: 0.26382018917187416]
	TIME [epoch: 8.58 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2357758550361918		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.2357758550361918 | validation: 0.26295879179580683]
	TIME [epoch: 8.57 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3994104485777601		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.3994104485777601 | validation: 0.3065507194671896]
	TIME [epoch: 8.58 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3255530696800209		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.3255530696800209 | validation: 0.25398530219145116]
	TIME [epoch: 8.59 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2315564103791702		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.2315564103791702 | validation: 0.2530596065859798]
	TIME [epoch: 8.58 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.366675989554231		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.366675989554231 | validation: 0.28862506853882663]
	TIME [epoch: 8.58 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27521834769877296		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.27521834769877296 | validation: 0.24750543150084656]
	TIME [epoch: 8.58 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2520110317253147		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.2520110317253147 | validation: 0.25088851801021406]
	TIME [epoch: 8.59 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2575310142396213		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.2575310142396213 | validation: 0.3493188950313857]
	TIME [epoch: 8.6 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34686993677635514		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.34686993677635514 | validation: 0.38324365707764263]
	TIME [epoch: 8.58 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34369191957487566		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.34369191957487566 | validation: 0.3291893923939079]
	TIME [epoch: 8.58 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2998805759945859		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.2998805759945859 | validation: 0.21157817869388837]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2173493811870008		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.2173493811870008 | validation: 0.25238928041571207]
	TIME [epoch: 8.6 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2854043425188959		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.2854043425188959 | validation: 0.21688952844274045]
	TIME [epoch: 8.58 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3137910339786895		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.3137910339786895 | validation: 0.23812165165626545]
	TIME [epoch: 8.59 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2691197025949351		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.2691197025949351 | validation: 0.2509780661587902]
	TIME [epoch: 8.6 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2229345592251089		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.2229345592251089 | validation: 0.200909299686108]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22061505951342864		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.22061505951342864 | validation: 0.22777725020035944]
	TIME [epoch: 8.59 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29283128935165953		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.29283128935165953 | validation: 0.5095430267138434]
	TIME [epoch: 8.58 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3194877727853581		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.3194877727853581 | validation: 0.3000631009990903]
	TIME [epoch: 8.59 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3282982648747882		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.3282982648747882 | validation: 0.3580646907323075]
	TIME [epoch: 8.59 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3092503236286596		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.3092503236286596 | validation: 0.2988735524958702]
	TIME [epoch: 8.58 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23583197663166677		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.23583197663166677 | validation: 0.25599241024175745]
	TIME [epoch: 8.58 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21727707191601087		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.21727707191601087 | validation: 0.21932583611069245]
	TIME [epoch: 8.58 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3509629320396063		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.3509629320396063 | validation: 0.21916903189512313]
	TIME [epoch: 8.6 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25539294643130755		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.25539294643130755 | validation: 0.23062686016390754]
	TIME [epoch: 8.58 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22468015113360326		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.22468015113360326 | validation: 0.29456906877164213]
	TIME [epoch: 8.57 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23778791856630693		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.23778791856630693 | validation: 0.3229133521803569]
	TIME [epoch: 8.58 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2933863491209185		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.2933863491209185 | validation: 0.2470893801735246]
	TIME [epoch: 8.59 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.280554938632141		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.280554938632141 | validation: 0.34085522793860334]
	TIME [epoch: 8.58 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2903269300010159		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.2903269300010159 | validation: 0.2203442416625608]
	TIME [epoch: 8.58 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1998938118036976		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.1998938118036976 | validation: 0.2127416342912774]
	TIME [epoch: 8.57 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27791144711071586		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.27791144711071586 | validation: 0.2063085253802264]
	TIME [epoch: 8.6 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2596101571479177		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.2596101571479177 | validation: 0.36380809267275405]
	TIME [epoch: 8.57 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30266507314298985		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.30266507314298985 | validation: 0.18168538227796915]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24321399124578585		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.24321399124578585 | validation: 0.2294757049927212]
	TIME [epoch: 8.56 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2070055940723257		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.2070055940723257 | validation: 0.3509512098667732]
	TIME [epoch: 8.57 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30536005042489267		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.30536005042489267 | validation: 0.19285143254749054]
	TIME [epoch: 8.57 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2390165064586786		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.2390165064586786 | validation: 0.19818676246310868]
	TIME [epoch: 8.58 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2030705838103777		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.2030705838103777 | validation: 0.27768329422328125]
	TIME [epoch: 8.57 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21267398823012157		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.21267398823012157 | validation: 0.2339873884113041]
	TIME [epoch: 8.58 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22542208886845594		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.22542208886845594 | validation: 0.5873982960041353]
	TIME [epoch: 8.57 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3448561640080674		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.3448561640080674 | validation: 0.3372542131823436]
	TIME [epoch: 8.57 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2960699407640048		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.2960699407640048 | validation: 0.1923952986310279]
	TIME [epoch: 8.56 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2278697415043263		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.2278697415043263 | validation: 0.19345101826173738]
	TIME [epoch: 8.57 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22135041818834578		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.22135041818834578 | validation: 0.2802668016695214]
	TIME [epoch: 8.59 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25530677527617746		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.25530677527617746 | validation: 0.2608231457452054]
	TIME [epoch: 8.57 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22100237273411444		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.22100237273411444 | validation: 0.18877709181187807]
	TIME [epoch: 8.59 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1820362329598055		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.1820362329598055 | validation: 0.1770801839027033]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24522286133957705		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.24522286133957705 | validation: 0.29972465903798823]
	TIME [epoch: 8.58 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23359460305403676		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.23359460305403676 | validation: 0.2069688157398138]
	TIME [epoch: 8.56 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21291138534726295		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.21291138534726295 | validation: 0.188844403475811]
	TIME [epoch: 8.57 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23320605858718832		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.23320605858718832 | validation: 0.3987202786229126]
	TIME [epoch: 8.57 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2697445968802		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.2697445968802 | validation: 0.19178232178306298]
	TIME [epoch: 8.58 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25006803557888013		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.25006803557888013 | validation: 0.2367217542636702]
	TIME [epoch: 8.57 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23742220906915074		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.23742220906915074 | validation: 0.21344725962915806]
	TIME [epoch: 8.56 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22065768810900205		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.22065768810900205 | validation: 0.206215998573876]
	TIME [epoch: 8.57 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24539812569093117		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.24539812569093117 | validation: 0.20206115462337748]
	TIME [epoch: 8.58 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2472483125028606		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.2472483125028606 | validation: 0.26202326527811093]
	TIME [epoch: 8.56 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21189957412511956		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.21189957412511956 | validation: 0.20894809807849868]
	TIME [epoch: 8.57 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24083183380865053		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.24083183380865053 | validation: 0.2713224345618273]
	TIME [epoch: 8.56 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2373042363020978		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.2373042363020978 | validation: 0.2483266165659479]
	TIME [epoch: 8.57 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2408886193329279		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.2408886193329279 | validation: 0.209389703471754]
	TIME [epoch: 8.56 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19139980854122635		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.19139980854122635 | validation: 0.19609272721326965]
	TIME [epoch: 8.57 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2304883923561264		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.2304883923561264 | validation: 0.1711067549460707]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23493768823414118		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.23493768823414118 | validation: 0.18685867405068113]
	TIME [epoch: 8.57 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17089784081919412		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.17089784081919412 | validation: 0.16053378381994818]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23633030430082264		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.23633030430082264 | validation: 0.18281817635047096]
	TIME [epoch: 8.57 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20566991399544765		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.20566991399544765 | validation: 0.30476225612799657]
	TIME [epoch: 8.57 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26907703055933485		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.26907703055933485 | validation: 0.18442603133496954]
	TIME [epoch: 8.58 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1854605327610987		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.1854605327610987 | validation: 0.19380237765715214]
	TIME [epoch: 8.58 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22650842562497975		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.22650842562497975 | validation: 0.20991287139109344]
	TIME [epoch: 8.58 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23276270161656926		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.23276270161656926 | validation: 0.2564729072438427]
	TIME [epoch: 8.56 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21658197629654957		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.21658197629654957 | validation: 0.15440774993344328]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2065199304082443		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.2065199304082443 | validation: 0.2384797866954126]
	TIME [epoch: 8.6 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25694552173112156		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.25694552173112156 | validation: 0.3081358643138148]
	TIME [epoch: 8.57 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21178062769844197		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.21178062769844197 | validation: 0.1749921937414927]
	TIME [epoch: 8.58 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1530206181175702		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.1530206181175702 | validation: 0.144188122018949]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2359712674734527		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.2359712674734527 | validation: 0.312955749487139]
	TIME [epoch: 8.59 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2694321342190467		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.2694321342190467 | validation: 0.24799382615124144]
	TIME [epoch: 8.57 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21611483170244947		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.21611483170244947 | validation: 0.20485489467673057]
	TIME [epoch: 8.58 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21823403215117823		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.21823403215117823 | validation: 0.18245206754137372]
	TIME [epoch: 8.57 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2205781752899823		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.2205781752899823 | validation: 0.17385758065293344]
	TIME [epoch: 8.59 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18290756213616993		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.18290756213616993 | validation: 0.21436778072036494]
	TIME [epoch: 8.58 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18550602811320815		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.18550602811320815 | validation: 0.16808913764561015]
	TIME [epoch: 8.57 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18302096614100774		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.18302096614100774 | validation: 0.26395206921456743]
	TIME [epoch: 8.57 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2264450303146602		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.2264450303146602 | validation: 0.2099925747990769]
	TIME [epoch: 8.58 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.193799732892968		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.193799732892968 | validation: 0.1905991612941701]
	TIME [epoch: 8.57 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20165068155086566		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.20165068155086566 | validation: 0.29382171741858587]
	TIME [epoch: 8.57 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27312315404877546		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.27312315404877546 | validation: 0.2072582500310654]
	TIME [epoch: 8.57 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24686478821477575		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.24686478821477575 | validation: 0.1804923325504301]
	TIME [epoch: 8.57 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18845602394751926		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.18845602394751926 | validation: 0.22071286055039174]
	TIME [epoch: 8.58 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2062599757246035		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.2062599757246035 | validation: 0.21710663966811883]
	TIME [epoch: 8.57 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.194282063011061		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.194282063011061 | validation: 0.15821604792256527]
	TIME [epoch: 8.57 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2826180669621573		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.2826180669621573 | validation: 0.21948460079359727]
	TIME [epoch: 8.56 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18040112975279718		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.18040112975279718 | validation: 0.2283417733118317]
	TIME [epoch: 8.58 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16822657144700887		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.16822657144700887 | validation: 0.20800863107566658]
	TIME [epoch: 8.57 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22749229261320603		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.22749229261320603 | validation: 0.19719831986900838]
	TIME [epoch: 8.57 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19455362497691972		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.19455362497691972 | validation: 0.18230321200310967]
	TIME [epoch: 8.56 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16939968986489762		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.16939968986489762 | validation: 0.15750088581986943]
	TIME [epoch: 8.57 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20218649918381892		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.20218649918381892 | validation: 0.17276118556971948]
	TIME [epoch: 8.59 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1735215608247666		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.1735215608247666 | validation: 0.14886130091315422]
	TIME [epoch: 8.58 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16415838519834244		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.16415838519834244 | validation: 0.2072244496436062]
	TIME [epoch: 8.56 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18552406236236044		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.18552406236236044 | validation: 0.1692389748619756]
	TIME [epoch: 8.58 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16064537556567343		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.16064537556567343 | validation: 0.22646272381344132]
	TIME [epoch: 8.57 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28928416518965977		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.28928416518965977 | validation: 0.24845747457807515]
	TIME [epoch: 8.57 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19113666751055322		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.19113666751055322 | validation: 0.16032970469889496]
	TIME [epoch: 8.56 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14504573644374166		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.14504573644374166 | validation: 0.1400234611275949]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1718406163449786		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.1718406163449786 | validation: 0.1308598275489454]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18503625764974935		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.18503625764974935 | validation: 0.21917788024772838]
	TIME [epoch: 8.55 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17912811005072493		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.17912811005072493 | validation: 0.16980280876707288]
	TIME [epoch: 8.56 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15961896952042243		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.15961896952042243 | validation: 0.16334956547073737]
	TIME [epoch: 8.57 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21743204196405488		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.21743204196405488 | validation: 0.1557602981340079]
	TIME [epoch: 8.58 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15536002594641768		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.15536002594641768 | validation: 0.12192275887215973]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1788903406831691		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.1788903406831691 | validation: 0.1398747834898419]
	TIME [epoch: 8.57 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17371888218037745		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.17371888218037745 | validation: 0.13197572277994987]
	TIME [epoch: 8.57 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15534491703551864		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.15534491703551864 | validation: 0.13599916877149124]
	TIME [epoch: 8.58 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2177491746397518		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.2177491746397518 | validation: 0.16152974274335746]
	TIME [epoch: 8.57 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15510035411474812		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.15510035411474812 | validation: 0.1737410061342866]
	TIME [epoch: 8.56 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1679836496063285		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.1679836496063285 | validation: 0.1852701190490566]
	TIME [epoch: 8.56 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16997814691987354		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.16997814691987354 | validation: 0.1842659879108314]
	TIME [epoch: 8.59 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16812443236370392		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.16812443236370392 | validation: 0.15276829158083288]
	TIME [epoch: 8.57 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1526197222008433		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.1526197222008433 | validation: 0.1507129605237931]
	TIME [epoch: 8.57 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23230378326862186		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.23230378326862186 | validation: 0.22107143197522555]
	TIME [epoch: 8.58 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22536340031424384		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.22536340031424384 | validation: 0.1401664927039987]
	TIME [epoch: 8.58 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14858613034272417		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.14858613034272417 | validation: 0.21162431290406536]
	TIME [epoch: 8.59 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21642367340756336		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.21642367340756336 | validation: 0.21357771364943973]
	TIME [epoch: 8.57 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19470760629778458		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.19470760629778458 | validation: 0.1363703995747665]
	TIME [epoch: 8.58 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13751156973286657		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.13751156973286657 | validation: 0.1628707881692349]
	TIME [epoch: 8.58 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14080951288028767		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.14080951288028767 | validation: 0.13390954304963093]
	TIME [epoch: 8.58 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16367653436965113		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.16367653436965113 | validation: 0.37561289699240097]
	TIME [epoch: 8.57 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34903192306492337		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.34903192306492337 | validation: 0.258483394395]
	TIME [epoch: 8.56 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21832884031599997		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.21832884031599997 | validation: 0.151853705746538]
	TIME [epoch: 8.58 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13244941249611425		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.13244941249611425 | validation: 0.116601065490956]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13129987517198588		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.13129987517198588 | validation: 0.22845292433041353]
	TIME [epoch: 8.58 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1640281353245165		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.1640281353245165 | validation: 0.12710264017980621]
	TIME [epoch: 8.58 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14545410357677732		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.14545410357677732 | validation: 0.2606635267287552]
	TIME [epoch: 8.59 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24729557570959532		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.24729557570959532 | validation: 0.30079808503690153]
	TIME [epoch: 8.6 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18025853061915478		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.18025853061915478 | validation: 0.12254220388574058]
	TIME [epoch: 8.59 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13511033069150974		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.13511033069150974 | validation: 0.16294270308584247]
	TIME [epoch: 8.59 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16422231619377142		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.16422231619377142 | validation: 0.1702960198439739]
	TIME [epoch: 8.58 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13828643914008754		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.13828643914008754 | validation: 0.11917338648666835]
	TIME [epoch: 8.59 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16942324542837195		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.16942324542837195 | validation: 0.2926136034995378]
	TIME [epoch: 8.58 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20962587540491268		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.20962587540491268 | validation: 0.12123632276559555]
	TIME [epoch: 8.58 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13456624926178218		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.13456624926178218 | validation: 0.14512096827897947]
	TIME [epoch: 8.58 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13695624850330734		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.13695624850330734 | validation: 0.11299941285815465]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1538071706160376		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.1538071706160376 | validation: 0.18792320248912103]
	TIME [epoch: 8.58 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1555497407419905		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.1555497407419905 | validation: 0.12910025996654248]
	TIME [epoch: 8.57 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14899292535069403		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.14899292535069403 | validation: 0.14700525360449757]
	TIME [epoch: 8.58 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13262231597316515		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.13262231597316515 | validation: 0.12325805766798476]
	TIME [epoch: 8.6 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11442940690479643		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.11442940690479643 | validation: 0.12470846819388318]
	TIME [epoch: 8.59 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19603995783176553		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.19603995783176553 | validation: 0.2502286744576555]
	TIME [epoch: 8.57 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20106647379482093		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.20106647379482093 | validation: 0.19931070906430948]
	TIME [epoch: 8.58 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15862781012233018		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.15862781012233018 | validation: 0.12826198141021347]
	TIME [epoch: 8.6 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13690491757462803		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.13690491757462803 | validation: 0.12874538410123496]
	TIME [epoch: 8.59 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11621183620828544		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.11621183620828544 | validation: 0.11285678143883862]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15414241017748206		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.15414241017748206 | validation: 0.1295317536915158]
	TIME [epoch: 8.58 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14878712207692477		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.14878712207692477 | validation: 0.15642350266950128]
	TIME [epoch: 8.59 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14029681345057288		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.14029681345057288 | validation: 0.1434300111507211]
	TIME [epoch: 8.58 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1362638581275975		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.1362638581275975 | validation: 0.20659955066026886]
	TIME [epoch: 8.57 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2403276350353342		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.2403276350353342 | validation: 0.21746460755151947]
	TIME [epoch: 8.57 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1649498525126136		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.1649498525126136 | validation: 0.10717057346968462]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14254507590831197		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.14254507590831197 | validation: 0.1380377284899611]
	TIME [epoch: 8.58 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15258704716066485		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.15258704716066485 | validation: 0.1818975867839729]
	TIME [epoch: 8.57 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13386799925691156		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.13386799925691156 | validation: 0.11538092564986964]
	TIME [epoch: 8.58 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1305089503280729		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.1305089503280729 | validation: 0.16194049184218418]
	TIME [epoch: 8.55 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16477449158874105		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.16477449158874105 | validation: 0.18262804285355427]
	TIME [epoch: 8.58 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17002019359189177		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.17002019359189177 | validation: 0.28024564923972556]
	TIME [epoch: 8.56 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16253427996087236		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.16253427996087236 | validation: 0.21946062653790804]
	TIME [epoch: 8.57 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15180379160790838		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.15180379160790838 | validation: 0.10285554683510714]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13301531422171375		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.13301531422171375 | validation: 0.16424103001594764]
	TIME [epoch: 8.59 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16033267445575405		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.16033267445575405 | validation: 0.16724267388112873]
	TIME [epoch: 8.57 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1482775509489443		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.1482775509489443 | validation: 0.10452854050265464]
	TIME [epoch: 8.56 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11550955916073369		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.11550955916073369 | validation: 0.1332447473865757]
	TIME [epoch: 8.57 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12178537662491444		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.12178537662491444 | validation: 0.09316317239160607]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14847781163456664		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.14847781163456664 | validation: 0.17499266695899207]
	TIME [epoch: 8.57 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13372386378950107		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.13372386378950107 | validation: 0.11422538746677147]
	TIME [epoch: 8.57 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11871140182935934		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.11871140182935934 | validation: 0.11675251100961928]
	TIME [epoch: 8.57 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12808831499207596		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.12808831499207596 | validation: 0.11595784331188952]
	TIME [epoch: 8.58 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11579289900158843		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.11579289900158843 | validation: 0.11509947144674672]
	TIME [epoch: 8.58 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14748603108061512		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.14748603108061512 | validation: 0.1985503329069241]
	TIME [epoch: 8.59 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14304900079579128		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.14304900079579128 | validation: 0.09760763900817843]
	TIME [epoch: 8.58 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11112124597773519		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.11112124597773519 | validation: 0.10682947283835421]
	TIME [epoch: 8.58 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1205736379686197		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.1205736379686197 | validation: 0.10562349675870722]
	TIME [epoch: 8.59 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19050676334812316		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.19050676334812316 | validation: 0.15776820165674652]
	TIME [epoch: 8.59 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14329931760123799		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.14329931760123799 | validation: 0.12022823431193341]
	TIME [epoch: 8.58 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14162181817333616		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.14162181817333616 | validation: 0.112868378568764]
	TIME [epoch: 8.58 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1059328087984717		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.1059328087984717 | validation: 0.0980030628420401]
	TIME [epoch: 8.59 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08947366199087604		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.08947366199087604 | validation: 0.13618153345137124]
	TIME [epoch: 8.58 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1529250441074225		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.1529250441074225 | validation: 0.11764329525313226]
	TIME [epoch: 8.57 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1256218172533446		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.1256218172533446 | validation: 0.20643486181326035]
	TIME [epoch: 8.59 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14089074875294352		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.14089074875294352 | validation: 0.09181487703153146]
	TIME [epoch: 8.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11081291358144738		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.11081291358144738 | validation: 0.1952385836389065]
	TIME [epoch: 8.57 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1793029603399986		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.1793029603399986 | validation: 0.10626229975444694]
	TIME [epoch: 8.58 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10703269803055229		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.10703269803055229 | validation: 0.08724837888655977]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10375098636962865		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.10375098636962865 | validation: 0.15389904498195328]
	TIME [epoch: 8.59 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17468397928811286		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.17468397928811286 | validation: 0.09688002046191263]
	TIME [epoch: 8.57 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09779630586597567		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.09779630586597567 | validation: 0.08290070136629848]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0922845924209489		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.0922845924209489 | validation: 0.0753980934810849]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09115718053789434		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.09115718053789434 | validation: 0.10349548590711549]
	TIME [epoch: 8.57 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13315668163741345		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.13315668163741345 | validation: 0.15016787661459233]
	TIME [epoch: 8.55 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10737999750258867		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.10737999750258867 | validation: 0.09673420375867842]
	TIME [epoch: 8.56 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11064677214395138		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.11064677214395138 | validation: 0.09591552246604843]
	TIME [epoch: 8.55 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1555626168769464		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.1555626168769464 | validation: 0.08621185276442025]
	TIME [epoch: 8.57 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11665536942780816		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.11665536942780816 | validation: 0.13238485444366252]
	TIME [epoch: 8.57 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12879578962203403		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.12879578962203403 | validation: 0.08763475194290034]
	TIME [epoch: 8.55 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07799108914383895		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.07799108914383895 | validation: 0.08650824782680766]
	TIME [epoch: 8.56 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11774857968170463		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.11774857968170463 | validation: 0.16045179664524883]
	TIME [epoch: 8.57 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11614881252936554		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.11614881252936554 | validation: 0.21766184334732652]
	TIME [epoch: 8.57 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16096318298594334		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.16096318298594334 | validation: 0.09528549655980043]
	TIME [epoch: 8.57 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09370118008787812		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.09370118008787812 | validation: 0.15122538398543]
	TIME [epoch: 8.57 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11399835345408649		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.11399835345408649 | validation: 0.15485897502353896]
	TIME [epoch: 8.57 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11395117502750804		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.11395117502750804 | validation: 0.17324126962660694]
	TIME [epoch: 8.57 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30161346959334223		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.30161346959334223 | validation: 0.19862562653299953]
	TIME [epoch: 8.56 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20107867781887537		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.20107867781887537 | validation: 0.15007829866054898]
	TIME [epoch: 8.56 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13599511130113376		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.13599511130113376 | validation: 0.09813629185423903]
	TIME [epoch: 8.55 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10234697004290845		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.10234697004290845 | validation: 0.12149103240025369]
	TIME [epoch: 8.56 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10705283443785735		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.10705283443785735 | validation: 0.08640497277686285]
	TIME [epoch: 8.55 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07690052052341562		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.07690052052341562 | validation: 0.09379371000508084]
	TIME [epoch: 8.57 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0842725510252507		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.0842725510252507 | validation: 0.08149172421887341]
	TIME [epoch: 8.56 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11263091057082208		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.11263091057082208 | validation: 0.12759830106368303]
	TIME [epoch: 8.58 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.121293196204704		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.121293196204704 | validation: 0.12329097346214626]
	TIME [epoch: 8.57 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10645663447192542		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.10645663447192542 | validation: 0.10652382108889108]
	TIME [epoch: 8.56 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16334204973302935		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.16334204973302935 | validation: 0.0797979767195193]
	TIME [epoch: 8.58 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13294923566848443		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.13294923566848443 | validation: 0.07910952322540463]
	TIME [epoch: 8.57 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11528435842539364		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.11528435842539364 | validation: 0.07362966649349809]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0824206812310096		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.0824206812310096 | validation: 0.07261989532537073]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15973722838720184		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.15973722838720184 | validation: 0.227285498978766]
	TIME [epoch: 8.57 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1446934333275433		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.1446934333275433 | validation: 0.13325364956798116]
	TIME [epoch: 8.59 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12421232144473918		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.12421232144473918 | validation: 0.07389827239583645]
	TIME [epoch: 8.59 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07917795620430682		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.07917795620430682 | validation: 0.06472134913567762]
	TIME [epoch: 8.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07160280620863976		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.07160280620863976 | validation: 0.10967769693451573]
	TIME [epoch: 8.57 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11034292581348132		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.11034292581348132 | validation: 0.10405357972509667]
	TIME [epoch: 8.57 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0966217512303384		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.0966217512303384 | validation: 0.17979318475252737]
	TIME [epoch: 8.58 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12505147798015695		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.12505147798015695 | validation: 0.07790748480021237]
	TIME [epoch: 8.57 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07831258829241114		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.07831258829241114 | validation: 0.12099347498944019]
	TIME [epoch: 8.57 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10484585638281507		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.10484585638281507 | validation: 0.07215103931717343]
	TIME [epoch: 8.57 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07221566757771955		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.07221566757771955 | validation: 0.08658771042913699]
	TIME [epoch: 8.58 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06692534949002509		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.06692534949002509 | validation: 0.08605657911260871]
	TIME [epoch: 8.57 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15388191278662477		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.15388191278662477 | validation: 0.1034819680464069]
	TIME [epoch: 8.57 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10594136464212091		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.10594136464212091 | validation: 0.08008604772556714]
	TIME [epoch: 8.56 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1334345263862642		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.1334345263862642 | validation: 0.07300128596856292]
	TIME [epoch: 8.58 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11920906793074679		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.11920906793074679 | validation: 0.05922272452838737]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07142479422724503		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.07142479422724503 | validation: 0.05377851189793223]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060987215717742724		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.060987215717742724 | validation: 0.0647305769249468]
	TIME [epoch: 8.56 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0628589898098454		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.0628589898098454 | validation: 0.11437454821927233]
	TIME [epoch: 8.58 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11269593481377696		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.11269593481377696 | validation: 0.09615722359387319]
	TIME [epoch: 8.56 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11398895909287031		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.11398895909287031 | validation: 0.05905971040380068]
	TIME [epoch: 8.56 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06701809158845669		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.06701809158845669 | validation: 0.08631316962423952]
	TIME [epoch: 8.56 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16950403216502397		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.16950403216502397 | validation: 0.11537006980687978]
	TIME [epoch: 8.58 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14510351662267254		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.14510351662267254 | validation: 0.07292326039203982]
	TIME [epoch: 8.57 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08718378984551052		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.08718378984551052 | validation: 0.07115079663099005]
	TIME [epoch: 8.57 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09497839099653616		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.09497839099653616 | validation: 0.04405289470975852]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16795536991182639		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.16795536991182639 | validation: 0.0748170893613137]
	TIME [epoch: 8.57 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0920274369158432		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.0920274369158432 | validation: 0.12498442666006102]
	TIME [epoch: 8.56 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08721182940355844		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.08721182940355844 | validation: 0.052899861103451615]
	TIME [epoch: 8.56 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09606872217073975		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.09606872217073975 | validation: 0.06543399448950259]
	TIME [epoch: 8.55 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3524258942697407		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.3524258942697407 | validation: 0.2375794609063339]
	TIME [epoch: 8.56 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13964847860087376		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.13964847860087376 | validation: 0.08152421131794649]
	TIME [epoch: 8.58 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06999542885388761		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.06999542885388761 | validation: 0.052288096002751985]
	TIME [epoch: 8.56 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09224116850594966		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.09224116850594966 | validation: 0.1623508711468022]
	TIME [epoch: 8.57 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10846335113781787		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.10846335113781787 | validation: 0.058489685976257706]
	TIME [epoch: 8.56 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06242761532731762		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.06242761532731762 | validation: 0.0624878808729768]
	TIME [epoch: 8.58 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06184721520902277		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.06184721520902277 | validation: 0.07006951785703511]
	TIME [epoch: 8.57 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07720181466330157		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.07720181466330157 | validation: 0.04774402337478095]
	TIME [epoch: 8.56 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07629302398876617		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.07629302398876617 | validation: 0.09186931699448321]
	TIME [epoch: 8.57 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07817706549316203		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.07817706549316203 | validation: 0.052508334261510525]
	TIME [epoch: 8.59 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06310550443003694		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.06310550443003694 | validation: 0.08271293076362717]
	TIME [epoch: 8.57 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06459827487428116		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.06459827487428116 | validation: 0.1165402664115645]
	TIME [epoch: 8.57 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0728931758375036		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.0728931758375036 | validation: 0.052595050082225245]
	TIME [epoch: 8.56 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08760168390682535		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.08760168390682535 | validation: 0.08806080070253597]
	TIME [epoch: 8.58 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1645709121266969		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.1645709121266969 | validation: 0.1779962057705007]
	TIME [epoch: 8.56 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11988226428650814		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.11988226428650814 | validation: 0.07727443541696273]
	TIME [epoch: 8.56 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09305017625382123		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.09305017625382123 | validation: 0.05944220979377675]
	TIME [epoch: 8.57 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06457048979841326		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.06457048979841326 | validation: 0.05872857333151621]
	TIME [epoch: 8.56 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05118666955095973		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.05118666955095973 | validation: 0.12284662188433212]
	TIME [epoch: 8.58 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09791668621248614		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.09791668621248614 | validation: 0.07305734176650575]
	TIME [epoch: 8.56 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08145424595597635		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.08145424595597635 | validation: 0.04995329909118909]
	TIME [epoch: 8.57 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0519210909591276		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.0519210909591276 | validation: 0.0780206802268508]
	TIME [epoch: 8.57 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08109252091661631		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.08109252091661631 | validation: 0.04173419780450818]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05178428906919269		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.05178428906919269 | validation: 0.04609056954987871]
	TIME [epoch: 8.58 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07774698557855547		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.07774698557855547 | validation: 0.04151409686515737]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05958128853091878		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.05958128853091878 | validation: 0.05090740132528589]
	TIME [epoch: 8.59 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.084539841841885		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.084539841841885 | validation: 0.08667879966722233]
	TIME [epoch: 8.58 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06451469561879851		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.06451469561879851 | validation: 0.04581168042848455]
	TIME [epoch: 8.58 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05037304635603797		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.05037304635603797 | validation: 0.04537017335466925]
	TIME [epoch: 8.56 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06533797588485846		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.06533797588485846 | validation: 0.17644956016726354]
	TIME [epoch: 8.55 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13856766304605514		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.13856766304605514 | validation: 0.08232665594361291]
	TIME [epoch: 8.59 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0703627822856906		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.0703627822856906 | validation: 0.1312947984060283]
	TIME [epoch: 8.57 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07964832317226686		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.07964832317226686 | validation: 0.04421191302010744]
	TIME [epoch: 8.57 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053526362419123605		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.053526362419123605 | validation: 0.03879008679102816]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057600170243770024		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.057600170243770024 | validation: 0.07416749977480554]
	TIME [epoch: 8.58 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13134444088222988		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.13134444088222988 | validation: 0.07976997354538969]
	TIME [epoch: 8.57 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05491549092008245		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.05491549092008245 | validation: 0.03559385134095993]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04138905338790499		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.04138905338790499 | validation: 0.03874025061115463]
	TIME [epoch: 8.57 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06826043156043206		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.06826043156043206 | validation: 0.04199039099038023]
	TIME [epoch: 8.58 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06898259738172326		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.06898259738172326 | validation: 0.06672899329543945]
	TIME [epoch: 8.57 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08372976630025776		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.08372976630025776 | validation: 0.05559823392267969]
	TIME [epoch: 8.57 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05295487518826909		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.05295487518826909 | validation: 0.03652264805795949]
	TIME [epoch: 8.57 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047749236010303245		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.047749236010303245 | validation: 0.050472200100106265]
	TIME [epoch: 8.58 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04175097587157115		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.04175097587157115 | validation: 0.047497506943499704]
	TIME [epoch: 8.58 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08883482622049045		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.08883482622049045 | validation: 0.05308886215671048]
	TIME [epoch: 8.57 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08391171731276968		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.08391171731276968 | validation: 0.09604616282875614]
	TIME [epoch: 8.57 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05580856122608134		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.05580856122608134 | validation: 0.03967145918496302]
	TIME [epoch: 8.58 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04241272547885677		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.04241272547885677 | validation: 0.05343494335199552]
	TIME [epoch: 8.59 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07084220030781842		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.07084220030781842 | validation: 0.044562342744340316]
	TIME [epoch: 8.56 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04670103230492448		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.04670103230492448 | validation: 0.03423859974930066]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07073694470483922		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.07073694470483922 | validation: 0.06431560427207775]
	TIME [epoch: 8.57 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0611787939058405		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.0611787939058405 | validation: 0.053260198446137]
	TIME [epoch: 8.57 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04849787525672581		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.04849787525672581 | validation: 0.06056707861253961]
	TIME [epoch: 8.56 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05787075215657765		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.05787075215657765 | validation: 0.041055426545674775]
	TIME [epoch: 8.56 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06328830915690743		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.06328830915690743 | validation: 0.030088800050933368]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03717298066965048		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.03717298066965048 | validation: 0.0266068195846381]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043112104629629916		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.043112104629629916 | validation: 0.046291574616918796]
	TIME [epoch: 8.59 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06743706157289626		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.06743706157289626 | validation: 0.02819436970194368]
	TIME [epoch: 8.58 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04457859422731509		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.04457859422731509 | validation: 0.09142466402165975]
	TIME [epoch: 8.57 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055748379877660424		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.055748379877660424 | validation: 0.028327601081295714]
	TIME [epoch: 8.58 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050318742221930175		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.050318742221930175 | validation: 0.07920988565959221]
	TIME [epoch: 8.57 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13114941551896858		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.13114941551896858 | validation: 0.07227291408424795]
	TIME [epoch: 8.58 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05679046003279129		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.05679046003279129 | validation: 0.03183217274367435]
	TIME [epoch: 8.57 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036794164521221084		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.036794164521221084 | validation: 0.027235591375070728]
	TIME [epoch: 8.59 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03184170689190309		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.03184170689190309 | validation: 0.03086269009299924]
	TIME [epoch: 8.57 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038901747157956426		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.038901747157956426 | validation: 0.036794520429928194]
	TIME [epoch: 8.57 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04442188779923956		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.04442188779923956 | validation: 0.10076955148545716]
	TIME [epoch: 8.57 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06449702645796895		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.06449702645796895 | validation: 0.030192581544522257]
	TIME [epoch: 8.58 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18309811544870525		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.18309811544870525 | validation: 0.11798588500998373]
	TIME [epoch: 8.56 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08195382916906982		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.08195382916906982 | validation: 0.0574150635551409]
	TIME [epoch: 8.57 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04833025350844686		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.04833025350844686 | validation: 0.02940404577349429]
	TIME [epoch: 8.57 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037546509305750625		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.037546509305750625 | validation: 0.022242417855684538]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049373595737060766		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.049373595737060766 | validation: 0.05883732101813215]
	TIME [epoch: 8.58 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041308247490209976		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.041308247490209976 | validation: 0.0558877388913026]
	TIME [epoch: 8.56 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11470960955709664		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.11470960955709664 | validation: 0.11552651163329446]
	TIME [epoch: 8.57 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06753039821417235		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.06753039821417235 | validation: 0.03384268073566293]
	TIME [epoch: 8.57 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03165837693436656		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.03165837693436656 | validation: 0.02576588153500079]
	TIME [epoch: 8.59 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03197857491241684		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.03197857491241684 | validation: 0.058115420424954406]
	TIME [epoch: 8.57 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07231792354080739		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.07231792354080739 | validation: 0.05074386758395753]
	TIME [epoch: 8.56 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043160128805756766		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.043160128805756766 | validation: 0.030124169476286176]
	TIME [epoch: 8.57 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04516508146662814		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.04516508146662814 | validation: 0.0695493290882151]
	TIME [epoch: 8.59 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059127294753281115		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.059127294753281115 | validation: 0.04719673077331627]
	TIME [epoch: 8.57 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04327708547921463		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.04327708547921463 | validation: 0.046441418247208016]
	TIME [epoch: 8.57 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038957458610414684		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.038957458610414684 | validation: 0.039322027828253094]
	TIME [epoch: 8.57 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09183637098255068		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.09183637098255068 | validation: 0.07445829859276852]
	TIME [epoch: 8.59 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061467877482696325		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.061467877482696325 | validation: 0.05677098524161458]
	TIME [epoch: 8.58 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03947232017536118		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.03947232017536118 | validation: 0.023052977861626293]
	TIME [epoch: 8.58 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03851783096706165		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.03851783096706165 | validation: 0.0411613738362608]
	TIME [epoch: 8.58 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03305650482334528		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.03305650482334528 | validation: 0.04279178659387983]
	TIME [epoch: 8.58 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034010399211817616		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.034010399211817616 | validation: 0.03182644729107777]
	TIME [epoch: 8.6 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033168410608807875		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.033168410608807875 | validation: 0.026742718512484193]
	TIME [epoch: 8.58 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06012066326994278		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.06012066326994278 | validation: 0.0406370711289261]
	TIME [epoch: 8.57 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17135066275378397		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.17135066275378397 | validation: 0.46164204281977295]
	TIME [epoch: 8.59 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3460618573954676		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.3460618573954676 | validation: 0.12353389817902834]
	TIME [epoch: 8.58 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13544206014164048		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.13544206014164048 | validation: 0.08665013874144337]
	TIME [epoch: 8.57 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09073515656816823		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.09073515656816823 | validation: 0.05804849743848582]
	TIME [epoch: 8.57 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05123226249600012		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.05123226249600012 | validation: 0.032149246230171076]
	TIME [epoch: 8.57 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034540994435252215		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.034540994435252215 | validation: 0.02356990300668698]
	TIME [epoch: 8.6 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027631784831760205		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.027631784831760205 | validation: 0.020863761008274237]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03292959082517321		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.03292959082517321 | validation: 0.04037485937771359]
	TIME [epoch: 8.57 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03616019034576738		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.03616019034576738 | validation: 0.019854541147663777]
	TIME [epoch: 8.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051848713258346155		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.051848713258346155 | validation: 0.02758045580629793]
	TIME [epoch: 8.57 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032625654734318846		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.032625654734318846 | validation: 0.02255829055889302]
	TIME [epoch: 8.57 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06878235706946449		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.06878235706946449 | validation: 0.13942527644402647]
	TIME [epoch: 8.57 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11390278404020514		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.11390278404020514 | validation: 0.05865533273302102]
	TIME [epoch: 8.57 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04444302270694313		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.04444302270694313 | validation: 0.030104893578279647]
	TIME [epoch: 8.57 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03251344720782243		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.03251344720782243 | validation: 0.02494705474242121]
	TIME [epoch: 8.58 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031245337355019213		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.031245337355019213 | validation: 0.0454609911273072]
	TIME [epoch: 8.56 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03993777520729935		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.03993777520729935 | validation: 0.04985910445248078]
	TIME [epoch: 8.57 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031208481963133868		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.031208481963133868 | validation: 0.021080091092974686]
	TIME [epoch: 8.59 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024727958285156146		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.024727958285156146 | validation: 0.018820013351770568]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046848249316889364		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.046848249316889364 | validation: 0.07333574457136421]
	TIME [epoch: 8.56 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07912055454569131		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.07912055454569131 | validation: 0.197389527090695]
	TIME [epoch: 8.56 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14074795907617788		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.14074795907617788 | validation: 0.10537299670664889]
	TIME [epoch: 8.56 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07309535349880648		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.07309535349880648 | validation: 0.03300908789129854]
	TIME [epoch: 8.56 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04272267772700945		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.04272267772700945 | validation: 0.03680511906989775]
	TIME [epoch: 8.54 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035221020658759046		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.035221020658759046 | validation: 0.025128613970500268]
	TIME [epoch: 8.54 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027375898499566145		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.027375898499566145 | validation: 0.02134339606434406]
	TIME [epoch: 8.56 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04569702770840263		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.04569702770840263 | validation: 0.03706812967984275]
	TIME [epoch: 8.56 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03883070919916126		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.03883070919916126 | validation: 0.026399466662487294]
	TIME [epoch: 8.54 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02921198793943499		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.02921198793943499 | validation: 0.0559687011245253]
	TIME [epoch: 8.54 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03556985059939594		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.03556985059939594 | validation: 0.0313794600492448]
	TIME [epoch: 8.55 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05261737685160535		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.05261737685160535 | validation: 0.04923250214292371]
	TIME [epoch: 8.57 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041652829196651926		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.041652829196651926 | validation: 0.02574743334011257]
	TIME [epoch: 8.55 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02666091633654101		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.02666091633654101 | validation: 0.02656515275159255]
	TIME [epoch: 8.55 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030630118890908834		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.030630118890908834 | validation: 0.029979009569138466]
	TIME [epoch: 8.54 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03387981075776231		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.03387981075776231 | validation: 0.04315825040555226]
	TIME [epoch: 8.57 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03481915188304563		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.03481915188304563 | validation: 0.031295536616066544]
	TIME [epoch: 8.56 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033438683774087465		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.033438683774087465 | validation: 0.0239828414798961]
	TIME [epoch: 8.55 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043159185051925		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.043159185051925 | validation: 0.025016989278928453]
	TIME [epoch: 8.55 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05036564111690038		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.05036564111690038 | validation: 0.1371900426985964]
	TIME [epoch: 8.56 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10014267933432394		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.10014267933432394 | validation: 0.043761220369994684]
	TIME [epoch: 8.55 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03655989153782328		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.03655989153782328 | validation: 0.028689604491254787]
	TIME [epoch: 8.56 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02938607342899734		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.02938607342899734 | validation: 0.023489255402782114]
	TIME [epoch: 8.56 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03884950054856535		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.03884950054856535 | validation: 0.035558308048118735]
	TIME [epoch: 8.56 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02786080322681645		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.02786080322681645 | validation: 0.01724363437511616]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030379569648175852		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.030379569648175852 | validation: 0.024560446337569576]
	TIME [epoch: 8.57 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024806527283788564		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.024806527283788564 | validation: 0.01709901059114488]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02426755992065417		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.02426755992065417 | validation: 0.03810992306092825]
	TIME [epoch: 8.55 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04598258996531574		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.04598258996531574 | validation: 0.06971244545328273]
	TIME [epoch: 8.57 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06292297650131196		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.06292297650131196 | validation: 0.024707220406344335]
	TIME [epoch: 8.57 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024583823268501122		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.024583823268501122 | validation: 0.018205434406688162]
	TIME [epoch: 8.57 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024526382648199826		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.024526382648199826 | validation: 0.025267691226576773]
	TIME [epoch: 8.57 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024582333367949433		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.024582333367949433 | validation: 0.45323845537711926]
	TIME [epoch: 8.59 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.318345050216733		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.318345050216733 | validation: 0.12314635629051413]
	TIME [epoch: 8.57 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07406081597645814		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.07406081597645814 | validation: 0.05953629802505746]
	TIME [epoch: 8.57 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045566920191448576		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.045566920191448576 | validation: 0.02664061303142288]
	TIME [epoch: 8.57 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024611226205714425		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.024611226205714425 | validation: 0.019918081442302834]
	TIME [epoch: 8.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023836166470884632		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.023836166470884632 | validation: 0.0195381538272187]
	TIME [epoch: 8.58 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022891416275483186		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.022891416275483186 | validation: 0.026691296086049324]
	TIME [epoch: 8.57 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02490877936070609		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.02490877936070609 | validation: 0.021175498049599582]
	TIME [epoch: 8.58 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02115523871471995		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.02115523871471995 | validation: 0.036330487898423594]
	TIME [epoch: 8.61 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06784753486324721		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.06784753486324721 | validation: 0.05826118152087878]
	TIME [epoch: 8.58 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03562776900468501		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.03562776900468501 | validation: 0.09342240811236661]
	TIME [epoch: 8.58 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040265356731672766		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.040265356731672766 | validation: 0.027401087346862314]
	TIME [epoch: 8.57 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02841647784475127		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.02841647784475127 | validation: 0.025691241871738914]
	TIME [epoch: 8.58 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021542269587345		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.021542269587345 | validation: 0.015531544580237054]
	TIME [epoch: 8.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027208371208122285		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.027208371208122285 | validation: 0.019309088147333984]
	TIME [epoch: 8.56 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02219524252237399		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.02219524252237399 | validation: 0.021808596891776152]
	TIME [epoch: 8.57 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028470055186337613		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.028470055186337613 | validation: 0.05347717391493347]
	TIME [epoch: 8.56 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03135078326241554		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.03135078326241554 | validation: 0.02302312352271285]
	TIME [epoch: 8.57 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038047777060717734		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.038047777060717734 | validation: 0.02974279310108549]
	TIME [epoch: 8.55 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029146887567100876		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.029146887567100876 | validation: 0.020071550545791128]
	TIME [epoch: 8.57 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035016055718567786		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.035016055718567786 | validation: 0.022817092079795584]
	TIME [epoch: 8.56 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025575948313448088		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.025575948313448088 | validation: 0.03819035949769342]
	TIME [epoch: 8.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02887418175322798		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.02887418175322798 | validation: 0.16566686980400266]
	TIME [epoch: 8.56 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08600860988624856		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.08600860988624856 | validation: 0.03230936768060022]
	TIME [epoch: 8.57 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021964607605529648		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.021964607605529648 | validation: 0.01516521253617999]
	TIME [epoch: 8.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_549.pth
	Model improved!!!
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019544508011638834		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.019544508011638834 | validation: 0.015216544741226499]
	TIME [epoch: 8.59 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04303878692324778		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.04303878692324778 | validation: 0.03999381658305685]
	TIME [epoch: 8.56 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03807022398190687		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.03807022398190687 | validation: 0.018786503676521273]
	TIME [epoch: 8.57 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023950500754750234		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.023950500754750234 | validation: 0.016697176337578112]
	TIME [epoch: 8.55 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031067557682810384		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.031067557682810384 | validation: 0.025542715367286466]
	TIME [epoch: 8.59 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02649518027395855		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.02649518027395855 | validation: 0.0767779739427902]
	TIME [epoch: 8.57 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044006255695248436		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.044006255695248436 | validation: 0.021099301545012276]
	TIME [epoch: 8.57 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017615104494472872		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.017615104494472872 | validation: 0.01637181900360483]
	TIME [epoch: 8.55 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03221756857881447		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.03221756857881447 | validation: 0.05105149624496411]
	TIME [epoch: 8.59 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0346875938912723		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.0346875938912723 | validation: 0.017146407115756258]
	TIME [epoch: 8.57 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020541351679481838		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.020541351679481838 | validation: 0.019117465679901417]
	TIME [epoch: 8.57 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01870674061861837		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.01870674061861837 | validation: 0.01354752144074312]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_561.pth
	Model improved!!!
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01787443998564955		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.01787443998564955 | validation: 0.016360952403956214]
	TIME [epoch: 8.57 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03499546834739522		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.03499546834739522 | validation: 0.015940719966231115]
	TIME [epoch: 8.56 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03670324455338779		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.03670324455338779 | validation: 0.04733855073394179]
	TIME [epoch: 8.56 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032025424155259774		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.032025424155259774 | validation: 0.016656270249879437]
	TIME [epoch: 8.56 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02057674669355737		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.02057674669355737 | validation: 0.03772301694254483]
	TIME [epoch: 8.82 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0277207744886282		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.0277207744886282 | validation: 0.024356597823265907]
	TIME [epoch: 8.57 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019541774553429005		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.019541774553429005 | validation: 0.017907699736610463]
	TIME [epoch: 8.54 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01818904890137211		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.01818904890137211 | validation: 0.01585400922413146]
	TIME [epoch: 8.54 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01854599034289345		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.01854599034289345 | validation: 0.026836087294620857]
	TIME [epoch: 8.55 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04278604959607047		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.04278604959607047 | validation: 0.0373982571494156]
	TIME [epoch: 8.57 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024323642413925303		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.024323642413925303 | validation: 0.020236242610185827]
	TIME [epoch: 8.56 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02728942111832301		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.02728942111832301 | validation: 0.040624832649746326]
	TIME [epoch: 8.55 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024480408649982636		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.024480408649982636 | validation: 0.016304232662735386]
	TIME [epoch: 8.56 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016946339474445387		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.016946339474445387 | validation: 0.015242008723514361]
	TIME [epoch: 8.58 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028043680288072283		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.028043680288072283 | validation: 0.050152228513527186]
	TIME [epoch: 8.57 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031233627337923172		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.031233627337923172 | validation: 0.017476908241056813]
	TIME [epoch: 8.55 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017820323861311393		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.017820323861311393 | validation: 0.03308785881604573]
	TIME [epoch: 8.56 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02517938644517503		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.02517938644517503 | validation: 0.022295196500911375]
	TIME [epoch: 8.56 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021714722211262534		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.021714722211262534 | validation: 0.021548792431610654]
	TIME [epoch: 8.57 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02062824656506601		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.02062824656506601 | validation: 0.026739502904478893]
	TIME [epoch: 8.56 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02636420156282719		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.02636420156282719 | validation: 0.017064112885654882]
	TIME [epoch: 8.57 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017211004562008317		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.017211004562008317 | validation: 0.01911863623065769]
	TIME [epoch: 8.57 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020519084075264076		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.020519084075264076 | validation: 0.03755801914617008]
	TIME [epoch: 8.58 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02938116656237659		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.02938116656237659 | validation: 0.01575671859788656]
	TIME [epoch: 8.58 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019267934545661024		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.019267934545661024 | validation: 0.01726293332938269]
	TIME [epoch: 8.57 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016788639370000413		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.016788639370000413 | validation: 0.023507012515961476]
	TIME [epoch: 8.57 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024901810753491706		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.024901810753491706 | validation: 0.024803143024584317]
	TIME [epoch: 8.58 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028245981617458067		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.028245981617458067 | validation: 0.027950462425789246]
	TIME [epoch: 8.56 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017197296915940283		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.017197296915940283 | validation: 0.014296065979452221]
	TIME [epoch: 8.57 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014945196588273268		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.014945196588273268 | validation: 0.04813698626132852]
	TIME [epoch: 8.56 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032158178083827954		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.032158178083827954 | validation: 0.023884225356667965]
	TIME [epoch: 8.58 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025112497935981786		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.025112497935981786 | validation: 0.07644363326391095]
	TIME [epoch: 8.57 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05498233368645699		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.05498233368645699 | validation: 0.024917076226865925]
	TIME [epoch: 8.57 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018178348105750176		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.018178348105750176 | validation: 0.012717343910851402]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016360309482866087		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.016360309482866087 | validation: 0.019269031455913993]
	TIME [epoch: 8.58 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01471252444068447		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.01471252444068447 | validation: 0.015828943986900525]
	TIME [epoch: 8.55 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02375621776324382		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.02375621776324382 | validation: 0.016045331727148074]
	TIME [epoch: 8.56 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018563767581601002		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.018563767581601002 | validation: 0.01512360860576397]
	TIME [epoch: 8.56 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016955640351150145		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.016955640351150145 | validation: 0.025472717795500692]
	TIME [epoch: 8.58 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01894879555968042		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.01894879555968042 | validation: 0.01936418626097751]
	TIME [epoch: 8.58 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021717872569113054		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.021717872569113054 | validation: 0.05521719514073342]
	TIME [epoch: 8.56 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04160117606389633		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.04160117606389633 | validation: 0.016091618947225796]
	TIME [epoch: 8.57 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01533832509417037		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.01533832509417037 | validation: 0.014238232542745498]
	TIME [epoch: 8.57 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014120261248136631		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.014120261248136631 | validation: 0.021137739701300267]
	TIME [epoch: 8.58 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015812426532497943		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.015812426532497943 | validation: 0.0241789375212004]
	TIME [epoch: 8.56 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020446366614740398		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.020446366614740398 | validation: 0.01490139494271565]
	TIME [epoch: 8.57 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022577445844233303		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.022577445844233303 | validation: 0.027753099092312244]
	TIME [epoch: 8.57 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02489309096366624		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.02489309096366624 | validation: 0.016841371771979446]
	TIME [epoch: 8.58 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0336290445992262		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.0336290445992262 | validation: 0.042951553206735105]
	TIME [epoch: 8.57 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024073591389511838		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.024073591389511838 | validation: 0.02004215041500253]
	TIME [epoch: 8.56 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014032459750006915		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.014032459750006915 | validation: 0.012057930859457257]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_612.pth
	Model improved!!!
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01664391532886815		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.01664391532886815 | validation: 0.017169827295673554]
	TIME [epoch: 8.58 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016301631129510226		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.016301631129510226 | validation: 0.015485599364591204]
	TIME [epoch: 8.57 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020398502484954225		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.020398502484954225 | validation: 0.022506403227419737]
	TIME [epoch: 8.57 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02723808575315793		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.02723808575315793 | validation: 0.012899955973542524]
	TIME [epoch: 8.57 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01730366061891804		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.01730366061891804 | validation: 0.07861674345225139]
	TIME [epoch: 8.58 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07162995805739411		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.07162995805739411 | validation: 0.03692125988578662]
	TIME [epoch: 8.57 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02638151724029027		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.02638151724029027 | validation: 0.01433338746040553]
	TIME [epoch: 8.57 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04746421734440143		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.04746421734440143 | validation: 0.045085475442863315]
	TIME [epoch: 8.56 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030914220971159537		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.030914220971159537 | validation: 0.022130741266949396]
	TIME [epoch: 8.58 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01664585374838312		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.01664585374838312 | validation: 0.014807627359669853]
	TIME [epoch: 8.57 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015754158811245186		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.015754158811245186 | validation: 0.018109458019885782]
	TIME [epoch: 8.57 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017648760562347703		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.017648760562347703 | validation: 0.013331859693436404]
	TIME [epoch: 8.58 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014004474317611018		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.014004474317611018 | validation: 0.01558782124280108]
	TIME [epoch: 8.57 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022180650452665228		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.022180650452665228 | validation: 0.043145653518958854]
	TIME [epoch: 8.58 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02906245526453309		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.02906245526453309 | validation: 0.01630530445152986]
	TIME [epoch: 8.57 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01485093326963715		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.01485093326963715 | validation: 0.017019043624446967]
	TIME [epoch: 8.58 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020154766660309527		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.020154766660309527 | validation: 0.02105102183575487]
	TIME [epoch: 8.57 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0241262743574258		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.0241262743574258 | validation: 0.023365881033331874]
	TIME [epoch: 8.58 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018208326654065892		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.018208326654065892 | validation: 0.031538014717021765]
	TIME [epoch: 8.58 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025316363301310416		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.025316363301310416 | validation: 0.01468207622670392]
	TIME [epoch: 8.57 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015125947903894328		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.015125947903894328 | validation: 0.01588360781211176]
	TIME [epoch: 8.56 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014631371186567219		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.014631371186567219 | validation: 0.010192850906298448]
	TIME [epoch: 8.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_634.pth
	Model improved!!!
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01899975429442695		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.01899975429442695 | validation: 0.02108302571780518]
	TIME [epoch: 8.56 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017052841011438724		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.017052841011438724 | validation: 0.013042547685469703]
	TIME [epoch: 8.55 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018393100393469804		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.018393100393469804 | validation: 0.017386451998154022]
	TIME [epoch: 8.55 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015191338517464445		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.015191338517464445 | validation: 0.013638802114125534]
	TIME [epoch: 8.58 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022703232751592903		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.022703232751592903 | validation: 0.017829701852901742]
	TIME [epoch: 8.56 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015117020447698158		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.015117020447698158 | validation: 0.014515856893736702]
	TIME [epoch: 8.56 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02110178513963582		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.02110178513963582 | validation: 0.018310220032647602]
	TIME [epoch: 8.55 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015967074912634438		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.015967074912634438 | validation: 0.013575429859186593]
	TIME [epoch: 8.57 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020481941142761764		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.020481941142761764 | validation: 0.03898451303862906]
	TIME [epoch: 8.56 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030285092393870873		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.030285092393870873 | validation: 0.020315131715110537]
	TIME [epoch: 8.56 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014012125235985943		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.014012125235985943 | validation: 0.012618090851992853]
	TIME [epoch: 8.57 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013459956010692449		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.013459956010692449 | validation: 0.016895471277589183]
	TIME [epoch: 8.56 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013137939647804088		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.013137939647804088 | validation: 0.011228587851721188]
	TIME [epoch: 8.58 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028528217217251843		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.028528217217251843 | validation: 0.016794211455661387]
	TIME [epoch: 8.56 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015559460238028698		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.015559460238028698 | validation: 0.018012744897530776]
	TIME [epoch: 8.56 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03118482346399392		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.03118482346399392 | validation: 0.017038399041259318]
	TIME [epoch: 8.56 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016909651295403404		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.016909651295403404 | validation: 0.017998213473695356]
	TIME [epoch: 8.58 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014525524337389507		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.014525524337389507 | validation: 0.013164293689894014]
	TIME [epoch: 8.56 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01224835241220905		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.01224835241220905 | validation: 0.046013758346416495]
	TIME [epoch: 8.55 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026063370879349478		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.026063370879349478 | validation: 0.0147296440969451]
	TIME [epoch: 8.56 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012165227713280928		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.012165227713280928 | validation: 0.029030758500114422]
	TIME [epoch: 8.57 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021669771375464626		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.021669771375464626 | validation: 0.01690655610723396]
	TIME [epoch: 8.57 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014180133483332654		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.014180133483332654 | validation: 0.010803272971719974]
	TIME [epoch: 8.56 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011613374183430246		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.011613374183430246 | validation: 0.011409027267423775]
	TIME [epoch: 8.55 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01630943943340582		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.01630943943340582 | validation: 0.022264623734058436]
	TIME [epoch: 8.56 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012868365516773634		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.012868365516773634 | validation: 0.012168326243250507]
	TIME [epoch: 8.58 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022311989032344513		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.022311989032344513 | validation: 0.01165424033323922]
	TIME [epoch: 8.56 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018407902893686		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.018407902893686 | validation: 0.012838017187437711]
	TIME [epoch: 8.57 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015472489409768676		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.015472489409768676 | validation: 0.018159355903360246]
	TIME [epoch: 8.57 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013164106719675741		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.013164106719675741 | validation: 0.012749151962841435]
	TIME [epoch: 8.57 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02491929023487857		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.02491929023487857 | validation: 0.021193409581346738]
	TIME [epoch: 8.56 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022231372936541163		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.022231372936541163 | validation: 0.021986920420797904]
	TIME [epoch: 8.57 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013843762978105415		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.013843762978105415 | validation: 0.010156723277708998]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_667.pth
	Model improved!!!
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014660842025318598		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.014660842025318598 | validation: 0.019122961179543006]
	TIME [epoch: 8.55 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014849847951241169		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.014849847951241169 | validation: 0.012852870705798354]
	TIME [epoch: 8.56 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012573886596449695		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.012573886596449695 | validation: 0.011940175652992073]
	TIME [epoch: 8.56 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01148422411642686		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.01148422411642686 | validation: 0.0127142057542773]
	TIME [epoch: 8.56 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014180972751869348		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.014180972751869348 | validation: 0.015275392606037805]
	TIME [epoch: 8.58 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016577287803545233		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.016577287803545233 | validation: 0.01918882076978632]
	TIME [epoch: 8.56 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013075943381236809		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.013075943381236809 | validation: 0.012236771719361016]
	TIME [epoch: 8.57 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022462489285829272		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.022462489285829272 | validation: 0.017251870756722812]
	TIME [epoch: 8.55 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013973059070642542		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.013973059070642542 | validation: 0.017335419298885768]
	TIME [epoch: 8.59 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010718038696440139		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.010718038696440139 | validation: 0.012233096733649777]
	TIME [epoch: 8.55 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011953111660096476		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.011953111660096476 | validation: 0.018052722685664078]
	TIME [epoch: 8.57 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0201861988480332		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.0201861988480332 | validation: 0.027719548058863043]
	TIME [epoch: 8.56 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016014932102133006		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.016014932102133006 | validation: 0.012388981771261728]
	TIME [epoch: 8.58 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012187737469984014		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.012187737469984014 | validation: 0.011252072944033499]
	TIME [epoch: 8.57 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011831121401603016		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.011831121401603016 | validation: 0.01154538827927537]
	TIME [epoch: 8.55 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014918999295290853		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.014918999295290853 | validation: 0.011307342774622652]
	TIME [epoch: 8.55 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011715903920024046		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.011715903920024046 | validation: 0.016197977380548796]
	TIME [epoch: 8.57 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013344154845114031		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.013344154845114031 | validation: 0.020758727145947897]
	TIME [epoch: 8.57 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01615185042943334		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.01615185042943334 | validation: 0.02623096999693983]
	TIME [epoch: 8.56 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017161277398311435		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.017161277398311435 | validation: 0.013199200842231938]
	TIME [epoch: 8.57 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015246636849435027		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.015246636849435027 | validation: 0.021113072543556138]
	TIME [epoch: 8.57 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015435761691630088		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.015435761691630088 | validation: 0.014224575406201326]
	TIME [epoch: 8.57 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01179672760613827		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.01179672760613827 | validation: 0.011454732257989823]
	TIME [epoch: 8.56 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011309785202715783		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.011309785202715783 | validation: 0.01506500423284392]
	TIME [epoch: 8.57 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01989179519782001		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.01989179519782001 | validation: 0.013568915992779299]
	TIME [epoch: 8.56 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0121660186365685		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.0121660186365685 | validation: 0.01271179604424063]
	TIME [epoch: 8.6 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010271416078714304		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.010271416078714304 | validation: 0.010436514335735881]
	TIME [epoch: 8.57 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012639202967001096		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.012639202967001096 | validation: 0.00999124071942636]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_695.pth
	Model improved!!!
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009945047681286508		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.009945047681286508 | validation: 0.012666839134904058]
	TIME [epoch: 8.57 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0147396801117296		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.0147396801117296 | validation: 0.012681478093601994]
	TIME [epoch: 8.58 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011658250959162751		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.011658250959162751 | validation: 0.009511991063229249]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_698.pth
	Model improved!!!
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01242119175871128		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.01242119175871128 | validation: 0.012816396783115374]
	TIME [epoch: 8.55 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013070441744004152		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.013070441744004152 | validation: 0.012840358676528375]
	TIME [epoch: 8.56 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013609114920241538		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.013609114920241538 | validation: 0.02368106532439988]
	TIME [epoch: 8.58 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015666568296948246		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.015666568296948246 | validation: 0.013066887041529602]
	TIME [epoch: 8.56 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011729442983942275		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.011729442983942275 | validation: 0.010600020438025801]
	TIME [epoch: 8.57 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012462073046469724		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.012462073046469724 | validation: 0.025132728471619757]
	TIME [epoch: 8.58 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013692479907480744		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.013692479907480744 | validation: 0.014072133667742697]
	TIME [epoch: 8.57 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011245817519287472		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.011245817519287472 | validation: 0.010759916806086685]
	TIME [epoch: 8.57 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010038407175839035		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.010038407175839035 | validation: 0.009839770131308226]
	TIME [epoch: 8.58 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014422324396122533		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.014422324396122533 | validation: 0.011705203363638065]
	TIME [epoch: 8.57 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013734204327667546		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.013734204327667546 | validation: 0.015514814543586775]
	TIME [epoch: 8.58 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012619368929037532		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.012619368929037532 | validation: 0.01109525772558482]
	TIME [epoch: 8.58 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012966671456409484		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.012966671456409484 | validation: 0.019190345231722923]
	TIME [epoch: 8.57 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027448325588497732		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.027448325588497732 | validation: 0.019618804971577072]
	TIME [epoch: 8.56 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012838631028471634		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.012838631028471634 | validation: 0.01232647993191996]
	TIME [epoch: 8.58 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009035728111207045		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.009035728111207045 | validation: 0.011635175908431463]
	TIME [epoch: 8.57 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011018270517994005		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.011018270517994005 | validation: 0.016974309012260245]
	TIME [epoch: 8.57 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011481120594171869		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.011481120594171869 | validation: 0.008981651924624192]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_716.pth
	Model improved!!!
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009826454422757657		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.009826454422757657 | validation: 0.011604800093964782]
	TIME [epoch: 8.56 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010466287386733523		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.010466287386733523 | validation: 0.014064141478509037]
	TIME [epoch: 8.57 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011113443355721103		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.011113443355721103 | validation: 0.01076186372166585]
	TIME [epoch: 8.56 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013004564483893134		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.013004564483893134 | validation: 0.017068273277820924]
	TIME [epoch: 8.54 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01664265724728888		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.01664265724728888 | validation: 0.016664968904363652]
	TIME [epoch: 8.56 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011402822000934669		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.011402822000934669 | validation: 0.013525296278836762]
	TIME [epoch: 8.57 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01150970361415266		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.01150970361415266 | validation: 0.011140426235310037]
	TIME [epoch: 8.56 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010634559948799475		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.010634559948799475 | validation: 0.014277781104652838]
	TIME [epoch: 8.55 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010439891839890416		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.010439891839890416 | validation: 0.057747619407222645]
	TIME [epoch: 8.56 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03693620345341732		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.03693620345341732 | validation: 0.014233531174392586]
	TIME [epoch: 8.57 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016738358841561964		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.016738358841561964 | validation: 0.013280849565870351]
	TIME [epoch: 8.58 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012423810839700563		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.012423810839700563 | validation: 0.01166329654211093]
	TIME [epoch: 8.57 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010058419000913386		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.010058419000913386 | validation: 0.01110242732808266]
	TIME [epoch: 8.56 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008744523372999989		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.008744523372999989 | validation: 0.009643478717426249]
	TIME [epoch: 8.56 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010735946333893565		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.010735946333893565 | validation: 0.015917474177244883]
	TIME [epoch: 8.57 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01752370363124517		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.01752370363124517 | validation: 0.031902981290891565]
	TIME [epoch: 8.58 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016059768523975136		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.016059768523975136 | validation: 0.016034550139466166]
	TIME [epoch: 8.56 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013547365560308782		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.013547365560308782 | validation: 0.009105791754586014]
	TIME [epoch: 8.6 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009904823402612815		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.009904823402612815 | validation: 0.010028781674717775]
	TIME [epoch: 8.58 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00945114133066859		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.00945114133066859 | validation: 0.010001908654285819]
	TIME [epoch: 8.57 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010102752247843896		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.010102752247843896 | validation: 0.01053315664885537]
	TIME [epoch: 8.57 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011351417482353566		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.011351417482353566 | validation: 0.014630957165140374]
	TIME [epoch: 8.57 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012385197176810227		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.012385197176810227 | validation: 0.01596597608371743]
	TIME [epoch: 8.58 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010406014749172144		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.010406014749172144 | validation: 0.009898499919728992]
	TIME [epoch: 8.56 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01082381638354719		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.01082381638354719 | validation: 0.015142140579258227]
	TIME [epoch: 8.57 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013627879049220262		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.013627879049220262 | validation: 0.012244946773053739]
	TIME [epoch: 8.57 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00927602508599091		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.00927602508599091 | validation: 0.009647429310259454]
	TIME [epoch: 8.58 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011233351811064982		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.011233351811064982 | validation: 0.01596385393517161]
	TIME [epoch: 8.57 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01169668374353192		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.01169668374353192 | validation: 0.00978283557270588]
	TIME [epoch: 8.57 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016795859714903473		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.016795859714903473 | validation: 0.018037142255036498]
	TIME [epoch: 8.57 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015910125388382355		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.015910125388382355 | validation: 0.009049859527510024]
	TIME [epoch: 8.58 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008307023467789257		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.008307023467789257 | validation: 0.00865844807380422]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008918986560149274		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.008918986560149274 | validation: 0.008604163241440215]
	TIME [epoch: 8.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009893854318714358		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.009893854318714358 | validation: 0.018616240883628343]
	TIME [epoch: 8.56 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01024241285558839		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.01024241285558839 | validation: 0.009425387464559166]
	TIME [epoch: 8.57 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009006599531531575		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.009006599531531575 | validation: 0.009796044628467778]
	TIME [epoch: 8.56 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0122806774193129		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.0122806774193129 | validation: 0.011858509843418862]
	TIME [epoch: 8.56 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009681473301321741		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.009681473301321741 | validation: 0.010269868100922834]
	TIME [epoch: 8.56 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010136125544220713		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.010136125544220713 | validation: 0.013321884812503387]
	TIME [epoch: 8.57 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011340727704144928		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.011340727704144928 | validation: 0.023505804389435787]
	TIME [epoch: 8.57 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015383474762761359		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.015383474762761359 | validation: 0.010933893496949393]
	TIME [epoch: 8.56 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008443399326652097		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.008443399326652097 | validation: 0.010643615069324975]
	TIME [epoch: 8.57 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009100431109642308		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.009100431109642308 | validation: 0.013283367937151758]
	TIME [epoch: 8.57 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010357149149385287		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.010357149149385287 | validation: 0.009996359483511463]
	TIME [epoch: 8.59 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013528865441921533		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.013528865441921533 | validation: 0.008673694522251266]
	TIME [epoch: 8.56 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0080681605259265		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.0080681605259265 | validation: 0.010089726366961815]
	TIME [epoch: 8.57 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008631422085422669		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.008631422085422669 | validation: 0.00855929912292891]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_763.pth
	Model improved!!!
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008688169259833994		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.008688169259833994 | validation: 0.016205384825771905]
	TIME [epoch: 8.59 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014796625275663588		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.014796625275663588 | validation: 0.015621334667956874]
	TIME [epoch: 8.56 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012164691171910458		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.012164691171910458 | validation: 0.008918426774684268]
	TIME [epoch: 8.57 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008659574709141256		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.008659574709141256 | validation: 0.009327869979758083]
	TIME [epoch: 8.57 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00981800101621217		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.00981800101621217 | validation: 0.010035576482211676]
	TIME [epoch: 8.58 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008154180653618354		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.008154180653618354 | validation: 0.00794570089722979]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_769.pth
	Model improved!!!
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007378165528013795		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.007378165528013795 | validation: 0.011396537762946236]
	TIME [epoch: 8.56 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013012484587812288		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.013012484587812288 | validation: 0.011007321532561651]
	TIME [epoch: 8.56 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009765385524859267		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.009765385524859267 | validation: 0.009056259073479955]
	TIME [epoch: 8.59 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010365340154679993		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.010365340154679993 | validation: 0.013565017057903583]
	TIME [epoch: 8.57 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010403130505868679		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.010403130505868679 | validation: 0.010851891211224575]
	TIME [epoch: 8.56 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008621513858426086		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.008621513858426086 | validation: 0.008377810349929297]
	TIME [epoch: 8.56 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008220491903396418		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.008220491903396418 | validation: 0.010282098990439047]
	TIME [epoch: 8.58 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010740615496057114		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.010740615496057114 | validation: 0.016004021104633788]
	TIME [epoch: 8.58 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009904101275354278		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.009904101275354278 | validation: 0.009004577996732741]
	TIME [epoch: 8.57 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011321000357676225		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.011321000357676225 | validation: 0.012927546425278652]
	TIME [epoch: 8.58 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010472175180462884		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.010472175180462884 | validation: 0.009833167742413251]
	TIME [epoch: 8.59 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009248177810798152		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.009248177810798152 | validation: 0.010965078156320774]
	TIME [epoch: 8.58 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009562086291063159		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.009562086291063159 | validation: 0.013301842242333006]
	TIME [epoch: 8.58 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00919982001359407		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.00919982001359407 | validation: 0.009665331627377285]
	TIME [epoch: 8.56 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01008206984538746		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.01008206984538746 | validation: 0.009858865424121326]
	TIME [epoch: 8.57 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014169838740419025		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.014169838740419025 | validation: 0.009458097502366815]
	TIME [epoch: 8.59 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007963613689901314		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.007963613689901314 | validation: 0.010087197797863125]
	TIME [epoch: 8.56 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008959236043366353		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.008959236043366353 | validation: 0.009267912467499872]
	TIME [epoch: 8.57 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00870429171395794		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.00870429171395794 | validation: 0.007603762509899269]
	TIME [epoch: 8.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_788.pth
	Model improved!!!
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008390977573353359		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.008390977573353359 | validation: 0.011449107307327098]
	TIME [epoch: 8.59 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010835270421043864		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.010835270421043864 | validation: 0.010751787411396381]
	TIME [epoch: 8.58 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01008035612422243		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.01008035612422243 | validation: 0.008613398174689597]
	TIME [epoch: 8.56 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007862888372882989		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.007862888372882989 | validation: 0.007932476230911183]
	TIME [epoch: 8.57 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010185275790399509		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.010185275790399509 | validation: 0.011454421043221873]
	TIME [epoch: 8.58 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008758742872961158		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.008758742872961158 | validation: 0.00978621418970534]
	TIME [epoch: 8.59 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007909974314749688		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.007909974314749688 | validation: 0.009550055342476483]
	TIME [epoch: 8.58 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010945630164090087		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.010945630164090087 | validation: 0.028252221870004215]
	TIME [epoch: 8.58 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016740980900887603		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.016740980900887603 | validation: 0.010317385720048134]
	TIME [epoch: 8.59 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008923450268705424		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.008923450268705424 | validation: 0.010427842548483968]
	TIME [epoch: 8.59 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007999493166846087		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.007999493166846087 | validation: 0.009186900000023831]
	TIME [epoch: 8.59 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00724664881808552		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.00724664881808552 | validation: 0.008249576370007328]
	TIME [epoch: 8.58 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013453691367771706		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.013453691367771706 | validation: 0.01119351699019698]
	TIME [epoch: 8.56 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010026432631962382		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.010026432631962382 | validation: 0.00966816224311427]
	TIME [epoch: 8.56 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008202001072717935		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.008202001072717935 | validation: 0.012509498968237148]
	TIME [epoch: 8.56 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009706609884546386		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.009706609884546386 | validation: 0.010800747484091806]
	TIME [epoch: 8.57 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007609733844578248		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.007609733844578248 | validation: 0.008851698300287342]
	TIME [epoch: 8.56 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011320038463205184		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.011320038463205184 | validation: 0.009736269242955672]
	TIME [epoch: 8.59 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009973404779193892		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.009973404779193892 | validation: 0.009596795693040616]
	TIME [epoch: 8.57 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008504703746079546		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.008504703746079546 | validation: 0.007802862145860914]
	TIME [epoch: 8.57 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01121864993862502		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.01121864993862502 | validation: 0.00834515860377805]
	TIME [epoch: 8.56 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007562856925759708		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.007562856925759708 | validation: 0.008355263186077053]
	TIME [epoch: 8.58 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0070938332383815965		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.0070938332383815965 | validation: 0.008624055467549307]
	TIME [epoch: 8.56 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007453647688303868		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.007453647688303868 | validation: 0.01328194152977843]
	TIME [epoch: 8.56 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008295951598146273		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.008295951598146273 | validation: 0.007808414412046811]
	TIME [epoch: 8.57 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008278858602309189		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.008278858602309189 | validation: 0.013699626989746711]
	TIME [epoch: 8.57 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01396594909043029		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.01396594909043029 | validation: 0.011553974917042138]
	TIME [epoch: 8.57 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011606957712830226		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.011606957712830226 | validation: 0.012273987323834262]
	TIME [epoch: 8.56 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009113494532163424		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.009113494532163424 | validation: 0.014860971366857722]
	TIME [epoch: 8.58 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010343293659948433		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.010343293659948433 | validation: 0.008637864638527955]
	TIME [epoch: 8.57 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006952787596111959		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.006952787596111959 | validation: 0.008552071246636095]
	TIME [epoch: 8.57 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008214352409689383		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.008214352409689383 | validation: 0.009247746226208758]
	TIME [epoch: 8.59 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007473449393107244		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.007473449393107244 | validation: 0.008458117994120553]
	TIME [epoch: 8.57 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007017325580201274		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.007017325580201274 | validation: 0.008238404833190733]
	TIME [epoch: 8.58 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00844446305526137		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.00844446305526137 | validation: 0.010385481398885914]
	TIME [epoch: 8.58 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008558700497644966		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.008558700497644966 | validation: 0.025626592989391142]
	TIME [epoch: 8.57 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013881510596198451		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.013881510596198451 | validation: 0.009834605286960453]
	TIME [epoch: 8.57 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0068486467216088845		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.0068486467216088845 | validation: 0.011327382214407091]
	TIME [epoch: 8.56 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007144444853214396		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.007144444853214396 | validation: 0.009073602568756597]
	TIME [epoch: 8.59 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00874729759256375		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.00874729759256375 | validation: 0.015141318605968086]
	TIME [epoch: 8.58 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008735192474754124		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.008735192474754124 | validation: 0.007825507271592962]
	TIME [epoch: 8.58 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008079174076996178		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.008079174076996178 | validation: 0.009765245285825263]
	TIME [epoch: 8.56 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007972736208768608		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.007972736208768608 | validation: 0.010221309726003961]
	TIME [epoch: 8.59 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008038761012318846		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.008038761012318846 | validation: 0.010704700861244424]
	TIME [epoch: 8.58 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00758152730767645		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.00758152730767645 | validation: 0.04191469226303446]
	TIME [epoch: 8.58 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030090465622384002		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.030090465622384002 | validation: 0.01290942625355405]
	TIME [epoch: 8.58 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008758776619211655		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.008758776619211655 | validation: 0.008243190329691693]
	TIME [epoch: 8.58 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00648930397116867		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.00648930397116867 | validation: 0.006914535747057824]
	TIME [epoch: 8.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_836.pth
	Model improved!!!
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006703190457979706		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.006703190457979706 | validation: 0.006429228994811789]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_837.pth
	Model improved!!!
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0070749860345488335		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.0070749860345488335 | validation: 0.008178622464789205]
	TIME [epoch: 8.55 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0064945033088174885		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.0064945033088174885 | validation: 0.008814236779865326]
	TIME [epoch: 8.58 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006816055865622188		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.006816055865622188 | validation: 0.012703710603433376]
	TIME [epoch: 8.57 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007965301425372594		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.007965301425372594 | validation: 0.007323197554686071]
	TIME [epoch: 8.57 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007914327286217795		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.007914327286217795 | validation: 0.009696671587776477]
	TIME [epoch: 8.58 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009576486449186511		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.009576486449186511 | validation: 0.007144276709454644]
	TIME [epoch: 8.58 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007726401489283056		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.007726401489283056 | validation: 0.009107887550391353]
	TIME [epoch: 8.57 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006781357085334343		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.006781357085334343 | validation: 0.012041301922173134]
	TIME [epoch: 8.56 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007769685036410576		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.007769685036410576 | validation: 0.007345948376919352]
	TIME [epoch: 8.55 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007591904639933606		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.007591904639933606 | validation: 0.00705358379102971]
	TIME [epoch: 8.56 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008066899639534325		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.008066899639534325 | validation: 0.006704773738399694]
	TIME [epoch: 8.57 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006169542890117853		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.006169542890117853 | validation: 0.008553614091197684]
	TIME [epoch: 8.54 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006980120099455979		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.006980120099455979 | validation: 0.008473502484380887]
	TIME [epoch: 8.54 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007762755397182357		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.007762755397182357 | validation: 0.007687268109784873]
	TIME [epoch: 8.54 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006953308051129205		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.006953308051129205 | validation: 0.008771836223960248]
	TIME [epoch: 8.56 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00831882773677645		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.00831882773677645 | validation: 0.009768437706637031]
	TIME [epoch: 8.55 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007753165745618497		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.007753165745618497 | validation: 0.010628864128605402]
	TIME [epoch: 8.55 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006169955371697309		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.006169955371697309 | validation: 0.010087960424473129]
	TIME [epoch: 8.54 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008885749924196181		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.008885749924196181 | validation: 0.011558932127574367]
	TIME [epoch: 8.55 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007846939517921035		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.007846939517921035 | validation: 0.006954133048864375]
	TIME [epoch: 8.55 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0063526086325774375		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.0063526086325774375 | validation: 0.008272985909428286]
	TIME [epoch: 8.54 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00695240815398953		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.00695240815398953 | validation: 0.007489749645926167]
	TIME [epoch: 8.55 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006638808809503259		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.006638808809503259 | validation: 0.00920570073475769]
	TIME [epoch: 8.56 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010381266253337395		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.010381266253337395 | validation: 0.012507156268451877]
	TIME [epoch: 8.54 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009245350914952679		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.009245350914952679 | validation: 0.008499685909975924]
	TIME [epoch: 8.56 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006366389528036676		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.006366389528036676 | validation: 0.01645435385377356]
	TIME [epoch: 8.55 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013892832499926632		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.013892832499926632 | validation: 0.01207412082798376]
	TIME [epoch: 8.54 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008899238098402283		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.008899238098402283 | validation: 0.00729576064646971]
	TIME [epoch: 8.54 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00602854864251324		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.00602854864251324 | validation: 0.00825820400401137]
	TIME [epoch: 8.56 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006173416651890089		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.006173416651890089 | validation: 0.007713883624179966]
	TIME [epoch: 8.55 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0069634137838917		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.0069634137838917 | validation: 0.01253852720003423]
	TIME [epoch: 8.53 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007387119249064775		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.007387119249064775 | validation: 0.0080973485659898]
	TIME [epoch: 8.56 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0069104014002380545		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.0069104014002380545 | validation: 0.013219885767695715]
	TIME [epoch: 8.54 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008089400641912249		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.008089400641912249 | validation: 0.009779453567058328]
	TIME [epoch: 8.55 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027537190728427384		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.027537190728427384 | validation: 0.0120849627978077]
	TIME [epoch: 8.54 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00912197480412625		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.00912197480412625 | validation: 0.007901219118780237]
	TIME [epoch: 8.55 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005971514351548464		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.005971514351548464 | validation: 0.0069104288915452245]
	TIME [epoch: 8.55 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005852882358516133		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.005852882358516133 | validation: 0.006536875410923023]
	TIME [epoch: 8.54 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0062941418918485825		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.0062941418918485825 | validation: 0.007285404079967396]
	TIME [epoch: 8.53 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006277841923005842		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.006277841923005842 | validation: 0.008092620377950275]
	TIME [epoch: 8.55 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005678967438519392		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.005678967438519392 | validation: 0.012312405381098918]
	TIME [epoch: 8.55 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00712430266192027		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.00712430266192027 | validation: 0.00773566375358206]
	TIME [epoch: 8.53 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00587364120341353		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.00587364120341353 | validation: 0.007167882614731315]
	TIME [epoch: 8.54 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010629519882598623		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.010629519882598623 | validation: 0.013893639805044062]
	TIME [epoch: 8.54 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008293968430654334		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.008293968430654334 | validation: 0.00714906065420155]
	TIME [epoch: 8.54 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006123467077809114		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.006123467077809114 | validation: 0.007046645675454992]
	TIME [epoch: 8.54 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006742138506097629		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.006742138506097629 | validation: 0.008044459640836079]
	TIME [epoch: 8.54 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072035522267674046		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.0072035522267674046 | validation: 0.007096913436735164]
	TIME [epoch: 8.54 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005776825100878584		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.005776825100878584 | validation: 0.007251245481425829]
	TIME [epoch: 8.55 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006418860626042055		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.006418860626042055 | validation: 0.009115383056032787]
	TIME [epoch: 8.53 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009544709611618081		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.009544709611618081 | validation: 0.01296184633799043]
	TIME [epoch: 8.54 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007784561600488299		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.007784561600488299 | validation: 0.012087093006672902]
	TIME [epoch: 8.54 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007699170514492957		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.007699170514492957 | validation: 0.009358291912035212]
	TIME [epoch: 8.55 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011791932828198858		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.011791932828198858 | validation: 0.018511199540073093]
	TIME [epoch: 8.55 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008444980375322628		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.008444980375322628 | validation: 0.007264052889586061]
	TIME [epoch: 8.54 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0058491064271007365		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.0058491064271007365 | validation: 0.009308338165384913]
	TIME [epoch: 8.54 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006036605291096004		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.006036605291096004 | validation: 0.006869552089252615]
	TIME [epoch: 8.55 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005547573896574575		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.005547573896574575 | validation: 0.009854204961643149]
	TIME [epoch: 8.53 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0063170103955982925		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.0063170103955982925 | validation: 0.006906871814702094]
	TIME [epoch: 8.55 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006397529723593752		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.006397529723593752 | validation: 0.007394957299842296]
	TIME [epoch: 8.54 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006120900806797293		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.006120900806797293 | validation: 0.007429769309027922]
	TIME [epoch: 8.55 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007427284976392329		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.007427284976392329 | validation: 0.009705624647631892]
	TIME [epoch: 8.53 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006518816403279951		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.006518816403279951 | validation: 0.0127596580766263]
	TIME [epoch: 8.54 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007445455274330874		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.007445455274330874 | validation: 0.024963148980372246]
	TIME [epoch: 8.56 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015743875301361922		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.015743875301361922 | validation: 0.007815159714600665]
	TIME [epoch: 8.54 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006583177014607447		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.006583177014607447 | validation: 0.0063472744751404495]
	TIME [epoch: 8.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_903.pth
	Model improved!!!
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005783090431773581		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.005783090431773581 | validation: 0.0066176298915088146]
	TIME [epoch: 8.54 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0063448766079641575		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.0063448766079641575 | validation: 0.009696241320160915]
	TIME [epoch: 8.54 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006448662927172389		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.006448662927172389 | validation: 0.0061987452263583456]
	TIME [epoch: 8.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_906.pth
	Model improved!!!
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005799551475463595		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.005799551475463595 | validation: 0.007646269702400803]
	TIME [epoch: 8.56 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005934884385655989		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.005934884385655989 | validation: 0.008435098962288097]
	TIME [epoch: 8.54 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006175474114761175		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.006175474114761175 | validation: 0.007605847971327514]
	TIME [epoch: 8.54 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005859255242738179		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.005859255242738179 | validation: 0.009636513051110242]
	TIME [epoch: 8.54 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007885107783898984		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.007885107783898984 | validation: 0.010137918116609416]
	TIME [epoch: 8.56 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007081099690593366		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.007081099690593366 | validation: 0.005903088588617425]
	TIME [epoch: 8.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_912.pth
	Model improved!!!
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005998583370158663		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.005998583370158663 | validation: 0.007562796164910101]
	TIME [epoch: 8.54 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006067804602182064		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.006067804602182064 | validation: 0.007162814239813355]
	TIME [epoch: 8.55 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005841750670904548		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.005841750670904548 | validation: 0.0071331505484044395]
	TIME [epoch: 8.55 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005770949743143904		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.005770949743143904 | validation: 0.012855469167130571]
	TIME [epoch: 8.55 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008102934827735072		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.008102934827735072 | validation: 0.0076497585715744835]
	TIME [epoch: 8.55 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005842822603511165		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.005842822603511165 | validation: 0.018729400220275458]
	TIME [epoch: 8.54 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009757357365367023		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.009757357365367023 | validation: 0.008538564310234156]
	TIME [epoch: 8.55 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007129174903002466		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.007129174903002466 | validation: 0.006903572361444842]
	TIME [epoch: 8.55 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005464087259108143		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.005464087259108143 | validation: 0.007089190352407704]
	TIME [epoch: 8.55 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076379993690293595		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.0076379993690293595 | validation: 0.007123479273609699]
	TIME [epoch: 8.53 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005538206921028688		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.005538206921028688 | validation: 0.0073554192941208355]
	TIME [epoch: 8.56 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005467467088744339		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.005467467088744339 | validation: 0.00695871499430833]
	TIME [epoch: 8.54 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006341953525204341		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.006341953525204341 | validation: 0.006322347507058276]
	TIME [epoch: 8.55 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00549767003036037		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.00549767003036037 | validation: 0.007949896235400846]
	TIME [epoch: 8.55 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00794916073238228		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.00794916073238228 | validation: 0.007402597958406204]
	TIME [epoch: 8.55 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005480756449525886		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.005480756449525886 | validation: 0.006333688851515732]
	TIME [epoch: 8.56 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005545524298153265		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.005545524298153265 | validation: 0.006310871967682293]
	TIME [epoch: 8.55 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005925045640258601		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.005925045640258601 | validation: 0.006415774079768699]
	TIME [epoch: 8.55 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005842943606463537		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.005842943606463537 | validation: 0.006270387713070364]
	TIME [epoch: 8.54 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006927119302464408		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.006927119302464408 | validation: 0.0059799582039341236]
	TIME [epoch: 8.56 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00525781147019308		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.00525781147019308 | validation: 0.006912163660749049]
	TIME [epoch: 8.55 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009177702024203743		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.009177702024203743 | validation: 0.008679662311615528]
	TIME [epoch: 8.55 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006464195984194239		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.006464195984194239 | validation: 0.0061707926750034555]
	TIME [epoch: 8.56 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005190225077487816		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.005190225077487816 | validation: 0.007472435452170223]
	TIME [epoch: 8.56 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006410347735102665		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.006410347735102665 | validation: 0.009895297864696666]
	TIME [epoch: 8.56 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00730155372487214		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.00730155372487214 | validation: 0.0063484696198391755]
	TIME [epoch: 8.55 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00643630369145499		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.00643630369145499 | validation: 0.008031614350377773]
	TIME [epoch: 8.56 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022333513379041946		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.022333513379041946 | validation: 0.015161990179676541]
	TIME [epoch: 8.54 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00788930510432706		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.00788930510432706 | validation: 0.006769756394861869]
	TIME [epoch: 8.55 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005357620892167637		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.005357620892167637 | validation: 0.009144133412421037]
	TIME [epoch: 8.55 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004918154735381434		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.004918154735381434 | validation: 0.007968094087451126]
	TIME [epoch: 8.55 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00511326946453654		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.00511326946453654 | validation: 0.005971212721871949]
	TIME [epoch: 8.56 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005505811960615593		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.005505811960615593 | validation: 0.0057336791201181535]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_945.pth
	Model improved!!!
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005250549271369796		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.005250549271369796 | validation: 0.006338699828731743]
	TIME [epoch: 8.55 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005457506625621715		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.005457506625621715 | validation: 0.007922532395878338]
	TIME [epoch: 8.54 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005397045557985191		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.005397045557985191 | validation: 0.005766380813232729]
	TIME [epoch: 8.55 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005726909206218676		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.005726909206218676 | validation: 0.007194705918371284]
	TIME [epoch: 8.56 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006658576105867625		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.006658576105867625 | validation: 0.005902952470006453]
	TIME [epoch: 8.53 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005752899115214708		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.005752899115214708 | validation: 0.009741481562497163]
	TIME [epoch: 8.55 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005857867898387367		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.005857867898387367 | validation: 0.006734254826058815]
	TIME [epoch: 8.54 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005587257912590678		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.005587257912590678 | validation: 0.007004501068169364]
	TIME [epoch: 8.55 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009308476132902083		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.009308476132902083 | validation: 0.006904135596615573]
	TIME [epoch: 8.55 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057232227903575746		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.0057232227903575746 | validation: 0.005588980152814851]
	TIME [epoch: 8.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_955.pth
	Model improved!!!
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005360758508388422		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.005360758508388422 | validation: 0.005382288349481662]
	TIME [epoch: 8.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_956.pth
	Model improved!!!
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005693768614032126		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.005693768614032126 | validation: 0.006296681570468934]
	TIME [epoch: 8.57 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006638003623979847		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.006638003623979847 | validation: 0.00760399616797406]
	TIME [epoch: 8.54 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005995301048743372		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.005995301048743372 | validation: 0.006855929086619629]
	TIME [epoch: 8.55 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004786841271275674		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.004786841271275674 | validation: 0.006454562186381463]
	TIME [epoch: 8.54 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005306368814313096		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.005306368814313096 | validation: 0.006374666576286504]
	TIME [epoch: 8.55 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004734746542973255		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.004734746542973255 | validation: 0.0055604813259548065]
	TIME [epoch: 8.55 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005699249511131201		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.005699249511131201 | validation: 0.008797260726383264]
	TIME [epoch: 8.54 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006008378347852289		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.006008378347852289 | validation: 0.0073597875036354905]
	TIME [epoch: 8.54 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006522280339530854		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.006522280339530854 | validation: 0.007112674727235716]
	TIME [epoch: 8.55 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005388522691260989		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.005388522691260989 | validation: 0.0062408778384380175]
	TIME [epoch: 8.54 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005431549919381724		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.005431549919381724 | validation: 0.009670785000399433]
	TIME [epoch: 8.55 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005986079907672629		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.005986079907672629 | validation: 0.007968619264001713]
	TIME [epoch: 8.55 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006070906768950347		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.006070906768950347 | validation: 0.005414681099797723]
	TIME [epoch: 8.56 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005539995727196002		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.005539995727196002 | validation: 0.006414619508889498]
	TIME [epoch: 8.56 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050979153006667		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.0050979153006667 | validation: 0.00866468335513227]
	TIME [epoch: 8.55 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076972896261989325		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.0076972896261989325 | validation: 0.008207962530544411]
	TIME [epoch: 8.55 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005676058228540525		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.005676058228540525 | validation: 0.0063383851972603445]
	TIME [epoch: 8.55 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004882383801429042		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.004882383801429042 | validation: 0.006599729051547405]
	TIME [epoch: 8.57 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056321262202475315		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.0056321262202475315 | validation: 0.0054822155041515394]
	TIME [epoch: 8.55 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005587213842080068		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.005587213842080068 | validation: 0.0058783082712705916]
	TIME [epoch: 8.55 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048500908271750295		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.0048500908271750295 | validation: 0.00556082889476595]
	TIME [epoch: 8.56 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005650681151371844		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.005650681151371844 | validation: 0.005985498870860214]
	TIME [epoch: 8.56 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051916824158733344		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.0051916824158733344 | validation: 0.005954603079601308]
	TIME [epoch: 8.56 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005005195759820718		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.005005195759820718 | validation: 0.006656436636387098]
	TIME [epoch: 8.55 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005815638257441422		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.005815638257441422 | validation: 0.005850008466645781]
	TIME [epoch: 8.55 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005397033969618697		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.005397033969618697 | validation: 0.005175790616037916]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_982.pth
	Model improved!!!
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005198556275881489		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.005198556275881489 | validation: 0.005698345051641655]
	TIME [epoch: 8.55 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006036664562276659		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.006036664562276659 | validation: 0.007128244204060833]
	TIME [epoch: 8.55 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006126231027437088		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.006126231027437088 | validation: 0.005520060484868175]
	TIME [epoch: 8.56 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005234689796286402		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.005234689796286402 | validation: 0.005931636696180663]
	TIME [epoch: 8.57 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005082749769915453		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.005082749769915453 | validation: 0.009526904038950335]
	TIME [epoch: 8.55 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007537580735831766		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.007537580735831766 | validation: 0.0045602568543446995]
	TIME [epoch: 8.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_988.pth
	Model improved!!!
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004869320375041168		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.004869320375041168 | validation: 0.004984988598062535]
	TIME [epoch: 8.54 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004733181475551263		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.004733181475551263 | validation: 0.005793234958746955]
	TIME [epoch: 8.55 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005212309747379445		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.005212309747379445 | validation: 0.0068334217344267634]
	TIME [epoch: 8.56 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005210681085535153		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.005210681085535153 | validation: 0.0060934397432153625]
	TIME [epoch: 8.54 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006504011408038682		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.006504011408038682 | validation: 0.009299371247815885]
	TIME [epoch: 8.53 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00464806277574603		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.00464806277574603 | validation: 0.0075466731940410925]
	TIME [epoch: 8.55 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004870029342608028		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.004870029342608028 | validation: 0.00655369389855266]
	TIME [epoch: 8.54 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004575454579462613		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.004575454579462613 | validation: 0.006576058323704049]
	TIME [epoch: 8.55 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006262132264420978		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.006262132264420978 | validation: 0.0058113327824310165]
	TIME [epoch: 8.54 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00476757139688498		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.00476757139688498 | validation: 0.0059882813052272756]
	TIME [epoch: 8.54 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005034622432241584		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.005034622432241584 | validation: 0.006153553893639939]
	TIME [epoch: 8.55 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004606736416691536		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.004606736416691536 | validation: 0.004767603691253455]
	TIME [epoch: 8.55 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004251370814405948		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.004251370814405948 | validation: 0.005136521924545498]
	TIME [epoch: 8.53 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005038803543482259		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.005038803543482259 | validation: 0.006190427311921338]
	TIME [epoch: 8.54 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049251583815580315		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.0049251583815580315 | validation: 0.006818689165866641]
	TIME [epoch: 8.55 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00650976477793584		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.00650976477793584 | validation: 0.005261230045901964]
	TIME [epoch: 8.54 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005448909601263707		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.005448909601263707 | validation: 0.005032791151473941]
	TIME [epoch: 8.53 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005225096380582939		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.005225096380582939 | validation: 0.005346925127917755]
	TIME [epoch: 8.54 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004910206930997128		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.004910206930997128 | validation: 0.008412775344922364]
	TIME [epoch: 8.55 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006378756844420262		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.006378756844420262 | validation: 0.005782716484821954]
	TIME [epoch: 8.54 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007442954441203891		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.007442954441203891 | validation: 0.013737089666520751]
	TIME [epoch: 8.54 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007093504406555772		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.007093504406555772 | validation: 0.007750607030102946]
	TIME [epoch: 8.54 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005032498464649251		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.005032498464649251 | validation: 0.0057288302449845395]
	TIME [epoch: 8.53 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041452032724369585		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.0041452032724369585 | validation: 0.004675319472843113]
	TIME [epoch: 8.55 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003990893396052362		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.003990893396052362 | validation: 0.006057431120545894]
	TIME [epoch: 8.54 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004746937217307879		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.004746937217307879 | validation: 0.004523009081932865]
	TIME [epoch: 8.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1014.pth
	Model improved!!!
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004080369155051991		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.004080369155051991 | validation: 0.0065700760052548705]
	TIME [epoch: 8.54 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004808979157683833		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.004808979157683833 | validation: 0.005483061171310556]
	TIME [epoch: 8.54 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005164655459251997		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.005164655459251997 | validation: 0.006760321268067068]
	TIME [epoch: 8.53 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008275625331279374		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.008275625331279374 | validation: 0.007887747303081368]
	TIME [epoch: 8.53 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006301428006651439		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.006301428006651439 | validation: 0.006495864690860709]
	TIME [epoch: 8.53 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004771104557582769		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.004771104557582769 | validation: 0.005374363495407923]
	TIME [epoch: 8.55 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004659739773329185		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.004659739773329185 | validation: 0.004850253214530546]
	TIME [epoch: 8.54 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004260848031221836		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.004260848031221836 | validation: 0.005117431241369345]
	TIME [epoch: 8.55 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004490366931168967		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.004490366931168967 | validation: 0.004998223520308773]
	TIME [epoch: 8.55 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004973881495823082		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.004973881495823082 | validation: 0.00561824412476737]
	TIME [epoch: 8.55 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00558481257424535		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.00558481257424535 | validation: 0.004703796468111888]
	TIME [epoch: 8.55 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004264618466662608		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.004264618466662608 | validation: 0.005242913671596036]
	TIME [epoch: 8.54 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044424883414903036		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.0044424883414903036 | validation: 0.006154831211349005]
	TIME [epoch: 8.55 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005072182737400422		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.005072182737400422 | validation: 0.005687642893343822]
	TIME [epoch: 8.55 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004433601211076802		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.004433601211076802 | validation: 0.005302428067146573]
	TIME [epoch: 8.55 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006704751360505383		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.006704751360505383 | validation: 0.006796026619069624]
	TIME [epoch: 8.54 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054276062564863085		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.0054276062564863085 | validation: 0.005114084665281749]
	TIME [epoch: 8.55 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004294383145172604		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.004294383145172604 | validation: 0.005430984830785921]
	TIME [epoch: 8.55 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00456628733833342		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.00456628733833342 | validation: 0.005045423817949545]
	TIME [epoch: 8.56 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004798251671821635		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.004798251671821635 | validation: 0.005505571798924733]
	TIME [epoch: 8.55 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004796068944995197		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.004796068944995197 | validation: 0.005110864483155298]
	TIME [epoch: 8.53 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005308105031102446		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.005308105031102446 | validation: 0.00642183419624289]
	TIME [epoch: 8.54 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004642304626324497		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.004642304626324497 | validation: 0.005526330675183881]
	TIME [epoch: 8.55 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005403349973811595		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.005403349973811595 | validation: 0.006846681136213848]
	TIME [epoch: 8.55 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004954461089274408		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.004954461089274408 | validation: 0.0064439376362016385]
	TIME [epoch: 8.55 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004514048265707228		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.004514048265707228 | validation: 0.006409170542146895]
	TIME [epoch: 8.55 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005718719296126435		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.005718719296126435 | validation: 0.005155786467813006]
	TIME [epoch: 8.55 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004386707666135061		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.004386707666135061 | validation: 0.005755573857022075]
	TIME [epoch: 8.55 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035936239790102242		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.0035936239790102242 | validation: 0.005490496140358847]
	TIME [epoch: 8.54 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005939102808412106		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.005939102808412106 | validation: 0.007277447866052114]
	TIME [epoch: 8.54 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004667378982638927		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.004667378982638927 | validation: 0.0062060333814714045]
	TIME [epoch: 8.56 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004083080808753946		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.004083080808753946 | validation: 0.006325050980447064]
	TIME [epoch: 8.54 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00439681047278004		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.00439681047278004 | validation: 0.005963444714872164]
	TIME [epoch: 8.54 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004318041586195108		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.004318041586195108 | validation: 0.005669906612383968]
	TIME [epoch: 8.54 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004343129642832195		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.004343129642832195 | validation: 0.006936244451293781]
	TIME [epoch: 8.55 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00506473568668605		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.00506473568668605 | validation: 0.00479798473197008]
	TIME [epoch: 8.54 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00436695027394198		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.00436695027394198 | validation: 0.006812266749329955]
	TIME [epoch: 8.54 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005783611658961239		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.005783611658961239 | validation: 0.006088740316161533]
	TIME [epoch: 8.55 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00455353475990762		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.00455353475990762 | validation: 0.006672718333145647]
	TIME [epoch: 8.54 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004138931470230535		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.004138931470230535 | validation: 0.005390308164665705]
	TIME [epoch: 8.55 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007422901929546568		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.007422901929546568 | validation: 0.006737125967757298]
	TIME [epoch: 8.54 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054918701289042695		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.0054918701289042695 | validation: 0.006143616388330834]
	TIME [epoch: 8.54 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004188511136301333		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.004188511136301333 | validation: 0.004367254774716475]
	TIME [epoch: 8.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1057.pth
	Model improved!!!
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004069397551575552		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.004069397551575552 | validation: 0.005231441535008138]
	TIME [epoch: 8.77 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004689778558174145		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.004689778558174145 | validation: 0.005109624151931469]
	TIME [epoch: 8.55 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052220811908288475		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.0052220811908288475 | validation: 0.005785654147331589]
	TIME [epoch: 8.54 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005082659774122435		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.005082659774122435 | validation: 0.006078727607187124]
	TIME [epoch: 8.55 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004406838391370132		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.004406838391370132 | validation: 0.005305075570331883]
	TIME [epoch: 8.56 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004638141689665195		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.004638141689665195 | validation: 0.004644706909714885]
	TIME [epoch: 8.55 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004164879818560663		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.004164879818560663 | validation: 0.005270143080365811]
	TIME [epoch: 8.54 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003948776023168825		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.003948776023168825 | validation: 0.005087022740319028]
	TIME [epoch: 8.55 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041388417812519025		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.0041388417812519025 | validation: 0.005370460494510244]
	TIME [epoch: 8.55 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004378292900942768		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.004378292900942768 | validation: 0.006962086478968546]
	TIME [epoch: 8.54 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004127705591122783		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.004127705591122783 | validation: 0.004809821680853345]
	TIME [epoch: 8.54 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004692613812554869		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.004692613812554869 | validation: 0.005483781200419778]
	TIME [epoch: 8.55 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00454238174451578		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.00454238174451578 | validation: 0.007063721002984928]
	TIME [epoch: 8.54 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004377318475714672		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.004377318475714672 | validation: 0.004608878940350759]
	TIME [epoch: 8.55 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004353102446431388		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.004353102446431388 | validation: 0.004979455362756681]
	TIME [epoch: 8.54 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004242997006099222		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.004242997006099222 | validation: 0.004849377934870209]
	TIME [epoch: 8.55 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004224436913858975		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.004224436913858975 | validation: 0.00596035696092956]
	TIME [epoch: 8.54 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004104945047432519		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.004104945047432519 | validation: 0.01028102188746978]
	TIME [epoch: 8.55 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005808651301831596		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.005808651301831596 | validation: 0.0062129132173335214]
	TIME [epoch: 8.54 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004206503827009529		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.004206503827009529 | validation: 0.00710703455925646]
	TIME [epoch: 8.54 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004852200013381888		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.004852200013381888 | validation: 0.004967826317024731]
	TIME [epoch: 8.54 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036981677763946777		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.0036981677763946777 | validation: 0.005685742831407523]
	TIME [epoch: 8.55 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003959658492557697		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.003959658492557697 | validation: 0.004981953089300063]
	TIME [epoch: 8.54 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004649529230471543		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.004649529230471543 | validation: 0.00557192877570198]
	TIME [epoch: 8.54 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003739313550244805		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.003739313550244805 | validation: 0.005089161741104146]
	TIME [epoch: 8.55 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043947864144635775		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.0043947864144635775 | validation: 0.00527226319397891]
	TIME [epoch: 8.56 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004271510862517784		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.004271510862517784 | validation: 0.004313341981750598]
	TIME [epoch: 8.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1084.pth
	Model improved!!!
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00401847356832706		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.00401847356832706 | validation: 0.004756771533641021]
	TIME [epoch: 8.55 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048706198538922135		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.0048706198538922135 | validation: 0.0055802959815494]
	TIME [epoch: 8.55 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004357956085650138		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.004357956085650138 | validation: 0.006538341944193804]
	TIME [epoch: 8.58 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004179990230620176		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.004179990230620176 | validation: 0.006418941506743831]
	TIME [epoch: 8.55 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00416921641147755		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.00416921641147755 | validation: 0.005304151745496609]
	TIME [epoch: 8.55 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004066584555715963		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.004066584555715963 | validation: 0.00539848358826954]
	TIME [epoch: 8.55 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00428871994797201		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.00428871994797201 | validation: 0.004078745209573241]
	TIME [epoch: 8.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1091.pth
	Model improved!!!
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004331599510741024		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.004331599510741024 | validation: 0.00588881063347251]
	TIME [epoch: 8.55 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004053382242732111		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.004053382242732111 | validation: 0.003913433683228571]
	TIME [epoch: 8.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1093.pth
	Model improved!!!
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038813554799506465		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.0038813554799506465 | validation: 0.007037661893225112]
	TIME [epoch: 8.54 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004737832212061949		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.004737832212061949 | validation: 0.004931475120323031]
	TIME [epoch: 8.55 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004249121428156303		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.004249121428156303 | validation: 0.004351701536880493]
	TIME [epoch: 8.55 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004597728438773783		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.004597728438773783 | validation: 0.00601116491521563]
	TIME [epoch: 8.54 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004535623329608885		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.004535623329608885 | validation: 0.005208445852117921]
	TIME [epoch: 8.55 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038066899150909464		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.0038066899150909464 | validation: 0.004621180571192707]
	TIME [epoch: 8.54 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038000619343093855		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.0038000619343093855 | validation: 0.004780704741584542]
	TIME [epoch: 8.55 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043553208065285065		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.0043553208065285065 | validation: 0.00443157224371407]
	TIME [epoch: 8.53 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060091778402351		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.0060091778402351 | validation: 0.006904305291595828]
	TIME [epoch: 8.54 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004217341301606954		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.004217341301606954 | validation: 0.005713254523171639]
	TIME [epoch: 8.53 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034185680920528067		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.0034185680920528067 | validation: 0.005069061435508728]
	TIME [epoch: 8.55 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003792406033353052		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.003792406033353052 | validation: 0.004582609632882571]
	TIME [epoch: 8.54 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003952141240001483		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.003952141240001483 | validation: 0.005615898102574183]
	TIME [epoch: 8.54 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003845565054584015		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.003845565054584015 | validation: 0.00529359445262946]
	TIME [epoch: 8.53 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036126548653817817		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.0036126548653817817 | validation: 0.005030833700929413]
	TIME [epoch: 8.55 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006674525631484905		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.006674525631484905 | validation: 0.0063142701562333686]
	TIME [epoch: 8.54 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004724988858338449		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.004724988858338449 | validation: 0.007876317654750746]
	TIME [epoch: 8.53 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003750650241202539		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.003750650241202539 | validation: 0.004995289614179631]
	TIME [epoch: 8.54 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004031959257094469		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.004031959257094469 | validation: 0.005335688032571605]
	TIME [epoch: 8.55 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003878620641270981		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.003878620641270981 | validation: 0.004139905457911103]
	TIME [epoch: 8.54 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004368784343705256		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.004368784343705256 | validation: 0.007013973933603262]
	TIME [epoch: 8.53 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004723563737002226		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.004723563737002226 | validation: 0.004680580522077671]
	TIME [epoch: 8.54 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004082597703461095		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.004082597703461095 | validation: 0.004530504571874778]
	TIME [epoch: 8.54 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003924335785570679		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.003924335785570679 | validation: 0.003934827714531883]
	TIME [epoch: 8.55 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038394750475054308		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.0038394750475054308 | validation: 0.0045578855791873416]
	TIME [epoch: 8.54 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003561720639260014		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.003561720639260014 | validation: 0.005247863169672851]
	TIME [epoch: 8.53 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003775281168928085		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.003775281168928085 | validation: 0.004920567062894173]
	TIME [epoch: 8.55 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035632432393690417		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.0035632432393690417 | validation: 0.005808740775312748]
	TIME [epoch: 8.56 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004148203475488954		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.004148203475488954 | validation: 0.005520361303754058]
	TIME [epoch: 8.54 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037789001591594117		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.0037789001591594117 | validation: 0.005515507436493806]
	TIME [epoch: 8.53 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00375511498919923		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.00375511498919923 | validation: 0.004905260769950795]
	TIME [epoch: 8.53 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004226253399739849		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.004226253399739849 | validation: 0.0044022220165643704]
	TIME [epoch: 8.54 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003598419370772716		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.003598419370772716 | validation: 0.00457753133952733]
	TIME [epoch: 8.54 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004116582988690912		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.004116582988690912 | validation: 0.004201881582802011]
	TIME [epoch: 8.53 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036820416096676116		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.0036820416096676116 | validation: 0.00480089448814495]
	TIME [epoch: 8.54 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003767958751398996		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.003767958751398996 | validation: 0.004587464686040932]
	TIME [epoch: 8.54 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040320702178448095		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.0040320702178448095 | validation: 0.005736410648885491]
	TIME [epoch: 8.54 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004350934752851874		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.004350934752851874 | validation: 0.0039120541413769985]
	TIME [epoch: 8.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1131.pth
	Model improved!!!
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035244982763560292		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.0035244982763560292 | validation: 0.004468162768858983]
	TIME [epoch: 8.55 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038951044338938535		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.0038951044338938535 | validation: 0.004922865902366323]
	TIME [epoch: 8.54 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004393178590586816		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.004393178590586816 | validation: 0.007411542725865941]
	TIME [epoch: 8.54 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00439318304086826		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.00439318304086826 | validation: 0.005639196880220672]
	TIME [epoch: 8.54 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045630582320804105		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.0045630582320804105 | validation: 0.004533436935352804]
	TIME [epoch: 8.53 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004145117491768531		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.004145117491768531 | validation: 0.00930165323748575]
	TIME [epoch: 8.53 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005349506287232204		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.005349506287232204 | validation: 0.004997330693300451]
	TIME [epoch: 8.54 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003515607237989091		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.003515607237989091 | validation: 0.0038426983465222097]
	TIME [epoch: 8.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1139.pth
	Model improved!!!
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003631982781633469		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.003631982781633469 | validation: 0.0045125613936034]
	TIME [epoch: 8.55 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036950904217852546		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.0036950904217852546 | validation: 0.003953446873375191]
	TIME [epoch: 8.55 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042122207112463015		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.0042122207112463015 | validation: 0.005521910218758169]
	TIME [epoch: 8.56 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038065315437902335		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.0038065315437902335 | validation: 0.004078372362109419]
	TIME [epoch: 8.55 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00371065601192614		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.00371065601192614 | validation: 0.004323147950302936]
	TIME [epoch: 8.55 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003714100618063065		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.003714100618063065 | validation: 0.006932216456281804]
	TIME [epoch: 8.55 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038342560850420726		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.0038342560850420726 | validation: 0.004972013933189767]
	TIME [epoch: 8.57 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003910351692245233		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.003910351692245233 | validation: 0.004137266780382904]
	TIME [epoch: 8.56 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036219077836330636		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.0036219077836330636 | validation: 0.0037065826117215037]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1148.pth
	Model improved!!!
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037298177867138376		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.0037298177867138376 | validation: 0.004697078914733166]
	TIME [epoch: 8.55 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003993758809364902		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.003993758809364902 | validation: 0.005177881028307375]
	TIME [epoch: 8.56 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035841383981477016		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.0035841383981477016 | validation: 0.00449498335244402]
	TIME [epoch: 8.56 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003518634172464321		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.003518634172464321 | validation: 0.003454971005924839]
	TIME [epoch: 8.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1152.pth
	Model improved!!!
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00403401597675365		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.00403401597675365 | validation: 0.005022393450299156]
	TIME [epoch: 8.55 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035157966298621623		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.0035157966298621623 | validation: 0.005628783472128719]
	TIME [epoch: 8.56 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003752373822433306		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.003752373822433306 | validation: 0.005588974605614626]
	TIME [epoch: 8.54 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003958043728711402		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.003958043728711402 | validation: 0.003936644271562455]
	TIME [epoch: 8.54 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004267706936178011		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.004267706936178011 | validation: 0.005974261060599116]
	TIME [epoch: 8.54 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033955066742482913		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.0033955066742482913 | validation: 0.004547042714819684]
	TIME [epoch: 8.55 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033175333538175682		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.0033175333538175682 | validation: 0.004605304742560074]
	TIME [epoch: 8.54 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033786762068043043		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.0033786762068043043 | validation: 0.004655848160634652]
	TIME [epoch: 8.55 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036991818448580812		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.0036991818448580812 | validation: 0.0038187939553554493]
	TIME [epoch: 8.55 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037942896727344174		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.0037942896727344174 | validation: 0.0061162259752241]
	TIME [epoch: 8.56 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003745049908768541		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.003745049908768541 | validation: 0.004382008199513274]
	TIME [epoch: 8.55 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038362078532470393		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.0038362078532470393 | validation: 0.011700665380957785]
	TIME [epoch: 8.55 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006663430479821506		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.006663430479821506 | validation: 0.004802036189228799]
	TIME [epoch: 8.56 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004024001949573906		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.004024001949573906 | validation: 0.003933178052134859]
	TIME [epoch: 8.55 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037899404146834372		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.0037899404146834372 | validation: 0.003497565864850686]
	TIME [epoch: 8.56 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032822006485020315		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.0032822006485020315 | validation: 0.003839687571543084]
	TIME [epoch: 8.55 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003927882785207993		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 0.003927882785207993 | validation: 0.005282405953487468]
	TIME [epoch: 8.54 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036282630484008663		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.0036282630484008663 | validation: 0.005020495388584979]
	TIME [epoch: 8.54 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003591543147509938		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.003591543147509938 | validation: 0.004859493434288088]
	TIME [epoch: 8.56 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003685152467299206		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.003685152467299206 | validation: 0.0036043480874515003]
	TIME [epoch: 8.56 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003825182460556323		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.003825182460556323 | validation: 0.005567487058762047]
	TIME [epoch: 8.54 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037471657773834963		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.0037471657773834963 | validation: 0.0047241098342745876]
	TIME [epoch: 8.55 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034294332178330844		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.0034294332178330844 | validation: 0.004292303571269071]
	TIME [epoch: 8.55 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003604263461193191		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.003604263461193191 | validation: 0.004075477823162031]
	TIME [epoch: 8.55 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036247827966567024		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.0036247827966567024 | validation: 0.003668332166818022]
	TIME [epoch: 8.54 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037187841323196814		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.0037187841323196814 | validation: 0.003974120079140063]
	TIME [epoch: 8.54 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003711673186561386		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.003711673186561386 | validation: 0.004189326703451039]
	TIME [epoch: 8.54 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003738063156607712		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.003738063156607712 | validation: 0.004127328931666377]
	TIME [epoch: 8.56 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004339304623103741		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.004339304623103741 | validation: 0.004830858192689514]
	TIME [epoch: 8.54 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037075355738397284		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.0037075355738397284 | validation: 0.005546303224178197]
	TIME [epoch: 8.55 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003947281010546466		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.003947281010546466 | validation: 0.003974220884423908]
	TIME [epoch: 8.54 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033117253273751562		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.0033117253273751562 | validation: 0.005789984713119967]
	TIME [epoch: 8.55 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003313168722914394		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.003313168722914394 | validation: 0.004585799375150818]
	TIME [epoch: 8.54 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003924443638342105		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.003924443638342105 | validation: 0.0049344389163115]
	TIME [epoch: 8.54 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034503458911352733		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.0034503458911352733 | validation: 0.00457125049203063]
	TIME [epoch: 8.53 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035546559409725216		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.0035546559409725216 | validation: 0.004542474566918513]
	TIME [epoch: 8.55 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003410456309236507		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.003410456309236507 | validation: 0.004899587958781437]
	TIME [epoch: 8.54 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033762822964180424		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.0033762822964180424 | validation: 0.004223477925639865]
	TIME [epoch: 8.55 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031111969073974646		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.0031111969073974646 | validation: 0.004824991307693094]
	TIME [epoch: 8.53 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031046291962321193		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.0031046291962321193 | validation: 0.004947321896337457]
	TIME [epoch: 8.56 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032369789651450773		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.0032369789651450773 | validation: 0.005679722238437904]
	TIME [epoch: 8.54 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004301228299897321		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.004301228299897321 | validation: 0.005481966319651078]
	TIME [epoch: 8.55 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004310910920513583		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.004310910920513583 | validation: 0.005408752575586995]
	TIME [epoch: 8.54 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037077992657557793		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.0037077992657557793 | validation: 0.005377033529874216]
	TIME [epoch: 8.55 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035235970901999958		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.0035235970901999958 | validation: 0.005398687279428004]
	TIME [epoch: 8.55 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034740994029258407		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.0034740994029258407 | validation: 0.005840199113208962]
	TIME [epoch: 8.55 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004471724329108643		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.004471724329108643 | validation: 0.00456051468388434]
	TIME [epoch: 8.54 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035517756460677697		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.0035517756460677697 | validation: 0.0035108449582282163]
	TIME [epoch: 8.55 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037333960050590044		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.0037333960050590044 | validation: 0.003351195946680598]
	TIME [epoch: 8.55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1201.pth
	Model improved!!!
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004137811379878644		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.004137811379878644 | validation: 0.005676182224464983]
	TIME [epoch: 8.53 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035993524866746434		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.0035993524866746434 | validation: 0.005411683932013524]
	TIME [epoch: 8.53 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033677617729276875		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.0033677617729276875 | validation: 0.004412724918683148]
	TIME [epoch: 8.54 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003325601743171633		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.003325601743171633 | validation: 0.0051198316563109]
	TIME [epoch: 8.55 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031115596577770822		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.0031115596577770822 | validation: 0.004138560609890104]
	TIME [epoch: 8.54 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004120233232271044		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.004120233232271044 | validation: 0.004543010327943811]
	TIME [epoch: 8.54 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003183728119090274		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.003183728119090274 | validation: 0.0042179717325038085]
	TIME [epoch: 8.54 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034533903509039506		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.0034533903509039506 | validation: 0.004316548389178305]
	TIME [epoch: 8.56 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032757838207659157		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.0032757838207659157 | validation: 0.005062527820576946]
	TIME [epoch: 8.53 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003643431405463255		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.003643431405463255 | validation: 0.00455633179701929]
	TIME [epoch: 8.54 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032673617543186825		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.0032673617543186825 | validation: 0.0039343289167489606]
	TIME [epoch: 8.54 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032179735132252985		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.0032179735132252985 | validation: 0.00389622320621477]
	TIME [epoch: 8.55 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036098918447841923		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.0036098918447841923 | validation: 0.004640743674327894]
	TIME [epoch: 8.55 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029903819543984234		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.0029903819543984234 | validation: 0.0042195490172790276]
	TIME [epoch: 8.55 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032837857718132388		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.0032837857718132388 | validation: 0.00477090464589756]
	TIME [epoch: 8.55 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003450516909481762		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.003450516909481762 | validation: 0.0037913205905033493]
	TIME [epoch: 8.54 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037647967783718764		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.0037647967783718764 | validation: 0.004073683934598754]
	TIME [epoch: 8.54 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034329182861440274		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.0034329182861440274 | validation: 0.0042928367381691954]
	TIME [epoch: 8.54 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003557426488501287		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.003557426488501287 | validation: 0.003962051475921507]
	TIME [epoch: 8.55 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033527999509735985		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.0033527999509735985 | validation: 0.004988186558493842]
	TIME [epoch: 8.56 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00355779522253134		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.00355779522253134 | validation: 0.004869800417171216]
	TIME [epoch: 8.56 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033451965727323354		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.0033451965727323354 | validation: 0.004901221986194617]
	TIME [epoch: 8.55 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003045201006809179		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.003045201006809179 | validation: 0.0038769155422612844]
	TIME [epoch: 8.53 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003378735991817296		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.003378735991817296 | validation: 0.003750961834867395]
	TIME [epoch: 8.54 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003280948318478826		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.003280948318478826 | validation: 0.003897040613983156]
	TIME [epoch: 8.55 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003512669231751902		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.003512669231751902 | validation: 0.004599654834869252]
	TIME [epoch: 8.55 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031376260477182067		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.0031376260477182067 | validation: 0.0037180419855046363]
	TIME [epoch: 8.54 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00354263139834408		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.00354263139834408 | validation: 0.0037134915666947347]
	TIME [epoch: 8.55 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003547093580407254		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.003547093580407254 | validation: 0.0036474591486174105]
	TIME [epoch: 8.56 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00304676188240588		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.00304676188240588 | validation: 0.003937673327093196]
	TIME [epoch: 8.55 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031968877955198573		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.0031968877955198573 | validation: 0.003977003697043283]
	TIME [epoch: 8.54 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031123713396965158		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.0031123713396965158 | validation: 0.005006927384193947]
	TIME [epoch: 8.55 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003550242819724143		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.003550242819724143 | validation: 0.004895371671579543]
	TIME [epoch: 8.55 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033677458902294415		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.0033677458902294415 | validation: 0.004786278613063542]
	TIME [epoch: 8.54 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031774583148439393		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.0031774583148439393 | validation: 0.005375521494488407]
	TIME [epoch: 8.52 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003434614642296766		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.003434614642296766 | validation: 0.003557754121174839]
	TIME [epoch: 8.53 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029807740566944354		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.0029807740566944354 | validation: 0.0035511642041746484]
	TIME [epoch: 8.55 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002929647549292598		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.002929647549292598 | validation: 0.004426318478137984]
	TIME [epoch: 8.55 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027872536678195513		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.0027872536678195513 | validation: 0.004300458419673133]
	TIME [epoch: 8.55 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035140792736494783		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.0035140792736494783 | validation: 0.005129460232224508]
	TIME [epoch: 8.55 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003327587826590573		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.003327587826590573 | validation: 0.004558884165982348]
	TIME [epoch: 8.55 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003389154981101811		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.003389154981101811 | validation: 0.004866038375086687]
	TIME [epoch: 8.56 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003312936432056326		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.003312936432056326 | validation: 0.003610048458690534]
	TIME [epoch: 8.55 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037292925105571672		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.0037292925105571672 | validation: 0.0036420081629957813]
	TIME [epoch: 8.55 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029313352399796865		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.0029313352399796865 | validation: 0.004965486240449101]
	TIME [epoch: 8.55 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003049216753487365		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.003049216753487365 | validation: 0.005085436908270223]
	TIME [epoch: 8.56 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003785034751213786		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.003785034751213786 | validation: 0.004315153329665801]
	TIME [epoch: 8.55 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003029888598503369		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.003029888598503369 | validation: 0.004676978733528071]
	TIME [epoch: 8.55 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032980794510205445		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.0032980794510205445 | validation: 0.005245271425057192]
	TIME [epoch: 8.54 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034709411448315404		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.0034709411448315404 | validation: 0.004313743487726137]
	TIME [epoch: 8.55 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034225315718696137		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.0034225315718696137 | validation: 0.004251565461746768]
	TIME [epoch: 8.54 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003082399971996892		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.003082399971996892 | validation: 0.003991559108768144]
	TIME [epoch: 8.54 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034420148161285196		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.0034420148161285196 | validation: 0.003752975203566036]
	TIME [epoch: 8.54 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031918319515527186		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.0031918319515527186 | validation: 0.00397073537516936]
	TIME [epoch: 8.55 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031330850454572563		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.0031330850454572563 | validation: 0.00518117127929378]
	TIME [epoch: 8.54 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033726012413866414		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.0033726012413866414 | validation: 0.003847023121268189]
	TIME [epoch: 8.54 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00323291700701144		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.00323291700701144 | validation: 0.003979501218961503]
	TIME [epoch: 8.54 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033877130025139765		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.0033877130025139765 | validation: 0.0046484458910306895]
	TIME [epoch: 8.55 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029560808456454944		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.0029560808456454944 | validation: 0.004545742694068067]
	TIME [epoch: 8.55 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037209335239089627		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.0037209335239089627 | validation: 0.005360778702220231]
	TIME [epoch: 8.55 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003134761675833403		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.003134761675833403 | validation: 0.004301687453305768]
	TIME [epoch: 8.54 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031302365595488445		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.0031302365595488445 | validation: 0.0046824024848645]
	TIME [epoch: 8.55 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003136697758828176		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.003136697758828176 | validation: 0.005313328027551312]
	TIME [epoch: 8.56 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003094029216196365		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.003094029216196365 | validation: 0.004338882282957772]
	TIME [epoch: 8.55 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003192215848065791		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.003192215848065791 | validation: 0.0042901869738786865]
	TIME [epoch: 8.54 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003055607914092554		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.003055607914092554 | validation: 0.0038193245131078694]
	TIME [epoch: 8.54 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002762194357073128		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.002762194357073128 | validation: 0.003329448059907178]
	TIME [epoch: 8.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1268.pth
	Model improved!!!
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003197719108904039		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.003197719108904039 | validation: 0.005022610973397199]
	TIME [epoch: 8.54 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033560767658907674		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.0033560767658907674 | validation: 0.005322660266959862]
	TIME [epoch: 8.54 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004186051864246297		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.004186051864246297 | validation: 0.004492665457724476]
	TIME [epoch: 8.57 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035348360562090803		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.0035348360562090803 | validation: 0.004218769359920463]
	TIME [epoch: 8.54 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035320310636679938		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.0035320310636679938 | validation: 0.0049910229580769835]
	TIME [epoch: 8.54 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033850328772497824		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.0033850328772497824 | validation: 0.004172786903091918]
	TIME [epoch: 8.53 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033714815989942274		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.0033714815989942274 | validation: 0.004340871084425292]
	TIME [epoch: 8.53 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003096804994878916		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.003096804994878916 | validation: 0.005331210710550529]
	TIME [epoch: 8.54 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003264256774351654		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.003264256774351654 | validation: 0.0048171942726614085]
	TIME [epoch: 8.53 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003399188381268117		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.003399188381268117 | validation: 0.0038402052890326036]
	TIME [epoch: 8.52 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031322937796929976		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.0031322937796929976 | validation: 0.0038263180996842613]
	TIME [epoch: 8.53 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003131956092045902		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.003131956092045902 | validation: 0.004853459148795756]
	TIME [epoch: 8.52 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002722160869860049		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.002722160869860049 | validation: 0.004139438401426984]
	TIME [epoch: 8.53 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030939063773746424		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.0030939063773746424 | validation: 0.003706582239566867]
	TIME [epoch: 8.52 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003331036549808969		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.003331036549808969 | validation: 0.0038285673056457086]
	TIME [epoch: 8.53 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003139306280629264		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.003139306280629264 | validation: 0.004055837383184422]
	TIME [epoch: 8.51 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003331353323463936		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.003331353323463936 | validation: 0.004874809626707633]
	TIME [epoch: 8.54 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030171011397879697		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.0030171011397879697 | validation: 0.0037757247474477793]
	TIME [epoch: 8.52 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033900422222813706		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.0033900422222813706 | validation: 0.0038450758041609252]
	TIME [epoch: 8.52 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002802402781527915		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.002802402781527915 | validation: 0.0034165840124311436]
	TIME [epoch: 8.52 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003539690881374168		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.003539690881374168 | validation: 0.004492440724073013]
	TIME [epoch: 8.54 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031729767541803246		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.0031729767541803246 | validation: 0.003097979788812064]
	TIME [epoch: 8.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1290.pth
	Model improved!!!
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030171003534270246		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.0030171003534270246 | validation: 0.0039360612153865725]
	TIME [epoch: 8.54 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003089737882976666		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.003089737882976666 | validation: 0.0037940375556748383]
	TIME [epoch: 8.54 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003130601607028516		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.003130601607028516 | validation: 0.003582507201412172]
	TIME [epoch: 8.55 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031395571021942768		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.0031395571021942768 | validation: 0.0039905858736114285]
	TIME [epoch: 8.54 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003252380747757859		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.003252380747757859 | validation: 0.0046687698405762324]
	TIME [epoch: 8.53 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030074464222900046		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.0030074464222900046 | validation: 0.005026392858258883]
	TIME [epoch: 8.53 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030130356693289714		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.0030130356693289714 | validation: 0.0038292995298055393]
	TIME [epoch: 8.54 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002971604984153661		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.002971604984153661 | validation: 0.0033893636723806796]
	TIME [epoch: 8.53 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003371217366440897		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.003371217366440897 | validation: 0.0038469722377586562]
	TIME [epoch: 8.53 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032845589580718236		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.0032845589580718236 | validation: 0.003593788887269445]
	TIME [epoch: 8.54 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003222006297657441		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.003222006297657441 | validation: 0.004042840202457088]
	TIME [epoch: 8.55 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033266212567515307		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.0033266212567515307 | validation: 0.004538513770741551]
	TIME [epoch: 8.55 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002483417401625011		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.002483417401625011 | validation: 0.0036859184702818493]
	TIME [epoch: 8.54 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032837720474768136		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.0032837720474768136 | validation: 0.004132945178058404]
	TIME [epoch: 8.54 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002964472834302046		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.002964472834302046 | validation: 0.004093992221834021]
	TIME [epoch: 8.54 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029059149730253793		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.0029059149730253793 | validation: 0.0037072202537358247]
	TIME [epoch: 8.56 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003087744100086033		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.003087744100086033 | validation: 0.004117122235746077]
	TIME [epoch: 8.55 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003152527045771188		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.003152527045771188 | validation: 0.004916056560949896]
	TIME [epoch: 8.54 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003185159767322856		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.003185159767322856 | validation: 0.0037203662236142128]
	TIME [epoch: 8.54 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028426835988433377		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 0.0028426835988433377 | validation: 0.0036364707473180154]
	TIME [epoch: 8.55 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028112894762925287		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 0.0028112894762925287 | validation: 0.004192410056713983]
	TIME [epoch: 8.54 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028603130017407505		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 0.0028603130017407505 | validation: 0.0042286359208876]
	TIME [epoch: 8.54 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002933072447414902		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 0.002933072447414902 | validation: 0.00441777377519561]
	TIME [epoch: 8.54 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002802820183583648		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 0.002802820183583648 | validation: 0.004520642996484866]
	TIME [epoch: 8.55 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029884285942062497		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 0.0029884285942062497 | validation: 0.003840852802919477]
	TIME [epoch: 8.54 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002959387209891102		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 0.002959387209891102 | validation: 0.003768860678908043]
	TIME [epoch: 8.54 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003904978755729065		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 0.003904978755729065 | validation: 0.004462504620844538]
	TIME [epoch: 8.55 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033713667927004623		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 0.0033713667927004623 | validation: 0.0038487248897061265]
	TIME [epoch: 8.55 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003214405152895389		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 0.003214405152895389 | validation: 0.0035649326645592555]
	TIME [epoch: 8.54 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028089879927376427		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 0.0028089879927376427 | validation: 0.003805789647279502]
	TIME [epoch: 8.54 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002770696574342742		[learning rate: 0.00011092]
	Learning Rate: 0.000110918
	LOSS [training: 0.002770696574342742 | validation: 0.004221379501909594]
	TIME [epoch: 8.54 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032530534887856553		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 0.0032530534887856553 | validation: 0.0035802216181888406]
	TIME [epoch: 8.54 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028777492053202		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 0.0028777492053202 | validation: 0.003871104303157307]
	TIME [epoch: 8.55 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002666961980873965		[learning rate: 0.00010975]
	Learning Rate: 0.000109745
	LOSS [training: 0.002666961980873965 | validation: 0.003633857998948752]
	TIME [epoch: 8.54 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028745888175085756		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 0.0028745888175085756 | validation: 0.003091543994070574]
	TIME [epoch: 8.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1325.pth
	Model improved!!!
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003120508510904103		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 0.003120508510904103 | validation: 0.0040141422256523675]
	TIME [epoch: 8.54 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032145024075884952		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 0.0032145024075884952 | validation: 0.004252204748840759]
	TIME [epoch: 8.55 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027414410416903663		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 0.0027414410416903663 | validation: 0.004414872250019676]
	TIME [epoch: 8.54 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030057482527817885		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 0.0030057482527817885 | validation: 0.0036490366373474557]
	TIME [epoch: 8.54 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033133717842071184		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 0.0033133717842071184 | validation: 0.004687631060646762]
	TIME [epoch: 8.52 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029635374022968083		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 0.0029635374022968083 | validation: 0.004695631227614776]
	TIME [epoch: 8.55 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029859586668837977		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 0.0029859586668837977 | validation: 0.0036605717947064947]
	TIME [epoch: 8.53 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032029951358276618		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 0.0032029951358276618 | validation: 0.004784571230524396]
	TIME [epoch: 8.53 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029419053432206805		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 0.0029419053432206805 | validation: 0.0044158254909691715]
	TIME [epoch: 8.52 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028449889615143158		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 0.0028449889615143158 | validation: 0.004085580169257081]
	TIME [epoch: 8.55 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002886401619941615		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 0.002886401619941615 | validation: 0.004270331050350454]
	TIME [epoch: 8.53 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029941106525707985		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 0.0029941106525707985 | validation: 0.0036494132308869517]
	TIME [epoch: 8.54 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025465468072043604		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 0.0025465468072043604 | validation: 0.003293793275623929]
	TIME [epoch: 8.53 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003272926097364316		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 0.003272926097364316 | validation: 0.0054066184898825505]
	TIME [epoch: 8.53 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029937307020267865		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 0.0029937307020267865 | validation: 0.004144264339375017]
	TIME [epoch: 8.54 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002606827793969689		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 0.002606827793969689 | validation: 0.0037767261945511168]
	TIME [epoch: 8.52 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030317695963342642		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 0.0030317695963342642 | validation: 0.004211961456199942]
	TIME [epoch: 8.52 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027477318569749053		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 0.0027477318569749053 | validation: 0.00401488065934889]
	TIME [epoch: 8.53 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027329648544488724		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 0.0027329648544488724 | validation: 0.003938459980727501]
	TIME [epoch: 8.52 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002772219540973484		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 0.002772219540973484 | validation: 0.0040383183337241385]
	TIME [epoch: 8.52 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028796823710658093		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 0.0028796823710658093 | validation: 0.0033957874720422173]
	TIME [epoch: 8.53 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003232552437586708		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.003232552437586708 | validation: 0.004303138475402169]
	TIME [epoch: 8.52 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002875674361220142		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 0.002875674361220142 | validation: 0.0036803681142286155]
	TIME [epoch: 8.55 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003041089977059968		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 0.003041089977059968 | validation: 0.0038336729213551066]
	TIME [epoch: 8.53 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002654085452467303		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 0.002654085452467303 | validation: 0.00328025559323402]
	TIME [epoch: 8.52 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025407432344496735		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 0.0025407432344496735 | validation: 0.003738889649677151]
	TIME [epoch: 8.53 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030488325920546952		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 0.0030488325920546952 | validation: 0.003717421919675438]
	TIME [epoch: 8.53 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027708248478181324		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 0.0027708248478181324 | validation: 0.003329294097114225]
	TIME [epoch: 8.52 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029032993736145883		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 0.0029032993736145883 | validation: 0.003451692365933134]
	TIME [epoch: 8.53 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027279140020090957		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 0.0027279140020090957 | validation: 0.003761050288900937]
	TIME [epoch: 8.52 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030439577104576904		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 0.0030439577104576904 | validation: 0.00466936177002414]
	TIME [epoch: 8.52 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029956515880400956		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 0.0029956515880400956 | validation: 0.004020252783869993]
	TIME [epoch: 8.53 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003043888325184478		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 0.003043888325184478 | validation: 0.004149462829411409]
	TIME [epoch: 8.54 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029517529714836836		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 0.0029517529714836836 | validation: 0.0044553478913700525]
	TIME [epoch: 8.53 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025336801897541043		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 0.0025336801897541043 | validation: 0.0035100338665795755]
	TIME [epoch: 8.51 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003030541730490932		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 0.003030541730490932 | validation: 0.00356243213639495]
	TIME [epoch: 8.52 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002902391670367105		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 0.002902391670367105 | validation: 0.003890158507113471]
	TIME [epoch: 8.53 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026435270082951633		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 0.0026435270082951633 | validation: 0.003178941391232487]
	TIME [epoch: 8.53 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029671917435632935		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 0.0029671917435632935 | validation: 0.003293542492243713]
	TIME [epoch: 8.51 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027006437111675363		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 0.0027006437111675363 | validation: 0.004084027523065685]
	TIME [epoch: 8.55 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028029798891644354		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 0.0028029798891644354 | validation: 0.0035058091331010496]
	TIME [epoch: 8.54 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029179263927622337		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 0.0029179263927622337 | validation: 0.003160237045365007]
	TIME [epoch: 8.54 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031724448917887374		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 0.0031724448917887374 | validation: 0.0042556586745494855]
	TIME [epoch: 8.53 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002849165743052677		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 0.002849165743052677 | validation: 0.004116078271495028]
	TIME [epoch: 8.55 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032383446711365307		[learning rate: 9.3243e-05]
	Learning Rate: 9.32428e-05
	LOSS [training: 0.0032383446711365307 | validation: 0.0031673495389358554]
	TIME [epoch: 8.54 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031150160659198147		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 0.0031150160659198147 | validation: 0.003348665794644692]
	TIME [epoch: 8.54 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002816311840582043		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 0.002816311840582043 | validation: 0.003712853019674245]
	TIME [epoch: 8.51 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002743875925121385		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 0.002743875925121385 | validation: 0.0045704043483692485]
	TIME [epoch: 8.54 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004538542694408722		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 0.004538542694408722 | validation: 0.004212376543580676]
	TIME [epoch: 8.54 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028751795691275913		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 0.0028751795691275913 | validation: 0.0039825289465423635]
	TIME [epoch: 8.51 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027800831542747336		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 0.0027800831542747336 | validation: 0.00406091553606278]
	TIME [epoch: 8.53 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029035872083009078		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 0.0029035872083009078 | validation: 0.004029715628561921]
	TIME [epoch: 8.54 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028061376679426714		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 0.0028061376679426714 | validation: 0.00390184647396113]
	TIME [epoch: 8.52 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028795984768842948		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 0.0028795984768842948 | validation: 0.0032307602729021445]
	TIME [epoch: 8.54 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00275993490424899		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 0.00275993490424899 | validation: 0.0033456737384043233]
	TIME [epoch: 8.52 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029772797207983136		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 0.0029772797207983136 | validation: 0.004180836493315785]
	TIME [epoch: 8.55 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030140077527113784		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 0.0030140077527113784 | validation: 0.003699723291509591]
	TIME [epoch: 8.55 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026736909660335594		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 0.0026736909660335594 | validation: 0.0043503840585825525]
	TIME [epoch: 8.53 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002553998296550916		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 0.002553998296550916 | validation: 0.0033049387954353715]
	TIME [epoch: 8.53 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033483240628408135		[learning rate: 8.8418e-05]
	Learning Rate: 8.84175e-05
	LOSS [training: 0.0033483240628408135 | validation: 0.0038085857955191885]
	TIME [epoch: 8.54 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003502756637736383		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.003502756637736383 | validation: 0.004411866150975074]
	TIME [epoch: 8.55 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003074698410933014		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 0.003074698410933014 | validation: 0.0036739486340954916]
	TIME [epoch: 8.52 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003081698661816963		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 0.003081698661816963 | validation: 0.0029849984727349545]
	TIME [epoch: 8.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1388.pth
	Model improved!!!
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024026395898913253		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 0.0024026395898913253 | validation: 0.0036293758360409364]
	TIME [epoch: 8.54 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024971682437427605		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 0.0024971682437427605 | validation: 0.003357104939761303]
	TIME [epoch: 8.54 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025646582314141856		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 0.0025646582314141856 | validation: 0.003080564843019385]
	TIME [epoch: 8.53 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031354449381209445		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 0.0031354449381209445 | validation: 0.003222145166185921]
	TIME [epoch: 8.52 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028899696742216135		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 0.0028899696742216135 | validation: 0.003722503914575172]
	TIME [epoch: 8.53 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002738751344531466		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 0.002738751344531466 | validation: 0.0037956758386868596]
	TIME [epoch: 8.54 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028996116359465075		[learning rate: 8.534e-05]
	Learning Rate: 8.53402e-05
	LOSS [training: 0.0028996116359465075 | validation: 0.0035142564141246303]
	TIME [epoch: 8.53 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026566599443714244		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 0.0026566599443714244 | validation: 0.003961449419396049]
	TIME [epoch: 8.53 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037735930409565515		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 0.0037735930409565515 | validation: 0.00469865968884287]
	TIME [epoch: 8.53 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002807631302355701		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 0.002807631302355701 | validation: 0.004142337017616151]
	TIME [epoch: 8.53 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028851214704079713		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 0.0028851214704079713 | validation: 0.0034641240912295116]
	TIME [epoch: 8.54 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002645694635075937		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 0.002645694635075937 | validation: 0.003690995011849548]
	TIME [epoch: 8.53 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027624329144639945		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 0.0027624329144639945 | validation: 0.002898166612600845]
	TIME [epoch: 8.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1401.pth
	Model improved!!!
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028711077891653302		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 0.0028711077891653302 | validation: 0.0026771565055848934]
	TIME [epoch: 8.53 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1402.pth
	Model improved!!!
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028147251420911597		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 0.0028147251420911597 | validation: 0.0034540984011680676]
	TIME [epoch: 8.54 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028839912079075786		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 0.0028839912079075786 | validation: 0.0030646845956577747]
	TIME [epoch: 8.53 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002949871565861458		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 0.002949871565861458 | validation: 0.003423957797245046]
	TIME [epoch: 8.54 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002706189905952272		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 0.002706189905952272 | validation: 0.0036079549583404426]
	TIME [epoch: 8.55 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002753735609650207		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 0.002753735609650207 | validation: 0.00443850041688985]
	TIME [epoch: 8.55 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028697235483753055		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 0.0028697235483753055 | validation: 0.0037486789125064568]
	TIME [epoch: 8.54 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002629219528763724		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 0.002629219528763724 | validation: 0.002611517402742931]
	TIME [epoch: 8.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1409.pth
	Model improved!!!
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003029540093513039		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 0.003029540093513039 | validation: 0.004228443450162687]
	TIME [epoch: 8.54 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002716028042428876		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 0.002716028042428876 | validation: 0.0036609677966713385]
	TIME [epoch: 8.55 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027748465274348194		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 0.0027748465274348194 | validation: 0.004534451905341263]
	TIME [epoch: 8.52 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002748412064301954		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 0.002748412064301954 | validation: 0.002658116976710579]
	TIME [epoch: 8.52 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028176275474137997		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 0.0028176275474137997 | validation: 0.003223970299902239]
	TIME [epoch: 8.53 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002784523556362878		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 0.002784523556362878 | validation: 0.00407378300299634]
	TIME [epoch: 8.54 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029967179946599224		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 0.0029967179946599224 | validation: 0.003976154716589268]
	TIME [epoch: 8.53 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002627409530259959		[learning rate: 7.8942e-05]
	Learning Rate: 7.8942e-05
	LOSS [training: 0.002627409530259959 | validation: 0.0036683302615461745]
	TIME [epoch: 8.53 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026118280160578856		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 0.0026118280160578856 | validation: 0.003242152606776316]
	TIME [epoch: 8.53 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002819157562915714		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 0.002819157562915714 | validation: 0.003424483007883321]
	TIME [epoch: 8.54 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002955101702810352		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 0.002955101702810352 | validation: 0.004275403224961465]
	TIME [epoch: 8.53 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029480182157790593		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 0.0029480182157790593 | validation: 0.003273127749844209]
	TIME [epoch: 8.51 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002629265642086174		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 0.002629265642086174 | validation: 0.0037599023467888406]
	TIME [epoch: 8.53 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002628152218445417		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 0.002628152218445417 | validation: 0.0024281038337512374]
	TIME [epoch: 8.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1423.pth
	Model improved!!!
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025773014115691795		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 0.0025773014115691795 | validation: 0.004643481641433713]
	TIME [epoch: 8.53 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00290659211085415		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.00290659211085415 | validation: 0.0035349853453656354]
	TIME [epoch: 8.52 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025513059692341767		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 0.0025513059692341767 | validation: 0.004077413679251466]
	TIME [epoch: 8.52 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027969112730180353		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 0.0027969112730180353 | validation: 0.00420544219937473]
	TIME [epoch: 8.53 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002499698782008547		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 0.002499698782008547 | validation: 0.0036484970384471505]
	TIME [epoch: 8.54 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027885149240246855		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 0.0027885149240246855 | validation: 0.003984906996478054]
	TIME [epoch: 8.53 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027663174139164705		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 0.0027663174139164705 | validation: 0.003908933851075144]
	TIME [epoch: 8.52 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028494994019953243		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 0.0028494994019953243 | validation: 0.0036207401691298003]
	TIME [epoch: 8.53 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002766590213524811		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 0.002766590213524811 | validation: 0.00350224003798859]
	TIME [epoch: 8.54 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029990324893761433		[learning rate: 7.4592e-05]
	Learning Rate: 7.4592e-05
	LOSS [training: 0.0029990324893761433 | validation: 0.0036873922830025396]
	TIME [epoch: 8.54 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029211358750436565		[learning rate: 7.4328e-05]
	Learning Rate: 7.43283e-05
	LOSS [training: 0.0029211358750436565 | validation: 0.003535421284589723]
	TIME [epoch: 8.52 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002849759507454772		[learning rate: 7.4065e-05]
	Learning Rate: 7.40654e-05
	LOSS [training: 0.002849759507454772 | validation: 0.004330886744606495]
	TIME [epoch: 8.52 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002561377279175393		[learning rate: 7.3803e-05]
	Learning Rate: 7.38035e-05
	LOSS [training: 0.002561377279175393 | validation: 0.003284046103251677]
	TIME [epoch: 8.54 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031151293897346776		[learning rate: 7.3543e-05]
	Learning Rate: 7.35425e-05
	LOSS [training: 0.0031151293897346776 | validation: 0.0033312091861132168]
	TIME [epoch: 8.54 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002741892005189953		[learning rate: 7.3282e-05]
	Learning Rate: 7.32825e-05
	LOSS [training: 0.002741892005189953 | validation: 0.003224793314598637]
	TIME [epoch: 8.53 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028515656663374526		[learning rate: 7.3023e-05]
	Learning Rate: 7.30233e-05
	LOSS [training: 0.0028515656663374526 | validation: 0.0036144543331548284]
	TIME [epoch: 8.53 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002710380133708741		[learning rate: 7.2765e-05]
	Learning Rate: 7.27651e-05
	LOSS [training: 0.002710380133708741 | validation: 0.0030274201283543496]
	TIME [epoch: 8.54 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002584230494918717		[learning rate: 7.2508e-05]
	Learning Rate: 7.25078e-05
	LOSS [training: 0.002584230494918717 | validation: 0.0039193596975213504]
	TIME [epoch: 8.53 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024535852959741648		[learning rate: 7.2251e-05]
	Learning Rate: 7.22514e-05
	LOSS [training: 0.0024535852959741648 | validation: 0.0029703424229063514]
	TIME [epoch: 8.53 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002918968296887721		[learning rate: 7.1996e-05]
	Learning Rate: 7.19959e-05
	LOSS [training: 0.002918968296887721 | validation: 0.00324533773210461]
	TIME [epoch: 8.53 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025937967436394835		[learning rate: 7.1741e-05]
	Learning Rate: 7.17413e-05
	LOSS [training: 0.0025937967436394835 | validation: 0.00338306851915757]
	TIME [epoch: 8.54 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024872827673302657		[learning rate: 7.1488e-05]
	Learning Rate: 7.14876e-05
	LOSS [training: 0.0024872827673302657 | validation: 0.003510235752374153]
	TIME [epoch: 8.54 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026732835577095654		[learning rate: 7.1235e-05]
	Learning Rate: 7.12348e-05
	LOSS [training: 0.0026732835577095654 | validation: 0.0032447074234690527]
	TIME [epoch: 8.52 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029193598356047523		[learning rate: 7.0983e-05]
	Learning Rate: 7.09829e-05
	LOSS [training: 0.0029193598356047523 | validation: 0.0036482478136162246]
	TIME [epoch: 8.53 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025049207071892203		[learning rate: 7.0732e-05]
	Learning Rate: 7.07319e-05
	LOSS [training: 0.0025049207071892203 | validation: 0.003420528698495813]
	TIME [epoch: 8.53 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002798908604504417		[learning rate: 7.0482e-05]
	Learning Rate: 7.04818e-05
	LOSS [training: 0.002798908604504417 | validation: 0.003580817936987229]
	TIME [epoch: 8.55 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002936142204590617		[learning rate: 7.0233e-05]
	Learning Rate: 7.02326e-05
	LOSS [training: 0.002936142204590617 | validation: 0.003926565285658073]
	TIME [epoch: 8.52 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002805339405794095		[learning rate: 6.9984e-05]
	Learning Rate: 6.99842e-05
	LOSS [training: 0.002805339405794095 | validation: 0.0039697921413603746]
	TIME [epoch: 8.53 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023292995660809117		[learning rate: 6.9737e-05]
	Learning Rate: 6.97367e-05
	LOSS [training: 0.0023292995660809117 | validation: 0.003656322883684167]
	TIME [epoch: 8.53 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025426446380705367		[learning rate: 6.949e-05]
	Learning Rate: 6.94901e-05
	LOSS [training: 0.0025426446380705367 | validation: 0.003398640269623085]
	TIME [epoch: 8.55 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003059829666191242		[learning rate: 6.9244e-05]
	Learning Rate: 6.92444e-05
	LOSS [training: 0.003059829666191242 | validation: 0.0028261077351886613]
	TIME [epoch: 8.53 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002780203688334513		[learning rate: 6.9e-05]
	Learning Rate: 6.89995e-05
	LOSS [training: 0.002780203688334513 | validation: 0.003034255019291239]
	TIME [epoch: 8.53 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026547778038843077		[learning rate: 6.8756e-05]
	Learning Rate: 6.87555e-05
	LOSS [training: 0.0026547778038843077 | validation: 0.0032464931848349343]
	TIME [epoch: 8.53 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026135744039204258		[learning rate: 6.8512e-05]
	Learning Rate: 6.85124e-05
	LOSS [training: 0.0026135744039204258 | validation: 0.005337706809240219]
	TIME [epoch: 8.54 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029599555494421695		[learning rate: 6.827e-05]
	Learning Rate: 6.82702e-05
	LOSS [training: 0.0029599555494421695 | validation: 0.003524069685517089]
	TIME [epoch: 8.53 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025021198395523817		[learning rate: 6.8029e-05]
	Learning Rate: 6.80287e-05
	LOSS [training: 0.0025021198395523817 | validation: 0.0036942159334521713]
	TIME [epoch: 8.52 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002819237350611575		[learning rate: 6.7788e-05]
	Learning Rate: 6.77882e-05
	LOSS [training: 0.002819237350611575 | validation: 0.003458554516458393]
	TIME [epoch: 8.52 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002695441382529399		[learning rate: 6.7548e-05]
	Learning Rate: 6.75485e-05
	LOSS [training: 0.002695441382529399 | validation: 0.0029926723458058974]
	TIME [epoch: 8.54 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024662128511802675		[learning rate: 6.731e-05]
	Learning Rate: 6.73096e-05
	LOSS [training: 0.0024662128511802675 | validation: 0.003032461207054338]
	TIME [epoch: 8.53 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002729359470789724		[learning rate: 6.7072e-05]
	Learning Rate: 6.70716e-05
	LOSS [training: 0.002729359470789724 | validation: 0.003705277789183902]
	TIME [epoch: 8.53 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002635068759642333		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 0.002635068759642333 | validation: 0.003997580829065237]
	TIME [epoch: 8.53 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002990741247182626		[learning rate: 6.6598e-05]
	Learning Rate: 6.65981e-05
	LOSS [training: 0.002990741247182626 | validation: 0.0035139599935964335]
	TIME [epoch: 8.52 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002397159207281645		[learning rate: 6.6363e-05]
	Learning Rate: 6.63626e-05
	LOSS [training: 0.002397159207281645 | validation: 0.0029043042807607895]
	TIME [epoch: 8.54 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002376564036379473		[learning rate: 6.6128e-05]
	Learning Rate: 6.61279e-05
	LOSS [training: 0.002376564036379473 | validation: 0.003301454568701261]
	TIME [epoch: 8.53 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002738083197793668		[learning rate: 6.5894e-05]
	Learning Rate: 6.58941e-05
	LOSS [training: 0.002738083197793668 | validation: 0.0035462698892340986]
	TIME [epoch: 8.53 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003151080239770297		[learning rate: 6.5661e-05]
	Learning Rate: 6.5661e-05
	LOSS [training: 0.003151080239770297 | validation: 0.003353718782721756]
	TIME [epoch: 8.52 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002683538424221568		[learning rate: 6.5429e-05]
	Learning Rate: 6.54289e-05
	LOSS [training: 0.002683538424221568 | validation: 0.003944679661305113]
	TIME [epoch: 8.53 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002445964780052435		[learning rate: 6.5197e-05]
	Learning Rate: 6.51975e-05
	LOSS [training: 0.002445964780052435 | validation: 0.003602725796866877]
	TIME [epoch: 8.52 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027230063371463135		[learning rate: 6.4967e-05]
	Learning Rate: 6.49669e-05
	LOSS [training: 0.0027230063371463135 | validation: 0.0034393115021475237]
	TIME [epoch: 8.51 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002813044749889892		[learning rate: 6.4737e-05]
	Learning Rate: 6.47372e-05
	LOSS [training: 0.002813044749889892 | validation: 0.003984758757741094]
	TIME [epoch: 8.52 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025059477451317355		[learning rate: 6.4508e-05]
	Learning Rate: 6.45083e-05
	LOSS [training: 0.0025059477451317355 | validation: 0.002830564533107896]
	TIME [epoch: 8.53 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002613055356892115		[learning rate: 6.428e-05]
	Learning Rate: 6.42802e-05
	LOSS [training: 0.002613055356892115 | validation: 0.0028921832276271016]
	TIME [epoch: 8.53 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027732309138257935		[learning rate: 6.4053e-05]
	Learning Rate: 6.40529e-05
	LOSS [training: 0.0027732309138257935 | validation: 0.0030880712438231686]
	TIME [epoch: 8.53 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002385895198226947		[learning rate: 6.3826e-05]
	Learning Rate: 6.38264e-05
	LOSS [training: 0.002385895198226947 | validation: 0.003355694872101249]
	TIME [epoch: 8.53 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002659859394960604		[learning rate: 6.3601e-05]
	Learning Rate: 6.36007e-05
	LOSS [training: 0.002659859394960604 | validation: 0.0031701940410654387]
	TIME [epoch: 8.53 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025148056798825194		[learning rate: 6.3376e-05]
	Learning Rate: 6.33758e-05
	LOSS [training: 0.0025148056798825194 | validation: 0.003612836704753153]
	TIME [epoch: 8.53 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023908846651066996		[learning rate: 6.3152e-05]
	Learning Rate: 6.31517e-05
	LOSS [training: 0.0023908846651066996 | validation: 0.004526443902501212]
	TIME [epoch: 8.52 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025651118954155196		[learning rate: 6.2928e-05]
	Learning Rate: 6.29283e-05
	LOSS [training: 0.0025651118954155196 | validation: 0.004144820076652047]
	TIME [epoch: 8.54 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002294120919577362		[learning rate: 6.2706e-05]
	Learning Rate: 6.27058e-05
	LOSS [training: 0.002294120919577362 | validation: 0.0030792168110990578]
	TIME [epoch: 8.53 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028403924533759355		[learning rate: 6.2484e-05]
	Learning Rate: 6.24841e-05
	LOSS [training: 0.0028403924533759355 | validation: 0.004292157640064335]
	TIME [epoch: 8.54 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002749089812707475		[learning rate: 6.2263e-05]
	Learning Rate: 6.22631e-05
	LOSS [training: 0.002749089812707475 | validation: 0.003531015356593555]
	TIME [epoch: 8.53 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027013752312594255		[learning rate: 6.2043e-05]
	Learning Rate: 6.20429e-05
	LOSS [training: 0.0027013752312594255 | validation: 0.0035317373643272906]
	TIME [epoch: 8.54 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027389501647494593		[learning rate: 6.1824e-05]
	Learning Rate: 6.18235e-05
	LOSS [training: 0.0027389501647494593 | validation: 0.003367711584239535]
	TIME [epoch: 8.53 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002739665397594772		[learning rate: 6.1605e-05]
	Learning Rate: 6.16049e-05
	LOSS [training: 0.002739665397594772 | validation: 0.004077339109739236]
	TIME [epoch: 8.54 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024797952286529346		[learning rate: 6.1387e-05]
	Learning Rate: 6.13871e-05
	LOSS [training: 0.0024797952286529346 | validation: 0.0038155404836208227]
	TIME [epoch: 8.54 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002603780901688009		[learning rate: 6.117e-05]
	Learning Rate: 6.117e-05
	LOSS [training: 0.002603780901688009 | validation: 0.003494727497029962]
	TIME [epoch: 8.53 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025317694884970173		[learning rate: 6.0954e-05]
	Learning Rate: 6.09537e-05
	LOSS [training: 0.0025317694884970173 | validation: 0.002609082359470323]
	TIME [epoch: 8.54 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030396234179381125		[learning rate: 6.0738e-05]
	Learning Rate: 6.07382e-05
	LOSS [training: 0.0030396234179381125 | validation: 0.003936485308128048]
	TIME [epoch: 8.55 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023668175981641456		[learning rate: 6.0523e-05]
	Learning Rate: 6.05234e-05
	LOSS [training: 0.0023668175981641456 | validation: 0.003652277987767847]
	TIME [epoch: 8.53 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002882745587211349		[learning rate: 6.0309e-05]
	Learning Rate: 6.03094e-05
	LOSS [training: 0.002882745587211349 | validation: 0.002958836969368866]
	TIME [epoch: 8.53 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026905995069826166		[learning rate: 6.0096e-05]
	Learning Rate: 6.00961e-05
	LOSS [training: 0.0026905995069826166 | validation: 0.0028749489251565217]
	TIME [epoch: 8.54 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028871240872140403		[learning rate: 5.9884e-05]
	Learning Rate: 5.98836e-05
	LOSS [training: 0.0028871240872140403 | validation: 0.0037395567887823044]
	TIME [epoch: 8.54 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027991509214357743		[learning rate: 5.9672e-05]
	Learning Rate: 5.96718e-05
	LOSS [training: 0.0027991509214357743 | validation: 0.003693061786427794]
	TIME [epoch: 8.54 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002495167151368106		[learning rate: 5.9461e-05]
	Learning Rate: 5.94608e-05
	LOSS [training: 0.002495167151368106 | validation: 0.0033835757224994893]
	TIME [epoch: 8.53 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003120135832598532		[learning rate: 5.9251e-05]
	Learning Rate: 5.92505e-05
	LOSS [training: 0.003120135832598532 | validation: 0.004545894404633005]
	TIME [epoch: 8.53 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002346740831486154		[learning rate: 5.9041e-05]
	Learning Rate: 5.9041e-05
	LOSS [training: 0.002346740831486154 | validation: 0.002866679745725974]
	TIME [epoch: 8.54 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027526146995916286		[learning rate: 5.8832e-05]
	Learning Rate: 5.88323e-05
	LOSS [training: 0.0027526146995916286 | validation: 0.003640421458098831]
	TIME [epoch: 8.54 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025903056578373738		[learning rate: 5.8624e-05]
	Learning Rate: 5.86242e-05
	LOSS [training: 0.0025903056578373738 | validation: 0.004397441685917768]
	TIME [epoch: 8.53 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029274581986639147		[learning rate: 5.8417e-05]
	Learning Rate: 5.84169e-05
	LOSS [training: 0.0029274581986639147 | validation: 0.0036153199546959834]
	TIME [epoch: 8.53 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025450388512669874		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 0.0025450388512669874 | validation: 0.003510303344470329]
	TIME [epoch: 8.53 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002465564774541768		[learning rate: 5.8004e-05]
	Learning Rate: 5.80045e-05
	LOSS [training: 0.002465564774541768 | validation: 0.004268525770284517]
	TIME [epoch: 8.54 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00282791964431635		[learning rate: 5.7799e-05]
	Learning Rate: 5.77994e-05
	LOSS [training: 0.00282791964431635 | validation: 0.0035911663083897983]
	TIME [epoch: 8.52 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002705488481040734		[learning rate: 5.7595e-05]
	Learning Rate: 5.7595e-05
	LOSS [training: 0.002705488481040734 | validation: 0.0034149648182409406]
	TIME [epoch: 8.53 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027070864652424965		[learning rate: 5.7391e-05]
	Learning Rate: 5.73913e-05
	LOSS [training: 0.0027070864652424965 | validation: 0.002787371800652485]
	TIME [epoch: 8.53 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002588727036469001		[learning rate: 5.7188e-05]
	Learning Rate: 5.71884e-05
	LOSS [training: 0.002588727036469001 | validation: 0.0036522036556664085]
	TIME [epoch: 8.54 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025015672963836747		[learning rate: 5.6986e-05]
	Learning Rate: 5.69861e-05
	LOSS [training: 0.0025015672963836747 | validation: 0.002746771469470459]
	TIME [epoch: 8.54 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023102373592780337		[learning rate: 5.6785e-05]
	Learning Rate: 5.67846e-05
	LOSS [training: 0.0023102373592780337 | validation: 0.0032469148553878452]
	TIME [epoch: 8.53 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025115005124561195		[learning rate: 5.6584e-05]
	Learning Rate: 5.65838e-05
	LOSS [training: 0.0025115005124561195 | validation: 0.0032404634578854846]
	TIME [epoch: 8.52 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002524741450621042		[learning rate: 5.6384e-05]
	Learning Rate: 5.63837e-05
	LOSS [training: 0.002524741450621042 | validation: 0.00389261278003236]
	TIME [epoch: 8.55 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025951479829536945		[learning rate: 5.6184e-05]
	Learning Rate: 5.61844e-05
	LOSS [training: 0.0025951479829536945 | validation: 0.00333457139756059]
	TIME [epoch: 8.53 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002550114230793816		[learning rate: 5.5986e-05]
	Learning Rate: 5.59857e-05
	LOSS [training: 0.002550114230793816 | validation: 0.003676179848928329]
	TIME [epoch: 8.54 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024614970203349082		[learning rate: 5.5788e-05]
	Learning Rate: 5.57877e-05
	LOSS [training: 0.0024614970203349082 | validation: 0.004000215989984752]
	TIME [epoch: 8.52 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022872489713232896		[learning rate: 5.559e-05]
	Learning Rate: 5.55904e-05
	LOSS [training: 0.0022872489713232896 | validation: 0.0037961427357472996]
	TIME [epoch: 8.54 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002810755752090624		[learning rate: 5.5394e-05]
	Learning Rate: 5.53939e-05
	LOSS [training: 0.002810755752090624 | validation: 0.004649472847236063]
	TIME [epoch: 8.53 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002457050413533073		[learning rate: 5.5198e-05]
	Learning Rate: 5.5198e-05
	LOSS [training: 0.002457050413533073 | validation: 0.003477179445882423]
	TIME [epoch: 8.53 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002381121323405673		[learning rate: 5.5003e-05]
	Learning Rate: 5.50028e-05
	LOSS [training: 0.002381121323405673 | validation: 0.0029999683900132357]
	TIME [epoch: 8.53 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002557047428573214		[learning rate: 5.4808e-05]
	Learning Rate: 5.48083e-05
	LOSS [training: 0.002557047428573214 | validation: 0.0034164351447061635]
	TIME [epoch: 8.53 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002668316936705626		[learning rate: 5.4614e-05]
	Learning Rate: 5.46145e-05
	LOSS [training: 0.002668316936705626 | validation: 0.003001641784243331]
	TIME [epoch: 8.54 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002823758900063928		[learning rate: 5.4421e-05]
	Learning Rate: 5.44213e-05
	LOSS [training: 0.002823758900063928 | validation: 0.002217161672689449]
	TIME [epoch: 8.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1522.pth
	Model improved!!!
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002458464036996254		[learning rate: 5.4229e-05]
	Learning Rate: 5.42289e-05
	LOSS [training: 0.002458464036996254 | validation: 0.0033790372654033786]
	TIME [epoch: 8.53 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024859672959282398		[learning rate: 5.4037e-05]
	Learning Rate: 5.40371e-05
	LOSS [training: 0.0024859672959282398 | validation: 0.003381784356330206]
	TIME [epoch: 8.53 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002497898627398373		[learning rate: 5.3846e-05]
	Learning Rate: 5.38461e-05
	LOSS [training: 0.002497898627398373 | validation: 0.0038043107477914563]
	TIME [epoch: 8.53 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027334398455954277		[learning rate: 5.3656e-05]
	Learning Rate: 5.36556e-05
	LOSS [training: 0.0027334398455954277 | validation: 0.003467727391424814]
	TIME [epoch: 8.53 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00257158504242884		[learning rate: 5.3466e-05]
	Learning Rate: 5.34659e-05
	LOSS [training: 0.00257158504242884 | validation: 0.0036396058385136884]
	TIME [epoch: 8.52 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002455593990516625		[learning rate: 5.3277e-05]
	Learning Rate: 5.32769e-05
	LOSS [training: 0.002455593990516625 | validation: 0.003385686834337682]
	TIME [epoch: 8.54 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00244167794047712		[learning rate: 5.3088e-05]
	Learning Rate: 5.30884e-05
	LOSS [training: 0.00244167794047712 | validation: 0.0027148630857245495]
	TIME [epoch: 8.53 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002792998521623321		[learning rate: 5.2901e-05]
	Learning Rate: 5.29007e-05
	LOSS [training: 0.002792998521623321 | validation: 0.0034520677677952324]
	TIME [epoch: 8.54 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002660795143579934		[learning rate: 5.2714e-05]
	Learning Rate: 5.27137e-05
	LOSS [training: 0.002660795143579934 | validation: 0.0029828056087862964]
	TIME [epoch: 8.53 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002225687700111522		[learning rate: 5.2527e-05]
	Learning Rate: 5.25272e-05
	LOSS [training: 0.002225687700111522 | validation: 0.0036884643503439585]
	TIME [epoch: 8.52 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025940596818431836		[learning rate: 5.2342e-05]
	Learning Rate: 5.23415e-05
	LOSS [training: 0.0025940596818431836 | validation: 0.003462260245765025]
	TIME [epoch: 8.53 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025946211722731745		[learning rate: 5.2156e-05]
	Learning Rate: 5.21564e-05
	LOSS [training: 0.0025946211722731745 | validation: 0.004107231030652235]
	TIME [epoch: 8.54 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025872199759103326		[learning rate: 5.1972e-05]
	Learning Rate: 5.1972e-05
	LOSS [training: 0.0025872199759103326 | validation: 0.003495195305713091]
	TIME [epoch: 8.53 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002326720321520555		[learning rate: 5.1788e-05]
	Learning Rate: 5.17882e-05
	LOSS [training: 0.002326720321520555 | validation: 0.0033539003873164167]
	TIME [epoch: 8.53 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018935217033080745		[learning rate: 5.1605e-05]
	Learning Rate: 5.16051e-05
	LOSS [training: 0.0018935217033080745 | validation: 0.003241944787016517]
	TIME [epoch: 8.53 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031077242102407086		[learning rate: 5.1423e-05]
	Learning Rate: 5.14226e-05
	LOSS [training: 0.0031077242102407086 | validation: 0.0035861185675069374]
	TIME [epoch: 8.53 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002583291305220909		[learning rate: 5.1241e-05]
	Learning Rate: 5.12407e-05
	LOSS [training: 0.002583291305220909 | validation: 0.0035937117509048442]
	TIME [epoch: 8.54 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002523083515128828		[learning rate: 5.106e-05]
	Learning Rate: 5.10596e-05
	LOSS [training: 0.002523083515128828 | validation: 0.003225798165433694]
	TIME [epoch: 8.53 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027452243945669925		[learning rate: 5.0879e-05]
	Learning Rate: 5.0879e-05
	LOSS [training: 0.0027452243945669925 | validation: 0.003320505980393884]
	TIME [epoch: 8.54 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026023579216873454		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 0.0026023579216873454 | validation: 0.003778881605617775]
	TIME [epoch: 8.54 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002630932910468252		[learning rate: 5.052e-05]
	Learning Rate: 5.05198e-05
	LOSS [training: 0.002630932910468252 | validation: 0.0035427234125076624]
	TIME [epoch: 8.53 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024271374071996163		[learning rate: 5.0341e-05]
	Learning Rate: 5.03412e-05
	LOSS [training: 0.0024271374071996163 | validation: 0.003221536747359262]
	TIME [epoch: 8.52 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002615842183827311		[learning rate: 5.0163e-05]
	Learning Rate: 5.01631e-05
	LOSS [training: 0.002615842183827311 | validation: 0.002738742646510004]
	TIME [epoch: 8.52 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002333486509044043		[learning rate: 4.9986e-05]
	Learning Rate: 4.99857e-05
	LOSS [training: 0.002333486509044043 | validation: 0.0029124920990526693]
	TIME [epoch: 8.55 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002668322468296193		[learning rate: 4.9809e-05]
	Learning Rate: 4.9809e-05
	LOSS [training: 0.002668322468296193 | validation: 0.00275825650958478]
	TIME [epoch: 8.53 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030890050766752626		[learning rate: 4.9633e-05]
	Learning Rate: 4.96329e-05
	LOSS [training: 0.0030890050766752626 | validation: 0.003830231935352061]
	TIME [epoch: 8.53 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002826201496378952		[learning rate: 4.9457e-05]
	Learning Rate: 4.94573e-05
	LOSS [training: 0.002826201496378952 | validation: 0.003279013515773083]
	TIME [epoch: 8.53 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002523557001824652		[learning rate: 4.9282e-05]
	Learning Rate: 4.92825e-05
	LOSS [training: 0.002523557001824652 | validation: 0.0034561423775689186]
	TIME [epoch: 8.55 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026550670556442133		[learning rate: 4.9108e-05]
	Learning Rate: 4.91082e-05
	LOSS [training: 0.0026550670556442133 | validation: 0.0034992469848275525]
	TIME [epoch: 8.54 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002284627370106595		[learning rate: 4.8935e-05]
	Learning Rate: 4.89345e-05
	LOSS [training: 0.002284627370106595 | validation: 0.0041638489308265525]
	TIME [epoch: 8.53 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023047769581064984		[learning rate: 4.8762e-05]
	Learning Rate: 4.87615e-05
	LOSS [training: 0.0023047769581064984 | validation: 0.003482384679800463]
	TIME [epoch: 8.54 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002597334196651968		[learning rate: 4.8589e-05]
	Learning Rate: 4.85891e-05
	LOSS [training: 0.002597334196651968 | validation: 0.0030634826253583512]
	TIME [epoch: 8.55 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025619671896977233		[learning rate: 4.8417e-05]
	Learning Rate: 4.84172e-05
	LOSS [training: 0.0025619671896977233 | validation: 0.0031071724275185783]
	TIME [epoch: 8.53 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002645499613915336		[learning rate: 4.8246e-05]
	Learning Rate: 4.8246e-05
	LOSS [training: 0.002645499613915336 | validation: 0.0028956754854760876]
	TIME [epoch: 8.53 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024870686493937318		[learning rate: 4.8075e-05]
	Learning Rate: 4.80754e-05
	LOSS [training: 0.0024870686493937318 | validation: 0.003890270351217446]
	TIME [epoch: 8.54 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024650751548280746		[learning rate: 4.7905e-05]
	Learning Rate: 4.79054e-05
	LOSS [training: 0.0024650751548280746 | validation: 0.004566447441184312]
	TIME [epoch: 8.54 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024393798283950945		[learning rate: 4.7736e-05]
	Learning Rate: 4.7736e-05
	LOSS [training: 0.0024393798283950945 | validation: 0.003282663022023702]
	TIME [epoch: 8.54 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027648638728102618		[learning rate: 4.7567e-05]
	Learning Rate: 4.75672e-05
	LOSS [training: 0.0027648638728102618 | validation: 0.0036902306340519785]
	TIME [epoch: 8.53 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002568010504118511		[learning rate: 4.7399e-05]
	Learning Rate: 4.7399e-05
	LOSS [training: 0.002568010504118511 | validation: 0.0028182626501848658]
	TIME [epoch: 8.53 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025744020839871865		[learning rate: 4.7231e-05]
	Learning Rate: 4.72314e-05
	LOSS [training: 0.0025744020839871865 | validation: 0.0038479126927702893]
	TIME [epoch: 8.53 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026529136792366056		[learning rate: 4.7064e-05]
	Learning Rate: 4.70644e-05
	LOSS [training: 0.0026529136792366056 | validation: 0.0033147741071584153]
	TIME [epoch: 8.54 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002814585513731366		[learning rate: 4.6898e-05]
	Learning Rate: 4.6898e-05
	LOSS [training: 0.002814585513731366 | validation: 0.0028728756480482735]
	TIME [epoch: 8.52 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023938505181174217		[learning rate: 4.6732e-05]
	Learning Rate: 4.67321e-05
	LOSS [training: 0.0023938505181174217 | validation: 0.0032321399666285127]
	TIME [epoch: 8.54 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025854872459196826		[learning rate: 4.6567e-05]
	Learning Rate: 4.65669e-05
	LOSS [training: 0.0025854872459196826 | validation: 0.0031699262079764837]
	TIME [epoch: 8.54 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002510888493450389		[learning rate: 4.6402e-05]
	Learning Rate: 4.64022e-05
	LOSS [training: 0.002510888493450389 | validation: 0.0033234110005100933]
	TIME [epoch: 8.55 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002683545442100501		[learning rate: 4.6238e-05]
	Learning Rate: 4.62381e-05
	LOSS [training: 0.002683545442100501 | validation: 0.004034455552631421]
	TIME [epoch: 8.54 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002392019744438801		[learning rate: 4.6075e-05]
	Learning Rate: 4.60746e-05
	LOSS [training: 0.002392019744438801 | validation: 0.004328078374012587]
	TIME [epoch: 8.54 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024542313257121416		[learning rate: 4.5912e-05]
	Learning Rate: 4.59117e-05
	LOSS [training: 0.0024542313257121416 | validation: 0.003190839660579373]
	TIME [epoch: 8.54 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003010369247330659		[learning rate: 4.5749e-05]
	Learning Rate: 4.57493e-05
	LOSS [training: 0.003010369247330659 | validation: 0.004477812995724766]
	TIME [epoch: 8.56 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003481613532023753		[learning rate: 4.5588e-05]
	Learning Rate: 4.55875e-05
	LOSS [training: 0.003481613532023753 | validation: 0.004082473774504069]
	TIME [epoch: 8.54 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003106846330017279		[learning rate: 4.5426e-05]
	Learning Rate: 4.54264e-05
	LOSS [training: 0.003106846330017279 | validation: 0.0034166874431068293]
	TIME [epoch: 8.54 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003188254985508513		[learning rate: 4.5266e-05]
	Learning Rate: 4.52657e-05
	LOSS [training: 0.003188254985508513 | validation: 0.0032534963163998393]
	TIME [epoch: 8.54 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002602599060509977		[learning rate: 4.5106e-05]
	Learning Rate: 4.51056e-05
	LOSS [training: 0.002602599060509977 | validation: 0.003292336648043559]
	TIME [epoch: 8.55 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024780325622701578		[learning rate: 4.4946e-05]
	Learning Rate: 4.49461e-05
	LOSS [training: 0.0024780325622701578 | validation: 0.003047433582174162]
	TIME [epoch: 8.54 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002652008858613676		[learning rate: 4.4787e-05]
	Learning Rate: 4.47872e-05
	LOSS [training: 0.002652008858613676 | validation: 0.0030451128910078413]
	TIME [epoch: 8.54 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002356073663377498		[learning rate: 4.4629e-05]
	Learning Rate: 4.46288e-05
	LOSS [training: 0.002356073663377498 | validation: 0.004685848286861814]
	TIME [epoch: 8.54 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026486170685896776		[learning rate: 4.4471e-05]
	Learning Rate: 4.4471e-05
	LOSS [training: 0.0026486170685896776 | validation: 0.0033544334737410403]
	TIME [epoch: 8.55 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026516964580330276		[learning rate: 4.4314e-05]
	Learning Rate: 4.43138e-05
	LOSS [training: 0.0026516964580330276 | validation: 0.0024915223191663005]
	TIME [epoch: 8.55 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002430297373478031		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 0.002430297373478031 | validation: 0.0031998905960762434]
	TIME [epoch: 8.53 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002515693482524103		[learning rate: 4.4001e-05]
	Learning Rate: 4.40009e-05
	LOSS [training: 0.002515693482524103 | validation: 0.0034487762220952937]
	TIME [epoch: 8.54 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002334596593296512		[learning rate: 4.3845e-05]
	Learning Rate: 4.38453e-05
	LOSS [training: 0.002334596593296512 | validation: 0.0038335734084928097]
	TIME [epoch: 8.54 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002268143972447877		[learning rate: 4.369e-05]
	Learning Rate: 4.36903e-05
	LOSS [training: 0.002268143972447877 | validation: 0.0032315711284585558]
	TIME [epoch: 8.54 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002583125860731768		[learning rate: 4.3536e-05]
	Learning Rate: 4.35358e-05
	LOSS [training: 0.002583125860731768 | validation: 0.003207279012830102]
	TIME [epoch: 8.55 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002432489727801261		[learning rate: 4.3382e-05]
	Learning Rate: 4.33818e-05
	LOSS [training: 0.002432489727801261 | validation: 0.0033317822182581426]
	TIME [epoch: 8.54 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033250779718219893		[learning rate: 4.3228e-05]
	Learning Rate: 4.32284e-05
	LOSS [training: 0.0033250779718219893 | validation: 0.0030622388285153125]
	TIME [epoch: 8.53 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002634475442177325		[learning rate: 4.3076e-05]
	Learning Rate: 4.30756e-05
	LOSS [training: 0.002634475442177325 | validation: 0.0036174867613724157]
	TIME [epoch: 8.54 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026738111124063662		[learning rate: 4.2923e-05]
	Learning Rate: 4.29232e-05
	LOSS [training: 0.0026738111124063662 | validation: 0.00275882060359442]
	TIME [epoch: 8.52 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027643187506250807		[learning rate: 4.2771e-05]
	Learning Rate: 4.27714e-05
	LOSS [training: 0.0027643187506250807 | validation: 0.0024727303575341796]
	TIME [epoch: 8.54 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026514664044208335		[learning rate: 4.262e-05]
	Learning Rate: 4.26202e-05
	LOSS [training: 0.0026514664044208335 | validation: 0.0034619591246527557]
	TIME [epoch: 8.54 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026461683453782234		[learning rate: 4.2469e-05]
	Learning Rate: 4.24695e-05
	LOSS [training: 0.0026461683453782234 | validation: 0.0025855251539136396]
	TIME [epoch: 8.55 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023867631688005886		[learning rate: 4.2319e-05]
	Learning Rate: 4.23193e-05
	LOSS [training: 0.0023867631688005886 | validation: 0.0029592056713589427]
	TIME [epoch: 8.55 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020741123015554585		[learning rate: 4.217e-05]
	Learning Rate: 4.21697e-05
	LOSS [training: 0.0020741123015554585 | validation: 0.0045101284413104695]
	TIME [epoch: 8.55 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024570211696499766		[learning rate: 4.2021e-05]
	Learning Rate: 4.20205e-05
	LOSS [training: 0.0024570211696499766 | validation: 0.0038908240090711173]
	TIME [epoch: 8.54 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021008155876575457		[learning rate: 4.1872e-05]
	Learning Rate: 4.18719e-05
	LOSS [training: 0.0021008155876575457 | validation: 0.0025369463196269074]
	TIME [epoch: 8.54 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002190922593061161		[learning rate: 4.1724e-05]
	Learning Rate: 4.17239e-05
	LOSS [training: 0.002190922593061161 | validation: 0.0036655607966250567]
	TIME [epoch: 8.55 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002255161053665357		[learning rate: 4.1576e-05]
	Learning Rate: 4.15763e-05
	LOSS [training: 0.002255161053665357 | validation: 0.0035979943085281568]
	TIME [epoch: 8.54 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002600978327458929		[learning rate: 4.1429e-05]
	Learning Rate: 4.14293e-05
	LOSS [training: 0.002600978327458929 | validation: 0.003628160323481979]
	TIME [epoch: 8.54 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026803917456328027		[learning rate: 4.1283e-05]
	Learning Rate: 4.12828e-05
	LOSS [training: 0.0026803917456328027 | validation: 0.004479101920315269]
	TIME [epoch: 8.55 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026875871131000705		[learning rate: 4.1137e-05]
	Learning Rate: 4.11368e-05
	LOSS [training: 0.0026875871131000705 | validation: 0.002604951147140839]
	TIME [epoch: 8.56 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002628608725682352		[learning rate: 4.0991e-05]
	Learning Rate: 4.09914e-05
	LOSS [training: 0.002628608725682352 | validation: 0.003335279153476976]
	TIME [epoch: 8.54 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028249042439692455		[learning rate: 4.0846e-05]
	Learning Rate: 4.08464e-05
	LOSS [training: 0.0028249042439692455 | validation: 0.003599821978419284]
	TIME [epoch: 8.54 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023926480114633067		[learning rate: 4.0702e-05]
	Learning Rate: 4.0702e-05
	LOSS [training: 0.0023926480114633067 | validation: 0.002734286142347998]
	TIME [epoch: 8.53 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002389468789204124		[learning rate: 4.0558e-05]
	Learning Rate: 4.0558e-05
	LOSS [training: 0.002389468789204124 | validation: 0.0032856346248110306]
	TIME [epoch: 8.55 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002606993516261309		[learning rate: 4.0415e-05]
	Learning Rate: 4.04146e-05
	LOSS [training: 0.002606993516261309 | validation: 0.003186068635403287]
	TIME [epoch: 8.54 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026639301225385337		[learning rate: 4.0272e-05]
	Learning Rate: 4.02717e-05
	LOSS [training: 0.0026639301225385337 | validation: 0.00385118711364475]
	TIME [epoch: 8.54 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026775502659094454		[learning rate: 4.0129e-05]
	Learning Rate: 4.01293e-05
	LOSS [training: 0.0026775502659094454 | validation: 0.0037357877155036637]
	TIME [epoch: 8.53 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002287798470184413		[learning rate: 3.9987e-05]
	Learning Rate: 3.99874e-05
	LOSS [training: 0.002287798470184413 | validation: 0.0033466819504901157]
	TIME [epoch: 8.56 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025108386511118646		[learning rate: 3.9846e-05]
	Learning Rate: 3.9846e-05
	LOSS [training: 0.0025108386511118646 | validation: 0.0036096885791096125]
	TIME [epoch: 8.54 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022731188544281226		[learning rate: 3.9705e-05]
	Learning Rate: 3.97051e-05
	LOSS [training: 0.0022731188544281226 | validation: 0.0034804956334146685]
	TIME [epoch: 8.54 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002504483693843467		[learning rate: 3.9565e-05]
	Learning Rate: 3.95647e-05
	LOSS [training: 0.002504483693843467 | validation: 0.002241229825225777]
	TIME [epoch: 8.54 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002430753863569625		[learning rate: 3.9425e-05]
	Learning Rate: 3.94248e-05
	LOSS [training: 0.002430753863569625 | validation: 0.00280947952294133]
	TIME [epoch: 8.55 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024231832613246878		[learning rate: 3.9285e-05]
	Learning Rate: 3.92854e-05
	LOSS [training: 0.0024231832613246878 | validation: 0.0033907503366887113]
	TIME [epoch: 8.54 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024250511449341312		[learning rate: 3.9146e-05]
	Learning Rate: 3.91464e-05
	LOSS [training: 0.0024250511449341312 | validation: 0.0023996219685183382]
	TIME [epoch: 8.53 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025769069454172067		[learning rate: 3.9008e-05]
	Learning Rate: 3.9008e-05
	LOSS [training: 0.0025769069454172067 | validation: 0.00364733964137712]
	TIME [epoch: 8.55 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024079312457564415		[learning rate: 3.887e-05]
	Learning Rate: 3.88701e-05
	LOSS [training: 0.0024079312457564415 | validation: 0.002527791389690617]
	TIME [epoch: 8.55 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025448039757458335		[learning rate: 3.8733e-05]
	Learning Rate: 3.87326e-05
	LOSS [training: 0.0025448039757458335 | validation: 0.003067717476361507]
	TIME [epoch: 8.55 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024257804493497035		[learning rate: 3.8596e-05]
	Learning Rate: 3.85957e-05
	LOSS [training: 0.0024257804493497035 | validation: 0.003364705016537269]
	TIME [epoch: 8.55 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002529371082871582		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 0.002529371082871582 | validation: 0.0033191185441365277]
	TIME [epoch: 8.55 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002322306943450006		[learning rate: 3.8323e-05]
	Learning Rate: 3.83232e-05
	LOSS [training: 0.002322306943450006 | validation: 0.0038483265702591815]
	TIME [epoch: 8.56 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003066458988716631		[learning rate: 3.8188e-05]
	Learning Rate: 3.81877e-05
	LOSS [training: 0.003066458988716631 | validation: 0.004314078227532843]
	TIME [epoch: 8.55 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024424103756655754		[learning rate: 3.8053e-05]
	Learning Rate: 3.80526e-05
	LOSS [training: 0.0024424103756655754 | validation: 0.0023034511848428964]
	TIME [epoch: 8.56 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_presample_20240627_193236/states/model_phi1_1a_v_mmd1_presample_1623.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 14072.543 seconds.
