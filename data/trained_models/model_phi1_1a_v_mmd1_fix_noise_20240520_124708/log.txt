Args:
Namespace(name='model_phi1_1a_v_mmd1_fix_noise', outdir='out/model_training/model_phi1_1a_v_mmd1_fix_noise', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.5, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 853047587

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.251811850749675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.251811850749675 | validation: 3.9265132492485084]
	TIME [epoch: 148 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7468659795484083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7468659795484083 | validation: 3.532510187851373]
	TIME [epoch: 8.41 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.496535879305757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.496535879305757 | validation: 3.4621034333832785]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.287445317662954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.287445317662954 | validation: 3.2689895031688803]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1474903016275473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1474903016275473 | validation: 3.137570820578464]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0072803547675546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0072803547675546 | validation: 2.993305103387181]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.806121617382093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.806121617382093 | validation: 2.832022378851021]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6140739815511185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6140739815511185 | validation: 2.7310335651003426]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3972067531665324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3972067531665324 | validation: 2.716160355077144]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2986650438932505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2986650438932505 | validation: 2.637353213792891]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1641918778577893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1641918778577893 | validation: 2.940039458125993]
	TIME [epoch: 8.3 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.249040341818903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.249040341818903 | validation: 2.637417781037352]
	TIME [epoch: 8.31 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0690758327868575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0690758327868575 | validation: 2.8616358240769832]
	TIME [epoch: 8.3 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1085289185041867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1085289185041867 | validation: 2.612181421383049]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0427577104068777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0427577104068777 | validation: 2.526672744995608]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9601835511046746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9601835511046746 | validation: 2.6644814036268953]
	TIME [epoch: 8.33 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.001218700076227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.001218700076227 | validation: 2.4540385535733913]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7991860346864468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7991860346864468 | validation: 2.9849712437819598]
	TIME [epoch: 8.32 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0618058090257687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0618058090257687 | validation: 2.371379861953641]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.733977507358031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.733977507358031 | validation: 2.36282539810414]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7744931894339728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7744931894339728 | validation: 2.3255341334658994]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.675327976324085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.675327976324085 | validation: 2.2425792046539597]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6898835349608734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6898835349608734 | validation: 2.3415083003794583]
	TIME [epoch: 8.32 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6466648270933093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6466648270933093 | validation: 2.319204842612292]
	TIME [epoch: 8.31 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.700298134792288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.700298134792288 | validation: 2.2311415666067127]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5811626826193215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5811626826193215 | validation: 2.181359610994765]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5769515609727753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5769515609727753 | validation: 2.288409506863454]
	TIME [epoch: 8.32 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5774138519368368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5774138519368368 | validation: 2.188312307252529]
	TIME [epoch: 8.32 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.599162463379897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.599162463379897 | validation: 2.2307571258048977]
	TIME [epoch: 8.32 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5320428683460647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5320428683460647 | validation: 2.180961412794873]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5481149704388562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5481149704388562 | validation: 2.3290196845834537]
	TIME [epoch: 8.34 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5496741688011655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5496741688011655 | validation: 2.152918693390075]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6377225899897643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6377225899897643 | validation: 2.224867590691922]
	TIME [epoch: 8.31 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5556534403190398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5556534403190398 | validation: 2.1741799850496006]
	TIME [epoch: 8.31 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5122384405841678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5122384405841678 | validation: 2.1547436769914468]
	TIME [epoch: 8.3 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.478353729783406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.478353729783406 | validation: 2.114874402079688]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.455816461079762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.455816461079762 | validation: 2.1594551057842803]
	TIME [epoch: 8.38 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4719820479207477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4719820479207477 | validation: 2.247435480057055]
	TIME [epoch: 8.34 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5179158488190763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5179158488190763 | validation: 2.1439353236596843]
	TIME [epoch: 8.32 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4554102999934995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4554102999934995 | validation: 2.283621668211711]
	TIME [epoch: 8.32 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2043955415286045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2043955415286045 | validation: 2.9275675384630033]
	TIME [epoch: 8.31 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9789228546510351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9789228546510351 | validation: 2.128471906484627]
	TIME [epoch: 8.34 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4402887434365137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4402887434365137 | validation: 2.0316586539022685]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8986148977204167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8986148977204167 | validation: 2.2487927147642166]
	TIME [epoch: 8.32 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5798372804797085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5798372804797085 | validation: 2.1238257659987774]
	TIME [epoch: 8.31 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4208036194598597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4208036194598597 | validation: 2.0671520737352593]
	TIME [epoch: 8.31 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5992466778573164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5992466778573164 | validation: 2.2307876790586914]
	TIME [epoch: 8.31 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4830456080428072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4830456080428072 | validation: 2.093085510509483]
	TIME [epoch: 8.4 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4242073545789378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4242073545789378 | validation: 2.064569271150641]
	TIME [epoch: 8.38 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4095721807527366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4095721807527366 | validation: 2.064186801908779]
	TIME [epoch: 8.36 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4782359024880836		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.4782359024880836 | validation: 2.0992497391947182]
	TIME [epoch: 8.32 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4821950783124347		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.4821950783124347 | validation: 2.1112682579985593]
	TIME [epoch: 8.31 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4065016263667247		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.4065016263667247 | validation: 2.090682579272606]
	TIME [epoch: 8.31 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.430934741075035		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.430934741075035 | validation: 2.5983237315123615]
	TIME [epoch: 8.35 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.959730362884346		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.959730362884346 | validation: 2.176336927566298]
	TIME [epoch: 8.31 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5655221156636807		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.5655221156636807 | validation: 2.1168299145763205]
	TIME [epoch: 8.36 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4269277840978967		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.4269277840978967 | validation: 2.094178017459942]
	TIME [epoch: 8.38 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3751250114199909		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.3751250114199909 | validation: 2.0754434431546622]
	TIME [epoch: 8.37 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4391892662170989		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.4391892662170989 | validation: 2.0908380655097334]
	TIME [epoch: 8.4 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3830149618607646		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.3830149618607646 | validation: 2.050809743360066]
	TIME [epoch: 8.39 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.39153993483679		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 1.39153993483679 | validation: 2.058400094694849]
	TIME [epoch: 8.31 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3711657503665138		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 1.3711657503665138 | validation: 2.209748520089571]
	TIME [epoch: 8.31 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4730325295009368		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 1.4730325295009368 | validation: 2.034017061488636]
	TIME [epoch: 8.31 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4664386093197797		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 1.4664386093197797 | validation: 2.0485866856822845]
	TIME [epoch: 8.31 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4895352843323875		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 1.4895352843323875 | validation: 2.040123374247325]
	TIME [epoch: 8.36 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.56301276782412		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 1.56301276782412 | validation: 1.9764411206648806]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3651428464272164		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 1.3651428464272164 | validation: 1.7855561172785164]
	TIME [epoch: 8.42 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2938193365362565		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 1.2938193365362565 | validation: 1.4333545888580754]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2350610162879843		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 1.2350610162879843 | validation: 1.0762290686881937]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9783134424977384		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.9783134424977384 | validation: 1.0356396252372884]
	TIME [epoch: 8.39 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9820255380776646		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.9820255380776646 | validation: 0.8341695126903479]
	TIME [epoch: 8.41 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6857720994910976		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.6857720994910976 | validation: 0.5629549364371049]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6126593644769602		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.6126593644769602 | validation: 0.5866371877207879]
	TIME [epoch: 8.33 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5337440256297002		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.5337440256297002 | validation: 0.61075963082812]
	TIME [epoch: 8.33 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6631748807688194		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.6631748807688194 | validation: 0.603132064296533]
	TIME [epoch: 8.33 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5454676987119549		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.5454676987119549 | validation: 0.5111504667293236]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7722020092509823		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.7722020092509823 | validation: 0.5395384219033352]
	TIME [epoch: 8.32 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5459936877547548		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.5459936877547548 | validation: 0.48463036799438314]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4914820117738494		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.4914820117738494 | validation: 0.475870995298189]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4732829368818662		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.4732829368818662 | validation: 0.4809787326912421]
	TIME [epoch: 8.3 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4809492118734949		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.4809492118734949 | validation: 0.4219363281684646]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.405141247308942		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 2.405141247308942 | validation: 1.4937498184193774]
	TIME [epoch: 8.34 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.08210500951357		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 1.08210500951357 | validation: 1.0932450840217927]
	TIME [epoch: 8.3 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7494637662662998		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.7494637662662998 | validation: 0.6003704501449036]
	TIME [epoch: 8.3 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5240231367367998		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.5240231367367998 | validation: 0.5249588030750922]
	TIME [epoch: 8.3 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4884500664355478		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.4884500664355478 | validation: 0.4769829924741783]
	TIME [epoch: 8.3 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4809086774208413		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.4809086774208413 | validation: 0.5109195743531936]
	TIME [epoch: 8.35 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.50146855185697		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.50146855185697 | validation: 0.5052055300293816]
	TIME [epoch: 8.31 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5661520729233078		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.5661520729233078 | validation: 0.4823733925204466]
	TIME [epoch: 8.31 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4275374382333398		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.4275374382333398 | validation: 0.3700604517552665]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42269936716454265		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.42269936716454265 | validation: 0.592866999739364]
	TIME [epoch: 8.31 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44122942312648683		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.44122942312648683 | validation: 0.3830469171926684]
	TIME [epoch: 8.34 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3800708984239667		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.3800708984239667 | validation: 0.3705013617720756]
	TIME [epoch: 8.35 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9600909778991151		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.9600909778991151 | validation: 0.8570116279414975]
	TIME [epoch: 8.31 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7510966291594404		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.7510966291594404 | validation: 0.5053612170138401]
	TIME [epoch: 8.3 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5463601861082439		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.5463601861082439 | validation: 0.46174298181221973]
	TIME [epoch: 8.3 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5228567087721326		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.5228567087721326 | validation: 0.42781572991400707]
	TIME [epoch: 8.31 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5140449317341321		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.5140449317341321 | validation: 0.4244984155937471]
	TIME [epoch: 8.35 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48890919677211475		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.48890919677211475 | validation: 0.4208625158704457]
	TIME [epoch: 8.33 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3929139276035023		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.3929139276035023 | validation: 0.3534727940645631]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5429945926100885		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.5429945926100885 | validation: 0.5555198599979138]
	TIME [epoch: 8.4 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44934736199365094		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.44934736199365094 | validation: 1.4124985865665591]
	TIME [epoch: 8.68 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9251775079257795		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.9251775079257795 | validation: 0.5424496104735237]
	TIME [epoch: 8.39 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46245201625943577		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.46245201625943577 | validation: 0.4985718247469456]
	TIME [epoch: 8.39 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5918659492863417		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.5918659492863417 | validation: 0.5083035667965726]
	TIME [epoch: 8.33 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37208601768889665		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.37208601768889665 | validation: 0.33492639005665614]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32438392709659825		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.32438392709659825 | validation: 0.2964274376172694]
	TIME [epoch: 8.48 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3034383526759602		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.3034383526759602 | validation: 0.28786096204589884]
	TIME [epoch: 8.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29794743127811085		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.29794743127811085 | validation: 0.3792443355013173]
	TIME [epoch: 8.49 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3445719957497645		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.3445719957497645 | validation: 0.47055313237297874]
	TIME [epoch: 8.38 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40813951150236305		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.40813951150236305 | validation: 1.4532958262401996]
	TIME [epoch: 8.34 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9079546042236083		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.9079546042236083 | validation: 0.5603471398502735]
	TIME [epoch: 8.32 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4697109009233613		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.4697109009233613 | validation: 0.4677482806397119]
	TIME [epoch: 8.32 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40801602915965407		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.40801602915965407 | validation: 0.457084249742435]
	TIME [epoch: 8.33 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3976996436986387		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.3976996436986387 | validation: 0.46570767895603604]
	TIME [epoch: 8.35 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39417487726788725		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.39417487726788725 | validation: 0.4577096268628513]
	TIME [epoch: 8.32 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4555595371374138		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.4555595371374138 | validation: 0.5551924937808889]
	TIME [epoch: 8.31 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48223530281961746		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.48223530281961746 | validation: 0.4861988993010799]
	TIME [epoch: 8.32 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4091823041238075		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.4091823041238075 | validation: 0.4540553625565127]
	TIME [epoch: 8.32 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3856783590994536		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.3856783590994536 | validation: 0.4616714445223651]
	TIME [epoch: 8.34 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38873895348201576		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.38873895348201576 | validation: 0.4534896428972078]
	TIME [epoch: 8.33 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42514364887001366		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.42514364887001366 | validation: 0.42881675693194043]
	TIME [epoch: 8.31 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3579916194284835		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.3579916194284835 | validation: 0.6856017632115547]
	TIME [epoch: 8.31 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4646180517690115		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.4646180517690115 | validation: 0.37837666589152386]
	TIME [epoch: 8.31 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.373348231368277		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.373348231368277 | validation: 0.4004786964779743]
	TIME [epoch: 8.32 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34900688190976814		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.34900688190976814 | validation: 0.43485788688482796]
	TIME [epoch: 8.36 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3313627798511687		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.3313627798511687 | validation: 0.3219872283398797]
	TIME [epoch: 8.31 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3377128308968738		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.3377128308968738 | validation: 0.30604818968968783]
	TIME [epoch: 8.31 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2998036918035327		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.2998036918035327 | validation: 0.2948577431482694]
	TIME [epoch: 8.32 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2293109320027159		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.2293109320027159 | validation: 0.25045693666441404]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2713814538455568		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.2713814538455568 | validation: 0.24957770868570406]
	TIME [epoch: 8.42 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2613417350637186		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.2613417350637186 | validation: 0.21283613128074588]
	TIME [epoch: 8.47 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21286035137388334		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.21286035137388334 | validation: 0.26697885052825565]
	TIME [epoch: 8.41 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3609183368086989		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.3609183368086989 | validation: 0.2664541812088852]
	TIME [epoch: 8.32 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22267413317973433		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.22267413317973433 | validation: 0.20834475419248114]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23421817827682828		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.23421817827682828 | validation: 0.24984915794367277]
	TIME [epoch: 8.4 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21001811470980458		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.21001811470980458 | validation: 0.20716009536801927]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2459738460889812		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.2459738460889812 | validation: 0.2415839635959623]
	TIME [epoch: 8.44 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18581418082981857		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.18581418082981857 | validation: 0.32789397749270766]
	TIME [epoch: 8.32 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3818912862768997		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.3818912862768997 | validation: 0.384642798270321]
	TIME [epoch: 8.32 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25352146693677124		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.25352146693677124 | validation: 0.2622911489201637]
	TIME [epoch: 8.32 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28306877967316385		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.28306877967316385 | validation: 0.24061915515809262]
	TIME [epoch: 8.35 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1942362775207464		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.1942362775207464 | validation: 0.30396660778854323]
	TIME [epoch: 8.36 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7418889876585173		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.7418889876585173 | validation: 0.7831853517501046]
	TIME [epoch: 8.37 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6223771586183913		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.6223771586183913 | validation: 0.5277572375553162]
	TIME [epoch: 8.37 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3851944688804695		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.3851944688804695 | validation: 0.3661685896961488]
	TIME [epoch: 8.34 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31666244728104337		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.31666244728104337 | validation: 0.34005423101930776]
	TIME [epoch: 8.33 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29063888591232845		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.29063888591232845 | validation: 0.3191016309449988]
	TIME [epoch: 8.36 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2970841697477502		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.2970841697477502 | validation: 0.30757488867319704]
	TIME [epoch: 8.33 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2637950453673267		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.2637950453673267 | validation: 0.269660038118555]
	TIME [epoch: 8.32 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24343375933834338		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.24343375933834338 | validation: 0.2606035749506706]
	TIME [epoch: 8.32 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20927804804452935		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.20927804804452935 | validation: 0.21607705350433482]
	TIME [epoch: 8.32 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19917273714182268		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.19917273714182268 | validation: 0.26516389993374445]
	TIME [epoch: 8.34 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21058284651646741		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.21058284651646741 | validation: 0.2067657169442233]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18538973018812754		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.18538973018812754 | validation: 0.19059598263451094]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15382108914037868		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.15382108914037868 | validation: 0.41587529439954074]
	TIME [epoch: 8.31 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2610304266686443		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.2610304266686443 | validation: 0.1665298371516114]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20365678289364955		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.20365678289364955 | validation: 0.2591522386653788]
	TIME [epoch: 8.32 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2067849616138926		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.2067849616138926 | validation: 0.1774544128666155]
	TIME [epoch: 8.35 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3814539719823913		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.3814539719823913 | validation: 0.48392705354220367]
	TIME [epoch: 8.33 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3268583128444863		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.3268583128444863 | validation: 0.16724917652209653]
	TIME [epoch: 8.32 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17478486652418085		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.17478486652418085 | validation: 0.2657788900367078]
	TIME [epoch: 8.31 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19101242560014958		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.19101242560014958 | validation: 0.1819003938667264]
	TIME [epoch: 8.3 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16073485426252623		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.16073485426252623 | validation: 0.20683774319912757]
	TIME [epoch: 8.31 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17722248213109945		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.17722248213109945 | validation: 0.1959655111178264]
	TIME [epoch: 8.34 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1720629731976211		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.1720629731976211 | validation: 0.3849888528647318]
	TIME [epoch: 8.3 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28742458085411726		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.28742458085411726 | validation: 0.17269640145399556]
	TIME [epoch: 8.3 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15771681063232929		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.15771681063232929 | validation: 0.22156965581436977]
	TIME [epoch: 8.3 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17893102097033903		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.17893102097033903 | validation: 0.13991100153371924]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1340036369182903		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.1340036369182903 | validation: 0.2110136657977854]
	TIME [epoch: 8.33 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2592969715329259		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.2592969715329259 | validation: 0.2761446592707223]
	TIME [epoch: 8.34 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23398142021359575		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.23398142021359575 | validation: 0.22974438151883952]
	TIME [epoch: 8.31 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17099647143161043		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.17099647143161043 | validation: 0.17007738651923654]
	TIME [epoch: 8.3 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14084769988709075		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.14084769988709075 | validation: 0.16033538320862875]
	TIME [epoch: 8.31 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2980802551541325		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.2980802551541325 | validation: 0.4065749338658661]
	TIME [epoch: 8.31 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29690684441521226		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.29690684441521226 | validation: 0.17676909106612793]
	TIME [epoch: 8.34 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16888938155474		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.16888938155474 | validation: 0.19014182965868498]
	TIME [epoch: 8.3 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17241578049561238		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.17241578049561238 | validation: 0.13693378753547694]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12954492913385593		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.12954492913385593 | validation: 0.13149698023098633]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1925796554552212		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.1925796554552212 | validation: 0.14531302936937085]
	TIME [epoch: 8.32 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1604458389320211		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.1604458389320211 | validation: 0.15323360423832677]
	TIME [epoch: 8.33 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13849387169254243		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.13849387169254243 | validation: 0.13371531224875166]
	TIME [epoch: 8.36 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1624676887381455		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.1624676887381455 | validation: 0.15044658960341278]
	TIME [epoch: 8.32 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12896870463111806		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.12896870463111806 | validation: 0.16381371952113488]
	TIME [epoch: 8.32 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29355628692351865		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.29355628692351865 | validation: 0.31167748188194266]
	TIME [epoch: 8.32 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19907863850720228		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.19907863850720228 | validation: 0.23902934547280133]
	TIME [epoch: 8.31 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19891604293176632		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.19891604293176632 | validation: 0.1783217516467159]
	TIME [epoch: 8.35 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1464045671142113		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.1464045671142113 | validation: 0.19244366844167476]
	TIME [epoch: 8.34 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19146027111161162		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.19146027111161162 | validation: 0.18282279113892566]
	TIME [epoch: 8.32 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15625145318207373		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.15625145318207373 | validation: 0.1909524784911043]
	TIME [epoch: 8.31 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1539181167972069		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.1539181167972069 | validation: 0.1514294363108018]
	TIME [epoch: 8.31 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15777592726986084		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.15777592726986084 | validation: 0.17292088160450553]
	TIME [epoch: 8.32 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1529553037722199		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.1529553037722199 | validation: 0.182802977710052]
	TIME [epoch: 8.37 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18525825243713415		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.18525825243713415 | validation: 0.12909151886503353]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.160890723887125		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.160890723887125 | validation: 0.14094467961429863]
	TIME [epoch: 8.3 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23781291363564544		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.23781291363564544 | validation: 0.19178152816490998]
	TIME [epoch: 8.31 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2874927256488507		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.2874927256488507 | validation: 0.2484423105706628]
	TIME [epoch: 8.31 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21378787255356327		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.21378787255356327 | validation: 0.1715661533400497]
	TIME [epoch: 8.33 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14195204139227705		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.14195204139227705 | validation: 0.13515906833748137]
	TIME [epoch: 8.34 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12884053886235614		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.12884053886235614 | validation: 0.12968935274399662]
	TIME [epoch: 8.31 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15005949590942955		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.15005949590942955 | validation: 0.13273254004492613]
	TIME [epoch: 8.31 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13158706082911137		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.13158706082911137 | validation: 0.13944701776073457]
	TIME [epoch: 8.33 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1557190474996723		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.1557190474996723 | validation: 0.13826362495631278]
	TIME [epoch: 8.33 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12179038020933036		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.12179038020933036 | validation: 0.13743089285185728]
	TIME [epoch: 8.37 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13520928510650088		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.13520928510650088 | validation: 0.15779191550522675]
	TIME [epoch: 8.33 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19309483115613318		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.19309483115613318 | validation: 0.12950395192809144]
	TIME [epoch: 8.31 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12923256350607046		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.12923256350607046 | validation: 0.1472128488498687]
	TIME [epoch: 8.31 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1429851069480455		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.1429851069480455 | validation: 0.11768666292226915]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2605129406145917		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.2605129406145917 | validation: 0.9459883107212148]
	TIME [epoch: 8.33 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5454272859576079		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.5454272859576079 | validation: 0.5644174521527883]
	TIME [epoch: 8.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26584423647555966		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.26584423647555966 | validation: 0.24485670602207144]
	TIME [epoch: 8.32 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19957166358375977		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.19957166358375977 | validation: 0.1828731169755906]
	TIME [epoch: 8.32 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15114633301541125		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.15114633301541125 | validation: 0.22067672282960954]
	TIME [epoch: 8.31 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1887851498714228		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.1887851498714228 | validation: 0.17033397439801315]
	TIME [epoch: 8.32 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1504627725317952		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.1504627725317952 | validation: 0.16566106546213633]
	TIME [epoch: 8.34 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15043268516726668		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.15043268516726668 | validation: 0.13176398044784196]
	TIME [epoch: 8.34 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13034179232971738		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.13034179232971738 | validation: 0.15366472130040712]
	TIME [epoch: 8.31 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16014717254499053		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.16014717254499053 | validation: 0.13412063566353236]
	TIME [epoch: 8.32 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12921831605988515		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.12921831605988515 | validation: 0.15553417324053878]
	TIME [epoch: 8.31 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14354791407035952		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.14354791407035952 | validation: 0.1359682601239759]
	TIME [epoch: 8.31 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11695668592595515		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.11695668592595515 | validation: 0.11480423204938936]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13198154244041832		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.13198154244041832 | validation: 0.21491643796574667]
	TIME [epoch: 8.32 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1516527961111171		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.1516527961111171 | validation: 0.17571201952737847]
	TIME [epoch: 8.32 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14088624474111838		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.14088624474111838 | validation: 0.12630244672651855]
	TIME [epoch: 8.32 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11513312157204648		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.11513312157204648 | validation: 0.12592620685427597]
	TIME [epoch: 8.31 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21427159796795303		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.21427159796795303 | validation: 0.16504689835960543]
	TIME [epoch: 8.33 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15467492956409754		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.15467492956409754 | validation: 0.13974566201183253]
	TIME [epoch: 8.34 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1179545673255257		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.1179545673255257 | validation: 0.12329008995728052]
	TIME [epoch: 8.32 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15292208353402442		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.15292208353402442 | validation: 0.14334171749781766]
	TIME [epoch: 8.32 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11149175120813518		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.11149175120813518 | validation: 0.10785588957705683]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14875776961012538		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.14875776961012538 | validation: 0.21113004174409342]
	TIME [epoch: 8.31 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15311623527316648		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.15311623527316648 | validation: 0.5319044699011932]
	TIME [epoch: 8.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28954437005244227		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.28954437005244227 | validation: 0.1976839044075388]
	TIME [epoch: 8.32 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1451903660293469		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.1451903660293469 | validation: 0.12335986770927673]
	TIME [epoch: 8.3 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2510048410954919		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.2510048410954919 | validation: 0.19274024297884232]
	TIME [epoch: 8.29 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14985857801191727		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.14985857801191727 | validation: 0.152704233213796]
	TIME [epoch: 8.31 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13362752156242375		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.13362752156242375 | validation: 0.11339713080352488]
	TIME [epoch: 8.31 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10774739306377917		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.10774739306377917 | validation: 0.1219207886774647]
	TIME [epoch: 8.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13249082140766788		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.13249082140766788 | validation: 0.12527106533820628]
	TIME [epoch: 8.31 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1219683846224539		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.1219683846224539 | validation: 0.19007329386514837]
	TIME [epoch: 8.31 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14842848500933847		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.14842848500933847 | validation: 0.11660034147837392]
	TIME [epoch: 8.3 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13594946634715138		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.13594946634715138 | validation: 0.14113575786289329]
	TIME [epoch: 8.31 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11771329941716815		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.11771329941716815 | validation: 0.11467522998216731]
	TIME [epoch: 8.33 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1400399577505156		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.1400399577505156 | validation: 0.15256836038742766]
	TIME [epoch: 8.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12340249678501089		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.12340249678501089 | validation: 0.13484437200375998]
	TIME [epoch: 8.31 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1228280901315991		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.1228280901315991 | validation: 0.12428702198134597]
	TIME [epoch: 8.31 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10858465142193634		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.10858465142193634 | validation: 0.15245515721190128]
	TIME [epoch: 8.31 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13607203979787724		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.13607203979787724 | validation: 0.1303282414201199]
	TIME [epoch: 8.31 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12076425727006516		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.12076425727006516 | validation: 0.14477753903275503]
	TIME [epoch: 8.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13919139687711019		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.13919139687711019 | validation: 0.11003969091130963]
	TIME [epoch: 8.33 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10812546624930416		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.10812546624930416 | validation: 0.12030726097803193]
	TIME [epoch: 8.3 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16021601998169344		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.16021601998169344 | validation: 0.18162001727601995]
	TIME [epoch: 8.31 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15618312573282833		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.15618312573282833 | validation: 0.14003543786057776]
	TIME [epoch: 8.31 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1190644321345668		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.1190644321345668 | validation: 0.11571689699443151]
	TIME [epoch: 8.31 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11465087946418864		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.11465087946418864 | validation: 0.13153771089962707]
	TIME [epoch: 8.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14308978322050342		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.14308978322050342 | validation: 0.137628997702543]
	TIME [epoch: 8.3 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12699279927991197		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.12699279927991197 | validation: 0.1289378464115286]
	TIME [epoch: 8.3 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15417481575021424		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.15417481575021424 | validation: 0.1561650310059219]
	TIME [epoch: 8.3 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12259067146648465		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.12259067146648465 | validation: 0.13602092352627204]
	TIME [epoch: 8.31 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11853238186629378		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.11853238186629378 | validation: 0.12220471556039295]
	TIME [epoch: 8.32 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11913443113948032		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.11913443113948032 | validation: 0.1223129897933185]
	TIME [epoch: 8.33 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.110782344891486		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.110782344891486 | validation: 0.11746174677038518]
	TIME [epoch: 8.29 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19961087230872243		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.19961087230872243 | validation: 0.3896180066566842]
	TIME [epoch: 8.3 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21856749838794184		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.21856749838794184 | validation: 0.16365663391749763]
	TIME [epoch: 8.29 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1380473808893128		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.1380473808893128 | validation: 0.12115855990237676]
	TIME [epoch: 8.29 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11321388188639919		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.11321388188639919 | validation: 0.12334287043166757]
	TIME [epoch: 8.34 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1082148987346795		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.1082148987346795 | validation: 0.10640150731811043]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13484915500175923		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.13484915500175923 | validation: 0.1547630744129935]
	TIME [epoch: 8.3 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11186649450392218		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.11186649450392218 | validation: 0.11147110918402356]
	TIME [epoch: 8.3 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11982449291205072		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.11982449291205072 | validation: 0.11817452013456116]
	TIME [epoch: 8.3 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11160735637340483		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.11160735637340483 | validation: 0.1265402280497182]
	TIME [epoch: 8.31 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21217444821159512		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.21217444821159512 | validation: 0.762630614193252]
	TIME [epoch: 8.35 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41748081459098685		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.41748081459098685 | validation: 0.49170464842645834]
	TIME [epoch: 8.3 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31676413988572877		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.31676413988572877 | validation: 0.15397054052950088]
	TIME [epoch: 8.3 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13133831333505136		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.13133831333505136 | validation: 0.13763397389936988]
	TIME [epoch: 8.3 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11883322080862514		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.11883322080862514 | validation: 0.13233660298044247]
	TIME [epoch: 8.3 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11046225512245622		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.11046225512245622 | validation: 0.12433954187743573]
	TIME [epoch: 8.33 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10853835894422917		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.10853835894422917 | validation: 0.11909411477725258]
	TIME [epoch: 8.34 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10390674784324645		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.10390674784324645 | validation: 0.12273366218048601]
	TIME [epoch: 8.31 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10407899398290923		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.10407899398290923 | validation: 0.12612378379531414]
	TIME [epoch: 8.29 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1112225087825199		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.1112225087825199 | validation: 0.12666886167401054]
	TIME [epoch: 8.3 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2892557429370357		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.2892557429370357 | validation: 0.45860306560761166]
	TIME [epoch: 8.3 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4844131606710308		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.4844131606710308 | validation: 0.2696671838452549]
	TIME [epoch: 8.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18452026370127		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.18452026370127 | validation: 0.1719783633886619]
	TIME [epoch: 8.32 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13917616237513733		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.13917616237513733 | validation: 0.14077736478628988]
	TIME [epoch: 8.32 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11778517874311872		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.11778517874311872 | validation: 0.12827823956818624]
	TIME [epoch: 8.3 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11085003586150369		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.11085003586150369 | validation: 0.1176768476054085]
	TIME [epoch: 8.3 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12008387778451059		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.12008387778451059 | validation: 0.22540716458790172]
	TIME [epoch: 8.32 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1434558666830253		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.1434558666830253 | validation: 0.14240498462526952]
	TIME [epoch: 8.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11209265719792719		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.11209265719792719 | validation: 0.11999151662377146]
	TIME [epoch: 8.31 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10133013243724102		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.10133013243724102 | validation: 0.12373733497259892]
	TIME [epoch: 8.32 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10024071016664597		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.10024071016664597 | validation: 0.11472480463118498]
	TIME [epoch: 8.3 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09933136043069032		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.09933136043069032 | validation: 0.12537259974697373]
	TIME [epoch: 8.31 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10343190365249924		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.10343190365249924 | validation: 0.12066099640079002]
	TIME [epoch: 8.32 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10273832695061277		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.10273832695061277 | validation: 0.31033611948635553]
	TIME [epoch: 8.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33219215736374774		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.33219215736374774 | validation: 0.22388058075647274]
	TIME [epoch: 8.3 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19107505986410758		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.19107505986410758 | validation: 0.15059504448303956]
	TIME [epoch: 8.31 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12019357722460536		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.12019357722460536 | validation: 0.1240826021864602]
	TIME [epoch: 8.32 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10128019863711267		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.10128019863711267 | validation: 0.1166072688590124]
	TIME [epoch: 8.31 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0945647004007579		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.0945647004007579 | validation: 0.11401443597143995]
	TIME [epoch: 8.37 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1064746855674171		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.1064746855674171 | validation: 0.13010959208652761]
	TIME [epoch: 8.33 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4987443863572183		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.4987443863572183 | validation: 1.5117006660313606]
	TIME [epoch: 8.32 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.052432991908192		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 1.052432991908192 | validation: 1.210695112491695]
	TIME [epoch: 8.31 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9504451552014694		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.9504451552014694 | validation: 1.5273167947983652]
	TIME [epoch: 8.31 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.466183308173205		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 1.466183308173205 | validation: 1.8108405289357818]
	TIME [epoch: 8.32 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6567042117633217		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 1.6567042117633217 | validation: 2.0995264752005367]
	TIME [epoch: 8.35 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.025130778727887		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 2.025130778727887 | validation: 2.4755321836231623]
	TIME [epoch: 8.32 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.238948627083154		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 2.238948627083154 | validation: 2.415347083133659]
	TIME [epoch: 8.31 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2350012101176833		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 2.2350012101176833 | validation: 2.4283516194444217]
	TIME [epoch: 8.32 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4323925338299865		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 2.4323925338299865 | validation: 3.092216457394936]
	TIME [epoch: 8.32 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0615292288758766		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 3.0615292288758766 | validation: 3.4069865150051615]
	TIME [epoch: 8.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.285967156200372		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 3.285967156200372 | validation: 2.763308284003476]
	TIME [epoch: 8.34 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2346950087471162		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 2.2346950087471162 | validation: 2.26922602989481]
	TIME [epoch: 8.3 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2592170939028766		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 2.2592170939028766 | validation: 2.771018468793552]
	TIME [epoch: 8.32 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6792358368490348		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 2.6792358368490348 | validation: 2.834842140913259]
	TIME [epoch: 8.31 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.487510963055111		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 2.487510963055111 | validation: 2.6873303803299744]
	TIME [epoch: 8.31 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.323221303892878		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 2.323221303892878 | validation: 2.379344321755867]
	TIME [epoch: 8.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9498731971866508		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 1.9498731971866508 | validation: 2.0869945455032295]
	TIME [epoch: 8.32 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6887437797748373		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 1.6887437797748373 | validation: 1.9852698304239988]
	TIME [epoch: 8.31 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4994807226938838		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 1.4994807226938838 | validation: 1.9642210333598333]
	TIME [epoch: 8.31 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.451110365610011		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 1.451110365610011 | validation: 1.8841589709530813]
	TIME [epoch: 8.31 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.335433480962487		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 1.335433480962487 | validation: 1.7819231142598317]
	TIME [epoch: 8.32 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.284867951721903		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 1.284867951721903 | validation: 1.8183718953006562]
	TIME [epoch: 8.34 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2526924973194955		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 1.2526924973194955 | validation: 1.759779061803865]
	TIME [epoch: 8.31 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1892849602943916		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 1.1892849602943916 | validation: 1.6731188881204593]
	TIME [epoch: 8.31 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.127957129808013		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 1.127957129808013 | validation: 1.629323070269458]
	TIME [epoch: 8.31 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.06049404489535		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 1.06049404489535 | validation: 1.5738533574661706]
	TIME [epoch: 8.31 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9840188493189695		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.9840188493189695 | validation: 1.505585674458384]
	TIME [epoch: 8.34 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8955836926974813		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.8955836926974813 | validation: 1.3856487804879025]
	TIME [epoch: 8.34 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7882402338499612		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.7882402338499612 | validation: 1.210039504177397]
	TIME [epoch: 8.31 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7303806359718996		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.7303806359718996 | validation: 1.0497377424039556]
	TIME [epoch: 8.31 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6175242310941186		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.6175242310941186 | validation: 0.9134428362645861]
	TIME [epoch: 8.31 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5425001988437368		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.5425001988437368 | validation: 0.8447548916364398]
	TIME [epoch: 8.33 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5227305747658496		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.5227305747658496 | validation: 0.8214027674635371]
	TIME [epoch: 8.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49938948314525955		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.49938948314525955 | validation: 0.7939801181752577]
	TIME [epoch: 8.33 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4860375457215688		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.4860375457215688 | validation: 0.7738260613158944]
	TIME [epoch: 8.31 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46794695047178314		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.46794695047178314 | validation: 0.7860223130711737]
	TIME [epoch: 8.31 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4707373856659609		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.4707373856659609 | validation: 1.105221903744379]
	TIME [epoch: 8.32 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8882432719498027		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.8882432719498027 | validation: 0.8477938970265086]
	TIME [epoch: 8.32 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5177191308288878		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.5177191308288878 | validation: 0.7347005508073396]
	TIME [epoch: 8.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44776243918307557		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.44776243918307557 | validation: 0.6887435561645736]
	TIME [epoch: 8.31 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4179258314650419		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.4179258314650419 | validation: 0.6705768196947323]
	TIME [epoch: 8.31 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4066584417702642		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.4066584417702642 | validation: 0.6521608471472587]
	TIME [epoch: 8.31 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38845089675490574		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.38845089675490574 | validation: 0.6447837208664835]
	TIME [epoch: 8.31 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3750097506171148		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.3750097506171148 | validation: 0.6380330922612136]
	TIME [epoch: 8.35 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3647640992762118		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.3647640992762118 | validation: 0.6302315453439484]
	TIME [epoch: 8.35 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34872743370256737		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.34872743370256737 | validation: 0.6043549758474238]
	TIME [epoch: 8.32 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4284016244019312		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.4284016244019312 | validation: 0.8944526471877805]
	TIME [epoch: 8.31 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47560222095420285		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.47560222095420285 | validation: 0.604148377701359]
	TIME [epoch: 8.31 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34432771898764686		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.34432771898764686 | validation: 0.5683501430944295]
	TIME [epoch: 8.31 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31867722418991484		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.31867722418991484 | validation: 0.5600956248483595]
	TIME [epoch: 8.34 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30620124439966717		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.30620124439966717 | validation: 0.558434432409719]
	TIME [epoch: 8.32 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3002830136498268		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.3002830136498268 | validation: 0.5314369747731242]
	TIME [epoch: 8.31 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2955061804615658		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.2955061804615658 | validation: 0.5061199068670934]
	TIME [epoch: 8.32 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28027722706547986		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.28027722706547986 | validation: 0.4658528220622388]
	TIME [epoch: 8.31 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4072571900865023		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.4072571900865023 | validation: 0.5936070687973871]
	TIME [epoch: 8.33 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37623414081926293		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.37623414081926293 | validation: 0.5568023354052096]
	TIME [epoch: 8.35 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3204642214156827		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.3204642214156827 | validation: 0.4964976602042698]
	TIME [epoch: 8.32 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2767247367327563		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.2767247367327563 | validation: 0.45355282635224137]
	TIME [epoch: 8.31 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25181333946698586		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.25181333946698586 | validation: 0.4074973830578698]
	TIME [epoch: 8.32 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23395054627427797		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.23395054627427797 | validation: 0.3791820605849602]
	TIME [epoch: 8.33 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23803039371443133		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.23803039371443133 | validation: 0.36955894395808764]
	TIME [epoch: 8.35 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23884579580060322		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.23884579580060322 | validation: 0.34608274874077927]
	TIME [epoch: 8.34 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2130567285980831		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.2130567285980831 | validation: 0.31358399432371187]
	TIME [epoch: 8.33 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21723562859602574		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.21723562859602574 | validation: 0.30118908626259666]
	TIME [epoch: 8.32 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2032034898524498		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.2032034898524498 | validation: 0.265986280425195]
	TIME [epoch: 8.33 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18767041613675847		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.18767041613675847 | validation: 0.2476023361471729]
	TIME [epoch: 8.32 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17922761707461718		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.17922761707461718 | validation: 0.24736838672200465]
	TIME [epoch: 8.35 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26624557233988755		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.26624557233988755 | validation: 0.2451885056888173]
	TIME [epoch: 8.31 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9224426694357903		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.9224426694357903 | validation: 1.388526187045567]
	TIME [epoch: 8.31 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9497699095447591		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.9497699095447591 | validation: 0.5349341089286903]
	TIME [epoch: 8.32 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29406552568323185		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.29406552568323185 | validation: 0.2330121311051655]
	TIME [epoch: 8.31 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17014254369508627		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.17014254369508627 | validation: 0.228233391363513]
	TIME [epoch: 8.33 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1627970540333858		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.1627970540333858 | validation: 0.21946382316276397]
	TIME [epoch: 8.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16112713126226927		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.16112713126226927 | validation: 0.20754201135419603]
	TIME [epoch: 8.32 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16618785225705762		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.16618785225705762 | validation: 0.22767051441889097]
	TIME [epoch: 8.32 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1776801073988663		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.1776801073988663 | validation: 0.21373340096206078]
	TIME [epoch: 8.31 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16865779036287532		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.16865779036287532 | validation: 0.19484336312114267]
	TIME [epoch: 8.33 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1629047065552753		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.1629047065552753 | validation: 0.18350969557050717]
	TIME [epoch: 8.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16305716599455625		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.16305716599455625 | validation: 0.3339018344997252]
	TIME [epoch: 8.33 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19681934610856322		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.19681934610856322 | validation: 0.23985767954996867]
	TIME [epoch: 8.31 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17816978978218745		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.17816978978218745 | validation: 0.19805778863786366]
	TIME [epoch: 8.31 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15840859815661532		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.15840859815661532 | validation: 0.16858695692901918]
	TIME [epoch: 8.31 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16573943490785117		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.16573943490785117 | validation: 0.21543449310323884]
	TIME [epoch: 8.32 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15649253981711828		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.15649253981711828 | validation: 0.17208894540325606]
	TIME [epoch: 8.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16748991217478254		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.16748991217478254 | validation: 0.2023251657821378]
	TIME [epoch: 8.32 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15621576695368383		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.15621576695368383 | validation: 0.19012922527560328]
	TIME [epoch: 8.31 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14002904339805713		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.14002904339805713 | validation: 0.1662049478592793]
	TIME [epoch: 8.31 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14574397185510732		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.14574397185510732 | validation: 0.1469865177809221]
	TIME [epoch: 8.32 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15631246423488382		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.15631246423488382 | validation: 0.31709304463797083]
	TIME [epoch: 8.33 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23430792492736924		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.23430792492736924 | validation: 0.27173328513443407]
	TIME [epoch: 8.37 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1992370447674206		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.1992370447674206 | validation: 0.27605123516414404]
	TIME [epoch: 8.32 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1886589328079411		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.1886589328079411 | validation: 0.15862660372308865]
	TIME [epoch: 8.31 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1326258032847601		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.1326258032847601 | validation: 0.1453930849420318]
	TIME [epoch: 8.31 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12418310760493702		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.12418310760493702 | validation: 0.14699867816228368]
	TIME [epoch: 8.32 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13108202191924687		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.13108202191924687 | validation: 0.14199651168606775]
	TIME [epoch: 8.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12485257978787002		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.12485257978787002 | validation: 0.13681417484608785]
	TIME [epoch: 8.34 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24126258135210665		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.24126258135210665 | validation: 0.2626057907679353]
	TIME [epoch: 8.31 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23031491794696052		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.23031491794696052 | validation: 0.27659237273673165]
	TIME [epoch: 8.31 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2147064442944472		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.2147064442944472 | validation: 0.1824968741996561]
	TIME [epoch: 8.31 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1890262498142795		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.1890262498142795 | validation: 0.1848094111607003]
	TIME [epoch: 8.32 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14731320201406442		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.14731320201406442 | validation: 0.16840227911329952]
	TIME [epoch: 8.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1297308953334837		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.1297308953334837 | validation: 0.13470017687523936]
	TIME [epoch: 8.32 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11970863249038724		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.11970863249038724 | validation: 0.13179553853054896]
	TIME [epoch: 8.31 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11774392401170786		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.11774392401170786 | validation: 0.12634517474201853]
	TIME [epoch: 8.31 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15305437997415972		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.15305437997415972 | validation: 0.15139895798054637]
	TIME [epoch: 8.31 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14462359798894447		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.14462359798894447 | validation: 0.1357447567855773]
	TIME [epoch: 8.33 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29620522229561913		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.29620522229561913 | validation: 0.31343017773041204]
	TIME [epoch: 8.34 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23376799978896687		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.23376799978896687 | validation: 0.2759741685298211]
	TIME [epoch: 8.31 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20394162138473793		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.20394162138473793 | validation: 0.2287564457691404]
	TIME [epoch: 8.31 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17621588834993424		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.17621588834993424 | validation: 0.18250014438359966]
	TIME [epoch: 8.31 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14522157014983575		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.14522157014983575 | validation: 0.1339521699945989]
	TIME [epoch: 8.33 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12001955832654701		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.12001955832654701 | validation: 0.13135362344900342]
	TIME [epoch: 8.42 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1502910261166892		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.1502910261166892 | validation: 0.13033918755710874]
	TIME [epoch: 8.38 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11991268446985223		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.11991268446985223 | validation: 0.119731352445275]
	TIME [epoch: 8.37 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10916159237055172		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.10916159237055172 | validation: 0.11545829606726316]
	TIME [epoch: 8.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10897152210979222		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.10897152210979222 | validation: 0.13501159914476793]
	TIME [epoch: 8.32 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11538805509242796		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.11538805509242796 | validation: 0.1406251932929783]
	TIME [epoch: 8.32 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15231212701746893		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.15231212701746893 | validation: 0.12487511428888398]
	TIME [epoch: 8.37 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13150233714177434		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.13150233714177434 | validation: 0.2323862818798592]
	TIME [epoch: 8.32 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3009766108016042		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.3009766108016042 | validation: 0.23755947331411414]
	TIME [epoch: 8.31 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17430080174555462		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.17430080174555462 | validation: 0.13304781296996054]
	TIME [epoch: 8.31 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1173860760138401		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.1173860760138401 | validation: 0.29241834052963445]
	TIME [epoch: 8.31 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24855611346223971		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.24855611346223971 | validation: 0.16117500192468748]
	TIME [epoch: 8.34 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13316008570321503		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.13316008570321503 | validation: 0.12317237737708917]
	TIME [epoch: 8.35 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.114092810469684		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.114092810469684 | validation: 0.11812455474840745]
	TIME [epoch: 8.31 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1250860566731922		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.1250860566731922 | validation: 0.13081483392442173]
	TIME [epoch: 8.31 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12957811472054437		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.12957811472054437 | validation: 0.11962649062640418]
	TIME [epoch: 8.31 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1093906202087164		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.1093906202087164 | validation: 0.11294433411456371]
	TIME [epoch: 8.31 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11052425128675628		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.11052425128675628 | validation: 0.11540018008072278]
	TIME [epoch: 8.37 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11378864565623895		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.11378864565623895 | validation: 0.1291476695842678]
	TIME [epoch: 8.32 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10983710138876877		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.10983710138876877 | validation: 0.12125042640226631]
	TIME [epoch: 8.31 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10805601871604145		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.10805601871604145 | validation: 0.11061387589430496]
	TIME [epoch: 8.32 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1099350430172789		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.1099350430172789 | validation: 0.11151725584780323]
	TIME [epoch: 8.32 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1298450233622089		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.1298450233622089 | validation: 0.17150374961075884]
	TIME [epoch: 8.3 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1336384705461101		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.1336384705461101 | validation: 0.12877004286279609]
	TIME [epoch: 8.35 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11588863464524947		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.11588863464524947 | validation: 0.11527996427259671]
	TIME [epoch: 8.31 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17621207403729003		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.17621207403729003 | validation: 0.12152559950622795]
	TIME [epoch: 8.32 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11317047413227749		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.11317047413227749 | validation: 0.1024284358133889]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09984765114110819		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.09984765114110819 | validation: 0.10739668934214353]
	TIME [epoch: 8.42 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1003295457374865		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.1003295457374865 | validation: 0.12425014704720841]
	TIME [epoch: 8.39 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19657067837379616		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.19657067837379616 | validation: 0.14027207742485476]
	TIME [epoch: 8.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12193381803112613		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.12193381803112613 | validation: 0.10675689781189851]
	TIME [epoch: 8.3 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1016291577222123		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.1016291577222123 | validation: 0.10500321452320474]
	TIME [epoch: 8.31 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10286046884749435		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.10286046884749435 | validation: 0.11053084484992705]
	TIME [epoch: 8.31 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12391970461694865		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.12391970461694865 | validation: 0.1132472727992027]
	TIME [epoch: 8.31 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11710170823040247		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.11710170823040247 | validation: 0.1084262901526474]
	TIME [epoch: 8.35 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1088292070981684		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.1088292070981684 | validation: 0.11545083454338667]
	TIME [epoch: 8.32 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10745800630380654		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.10745800630380654 | validation: 0.10437753023273774]
	TIME [epoch: 8.3 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10240830322675305		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.10240830322675305 | validation: 0.10341631886632095]
	TIME [epoch: 8.3 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09777124252586225		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.09777124252586225 | validation: 0.10195512435157628]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13083281192048188		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.13083281192048188 | validation: 0.12268210649000819]
	TIME [epoch: 8.43 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14818560745855064		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.14818560745855064 | validation: 0.11056849864394964]
	TIME [epoch: 8.34 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10747820186061303		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.10747820186061303 | validation: 0.10221927589761354]
	TIME [epoch: 8.32 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09779857902930675		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.09779857902930675 | validation: 0.10253456340232356]
	TIME [epoch: 8.32 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09600230773838356		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.09600230773838356 | validation: 0.10077058315141718]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09913207401696228		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.09913207401696228 | validation: 0.11545313301000242]
	TIME [epoch: 8.42 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10710338465174121		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.10710338465174121 | validation: 0.11747279190893434]
	TIME [epoch: 8.36 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18804163328412907		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.18804163328412907 | validation: 0.1891234005643506]
	TIME [epoch: 8.32 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18613677562055372		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.18613677562055372 | validation: 0.15003018372805277]
	TIME [epoch: 8.32 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1294070287653137		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.1294070287653137 | validation: 0.1279138421457151]
	TIME [epoch: 8.31 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11737377590537396		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.11737377590537396 | validation: 0.13288291667835966]
	TIME [epoch: 8.31 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13207045227371336		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.13207045227371336 | validation: 0.1776079194195575]
	TIME [epoch: 8.32 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1333515535072771		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.1333515535072771 | validation: 0.15259887540411068]
	TIME [epoch: 8.37 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11922965420623137		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.11922965420623137 | validation: 0.11349758837428847]
	TIME [epoch: 8.32 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10505624217917367		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.10505624217917367 | validation: 0.10421266213199004]
	TIME [epoch: 8.32 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10109258274995916		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.10109258274995916 | validation: 0.11541016776416405]
	TIME [epoch: 8.31 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09972434293374778		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.09972434293374778 | validation: 0.11729155593143188]
	TIME [epoch: 8.3 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09664308109038494		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.09664308109038494 | validation: 0.09906822279968086]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09432656700204842		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.09432656700204842 | validation: 0.09893568607958184]
	TIME [epoch: 8.43 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1026678540158695		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.1026678540158695 | validation: 0.12270715769020468]
	TIME [epoch: 8.4 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11386644988926403		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.11386644988926403 | validation: 0.09988994660393442]
	TIME [epoch: 8.36 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09403163020453267		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.09403163020453267 | validation: 0.1024544487000009]
	TIME [epoch: 8.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10149680304735488		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.10149680304735488 | validation: 0.12910727470243522]
	TIME [epoch: 8.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11585427822866912		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.11585427822866912 | validation: 0.10464218102083486]
	TIME [epoch: 8.4 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09852630878034199		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.09852630878034199 | validation: 0.0974446664992345]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14849587442686119		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.14849587442686119 | validation: 0.12209920824960938]
	TIME [epoch: 8.39 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10959747544868563		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.10959747544868563 | validation: 0.10617048988275493]
	TIME [epoch: 8.31 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.095496800359402		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.095496800359402 | validation: 0.10315202701200443]
	TIME [epoch: 8.31 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0887455862509843		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.0887455862509843 | validation: 0.09908326683443233]
	TIME [epoch: 8.33 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09646896176031672		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.09646896176031672 | validation: 0.10239827900567985]
	TIME [epoch: 8.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09517979058963538		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.09517979058963538 | validation: 0.11022455643701778]
	TIME [epoch: 8.32 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09411979088024142		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.09411979088024142 | validation: 0.09746156621790686]
	TIME [epoch: 8.32 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09809584048038548		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.09809584048038548 | validation: 0.1040736932659698]
	TIME [epoch: 8.32 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09400909112327144		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.09400909112327144 | validation: 0.09333093988368021]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09614296722140395		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.09614296722140395 | validation: 0.09275200770254421]
	TIME [epoch: 8.42 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09209822698084205		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.09209822698084205 | validation: 0.0944213638295305]
	TIME [epoch: 8.41 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1019511399508889		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.1019511399508889 | validation: 0.09708651956872623]
	TIME [epoch: 8.31 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08864219019739518		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.08864219019739518 | validation: 0.0928704984442465]
	TIME [epoch: 8.31 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09192616942896593		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.09192616942896593 | validation: 0.09657243468974747]
	TIME [epoch: 8.3 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0887620125932082		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.0887620125932082 | validation: 0.09978042652565466]
	TIME [epoch: 8.32 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0954318289460705		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.0954318289460705 | validation: 0.18155379425093549]
	TIME [epoch: 8.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14693544328558011		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.14693544328558011 | validation: 0.11377609228376966]
	TIME [epoch: 8.3 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09761052885609504		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.09761052885609504 | validation: 0.1041141987114057]
	TIME [epoch: 8.3 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09209312572841581		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.09209312572841581 | validation: 0.10039676595962269]
	TIME [epoch: 8.3 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0875976991867465		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.0875976991867465 | validation: 0.09290990858034044]
	TIME [epoch: 8.3 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09790724466840263		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.09790724466840263 | validation: 0.10495375043650307]
	TIME [epoch: 8.34 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09513360929418349		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.09513360929418349 | validation: 0.09726640884580051]
	TIME [epoch: 8.32 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08910772695678694		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.08910772695678694 | validation: 0.12188060216803315]
	TIME [epoch: 8.3 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09932129725656758		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.09932129725656758 | validation: 0.09928589720694819]
	TIME [epoch: 8.3 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08323979002271045		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.08323979002271045 | validation: 0.09204172884227846]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09057959293331294		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.09057959293331294 | validation: 0.09897209013745945]
	TIME [epoch: 8.38 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09039358392486813		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.09039358392486813 | validation: 0.08750773081549527]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_503.pth
	Model improved!!!
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08386726839276148		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.08386726839276148 | validation: 0.09027379739919472]
	TIME [epoch: 8.4 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08684269461014875		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.08684269461014875 | validation: 0.09417191572128569]
	TIME [epoch: 8.33 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08932957297622965		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.08932957297622965 | validation: 0.18165635115733675]
	TIME [epoch: 8.33 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14088909251123027		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.14088909251123027 | validation: 0.14082076282285433]
	TIME [epoch: 8.33 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0990758262005674		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.0990758262005674 | validation: 0.09212496960707223]
	TIME [epoch: 8.35 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08306457862983833		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.08306457862983833 | validation: 0.08009127458295065]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10647172072585134		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.10647172072585134 | validation: 0.11553200671312044]
	TIME [epoch: 8.4 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1008289491480133		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.1008289491480133 | validation: 0.10704301144966673]
	TIME [epoch: 8.36 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09179702131595784		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.09179702131595784 | validation: 0.10028497597691582]
	TIME [epoch: 8.38 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09290377927041078		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.09290377927041078 | validation: 0.10678907321091641]
	TIME [epoch: 8.38 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08691149110717171		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.08691149110717171 | validation: 0.091823622445077]
	TIME [epoch: 8.42 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09091561577082743		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.09091561577082743 | validation: 0.10487272018184407]
	TIME [epoch: 8.36 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08984936872208032		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.08984936872208032 | validation: 0.08950598914102156]
	TIME [epoch: 8.32 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08064587117054504		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.08064587117054504 | validation: 0.09738857057981043]
	TIME [epoch: 8.32 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08305828451006907		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.08305828451006907 | validation: 0.09680827826310029]
	TIME [epoch: 8.32 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09300599535190168		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.09300599535190168 | validation: 0.09251890937828042]
	TIME [epoch: 8.34 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08619927239444876		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.08619927239444876 | validation: 0.09121740174879699]
	TIME [epoch: 8.35 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08954344692454916		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.08954344692454916 | validation: 0.08511876126065575]
	TIME [epoch: 8.32 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08959665669049055		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.08959665669049055 | validation: 0.09909714181960291]
	TIME [epoch: 8.32 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09035281345932872		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.09035281345932872 | validation: 0.08504332723736141]
	TIME [epoch: 8.32 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0789228615651997		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.0789228615651997 | validation: 0.10033112059150298]
	TIME [epoch: 8.31 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09759123866841611		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.09759123866841611 | validation: 0.1058396137881736]
	TIME [epoch: 8.35 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08253680476359324		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.08253680476359324 | validation: 0.15293411492891665]
	TIME [epoch: 8.33 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1360475291364148		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.1360475291364148 | validation: 0.11124734992965109]
	TIME [epoch: 8.32 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09731404135441893		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.09731404135441893 | validation: 0.08971635871079923]
	TIME [epoch: 8.32 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08879459860037997		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.08879459860037997 | validation: 0.08993925166042986]
	TIME [epoch: 8.32 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08787446784503726		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.08787446784503726 | validation: 0.08800131070780028]
	TIME [epoch: 8.33 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08069258275996144		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.08069258275996144 | validation: 0.07823228429524053]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0801604759913607		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.0801604759913607 | validation: 0.08280454683082838]
	TIME [epoch: 8.46 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08751691210074061		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.08751691210074061 | validation: 0.0949011444593369]
	TIME [epoch: 8.37 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09073567981246222		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.09073567981246222 | validation: 0.11014930281515767]
	TIME [epoch: 8.34 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09556205164920792		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.09556205164920792 | validation: 0.1436516882941573]
	TIME [epoch: 8.41 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12383725519118549		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.12383725519118549 | validation: 0.11929665844832738]
	TIME [epoch: 8.35 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1116356992625211		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.1116356992625211 | validation: 0.11087401209781689]
	TIME [epoch: 8.35 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09541181467001067		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.09541181467001067 | validation: 0.10000699006240435]
	TIME [epoch: 8.32 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0852002735812501		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.0852002735812501 | validation: 0.0879275142581478]
	TIME [epoch: 8.33 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08605592528133826		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.08605592528133826 | validation: 0.0804887266855344]
	TIME [epoch: 8.33 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07764591800180828		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.07764591800180828 | validation: 0.08080419686881474]
	TIME [epoch: 8.33 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0797853006269776		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.0797853006269776 | validation: 0.08681360278009617]
	TIME [epoch: 8.38 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.078317066277302		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.078317066277302 | validation: 0.10149821306853093]
	TIME [epoch: 8.33 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10033829397905913		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.10033829397905913 | validation: 0.11443537068876763]
	TIME [epoch: 8.33 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09979121245646319		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.09979121245646319 | validation: 0.08770926439201346]
	TIME [epoch: 8.33 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08163032484081388		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.08163032484081388 | validation: 0.08038478263495372]
	TIME [epoch: 8.32 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0794431868140274		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.0794431868140274 | validation: 0.08312754170025208]
	TIME [epoch: 8.33 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07944547941125685		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.07944547941125685 | validation: 0.09139812524882844]
	TIME [epoch: 8.4 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07798430870482888		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.07798430870482888 | validation: 0.08518776109265389]
	TIME [epoch: 8.37 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.077524470322095		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.077524470322095 | validation: 0.0850752849140917]
	TIME [epoch: 8.35 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09824157359055229		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.09824157359055229 | validation: 0.09872512802444497]
	TIME [epoch: 8.32 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09202977322103803		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.09202977322103803 | validation: 0.08816342790039453]
	TIME [epoch: 8.32 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07641620998133367		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.07641620998133367 | validation: 0.08584343194112985]
	TIME [epoch: 8.36 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08455629322849591		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.08455629322849591 | validation: 0.09608294195144237]
	TIME [epoch: 8.34 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08819665768565513		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.08819665768565513 | validation: 0.09001176052504928]
	TIME [epoch: 8.32 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08891344818211262		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.08891344818211262 | validation: 0.0897107416980885]
	TIME [epoch: 8.33 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10968981677799873		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.10968981677799873 | validation: 0.09154587696611514]
	TIME [epoch: 8.32 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09103051694795761		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.09103051694795761 | validation: 0.08490816969431407]
	TIME [epoch: 8.33 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0770281772460846		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.0770281772460846 | validation: 0.08106475869197566]
	TIME [epoch: 8.36 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07880620610466277		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.07880620610466277 | validation: 0.07763317904954146]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_560.pth
	Model improved!!!
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07648014630923214		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.07648014630923214 | validation: 0.08556209969033264]
	TIME [epoch: 8.4 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07836524890355212		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.07836524890355212 | validation: 0.08445266572381824]
	TIME [epoch: 8.31 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08645598801655739		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.08645598801655739 | validation: 0.08276400719365118]
	TIME [epoch: 8.31 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09020245395912257		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.09020245395912257 | validation: 0.10710026525849248]
	TIME [epoch: 8.33 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08531121466531685		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.08531121466531685 | validation: 0.09203065803306215]
	TIME [epoch: 8.37 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07918272910588989		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.07918272910588989 | validation: 0.08946203815585371]
	TIME [epoch: 8.37 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08492302585500351		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.08492302585500351 | validation: 0.08763592255723006]
	TIME [epoch: 8.35 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11005414410961942		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.11005414410961942 | validation: 0.10518815527938147]
	TIME [epoch: 8.36 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09229780278492976		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.09229780278492976 | validation: 0.08570057095366448]
	TIME [epoch: 8.36 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07656192369454025		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.07656192369454025 | validation: 0.07976068086983917]
	TIME [epoch: 8.36 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0748031108845527		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.0748031108845527 | validation: 0.0794405259837668]
	TIME [epoch: 8.32 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08073795493058282		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.08073795493058282 | validation: 0.09310183753944451]
	TIME [epoch: 8.3 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07866137960265325		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.07866137960265325 | validation: 0.08297594875194708]
	TIME [epoch: 8.31 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07811616070691724		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.07811616070691724 | validation: 0.0861786078975359]
	TIME [epoch: 8.31 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07641169924938554		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.07641169924938554 | validation: 0.09463830764399171]
	TIME [epoch: 8.31 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08336686174524273		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.08336686174524273 | validation: 0.07953842778772251]
	TIME [epoch: 8.34 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07351118330992604		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.07351118330992604 | validation: 0.07558874311107533]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08023569971617792		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.08023569971617792 | validation: 0.09659294767612489]
	TIME [epoch: 8.39 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08086675366868404		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.08086675366868404 | validation: 0.0889876355320246]
	TIME [epoch: 8.31 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08165785497012659		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.08165785497012659 | validation: 0.07752342562512257]
	TIME [epoch: 8.3 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07739966942905406		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.07739966942905406 | validation: 0.08047407716092422]
	TIME [epoch: 8.32 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07404636814704399		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.07404636814704399 | validation: 0.08803941163032578]
	TIME [epoch: 8.34 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07776097487502975		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.07776097487502975 | validation: 0.08718059307000203]
	TIME [epoch: 8.3 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08119787952539749		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.08119787952539749 | validation: 0.09903082086075655]
	TIME [epoch: 8.31 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1211600693307153		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.1211600693307153 | validation: 0.10807236187700059]
	TIME [epoch: 8.31 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09055175689974174		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.09055175689974174 | validation: 0.0869147700388471]
	TIME [epoch: 8.31 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.075697081627995		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.075697081627995 | validation: 0.0838854573046339]
	TIME [epoch: 8.35 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07534720868832959		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.07534720868832959 | validation: 0.08379015135498924]
	TIME [epoch: 8.31 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08353683843570008		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.08353683843570008 | validation: 0.08843076194882178]
	TIME [epoch: 8.3 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07960315196674024		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.07960315196674024 | validation: 0.088228119712667]
	TIME [epoch: 8.3 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08069118891524124		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.08069118891524124 | validation: 0.08269133679342547]
	TIME [epoch: 8.31 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08973239536233821		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.08973239536233821 | validation: 0.09049214732661934]
	TIME [epoch: 8.33 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07786751294687197		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.07786751294687197 | validation: 0.07878204542981777]
	TIME [epoch: 8.35 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07473377876101497		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.07473377876101497 | validation: 0.07834069877801089]
	TIME [epoch: 8.32 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07195235597300005		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.07195235597300005 | validation: 0.07369950499450391]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07467969268855532		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.07467969268855532 | validation: 0.08536264489581591]
	TIME [epoch: 8.47 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08007890580434533		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.08007890580434533 | validation: 0.08484813763986107]
	TIME [epoch: 8.35 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07632949680908248		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.07632949680908248 | validation: 0.08818936275649394]
	TIME [epoch: 8.35 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07589409141051204		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.07589409141051204 | validation: 0.07887577846336985]
	TIME [epoch: 8.33 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07377662876975431		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.07377662876975431 | validation: 0.08679102739920327]
	TIME [epoch: 8.32 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07686725983356657		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.07686725983356657 | validation: 0.07340980635975206]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_601.pth
	Model improved!!!
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07627942249161397		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.07627942249161397 | validation: 0.079860137147108]
	TIME [epoch: 8.38 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07597936179266132		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.07597936179266132 | validation: 0.089869704403739]
	TIME [epoch: 8.32 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0765987268557677		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.0765987268557677 | validation: 0.08543400382683493]
	TIME [epoch: 8.35 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07866583628190715		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.07866583628190715 | validation: 0.080213164205052]
	TIME [epoch: 8.32 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0806713941534962		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.0806713941534962 | validation: 0.09431697388596766]
	TIME [epoch: 8.34 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08155796011867628		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.08155796011867628 | validation: 0.08510989348744147]
	TIME [epoch: 8.35 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08027417965888516		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.08027417965888516 | validation: 0.08189410716834566]
	TIME [epoch: 8.32 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07064538671122503		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.07064538671122503 | validation: 0.08264852681709847]
	TIME [epoch: 8.33 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07531101898594075		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.07531101898594075 | validation: 0.08591194325094273]
	TIME [epoch: 8.35 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07899155214814929		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.07899155214814929 | validation: 0.09051522101398005]
	TIME [epoch: 8.31 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.076878151104904		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.076878151104904 | validation: 0.08173176797212123]
	TIME [epoch: 8.35 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07469658692141311		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.07469658692141311 | validation: 0.08907216250928404]
	TIME [epoch: 8.36 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07497951494251995		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.07497951494251995 | validation: 0.08316858172148421]
	TIME [epoch: 8.35 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07379909707176972		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.07379909707176972 | validation: 0.08740626495677355]
	TIME [epoch: 8.42 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07708440581196371		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.07708440581196371 | validation: 0.07722225598481561]
	TIME [epoch: 8.36 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07191115729487299		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.07191115729487299 | validation: 0.07833032764893019]
	TIME [epoch: 8.31 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07734515778675208		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.07734515778675208 | validation: 0.08599189093171061]
	TIME [epoch: 8.29 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07646009833908325		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.07646009833908325 | validation: 0.089019139672695]
	TIME [epoch: 8.3 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08245734003636125		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.08245734003636125 | validation: 0.09053541297224368]
	TIME [epoch: 8.31 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07643041925677446		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.07643041925677446 | validation: 0.07989265537865789]
	TIME [epoch: 8.35 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07286688950501824		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.07286688950501824 | validation: 0.07353154795585386]
	TIME [epoch: 8.31 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07376275045067576		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.07376275045067576 | validation: 0.08257491156232977]
	TIME [epoch: 8.3 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08871028769895402		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.08871028769895402 | validation: 0.08014508851802815]
	TIME [epoch: 8.29 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.077017132336711		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.077017132336711 | validation: 0.07545544890983061]
	TIME [epoch: 8.3 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07051853577960951		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.07051853577960951 | validation: 0.07238262674483822]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07584359905696088		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.07584359905696088 | validation: 0.09096454339484844]
	TIME [epoch: 8.51 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08784879960760858		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.08784879960760858 | validation: 0.09056801606016385]
	TIME [epoch: 8.38 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07731271180286327		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.07731271180286327 | validation: 0.08296199556238401]
	TIME [epoch: 8.34 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07048060976094077		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.07048060976094077 | validation: 0.08329266630426102]
	TIME [epoch: 8.35 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0798490983525863		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.0798490983525863 | validation: 0.08063029156641097]
	TIME [epoch: 8.38 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07377448765846295		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.07377448765846295 | validation: 0.07485755973534727]
	TIME [epoch: 8.41 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0795026274893996		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.0795026274893996 | validation: 0.07887375186841039]
	TIME [epoch: 8.36 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0740351121242679		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.0740351121242679 | validation: 0.07711520012382396]
	TIME [epoch: 8.33 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08351145918052509		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.08351145918052509 | validation: 0.1470566911587321]
	TIME [epoch: 8.33 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11641218913232222		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.11641218913232222 | validation: 0.12235610058844379]
	TIME [epoch: 8.35 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09834001306691172		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.09834001306691172 | validation: 0.09203906476213569]
	TIME [epoch: 8.4 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08090310796767795		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.08090310796767795 | validation: 0.08463997414185649]
	TIME [epoch: 8.41 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07623398547982455		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.07623398547982455 | validation: 0.07619855242819981]
	TIME [epoch: 8.33 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06998132237616407		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.06998132237616407 | validation: 0.07903508140983662]
	TIME [epoch: 8.33 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07396020598552314		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.07396020598552314 | validation: 0.08793850580693345]
	TIME [epoch: 8.33 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07918253954001958		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.07918253954001958 | validation: 0.09430539245972328]
	TIME [epoch: 8.33 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08402998010864096		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.08402998010864096 | validation: 0.21825242201146428]
	TIME [epoch: 8.37 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24886021041604495		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.24886021041604495 | validation: 0.23114886992238037]
	TIME [epoch: 8.34 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19729181114140174		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.19729181114140174 | validation: 0.1744281903174597]
	TIME [epoch: 8.33 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1576610525759202		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.1576610525759202 | validation: 0.12035378303620305]
	TIME [epoch: 8.33 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11805954051325158		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.11805954051325158 | validation: 0.10540669175319817]
	TIME [epoch: 8.32 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09234511294068616		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.09234511294068616 | validation: 0.08388627732692402]
	TIME [epoch: 8.33 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07901596726527318		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.07901596726527318 | validation: 0.07508580791463548]
	TIME [epoch: 8.41 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07405926498551707		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.07405926498551707 | validation: 0.13027196878486252]
	TIME [epoch: 8.38 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1348632968565298		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.1348632968565298 | validation: 0.12207403142861986]
	TIME [epoch: 8.36 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10840178522880442		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.10840178522880442 | validation: 0.11418864091173916]
	TIME [epoch: 8.33 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10358827768660939		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.10358827768660939 | validation: 0.09663118680493898]
	TIME [epoch: 8.32 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0813796668949201		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.0813796668949201 | validation: 0.07279104442828056]
	TIME [epoch: 8.36 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0699285513096121		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.0699285513096121 | validation: 0.074279447710976]
	TIME [epoch: 8.37 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06918432769618324		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.06918432769618324 | validation: 0.07100384597864118]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_656.pth
	Model improved!!!
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11598632789008		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.11598632789008 | validation: 0.0991726000139507]
	TIME [epoch: 8.4 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.093396779674109		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.093396779674109 | validation: 0.08463644923248298]
	TIME [epoch: 8.32 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08025530370813325		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.08025530370813325 | validation: 0.08403035658914323]
	TIME [epoch: 8.32 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07558087533343807		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.07558087533343807 | validation: 0.07987844778612621]
	TIME [epoch: 8.37 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07148538141992682		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.07148538141992682 | validation: 0.07682193219854806]
	TIME [epoch: 8.33 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07042615312671917		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.07042615312671917 | validation: 0.0729884321126536]
	TIME [epoch: 8.32 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07603982558578307		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.07603982558578307 | validation: 0.07778969414136203]
	TIME [epoch: 8.32 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07459711601841074		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.07459711601841074 | validation: 0.08064644366133039]
	TIME [epoch: 8.36 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07712954382015196		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.07712954382015196 | validation: 0.07465436534968789]
	TIME [epoch: 8.38 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.068642328333631		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.068642328333631 | validation: 0.06992381902438656]
	TIME [epoch: 8.42 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_666.pth
	Model improved!!!
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.067261454376308		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.067261454376308 | validation: 0.07001934690719461]
	TIME [epoch: 8.32 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07757196041071326		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.07757196041071326 | validation: 0.07577158602625386]
	TIME [epoch: 8.37 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07043079631356489		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.07043079631356489 | validation: 0.08104424731926434]
	TIME [epoch: 8.37 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07300699174316569		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.07300699174316569 | validation: 0.08404449517654447]
	TIME [epoch: 8.37 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07297246303422109		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.07297246303422109 | validation: 0.07117834480774364]
	TIME [epoch: 8.35 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06670918611471553		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.06670918611471553 | validation: 0.06961048429252709]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_672.pth
	Model improved!!!
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0682522137370188		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.0682522137370188 | validation: 0.08106785129521224]
	TIME [epoch: 8.42 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07346262256124357		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.07346262256124357 | validation: 0.08421075927092458]
	TIME [epoch: 8.31 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0721131938787877		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.0721131938787877 | validation: 0.07284070214743721]
	TIME [epoch: 8.31 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08491984637062969		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.08491984637062969 | validation: 0.10760961964433988]
	TIME [epoch: 8.32 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09283114676894375		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.09283114676894375 | validation: 0.07809953334581013]
	TIME [epoch: 8.37 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08459139498195109		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.08459139498195109 | validation: 0.10457097963132285]
	TIME [epoch: 8.35 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10020816583257952		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.10020816583257952 | validation: 0.09932887608083427]
	TIME [epoch: 8.38 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08931394854160307		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.08931394854160307 | validation: 0.07602793722375029]
	TIME [epoch: 8.38 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07755031032891249		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.07755031032891249 | validation: 0.07458445498880592]
	TIME [epoch: 8.33 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07347871724017138		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.07347871724017138 | validation: 0.08204331589135339]
	TIME [epoch: 8.34 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07576871154493993		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.07576871154493993 | validation: 0.08036291356323785]
	TIME [epoch: 8.36 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07261388121868234		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.07261388121868234 | validation: 0.07823081063414858]
	TIME [epoch: 8.34 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07558402219614753		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.07558402219614753 | validation: 0.08536012759077918]
	TIME [epoch: 8.33 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07048860084042466		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.07048860084042466 | validation: 0.0880822795110687]
	TIME [epoch: 8.32 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0724176582692549		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.0724176582692549 | validation: 0.07666279754369391]
	TIME [epoch: 8.32 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0718341914895291		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.0718341914895291 | validation: 0.07283854913391571]
	TIME [epoch: 8.37 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0713403863243696		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.0713403863243696 | validation: 0.07230161437298138]
	TIME [epoch: 8.33 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06817248588559185		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.06817248588559185 | validation: 0.06803149910764882]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_690.pth
	Model improved!!!
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0697177278696029		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.0697177278696029 | validation: 0.07826929674266399]
	TIME [epoch: 8.39 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06942015213408385		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.06942015213408385 | validation: 0.08047334170333614]
	TIME [epoch: 8.3 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07430674363581617		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.07430674363581617 | validation: 0.08404655726010829]
	TIME [epoch: 8.31 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07379577891151334		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.07379577891151334 | validation: 0.07077888392704104]
	TIME [epoch: 8.35 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06685787351435282		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.06685787351435282 | validation: 0.06712981918825989]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_695.pth
	Model improved!!!
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06715706856985823		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.06715706856985823 | validation: 0.07899936943566466]
	TIME [epoch: 8.39 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07109310787809937		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.07109310787809937 | validation: 0.0788982307385811]
	TIME [epoch: 8.3 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08272940591636599		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.08272940591636599 | validation: 0.11453010904016947]
	TIME [epoch: 8.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09126315887515062		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.09126315887515062 | validation: 0.09311904547424535]
	TIME [epoch: 8.34 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0806513541467654		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.0806513541467654 | validation: 0.09520002625484718]
	TIME [epoch: 8.32 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07127489561775036		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.07127489561775036 | validation: 0.08734835288718583]
	TIME [epoch: 8.3 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07005484729556373		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.07005484729556373 | validation: 0.08201401889380978]
	TIME [epoch: 8.44 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06758632956052722		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.06758632956052722 | validation: 0.079312640158088]
	TIME [epoch: 8.32 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0699771671895451		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.0699771671895451 | validation: 0.07560176543568115]
	TIME [epoch: 8.31 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06761958964972424		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.06761958964972424 | validation: 0.06971128050784614]
	TIME [epoch: 8.35 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06685529738956354		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.06685529738956354 | validation: 0.07561200431654817]
	TIME [epoch: 8.32 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06766103433426293		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.06766103433426293 | validation: 0.07650269306971776]
	TIME [epoch: 8.31 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06974062141322322		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.06974062141322322 | validation: 0.07192043141768023]
	TIME [epoch: 8.31 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06649275699367926		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.06649275699367926 | validation: 0.07185908067022195]
	TIME [epoch: 8.36 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0716287147711475		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.0716287147711475 | validation: 0.08432220513294483]
	TIME [epoch: 8.4 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07514804692197097		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.07514804692197097 | validation: 0.07571442951746607]
	TIME [epoch: 8.41 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07201002615086585		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.07201002615086585 | validation: 0.09088151466629868]
	TIME [epoch: 8.37 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07938736547584735		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.07938736547584735 | validation: 0.08797755137558161]
	TIME [epoch: 8.37 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07544919617655141		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.07544919617655141 | validation: 0.0793064566235703]
	TIME [epoch: 8.37 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07072940082952905		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.07072940082952905 | validation: 0.07586138083217979]
	TIME [epoch: 8.33 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07014288921048022		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.07014288921048022 | validation: 0.08315845581490904]
	TIME [epoch: 8.37 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0748372836131875		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.0748372836131875 | validation: 0.07319427502714142]
	TIME [epoch: 8.32 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07621545275860511		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.07621545275860511 | validation: 0.07223594178634314]
	TIME [epoch: 8.31 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0817228573009815		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.0817228573009815 | validation: 0.10707156740720242]
	TIME [epoch: 8.31 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08719114580319967		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.08719114580319967 | validation: 0.08676177212560249]
	TIME [epoch: 8.31 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07293435331710516		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.07293435331710516 | validation: 0.08282945365503208]
	TIME [epoch: 8.32 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07280714938765181		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.07280714938765181 | validation: 0.099593466176877]
	TIME [epoch: 8.35 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08237946630057198		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.08237946630057198 | validation: 0.11696658841669486]
	TIME [epoch: 8.31 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10808809134374384		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.10808809134374384 | validation: 0.13702523249736237]
	TIME [epoch: 8.3 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11421027957154678		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.11421027957154678 | validation: 0.1341989942695148]
	TIME [epoch: 8.3 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09477386835564242		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.09477386835564242 | validation: 0.11300347835252764]
	TIME [epoch: 8.3 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08238476869765049		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.08238476869765049 | validation: 0.10152473457751715]
	TIME [epoch: 8.32 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07637656042999474		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.07637656042999474 | validation: 0.08634688309668254]
	TIME [epoch: 8.34 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07495559825173334		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.07495559825173334 | validation: 0.08661407184427522]
	TIME [epoch: 8.31 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0707898594748803		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.0707898594748803 | validation: 0.07851148891636525]
	TIME [epoch: 8.37 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06918620750670744		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.06918620750670744 | validation: 0.07459060940867672]
	TIME [epoch: 8.36 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06800537535299797		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.06800537535299797 | validation: 0.07833254883453734]
	TIME [epoch: 8.31 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06546032724701643		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.06546032724701643 | validation: 0.07512421005852918]
	TIME [epoch: 8.36 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09099153701468732		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.09099153701468732 | validation: 0.1077918613388481]
	TIME [epoch: 8.3 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09618433862912047		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.09618433862912047 | validation: 0.0895390610024171]
	TIME [epoch: 8.29 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08436776819795457		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.08436776819795457 | validation: 0.08298135810939988]
	TIME [epoch: 8.3 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08002684060258797		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.08002684060258797 | validation: 0.08323793770304477]
	TIME [epoch: 8.3 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07735798687822712		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.07735798687822712 | validation: 0.08142237856634112]
	TIME [epoch: 8.31 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06798258224690722		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.06798258224690722 | validation: 0.07907320883879013]
	TIME [epoch: 8.35 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06625527572673443		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.06625527572673443 | validation: 0.06853534891877297]
	TIME [epoch: 8.32 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06516166904080557		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.06516166904080557 | validation: 0.07748517585110443]
	TIME [epoch: 8.3 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0805118504174744		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.0805118504174744 | validation: 0.07013145100283844]
	TIME [epoch: 8.29 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0726398568323018		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.0726398568323018 | validation: 0.07472796421858456]
	TIME [epoch: 8.3 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06857149926292884		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.06857149926292884 | validation: 0.06531315278879546]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_744.pth
	Model improved!!!
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06547884574175108		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.06547884574175108 | validation: 0.11286622118014597]
	TIME [epoch: 8.44 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13119453236081277		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.13119453236081277 | validation: 0.11311566681219018]
	TIME [epoch: 8.28 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10110850726771353		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.10110850726771353 | validation: 0.11910438296367878]
	TIME [epoch: 8.29 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08302460261338361		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.08302460261338361 | validation: 0.10540632166954608]
	TIME [epoch: 8.29 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1071867973623011		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.1071867973623011 | validation: 0.3102821469581057]
	TIME [epoch: 8.32 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10107125156112759		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.10107125156112759 | validation: 0.17025871686182695]
	TIME [epoch: 8.34 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07912142978241196		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.07912142978241196 | validation: 0.11626864065419397]
	TIME [epoch: 8.31 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07561702010250979		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.07561702010250979 | validation: 0.0837893007532686]
	TIME [epoch: 8.31 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0713661505651833		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.0713661505651833 | validation: 0.0767760792742982]
	TIME [epoch: 8.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06808134312628869		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.06808134312628869 | validation: 0.07412271169498606]
	TIME [epoch: 8.32 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06921130147424726		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.06921130147424726 | validation: 0.07441524673444914]
	TIME [epoch: 8.36 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06578084264372112		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.06578084264372112 | validation: 0.06947628416653852]
	TIME [epoch: 8.39 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06652168379065061		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.06652168379065061 | validation: 0.07074177568957637]
	TIME [epoch: 8.35 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06784575290188753		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.06784575290188753 | validation: 0.06982293326856427]
	TIME [epoch: 8.34 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06626770762213699		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.06626770762213699 | validation: 0.07185211548942022]
	TIME [epoch: 8.31 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06595935333803044		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.06595935333803044 | validation: 0.06849191095099226]
	TIME [epoch: 8.29 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06478574003906452		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.06478574003906452 | validation: 0.06646515217156532]
	TIME [epoch: 8.33 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06573796356193355		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.06573796356193355 | validation: 0.06611964686352068]
	TIME [epoch: 8.31 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0636702587832993		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.0636702587832993 | validation: 0.075712655456483]
	TIME [epoch: 8.29 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06936214331109043		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.06936214331109043 | validation: 0.06942620644712111]
	TIME [epoch: 8.29 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06751608382148223		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.06751608382148223 | validation: 0.07047516538489823]
	TIME [epoch: 8.29 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06861113502834408		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.06861113502834408 | validation: 0.0684625188178245]
	TIME [epoch: 8.3 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06750373004053356		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.06750373004053356 | validation: 0.07173215836561686]
	TIME [epoch: 8.39 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06563346604613494		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.06563346604613494 | validation: 0.0739222676283234]
	TIME [epoch: 8.36 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06665705137987504		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.06665705137987504 | validation: 0.06858992157046817]
	TIME [epoch: 8.33 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06490762530149834		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.06490762530149834 | validation: 0.0694031455345806]
	TIME [epoch: 8.29 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0669179887438317		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.0669179887438317 | validation: 0.0774442759949332]
	TIME [epoch: 8.3 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0669768576854594		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.0669768576854594 | validation: 0.07250751769429822]
	TIME [epoch: 8.31 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06520690275508462		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.06520690275508462 | validation: 0.07463319706865149]
	TIME [epoch: 8.32 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07004416375277984		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.07004416375277984 | validation: 0.07284969163880053]
	TIME [epoch: 8.29 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06706796822773217		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.06706796822773217 | validation: 0.07317231928373744]
	TIME [epoch: 8.33 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07304075331880916		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.07304075331880916 | validation: 0.08777355000920525]
	TIME [epoch: 8.35 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07703688164662018		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.07703688164662018 | validation: 0.07654266504201576]
	TIME [epoch: 8.33 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06874031009350927		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.06874031009350927 | validation: 0.07123308840831562]
	TIME [epoch: 8.33 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06489340424297844		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.06489340424297844 | validation: 0.07100995920262484]
	TIME [epoch: 8.31 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06479555665883453		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.06479555665883453 | validation: 0.07523803221019765]
	TIME [epoch: 8.29 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06750348847093285		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.06750348847093285 | validation: 0.0704427382358564]
	TIME [epoch: 8.3 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08646709108443743		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.08646709108443743 | validation: 0.0787363353060949]
	TIME [epoch: 8.29 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07785687822360095		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.07785687822360095 | validation: 0.0735257010396777]
	TIME [epoch: 8.33 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0723186026730069		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.0723186026730069 | validation: 0.07127552420656691]
	TIME [epoch: 8.4 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06759163009868979		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.06759163009868979 | validation: 0.07443149600797058]
	TIME [epoch: 8.35 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07110859615765629		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.07110859615765629 | validation: 0.09176403810299796]
	TIME [epoch: 8.35 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07090543615547687		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.07090543615547687 | validation: 0.0755997456215993]
	TIME [epoch: 8.34 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06790140488917183		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.06790140488917183 | validation: 0.07711789994412606]
	TIME [epoch: 8.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0647966193167717		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.0647966193167717 | validation: 0.07509515065852324]
	TIME [epoch: 8.32 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06602914131986813		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.06602914131986813 | validation: 0.08437295583152607]
	TIME [epoch: 8.33 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09923932263490107		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.09923932263490107 | validation: 0.08509583744212543]
	TIME [epoch: 8.3 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0785775038962926		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.0785775038962926 | validation: 0.07549095225396796]
	TIME [epoch: 8.29 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07285160604674133		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.07285160604674133 | validation: 0.06950861512519384]
	TIME [epoch: 8.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06995824271986795		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.06995824271986795 | validation: 0.07103297974672768]
	TIME [epoch: 8.29 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06885754428192048		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.06885754428192048 | validation: 0.07033819792896771]
	TIME [epoch: 8.37 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0667425580527139		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.0667425580527139 | validation: 0.07571266471372132]
	TIME [epoch: 8.38 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06856375582572082		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.06856375582572082 | validation: 0.09835490823947299]
	TIME [epoch: 8.33 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0830822933495492		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.0830822933495492 | validation: 0.08269049449059501]
	TIME [epoch: 8.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0763289606801751		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.0763289606801751 | validation: 0.07099904031448742]
	TIME [epoch: 8.3 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07368679264988634		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.07368679264988634 | validation: 0.0758901909247226]
	TIME [epoch: 8.62 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0655066306515931		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.0655066306515931 | validation: 0.07478949746702854]
	TIME [epoch: 8.36 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06821549696588171		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.06821549696588171 | validation: 0.0797566802682924]
	TIME [epoch: 8.31 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07020958894210703		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.07020958894210703 | validation: 0.09920234946625688]
	TIME [epoch: 8.31 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07273261260118624		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.07273261260118624 | validation: 0.09510269017582344]
	TIME [epoch: 8.3 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07237398979227772		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.07237398979227772 | validation: 0.08706527673095002]
	TIME [epoch: 8.31 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06772706265629753		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.06772706265629753 | validation: 0.1029331899842016]
	TIME [epoch: 8.32 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06799760936457523		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.06799760936457523 | validation: 0.07292818443654034]
	TIME [epoch: 8.34 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06687455483419817		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.06687455483419817 | validation: 0.07160826393543368]
	TIME [epoch: 8.3 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06885609000884962		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.06885609000884962 | validation: 0.0705734426130282]
	TIME [epoch: 8.31 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06859281822189553		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.06859281822189553 | validation: 0.0765382854318697]
	TIME [epoch: 8.34 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06879848177808501		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.06879848177808501 | validation: 0.07991073020608634]
	TIME [epoch: 8.36 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07253441728157108		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.07253441728157108 | validation: 0.07551316577239824]
	TIME [epoch: 8.41 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06944653578248425		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.06944653578248425 | validation: 0.07757422096742936]
	TIME [epoch: 8.36 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07296137177663939		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.07296137177663939 | validation: 0.08380112978038265]
	TIME [epoch: 8.33 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08929421744377022		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.08929421744377022 | validation: 0.08025525780029526]
	TIME [epoch: 8.3 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06828722565678129		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.06828722565678129 | validation: 0.07350608542756809]
	TIME [epoch: 8.3 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06535097717411202		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.06535097717411202 | validation: 0.0749435258711245]
	TIME [epoch: 8.31 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06492627541805125		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.06492627541805125 | validation: 0.07158175798911652]
	TIME [epoch: 8.35 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06373571777848026		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.06373571777848026 | validation: 0.07396319327738138]
	TIME [epoch: 8.32 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06360405759747587		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.06360405759747587 | validation: 0.07586537638271361]
	TIME [epoch: 8.37 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07007358711503395		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.07007358711503395 | validation: 0.15230302778670687]
	TIME [epoch: 8.35 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0996338747363862		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.0996338747363862 | validation: 0.12858742626894998]
	TIME [epoch: 8.3 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08759006996118625		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.08759006996118625 | validation: 0.11500374440911974]
	TIME [epoch: 8.34 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08326005358671246		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.08326005358671246 | validation: 0.09843894267320619]
	TIME [epoch: 8.34 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07779538082898567		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.07779538082898567 | validation: 0.08652687271589729]
	TIME [epoch: 8.36 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07545558541832803		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.07545558541832803 | validation: 0.08199191497187008]
	TIME [epoch: 8.35 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07024325203823627		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.07024325203823627 | validation: 0.07385277210786928]
	TIME [epoch: 8.36 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07038547387284767		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.07038547387284767 | validation: 0.07570167732485328]
	TIME [epoch: 8.37 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0688290569588669		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.0688290569588669 | validation: 0.0736747018838837]
	TIME [epoch: 8.36 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06774822854091317		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.06774822854091317 | validation: 0.07526888524975567]
	TIME [epoch: 8.31 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06889340473804438		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.06889340473804438 | validation: 0.07181570215048945]
	TIME [epoch: 8.3 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06612229810605706		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.06612229810605706 | validation: 0.07837123288577087]
	TIME [epoch: 8.3 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06872649825939534		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.06872649825939534 | validation: 0.07160721903910591]
	TIME [epoch: 8.3 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07359300856356238		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.07359300856356238 | validation: 0.1140109682760905]
	TIME [epoch: 8.31 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10186066980455603		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.10186066980455603 | validation: 0.09405097728497866]
	TIME [epoch: 8.35 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08601355507764392		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.08601355507764392 | validation: 0.08764357091967831]
	TIME [epoch: 8.3 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07826678082021271		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.07826678082021271 | validation: 0.08032287185143477]
	TIME [epoch: 8.3 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0747509855166057		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.0747509855166057 | validation: 0.0834437994258537]
	TIME [epoch: 8.31 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07427438786163341		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.07427438786163341 | validation: 0.08515290966617532]
	TIME [epoch: 8.31 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07349693686748769		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.07349693686748769 | validation: 0.07993875799238478]
	TIME [epoch: 8.34 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07205668445127715		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.07205668445127715 | validation: 0.07528556043643578]
	TIME [epoch: 8.32 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0702000185223102		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.0702000185223102 | validation: 0.07349035948550028]
	TIME [epoch: 8.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06669007117922321		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.06669007117922321 | validation: 0.07395307488845028]
	TIME [epoch: 8.3 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06891355931743179		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.06891355931743179 | validation: 0.08156950495091585]
	TIME [epoch: 8.32 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06938838322619353		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.06938838322619353 | validation: 0.07852857084737783]
	TIME [epoch: 8.36 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06787019613533765		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.06787019613533765 | validation: 0.08263936348192583]
	TIME [epoch: 8.39 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0700377797472236		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.0700377797472236 | validation: 0.07926568809417896]
	TIME [epoch: 8.34 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06868949538542007		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.06868949538542007 | validation: 0.07141085065892959]
	TIME [epoch: 8.3 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06795122705401911		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.06795122705401911 | validation: 0.08345640759240816]
	TIME [epoch: 8.3 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0766320025475474		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.0766320025475474 | validation: 0.0810998305967592]
	TIME [epoch: 8.34 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07077171545043667		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.07077171545043667 | validation: 0.07848870356494367]
	TIME [epoch: 8.38 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06861444568535893		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.06861444568535893 | validation: 0.07057770756536463]
	TIME [epoch: 8.38 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06534345249859613		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.06534345249859613 | validation: 0.06997441034869026]
	TIME [epoch: 8.31 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06903341188207265		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.06903341188207265 | validation: 0.07622717552422273]
	TIME [epoch: 8.31 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06838847854747582		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.06838847854747582 | validation: 0.07741721249195985]
	TIME [epoch: 8.31 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0679908940322427		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.0679908940322427 | validation: 0.06672847513829225]
	TIME [epoch: 8.3 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06658681775073158		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.06658681775073158 | validation: 0.07362742079593873]
	TIME [epoch: 8.34 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06887657486313915		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.06887657486313915 | validation: 0.07234015868855069]
	TIME [epoch: 8.32 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06904470668654135		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.06904470668654135 | validation: 0.0749196406256727]
	TIME [epoch: 8.3 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06822551889455913		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.06822551889455913 | validation: 0.07033118470393607]
	TIME [epoch: 8.3 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06500135398880338		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.06500135398880338 | validation: 0.07008171839025981]
	TIME [epoch: 8.3 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06418699991958597		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.06418699991958597 | validation: 0.07065794939467217]
	TIME [epoch: 8.31 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06712359476489368		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.06712359476489368 | validation: 0.08289189040754841]
	TIME [epoch: 8.35 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07118407942185251		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.07118407942185251 | validation: 0.06905335618706386]
	TIME [epoch: 8.31 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06500347425391796		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.06500347425391796 | validation: 0.06930902196851203]
	TIME [epoch: 8.3 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0635059494411663		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.0635059494411663 | validation: 0.0645175295222926]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_866.pth
	Model improved!!!
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06277480110075231		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.06277480110075231 | validation: 0.07125353159922644]
	TIME [epoch: 8.38 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06094205093715238		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.06094205093715238 | validation: 0.06807449943796734]
	TIME [epoch: 8.32 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06370860286597559		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.06370860286597559 | validation: 0.08039869697117934]
	TIME [epoch: 8.33 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06770820713287809		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.06770820713287809 | validation: 0.07670448965485036]
	TIME [epoch: 8.29 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06618226329340529		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.06618226329340529 | validation: 0.0707494104009542]
	TIME [epoch: 8.29 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.063697443635912		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.063697443635912 | validation: 0.07124194098004319]
	TIME [epoch: 8.3 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06180827360236589		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.06180827360236589 | validation: 0.07131726458638132]
	TIME [epoch: 8.29 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0642804034052974		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.0642804034052974 | validation: 0.08082608822434553]
	TIME [epoch: 8.34 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06510536390557814		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.06510536390557814 | validation: 0.07167098116635148]
	TIME [epoch: 8.31 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0660510581282849		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.0660510581282849 | validation: 0.07632882623745588]
	TIME [epoch: 8.29 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06478877517197658		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.06478877517197658 | validation: 0.07022247979918644]
	TIME [epoch: 8.3 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06612632313195124		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.06612632313195124 | validation: 0.08841429103001537]
	TIME [epoch: 8.3 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08721720852789075		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.08721720852789075 | validation: 0.15020306493553348]
	TIME [epoch: 8.34 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10101619749689784		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.10101619749689784 | validation: 0.12959894900396823]
	TIME [epoch: 8.4 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09332504869944397		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.09332504869944397 | validation: 0.11533277384002151]
	TIME [epoch: 8.34 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08865927942713162		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.08865927942713162 | validation: 0.09926245328020289]
	TIME [epoch: 8.34 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08445287510407873		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.08445287510407873 | validation: 0.09807283886560927]
	TIME [epoch: 8.33 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08149167767651722		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.08149167767651722 | validation: 0.09412736048680433]
	TIME [epoch: 8.3 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07748078405890446		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.07748078405890446 | validation: 0.08125980587126125]
	TIME [epoch: 8.32 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07364370793686081		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.07364370793686081 | validation: 0.07758334305797618]
	TIME [epoch: 8.33 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07370954149455725		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.07370954149455725 | validation: 0.08987234833993538]
	TIME [epoch: 8.3 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07230922714834521		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.07230922714834521 | validation: 0.08803314003768636]
	TIME [epoch: 8.3 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07101221611058064		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.07101221611058064 | validation: 0.07698601503241528]
	TIME [epoch: 8.3 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07012615128295116		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.07012615128295116 | validation: 0.07539431682017995]
	TIME [epoch: 8.3 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0687748142111459		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.0687748142111459 | validation: 0.08097674586495336]
	TIME [epoch: 8.35 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06600871766113223		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.06600871766113223 | validation: 0.0784554641931687]
	TIME [epoch: 8.3 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07096366174985566		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.07096366174985566 | validation: 0.07273033345568097]
	TIME [epoch: 8.3 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06545645089620844		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.06545645089620844 | validation: 0.07264394050415551]
	TIME [epoch: 8.3 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06484633106887616		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.06484633106887616 | validation: 0.0743451097180052]
	TIME [epoch: 8.3 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06942692094127609		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.06942692094127609 | validation: 0.08639030970856515]
	TIME [epoch: 8.31 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07355443461857887		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.07355443461857887 | validation: 0.07819747076929126]
	TIME [epoch: 8.35 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06765260188763722		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.06765260188763722 | validation: 0.07170588799837507]
	TIME [epoch: 8.3 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06644564067452967		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.06644564067452967 | validation: 0.06642993117128845]
	TIME [epoch: 8.3 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06533719326637083		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.06533719326637083 | validation: 0.07061127395882702]
	TIME [epoch: 8.3 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06340736294485741		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.06340736294485741 | validation: 0.06624511713073761]
	TIME [epoch: 8.29 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06472808834418334		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.06472808834418334 | validation: 0.06962380998480708]
	TIME [epoch: 8.42 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06292019106089378		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.06292019106089378 | validation: 0.06185485153066952]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_903.pth
	Model improved!!!
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06478437740635469		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.06478437740635469 | validation: 0.062134572841941946]
	TIME [epoch: 8.39 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06236019560131817		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.06236019560131817 | validation: 0.06455792511238417]
	TIME [epoch: 8.3 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06464300297084051		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.06464300297084051 | validation: 0.06940244245410532]
	TIME [epoch: 8.3 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0645617859831098		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.0645617859831098 | validation: 0.06661520640851765]
	TIME [epoch: 8.3 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06360676553747371		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.06360676553747371 | validation: 0.07279847437911374]
	TIME [epoch: 8.34 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06730897633108862		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.06730897633108862 | validation: 0.07169094415842704]
	TIME [epoch: 8.3 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06724498901364914		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.06724498901364914 | validation: 0.06959615698372726]
	TIME [epoch: 8.29 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06431103678849986		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.06431103678849986 | validation: 0.06666296320503477]
	TIME [epoch: 8.29 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06500184440453982		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.06500184440453982 | validation: 0.07334234286465025]
	TIME [epoch: 8.29 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0650892114896386		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.0650892114896386 | validation: 0.07019358750284548]
	TIME [epoch: 8.3 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06542125319284434		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.06542125319284434 | validation: 0.07099050917343813]
	TIME [epoch: 8.33 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06533230022183667		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.06533230022183667 | validation: 0.07527297485011487]
	TIME [epoch: 8.29 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07017611937045219		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.07017611937045219 | validation: 0.07634784355431601]
	TIME [epoch: 8.3 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06613178097020377		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.06613178097020377 | validation: 0.06939769775224677]
	TIME [epoch: 8.36 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06136681343714349		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.06136681343714349 | validation: 0.07204783664914975]
	TIME [epoch: 8.35 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06364317534939415		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.06364317534939415 | validation: 0.06365869647161845]
	TIME [epoch: 8.38 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06505266561167207		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.06505266561167207 | validation: 0.06972731948012775]
	TIME [epoch: 8.37 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0664536339358069		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.0664536339358069 | validation: 0.06672709377016796]
	TIME [epoch: 8.34 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06317715140180513		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.06317715140180513 | validation: 0.06832324207189074]
	TIME [epoch: 8.29 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06090905061979493		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.06090905061979493 | validation: 0.07179232871934699]
	TIME [epoch: 8.29 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061697935902254024		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.061697935902254024 | validation: 0.07292696564759141]
	TIME [epoch: 8.29 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06268769736122867		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.06268769736122867 | validation: 0.06903318760795146]
	TIME [epoch: 8.35 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06476437826217848		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.06476437826217848 | validation: 0.07419500759354655]
	TIME [epoch: 8.36 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0665458483976759		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.0665458483976759 | validation: 0.07044210269418673]
	TIME [epoch: 8.34 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06158236176803786		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.06158236176803786 | validation: 0.06894978359015849]
	TIME [epoch: 8.34 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06254891646973024		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.06254891646973024 | validation: 0.06664611434907672]
	TIME [epoch: 8.34 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06322623550589043		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.06322623550589043 | validation: 0.07063987578676842]
	TIME [epoch: 8.33 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06366744280307815		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.06366744280307815 | validation: 0.07201447832250024]
	TIME [epoch: 8.32 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06448745067951941		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.06448745067951941 | validation: 0.08095396017746645]
	TIME [epoch: 8.29 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07545623959838142		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.07545623959838142 | validation: 0.08372779779279228]
	TIME [epoch: 8.3 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07190122979596082		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.07190122979596082 | validation: 0.09194244284466827]
	TIME [epoch: 8.3 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09077135950382124		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.09077135950382124 | validation: 0.21438001923815844]
	TIME [epoch: 8.29 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07696156231685698		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.07696156231685698 | validation: 0.17906316786912885]
	TIME [epoch: 8.33 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06822255349059583		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.06822255349059583 | validation: 0.10542270632870535]
	TIME [epoch: 8.3 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06568002249246106		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.06568002249246106 | validation: 0.09506500142412896]
	TIME [epoch: 8.29 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06545496056716929		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.06545496056716929 | validation: 0.08493770906976209]
	TIME [epoch: 8.28 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06574119971850936		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.06574119971850936 | validation: 0.07661028115147239]
	TIME [epoch: 8.29 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06830167602089507		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.06830167602089507 | validation: 0.08113587372660222]
	TIME [epoch: 8.3 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06732710308724649		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.06732710308724649 | validation: 0.07557008718900127]
	TIME [epoch: 8.33 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06726332821111662		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.06726332821111662 | validation: 0.07568937230498217]
	TIME [epoch: 8.34 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06538483863973228		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.06538483863973228 | validation: 0.07182158145419737]
	TIME [epoch: 8.36 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06545410157545221		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.06545410157545221 | validation: 0.07253750531480821]
	TIME [epoch: 8.33 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06611637778856203		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.06611637778856203 | validation: 0.06715730425329182]
	TIME [epoch: 8.29 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06319768241615045		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.06319768241615045 | validation: 0.07254629978593842]
	TIME [epoch: 8.31 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06574342205806497		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.06574342205806497 | validation: 0.07683739354284236]
	TIME [epoch: 8.32 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06691380820553765		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.06691380820553765 | validation: 0.0782013135931593]
	TIME [epoch: 8.29 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0659347904669122		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.0659347904669122 | validation: 0.06934908860860234]
	TIME [epoch: 8.29 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06312470315643379		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.06312470315643379 | validation: 0.08181112226631843]
	TIME [epoch: 8.29 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07003955529106869		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.07003955529106869 | validation: 0.06968331073508005]
	TIME [epoch: 8.29 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06336191057903416		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.06336191057903416 | validation: 0.07128277132045299]
	TIME [epoch: 8.33 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06240021698222622		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.06240021698222622 | validation: 0.06678032948683546]
	TIME [epoch: 8.31 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061769874003831556		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.061769874003831556 | validation: 0.06641503505542883]
	TIME [epoch: 8.29 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06037448039398544		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.06037448039398544 | validation: 0.0675171368082174]
	TIME [epoch: 8.29 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059747636554263656		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.059747636554263656 | validation: 0.06667500480798298]
	TIME [epoch: 8.29 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060124247489474744		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.060124247489474744 | validation: 0.06421815408304683]
	TIME [epoch: 8.29 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06453736846557695		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.06453736846557695 | validation: 0.06674103211562274]
	TIME [epoch: 8.37 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06227346273991683		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.06227346273991683 | validation: 0.06630645338777366]
	TIME [epoch: 8.36 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06299782754222125		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.06299782754222125 | validation: 0.0662803828092436]
	TIME [epoch: 8.33 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05974274595870506		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.05974274595870506 | validation: 0.062169749291106]
	TIME [epoch: 8.29 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061855136610125656		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.061855136610125656 | validation: 0.06373924223072824]
	TIME [epoch: 8.29 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06181268564480685		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.06181268564480685 | validation: 0.06664499826027953]
	TIME [epoch: 8.31 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06893618312678346		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.06893618312678346 | validation: 0.06729952516675422]
	TIME [epoch: 8.32 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0668344803728689		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.0668344803728689 | validation: 0.06867586703718556]
	TIME [epoch: 8.29 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0640612671482659		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.0640612671482659 | validation: 0.06405037137621719]
	TIME [epoch: 8.29 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062265540516256976		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.062265540516256976 | validation: 0.06482812911943767]
	TIME [epoch: 8.3 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06341973552343425		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.06341973552343425 | validation: 0.06852630141386136]
	TIME [epoch: 8.29 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06151850971049061		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.06151850971049061 | validation: 0.06752411814595302]
	TIME [epoch: 8.33 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05987876456492617		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.05987876456492617 | validation: 0.061260783146175384]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_971.pth
	Model improved!!!
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06107495229688516		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.06107495229688516 | validation: 0.06215618388404514]
	TIME [epoch: 8.45 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06272713661174181		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.06272713661174181 | validation: 0.07061171168221211]
	TIME [epoch: 8.35 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06229397672430606		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.06229397672430606 | validation: 0.07245901124601575]
	TIME [epoch: 8.3 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06257554390999773		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.06257554390999773 | validation: 0.06570718129517791]
	TIME [epoch: 8.29 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062040521080298686		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.062040521080298686 | validation: 0.06757387144892679]
	TIME [epoch: 8.33 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06136628743253266		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.06136628743253266 | validation: 0.0643995769327417]
	TIME [epoch: 8.29 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060211123375448344		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.060211123375448344 | validation: 0.06650501604838524]
	TIME [epoch: 8.28 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06153643300507419		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.06153643300507419 | validation: 0.07079248235335281]
	TIME [epoch: 8.29 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06094513841010479		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.06094513841010479 | validation: 0.06774748821609253]
	TIME [epoch: 8.29 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06469413528470909		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.06469413528470909 | validation: 0.07749456463198152]
	TIME [epoch: 8.31 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06747284924243523		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.06747284924243523 | validation: 0.06760882969406526]
	TIME [epoch: 8.32 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0649711424587599		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.0649711424587599 | validation: 0.06586922256797453]
	TIME [epoch: 8.29 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06202191474122849		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.06202191474122849 | validation: 0.07196028931289614]
	TIME [epoch: 8.29 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06102763646818429		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.06102763646818429 | validation: 0.07360765509674741]
	TIME [epoch: 8.29 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062393808381598304		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.062393808381598304 | validation: 0.07190213776983204]
	TIME [epoch: 8.29 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060895076391377846		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.060895076391377846 | validation: 0.07531841216776375]
	TIME [epoch: 8.34 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06151454674616908		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.06151454674616908 | validation: 0.08188926478587032]
	TIME [epoch: 8.3 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060955343982560306		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.060955343982560306 | validation: 0.10588226956090244]
	TIME [epoch: 8.29 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06755890833153955		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.06755890833153955 | validation: 0.10022563728759049]
	TIME [epoch: 8.29 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06728430459363022		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.06728430459363022 | validation: 0.08412180188910144]
	TIME [epoch: 8.29 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06680260022437559		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.06680260022437559 | validation: 0.07050814268328447]
	TIME [epoch: 8.3 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06404900000788089		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.06404900000788089 | validation: 0.07390236746233404]
	TIME [epoch: 8.33 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06506999327394247		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.06506999327394247 | validation: 0.09553375567255705]
	TIME [epoch: 8.29 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06687032434145837		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.06687032434145837 | validation: 0.08176683607563813]
	TIME [epoch: 8.29 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06541838476867975		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.06541838476867975 | validation: 0.07815699055997341]
	TIME [epoch: 8.29 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06286877835552565		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.06286877835552565 | validation: 0.08131894250449798]
	TIME [epoch: 8.29 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06330401642300361		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.06330401642300361 | validation: 0.08266780832781057]
	TIME [epoch: 8.3 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06510263819516549		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.06510263819516549 | validation: 0.07584949107957803]
	TIME [epoch: 8.32 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06370715909442144		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.06370715909442144 | validation: 0.06315042025550746]
	TIME [epoch: 8.29 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06149015122330624		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.06149015122330624 | validation: 0.06549224407996482]
	TIME [epoch: 8.29 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06118888801722039		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.06118888801722039 | validation: 0.06449902656649054]
	TIME [epoch: 8.38 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061902672509689415		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.061902672509689415 | validation: 0.06353499949447701]
	TIME [epoch: 8.29 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06135991847263675		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.06135991847263675 | validation: 0.06529153242302989]
	TIME [epoch: 8.33 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059579438677407996		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.059579438677407996 | validation: 0.06474508441074739]
	TIME [epoch: 8.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059405973432392484		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.059405973432392484 | validation: 0.06651310646960523]
	TIME [epoch: 8.29 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05948263412072996		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.05948263412072996 | validation: 0.0676956888425916]
	TIME [epoch: 8.29 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06147385561993096		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.06147385561993096 | validation: 0.06412663272991442]
	TIME [epoch: 8.29 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059439658384222396		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.059439658384222396 | validation: 0.06287759155059697]
	TIME [epoch: 8.29 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06094963971542383		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.06094963971542383 | validation: 0.0625068601336873]
	TIME [epoch: 8.33 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06163680567977864		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.06163680567977864 | validation: 0.0667883899336143]
	TIME [epoch: 8.29 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06125696716743488		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.06125696716743488 | validation: 0.06874596588241952]
	TIME [epoch: 8.29 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06307700539123852		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.06307700539123852 | validation: 0.07794509932450627]
	TIME [epoch: 8.29 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06463490573260788		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.06463490573260788 | validation: 0.07686163118583383]
	TIME [epoch: 8.29 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07111332926997802		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.07111332926997802 | validation: 0.07552261521717907]
	TIME [epoch: 8.32 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06432226103132013		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.06432226103132013 | validation: 0.0711143202146529]
	TIME [epoch: 8.31 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06503938142585249		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.06503938142585249 | validation: 0.0677719994795562]
	TIME [epoch: 8.28 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0655883368288367		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.0655883368288367 | validation: 0.07003776777690507]
	TIME [epoch: 8.29 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06283522212236398		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.06283522212236398 | validation: 0.07655128235704836]
	TIME [epoch: 8.28 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06536075082866954		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.06536075082866954 | validation: 0.06641904948621012]
	TIME [epoch: 8.29 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06198876403635444		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.06198876403635444 | validation: 0.0633882306547976]
	TIME [epoch: 8.33 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06069323336521832		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.06069323336521832 | validation: 0.06376413251120835]
	TIME [epoch: 8.29 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059899356529958564		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.059899356529958564 | validation: 0.06444368395439105]
	TIME [epoch: 8.28 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06055050071941369		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.06055050071941369 | validation: 0.06507885839489519]
	TIME [epoch: 8.29 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05801639102274495		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.05801639102274495 | validation: 0.06844485921510397]
	TIME [epoch: 8.29 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06165509939296278		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.06165509939296278 | validation: 0.06684000855809544]
	TIME [epoch: 8.29 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061084014917149446		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.061084014917149446 | validation: 0.06615338260297965]
	TIME [epoch: 8.33 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06083413100135082		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.06083413100135082 | validation: 0.06331217513131623]
	TIME [epoch: 8.29 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0608686912852302		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.0608686912852302 | validation: 0.06295761309548364]
	TIME [epoch: 8.29 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06072025192223612		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.06072025192223612 | validation: 0.0637749966197095]
	TIME [epoch: 8.32 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06057107594478342		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.06057107594478342 | validation: 0.062035252629873694]
	TIME [epoch: 8.29 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05931746468506418		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.05931746468506418 | validation: 0.06316089957393481]
	TIME [epoch: 8.31 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059670718858077156		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.059670718858077156 | validation: 0.06454883964364957]
	TIME [epoch: 8.32 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06205011425067913		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.06205011425067913 | validation: 0.06432383481007942]
	TIME [epoch: 8.29 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05973494332349573		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.05973494332349573 | validation: 0.06411332728659967]
	TIME [epoch: 8.29 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06088779074707368		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.06088779074707368 | validation: 0.06675849286992118]
	TIME [epoch: 8.28 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059636187396253995		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.059636187396253995 | validation: 0.06415439857872075]
	TIME [epoch: 8.29 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05986360344474044		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.05986360344474044 | validation: 0.06734810147121202]
	TIME [epoch: 8.34 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06014717020633311		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.06014717020633311 | validation: 0.06618334566543804]
	TIME [epoch: 8.29 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06880563076019079		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.06880563076019079 | validation: 0.07510595772479914]
	TIME [epoch: 8.29 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06825441081596		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.06825441081596 | validation: 0.06765852802914063]
	TIME [epoch: 8.29 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06661467572636558		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.06661467572636558 | validation: 0.06781693192176338]
	TIME [epoch: 8.31 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06493945342135478		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.06493945342135478 | validation: 0.06728883710667369]
	TIME [epoch: 8.36 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06446194323753526		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.06446194323753526 | validation: 0.06395470264833539]
	TIME [epoch: 8.38 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06309565010834543		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.06309565010834543 | validation: 0.06901148159408428]
	TIME [epoch: 8.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06177012582417575		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.06177012582417575 | validation: 0.07043821130280156]
	TIME [epoch: 8.29 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06273840107573425		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.06273840107573425 | validation: 0.06623691950143704]
	TIME [epoch: 8.29 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06127465540431683		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.06127465540431683 | validation: 0.06655865669715705]
	TIME [epoch: 8.29 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060736447830929616		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.060736447830929616 | validation: 0.06567966424043574]
	TIME [epoch: 8.32 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06157387105118701		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.06157387105118701 | validation: 0.06272512365764812]
	TIME [epoch: 8.31 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06066768006740275		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.06066768006740275 | validation: 0.06566277311876695]
	TIME [epoch: 8.29 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06343234562504334		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.06343234562504334 | validation: 0.08845625131839796]
	TIME [epoch: 8.29 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0629196958845035		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.0629196958845035 | validation: 0.08623449636674504]
	TIME [epoch: 8.29 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0612966629203177		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.0612966629203177 | validation: 0.08461744882074546]
	TIME [epoch: 8.29 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062049063472156926		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.062049063472156926 | validation: 0.1275345025937728]
	TIME [epoch: 8.33 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06406851698821125		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.06406851698821125 | validation: 0.11485318511932771]
	TIME [epoch: 8.3 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07321746393683332		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.07321746393683332 | validation: 0.34275117872903976]
	TIME [epoch: 8.29 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09394225162042585		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.09394225162042585 | validation: 0.28356220366041085]
	TIME [epoch: 8.29 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08204602881144013		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.08204602881144013 | validation: 0.3291643890989848]
	TIME [epoch: 8.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08860863906323951		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.08860863906323951 | validation: 0.16659399039112666]
	TIME [epoch: 8.32 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07197886591722516		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.07197886591722516 | validation: 0.17218129457792047]
	TIME [epoch: 8.35 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07071225731344527		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.07071225731344527 | validation: 0.19119025244780086]
	TIME [epoch: 8.29 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.068899316799514		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.068899316799514 | validation: 0.15071202696617336]
	TIME [epoch: 8.28 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0695950710262343		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.0695950710262343 | validation: 0.14003502942076618]
	TIME [epoch: 8.29 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06684699394666664		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.06684699394666664 | validation: 0.13513165709498556]
	TIME [epoch: 8.29 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06485350270336787		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.06485350270336787 | validation: 0.11878353397292743]
	TIME [epoch: 8.32 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06523792250916975		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.06523792250916975 | validation: 0.09757072487793642]
	TIME [epoch: 8.31 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06489506899968037		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.06489506899968037 | validation: 0.10889589822910528]
	TIME [epoch: 8.29 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06314776590442006		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.06314776590442006 | validation: 0.09770153551485963]
	TIME [epoch: 8.29 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06384569755342101		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.06384569755342101 | validation: 0.0896463996825756]
	TIME [epoch: 8.29 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0684050495175783		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.0684050495175783 | validation: 0.08249218935263541]
	TIME [epoch: 8.29 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06848968679610112		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.06848968679610112 | validation: 0.07413531395431823]
	TIME [epoch: 8.34 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06504177444627748		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.06504177444627748 | validation: 0.07123169409802689]
	TIME [epoch: 8.3 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06520161873701465		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.06520161873701465 | validation: 0.06937437696718171]
	TIME [epoch: 8.29 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06056691599514996		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.06056691599514996 | validation: 0.06702003299429421]
	TIME [epoch: 8.29 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06176319797333948		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.06176319797333948 | validation: 0.06526144445632541]
	TIME [epoch: 8.29 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061329905193239706		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.061329905193239706 | validation: 0.06696240866048689]
	TIME [epoch: 8.29 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06188371816515331		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.06188371816515331 | validation: 0.07010544875302022]
	TIME [epoch: 8.33 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06240604143152784		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.06240604143152784 | validation: 0.06562755489273983]
	TIME [epoch: 8.29 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061120681854724385		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.061120681854724385 | validation: 0.06735896566642527]
	TIME [epoch: 8.29 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060664226055003635		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.060664226055003635 | validation: 0.06691650093745102]
	TIME [epoch: 8.29 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05928458899678914		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.05928458899678914 | validation: 0.06933586361299698]
	TIME [epoch: 8.29 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06046647465677926		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.06046647465677926 | validation: 0.06770621519393248]
	TIME [epoch: 8.32 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05899329427567406		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.05899329427567406 | validation: 0.06264935467964683]
	TIME [epoch: 8.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062093120444236594		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.062093120444236594 | validation: 0.07060375287403955]
	TIME [epoch: 8.29 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06376704894287448		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.06376704894287448 | validation: 0.06647497844425035]
	TIME [epoch: 8.29 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06564305574533594		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.06564305574533594 | validation: 0.06379639292800338]
	TIME [epoch: 8.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06362953415711908		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.06362953415711908 | validation: 0.06671806674766351]
	TIME [epoch: 8.29 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06146526721471171		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.06146526721471171 | validation: 0.06011867775336105]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_1089.pth
	Model improved!!!
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06141578247301911		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.06141578247301911 | validation: 0.06045432775780499]
	TIME [epoch: 8.3 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05944981039028541		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.05944981039028541 | validation: 0.0619473064499966]
	TIME [epoch: 8.29 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06137450223900659		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.06137450223900659 | validation: 0.06645360139303441]
	TIME [epoch: 8.29 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06184982259122162		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.06184982259122162 | validation: 0.06664040536515534]
	TIME [epoch: 8.29 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05997067879746394		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.05997067879746394 | validation: 0.06345198368760399]
	TIME [epoch: 8.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06285510193802862		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.06285510193802862 | validation: 0.06287753596221592]
	TIME [epoch: 8.32 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06075709903794842		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.06075709903794842 | validation: 0.07252028262638477]
	TIME [epoch: 8.29 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06266787102033615		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.06266787102033615 | validation: 0.07109302009984171]
	TIME [epoch: 8.28 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06204266377454449		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.06204266377454449 | validation: 0.07074955798897098]
	TIME [epoch: 8.28 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06360143276182088		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.06360143276182088 | validation: 0.07119751589807642]
	TIME [epoch: 8.29 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06328918206433541		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.06328918206433541 | validation: 0.06821154669090632]
	TIME [epoch: 8.33 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06178761823906038		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.06178761823906038 | validation: 0.06941006884010902]
	TIME [epoch: 8.31 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06010629588614184		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.06010629588614184 | validation: 0.062493723223763974]
	TIME [epoch: 8.31 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05890224336665695		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.05890224336665695 | validation: 0.06498450903903562]
	TIME [epoch: 8.31 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06135885656265419		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.06135885656265419 | validation: 0.06495508780354767]
	TIME [epoch: 8.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06153747252099162		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.06153747252099162 | validation: 0.0690242970699694]
	TIME [epoch: 8.31 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06213172369944081		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.06213172369944081 | validation: 0.06772910616836048]
	TIME [epoch: 8.35 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060795174514532285		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.060795174514532285 | validation: 0.06434523996642814]
	TIME [epoch: 8.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06062732004530662		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.06062732004530662 | validation: 0.06693131491330453]
	TIME [epoch: 8.31 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060633188140799		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.060633188140799 | validation: 0.07726795912818152]
	TIME [epoch: 8.3 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06259096482502048		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.06259096482502048 | validation: 0.0778412601410412]
	TIME [epoch: 8.31 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061277427124568226		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.061277427124568226 | validation: 0.10121457107916945]
	TIME [epoch: 8.33 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06521178845843432		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.06521178845843432 | validation: 0.09460899283102754]
	TIME [epoch: 8.33 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06378566653967754		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.06378566653967754 | validation: 0.08913180858211939]
	TIME [epoch: 8.31 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06452337769994777		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.06452337769994777 | validation: 0.07744493669715488]
	TIME [epoch: 8.31 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06236709946325516		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.06236709946325516 | validation: 0.07901412190355515]
	TIME [epoch: 8.31 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061866530709879755		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.061866530709879755 | validation: 0.08916429220355046]
	TIME [epoch: 8.31 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062410987426716635		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.062410987426716635 | validation: 0.0861932267387413]
	TIME [epoch: 8.36 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06401352350266366		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.06401352350266366 | validation: 0.08302594287555351]
	TIME [epoch: 8.32 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06202918095643812		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.06202918095643812 | validation: 0.0799864654338294]
	TIME [epoch: 8.31 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0640610079844257		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.0640610079844257 | validation: 0.08134213491227338]
	TIME [epoch: 8.31 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0603417032956168		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.0603417032956168 | validation: 0.08058506031895438]
	TIME [epoch: 8.31 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061011074190795864		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.061011074190795864 | validation: 0.08036341831033272]
	TIME [epoch: 8.31 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06129831007286493		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.06129831007286493 | validation: 0.07494040948278803]
	TIME [epoch: 8.36 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061043387800139765		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.061043387800139765 | validation: 0.08219937686923143]
	TIME [epoch: 8.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0637806024836098		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.0637806024836098 | validation: 0.08130437300344612]
	TIME [epoch: 8.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06580617631251681		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.06580617631251681 | validation: 0.07766489721799447]
	TIME [epoch: 8.31 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06346761501663828		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.06346761501663828 | validation: 0.0733500356924516]
	TIME [epoch: 8.31 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061947042252605625		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.061947042252605625 | validation: 0.07140122133485508]
	TIME [epoch: 8.33 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06204657308910021		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.06204657308910021 | validation: 0.0707481201848058]
	TIME [epoch: 8.35 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06186434054881099		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.06186434054881099 | validation: 0.07091668296131039]
	TIME [epoch: 8.31 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06043671409361269		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.06043671409361269 | validation: 0.07846586333827041]
	TIME [epoch: 8.31 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06580128780285366		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.06580128780285366 | validation: 0.07434173024488094]
	TIME [epoch: 8.3 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06359669277560495		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.06359669277560495 | validation: 0.06794651912924846]
	TIME [epoch: 8.31 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06123079766889315		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.06123079766889315 | validation: 0.0706903704250168]
	TIME [epoch: 8.35 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06204698746834888		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.06204698746834888 | validation: 0.06631286278553436]
	TIME [epoch: 8.32 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06227656509182854		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.06227656509182854 | validation: 0.06921035656533206]
	TIME [epoch: 8.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06281448692084662		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.06281448692084662 | validation: 0.06793743272737193]
	TIME [epoch: 8.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06167253893733485		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.06167253893733485 | validation: 0.06375902528321094]
	TIME [epoch: 8.31 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06160135031082032		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.06160135031082032 | validation: 0.06546737051213752]
	TIME [epoch: 8.31 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05985581706188478		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.05985581706188478 | validation: 0.05906087649087402]
	TIME [epoch: 8.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_1140.pth
	Model improved!!!
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05992966544787863		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.05992966544787863 | validation: 0.0664605149979341]
	TIME [epoch: 8.3 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059873500934669324		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.059873500934669324 | validation: 0.06421622082762693]
	TIME [epoch: 8.31 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059067489929135276		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.059067489929135276 | validation: 0.06391471548699018]
	TIME [epoch: 8.3 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06063707891275091		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.06063707891275091 | validation: 0.0639108165548507]
	TIME [epoch: 8.3 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06257176765922262		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.06257176765922262 | validation: 0.06608427425941348]
	TIME [epoch: 8.33 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05915192656357559		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.05915192656357559 | validation: 0.06729575339844016]
	TIME [epoch: 8.32 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06251472375610624		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.06251472375610624 | validation: 0.06171276265717531]
	TIME [epoch: 8.29 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05916436000871659		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.05916436000871659 | validation: 0.06345189841574837]
	TIME [epoch: 8.3 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05909440255728078		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.05909440255728078 | validation: 0.06305240873702483]
	TIME [epoch: 8.3 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0611724257670773		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.0611724257670773 | validation: 0.06276287650411162]
	TIME [epoch: 8.3 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05908574836125365		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.05908574836125365 | validation: 0.06426172044891984]
	TIME [epoch: 8.33 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059494236285596115		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.059494236285596115 | validation: 0.061659480009845155]
	TIME [epoch: 8.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05845499024836099		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.05845499024836099 | validation: 0.06370443759866055]
	TIME [epoch: 8.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06053018735798837		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.06053018735798837 | validation: 0.06949963378889737]
	TIME [epoch: 8.3 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06421274844659354		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.06421274844659354 | validation: 0.06556322863843618]
	TIME [epoch: 8.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06305605809764267		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.06305605809764267 | validation: 0.06297364368009398]
	TIME [epoch: 8.31 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06333771821904019		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.06333771821904019 | validation: 0.07483109270996303]
	TIME [epoch: 8.34 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06131971255716874		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.06131971255716874 | validation: 0.06698078349814941]
	TIME [epoch: 8.3 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06274072009186414		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.06274072009186414 | validation: 0.06777696560240266]
	TIME [epoch: 8.3 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06088586511147466		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.06088586511147466 | validation: 0.06630102354841981]
	TIME [epoch: 8.29 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06106270224884937		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.06106270224884937 | validation: 0.0676644070486086]
	TIME [epoch: 8.29 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06047113578658448		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.06047113578658448 | validation: 0.07143234236344265]
	TIME [epoch: 8.32 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06035845660067173		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.06035845660067173 | validation: 0.06680498916609345]
	TIME [epoch: 8.31 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05966853339221208		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.05966853339221208 | validation: 0.06856883085900872]
	TIME [epoch: 8.29 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06128228088091983		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.06128228088091983 | validation: 0.06677429314871565]
	TIME [epoch: 8.3 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061835861763749375		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.061835861763749375 | validation: 0.06919485004443673]
	TIME [epoch: 8.3 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061231173204232624		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.061231173204232624 | validation: 0.06534726321167264]
	TIME [epoch: 8.3 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060794115125622716		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.060794115125622716 | validation: 0.06443395884173596]
	TIME [epoch: 8.34 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06332378198372843		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 0.06332378198372843 | validation: 0.06489181998048739]
	TIME [epoch: 8.3 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06147670178236879		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.06147670178236879 | validation: 0.06833609140190441]
	TIME [epoch: 8.29 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06156320387103063		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.06156320387103063 | validation: 0.06489042379462946]
	TIME [epoch: 8.3 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06219971290113177		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.06219971290113177 | validation: 0.06407555731798265]
	TIME [epoch: 8.3 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06053095936500755		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.06053095936500755 | validation: 0.06575694114690987]
	TIME [epoch: 8.31 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060168624362793244		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.060168624362793244 | validation: 0.06825994242937684]
	TIME [epoch: 8.33 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06344142426731009		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.06344142426731009 | validation: 0.06763682584554583]
	TIME [epoch: 8.3 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06394788061506758		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.06394788061506758 | validation: 0.06631718287795349]
	TIME [epoch: 8.29 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06194487490098628		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.06194487490098628 | validation: 0.06503432847506002]
	TIME [epoch: 8.29 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062149055310245664		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.062149055310245664 | validation: 0.06236851737272016]
	TIME [epoch: 8.29 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06137026386592088		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.06137026386592088 | validation: 0.0668331882334707]
	TIME [epoch: 8.32 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06004216077630574		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.06004216077630574 | validation: 0.0667913109032909]
	TIME [epoch: 8.31 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06223630263784684		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.06223630263784684 | validation: 0.07303081861962799]
	TIME [epoch: 8.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061039116950358484		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.061039116950358484 | validation: 0.06256523430753379]
	TIME [epoch: 8.3 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058600118876215676		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.058600118876215676 | validation: 0.0622138337070027]
	TIME [epoch: 8.3 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06029080578394635		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.06029080578394635 | validation: 0.0627741279459692]
	TIME [epoch: 8.31 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057148047432566185		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.057148047432566185 | validation: 0.06608944793757504]
	TIME [epoch: 8.34 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05839464526801076		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.05839464526801076 | validation: 0.07310012020026296]
	TIME [epoch: 8.31 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06188420204965073		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.06188420204965073 | validation: 0.06317021758549089]
	TIME [epoch: 8.3 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0607976154178016		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.0607976154178016 | validation: 0.06613995557791236]
	TIME [epoch: 8.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06092426084378472		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.06092426084378472 | validation: 0.06877576171623831]
	TIME [epoch: 8.3 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06157580557035551		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.06157580557035551 | validation: 0.06781898555397017]
	TIME [epoch: 8.31 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06106913434804205		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.06106913434804205 | validation: 0.06950866164934583]
	TIME [epoch: 8.34 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06034553602033628		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.06034553602033628 | validation: 0.06573103776622388]
	TIME [epoch: 8.3 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06170976283675528		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.06170976283675528 | validation: 0.06846972785194287]
	TIME [epoch: 8.3 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06152275328374736		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.06152275328374736 | validation: 0.07307977149825788]
	TIME [epoch: 8.3 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06595877828961692		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.06595877828961692 | validation: 0.071604323260962]
	TIME [epoch: 8.3 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06490665456466742		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.06490665456466742 | validation: 0.06923857523623819]
	TIME [epoch: 8.33 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06249575606457117		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.06249575606457117 | validation: 0.06816217958512806]
	TIME [epoch: 8.32 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0607503806362588		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.0607503806362588 | validation: 0.07004268212031589]
	TIME [epoch: 8.3 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061779860273387374		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.061779860273387374 | validation: 0.07101263724563558]
	TIME [epoch: 8.29 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06146867771180175		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.06146867771180175 | validation: 0.06659190770479786]
	TIME [epoch: 8.3 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06311885675750689		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.06311885675750689 | validation: 0.06761712755849007]
	TIME [epoch: 8.31 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05933858749584879		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.05933858749584879 | validation: 0.06740819699696686]
	TIME [epoch: 8.35 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0615938599533085		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.0615938599533085 | validation: 0.07290554557522261]
	TIME [epoch: 8.3 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06373751798863503		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.06373751798863503 | validation: 0.06885777202562704]
	TIME [epoch: 8.29 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0629580221305034		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.0629580221305034 | validation: 0.0638643511038789]
	TIME [epoch: 8.29 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06255444201522839		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.06255444201522839 | validation: 0.07111435310055114]
	TIME [epoch: 8.29 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06231145157406772		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.06231145157406772 | validation: 0.06884375236022822]
	TIME [epoch: 8.31 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06089472027862611		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.06089472027862611 | validation: 0.07112718312394944]
	TIME [epoch: 8.32 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06524520815098549		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.06524520815098549 | validation: 0.07282165600990032]
	TIME [epoch: 8.29 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06388828642034292		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.06388828642034292 | validation: 0.07436348941897691]
	TIME [epoch: 8.29 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06336088299359328		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.06336088299359328 | validation: 0.06758137232825423]
	TIME [epoch: 8.29 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06401975947031259		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.06401975947031259 | validation: 0.07255443294649666]
	TIME [epoch: 8.29 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06137411286519574		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.06137411286519574 | validation: 0.07052978927605726]
	TIME [epoch: 8.34 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06241733177040619		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.06241733177040619 | validation: 0.06884929061663886]
	TIME [epoch: 8.31 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062041811468197		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.062041811468197 | validation: 0.07174143191122456]
	TIME [epoch: 8.3 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06224655614757305		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.06224655614757305 | validation: 0.06583366875620025]
	TIME [epoch: 8.3 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06189916413549723		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.06189916413549723 | validation: 0.06989090717468135]
	TIME [epoch: 8.29 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061613166841045106		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.061613166841045106 | validation: 0.06529865546181829]
	TIME [epoch: 8.3 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061037062010484894		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.061037062010484894 | validation: 0.0694109815468489]
	TIME [epoch: 8.34 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060638348479162635		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.060638348479162635 | validation: 0.06702624964275998]
	TIME [epoch: 8.29 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061219554757110224		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.061219554757110224 | validation: 0.06416780472942459]
	TIME [epoch: 8.29 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05997674509597403		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.05997674509597403 | validation: 0.05781528514312673]
	TIME [epoch: 8.29 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_20240520_124708/states/model_phi1_1a_v_mmd1_fix_noise_1222.pth
	Model improved!!!
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05904106594323137		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.05904106594323137 | validation: 0.06332227221914905]
	TIME [epoch: 8.3 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059262349123375066		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.059262349123375066 | validation: 0.06502949380306854]
	TIME [epoch: 8.31 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06115819015742219		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.06115819015742219 | validation: 0.06499128714466447]
	TIME [epoch: 8.32 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060090392788740274		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.060090392788740274 | validation: 0.06855168238560151]
	TIME [epoch: 8.29 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06177216358596617		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.06177216358596617 | validation: 0.0668574964026941]
	TIME [epoch: 8.29 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06192420992057075		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.06192420992057075 | validation: 0.0686548607257458]
	TIME [epoch: 8.29 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06251546798004759		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.06251546798004759 | validation: 0.06561157908151297]
	TIME [epoch: 8.29 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06387300451575678		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.06387300451575678 | validation: 0.0738528549984028]
	TIME [epoch: 8.33 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06392733734631599		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.06392733734631599 | validation: 0.08050063667189282]
	TIME [epoch: 8.29 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06270940085912276		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.06270940085912276 | validation: 0.0773121774517983]
	TIME [epoch: 8.29 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06418451322116957		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.06418451322116957 | validation: 0.0806919155667203]
	TIME [epoch: 8.29 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06777284561452421		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.06777284561452421 | validation: 0.07176914471022898]
	TIME [epoch: 8.29 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06747488367515472		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.06747488367515472 | validation: 0.06953124277206503]
	TIME [epoch: 8.3 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06548397036742683		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.06548397036742683 | validation: 0.07374911360652542]
	TIME [epoch: 8.33 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06431419841030808		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.06431419841030808 | validation: 0.07551385924940908]
	TIME [epoch: 8.29 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06280510356623706		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.06280510356623706 | validation: 0.07378548329381415]
	TIME [epoch: 8.29 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06275384760583033		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.06275384760583033 | validation: 0.06853428621622368]
	TIME [epoch: 8.29 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06399770820708811		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.06399770820708811 | validation: 0.07493052201328829]
	TIME [epoch: 8.29 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06377726495730195		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.06377726495730195 | validation: 0.07600665250818511]
	TIME [epoch: 8.31 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0657818717243697		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.0657818717243697 | validation: 0.07188826391663942]
	TIME [epoch: 8.32 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06251433213566569		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.06251433213566569 | validation: 0.07470702356851867]
	TIME [epoch: 8.3 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0639891985150146		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.0639891985150146 | validation: 0.08280300932147487]
	TIME [epoch: 8.29 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06300117902752661		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.06300117902752661 | validation: 0.07028869216082678]
	TIME [epoch: 8.29 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06136593291816847		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.06136593291816847 | validation: 0.07190665384248536]
	TIME [epoch: 8.29 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06135543812353112		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.06135543812353112 | validation: 0.07016943608624915]
	TIME [epoch: 8.33 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06129812763871776		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.06129812763871776 | validation: 0.07661362019623706]
	TIME [epoch: 8.29 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06122240375999623		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.06122240375999623 | validation: 0.06696454256898277]
	TIME [epoch: 8.29 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060059518771686006		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.060059518771686006 | validation: 0.06556308508100869]
	TIME [epoch: 8.3 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060602149450397644		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.060602149450397644 | validation: 0.06887400502793851]
	TIME [epoch: 8.3 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06121782086898874		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.06121782086898874 | validation: 0.06700370783744469]
	TIME [epoch: 8.3 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0599840573282242		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.0599840573282242 | validation: 0.0641253762921791]
	TIME [epoch: 8.33 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059478613057419236		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.059478613057419236 | validation: 0.06421448668201417]
	TIME [epoch: 8.29 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061345503317719846		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.061345503317719846 | validation: 0.06843694014818305]
	TIME [epoch: 8.29 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060796820761822315		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.060796820761822315 | validation: 0.07209636226251273]
	TIME [epoch: 8.29 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05952568519560153		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.05952568519560153 | validation: 0.06864101837822227]
	TIME [epoch: 8.3 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06027656331975051		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.06027656331975051 | validation: 0.06526186047674618]
	TIME [epoch: 8.31 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06001388892056982		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.06001388892056982 | validation: 0.06568252473753611]
	TIME [epoch: 8.32 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060124478048685225		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.060124478048685225 | validation: 0.06804126514354181]
	TIME [epoch: 8.29 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059701324972474615		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.059701324972474615 | validation: 0.06768484301607897]
	TIME [epoch: 8.29 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060562274043675535		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.060562274043675535 | validation: 0.06319689886633514]
	TIME [epoch: 8.29 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05886316605093589		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.05886316605093589 | validation: 0.06329502366286457]
	TIME [epoch: 8.29 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05955453492549413		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.05955453492549413 | validation: 0.06715628913674875]
	TIME [epoch: 8.33 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05900729154515214		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.05900729154515214 | validation: 0.06995690746367955]
	TIME [epoch: 8.3 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05903265569967844		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.05903265569967844 | validation: 0.06328048940993214]
	TIME [epoch: 8.29 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060738011968134456		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.060738011968134456 | validation: 0.06474814200393744]
	TIME [epoch: 8.28 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06028252478244618		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.06028252478244618 | validation: 0.06560184058862348]
	TIME [epoch: 8.29 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0583223133219225		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.0583223133219225 | validation: 0.06444271475752399]
	TIME [epoch: 8.3 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05979019324282631		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.05979019324282631 | validation: 0.06550430634875644]
	TIME [epoch: 8.33 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058232095888242165		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.058232095888242165 | validation: 0.0636897493962806]
	TIME [epoch: 8.3 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05913151709566275		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.05913151709566275 | validation: 0.06607132241134547]
	TIME [epoch: 8.29 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05978560273702534		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.05978560273702534 | validation: 0.06150310836888838]
	TIME [epoch: 8.29 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058752892427456856		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.058752892427456856 | validation: 0.06284185994578818]
	TIME [epoch: 8.29 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05910850215320237		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.05910850215320237 | validation: 0.06305300908185518]
	TIME [epoch: 8.3 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06055964833872908		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.06055964833872908 | validation: 0.06324218967548421]
	TIME [epoch: 8.32 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05868130535613657		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.05868130535613657 | validation: 0.06132179652680014]
	TIME [epoch: 8.28 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05896335116598718		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.05896335116598718 | validation: 0.06137007262567423]
	TIME [epoch: 8.29 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059275723178994294		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.059275723178994294 | validation: 0.06473279416084017]
	TIME [epoch: 8.29 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06048853206331703		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.06048853206331703 | validation: 0.06358117290948205]
	TIME [epoch: 8.29 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059907030497388755		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.059907030497388755 | validation: 0.06561320128293782]
	TIME [epoch: 8.33 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05878380970988052		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.05878380970988052 | validation: 0.06310876841075436]
	TIME [epoch: 8.29 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05867261790078817		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.05867261790078817 | validation: 0.06066682863480243]
	TIME [epoch: 8.29 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05818844679010589		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.05818844679010589 | validation: 0.06471327988108608]
	TIME [epoch: 8.29 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05776196875805445		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.05776196875805445 | validation: 0.06789321794227705]
	TIME [epoch: 8.29 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05815179087603193		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.05815179087603193 | validation: 0.06938540922209194]
	TIME [epoch: 8.3 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061127830211290396		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.061127830211290396 | validation: 0.07814202393590211]
	TIME [epoch: 8.33 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0646231081459904		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.0646231081459904 | validation: 0.07104563282176982]
	TIME [epoch: 8.29 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06436351984709476		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.06436351984709476 | validation: 0.06951059996401611]
	TIME [epoch: 8.29 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06317952423006766		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.06317952423006766 | validation: 0.07313721009544427]
	TIME [epoch: 8.29 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06416203838212724		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.06416203838212724 | validation: 0.07132303001270793]
	TIME [epoch: 8.29 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06186491371706799		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.06186491371706799 | validation: 0.07194760292236918]
	TIME [epoch: 8.32 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06370745263556435		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.06370745263556435 | validation: 0.06991171049390074]
	TIME [epoch: 8.31 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06260617216492775		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.06260617216492775 | validation: 0.07158439726368351]
	TIME [epoch: 8.29 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0640696304473033		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.0640696304473033 | validation: 0.0732456238978439]
	TIME [epoch: 8.29 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06163348893302556		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.06163348893302556 | validation: 0.0733265262713213]
	TIME [epoch: 8.29 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06163008290124042		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.06163008290124042 | validation: 0.06974927572232012]
	TIME [epoch: 8.29 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0601073250647847		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.0601073250647847 | validation: 0.07362513078748452]
	TIME [epoch: 8.34 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060676593201343255		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.060676593201343255 | validation: 0.06864275736777446]
	TIME [epoch: 8.3 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06108072557485543		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.06108072557485543 | validation: 0.06699559555901351]
	TIME [epoch: 8.3 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06149832629703501		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.06149832629703501 | validation: 0.0733289462561483]
	TIME [epoch: 8.29 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06024296245287297		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.06024296245287297 | validation: 0.06525787225855086]
	TIME [epoch: 8.3 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061114351701144994		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.061114351701144994 | validation: 0.06836049399115914]
	TIME [epoch: 8.31 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05922662607493426		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.05922662607493426 | validation: 0.06778020730223294]
	TIME [epoch: 8.33 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059227288015797935		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.059227288015797935 | validation: 0.06420666566135477]
	TIME [epoch: 8.3 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05756859602497296		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.05756859602497296 | validation: 0.06541607791388619]
	TIME [epoch: 8.29 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05929979218244063		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.05929979218244063 | validation: 0.06515667266203581]
	TIME [epoch: 8.29 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05967224548099733		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.05967224548099733 | validation: 0.0681067676666146]
	TIME [epoch: 8.29 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060348195209624304		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.060348195209624304 | validation: 0.06481955659011446]
	TIME [epoch: 8.32 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061891081400165325		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 0.061891081400165325 | validation: 0.06493847550837907]
	TIME [epoch: 8.31 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06109043731883904		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 0.06109043731883904 | validation: 0.06716628075192918]
	TIME [epoch: 8.29 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060288813559378526		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 0.060288813559378526 | validation: 0.06593512042275775]
	TIME [epoch: 8.29 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059856276791788586		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 0.059856276791788586 | validation: 0.0658902122085653]
	TIME [epoch: 8.29 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06018028191991335		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 0.06018028191991335 | validation: 0.06828619576954556]
	TIME [epoch: 8.3 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05994667461982423		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 0.05994667461982423 | validation: 0.06745768986980859]
	TIME [epoch: 8.34 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06219873911551523		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 0.06219873911551523 | validation: 0.07192467695921845]
	TIME [epoch: 8.3 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06105890629413504		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 0.06105890629413504 | validation: 0.06441263943030605]
	TIME [epoch: 8.29 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06215495256743482		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 0.06215495256743482 | validation: 0.06805168163558235]
	TIME [epoch: 8.29 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06105417246694077		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 0.06105417246694077 | validation: 0.06487644843514274]
	TIME [epoch: 8.29 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059556142847827016		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 0.059556142847827016 | validation: 0.06878683148715725]
	TIME [epoch: 8.31 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06030766295599406		[learning rate: 0.00011092]
	Learning Rate: 0.000110918
	LOSS [training: 0.06030766295599406 | validation: 0.07267081604014061]
	TIME [epoch: 8.32 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06029946437350103		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 0.06029946437350103 | validation: 0.07018391129009363]
	TIME [epoch: 8.29 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0602756071790587		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 0.0602756071790587 | validation: 0.06483637288473096]
	TIME [epoch: 8.29 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06095503581823873		[learning rate: 0.00010975]
	Learning Rate: 0.000109745
	LOSS [training: 0.06095503581823873 | validation: 0.06388452457081499]
	TIME [epoch: 8.29 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05855202330787303		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 0.05855202330787303 | validation: 0.06693195020706319]
	TIME [epoch: 8.29 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05927367102244381		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 0.05927367102244381 | validation: 0.06457911469514749]
	TIME [epoch: 8.32 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059205246520869254		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 0.059205246520869254 | validation: 0.06496796175249742]
	TIME [epoch: 8.3 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05887771692635563		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 0.05887771692635563 | validation: 0.06710326354222078]
	TIME [epoch: 8.29 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06069994929641781		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 0.06069994929641781 | validation: 0.06158781682804364]
	TIME [epoch: 8.29 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059321281696552666		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 0.059321281696552666 | validation: 0.07209417154072759]
	TIME [epoch: 8.29 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05999593232821038		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 0.05999593232821038 | validation: 0.0636421090358198]
	TIME [epoch: 8.29 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06063437954264288		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 0.06063437954264288 | validation: 0.06950131109998404]
	TIME [epoch: 8.33 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0611113906790604		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 0.0611113906790604 | validation: 0.07194871626268626]
	TIME [epoch: 8.29 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06232696619026258		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 0.06232696619026258 | validation: 0.07293470218337506]
	TIME [epoch: 8.29 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059506390037434714		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 0.059506390037434714 | validation: 0.07281480651434276]
	TIME [epoch: 8.29 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06000358175075514		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 0.06000358175075514 | validation: 0.06382166208969386]
	TIME [epoch: 8.29 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06172394443057683		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 0.06172394443057683 | validation: 0.06630351762865386]
	TIME [epoch: 8.31 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06214253681251991		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 0.06214253681251991 | validation: 0.06819262675931598]
	TIME [epoch: 8.32 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06362419287145797		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 0.06362419287145797 | validation: 0.07476941205387744]
	TIME [epoch: 8.29 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06276104399954105		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 0.06276104399954105 | validation: 0.06681829959614574]
	TIME [epoch: 8.29 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0623239961324378		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 0.0623239961324378 | validation: 0.07068201343450771]
	TIME [epoch: 8.29 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06172264404501775		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 0.06172264404501775 | validation: 0.07379797786469255]
	TIME [epoch: 8.29 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06023767438555498		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 0.06023767438555498 | validation: 0.06810768728442221]
	TIME [epoch: 8.33 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05991621322084615		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 0.05991621322084615 | validation: 0.0683559497663751]
	TIME [epoch: 8.31 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06056429142359703		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 0.06056429142359703 | validation: 0.06988545262512239]
	TIME [epoch: 8.3 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060075319059203006		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 0.060075319059203006 | validation: 0.06361062689494161]
	TIME [epoch: 8.29 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059738359711750635		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.059738359711750635 | validation: 0.06744225972835427]
	TIME [epoch: 8.29 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06125567533067939		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 0.06125567533067939 | validation: 0.06818631066708584]
	TIME [epoch: 8.29 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06001300853023954		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 0.06001300853023954 | validation: 0.06665951599963899]
	TIME [epoch: 8.33 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06036709291447341		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 0.06036709291447341 | validation: 0.06940469251134714]
	TIME [epoch: 8.3 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059443556660440613		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 0.059443556660440613 | validation: 0.06307556737138846]
	TIME [epoch: 8.29 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06060900809172658		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 0.06060900809172658 | validation: 0.06653864211266616]
	TIME [epoch: 8.29 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06071589276756513		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 0.06071589276756513 | validation: 0.06497563836767292]
	TIME [epoch: 8.29 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058447303793315225		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 0.058447303793315225 | validation: 0.06218527211993671]
	TIME [epoch: 8.31 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05889732114934117		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 0.05889732114934117 | validation: 0.06428425789380378]
	TIME [epoch: 8.31 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059406455923314345		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 0.059406455923314345 | validation: 0.06525878576904556]
	TIME [epoch: 8.3 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0599321028593639		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 0.0599321028593639 | validation: 0.06756475874680565]
	TIME [epoch: 8.3 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06017005345523612		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 0.06017005345523612 | validation: 0.06344838528696739]
	TIME [epoch: 8.29 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059109159276905576		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 0.059109159276905576 | validation: 0.06737499029034404]
	TIME [epoch: 8.29 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059915505200289855		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 0.059915505200289855 | validation: 0.06328032663097474]
	TIME [epoch: 8.33 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059035849092058154		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 0.059035849092058154 | validation: 0.06576043380138996]
	TIME [epoch: 8.3 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06073184092363216		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 0.06073184092363216 | validation: 0.06671938526681132]
	TIME [epoch: 8.29 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05839179475691947		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 0.05839179475691947 | validation: 0.06565791023267341]
	TIME [epoch: 8.29 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06028587876584714		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 0.06028587876584714 | validation: 0.0675773455997641]
	TIME [epoch: 8.29 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05835635887724309		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 0.05835635887724309 | validation: 0.06709210261667999]
	TIME [epoch: 8.29 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06150233213345898		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 0.06150233213345898 | validation: 0.06694763932928038]
	TIME [epoch: 8.33 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06071897297997364		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 0.06071897297997364 | validation: 0.07080153086344057]
	TIME [epoch: 8.3 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06044072153775372		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 0.06044072153775372 | validation: 0.06913847673661178]
	TIME [epoch: 8.29 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06152691392545509		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 0.06152691392545509 | validation: 0.07187742984615217]
	TIME [epoch: 8.29 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05995676833905274		[learning rate: 9.3243e-05]
	Learning Rate: 9.32428e-05
	LOSS [training: 0.05995676833905274 | validation: 0.06771981865215948]
	TIME [epoch: 8.29 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060783507607799006		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 0.060783507607799006 | validation: 0.06858507622145632]
	TIME [epoch: 8.31 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06086101281686265		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 0.06086101281686265 | validation: 0.06764581120175076]
	TIME [epoch: 8.32 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061439519226847074		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 0.061439519226847074 | validation: 0.06847407817847781]
	TIME [epoch: 8.29 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05928278721226242		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 0.05928278721226242 | validation: 0.06483731017984605]
	TIME [epoch: 8.3 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061222728989711095		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 0.061222728989711095 | validation: 0.06622926223939557]
	TIME [epoch: 8.29 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06021702458001464		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 0.06021702458001464 | validation: 0.07194757457699295]
	TIME [epoch: 8.29 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06009160116449484		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 0.06009160116449484 | validation: 0.06608882192061256]
	TIME [epoch: 8.33 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061392783390653016		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 0.061392783390653016 | validation: 0.06897843366939038]
	TIME [epoch: 8.3 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06135983788606838		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 0.06135983788606838 | validation: 0.08097463714437192]
	TIME [epoch: 8.29 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061657223300027986		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 0.061657223300027986 | validation: 0.07999493116652753]
	TIME [epoch: 8.29 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06109194292243409		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 0.06109194292243409 | validation: 0.07916485549752728]
	TIME [epoch: 8.29 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061759990421824534		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 0.061759990421824534 | validation: 0.07825380299000198]
	TIME [epoch: 8.29 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06074147846860145		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 0.06074147846860145 | validation: 0.0732799986204429]
	TIME [epoch: 8.34 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06238412960971365		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 0.06238412960971365 | validation: 0.07840558044500798]
	TIME [epoch: 8.29 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060705560345227144		[learning rate: 8.8418e-05]
	Learning Rate: 8.84175e-05
	LOSS [training: 0.060705560345227144 | validation: 0.07573621980694803]
	TIME [epoch: 8.29 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06062576003282146		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.06062576003282146 | validation: 0.0738919913072762]
	TIME [epoch: 8.3 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0594593919614957		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 0.0594593919614957 | validation: 0.07276856729836143]
	TIME [epoch: 8.3 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059575056310732885		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 0.059575056310732885 | validation: 0.08947248797554246]
	TIME [epoch: 8.31 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06067569768163392		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 0.06067569768163392 | validation: 0.08202234832232859]
	TIME [epoch: 8.32 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061401330935165206		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 0.061401330935165206 | validation: 0.07710545023700693]
	TIME [epoch: 8.29 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059519050989139816		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 0.059519050989139816 | validation: 0.08061334905624955]
	TIME [epoch: 8.29 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06074568485922304		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 0.06074568485922304 | validation: 0.07982126926237872]
	TIME [epoch: 8.29 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06218838941363118		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 0.06218838941363118 | validation: 0.07908600423296344]
	TIME [epoch: 8.29 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06105156384645828		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 0.06105156384645828 | validation: 0.08398949755274776]
	TIME [epoch: 8.33 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06040137393293359		[learning rate: 8.534e-05]
	Learning Rate: 8.53402e-05
	LOSS [training: 0.06040137393293359 | validation: 0.07934263774525221]
	TIME [epoch: 8.3 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061088017780215983		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 0.061088017780215983 | validation: 0.07192951248910752]
	TIME [epoch: 8.29 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06082515959460897		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 0.06082515959460897 | validation: 0.07074226496000352]
	TIME [epoch: 8.28 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05997348984740144		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 0.05997348984740144 | validation: 0.07358849789790758]
	TIME [epoch: 8.29 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05967958364606506		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 0.05967958364606506 | validation: 0.07394504662561757]
	TIME [epoch: 8.29 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05812495764896039		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 0.05812495764896039 | validation: 0.06919252097104946]
	TIME [epoch: 8.33 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057026148142756146		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 0.057026148142756146 | validation: 0.06788020353746449]
	TIME [epoch: 8.3 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059903584105848194		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 0.059903584105848194 | validation: 0.07130402992284424]
	TIME [epoch: 8.29 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05869099701634288		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 0.05869099701634288 | validation: 0.06754865885033247]
	TIME [epoch: 8.29 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05854777901368591		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 0.05854777901368591 | validation: 0.06763752934842415]
	TIME [epoch: 8.29 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059365009649724465		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 0.059365009649724465 | validation: 0.06394760240661558]
	TIME [epoch: 8.3 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058299809217012256		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 0.058299809217012256 | validation: 0.06458279346241136]
	TIME [epoch: 8.32 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05776150812114078		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 0.05776150812114078 | validation: 0.06172699543888689]
	TIME [epoch: 8.28 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05826894643328867		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 0.05826894643328867 | validation: 0.06394060922046332]
	TIME [epoch: 8.29 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057850666425212166		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 0.057850666425212166 | validation: 0.062075128201875794]
	TIME [epoch: 8.29 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056662726526656396		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 0.056662726526656396 | validation: 0.060876983961559224]
	TIME [epoch: 8.29 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05813407911501918		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 0.05813407911501918 | validation: 0.06142254672941191]
	TIME [epoch: 8.33 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05786708677922563		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 0.05786708677922563 | validation: 0.06131606239347637]
	TIME [epoch: 8.29 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05779952704597463		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 0.05779952704597463 | validation: 0.06308722243395096]
	TIME [epoch: 8.28 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057878102631140155		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 0.057878102631140155 | validation: 0.0624464366568479]
	TIME [epoch: 8.29 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05903441557753198		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 0.05903441557753198 | validation: 0.06659437196290119]
	TIME [epoch: 8.3 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060441061332701954		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 0.060441061332701954 | validation: 0.06140306417875563]
	TIME [epoch: 8.3 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05909807498993759		[learning rate: 7.8942e-05]
	Learning Rate: 7.8942e-05
	LOSS [training: 0.05909807498993759 | validation: 0.06450893888559181]
	TIME [epoch: 8.34 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05961388771461368		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 0.05961388771461368 | validation: 0.0647495895787834]
	TIME [epoch: 8.29 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05829824327119433		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 0.05829824327119433 | validation: 0.06560772973123614]
	TIME [epoch: 8.29 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05822799204522515		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 0.05822799204522515 | validation: 0.0630012869490254]
	TIME [epoch: 8.29 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058262246386003025		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 0.058262246386003025 | validation: 0.06575018563877866]
	TIME [epoch: 8.29 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059570816509608576		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 0.059570816509608576 | validation: 0.06792961471791462]
	TIME [epoch: 8.31 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059338296940057586		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 0.059338296940057586 | validation: 0.06341039777548943]
	TIME [epoch: 8.32 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06112672211924929		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 0.06112672211924929 | validation: 0.0706052692717606]
	TIME [epoch: 8.29 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06022593473621253		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.06022593473621253 | validation: 0.063709428646475]
	TIME [epoch: 8.29 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058705168915091074		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 0.058705168915091074 | validation: 0.06368040642362141]
	TIME [epoch: 8.29 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059938844967937596		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 0.059938844967937596 | validation: 0.06848080901037108]
	TIME [epoch: 8.29 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05921725436977725		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 0.05921725436977725 | validation: 0.06608289138544733]
	TIME [epoch: 8.33 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058589741069499796		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 0.058589741069499796 | validation: 0.0680749515572712]
	TIME [epoch: 8.3 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059972348847067175		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 0.059972348847067175 | validation: 0.07289975797054568]
	TIME [epoch: 8.29 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059590120465978266		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 0.059590120465978266 | validation: 0.06511414881562745]
	TIME [epoch: 8.29 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060761098854132264		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 0.060761098854132264 | validation: 0.06947695280531323]
	TIME [epoch: 8.29 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05943189750156787		[learning rate: 7.4592e-05]
	Learning Rate: 7.4592e-05
	LOSS [training: 0.05943189750156787 | validation: 0.07238464399045341]
	TIME [epoch: 8.29 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06032222841663171		[learning rate: 7.4328e-05]
	Learning Rate: 7.43283e-05
	LOSS [training: 0.06032222841663171 | validation: 0.06832067008253383]
	TIME [epoch: 8.33 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06087535195320999		[learning rate: 7.4065e-05]
	Learning Rate: 7.40654e-05
	LOSS [training: 0.06087535195320999 | validation: 0.07351677611377104]
	TIME [epoch: 8.29 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06015423000251085		[learning rate: 7.3803e-05]
	Learning Rate: 7.38035e-05
	LOSS [training: 0.06015423000251085 | validation: 0.07169329389514617]
	TIME [epoch: 8.29 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05999161451849008		[learning rate: 7.3543e-05]
	Learning Rate: 7.35425e-05
	LOSS [training: 0.05999161451849008 | validation: 0.06349863854430049]
	TIME [epoch: 8.28 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059737715165024026		[learning rate: 7.3282e-05]
	Learning Rate: 7.32825e-05
	LOSS [training: 0.059737715165024026 | validation: 0.06590181791469504]
	TIME [epoch: 8.29 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059432758726601156		[learning rate: 7.3023e-05]
	Learning Rate: 7.30233e-05
	LOSS [training: 0.059432758726601156 | validation: 0.06612245082150846]
	TIME [epoch: 8.31 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05897893680200426		[learning rate: 7.2765e-05]
	Learning Rate: 7.27651e-05
	LOSS [training: 0.05897893680200426 | validation: 0.06684750109643631]
	TIME [epoch: 8.32 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05960940058465412		[learning rate: 7.2508e-05]
	Learning Rate: 7.25078e-05
	LOSS [training: 0.05960940058465412 | validation: 0.06907714678482868]
	TIME [epoch: 8.29 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05884958960781972		[learning rate: 7.2251e-05]
	Learning Rate: 7.22514e-05
	LOSS [training: 0.05884958960781972 | validation: 0.06709029710039655]
	TIME [epoch: 8.29 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05852622455267273		[learning rate: 7.1996e-05]
	Learning Rate: 7.19959e-05
	LOSS [training: 0.05852622455267273 | validation: 0.0654251562940546]
	TIME [epoch: 8.29 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05867024038247118		[learning rate: 7.1741e-05]
	Learning Rate: 7.17413e-05
	LOSS [training: 0.05867024038247118 | validation: 0.06598992170118215]
	TIME [epoch: 8.29 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05953354222325969		[learning rate: 7.1488e-05]
	Learning Rate: 7.14876e-05
	LOSS [training: 0.05953354222325969 | validation: 0.06528060830613902]
	TIME [epoch: 8.33 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05948309407132425		[learning rate: 7.1235e-05]
	Learning Rate: 7.12348e-05
	LOSS [training: 0.05948309407132425 | validation: 0.06854848999942488]
	TIME [epoch: 8.3 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06186198420883246		[learning rate: 7.0983e-05]
	Learning Rate: 7.09829e-05
	LOSS [training: 0.06186198420883246 | validation: 0.06722062197215713]
	TIME [epoch: 8.29 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06122083989968841		[learning rate: 7.0732e-05]
	Learning Rate: 7.07319e-05
	LOSS [training: 0.06122083989968841 | validation: 0.07086039066050168]
	TIME [epoch: 8.29 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06096047855275767		[learning rate: 7.0482e-05]
	Learning Rate: 7.04818e-05
	LOSS [training: 0.06096047855275767 | validation: 0.06839438870293067]
	TIME [epoch: 8.29 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061651355544546975		[learning rate: 7.0233e-05]
	Learning Rate: 7.02326e-05
	LOSS [training: 0.061651355544546975 | validation: 0.06455080874725803]
	TIME [epoch: 8.3 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06066526372662611		[learning rate: 6.9984e-05]
	Learning Rate: 6.99842e-05
	LOSS [training: 0.06066526372662611 | validation: 0.07223257164743419]
	TIME [epoch: 8.33 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06069394694139409		[learning rate: 6.9737e-05]
	Learning Rate: 6.97367e-05
	LOSS [training: 0.06069394694139409 | validation: 0.06536405197414027]
	TIME [epoch: 8.29 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060139745999478945		[learning rate: 6.949e-05]
	Learning Rate: 6.94901e-05
	LOSS [training: 0.060139745999478945 | validation: 0.06798270858280794]
	TIME [epoch: 8.29 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06187313937871025		[learning rate: 6.9244e-05]
	Learning Rate: 6.92444e-05
	LOSS [training: 0.06187313937871025 | validation: 0.07100788486897366]
	TIME [epoch: 8.29 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06154145379997838		[learning rate: 6.9e-05]
	Learning Rate: 6.89995e-05
	LOSS [training: 0.06154145379997838 | validation: 0.0693255854427787]
	TIME [epoch: 8.29 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06221021196292903		[learning rate: 6.8756e-05]
	Learning Rate: 6.87555e-05
	LOSS [training: 0.06221021196292903 | validation: 0.07101256732328476]
	TIME [epoch: 8.3 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059864271073532226		[learning rate: 6.8512e-05]
	Learning Rate: 6.85124e-05
	LOSS [training: 0.059864271073532226 | validation: 0.06576794643897813]
	TIME [epoch: 8.32 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06210846752176236		[learning rate: 6.827e-05]
	Learning Rate: 6.82702e-05
	LOSS [training: 0.06210846752176236 | validation: 0.06320151319682941]
	TIME [epoch: 8.29 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059593274429642365		[learning rate: 6.8029e-05]
	Learning Rate: 6.80287e-05
	LOSS [training: 0.059593274429642365 | validation: 0.06786543578218504]
	TIME [epoch: 8.29 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060710808181658346		[learning rate: 6.7788e-05]
	Learning Rate: 6.77882e-05
	LOSS [training: 0.060710808181658346 | validation: 0.06668154479092578]
	TIME [epoch: 8.29 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05995974927228337		[learning rate: 6.7548e-05]
	Learning Rate: 6.75485e-05
	LOSS [training: 0.05995974927228337 | validation: 0.06726138955517261]
	TIME [epoch: 8.29 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05986101980149508		[learning rate: 6.731e-05]
	Learning Rate: 6.73096e-05
	LOSS [training: 0.05986101980149508 | validation: 0.0667954555271095]
	TIME [epoch: 8.33 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059626729020514094		[learning rate: 6.7072e-05]
	Learning Rate: 6.70716e-05
	LOSS [training: 0.059626729020514094 | validation: 0.06519971959494714]
	TIME [epoch: 8.29 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05959205773805626		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 0.05959205773805626 | validation: 0.06757864568377532]
	TIME [epoch: 8.29 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0604194262018887		[learning rate: 6.6598e-05]
	Learning Rate: 6.65981e-05
	LOSS [training: 0.0604194262018887 | validation: 0.06747354432840559]
	TIME [epoch: 8.29 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06160781294707604		[learning rate: 6.6363e-05]
	Learning Rate: 6.63626e-05
	LOSS [training: 0.06160781294707604 | validation: 0.07048444642537041]
	TIME [epoch: 8.29 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0621174892927156		[learning rate: 6.6128e-05]
	Learning Rate: 6.61279e-05
	LOSS [training: 0.0621174892927156 | validation: 0.06951703014405011]
	TIME [epoch: 8.3 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06110085329777027		[learning rate: 6.5894e-05]
	Learning Rate: 6.58941e-05
	LOSS [training: 0.06110085329777027 | validation: 0.06814827081216204]
	TIME [epoch: 8.33 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05930720833805703		[learning rate: 6.5661e-05]
	Learning Rate: 6.5661e-05
	LOSS [training: 0.05930720833805703 | validation: 0.06596626052141648]
	TIME [epoch: 8.29 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06109223359667795		[learning rate: 6.5429e-05]
	Learning Rate: 6.54289e-05
	LOSS [training: 0.06109223359667795 | validation: 0.07243303177386054]
	TIME [epoch: 8.29 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05909698698846118		[learning rate: 6.5197e-05]
	Learning Rate: 6.51975e-05
	LOSS [training: 0.05909698698846118 | validation: 0.07391391113536877]
	TIME [epoch: 8.29 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060594863529492546		[learning rate: 6.4967e-05]
	Learning Rate: 6.49669e-05
	LOSS [training: 0.060594863529492546 | validation: 0.07149940811175629]
	TIME [epoch: 8.3 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06084059056332769		[learning rate: 6.4737e-05]
	Learning Rate: 6.47372e-05
	LOSS [training: 0.06084059056332769 | validation: 0.07184053092590312]
	TIME [epoch: 8.31 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06045394228130438		[learning rate: 6.4508e-05]
	Learning Rate: 6.45083e-05
	LOSS [training: 0.06045394228130438 | validation: 0.06962280546483168]
	TIME [epoch: 8.32 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060491098134021766		[learning rate: 6.428e-05]
	Learning Rate: 6.42802e-05
	LOSS [training: 0.060491098134021766 | validation: 0.06645837088109283]
	TIME [epoch: 8.29 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05930685182109552		[learning rate: 6.4053e-05]
	Learning Rate: 6.40529e-05
	LOSS [training: 0.05930685182109552 | validation: 0.0699917922106759]
	TIME [epoch: 8.29 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06099255690153359		[learning rate: 6.3826e-05]
	Learning Rate: 6.38264e-05
	LOSS [training: 0.06099255690153359 | validation: 0.07534492720323671]
	TIME [epoch: 8.29 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06019628899269039		[learning rate: 6.3601e-05]
	Learning Rate: 6.36007e-05
	LOSS [training: 0.06019628899269039 | validation: 0.06843055382708044]
	TIME [epoch: 8.29 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05915529576348824		[learning rate: 6.3376e-05]
	Learning Rate: 6.33758e-05
	LOSS [training: 0.05915529576348824 | validation: 0.068104230096972]
	TIME [epoch: 8.33 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05958463664301329		[learning rate: 6.3152e-05]
	Learning Rate: 6.31517e-05
	LOSS [training: 0.05958463664301329 | validation: 0.06744482805961302]
	TIME [epoch: 8.3 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06186681368163253		[learning rate: 6.2928e-05]
	Learning Rate: 6.29283e-05
	LOSS [training: 0.06186681368163253 | validation: 0.07020283137996376]
	TIME [epoch: 8.29 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058966728594247085		[learning rate: 6.2706e-05]
	Learning Rate: 6.27058e-05
	LOSS [training: 0.058966728594247085 | validation: 0.07137281989234087]
	TIME [epoch: 8.29 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05842865727730137		[learning rate: 6.2484e-05]
	Learning Rate: 6.24841e-05
	LOSS [training: 0.05842865727730137 | validation: 0.06831466552463637]
	TIME [epoch: 8.29 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06128552716155071		[learning rate: 6.2263e-05]
	Learning Rate: 6.22631e-05
	LOSS [training: 0.06128552716155071 | validation: 0.0657721141950387]
	TIME [epoch: 8.3 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05983836727804211		[learning rate: 6.2043e-05]
	Learning Rate: 6.20429e-05
	LOSS [training: 0.05983836727804211 | validation: 0.06880579034601406]
	TIME [epoch: 8.33 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05958654464805309		[learning rate: 6.1824e-05]
	Learning Rate: 6.18235e-05
	LOSS [training: 0.05958654464805309 | validation: 0.06597728735129836]
	TIME [epoch: 8.29 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060142655000194695		[learning rate: 6.1605e-05]
	Learning Rate: 6.16049e-05
	LOSS [training: 0.060142655000194695 | validation: 0.0691141435669027]
	TIME [epoch: 8.3 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0593117741998454		[learning rate: 6.1387e-05]
	Learning Rate: 6.13871e-05
	LOSS [training: 0.0593117741998454 | validation: 0.07090857877503468]
	TIME [epoch: 8.3 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06049178823480685		[learning rate: 6.117e-05]
	Learning Rate: 6.117e-05
	LOSS [training: 0.06049178823480685 | validation: 0.06488628334952203]
	TIME [epoch: 8.3 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05892323855651556		[learning rate: 6.0954e-05]
	Learning Rate: 6.09537e-05
	LOSS [training: 0.05892323855651556 | validation: 0.06753794146331332]
	TIME [epoch: 8.32 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05910440715528492		[learning rate: 6.0738e-05]
	Learning Rate: 6.07382e-05
	LOSS [training: 0.05910440715528492 | validation: 0.06615995051892545]
	TIME [epoch: 8.31 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05873893230911838		[learning rate: 6.0523e-05]
	Learning Rate: 6.05234e-05
	LOSS [training: 0.05873893230911838 | validation: 0.0650224878181263]
	TIME [epoch: 8.28 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05989128224840455		[learning rate: 6.0309e-05]
	Learning Rate: 6.03094e-05
	LOSS [training: 0.05989128224840455 | validation: 0.06603612293120922]
	TIME [epoch: 8.28 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05988714083040706		[learning rate: 6.0096e-05]
	Learning Rate: 6.00961e-05
	LOSS [training: 0.05988714083040706 | validation: 0.06551240864828427]
	TIME [epoch: 8.29 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059337351030188674		[learning rate: 5.9884e-05]
	Learning Rate: 5.98836e-05
	LOSS [training: 0.059337351030188674 | validation: 0.07129047623071247]
	TIME [epoch: 8.29 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05905358433031026		[learning rate: 5.9672e-05]
	Learning Rate: 5.96718e-05
	LOSS [training: 0.05905358433031026 | validation: 0.06310205323711421]
	TIME [epoch: 8.33 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06029901113558676		[learning rate: 5.9461e-05]
	Learning Rate: 5.94608e-05
	LOSS [training: 0.06029901113558676 | validation: 0.07015405692624248]
	TIME [epoch: 8.3 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059176179208610256		[learning rate: 5.9251e-05]
	Learning Rate: 5.92505e-05
	LOSS [training: 0.059176179208610256 | validation: 0.06680524474312216]
	TIME [epoch: 8.29 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0598000756001239		[learning rate: 5.9041e-05]
	Learning Rate: 5.9041e-05
	LOSS [training: 0.0598000756001239 | validation: 0.0651402375570196]
	TIME [epoch: 8.29 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05868377909885275		[learning rate: 5.8832e-05]
	Learning Rate: 5.88323e-05
	LOSS [training: 0.05868377909885275 | validation: 0.06839850584638588]
	TIME [epoch: 8.29 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0617694757727992		[learning rate: 5.8624e-05]
	Learning Rate: 5.86242e-05
	LOSS [training: 0.0617694757727992 | validation: 0.06422772387819797]
	TIME [epoch: 8.3 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06038757865090838		[learning rate: 5.8417e-05]
	Learning Rate: 5.84169e-05
	LOSS [training: 0.06038757865090838 | validation: 0.06957111895029368]
	TIME [epoch: 8.33 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05963611233970889		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 0.05963611233970889 | validation: 0.06228708475737571]
	TIME [epoch: 8.29 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05877223395867309		[learning rate: 5.8004e-05]
	Learning Rate: 5.80045e-05
	LOSS [training: 0.05877223395867309 | validation: 0.06191288089940203]
	TIME [epoch: 8.29 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05820137560789337		[learning rate: 5.7799e-05]
	Learning Rate: 5.77994e-05
	LOSS [training: 0.05820137560789337 | validation: 0.060962682429315906]
	TIME [epoch: 8.29 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06009173295162245		[learning rate: 5.7595e-05]
	Learning Rate: 5.7595e-05
	LOSS [training: 0.06009173295162245 | validation: 0.0645527628022387]
	TIME [epoch: 8.29 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05931293385312225		[learning rate: 5.7391e-05]
	Learning Rate: 5.73913e-05
	LOSS [training: 0.05931293385312225 | validation: 0.06806180943744589]
	TIME [epoch: 8.32 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06225202253168807		[learning rate: 5.7188e-05]
	Learning Rate: 5.71884e-05
	LOSS [training: 0.06225202253168807 | validation: 0.07096757575739499]
	TIME [epoch: 8.3 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059978714804506644		[learning rate: 5.6986e-05]
	Learning Rate: 5.69861e-05
	LOSS [training: 0.059978714804506644 | validation: 0.0647717222465831]
	TIME [epoch: 8.29 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060336879248873085		[learning rate: 5.6785e-05]
	Learning Rate: 5.67846e-05
	LOSS [training: 0.060336879248873085 | validation: 0.06578114781954725]
	TIME [epoch: 8.28 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060254183652708904		[learning rate: 5.6584e-05]
	Learning Rate: 5.65838e-05
	LOSS [training: 0.060254183652708904 | validation: 0.06330180349071396]
	TIME [epoch: 8.29 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060432475221109824		[learning rate: 5.6384e-05]
	Learning Rate: 5.63837e-05
	LOSS [training: 0.060432475221109824 | validation: 0.06699412659667386]
	TIME [epoch: 8.3 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059319269162323586		[learning rate: 5.6184e-05]
	Learning Rate: 5.61844e-05
	LOSS [training: 0.059319269162323586 | validation: 0.0665560734918257]
	TIME [epoch: 8.33 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06006606919923474		[learning rate: 5.5986e-05]
	Learning Rate: 5.59857e-05
	LOSS [training: 0.06006606919923474 | validation: 0.06516806700776122]
	TIME [epoch: 8.29 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05995337479669836		[learning rate: 5.5788e-05]
	Learning Rate: 5.57877e-05
	LOSS [training: 0.05995337479669836 | validation: 0.06874658160775426]
	TIME [epoch: 8.29 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06051324441073459		[learning rate: 5.559e-05]
	Learning Rate: 5.55904e-05
	LOSS [training: 0.06051324441073459 | validation: 0.07031356109745952]
	TIME [epoch: 8.29 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059265722032470276		[learning rate: 5.5394e-05]
	Learning Rate: 5.53939e-05
	LOSS [training: 0.059265722032470276 | validation: 0.0680557902145448]
	TIME [epoch: 8.29 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06052602855165992		[learning rate: 5.5198e-05]
	Learning Rate: 5.5198e-05
	LOSS [training: 0.06052602855165992 | validation: 0.06653857595153055]
	TIME [epoch: 8.32 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059325918995734694		[learning rate: 5.5003e-05]
	Learning Rate: 5.50028e-05
	LOSS [training: 0.059325918995734694 | validation: 0.06483631826857499]
	TIME [epoch: 8.31 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05825372053616805		[learning rate: 5.4808e-05]
	Learning Rate: 5.48083e-05
	LOSS [training: 0.05825372053616805 | validation: 0.06503903941741804]
	TIME [epoch: 8.28 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0589208071153888		[learning rate: 5.4614e-05]
	Learning Rate: 5.46145e-05
	LOSS [training: 0.0589208071153888 | validation: 0.06523846478920499]
	TIME [epoch: 8.28 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05853723722582284		[learning rate: 5.4421e-05]
	Learning Rate: 5.44213e-05
	LOSS [training: 0.05853723722582284 | validation: 0.07102366875657944]
	TIME [epoch: 8.28 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05848174943653255		[learning rate: 5.4229e-05]
	Learning Rate: 5.42289e-05
	LOSS [training: 0.05848174943653255 | validation: 0.0642524690860139]
	TIME [epoch: 8.28 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059583609866657325		[learning rate: 5.4037e-05]
	Learning Rate: 5.40371e-05
	LOSS [training: 0.059583609866657325 | validation: 0.06474239034287993]
	TIME [epoch: 8.32 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06002782178648909		[learning rate: 5.3846e-05]
	Learning Rate: 5.38461e-05
	LOSS [training: 0.06002782178648909 | validation: 0.06464638832099]
	TIME [epoch: 8.3 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058864321190628704		[learning rate: 5.3656e-05]
	Learning Rate: 5.36556e-05
	LOSS [training: 0.058864321190628704 | validation: 0.06572514261401043]
	TIME [epoch: 8.28 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059348858824910505		[learning rate: 5.3466e-05]
	Learning Rate: 5.34659e-05
	LOSS [training: 0.059348858824910505 | validation: 0.06088387192050652]
	TIME [epoch: 8.28 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05978822800286249		[learning rate: 5.3277e-05]
	Learning Rate: 5.32769e-05
	LOSS [training: 0.05978822800286249 | validation: 0.060370610416636716]
	TIME [epoch: 8.28 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05905112043713978		[learning rate: 5.3088e-05]
	Learning Rate: 5.30884e-05
	LOSS [training: 0.05905112043713978 | validation: 0.06671225572399704]
	TIME [epoch: 8.29 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0590694045235559		[learning rate: 5.2901e-05]
	Learning Rate: 5.29007e-05
	LOSS [training: 0.0590694045235559 | validation: 0.06270283080827015]
	TIME [epoch: 8.34 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05785191557625178		[learning rate: 5.2714e-05]
	Learning Rate: 5.27137e-05
	LOSS [training: 0.05785191557625178 | validation: 0.06457083987100122]
	TIME [epoch: 8.3 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058178803206940535		[learning rate: 5.2527e-05]
	Learning Rate: 5.25272e-05
	LOSS [training: 0.058178803206940535 | validation: 0.06376955838941983]
	TIME [epoch: 8.29 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05913636447783737		[learning rate: 5.2342e-05]
	Learning Rate: 5.23415e-05
	LOSS [training: 0.05913636447783737 | validation: 0.06400833012857864]
	TIME [epoch: 8.29 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058670291208289586		[learning rate: 5.2156e-05]
	Learning Rate: 5.21564e-05
	LOSS [training: 0.058670291208289586 | validation: 0.06366390570595362]
	TIME [epoch: 8.29 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058409477639606064		[learning rate: 5.1972e-05]
	Learning Rate: 5.1972e-05
	LOSS [training: 0.058409477639606064 | validation: 0.06829234731532179]
	TIME [epoch: 8.31 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05777499194552484		[learning rate: 5.1788e-05]
	Learning Rate: 5.17882e-05
	LOSS [training: 0.05777499194552484 | validation: 0.062187606900093534]
	TIME [epoch: 8.32 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05852867369779738		[learning rate: 5.1605e-05]
	Learning Rate: 5.16051e-05
	LOSS [training: 0.05852867369779738 | validation: 0.060310453083438235]
	TIME [epoch: 8.29 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0581435083983293		[learning rate: 5.1423e-05]
	Learning Rate: 5.14226e-05
	LOSS [training: 0.0581435083983293 | validation: 0.06476672735945964]
	TIME [epoch: 8.29 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05800828959762942		[learning rate: 5.1241e-05]
	Learning Rate: 5.12407e-05
	LOSS [training: 0.05800828959762942 | validation: 0.06390973232803647]
	TIME [epoch: 8.28 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05749608926982695		[learning rate: 5.106e-05]
	Learning Rate: 5.10596e-05
	LOSS [training: 0.05749608926982695 | validation: 0.06452277611344534]
	TIME [epoch: 8.29 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0589917916039282		[learning rate: 5.0879e-05]
	Learning Rate: 5.0879e-05
	LOSS [training: 0.0589917916039282 | validation: 0.059661101136137394]
	TIME [epoch: 8.33 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05934416769605289		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 0.05934416769605289 | validation: 0.06424273100394591]
	TIME [epoch: 8.3 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05895906798845495		[learning rate: 5.052e-05]
	Learning Rate: 5.05198e-05
	LOSS [training: 0.05895906798845495 | validation: 0.06225357493910516]
	TIME [epoch: 8.28 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05722346688238691		[learning rate: 5.0341e-05]
	Learning Rate: 5.03412e-05
	LOSS [training: 0.05722346688238691 | validation: 0.06169125188881747]
	TIME [epoch: 8.29 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058587667152973325		[learning rate: 5.0163e-05]
	Learning Rate: 5.01631e-05
	LOSS [training: 0.058587667152973325 | validation: 0.06494726553994783]
	TIME [epoch: 8.29 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05891510512375205		[learning rate: 4.9986e-05]
	Learning Rate: 4.99857e-05
	LOSS [training: 0.05891510512375205 | validation: 0.06710740190008235]
	TIME [epoch: 8.29 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059623411006332845		[learning rate: 4.9809e-05]
	Learning Rate: 4.9809e-05
	LOSS [training: 0.059623411006332845 | validation: 0.0629377477755383]
	TIME [epoch: 8.33 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06021686284294187		[learning rate: 4.9633e-05]
	Learning Rate: 4.96329e-05
	LOSS [training: 0.06021686284294187 | validation: 0.06988204364533072]
	TIME [epoch: 8.3 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0599141187318053		[learning rate: 4.9457e-05]
	Learning Rate: 4.94573e-05
	LOSS [training: 0.0599141187318053 | validation: 0.06303340202848082]
	TIME [epoch: 8.29 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05891641564767573		[learning rate: 4.9282e-05]
	Learning Rate: 4.92825e-05
	LOSS [training: 0.05891641564767573 | validation: 0.06104755084299568]
	TIME [epoch: 8.29 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058257592865514546		[learning rate: 4.9108e-05]
	Learning Rate: 4.91082e-05
	LOSS [training: 0.058257592865514546 | validation: 0.05899710974851351]
	TIME [epoch: 8.29 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05792612241517589		[learning rate: 4.8935e-05]
	Learning Rate: 4.89345e-05
	LOSS [training: 0.05792612241517589 | validation: 0.0622852614201242]
	TIME [epoch: 8.31 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05872111727509269		[learning rate: 4.8762e-05]
	Learning Rate: 4.87615e-05
	LOSS [training: 0.05872111727509269 | validation: 0.06358585111238399]
	TIME [epoch: 8.32 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05694083991178925		[learning rate: 4.8589e-05]
	Learning Rate: 4.85891e-05
	LOSS [training: 0.05694083991178925 | validation: 0.06073413866038682]
	TIME [epoch: 8.29 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0585920832968633		[learning rate: 4.8417e-05]
	Learning Rate: 4.84172e-05
	LOSS [training: 0.0585920832968633 | validation: 0.060741079217784065]
	TIME [epoch: 8.29 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05794901572174552		[learning rate: 4.8246e-05]
	Learning Rate: 4.8246e-05
	LOSS [training: 0.05794901572174552 | validation: 0.06399505922932192]
	TIME [epoch: 8.29 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057950767478941616		[learning rate: 4.8075e-05]
	Learning Rate: 4.80754e-05
	LOSS [training: 0.057950767478941616 | validation: 0.06031787055662677]
	TIME [epoch: 8.29 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058395067917521346		[learning rate: 4.7905e-05]
	Learning Rate: 4.79054e-05
	LOSS [training: 0.058395067917521346 | validation: 0.0629743460722062]
	TIME [epoch: 8.33 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05705200718697577		[learning rate: 4.7736e-05]
	Learning Rate: 4.7736e-05
	LOSS [training: 0.05705200718697577 | validation: 0.0636070861161136]
	TIME [epoch: 8.31 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057721637778446674		[learning rate: 4.7567e-05]
	Learning Rate: 4.75672e-05
	LOSS [training: 0.057721637778446674 | validation: 0.06222585238312199]
	TIME [epoch: 8.29 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05890042968517996		[learning rate: 4.7399e-05]
	Learning Rate: 4.7399e-05
	LOSS [training: 0.05890042968517996 | validation: 0.06251462830451403]
	TIME [epoch: 8.29 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05868904796959493		[learning rate: 4.7231e-05]
	Learning Rate: 4.72314e-05
	LOSS [training: 0.05868904796959493 | validation: 0.06733346392810387]
	TIME [epoch: 8.29 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05903040402469323		[learning rate: 4.7064e-05]
	Learning Rate: 4.70644e-05
	LOSS [training: 0.05903040402469323 | validation: 0.06463217058372572]
	TIME [epoch: 8.3 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0591924925149044		[learning rate: 4.6898e-05]
	Learning Rate: 4.6898e-05
	LOSS [training: 0.0591924925149044 | validation: 0.06416437124437434]
	TIME [epoch: 8.33 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05800049517906779		[learning rate: 4.6732e-05]
	Learning Rate: 4.67321e-05
	LOSS [training: 0.05800049517906779 | validation: 0.06186610153039282]
	TIME [epoch: 8.3 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057020388549793945		[learning rate: 4.6567e-05]
	Learning Rate: 4.65669e-05
	LOSS [training: 0.057020388549793945 | validation: 0.06616965098629954]
	TIME [epoch: 8.29 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05882823667690891		[learning rate: 4.6402e-05]
	Learning Rate: 4.64022e-05
	LOSS [training: 0.05882823667690891 | validation: 0.06484473848521914]
	TIME [epoch: 8.29 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0571954796343204		[learning rate: 4.6238e-05]
	Learning Rate: 4.62381e-05
	LOSS [training: 0.0571954796343204 | validation: 0.06219317909379509]
	TIME [epoch: 8.29 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05779041114877272		[learning rate: 4.6075e-05]
	Learning Rate: 4.60746e-05
	LOSS [training: 0.05779041114877272 | validation: 0.06221104581392997]
	TIME [epoch: 8.31 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05834384238148795		[learning rate: 4.5912e-05]
	Learning Rate: 4.59117e-05
	LOSS [training: 0.05834384238148795 | validation: 0.06302092499129855]
	TIME [epoch: 8.32 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05763182359297135		[learning rate: 4.5749e-05]
	Learning Rate: 4.57493e-05
	LOSS [training: 0.05763182359297135 | validation: 0.06851970904682952]
	TIME [epoch: 8.29 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05861902508762489		[learning rate: 4.5588e-05]
	Learning Rate: 4.55875e-05
	LOSS [training: 0.05861902508762489 | validation: 0.06374457793392951]
	TIME [epoch: 8.29 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059657489941934545		[learning rate: 4.5426e-05]
	Learning Rate: 4.54264e-05
	LOSS [training: 0.059657489941934545 | validation: 0.06546786738034287]
	TIME [epoch: 8.29 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05913373650499929		[learning rate: 4.5266e-05]
	Learning Rate: 4.52657e-05
	LOSS [training: 0.05913373650499929 | validation: 0.06414115939236431]
	TIME [epoch: 8.29 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059524520267050206		[learning rate: 4.5106e-05]
	Learning Rate: 4.51056e-05
	LOSS [training: 0.059524520267050206 | validation: 0.06679738301322234]
	TIME [epoch: 8.34 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05912885129314299		[learning rate: 4.4946e-05]
	Learning Rate: 4.49461e-05
	LOSS [training: 0.05912885129314299 | validation: 0.06574116460812326]
	TIME [epoch: 8.3 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057866805265043224		[learning rate: 4.4787e-05]
	Learning Rate: 4.47872e-05
	LOSS [training: 0.057866805265043224 | validation: 0.06714876392275572]
	TIME [epoch: 8.28 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05871060003268235		[learning rate: 4.4629e-05]
	Learning Rate: 4.46288e-05
	LOSS [training: 0.05871060003268235 | validation: 0.06545099014446054]
	TIME [epoch: 8.29 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05841628705384108		[learning rate: 4.4471e-05]
	Learning Rate: 4.4471e-05
	LOSS [training: 0.05841628705384108 | validation: 0.06281867404066072]
	TIME [epoch: 8.37 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05833000341134488		[learning rate: 4.4314e-05]
	Learning Rate: 4.43138e-05
	LOSS [training: 0.05833000341134488 | validation: 0.06513362922443869]
	TIME [epoch: 8.29 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059730113904509885		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 0.059730113904509885 | validation: 0.06212468224732146]
	TIME [epoch: 8.34 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0585611970245375		[learning rate: 4.4001e-05]
	Learning Rate: 4.40009e-05
	LOSS [training: 0.0585611970245375 | validation: 0.0636180445500738]
	TIME [epoch: 8.29 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05885617533631048		[learning rate: 4.3845e-05]
	Learning Rate: 4.38453e-05
	LOSS [training: 0.05885617533631048 | validation: 0.061361125449301904]
	TIME [epoch: 8.28 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05967600257776372		[learning rate: 4.369e-05]
	Learning Rate: 4.36903e-05
	LOSS [training: 0.05967600257776372 | validation: 0.06190198244784149]
	TIME [epoch: 8.29 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058234281963862586		[learning rate: 4.3536e-05]
	Learning Rate: 4.35358e-05
	LOSS [training: 0.058234281963862586 | validation: 0.06640343576052293]
	TIME [epoch: 8.29 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0600682656604482		[learning rate: 4.3382e-05]
	Learning Rate: 4.33818e-05
	LOSS [training: 0.0600682656604482 | validation: 0.0659849662200549]
	TIME [epoch: 8.31 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05818191938590204		[learning rate: 4.3228e-05]
	Learning Rate: 4.32284e-05
	LOSS [training: 0.05818191938590204 | validation: 0.05809123634438592]
	TIME [epoch: 8.33 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057919989532639375		[learning rate: 4.3076e-05]
	Learning Rate: 4.30756e-05
	LOSS [training: 0.057919989532639375 | validation: 0.06300947238969719]
	TIME [epoch: 8.3 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05880996938980762		[learning rate: 4.2923e-05]
	Learning Rate: 4.29232e-05
	LOSS [training: 0.05880996938980762 | validation: 0.06343884311356854]
	TIME [epoch: 8.29 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05919054345074813		[learning rate: 4.2771e-05]
	Learning Rate: 4.27714e-05
	LOSS [training: 0.05919054345074813 | validation: 0.06797623848548834]
	TIME [epoch: 8.29 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05868118152676237		[learning rate: 4.262e-05]
	Learning Rate: 4.26202e-05
	LOSS [training: 0.05868118152676237 | validation: 0.06456397570845619]
	TIME [epoch: 8.29 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057751606011609224		[learning rate: 4.2469e-05]
	Learning Rate: 4.24695e-05
	LOSS [training: 0.057751606011609224 | validation: 0.06478506373269072]
	TIME [epoch: 8.33 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05758037565471333		[learning rate: 4.2319e-05]
	Learning Rate: 4.23193e-05
	LOSS [training: 0.05758037565471333 | validation: 0.06335326887865649]
	TIME [epoch: 8.29 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058459342262264374		[learning rate: 4.217e-05]
	Learning Rate: 4.21697e-05
	LOSS [training: 0.058459342262264374 | validation: 0.06307933323928572]
	TIME [epoch: 8.29 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05683156277434074		[learning rate: 4.2021e-05]
	Learning Rate: 4.20205e-05
	LOSS [training: 0.05683156277434074 | validation: 0.06045064851070226]
	TIME [epoch: 8.29 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058658407143823824		[learning rate: 4.1872e-05]
	Learning Rate: 4.18719e-05
	LOSS [training: 0.058658407143823824 | validation: 0.06447487065285744]
	TIME [epoch: 8.29 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05884252606938895		[learning rate: 4.1724e-05]
	Learning Rate: 4.17239e-05
	LOSS [training: 0.05884252606938895 | validation: 0.06294980365623651]
	TIME [epoch: 8.3 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05856078919252586		[learning rate: 4.1576e-05]
	Learning Rate: 4.15763e-05
	LOSS [training: 0.05856078919252586 | validation: 0.0652756663159222]
	TIME [epoch: 8.33 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05784452940852207		[learning rate: 4.1429e-05]
	Learning Rate: 4.14293e-05
	LOSS [training: 0.05784452940852207 | validation: 0.06655941636682114]
	TIME [epoch: 8.29 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05849866356471836		[learning rate: 4.1283e-05]
	Learning Rate: 4.12828e-05
	LOSS [training: 0.05849866356471836 | validation: 0.0629792384306562]
	TIME [epoch: 8.29 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05811648058569781		[learning rate: 4.1137e-05]
	Learning Rate: 4.11368e-05
	LOSS [training: 0.05811648058569781 | validation: 0.061331420482776476]
	TIME [epoch: 8.29 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05828751519235159		[learning rate: 4.0991e-05]
	Learning Rate: 4.09914e-05
	LOSS [training: 0.05828751519235159 | validation: 0.063621891553727]
	TIME [epoch: 8.31 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0583262496726341		[learning rate: 4.0846e-05]
	Learning Rate: 4.08464e-05
	LOSS [training: 0.0583262496726341 | validation: 0.06343123844364595]
	TIME [epoch: 8.34 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05822800903227885		[learning rate: 4.0702e-05]
	Learning Rate: 4.0702e-05
	LOSS [training: 0.05822800903227885 | validation: 0.06683411865173547]
	TIME [epoch: 8.34 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0570760220558252		[learning rate: 4.0558e-05]
	Learning Rate: 4.0558e-05
	LOSS [training: 0.0570760220558252 | validation: 0.06436491926393238]
	TIME [epoch: 8.3 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05836959693834899		[learning rate: 4.0415e-05]
	Learning Rate: 4.04146e-05
	LOSS [training: 0.05836959693834899 | validation: 0.06051057175738972]
	TIME [epoch: 8.3 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05873251806562902		[learning rate: 4.0272e-05]
	Learning Rate: 4.02717e-05
	LOSS [training: 0.05873251806562902 | validation: 0.062275397802389026]
	TIME [epoch: 8.3 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059724791431944826		[learning rate: 4.0129e-05]
	Learning Rate: 4.01293e-05
	LOSS [training: 0.059724791431944826 | validation: 0.06155608741819396]
	TIME [epoch: 8.3 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06242040313971224		[learning rate: 3.9987e-05]
	Learning Rate: 3.99874e-05
	LOSS [training: 0.06242040313971224 | validation: 0.06616035766886463]
	TIME [epoch: 8.35 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061220486754724156		[learning rate: 3.9846e-05]
	Learning Rate: 3.9846e-05
	LOSS [training: 0.061220486754724156 | validation: 0.06480131234228281]
	TIME [epoch: 8.31 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06086648697015736		[learning rate: 3.9705e-05]
	Learning Rate: 3.97051e-05
	LOSS [training: 0.06086648697015736 | validation: 0.06810420069401528]
	TIME [epoch: 8.3 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060973302313166104		[learning rate: 3.9565e-05]
	Learning Rate: 3.95647e-05
	LOSS [training: 0.060973302313166104 | validation: 0.06215761756743919]
	TIME [epoch: 8.3 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06109090308761382		[learning rate: 3.9425e-05]
	Learning Rate: 3.94248e-05
	LOSS [training: 0.06109090308761382 | validation: 0.06757673107216675]
	TIME [epoch: 8.32 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06122718039682641		[learning rate: 3.9285e-05]
	Learning Rate: 3.92854e-05
	LOSS [training: 0.06122718039682641 | validation: 0.06696753659034718]
	TIME [epoch: 8.32 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06022422781149628		[learning rate: 3.9146e-05]
	Learning Rate: 3.91464e-05
	LOSS [training: 0.06022422781149628 | validation: 0.0681783289133441]
	TIME [epoch: 8.34 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060236887165071175		[learning rate: 3.9008e-05]
	Learning Rate: 3.9008e-05
	LOSS [training: 0.060236887165071175 | validation: 0.06650641716997502]
	TIME [epoch: 8.31 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06100388977089623		[learning rate: 3.887e-05]
	Learning Rate: 3.88701e-05
	LOSS [training: 0.06100388977089623 | validation: 0.06261141620455571]
	TIME [epoch: 8.3 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059134619014579254		[learning rate: 3.8733e-05]
	Learning Rate: 3.87326e-05
	LOSS [training: 0.059134619014579254 | validation: 0.06701398652945616]
	TIME [epoch: 8.31 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05865752324953223		[learning rate: 3.8596e-05]
	Learning Rate: 3.85957e-05
	LOSS [training: 0.05865752324953223 | validation: 0.06280326608493147]
	TIME [epoch: 8.3 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06076847545868291		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 0.06076847545868291 | validation: 0.061797643916178605]
	TIME [epoch: 8.34 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0608983878584627		[learning rate: 3.8323e-05]
	Learning Rate: 3.83232e-05
	LOSS [training: 0.0608983878584627 | validation: 0.0658151247560846]
	TIME [epoch: 8.32 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05999514848160946		[learning rate: 3.8188e-05]
	Learning Rate: 3.81877e-05
	LOSS [training: 0.05999514848160946 | validation: 0.06769555423575914]
	TIME [epoch: 8.3 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05952529678122484		[learning rate: 3.8053e-05]
	Learning Rate: 3.80526e-05
	LOSS [training: 0.05952529678122484 | validation: 0.0638762423545658]
	TIME [epoch: 8.31 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056898192162346845		[learning rate: 3.7918e-05]
	Learning Rate: 3.79181e-05
	LOSS [training: 0.056898192162346845 | validation: 0.0678659145771339]
	TIME [epoch: 8.31 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059200998831792286		[learning rate: 3.7784e-05]
	Learning Rate: 3.7784e-05
	LOSS [training: 0.059200998831792286 | validation: 0.060686962879067156]
	TIME [epoch: 8.31 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05906333706754105		[learning rate: 3.765e-05]
	Learning Rate: 3.76504e-05
	LOSS [training: 0.05906333706754105 | validation: 0.06777097108645228]
	TIME [epoch: 8.35 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060538106944021494		[learning rate: 3.7517e-05]
	Learning Rate: 3.75172e-05
	LOSS [training: 0.060538106944021494 | validation: 0.06596731657655591]
	TIME [epoch: 8.31 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05951243336777421		[learning rate: 3.7385e-05]
	Learning Rate: 3.73846e-05
	LOSS [training: 0.05951243336777421 | validation: 0.06699255458134994]
	TIME [epoch: 8.31 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060241214530747364		[learning rate: 3.7252e-05]
	Learning Rate: 3.72524e-05
	LOSS [training: 0.060241214530747364 | validation: 0.06322469887119783]
	TIME [epoch: 8.31 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06124827345881291		[learning rate: 3.7121e-05]
	Learning Rate: 3.71206e-05
	LOSS [training: 0.06124827345881291 | validation: 0.0616906501896978]
	TIME [epoch: 8.31 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05972131668053416		[learning rate: 3.6989e-05]
	Learning Rate: 3.69894e-05
	LOSS [training: 0.05972131668053416 | validation: 0.061843874322442274]
	TIME [epoch: 8.33 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05874902917978766		[learning rate: 3.6859e-05]
	Learning Rate: 3.68586e-05
	LOSS [training: 0.05874902917978766 | validation: 0.05914012543982276]
	TIME [epoch: 8.35 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058809273169327075		[learning rate: 3.6728e-05]
	Learning Rate: 3.67282e-05
	LOSS [training: 0.058809273169327075 | validation: 0.06325129603784484]
	TIME [epoch: 8.31 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05850766057051803		[learning rate: 3.6598e-05]
	Learning Rate: 3.65984e-05
	LOSS [training: 0.05850766057051803 | validation: 0.06169566352361578]
	TIME [epoch: 8.31 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05930402490489736		[learning rate: 3.6469e-05]
	Learning Rate: 3.64689e-05
	LOSS [training: 0.05930402490489736 | validation: 0.0647929082094513]
	TIME [epoch: 8.3 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058126240417928646		[learning rate: 3.634e-05]
	Learning Rate: 3.634e-05
	LOSS [training: 0.058126240417928646 | validation: 0.06437211646648366]
	TIME [epoch: 8.3 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05874357932181805		[learning rate: 3.6211e-05]
	Learning Rate: 3.62115e-05
	LOSS [training: 0.05874357932181805 | validation: 0.06350573226867896]
	TIME [epoch: 8.34 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05965385768849783		[learning rate: 3.6083e-05]
	Learning Rate: 3.60834e-05
	LOSS [training: 0.05965385768849783 | validation: 0.06407743770126402]
	TIME [epoch: 8.32 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05868241915950459		[learning rate: 3.5956e-05]
	Learning Rate: 3.59558e-05
	LOSS [training: 0.05868241915950459 | validation: 0.0653427449265633]
	TIME [epoch: 8.3 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059033989980176026		[learning rate: 3.5829e-05]
	Learning Rate: 3.58287e-05
	LOSS [training: 0.059033989980176026 | validation: 0.06278387965149057]
	TIME [epoch: 8.3 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05720770773755027		[learning rate: 3.5702e-05]
	Learning Rate: 3.5702e-05
	LOSS [training: 0.05720770773755027 | validation: 0.060924739512387235]
	TIME [epoch: 8.3 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05856488384292688		[learning rate: 3.5576e-05]
	Learning Rate: 3.55757e-05
	LOSS [training: 0.05856488384292688 | validation: 0.06264228453255112]
	TIME [epoch: 8.31 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058273705755595755		[learning rate: 3.545e-05]
	Learning Rate: 3.54499e-05
	LOSS [training: 0.058273705755595755 | validation: 0.0636339908698599]
	TIME [epoch: 8.34 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05779239806083159		[learning rate: 3.5325e-05]
	Learning Rate: 3.53246e-05
	LOSS [training: 0.05779239806083159 | validation: 0.06514478767767842]
	TIME [epoch: 8.3 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058863374604491814		[learning rate: 3.52e-05]
	Learning Rate: 3.51997e-05
	LOSS [training: 0.058863374604491814 | validation: 0.06562679918354664]
	TIME [epoch: 8.3 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058073136068614785		[learning rate: 3.5075e-05]
	Learning Rate: 3.50752e-05
	LOSS [training: 0.058073136068614785 | validation: 0.06095935732905361]
	TIME [epoch: 8.31 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058785974028645716		[learning rate: 3.4951e-05]
	Learning Rate: 3.49512e-05
	LOSS [training: 0.058785974028645716 | validation: 0.06557741825185794]
	TIME [epoch: 8.31 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05750493472268559		[learning rate: 3.4828e-05]
	Learning Rate: 3.48276e-05
	LOSS [training: 0.05750493472268559 | validation: 0.06594211256323826]
	TIME [epoch: 8.33 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058081880483119734		[learning rate: 3.4704e-05]
	Learning Rate: 3.47044e-05
	LOSS [training: 0.058081880483119734 | validation: 0.05990705536309183]
	TIME [epoch: 8.34 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05795920659373444		[learning rate: 3.4582e-05]
	Learning Rate: 3.45817e-05
	LOSS [training: 0.05795920659373444 | validation: 0.06462851428481017]
	TIME [epoch: 8.31 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05798130682694658		[learning rate: 3.4459e-05]
	Learning Rate: 3.44594e-05
	LOSS [training: 0.05798130682694658 | validation: 0.06568086056612282]
	TIME [epoch: 8.31 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057808268063199136		[learning rate: 3.4338e-05]
	Learning Rate: 3.43375e-05
	LOSS [training: 0.057808268063199136 | validation: 0.06502111149560752]
	TIME [epoch: 8.3 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05739726910170912		[learning rate: 3.4216e-05]
	Learning Rate: 3.42161e-05
	LOSS [training: 0.05739726910170912 | validation: 0.06012381622399646]
	TIME [epoch: 8.3 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05785537821644432		[learning rate: 3.4095e-05]
	Learning Rate: 3.40951e-05
	LOSS [training: 0.05785537821644432 | validation: 0.06332442919044648]
	TIME [epoch: 8.35 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060053283315916896		[learning rate: 3.3975e-05]
	Learning Rate: 3.39746e-05
	LOSS [training: 0.060053283315916896 | validation: 0.06574072685095309]
	TIME [epoch: 8.31 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058586512999630413		[learning rate: 3.3854e-05]
	Learning Rate: 3.38544e-05
	LOSS [training: 0.058586512999630413 | validation: 0.06455554364787541]
	TIME [epoch: 8.3 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05761613314075273		[learning rate: 3.3735e-05]
	Learning Rate: 3.37347e-05
	LOSS [training: 0.05761613314075273 | validation: 0.06541652450035833]
	TIME [epoch: 8.3 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0581707127480082		[learning rate: 3.3615e-05]
	Learning Rate: 3.36154e-05
	LOSS [training: 0.0581707127480082 | validation: 0.06618670851392895]
	TIME [epoch: 8.3 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05838992320075981		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 0.05838992320075981 | validation: 0.0634261196471852]
	TIME [epoch: 8.31 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057819750422346584		[learning rate: 3.3378e-05]
	Learning Rate: 3.33781e-05
	LOSS [training: 0.057819750422346584 | validation: 0.06269614391796181]
	TIME [epoch: 8.35 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05834744732896799		[learning rate: 3.326e-05]
	Learning Rate: 3.32601e-05
	LOSS [training: 0.05834744732896799 | validation: 0.06610703566798894]
	TIME [epoch: 8.31 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05830991648575629		[learning rate: 3.3142e-05]
	Learning Rate: 3.31425e-05
	LOSS [training: 0.05830991648575629 | validation: 0.06380037228001499]
	TIME [epoch: 8.3 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05839616981602831		[learning rate: 3.3025e-05]
	Learning Rate: 3.30253e-05
	LOSS [training: 0.05839616981602831 | validation: 0.06930534818977199]
	TIME [epoch: 8.3 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05931646529525825		[learning rate: 3.2908e-05]
	Learning Rate: 3.29085e-05
	LOSS [training: 0.05931646529525825 | validation: 0.060822054205973086]
	TIME [epoch: 8.3 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06009719525712869		[learning rate: 3.2792e-05]
	Learning Rate: 3.27921e-05
	LOSS [training: 0.06009719525712869 | validation: 0.06152672677910599]
	TIME [epoch: 8.32 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05994115907211038		[learning rate: 3.2676e-05]
	Learning Rate: 3.26761e-05
	LOSS [training: 0.05994115907211038 | validation: 0.0640962813417369]
	TIME [epoch: 8.33 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059468217791360245		[learning rate: 3.2561e-05]
	Learning Rate: 3.25606e-05
	LOSS [training: 0.059468217791360245 | validation: 0.06575597289626361]
	TIME [epoch: 8.3 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05923087290418452		[learning rate: 3.2445e-05]
	Learning Rate: 3.24455e-05
	LOSS [training: 0.05923087290418452 | validation: 0.06523158644070003]
	TIME [epoch: 8.3 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059090037125953385		[learning rate: 3.2331e-05]
	Learning Rate: 3.23307e-05
	LOSS [training: 0.059090037125953385 | validation: 0.06180415814436191]
	TIME [epoch: 8.3 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05881150691826705		[learning rate: 3.2216e-05]
	Learning Rate: 3.22164e-05
	LOSS [training: 0.05881150691826705 | validation: 0.06685490914149882]
	TIME [epoch: 8.3 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05918534015356485		[learning rate: 3.2102e-05]
	Learning Rate: 3.21025e-05
	LOSS [training: 0.05918534015356485 | validation: 0.05971908938242346]
	TIME [epoch: 8.37 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058763873364379306		[learning rate: 3.1989e-05]
	Learning Rate: 3.1989e-05
	LOSS [training: 0.058763873364379306 | validation: 0.06417825849911077]
	TIME [epoch: 8.33 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0588083423106027		[learning rate: 3.1876e-05]
	Learning Rate: 3.18758e-05
	LOSS [training: 0.0588083423106027 | validation: 0.06195617647678706]
	TIME [epoch: 8.31 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05944964141846813		[learning rate: 3.1763e-05]
	Learning Rate: 3.17631e-05
	LOSS [training: 0.05944964141846813 | validation: 0.0596247995976838]
	TIME [epoch: 8.31 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059884868188630425		[learning rate: 3.1651e-05]
	Learning Rate: 3.16508e-05
	LOSS [training: 0.059884868188630425 | validation: 0.06203050071560223]
	TIME [epoch: 8.31 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058587656340525915		[learning rate: 3.1539e-05]
	Learning Rate: 3.15389e-05
	LOSS [training: 0.058587656340525915 | validation: 0.06292092564996477]
	TIME [epoch: 8.31 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05711214471095616		[learning rate: 3.1427e-05]
	Learning Rate: 3.14274e-05
	LOSS [training: 0.05711214471095616 | validation: 0.06521196542821192]
	TIME [epoch: 8.35 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0588813257477704		[learning rate: 3.1316e-05]
	Learning Rate: 3.13162e-05
	LOSS [training: 0.0588813257477704 | validation: 0.06020604897185866]
	TIME [epoch: 8.31 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05878160305775738		[learning rate: 3.1205e-05]
	Learning Rate: 3.12055e-05
	LOSS [training: 0.05878160305775738 | validation: 0.06119741058117022]
	TIME [epoch: 8.31 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058515985968991345		[learning rate: 3.1095e-05]
	Learning Rate: 3.10951e-05
	LOSS [training: 0.058515985968991345 | validation: 0.06476149007769011]
	TIME [epoch: 8.3 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05746139413827847		[learning rate: 3.0985e-05]
	Learning Rate: 3.09852e-05
	LOSS [training: 0.05746139413827847 | validation: 0.06619973240220228]
	TIME [epoch: 8.3 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05885023817674098		[learning rate: 3.0876e-05]
	Learning Rate: 3.08756e-05
	LOSS [training: 0.05885023817674098 | validation: 0.06602111760243978]
	TIME [epoch: 8.33 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05985514008965718		[learning rate: 3.0766e-05]
	Learning Rate: 3.07664e-05
	LOSS [training: 0.05985514008965718 | validation: 0.06375152969013882]
	TIME [epoch: 8.34 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05808948124005737		[learning rate: 3.0658e-05]
	Learning Rate: 3.06576e-05
	LOSS [training: 0.05808948124005737 | validation: 0.06443631528410737]
	TIME [epoch: 8.3 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05959347961214586		[learning rate: 3.0549e-05]
	Learning Rate: 3.05492e-05
	LOSS [training: 0.05959347961214586 | validation: 0.06506289757481762]
	TIME [epoch: 8.3 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05893403854188772		[learning rate: 3.0441e-05]
	Learning Rate: 3.04412e-05
	LOSS [training: 0.05893403854188772 | validation: 0.06203623487605317]
	TIME [epoch: 8.3 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0582029635524809		[learning rate: 3.0334e-05]
	Learning Rate: 3.03336e-05
	LOSS [training: 0.0582029635524809 | validation: 0.06646492968928558]
	TIME [epoch: 8.31 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06060627002263813		[learning rate: 3.0226e-05]
	Learning Rate: 3.02263e-05
	LOSS [training: 0.06060627002263813 | validation: 0.060468934500533464]
	TIME [epoch: 8.36 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059110119013369496		[learning rate: 3.0119e-05]
	Learning Rate: 3.01194e-05
	LOSS [training: 0.059110119013369496 | validation: 0.06447246930010389]
	TIME [epoch: 8.31 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0587445430179125		[learning rate: 3.0013e-05]
	Learning Rate: 3.00129e-05
	LOSS [training: 0.0587445430179125 | validation: 0.0661840019298042]
	TIME [epoch: 8.31 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05929519065811059		[learning rate: 2.9907e-05]
	Learning Rate: 2.99068e-05
	LOSS [training: 0.05929519065811059 | validation: 0.06626815979823727]
	TIME [epoch: 8.3 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059935039233157836		[learning rate: 2.9801e-05]
	Learning Rate: 2.9801e-05
	LOSS [training: 0.059935039233157836 | validation: 0.06481056455148039]
	TIME [epoch: 8.3 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05914448540953404		[learning rate: 2.9696e-05]
	Learning Rate: 2.96956e-05
	LOSS [training: 0.05914448540953404 | validation: 0.064349214308753]
	TIME [epoch: 8.32 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059406966387926904		[learning rate: 2.9591e-05]
	Learning Rate: 2.95906e-05
	LOSS [training: 0.059406966387926904 | validation: 0.06908054524773305]
	TIME [epoch: 8.36 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05801275230916811		[learning rate: 2.9486e-05]
	Learning Rate: 2.9486e-05
	LOSS [training: 0.05801275230916811 | validation: 0.06264721118589466]
	TIME [epoch: 8.31 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05808054741036624		[learning rate: 2.9382e-05]
	Learning Rate: 2.93817e-05
	LOSS [training: 0.05808054741036624 | validation: 0.06551872315511217]
	TIME [epoch: 8.3 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058473026436183295		[learning rate: 2.9278e-05]
	Learning Rate: 2.92778e-05
	LOSS [training: 0.058473026436183295 | validation: 0.06930337561352633]
	TIME [epoch: 8.31 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058840611852302285		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 0.058840611852302285 | validation: 0.0643928153866673]
	TIME [epoch: 8.31 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061086198931321244		[learning rate: 2.9071e-05]
	Learning Rate: 2.90711e-05
	LOSS [training: 0.061086198931321244 | validation: 0.06493185092480994]
	TIME [epoch: 8.34 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05960138240324491		[learning rate: 2.8968e-05]
	Learning Rate: 2.89683e-05
	LOSS [training: 0.05960138240324491 | validation: 0.06596888273288001]
	TIME [epoch: 8.32 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058089368809265435		[learning rate: 2.8866e-05]
	Learning Rate: 2.88659e-05
	LOSS [training: 0.058089368809265435 | validation: 0.0664959078017881]
	TIME [epoch: 8.3 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05914456232850315		[learning rate: 2.8764e-05]
	Learning Rate: 2.87638e-05
	LOSS [training: 0.05914456232850315 | validation: 0.0646534421325605]
	TIME [epoch: 8.3 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05904329633025229		[learning rate: 2.8662e-05]
	Learning Rate: 2.86621e-05
	LOSS [training: 0.05904329633025229 | validation: 0.06001839286043835]
	TIME [epoch: 8.3 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05883469378512101		[learning rate: 2.8561e-05]
	Learning Rate: 2.85607e-05
	LOSS [training: 0.05883469378512101 | validation: 0.062213510290526644]
	TIME [epoch: 8.3 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058530831995374136		[learning rate: 2.846e-05]
	Learning Rate: 2.84597e-05
	LOSS [training: 0.058530831995374136 | validation: 0.06363868426172387]
	TIME [epoch: 8.34 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0590123258583934		[learning rate: 2.8359e-05]
	Learning Rate: 2.83591e-05
	LOSS [training: 0.0590123258583934 | validation: 0.06619116673129527]
	TIME [epoch: 8.31 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05928544755188631		[learning rate: 2.8259e-05]
	Learning Rate: 2.82588e-05
	LOSS [training: 0.05928544755188631 | validation: 0.06543627196907362]
	TIME [epoch: 8.3 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058981000443423026		[learning rate: 2.8159e-05]
	Learning Rate: 2.81589e-05
	LOSS [training: 0.058981000443423026 | validation: 0.06390224817439069]
	TIME [epoch: 8.29 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059411168636294046		[learning rate: 2.8059e-05]
	Learning Rate: 2.80593e-05
	LOSS [training: 0.059411168636294046 | validation: 0.06474498786551967]
	TIME [epoch: 8.29 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060870882522338424		[learning rate: 2.796e-05]
	Learning Rate: 2.79601e-05
	LOSS [training: 0.060870882522338424 | validation: 0.06843496532219706]
	TIME [epoch: 8.31 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05692625515701025		[learning rate: 2.7861e-05]
	Learning Rate: 2.78612e-05
	LOSS [training: 0.05692625515701025 | validation: 0.06361017961234165]
	TIME [epoch: 8.32 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05910887105068241		[learning rate: 2.7763e-05]
	Learning Rate: 2.77627e-05
	LOSS [training: 0.05910887105068241 | validation: 0.0634631862096612]
	TIME [epoch: 8.29 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05861384687455189		[learning rate: 2.7665e-05]
	Learning Rate: 2.76645e-05
	LOSS [training: 0.05861384687455189 | validation: 0.06612147659719818]
	TIME [epoch: 8.29 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05794974713052693		[learning rate: 2.7567e-05]
	Learning Rate: 2.75667e-05
	LOSS [training: 0.05794974713052693 | validation: 0.06127362157925083]
	TIME [epoch: 8.29 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05868782931415606		[learning rate: 2.7469e-05]
	Learning Rate: 2.74692e-05
	LOSS [training: 0.05868782931415606 | validation: 0.06504850996067799]
	TIME [epoch: 8.29 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05793234951728004		[learning rate: 2.7372e-05]
	Learning Rate: 2.73721e-05
	LOSS [training: 0.05793234951728004 | validation: 0.06292263410838402]
	TIME [epoch: 8.34 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05950315408101771		[learning rate: 2.7275e-05]
	Learning Rate: 2.72753e-05
	LOSS [training: 0.05950315408101771 | validation: 0.061422442207708323]
	TIME [epoch: 8.31 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05884564799226706		[learning rate: 2.7179e-05]
	Learning Rate: 2.71788e-05
	LOSS [training: 0.05884564799226706 | validation: 0.06446136353313511]
	TIME [epoch: 8.29 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05911132250412614		[learning rate: 2.7083e-05]
	Learning Rate: 2.70827e-05
	LOSS [training: 0.05911132250412614 | validation: 0.06594432432457326]
	TIME [epoch: 8.3 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05880108460888037		[learning rate: 2.6987e-05]
	Learning Rate: 2.6987e-05
	LOSS [training: 0.05880108460888037 | validation: 0.06740281629156081]
	TIME [epoch: 8.3 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05753086248031351		[learning rate: 2.6892e-05]
	Learning Rate: 2.68915e-05
	LOSS [training: 0.05753086248031351 | validation: 0.06268692150209888]
	TIME [epoch: 8.3 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0582813666361201		[learning rate: 2.6796e-05]
	Learning Rate: 2.67964e-05
	LOSS [training: 0.0582813666361201 | validation: 0.07125288942905804]
	TIME [epoch: 8.34 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058485431516399365		[learning rate: 2.6702e-05]
	Learning Rate: 2.67017e-05
	LOSS [training: 0.058485431516399365 | validation: 0.06291257425716665]
	TIME [epoch: 8.3 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05905390994690948		[learning rate: 2.6607e-05]
	Learning Rate: 2.66073e-05
	LOSS [training: 0.05905390994690948 | validation: 0.06429682263358003]
	TIME [epoch: 8.29 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05818121573328028		[learning rate: 2.6513e-05]
	Learning Rate: 2.65132e-05
	LOSS [training: 0.05818121573328028 | validation: 0.06297384857599497]
	TIME [epoch: 8.29 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05945588561511295		[learning rate: 2.6419e-05]
	Learning Rate: 2.64194e-05
	LOSS [training: 0.05945588561511295 | validation: 0.062212110550744604]
	TIME [epoch: 8.3 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059448201082830186		[learning rate: 2.6326e-05]
	Learning Rate: 2.6326e-05
	LOSS [training: 0.059448201082830186 | validation: 0.06453079302261941]
	TIME [epoch: 8.32 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05693259001890309		[learning rate: 2.6233e-05]
	Learning Rate: 2.62329e-05
	LOSS [training: 0.05693259001890309 | validation: 0.06610352333980314]
	TIME [epoch: 8.32 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05816705995040256		[learning rate: 2.614e-05]
	Learning Rate: 2.61401e-05
	LOSS [training: 0.05816705995040256 | validation: 0.06500337453483993]
	TIME [epoch: 8.29 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058969285492379786		[learning rate: 2.6048e-05]
	Learning Rate: 2.60477e-05
	LOSS [training: 0.058969285492379786 | validation: 0.06630776612836332]
	TIME [epoch: 8.3 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05861121413349312		[learning rate: 2.5956e-05]
	Learning Rate: 2.59556e-05
	LOSS [training: 0.05861121413349312 | validation: 0.05861425679129693]
	TIME [epoch: 8.3 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05971977887826993		[learning rate: 2.5864e-05]
	Learning Rate: 2.58638e-05
	LOSS [training: 0.05971977887826993 | validation: 0.06694503909144309]
	TIME [epoch: 8.3 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058926890963063194		[learning rate: 2.5772e-05]
	Learning Rate: 2.57723e-05
	LOSS [training: 0.058926890963063194 | validation: 0.06528647285302515]
	TIME [epoch: 8.35 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05966484129851646		[learning rate: 2.5681e-05]
	Learning Rate: 2.56812e-05
	LOSS [training: 0.05966484129851646 | validation: 0.06395036967555556]
	TIME [epoch: 8.3 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05810759785304176		[learning rate: 2.559e-05]
	Learning Rate: 2.55904e-05
	LOSS [training: 0.05810759785304176 | validation: 0.06304142474539365]
	TIME [epoch: 8.29 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059459512089302065		[learning rate: 2.55e-05]
	Learning Rate: 2.54999e-05
	LOSS [training: 0.059459512089302065 | validation: 0.06790811174218345]
	TIME [epoch: 8.29 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05961702945443724		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 0.05961702945443724 | validation: 0.06759553500760383]
	TIME [epoch: 8.29 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05866852164605754		[learning rate: 2.532e-05]
	Learning Rate: 2.53199e-05
	LOSS [training: 0.05866852164605754 | validation: 0.0659613156827597]
	TIME [epoch: 8.3 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05971388056183873		[learning rate: 2.523e-05]
	Learning Rate: 2.52303e-05
	LOSS [training: 0.05971388056183873 | validation: 0.07026461108655964]
	TIME [epoch: 8.34 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05874000466393042		[learning rate: 2.5141e-05]
	Learning Rate: 2.51411e-05
	LOSS [training: 0.05874000466393042 | validation: 0.06327811595626054]
	TIME [epoch: 8.3 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059914587540039976		[learning rate: 2.5052e-05]
	Learning Rate: 2.50522e-05
	LOSS [training: 0.059914587540039976 | validation: 0.06279872632391564]
	TIME [epoch: 8.29 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05908820321583478		[learning rate: 2.4964e-05]
	Learning Rate: 2.49636e-05
	LOSS [training: 0.05908820321583478 | validation: 0.06519853138797431]
	TIME [epoch: 8.29 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05814170375229058		[learning rate: 2.4875e-05]
	Learning Rate: 2.48754e-05
	LOSS [training: 0.05814170375229058 | validation: 0.06130903202181283]
	TIME [epoch: 8.29 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05710920094666781		[learning rate: 2.4787e-05]
	Learning Rate: 2.47874e-05
	LOSS [training: 0.05710920094666781 | validation: 0.06487133065282663]
	TIME [epoch: 8.31 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05913354739210812		[learning rate: 2.47e-05]
	Learning Rate: 2.46997e-05
	LOSS [training: 0.05913354739210812 | validation: 0.0629824841222091]
	TIME [epoch: 8.33 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058812074762218995		[learning rate: 2.4612e-05]
	Learning Rate: 2.46124e-05
	LOSS [training: 0.058812074762218995 | validation: 0.06802589764782345]
	TIME [epoch: 8.3 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05884374053488634		[learning rate: 2.4525e-05]
	Learning Rate: 2.45254e-05
	LOSS [training: 0.05884374053488634 | validation: 0.06486007282361846]
	TIME [epoch: 8.3 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05822709764155145		[learning rate: 2.4439e-05]
	Learning Rate: 2.44386e-05
	LOSS [training: 0.05822709764155145 | validation: 0.06086124711013455]
	TIME [epoch: 8.29 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0593008937450858		[learning rate: 2.4352e-05]
	Learning Rate: 2.43522e-05
	LOSS [training: 0.0593008937450858 | validation: 0.06041957014876273]
	TIME [epoch: 8.29 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05777745287454167		[learning rate: 2.4266e-05]
	Learning Rate: 2.42661e-05
	LOSS [training: 0.05777745287454167 | validation: 0.06456748799691386]
	TIME [epoch: 8.33 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0587406588921175		[learning rate: 2.418e-05]
	Learning Rate: 2.41803e-05
	LOSS [training: 0.0587406588921175 | validation: 0.06666482370385002]
	TIME [epoch: 8.3 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05928419714112858		[learning rate: 2.4095e-05]
	Learning Rate: 2.40948e-05
	LOSS [training: 0.05928419714112858 | validation: 0.06640581987112146]
	TIME [epoch: 8.29 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05882530717963989		[learning rate: 2.401e-05]
	Learning Rate: 2.40096e-05
	LOSS [training: 0.05882530717963989 | validation: 0.061002646825621044]
	TIME [epoch: 8.3 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0595545592225375		[learning rate: 2.3925e-05]
	Learning Rate: 2.39247e-05
	LOSS [training: 0.0595545592225375 | validation: 0.06458665636348901]
	TIME [epoch: 8.3 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05898533581210262		[learning rate: 2.384e-05]
	Learning Rate: 2.38401e-05
	LOSS [training: 0.05898533581210262 | validation: 0.06491785713611857]
	TIME [epoch: 8.3 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0594195238672054		[learning rate: 2.3756e-05]
	Learning Rate: 2.37558e-05
	LOSS [training: 0.0594195238672054 | validation: 0.0636511033864746]
	TIME [epoch: 8.34 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0583727276908488		[learning rate: 2.3672e-05]
	Learning Rate: 2.36718e-05
	LOSS [training: 0.0583727276908488 | validation: 0.060658563530605565]
	TIME [epoch: 8.29 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05917031860493441		[learning rate: 2.3588e-05]
	Learning Rate: 2.35881e-05
	LOSS [training: 0.05917031860493441 | validation: 0.06534132316694197]
	TIME [epoch: 8.3 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05758039126321122		[learning rate: 2.3505e-05]
	Learning Rate: 2.35047e-05
	LOSS [training: 0.05758039126321122 | validation: 0.0650446745712287]
	TIME [epoch: 8.3 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057852731262908144		[learning rate: 2.3422e-05]
	Learning Rate: 2.34215e-05
	LOSS [training: 0.057852731262908144 | validation: 0.06161512637271366]
	TIME [epoch: 8.3 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05853458122767782		[learning rate: 2.3339e-05]
	Learning Rate: 2.33387e-05
	LOSS [training: 0.05853458122767782 | validation: 0.0617102338238429]
	TIME [epoch: 8.31 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057656750888112555		[learning rate: 2.3256e-05]
	Learning Rate: 2.32562e-05
	LOSS [training: 0.057656750888112555 | validation: 0.06521984888334649]
	TIME [epoch: 8.33 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058930190860099485		[learning rate: 2.3174e-05]
	Learning Rate: 2.31739e-05
	LOSS [training: 0.058930190860099485 | validation: 0.06282251262251735]
	TIME [epoch: 8.3 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05923021868311665		[learning rate: 2.3092e-05]
	Learning Rate: 2.3092e-05
	LOSS [training: 0.05923021868311665 | validation: 0.06109312463336534]
	TIME [epoch: 8.29 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05901072713671016		[learning rate: 2.301e-05]
	Learning Rate: 2.30103e-05
	LOSS [training: 0.05901072713671016 | validation: 0.0625066711212262]
	TIME [epoch: 8.29 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058124381340458074		[learning rate: 2.2929e-05]
	Learning Rate: 2.2929e-05
	LOSS [training: 0.058124381340458074 | validation: 0.06613763689292931]
	TIME [epoch: 8.29 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05902705216053113		[learning rate: 2.2848e-05]
	Learning Rate: 2.28479e-05
	LOSS [training: 0.05902705216053113 | validation: 0.07316823756287438]
	TIME [epoch: 8.34 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06099779835671433		[learning rate: 2.2767e-05]
	Learning Rate: 2.27671e-05
	LOSS [training: 0.06099779835671433 | validation: 0.06398717910969455]
	TIME [epoch: 8.31 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060783364363431513		[learning rate: 2.2687e-05]
	Learning Rate: 2.26866e-05
	LOSS [training: 0.060783364363431513 | validation: 0.0682640484442429]
	TIME [epoch: 8.3 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05974637371568197		[learning rate: 2.2606e-05]
	Learning Rate: 2.26064e-05
	LOSS [training: 0.05974637371568197 | validation: 0.06057349361008389]
	TIME [epoch: 8.29 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059218869743433634		[learning rate: 2.2526e-05]
	Learning Rate: 2.25264e-05
	LOSS [training: 0.059218869743433634 | validation: 0.06725079752263548]
	TIME [epoch: 8.29 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059293391698093376		[learning rate: 2.2447e-05]
	Learning Rate: 2.24468e-05
	LOSS [training: 0.059293391698093376 | validation: 0.06720010796475936]
	TIME [epoch: 8.3 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05958712810011496		[learning rate: 2.2367e-05]
	Learning Rate: 2.23674e-05
	LOSS [training: 0.05958712810011496 | validation: 0.06963945661007848]
	TIME [epoch: 8.34 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058550755112797434		[learning rate: 2.2288e-05]
	Learning Rate: 2.22883e-05
	LOSS [training: 0.058550755112797434 | validation: 0.0695289498054143]
	TIME [epoch: 8.3 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0593780440847322		[learning rate: 2.2209e-05]
	Learning Rate: 2.22095e-05
	LOSS [training: 0.0593780440847322 | validation: 0.0680378013804385]
	TIME [epoch: 8.3 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05895756621915804		[learning rate: 2.2131e-05]
	Learning Rate: 2.21309e-05
	LOSS [training: 0.05895756621915804 | validation: 0.06539835928793476]
	TIME [epoch: 8.3 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059547127865670235		[learning rate: 2.2053e-05]
	Learning Rate: 2.20527e-05
	LOSS [training: 0.059547127865670235 | validation: 0.06796710375151813]
	TIME [epoch: 8.3 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05777689947908988		[learning rate: 2.1975e-05]
	Learning Rate: 2.19747e-05
	LOSS [training: 0.05777689947908988 | validation: 0.06390012867514425]
	TIME [epoch: 8.31 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059775420944718945		[learning rate: 2.1897e-05]
	Learning Rate: 2.1897e-05
	LOSS [training: 0.059775420944718945 | validation: 0.06586290085283983]
	TIME [epoch: 8.33 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057855874143277725		[learning rate: 2.182e-05]
	Learning Rate: 2.18196e-05
	LOSS [training: 0.057855874143277725 | validation: 0.06228169053640689]
	TIME [epoch: 8.29 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05906213914603416		[learning rate: 2.1742e-05]
	Learning Rate: 2.17424e-05
	LOSS [training: 0.05906213914603416 | validation: 0.061999839820692806]
	TIME [epoch: 8.29 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05772245070317969		[learning rate: 2.1666e-05]
	Learning Rate: 2.16655e-05
	LOSS [training: 0.05772245070317969 | validation: 0.06262174108658394]
	TIME [epoch: 8.29 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05830852605638309		[learning rate: 2.1589e-05]
	Learning Rate: 2.15889e-05
	LOSS [training: 0.05830852605638309 | validation: 0.0648262919669657]
	TIME [epoch: 8.3 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059999027420028106		[learning rate: 2.1513e-05]
	Learning Rate: 2.15126e-05
	LOSS [training: 0.059999027420028106 | validation: 0.06509213433911998]
	TIME [epoch: 8.33 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058072330575969396		[learning rate: 2.1437e-05]
	Learning Rate: 2.14365e-05
	LOSS [training: 0.058072330575969396 | validation: 0.0629853482778569]
	TIME [epoch: 8.3 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05878308719122492		[learning rate: 2.1361e-05]
	Learning Rate: 2.13607e-05
	LOSS [training: 0.05878308719122492 | validation: 0.06431160134137669]
	TIME [epoch: 8.29 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059931689471287325		[learning rate: 2.1285e-05]
	Learning Rate: 2.12852e-05
	LOSS [training: 0.059931689471287325 | validation: 0.06615946565473038]
	TIME [epoch: 8.29 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05987295155548711		[learning rate: 2.121e-05]
	Learning Rate: 2.12099e-05
	LOSS [training: 0.05987295155548711 | validation: 0.06448644503497872]
	TIME [epoch: 8.3 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0603656457339803		[learning rate: 2.1135e-05]
	Learning Rate: 2.11349e-05
	LOSS [training: 0.0603656457339803 | validation: 0.06705749287456515]
	TIME [epoch: 8.3 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05771791381105888		[learning rate: 2.106e-05]
	Learning Rate: 2.10602e-05
	LOSS [training: 0.05771791381105888 | validation: 0.06156471363935388]
	TIME [epoch: 8.33 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058630687830542996		[learning rate: 2.0986e-05]
	Learning Rate: 2.09857e-05
	LOSS [training: 0.058630687830542996 | validation: 0.06371823567425669]
	TIME [epoch: 8.29 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05980087198304508		[learning rate: 2.0911e-05]
	Learning Rate: 2.09115e-05
	LOSS [training: 0.05980087198304508 | validation: 0.06720496908861805]
	TIME [epoch: 8.3 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06064421605016776		[learning rate: 2.0838e-05]
	Learning Rate: 2.08375e-05
	LOSS [training: 0.06064421605016776 | validation: 0.06588302709388741]
	TIME [epoch: 8.3 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05912731255566584		[learning rate: 2.0764e-05]
	Learning Rate: 2.07638e-05
	LOSS [training: 0.05912731255566584 | validation: 0.06127534520749641]
	TIME [epoch: 8.29 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05785012462035374		[learning rate: 2.069e-05]
	Learning Rate: 2.06904e-05
	LOSS [training: 0.05785012462035374 | validation: 0.06321132301118915]
	TIME [epoch: 8.33 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05872803970226172		[learning rate: 2.0617e-05]
	Learning Rate: 2.06173e-05
	LOSS [training: 0.05872803970226172 | validation: 0.06616444135594138]
	TIME [epoch: 8.31 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059898560509841034		[learning rate: 2.0544e-05]
	Learning Rate: 2.05444e-05
	LOSS [training: 0.059898560509841034 | validation: 0.06477942265630575]
	TIME [epoch: 8.3 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05995709807151237		[learning rate: 2.0472e-05]
	Learning Rate: 2.04717e-05
	LOSS [training: 0.05995709807151237 | validation: 0.063039247550818]
	TIME [epoch: 8.3 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05923624299597419		[learning rate: 2.0399e-05]
	Learning Rate: 2.03993e-05
	LOSS [training: 0.05923624299597419 | validation: 0.06241708226837173]
	TIME [epoch: 8.3 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05900479632415689		[learning rate: 2.0327e-05]
	Learning Rate: 2.03272e-05
	LOSS [training: 0.05900479632415689 | validation: 0.06635619554517133]
	TIME [epoch: 8.31 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05867395370500009		[learning rate: 2.0255e-05]
	Learning Rate: 2.02553e-05
	LOSS [training: 0.05867395370500009 | validation: 0.06333976438187328]
	TIME [epoch: 8.34 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058690609077985276		[learning rate: 2.0184e-05]
	Learning Rate: 2.01837e-05
	LOSS [training: 0.058690609077985276 | validation: 0.06652061698401135]
	TIME [epoch: 8.3 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05992216773129587		[learning rate: 2.0112e-05]
	Learning Rate: 2.01123e-05
	LOSS [training: 0.05992216773129587 | validation: 0.061385502626353956]
	TIME [epoch: 8.29 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05843581639170045		[learning rate: 2.0041e-05]
	Learning Rate: 2.00412e-05
	LOSS [training: 0.05843581639170045 | validation: 0.06406689652786361]
	TIME [epoch: 8.29 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0594018764518451		[learning rate: 1.997e-05]
	Learning Rate: 1.99703e-05
	LOSS [training: 0.0594018764518451 | validation: 0.06733954737251979]
	TIME [epoch: 8.3 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05872890008950855		[learning rate: 1.99e-05]
	Learning Rate: 1.98997e-05
	LOSS [training: 0.05872890008950855 | validation: 0.0667673592563207]
	TIME [epoch: 8.32 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05888848928080752		[learning rate: 1.9829e-05]
	Learning Rate: 1.98293e-05
	LOSS [training: 0.05888848928080752 | validation: 0.06425991692274341]
	TIME [epoch: 8.32 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05902055497181345		[learning rate: 1.9759e-05]
	Learning Rate: 1.97592e-05
	LOSS [training: 0.05902055497181345 | validation: 0.06701277137273781]
	TIME [epoch: 8.29 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059385412177681465		[learning rate: 1.9689e-05]
	Learning Rate: 1.96893e-05
	LOSS [training: 0.059385412177681465 | validation: 0.06563096948886596]
	TIME [epoch: 8.29 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05878307436510195		[learning rate: 1.962e-05]
	Learning Rate: 1.96197e-05
	LOSS [training: 0.05878307436510195 | validation: 0.06522363414092969]
	TIME [epoch: 8.29 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05819681159432474		[learning rate: 1.955e-05]
	Learning Rate: 1.95503e-05
	LOSS [training: 0.05819681159432474 | validation: 0.06728002648285197]
	TIME [epoch: 8.29 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060274094774774786		[learning rate: 1.9481e-05]
	Learning Rate: 1.94812e-05
	LOSS [training: 0.060274094774774786 | validation: 0.06439987057183766]
	TIME [epoch: 8.33 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05823741245472816		[learning rate: 1.9412e-05]
	Learning Rate: 1.94123e-05
	LOSS [training: 0.05823741245472816 | validation: 0.06622885317602617]
	TIME [epoch: 8.31 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0584652130991375		[learning rate: 1.9344e-05]
	Learning Rate: 1.93437e-05
	LOSS [training: 0.0584652130991375 | validation: 0.06407672003270477]
	TIME [epoch: 8.29 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05794210637279118		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 0.05794210637279118 | validation: 0.06350028858464121]
	TIME [epoch: 8.29 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05862183108264567		[learning rate: 1.9207e-05]
	Learning Rate: 1.92071e-05
	LOSS [training: 0.05862183108264567 | validation: 0.06275167509667802]
	TIME [epoch: 8.3 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05934143237710229		[learning rate: 1.9139e-05]
	Learning Rate: 1.91392e-05
	LOSS [training: 0.05934143237710229 | validation: 0.06487568514812554]
	TIME [epoch: 8.3 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05852175113568187		[learning rate: 1.9071e-05]
	Learning Rate: 1.90715e-05
	LOSS [training: 0.05852175113568187 | validation: 0.06567719676846154]
	TIME [epoch: 8.33 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05892019914406763		[learning rate: 1.9004e-05]
	Learning Rate: 1.90041e-05
	LOSS [training: 0.05892019914406763 | validation: 0.060374519144971925]
	TIME [epoch: 8.29 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05986108317348951		[learning rate: 1.8937e-05]
	Learning Rate: 1.89369e-05
	LOSS [training: 0.05986108317348951 | validation: 0.06586261270000394]
	TIME [epoch: 8.28 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059712398758325666		[learning rate: 1.887e-05]
	Learning Rate: 1.88699e-05
	LOSS [training: 0.059712398758325666 | validation: 0.0624018495837248]
	TIME [epoch: 8.29 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05848320644033034		[learning rate: 1.8803e-05]
	Learning Rate: 1.88032e-05
	LOSS [training: 0.05848320644033034 | validation: 0.06579090546420802]
	TIME [epoch: 8.29 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05933828125477737		[learning rate: 1.8737e-05]
	Learning Rate: 1.87367e-05
	LOSS [training: 0.05933828125477737 | validation: 0.06542720461141616]
	TIME [epoch: 8.31 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060756417150657216		[learning rate: 1.867e-05]
	Learning Rate: 1.86704e-05
	LOSS [training: 0.060756417150657216 | validation: 0.06480993947128923]
	TIME [epoch: 8.32 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05987182329233939		[learning rate: 1.8604e-05]
	Learning Rate: 1.86044e-05
	LOSS [training: 0.05987182329233939 | validation: 0.06361220105090934]
	TIME [epoch: 8.29 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058655623685045066		[learning rate: 1.8539e-05]
	Learning Rate: 1.85386e-05
	LOSS [training: 0.058655623685045066 | validation: 0.06283851083856203]
	TIME [epoch: 8.29 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05894600576874829		[learning rate: 1.8473e-05]
	Learning Rate: 1.84731e-05
	LOSS [training: 0.05894600576874829 | validation: 0.06146734811210096]
	TIME [epoch: 8.29 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05987736404374947		[learning rate: 1.8408e-05]
	Learning Rate: 1.84077e-05
	LOSS [training: 0.05987736404374947 | validation: 0.0656302792109359]
	TIME [epoch: 8.3 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058776458432822316		[learning rate: 1.8343e-05]
	Learning Rate: 1.83426e-05
	LOSS [training: 0.058776458432822316 | validation: 0.06416985336957816]
	TIME [epoch: 8.33 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06000069992024798		[learning rate: 1.8278e-05]
	Learning Rate: 1.82778e-05
	LOSS [training: 0.06000069992024798 | validation: 0.064875933662229]
	TIME [epoch: 8.3 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05876330975836877		[learning rate: 1.8213e-05]
	Learning Rate: 1.82131e-05
	LOSS [training: 0.05876330975836877 | validation: 0.06650378593073561]
	TIME [epoch: 8.3 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06062435726094233		[learning rate: 1.8149e-05]
	Learning Rate: 1.81487e-05
	LOSS [training: 0.06062435726094233 | validation: 0.06454617338702895]
	TIME [epoch: 8.29 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05894513585489679		[learning rate: 1.8085e-05]
	Learning Rate: 1.80846e-05
	LOSS [training: 0.05894513585489679 | validation: 0.0679260413455951]
	TIME [epoch: 8.29 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059159155983982276		[learning rate: 1.8021e-05]
	Learning Rate: 1.80206e-05
	LOSS [training: 0.059159155983982276 | validation: 0.06627492916999331]
	TIME [epoch: 8.29 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059686569342233195		[learning rate: 1.7957e-05]
	Learning Rate: 1.79569e-05
	LOSS [training: 0.059686569342233195 | validation: 0.06479605336625649]
	TIME [epoch: 8.33 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06117482456153095		[learning rate: 1.7893e-05]
	Learning Rate: 1.78934e-05
	LOSS [training: 0.06117482456153095 | validation: 0.07006012141739804]
	TIME [epoch: 8.29 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06053567743022646		[learning rate: 1.783e-05]
	Learning Rate: 1.78301e-05
	LOSS [training: 0.06053567743022646 | validation: 0.06359354757918152]
	TIME [epoch: 8.29 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06039297127568145		[learning rate: 1.7767e-05]
	Learning Rate: 1.77671e-05
	LOSS [training: 0.06039297127568145 | validation: 0.06942056312257731]
	TIME [epoch: 8.29 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05968611599799905		[learning rate: 1.7704e-05]
	Learning Rate: 1.77042e-05
	LOSS [training: 0.05968611599799905 | validation: 0.0665394681619556]
	TIME [epoch: 8.29 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05929843994784222		[learning rate: 1.7642e-05]
	Learning Rate: 1.76416e-05
	LOSS [training: 0.05929843994784222 | validation: 0.06661443229160371]
	TIME [epoch: 8.31 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058783249876317875		[learning rate: 1.7579e-05]
	Learning Rate: 1.75792e-05
	LOSS [training: 0.058783249876317875 | validation: 0.06741631942879729]
	TIME [epoch: 8.32 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05948510739431326		[learning rate: 1.7517e-05]
	Learning Rate: 1.75171e-05
	LOSS [training: 0.05948510739431326 | validation: 0.06260872752389819]
	TIME [epoch: 8.3 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06003899589213097		[learning rate: 1.7455e-05]
	Learning Rate: 1.74551e-05
	LOSS [training: 0.06003899589213097 | validation: 0.06435462683230156]
	TIME [epoch: 8.3 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059276670558883485		[learning rate: 1.7393e-05]
	Learning Rate: 1.73934e-05
	LOSS [training: 0.059276670558883485 | validation: 0.06869522621221716]
	TIME [epoch: 8.29 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058246349799589314		[learning rate: 1.7332e-05]
	Learning Rate: 1.73319e-05
	LOSS [training: 0.058246349799589314 | validation: 0.07153367859804752]
	TIME [epoch: 8.29 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058520966474287894		[learning rate: 1.7271e-05]
	Learning Rate: 1.72706e-05
	LOSS [training: 0.058520966474287894 | validation: 0.06747743230253561]
	TIME [epoch: 8.34 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06072137072642042		[learning rate: 1.721e-05]
	Learning Rate: 1.72095e-05
	LOSS [training: 0.06072137072642042 | validation: 0.06388587048280131]
	TIME [epoch: 8.31 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05905616848594533		[learning rate: 1.7149e-05]
	Learning Rate: 1.71487e-05
	LOSS [training: 0.05905616848594533 | validation: 0.06406864364110439]
	TIME [epoch: 8.29 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05905101117235326		[learning rate: 1.7088e-05]
	Learning Rate: 1.7088e-05
	LOSS [training: 0.05905101117235326 | validation: 0.06893595613920171]
	TIME [epoch: 8.29 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058319298379148427		[learning rate: 1.7028e-05]
	Learning Rate: 1.70276e-05
	LOSS [training: 0.058319298379148427 | validation: 0.062175265334378685]
	TIME [epoch: 8.29 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0592442437495182		[learning rate: 1.6967e-05]
	Learning Rate: 1.69674e-05
	LOSS [training: 0.0592442437495182 | validation: 0.06484853116280952]
	TIME [epoch: 8.29 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059251580406159465		[learning rate: 1.6907e-05]
	Learning Rate: 1.69074e-05
	LOSS [training: 0.059251580406159465 | validation: 0.06549851185001417]
	TIME [epoch: 8.33 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06079943779816279		[learning rate: 1.6848e-05]
	Learning Rate: 1.68476e-05
	LOSS [training: 0.06079943779816279 | validation: 0.06865543793225241]
	TIME [epoch: 8.3 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0593014116804606		[learning rate: 1.6788e-05]
	Learning Rate: 1.6788e-05
	LOSS [training: 0.0593014116804606 | validation: 0.06612885816309115]
	TIME [epoch: 8.29 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060122522737629455		[learning rate: 1.6729e-05]
	Learning Rate: 1.67287e-05
	LOSS [training: 0.060122522737629455 | validation: 0.06083578245616873]
	TIME [epoch: 8.29 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05900591286035283		[learning rate: 1.667e-05]
	Learning Rate: 1.66695e-05
	LOSS [training: 0.05900591286035283 | validation: 0.059977487672822384]
	TIME [epoch: 8.28 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0601786237937931		[learning rate: 1.6611e-05]
	Learning Rate: 1.66106e-05
	LOSS [training: 0.0601786237937931 | validation: 0.06340848972219126]
	TIME [epoch: 8.29 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05839386972583566		[learning rate: 1.6552e-05]
	Learning Rate: 1.65518e-05
	LOSS [training: 0.05839386972583566 | validation: 0.0637665764954874]
	TIME [epoch: 8.31 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05841056594217166		[learning rate: 1.6493e-05]
	Learning Rate: 1.64933e-05
	LOSS [training: 0.05841056594217166 | validation: 0.06501991291659474]
	TIME [epoch: 8.28 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060156004825955		[learning rate: 1.6435e-05]
	Learning Rate: 1.6435e-05
	LOSS [training: 0.060156004825955 | validation: 0.06467417965965759]
	TIME [epoch: 8.29 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059472908739511804		[learning rate: 1.6377e-05]
	Learning Rate: 1.63769e-05
	LOSS [training: 0.059472908739511804 | validation: 0.06278523574189386]
	TIME [epoch: 8.3 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05853151325674155		[learning rate: 1.6319e-05]
	Learning Rate: 1.6319e-05
	LOSS [training: 0.05853151325674155 | validation: 0.06538905325504785]
	TIME [epoch: 8.29 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058638509873366496		[learning rate: 1.6261e-05]
	Learning Rate: 1.62612e-05
	LOSS [training: 0.058638509873366496 | validation: 0.06898010083229164]
	TIME [epoch: 8.34 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05831833341436894		[learning rate: 1.6204e-05]
	Learning Rate: 1.62038e-05
	LOSS [training: 0.05831833341436894 | validation: 0.06591744807230127]
	TIME [epoch: 8.3 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0600470546541879		[learning rate: 1.6146e-05]
	Learning Rate: 1.61464e-05
	LOSS [training: 0.0600470546541879 | validation: 0.06532921102181213]
	TIME [epoch: 8.29 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05923119861350517		[learning rate: 1.6089e-05]
	Learning Rate: 1.60894e-05
	LOSS [training: 0.05923119861350517 | validation: 0.06999471377018165]
	TIME [epoch: 8.29 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06143364712588453		[learning rate: 1.6032e-05]
	Learning Rate: 1.60325e-05
	LOSS [training: 0.06143364712588453 | validation: 0.0638957003589968]
	TIME [epoch: 8.29 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059641278872111195		[learning rate: 1.5976e-05]
	Learning Rate: 1.59758e-05
	LOSS [training: 0.059641278872111195 | validation: 0.06568242474852527]
	TIME [epoch: 8.3 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06125574265668197		[learning rate: 1.5919e-05]
	Learning Rate: 1.59193e-05
	LOSS [training: 0.06125574265668197 | validation: 0.0662140819698007]
	TIME [epoch: 8.33 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06018824331121197		[learning rate: 1.5863e-05]
	Learning Rate: 1.5863e-05
	LOSS [training: 0.06018824331121197 | validation: 0.0644348486280401]
	TIME [epoch: 8.29 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06060763884756252		[learning rate: 1.5807e-05]
	Learning Rate: 1.58069e-05
	LOSS [training: 0.06060763884756252 | validation: 0.0664295809837385]
	TIME [epoch: 8.29 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0590983096627295		[learning rate: 1.5751e-05]
	Learning Rate: 1.5751e-05
	LOSS [training: 0.0590983096627295 | validation: 0.06069363658000973]
	TIME [epoch: 8.3 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06104846808183441		[learning rate: 1.5695e-05]
	Learning Rate: 1.56953e-05
	LOSS [training: 0.06104846808183441 | validation: 0.06570076647792013]
	TIME [epoch: 8.29 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05948810354319916		[learning rate: 1.564e-05]
	Learning Rate: 1.56398e-05
	LOSS [training: 0.05948810354319916 | validation: 0.0685299234853915]
	TIME [epoch: 8.32 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06052418031608496		[learning rate: 1.5584e-05]
	Learning Rate: 1.55845e-05
	LOSS [training: 0.06052418031608496 | validation: 0.06757595016581477]
	TIME [epoch: 8.33 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06067091125981812		[learning rate: 1.5529e-05]
	Learning Rate: 1.55294e-05
	LOSS [training: 0.06067091125981812 | validation: 0.06198683703622823]
	TIME [epoch: 8.29 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059966203344210454		[learning rate: 1.5474e-05]
	Learning Rate: 1.54745e-05
	LOSS [training: 0.059966203344210454 | validation: 0.0650810767137571]
	TIME [epoch: 8.29 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05953636782255996		[learning rate: 1.542e-05]
	Learning Rate: 1.54197e-05
	LOSS [training: 0.05953636782255996 | validation: 0.06871227619615342]
	TIME [epoch: 8.29 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06201726715905603		[learning rate: 1.5365e-05]
	Learning Rate: 1.53652e-05
	LOSS [training: 0.06201726715905603 | validation: 0.06808142421271406]
	TIME [epoch: 8.29 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06060758700803322		[learning rate: 1.5311e-05]
	Learning Rate: 1.53109e-05
	LOSS [training: 0.06060758700803322 | validation: 0.06650856664647409]
	TIME [epoch: 8.33 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05939282667807303		[learning rate: 1.5257e-05]
	Learning Rate: 1.52567e-05
	LOSS [training: 0.05939282667807303 | validation: 0.06573010673839969]
	TIME [epoch: 8.29 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060878383524607595		[learning rate: 1.5203e-05]
	Learning Rate: 1.52028e-05
	LOSS [training: 0.060878383524607595 | validation: 0.06632517523076156]
	TIME [epoch: 8.29 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05924798225582265		[learning rate: 1.5149e-05]
	Learning Rate: 1.5149e-05
	LOSS [training: 0.05924798225582265 | validation: 0.06534774457242906]
	TIME [epoch: 8.29 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06073906461170377		[learning rate: 1.5095e-05]
	Learning Rate: 1.50955e-05
	LOSS [training: 0.06073906461170377 | validation: 0.06469599420100926]
	TIME [epoch: 8.29 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059036989922320704		[learning rate: 1.5042e-05]
	Learning Rate: 1.50421e-05
	LOSS [training: 0.059036989922320704 | validation: 0.06707245392557558]
	TIME [epoch: 8.3 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060469957905388845		[learning rate: 1.4989e-05]
	Learning Rate: 1.49889e-05
	LOSS [training: 0.060469957905388845 | validation: 0.06468288919064771]
	TIME [epoch: 8.33 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05924844921420454		[learning rate: 1.4936e-05]
	Learning Rate: 1.49359e-05
	LOSS [training: 0.05924844921420454 | validation: 0.06718110133997489]
	TIME [epoch: 8.29 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06020312784536294		[learning rate: 1.4883e-05]
	Learning Rate: 1.48831e-05
	LOSS [training: 0.06020312784536294 | validation: 0.061383447480329006]
	TIME [epoch: 8.29 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0589223196555817		[learning rate: 1.483e-05]
	Learning Rate: 1.48304e-05
	LOSS [training: 0.0589223196555817 | validation: 0.06281954349694845]
	TIME [epoch: 8.3 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0593148095548046		[learning rate: 1.4778e-05]
	Learning Rate: 1.4778e-05
	LOSS [training: 0.0593148095548046 | validation: 0.06360489447557185]
	TIME [epoch: 8.3 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05919327255940392		[learning rate: 1.4726e-05]
	Learning Rate: 1.47257e-05
	LOSS [training: 0.05919327255940392 | validation: 0.06674987592891385]
	TIME [epoch: 8.32 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06010422808118693		[learning rate: 1.4674e-05]
	Learning Rate: 1.46737e-05
	LOSS [training: 0.06010422808118693 | validation: 0.06669503844236302]
	TIME [epoch: 8.32 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05933331882048505		[learning rate: 1.4622e-05]
	Learning Rate: 1.46218e-05
	LOSS [training: 0.05933331882048505 | validation: 0.07255387260403415]
	TIME [epoch: 8.29 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05882847778440173		[learning rate: 1.457e-05]
	Learning Rate: 1.45701e-05
	LOSS [training: 0.05882847778440173 | validation: 0.06979164854216319]
	TIME [epoch: 8.29 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05997060085339832		[learning rate: 1.4519e-05]
	Learning Rate: 1.45185e-05
	LOSS [training: 0.05997060085339832 | validation: 0.060411896254729855]
	TIME [epoch: 8.3 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06089381618652985		[learning rate: 1.4467e-05]
	Learning Rate: 1.44672e-05
	LOSS [training: 0.06089381618652985 | validation: 0.06479536993181453]
	TIME [epoch: 8.29 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059853951500259704		[learning rate: 1.4416e-05]
	Learning Rate: 1.44161e-05
	LOSS [training: 0.059853951500259704 | validation: 0.06274162894964418]
	TIME [epoch: 8.34 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05868199372886394		[learning rate: 1.4365e-05]
	Learning Rate: 1.43651e-05
	LOSS [training: 0.05868199372886394 | validation: 0.06327430734863973]
	TIME [epoch: 8.3 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059950434185641		[learning rate: 1.4314e-05]
	Learning Rate: 1.43143e-05
	LOSS [training: 0.059950434185641 | validation: 0.0672405720902395]
	TIME [epoch: 8.29 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05960094525643514		[learning rate: 1.4264e-05]
	Learning Rate: 1.42637e-05
	LOSS [training: 0.05960094525643514 | validation: 0.06603782507859987]
	TIME [epoch: 8.29 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059450406235218495		[learning rate: 1.4213e-05]
	Learning Rate: 1.42132e-05
	LOSS [training: 0.059450406235218495 | validation: 0.06176154668125846]
	TIME [epoch: 8.29 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058768352948892835		[learning rate: 1.4163e-05]
	Learning Rate: 1.4163e-05
	LOSS [training: 0.058768352948892835 | validation: 0.06411000585761649]
	TIME [epoch: 8.3 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05885302218661464		[learning rate: 1.4113e-05]
	Learning Rate: 1.41129e-05
	LOSS [training: 0.05885302218661464 | validation: 0.06611492579762362]
	TIME [epoch: 8.33 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05853579627566011		[learning rate: 1.4063e-05]
	Learning Rate: 1.4063e-05
	LOSS [training: 0.05853579627566011 | validation: 0.06777448101847357]
	TIME [epoch: 8.3 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059230624980722305		[learning rate: 1.4013e-05]
	Learning Rate: 1.40132e-05
	LOSS [training: 0.059230624980722305 | validation: 0.06646531568455172]
	TIME [epoch: 8.29 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05837690242939127		[learning rate: 1.3964e-05]
	Learning Rate: 1.39637e-05
	LOSS [training: 0.05837690242939127 | validation: 0.06467683925727834]
	TIME [epoch: 8.29 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058436889224241964		[learning rate: 1.3914e-05]
	Learning Rate: 1.39143e-05
	LOSS [training: 0.058436889224241964 | validation: 0.06074831926761769]
	TIME [epoch: 8.29 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059509406874044435		[learning rate: 1.3865e-05]
	Learning Rate: 1.38651e-05
	LOSS [training: 0.059509406874044435 | validation: 0.06075164856107894]
	TIME [epoch: 8.32 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05808191055667562		[learning rate: 1.3816e-05]
	Learning Rate: 1.38161e-05
	LOSS [training: 0.05808191055667562 | validation: 0.06182179245636239]
	TIME [epoch: 8.31 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059302566713812024		[learning rate: 1.3767e-05]
	Learning Rate: 1.37672e-05
	LOSS [training: 0.059302566713812024 | validation: 0.06582654957820203]
	TIME [epoch: 8.29 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05788660206058162		[learning rate: 1.3719e-05]
	Learning Rate: 1.37185e-05
	LOSS [training: 0.05788660206058162 | validation: 0.06662051325899612]
	TIME [epoch: 8.29 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05980454775764055		[learning rate: 1.367e-05]
	Learning Rate: 1.367e-05
	LOSS [training: 0.05980454775764055 | validation: 0.06410740476708938]
	TIME [epoch: 8.29 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0592726778091609		[learning rate: 1.3622e-05]
	Learning Rate: 1.36217e-05
	LOSS [training: 0.0592726778091609 | validation: 0.0631442534837445]
	TIME [epoch: 8.29 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05900047382800552		[learning rate: 1.3574e-05]
	Learning Rate: 1.35735e-05
	LOSS [training: 0.05900047382800552 | validation: 0.06219328519728552]
	TIME [epoch: 8.33 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05971651137112713		[learning rate: 1.3526e-05]
	Learning Rate: 1.35255e-05
	LOSS [training: 0.05971651137112713 | validation: 0.06173948748727778]
	TIME [epoch: 8.29 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060724217105069915		[learning rate: 1.3478e-05]
	Learning Rate: 1.34777e-05
	LOSS [training: 0.060724217105069915 | validation: 0.07094644262149064]
	TIME [epoch: 8.29 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05838318715013892		[learning rate: 1.343e-05]
	Learning Rate: 1.343e-05
	LOSS [training: 0.05838318715013892 | validation: 0.0639271617570441]
	TIME [epoch: 8.29 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06036793269656063		[learning rate: 1.3383e-05]
	Learning Rate: 1.33825e-05
	LOSS [training: 0.06036793269656063 | validation: 0.06574363157489656]
	TIME [epoch: 8.29 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06059270360546665		[learning rate: 1.3335e-05]
	Learning Rate: 1.33352e-05
	LOSS [training: 0.06059270360546665 | validation: 0.06640885666877594]
	TIME [epoch: 8.31 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0590374880029892		[learning rate: 1.3288e-05]
	Learning Rate: 1.32881e-05
	LOSS [training: 0.0590374880029892 | validation: 0.06829629257661128]
	TIME [epoch: 8.32 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06009001098647496		[learning rate: 1.3241e-05]
	Learning Rate: 1.32411e-05
	LOSS [training: 0.06009001098647496 | validation: 0.06211896522044409]
	TIME [epoch: 8.29 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059519141618979615		[learning rate: 1.3194e-05]
	Learning Rate: 1.31942e-05
	LOSS [training: 0.059519141618979615 | validation: 0.0629631116850467]
	TIME [epoch: 8.29 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05914642788748675		[learning rate: 1.3148e-05]
	Learning Rate: 1.31476e-05
	LOSS [training: 0.05914642788748675 | validation: 0.06204505468616618]
	TIME [epoch: 8.29 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059749327102182365		[learning rate: 1.3101e-05]
	Learning Rate: 1.31011e-05
	LOSS [training: 0.059749327102182365 | validation: 0.06383143544311462]
	TIME [epoch: 8.29 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0596825971979921		[learning rate: 1.3055e-05]
	Learning Rate: 1.30548e-05
	LOSS [training: 0.0596825971979921 | validation: 0.06346381810918454]
	TIME [epoch: 8.32 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059146225236073546		[learning rate: 1.3009e-05]
	Learning Rate: 1.30086e-05
	LOSS [training: 0.059146225236073546 | validation: 0.06451443626485429]
	TIME [epoch: 8.3 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05871702037965544		[learning rate: 1.2963e-05]
	Learning Rate: 1.29626e-05
	LOSS [training: 0.05871702037965544 | validation: 0.06957181530825077]
	TIME [epoch: 8.29 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06120681756431241		[learning rate: 1.2917e-05]
	Learning Rate: 1.29168e-05
	LOSS [training: 0.06120681756431241 | validation: 0.0648751611231407]
	TIME [epoch: 8.29 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05939141234023707		[learning rate: 1.2871e-05]
	Learning Rate: 1.28711e-05
	LOSS [training: 0.05939141234023707 | validation: 0.06142981800547165]
	TIME [epoch: 8.29 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05807246462320796		[learning rate: 1.2826e-05]
	Learning Rate: 1.28256e-05
	LOSS [training: 0.05807246462320796 | validation: 0.06791094038267666]
	TIME [epoch: 8.3 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05982193434850676		[learning rate: 1.278e-05]
	Learning Rate: 1.27802e-05
	LOSS [training: 0.05982193434850676 | validation: 0.06522645662282867]
	TIME [epoch: 8.34 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05916823271917368		[learning rate: 1.2735e-05]
	Learning Rate: 1.2735e-05
	LOSS [training: 0.05916823271917368 | validation: 0.06447048326681348]
	TIME [epoch: 8.29 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05933682629478139		[learning rate: 1.269e-05]
	Learning Rate: 1.269e-05
	LOSS [training: 0.05933682629478139 | validation: 0.06648887992114416]
	TIME [epoch: 8.29 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05840048427569788		[learning rate: 1.2645e-05]
	Learning Rate: 1.26451e-05
	LOSS [training: 0.05840048427569788 | validation: 0.06899512301440511]
	TIME [epoch: 8.29 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05843914627300673		[learning rate: 1.26e-05]
	Learning Rate: 1.26004e-05
	LOSS [training: 0.05843914627300673 | validation: 0.06490350858933577]
	TIME [epoch: 8.29 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05994464069224001		[learning rate: 1.2556e-05]
	Learning Rate: 1.25559e-05
	LOSS [training: 0.05994464069224001 | validation: 0.06732820463552602]
	TIME [epoch: 8.31 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05834122521668891		[learning rate: 1.2511e-05]
	Learning Rate: 1.25115e-05
	LOSS [training: 0.05834122521668891 | validation: 0.0665240706079254]
	TIME [epoch: 8.32 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05872777798121494		[learning rate: 1.2467e-05]
	Learning Rate: 1.24672e-05
	LOSS [training: 0.05872777798121494 | validation: 0.06539229269247375]
	TIME [epoch: 8.29 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05943838214616916		[learning rate: 1.2423e-05]
	Learning Rate: 1.24231e-05
	LOSS [training: 0.05943838214616916 | validation: 0.06289181647814462]
	TIME [epoch: 8.29 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05935214415815522		[learning rate: 1.2379e-05]
	Learning Rate: 1.23792e-05
	LOSS [training: 0.05935214415815522 | validation: 0.0601674497572348]
	TIME [epoch: 8.3 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05979727061204508		[learning rate: 1.2335e-05]
	Learning Rate: 1.23354e-05
	LOSS [training: 0.05979727061204508 | validation: 0.06564837571631818]
	TIME [epoch: 8.29 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05920820309706565		[learning rate: 1.2292e-05]
	Learning Rate: 1.22918e-05
	LOSS [training: 0.05920820309706565 | validation: 0.06307605609202752]
	TIME [epoch: 8.34 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05939566918273233		[learning rate: 1.2248e-05]
	Learning Rate: 1.22483e-05
	LOSS [training: 0.05939566918273233 | validation: 0.06687414274181291]
	TIME [epoch: 8.3 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058028934652842354		[learning rate: 1.2205e-05]
	Learning Rate: 1.2205e-05
	LOSS [training: 0.058028934652842354 | validation: 0.06001200907985134]
	TIME [epoch: 8.29 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05822891664701549		[learning rate: 1.2162e-05]
	Learning Rate: 1.21619e-05
	LOSS [training: 0.05822891664701549 | validation: 0.0616973207729193]
	TIME [epoch: 8.29 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0564026546631054		[learning rate: 1.2119e-05]
	Learning Rate: 1.21189e-05
	LOSS [training: 0.0564026546631054 | validation: 0.06410858624735807]
	TIME [epoch: 8.29 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05736789099766691		[learning rate: 1.2076e-05]
	Learning Rate: 1.2076e-05
	LOSS [training: 0.05736789099766691 | validation: 0.06790508019728136]
	TIME [epoch: 8.3 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05918054430485503		[learning rate: 1.2033e-05]
	Learning Rate: 1.20333e-05
	LOSS [training: 0.05918054430485503 | validation: 0.06266330424865862]
	TIME [epoch: 8.33 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05815636370433107		[learning rate: 1.1991e-05]
	Learning Rate: 1.19907e-05
	LOSS [training: 0.05815636370433107 | validation: 0.06849034981381283]
	TIME [epoch: 8.28 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05977527629659392		[learning rate: 1.1948e-05]
	Learning Rate: 1.19483e-05
	LOSS [training: 0.05977527629659392 | validation: 0.0650173801637839]
	TIME [epoch: 8.27 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058226540550943395		[learning rate: 1.1906e-05]
	Learning Rate: 1.19061e-05
	LOSS [training: 0.058226540550943395 | validation: 0.06442212555245003]
	TIME [epoch: 8.28 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0584521553233028		[learning rate: 1.1864e-05]
	Learning Rate: 1.1864e-05
	LOSS [training: 0.0584521553233028 | validation: 0.06505157981697496]
	TIME [epoch: 8.27 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05821707480936878		[learning rate: 1.1822e-05]
	Learning Rate: 1.1822e-05
	LOSS [training: 0.05821707480936878 | validation: 0.06470618490689789]
	TIME [epoch: 8.28 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0590772542007729		[learning rate: 1.178e-05]
	Learning Rate: 1.17802e-05
	LOSS [training: 0.0590772542007729 | validation: 0.06395465475176393]
	TIME [epoch: 8.27 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05729587925362504		[learning rate: 1.1739e-05]
	Learning Rate: 1.17386e-05
	LOSS [training: 0.05729587925362504 | validation: 0.06475033871891683]
	TIME [epoch: 8.24 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05751446122208522		[learning rate: 1.1697e-05]
	Learning Rate: 1.16971e-05
	LOSS [training: 0.05751446122208522 | validation: 0.06499146205547836]
	TIME [epoch: 8.24 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05922849152888618		[learning rate: 1.1656e-05]
	Learning Rate: 1.16557e-05
	LOSS [training: 0.05922849152888618 | validation: 0.0653525741882336]
	TIME [epoch: 8.22 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058400639796299184		[learning rate: 1.1614e-05]
	Learning Rate: 1.16145e-05
	LOSS [training: 0.058400639796299184 | validation: 0.06465848196828887]
	TIME [epoch: 8.24 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05796565311172018		[learning rate: 1.1573e-05]
	Learning Rate: 1.15734e-05
	LOSS [training: 0.05796565311172018 | validation: 0.062170648148888216]
	TIME [epoch: 8.36 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058624451016452805		[learning rate: 1.1532e-05]
	Learning Rate: 1.15325e-05
	LOSS [training: 0.058624451016452805 | validation: 0.06390800132407431]
	TIME [epoch: 8.31 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058492983417320725		[learning rate: 1.1492e-05]
	Learning Rate: 1.14917e-05
	LOSS [training: 0.058492983417320725 | validation: 0.06274961281112353]
	TIME [epoch: 8.29 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05812206536707967		[learning rate: 1.1451e-05]
	Learning Rate: 1.14511e-05
	LOSS [training: 0.05812206536707967 | validation: 0.06835054506931193]
	TIME [epoch: 8.28 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058653350472822996		[learning rate: 1.1411e-05]
	Learning Rate: 1.14106e-05
	LOSS [training: 0.058653350472822996 | validation: 0.06261407969239523]
	TIME [epoch: 8.27 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059242240265909345		[learning rate: 1.137e-05]
	Learning Rate: 1.13702e-05
	LOSS [training: 0.059242240265909345 | validation: 0.06435446332200034]
	TIME [epoch: 8.28 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05755852549188831		[learning rate: 1.133e-05]
	Learning Rate: 1.133e-05
	LOSS [training: 0.05755852549188831 | validation: 0.061622317825392084]
	TIME [epoch: 8.32 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058245376575813336		[learning rate: 1.129e-05]
	Learning Rate: 1.129e-05
	LOSS [training: 0.058245376575813336 | validation: 0.06706674407376897]
	TIME [epoch: 8.28 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059696569944855976		[learning rate: 1.125e-05]
	Learning Rate: 1.125e-05
	LOSS [training: 0.059696569944855976 | validation: 0.0640269890548843]
	TIME [epoch: 8.28 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05839975011574608		[learning rate: 1.121e-05]
	Learning Rate: 1.12103e-05
	LOSS [training: 0.05839975011574608 | validation: 0.06626593974500747]
	TIME [epoch: 8.28 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058939030263008535		[learning rate: 1.1171e-05]
	Learning Rate: 1.11706e-05
	LOSS [training: 0.058939030263008535 | validation: 0.06936712016547769]
	TIME [epoch: 8.28 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05914933907847511		[learning rate: 1.1131e-05]
	Learning Rate: 1.11311e-05
	LOSS [training: 0.05914933907847511 | validation: 0.06969148030484304]
	TIME [epoch: 8.29 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058326715049803655		[learning rate: 1.1092e-05]
	Learning Rate: 1.10918e-05
	LOSS [training: 0.058326715049803655 | validation: 0.061940086865982576]
	TIME [epoch: 8.3 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05846824384177571		[learning rate: 1.1053e-05]
	Learning Rate: 1.10525e-05
	LOSS [training: 0.05846824384177571 | validation: 0.06400796774392159]
	TIME [epoch: 8.27 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060184801419906064		[learning rate: 1.1013e-05]
	Learning Rate: 1.10134e-05
	LOSS [training: 0.060184801419906064 | validation: 0.06383770150377427]
	TIME [epoch: 8.27 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058337984877770484		[learning rate: 1.0975e-05]
	Learning Rate: 1.09745e-05
	LOSS [training: 0.058337984877770484 | validation: 0.06305097013274102]
	TIME [epoch: 8.27 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05860439690029924		[learning rate: 1.0936e-05]
	Learning Rate: 1.09357e-05
	LOSS [training: 0.05860439690029924 | validation: 0.06635534459397079]
	TIME [epoch: 8.28 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057833381754072696		[learning rate: 1.0897e-05]
	Learning Rate: 1.0897e-05
	LOSS [training: 0.057833381754072696 | validation: 0.06680985993220924]
	TIME [epoch: 8.32 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058947182978187226		[learning rate: 1.0858e-05]
	Learning Rate: 1.08585e-05
	LOSS [training: 0.058947182978187226 | validation: 0.06490403725603164]
	TIME [epoch: 8.28 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05732468973265896		[learning rate: 1.082e-05]
	Learning Rate: 1.08201e-05
	LOSS [training: 0.05732468973265896 | validation: 0.06809813204216819]
	TIME [epoch: 8.27 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05890655354601844		[learning rate: 1.0782e-05]
	Learning Rate: 1.07818e-05
	LOSS [training: 0.05890655354601844 | validation: 0.061628981002313774]
	TIME [epoch: 8.28 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05811318466898817		[learning rate: 1.0744e-05]
	Learning Rate: 1.07437e-05
	LOSS [training: 0.05811318466898817 | validation: 0.0655529044956282]
	TIME [epoch: 8.27 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05777129613936628		[learning rate: 1.0706e-05]
	Learning Rate: 1.07057e-05
	LOSS [training: 0.05777129613936628 | validation: 0.0635249871783234]
	TIME [epoch: 8.27 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057273904141688134		[learning rate: 1.0668e-05]
	Learning Rate: 1.06679e-05
	LOSS [training: 0.057273904141688134 | validation: 0.06306857100930865]
	TIME [epoch: 8.32 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05838992019871993		[learning rate: 1.063e-05]
	Learning Rate: 1.06301e-05
	LOSS [training: 0.05838992019871993 | validation: 0.06601635352747076]
	TIME [epoch: 8.28 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0576775106718444		[learning rate: 1.0593e-05]
	Learning Rate: 1.05925e-05
	LOSS [training: 0.0576775106718444 | validation: 0.06909650520509039]
	TIME [epoch: 8.27 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05938528671720257		[learning rate: 1.0555e-05]
	Learning Rate: 1.05551e-05
	LOSS [training: 0.05938528671720257 | validation: 0.06466486188681993]
	TIME [epoch: 8.27 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058207962851840184		[learning rate: 1.0518e-05]
	Learning Rate: 1.05178e-05
	LOSS [training: 0.058207962851840184 | validation: 0.06337012464122233]
	TIME [epoch: 8.27 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058765367862856355		[learning rate: 1.0481e-05]
	Learning Rate: 1.04806e-05
	LOSS [training: 0.058765367862856355 | validation: 0.06615517472877223]
	TIME [epoch: 8.29 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05781285783696305		[learning rate: 1.0444e-05]
	Learning Rate: 1.04435e-05
	LOSS [training: 0.05781285783696305 | validation: 0.06715452366937077]
	TIME [epoch: 8.3 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05922699666225231		[learning rate: 1.0407e-05]
	Learning Rate: 1.04066e-05
	LOSS [training: 0.05922699666225231 | validation: 0.06570707564541287]
	TIME [epoch: 8.28 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058666914857633576		[learning rate: 1.037e-05]
	Learning Rate: 1.03698e-05
	LOSS [training: 0.058666914857633576 | validation: 0.06667998342749475]
	TIME [epoch: 8.28 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057890569791089086		[learning rate: 1.0333e-05]
	Learning Rate: 1.03331e-05
	LOSS [training: 0.057890569791089086 | validation: 0.06479838300597811]
	TIME [epoch: 8.28 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0578398518412991		[learning rate: 1.0297e-05]
	Learning Rate: 1.02966e-05
	LOSS [training: 0.0578398518412991 | validation: 0.06854782188297148]
	TIME [epoch: 8.27 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057888557451231004		[learning rate: 1.026e-05]
	Learning Rate: 1.02602e-05
	LOSS [training: 0.057888557451231004 | validation: 0.06527185434676148]
	TIME [epoch: 8.31 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0590689305197298		[learning rate: 1.0224e-05]
	Learning Rate: 1.02239e-05
	LOSS [training: 0.0590689305197298 | validation: 0.06408058802736688]
	TIME [epoch: 8.28 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06023801831006144		[learning rate: 1.0188e-05]
	Learning Rate: 1.01877e-05
	LOSS [training: 0.06023801831006144 | validation: 0.0643226355224911]
	TIME [epoch: 8.28 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05946745657838606		[learning rate: 1.0152e-05]
	Learning Rate: 1.01517e-05
	LOSS [training: 0.05946745657838606 | validation: 0.06949456910236444]
	TIME [epoch: 8.27 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05826507689319042		[learning rate: 1.0116e-05]
	Learning Rate: 1.01158e-05
	LOSS [training: 0.05826507689319042 | validation: 0.06279465033877826]
	TIME [epoch: 8.27 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059187828979424636		[learning rate: 1.008e-05]
	Learning Rate: 1.008e-05
	LOSS [training: 0.059187828979424636 | validation: 0.06424155914426723]
	TIME [epoch: 8.28 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059092928913908147		[learning rate: 1.0044e-05]
	Learning Rate: 1.00444e-05
	LOSS [training: 0.059092928913908147 | validation: 0.06123435852758524]
	TIME [epoch: 8.31 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05909079418734037		[learning rate: 1.0009e-05]
	Learning Rate: 1.00089e-05
	LOSS [training: 0.05909079418734037 | validation: 0.06319469480314308]
	TIME [epoch: 8.27 sec]
Finished training in 17883.091 seconds.
