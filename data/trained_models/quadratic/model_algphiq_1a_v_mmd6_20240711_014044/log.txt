Args:
Namespace(name='model_algphiq_1a_v_mmd6', outdir='out/model_training/model_algphiq_1a_v_mmd6', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='quadratic', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='constant', dt_schedule_bounds=[0], dt_schedule_scales=[1.0], signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[10.0], optimizer='rms', momentum=0.0, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1635127726

Training model...

Saving initial model state to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6258637179495141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6258637179495141 | validation: 0.6577670135503393]
	TIME [epoch: 95.1 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6146032849761774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6146032849761774 | validation: 0.6490914482432445]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_2.pth
	Model improved!!!
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6059785895839599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6059785895839599 | validation: 0.6413581447632584]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_3.pth
	Model improved!!!
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5976571970955088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5976571970955088 | validation: 0.6339273650804702]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_4.pth
	Model improved!!!
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5891802215033064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5891802215033064 | validation: 0.6261483291298647]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_5.pth
	Model improved!!!
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5801237613054859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5801237613054859 | validation: 0.6179905011224133]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_6.pth
	Model improved!!!
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5705527504362733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5705527504362733 | validation: 0.6091579471371351]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_7.pth
	Model improved!!!
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5604214858550345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5604214858550345 | validation: 0.5999467753213625]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_8.pth
	Model improved!!!
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5499120847264436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5499120847264436 | validation: 0.5905636712259998]
	TIME [epoch: 4.24 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_9.pth
	Model improved!!!
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5392878162999991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5392878162999991 | validation: 0.5813108959345242]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_10.pth
	Model improved!!!
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5288240877818059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5288240877818059 | validation: 0.5726122646980677]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_11.pth
	Model improved!!!
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 0.518596773932823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.518596773932823 | validation: 0.5642259883810075]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_12.pth
	Model improved!!!
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5079435916669239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5079435916669239 | validation: 0.5549420631887481]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_13.pth
	Model improved!!!
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 0.49569048928066145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49569048928066145 | validation: 0.5438750209818703]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_14.pth
	Model improved!!!
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4803324551359958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4803324551359958 | validation: 0.5295300336823509]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_15.pth
	Model improved!!!
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 0.46026657033876794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46026657033876794 | validation: 0.5099879820884057]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_16.pth
	Model improved!!!
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43264778885471505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43264778885471505 | validation: 0.48178028131329154]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_17.pth
	Model improved!!!
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 0.39295815441068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39295815441068 | validation: 0.4384232531591022]
	TIME [epoch: 4.23 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_18.pth
	Model improved!!!
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3355974471590008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3355974471590008 | validation: 0.3716804515535339]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_19.pth
	Model improved!!!
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2706311902189325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2706311902189325 | validation: 0.3055808057465159]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_20.pth
	Model improved!!!
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22666838581289736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22666838581289736 | validation: 0.2523288453476423]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_21.pth
	Model improved!!!
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19322852109653993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19322852109653993 | validation: 0.207671148060873]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_22.pth
	Model improved!!!
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16550486632451933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16550486632451933 | validation: 0.18132570400667625]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_23.pth
	Model improved!!!
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1397244065946855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1397244065946855 | validation: 0.14483684345456677]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_24.pth
	Model improved!!!
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 0.11645210063728542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11645210063728542 | validation: 0.1153256988693306]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_25.pth
	Model improved!!!
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 0.09794850767885699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09794850767885699 | validation: 0.09768108842845294]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_26.pth
	Model improved!!!
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07809924582862045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07809924582862045 | validation: 0.0791538038284225]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_27.pth
	Model improved!!!
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06220428138439607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06220428138439607 | validation: 0.05552532476971618]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_28.pth
	Model improved!!!
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05098616632400026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05098616632400026 | validation: 0.042372649444823246]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_29.pth
	Model improved!!!
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 0.039316306443161586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039316306443161586 | validation: 0.03306382270326658]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_30.pth
	Model improved!!!
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 0.027883956214235898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027883956214235898 | validation: 0.023716893140477084]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_31.pth
	Model improved!!!
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02298021472586599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02298021472586599 | validation: 0.023422671875350513]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_32.pth
	Model improved!!!
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 0.017587585752967875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017587585752967875 | validation: 0.013746724039700658]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_33.pth
	Model improved!!!
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 0.010702733283279628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010702733283279628 | validation: 0.011017616614748794]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_34.pth
	Model improved!!!
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007669161745206469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.007669161745206469 | validation: 0.005789265930265213]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_35.pth
	Model improved!!!
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004403234961879518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004403234961879518 | validation: 0.005563305671092526]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_36.pth
	Model improved!!!
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005031769216415372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.005031769216415372 | validation: 0.0072654935714222685]
	TIME [epoch: 4.26 sec]
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004160350568141076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004160350568141076 | validation: 0.0021255061943072752]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_38.pth
	Model improved!!!
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016194817092476809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0016194817092476809 | validation: 0.0015138612486795345]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_39.pth
	Model improved!!!
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018668402440678807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0018668402440678807 | validation: 0.00517185754443788]
	TIME [epoch: 4.19 sec]
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0039239804289718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0039239804289718 | validation: 0.0030769683428538935]
	TIME [epoch: 4.19 sec]
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0023569160463498724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0023569160463498724 | validation: 0.0024810523921336455]
	TIME [epoch: 4.19 sec]
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0022934129312718117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0022934129312718117 | validation: 0.0019194276848755016]
	TIME [epoch: 4.19 sec]
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021697913033754276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0021697913033754276 | validation: 0.0028711679579716785]
	TIME [epoch: 4.19 sec]
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0023513573475083295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0023513573475083295 | validation: 0.0031253459223629476]
	TIME [epoch: 4.19 sec]
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003053403142653675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.003053403142653675 | validation: 0.0021078117218526664]
	TIME [epoch: 4.19 sec]
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018649156343368762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0018649156343368762 | validation: 0.002559761088113603]
	TIME [epoch: 4.2 sec]
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002733753123946036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002733753123946036 | validation: 0.002058039100209447]
	TIME [epoch: 4.21 sec]
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002284079953226956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002284079953226956 | validation: 0.0030100701127218367]
	TIME [epoch: 4.19 sec]
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002508324319659523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002508324319659523 | validation: 0.0033981875416213337]
	TIME [epoch: 4.19 sec]
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002594643333538384		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 0.002594643333538384 | validation: 0.0033464167999536255]
	TIME [epoch: 4.18 sec]
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00241650357888177		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 0.00241650357888177 | validation: 0.002014509947010793]
	TIME [epoch: 4.19 sec]
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021824267749118663		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 0.0021824267749118663 | validation: 0.0020420179932520576]
	TIME [epoch: 4.19 sec]
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002114446624893521		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 0.002114446624893521 | validation: 0.002490468462683341]
	TIME [epoch: 4.19 sec]
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021269336611702988		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 0.0021269336611702988 | validation: 0.0017397154936907725]
	TIME [epoch: 4.19 sec]
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018533802682357161		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 0.0018533802682357161 | validation: 0.0014303789922714316]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_56.pth
	Model improved!!!
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001504259837124693		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 0.001504259837124693 | validation: 0.00277071800742617]
	TIME [epoch: 4.2 sec]
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0026828002746011824		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 0.0026828002746011824 | validation: 0.0020328486875064973]
	TIME [epoch: 4.21 sec]
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017849674178532218		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 0.0017849674178532218 | validation: 0.001342280834580589]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_59.pth
	Model improved!!!
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012255901757563421		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.0012255901757563421 | validation: 0.0013376864048710076]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_60.pth
	Model improved!!!
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020102717436419892		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 0.0020102717436419892 | validation: 0.0017926217572942962]
	TIME [epoch: 4.19 sec]
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002021178099701744		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 0.002021178099701744 | validation: 0.001537170443617947]
	TIME [epoch: 4.18 sec]
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014181833553191347		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 0.0014181833553191347 | validation: 0.0011793278540684743]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_63.pth
	Model improved!!!
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0015442828307699948		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 0.0015442828307699948 | validation: 0.0011774825378921762]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_64.pth
	Model improved!!!
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001467638543145355		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 0.001467638543145355 | validation: 0.001979566397999537]
	TIME [epoch: 4.19 sec]
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020714805664611502		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 0.0020714805664611502 | validation: 0.001151549052696893]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_66.pth
	Model improved!!!
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010964082725183105		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 0.0010964082725183105 | validation: 0.0011652569801447231]
	TIME [epoch: 4.2 sec]
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011405969048248325		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 0.0011405969048248325 | validation: 0.0011796676918522553]
	TIME [epoch: 4.21 sec]
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001600681305174161		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 0.001600681305174161 | validation: 0.0016875356196179743]
	TIME [epoch: 4.22 sec]
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001493752509743637		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 0.001493752509743637 | validation: 0.0010462802493780721]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_70.pth
	Model improved!!!
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009121039942768927		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 0.0009121039942768927 | validation: 0.0008563120355070833]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_71.pth
	Model improved!!!
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011570239055424287		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 0.0011570239055424287 | validation: 0.002215909149281203]
	TIME [epoch: 4.2 sec]
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018661433727520035		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 0.0018661433727520035 | validation: 0.0008255822868542689]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_73.pth
	Model improved!!!
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006740012826593031		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 0.0006740012826593031 | validation: 0.0006406458420094717]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_74.pth
	Model improved!!!
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008469826978760233		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.0008469826978760233 | validation: 0.00161604643748353]
	TIME [epoch: 4.19 sec]
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017052936697743246		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 0.0017052936697743246 | validation: 0.0011038737550204114]
	TIME [epoch: 4.19 sec]
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009839691137943636		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 0.0009839691137943636 | validation: 0.0007319724973269043]
	TIME [epoch: 4.18 sec]
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006786554545632671		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 0.0006786554545632671 | validation: 0.00089383519770459]
	TIME [epoch: 4.19 sec]
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001094977484564086		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 0.001094977484564086 | validation: 0.0012485432633325467]
	TIME [epoch: 4.22 sec]
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012209701725918252		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 0.0012209701725918252 | validation: 0.0009191341425513318]
	TIME [epoch: 4.19 sec]
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008984054351902221		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 0.0008984054351902221 | validation: 0.0006008083223672518]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_81.pth
	Model improved!!!
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008168514873837258		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 0.0008168514873837258 | validation: 0.0008520811649434593]
	TIME [epoch: 4.2 sec]
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008575936970204414		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 0.0008575936970204414 | validation: 0.0010874615215738636]
	TIME [epoch: 4.19 sec]
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001139056995061904		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 0.001139056995061904 | validation: 0.0006670205183935195]
	TIME [epoch: 4.19 sec]
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007907143765088786		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 0.0007907143765088786 | validation: 0.0008153853090093104]
	TIME [epoch: 4.2 sec]
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00082191063718036		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 0.00082191063718036 | validation: 0.0006396752931509737]
	TIME [epoch: 4.19 sec]
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007406715574414934		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 0.0007406715574414934 | validation: 0.0008040176032326076]
	TIME [epoch: 4.19 sec]
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009230329665398413		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 0.0009230329665398413 | validation: 0.0006705731834019133]
	TIME [epoch: 4.19 sec]
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006976848709930859		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 0.0006976848709930859 | validation: 0.000622831901160223]
	TIME [epoch: 4.21 sec]
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006980200615175742		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.0006980200615175742 | validation: 0.0007521937505012713]
	TIME [epoch: 4.23 sec]
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000841130243448158		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 0.000841130243448158 | validation: 0.0008486295781330737]
	TIME [epoch: 4.2 sec]
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007954542128185164		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 0.0007954542128185164 | validation: 0.0005028776576820508]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_92.pth
	Model improved!!!
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00042813615193781207		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 0.00042813615193781207 | validation: 0.000662104749565698]
	TIME [epoch: 4.19 sec]
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008533117437453553		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 0.0008533117437453553 | validation: 0.0008925915970698037]
	TIME [epoch: 4.18 sec]
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006289901860663581		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 0.0006289901860663581 | validation: 0.0005843801777931929]
	TIME [epoch: 4.18 sec]
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005930190980479374		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 0.0005930190980479374 | validation: 0.0006471544220090965]
	TIME [epoch: 4.19 sec]
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006379574199125128		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 0.0006379574199125128 | validation: 0.0012424292360809805]
	TIME [epoch: 4.18 sec]
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008412775289427848		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 0.0008412775289427848 | validation: 0.00035544515116833053]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_98.pth
	Model improved!!!
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004720271168959251		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 0.0004720271168959251 | validation: 0.0004903070043124143]
	TIME [epoch: 4.2 sec]
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006417534463908418		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 0.0006417534463908418 | validation: 0.0008832289928190637]
	TIME [epoch: 4.2 sec]
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000785753553263477		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 0.000785753553263477 | validation: 0.00036601845806912925]
	TIME [epoch: 4.21 sec]
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003314447023523978		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 0.0003314447023523978 | validation: 0.0002927887005138288]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_102.pth
	Model improved!!!
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004761160281588849		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 0.0004761160281588849 | validation: 0.0005335151459885921]
	TIME [epoch: 4.18 sec]
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006376067937516746		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 0.0006376067937516746 | validation: 0.0006199405529280005]
	TIME [epoch: 4.18 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005664262695775885		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.0005664262695775885 | validation: 0.00039956588155869177]
	TIME [epoch: 4.19 sec]
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00044918270419963546		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 0.00044918270419963546 | validation: 0.0005147672344247105]
	TIME [epoch: 4.18 sec]
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005268032713986899		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 0.0005268032713986899 | validation: 0.0005711492689449904]
	TIME [epoch: 4.18 sec]
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005160336058926122		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 0.0005160336058926122 | validation: 0.0003769870014812233]
	TIME [epoch: 4.18 sec]
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004193345680739213		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: 0.0004193345680739213 | validation: 0.00047910663647835535]
	TIME [epoch: 4.18 sec]
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005172545929993701		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: 0.0005172545929993701 | validation: 0.0006151627312878068]
	TIME [epoch: 4.17 sec]
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004374927307895296		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: 0.0004374927307895296 | validation: 0.00027834045697146026]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_111.pth
	Model improved!!!
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003642501173467516		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: 0.0003642501173467516 | validation: 0.0003732997888637997]
	TIME [epoch: 4.21 sec]
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005214657441243814		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: 0.0005214657441243814 | validation: 0.000566803203913127]
	TIME [epoch: 4.18 sec]
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00042190620257852063		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: 0.00042190620257852063 | validation: 0.00023459586567038193]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_114.pth
	Model improved!!!
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00032130856578875793		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: 0.00032130856578875793 | validation: 0.0003004387299801179]
	TIME [epoch: 4.2 sec]
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00031629785901392705		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: 0.00031629785901392705 | validation: 0.0004850536437749078]
	TIME [epoch: 4.19 sec]
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005683863129037533		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: 0.0005683863129037533 | validation: 0.00030971807910322793]
	TIME [epoch: 4.19 sec]
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002893213152225571		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: 0.0002893213152225571 | validation: 0.00022977472286573854]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_118.pth
	Model improved!!!
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003286874341563579		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: 0.0003286874341563579 | validation: 0.00047697895895874236]
	TIME [epoch: 4.2 sec]
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004142681997705058		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.0004142681997705058 | validation: 0.00040704301930673004]
	TIME [epoch: 4.19 sec]
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00031307931434760065		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: 0.00031307931434760065 | validation: 0.00024368240860494605]
	TIME [epoch: 4.2 sec]
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00028949537508434175		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: 0.00028949537508434175 | validation: 0.00040866528241071775]
	TIME [epoch: 4.24 sec]
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00040770845177724437		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: 0.00040770845177724437 | validation: 0.00034198614590134045]
	TIME [epoch: 4.2 sec]
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00031997503277586184		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: 0.00031997503277586184 | validation: 0.00014856394945615014]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_124.pth
	Model improved!!!
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00022536687703256366		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: 0.00022536687703256366 | validation: 0.00036707019034909716]
	TIME [epoch: 4.2 sec]
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00039205398142345385		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: 0.00039205398142345385 | validation: 0.0002309419738518239]
	TIME [epoch: 4.21 sec]
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00025426211867550584		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: 0.00025426211867550584 | validation: 0.00024972171671564425]
	TIME [epoch: 4.2 sec]
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002971757030274874		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: 0.0002971757030274874 | validation: 0.00030307792054018234]
	TIME [epoch: 4.23 sec]
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003498461354387671		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: 0.0003498461354387671 | validation: 0.00018246521448156728]
	TIME [epoch: 4.23 sec]
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00020869389411477368		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: 0.00020869389411477368 | validation: 0.0001604822149549361]
	TIME [epoch: 4.21 sec]
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00026901464922280083		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: 0.00026901464922280083 | validation: 0.00039335163156775763]
	TIME [epoch: 4.22 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00031259039629097385		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: 0.00031259039629097385 | validation: 0.0002167959812428211]
	TIME [epoch: 4.22 sec]
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00020476200218355842		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: 0.00020476200218355842 | validation: 0.00017203670196274713]
	TIME [epoch: 4.24 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001863188013316869		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: 0.0001863188013316869 | validation: 0.00028302927361113415]
	TIME [epoch: 4.22 sec]
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00027749946689982323		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 0.00027749946689982323 | validation: 0.00027434365349982026]
	TIME [epoch: 4.21 sec]
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00026167519285525244		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: 0.00026167519285525244 | validation: 0.00019891704008070032]
	TIME [epoch: 4.21 sec]
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00024779066788637106		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: 0.00024779066788637106 | validation: 0.0001642960482911533]
	TIME [epoch: 4.21 sec]
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00022363173695553307		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: 0.00022363173695553307 | validation: 0.00030614631118279734]
	TIME [epoch: 4.2 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002958393109599119		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: 0.0002958393109599119 | validation: 0.00016291623939159507]
	TIME [epoch: 4.19 sec]
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001539204901887894		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: 0.0001539204901887894 | validation: 0.00011437066882688773]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_140.pth
	Model improved!!!
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00013877749927350945		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: 0.00013877749927350945 | validation: 0.00015702742954830696]
	TIME [epoch: 4.19 sec]
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00022118183720411933		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: 0.00022118183720411933 | validation: 0.0003008186542330587]
	TIME [epoch: 4.22 sec]
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00023687263513202128		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: 0.00023687263513202128 | validation: 0.00022003749411366046]
	TIME [epoch: 4.2 sec]
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00016814475439932673		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: 0.00016814475439932673 | validation: 0.00011831612261645796]
	TIME [epoch: 4.24 sec]
EPOCH 145/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00016773049611481085		[learning rate: 0.0023352]
	Learning Rate: 0.00233525
	LOSS [training: 0.00016773049611481085 | validation: 0.0002618891213043695]
	TIME [epoch: 4.23 sec]
EPOCH 146/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00021034879529688045		[learning rate: 0.0022997]
	Learning Rate: 0.00229968
	LOSS [training: 0.00021034879529688045 | validation: 0.00022697773247429964]
	TIME [epoch: 4.19 sec]
EPOCH 147/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00019955367557450155		[learning rate: 0.0022646]
	Learning Rate: 0.00226464
	LOSS [training: 0.00019955367557450155 | validation: 0.00010944378509713658]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_147.pth
	Model improved!!!
EPOCH 148/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00015462071028486267		[learning rate: 0.0022301]
	Learning Rate: 0.00223015
	LOSS [training: 0.00015462071028486267 | validation: 0.00010585828172044964]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_148.pth
	Model improved!!!
EPOCH 149/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00013740596487376645		[learning rate: 0.0021962]
	Learning Rate: 0.00219617
	LOSS [training: 0.00013740596487376645 | validation: 0.00016700084366568646]
	TIME [epoch: 4.18 sec]
EPOCH 150/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00022878250286359757		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 0.00022878250286359757 | validation: 0.00024520484529994935]
	TIME [epoch: 4.18 sec]
EPOCH 151/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00018430873552663618		[learning rate: 0.0021298]
	Learning Rate: 0.00212977
	LOSS [training: 0.00018430873552663618 | validation: 7.482753587893055e-05]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_151.pth
	Model improved!!!
EPOCH 152/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011112232312546089		[learning rate: 0.0020973]
	Learning Rate: 0.00209733
	LOSS [training: 0.00011112232312546089 | validation: 0.00010871152502334481]
	TIME [epoch: 4.19 sec]
EPOCH 153/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00017081847476451873		[learning rate: 0.0020654]
	Learning Rate: 0.00206538
	LOSS [training: 0.00017081847476451873 | validation: 0.00022336741634952742]
	TIME [epoch: 4.18 sec]
EPOCH 154/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00016608355692180845		[learning rate: 0.0020339]
	Learning Rate: 0.00203392
	LOSS [training: 0.00016608355692180845 | validation: 0.0001253892267537451]
	TIME [epoch: 4.2 sec]
EPOCH 155/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00014645577525132126		[learning rate: 0.0020029]
	Learning Rate: 0.00200293
	LOSS [training: 0.00014645577525132126 | validation: 0.00015875599051687472]
	TIME [epoch: 4.23 sec]
EPOCH 156/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00017083875439680737		[learning rate: 0.0019724]
	Learning Rate: 0.00197242
	LOSS [training: 0.00017083875439680737 | validation: 0.00012893731851264055]
	TIME [epoch: 4.19 sec]
EPOCH 157/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00012641454387790063		[learning rate: 0.0019424]
	Learning Rate: 0.00194238
	LOSS [training: 0.00012641454387790063 | validation: 0.00014045285884609426]
	TIME [epoch: 4.2 sec]
EPOCH 158/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001447103630913712		[learning rate: 0.0019128]
	Learning Rate: 0.00191279
	LOSS [training: 0.0001447103630913712 | validation: 0.00013047787294385738]
	TIME [epoch: 4.2 sec]
EPOCH 159/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001320307221205871		[learning rate: 0.0018836]
	Learning Rate: 0.00188365
	LOSS [training: 0.0001320307221205871 | validation: 7.301148302382953e-05]
	TIME [epoch: 4.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_159.pth
	Model improved!!!
EPOCH 160/500:
	Training over batches...
		[batch 4/4] avg loss: 9.783414356518928e-05		[learning rate: 0.001855]
	Learning Rate: 0.00185495
	LOSS [training: 9.783414356518928e-05 | validation: 6.396939115892874e-05]
	TIME [epoch: 4.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_160.pth
	Model improved!!!
EPOCH 161/500:
	Training over batches...
		[batch 4/4] avg loss: 9.088624353120167e-05		[learning rate: 0.0018267]
	Learning Rate: 0.0018267
	LOSS [training: 9.088624353120167e-05 | validation: 9.099552943627387e-05]
	TIME [epoch: 4.19 sec]
EPOCH 162/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00024491740484730264		[learning rate: 0.0017989]
	Learning Rate: 0.00179887
	LOSS [training: 0.00024491740484730264 | validation: 0.00012589552134512183]
	TIME [epoch: 4.19 sec]
EPOCH 163/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011359764366878566		[learning rate: 0.0017715]
	Learning Rate: 0.00177147
	LOSS [training: 0.00011359764366878566 | validation: 3.492703018795962e-05]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_163.pth
	Model improved!!!
EPOCH 164/500:
	Training over batches...
		[batch 4/4] avg loss: 7.768498365027255e-05		[learning rate: 0.0017445]
	Learning Rate: 0.00174448
	LOSS [training: 7.768498365027255e-05 | validation: 3.467125015352224e-05]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_164.pth
	Model improved!!!
EPOCH 165/500:
	Training over batches...
		[batch 4/4] avg loss: 6.850903752062432e-05		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: 6.850903752062432e-05 | validation: 4.241049619688142e-05]
	TIME [epoch: 4.21 sec]
EPOCH 166/500:
	Training over batches...
		[batch 4/4] avg loss: 6.144051109055037e-05		[learning rate: 0.0016917]
	Learning Rate: 0.00169174
	LOSS [training: 6.144051109055037e-05 | validation: 4.753491089682482e-05]
	TIME [epoch: 4.19 sec]
EPOCH 167/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001524325548478288		[learning rate: 0.001666]
	Learning Rate: 0.00166597
	LOSS [training: 0.0001524325548478288 | validation: 0.00036810745590329066]
	TIME [epoch: 4.19 sec]
EPOCH 168/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00021017462428208733		[learning rate: 0.0016406]
	Learning Rate: 0.00164059
	LOSS [training: 0.00021017462428208733 | validation: 7.456060214946136e-05]
	TIME [epoch: 4.19 sec]
EPOCH 169/500:
	Training over batches...
		[batch 4/4] avg loss: 8.801150284818493e-05		[learning rate: 0.0016156]
	Learning Rate: 0.0016156
	LOSS [training: 8.801150284818493e-05 | validation: 8.610440428400313e-05]
	TIME [epoch: 4.19 sec]
EPOCH 170/500:
	Training over batches...
		[batch 4/4] avg loss: 7.996885648902685e-05		[learning rate: 0.001591]
	Learning Rate: 0.00159099
	LOSS [training: 7.996885648902685e-05 | validation: 7.955649896920392e-05]
	TIME [epoch: 4.18 sec]
EPOCH 171/500:
	Training over batches...
		[batch 4/4] avg loss: 8.257137215526767e-05		[learning rate: 0.0015668]
	Learning Rate: 0.00156675
	LOSS [training: 8.257137215526767e-05 | validation: 0.0001054187659616168]
	TIME [epoch: 4.18 sec]
EPOCH 172/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00012487882836623943		[learning rate: 0.0015429]
	Learning Rate: 0.00154288
	LOSS [training: 0.00012487882836623943 | validation: 0.00010041083724633771]
	TIME [epoch: 4.18 sec]
EPOCH 173/500:
	Training over batches...
		[batch 4/4] avg loss: 8.190482313365733e-05		[learning rate: 0.0015194]
	Learning Rate: 0.00151938
	LOSS [training: 8.190482313365733e-05 | validation: 0.00012913978782358118]
	TIME [epoch: 4.17 sec]
EPOCH 174/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011207982019620866		[learning rate: 0.0014962]
	Learning Rate: 0.00149624
	LOSS [training: 0.00011207982019620866 | validation: 5.062497293874268e-05]
	TIME [epoch: 4.22 sec]
EPOCH 175/500:
	Training over batches...
		[batch 4/4] avg loss: 5.231096529652813e-05		[learning rate: 0.0014734]
	Learning Rate: 0.00147344
	LOSS [training: 5.231096529652813e-05 | validation: 5.1260052394345834e-05]
	TIME [epoch: 4.2 sec]
EPOCH 176/500:
	Training over batches...
		[batch 4/4] avg loss: 6.499091356701792e-05		[learning rate: 0.001451]
	Learning Rate: 0.001451
	LOSS [training: 6.499091356701792e-05 | validation: 0.0001517217292043551]
	TIME [epoch: 4.2 sec]
EPOCH 177/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001456167727523706		[learning rate: 0.0014289]
	Learning Rate: 0.00142889
	LOSS [training: 0.0001456167727523706 | validation: 8.355215966298734e-05]
	TIME [epoch: 4.18 sec]
EPOCH 178/500:
	Training over batches...
		[batch 4/4] avg loss: 6.898313541260204e-05		[learning rate: 0.0014071]
	Learning Rate: 0.00140713
	LOSS [training: 6.898313541260204e-05 | validation: 4.0713560622683166e-05]
	TIME [epoch: 4.21 sec]
EPOCH 179/500:
	Training over batches...
		[batch 4/4] avg loss: 8.542923120293955e-05		[learning rate: 0.0013857]
	Learning Rate: 0.00138569
	LOSS [training: 8.542923120293955e-05 | validation: 0.0001519386512785599]
	TIME [epoch: 4.18 sec]
EPOCH 180/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001121368302508874		[learning rate: 0.0013646]
	Learning Rate: 0.00136458
	LOSS [training: 0.0001121368302508874 | validation: 5.496913383687341e-05]
	TIME [epoch: 4.21 sec]
EPOCH 181/500:
	Training over batches...
		[batch 4/4] avg loss: 8.398075512309412e-05		[learning rate: 0.0013438]
	Learning Rate: 0.0013438
	LOSS [training: 8.398075512309412e-05 | validation: 4.116307543800679e-05]
	TIME [epoch: 4.18 sec]
EPOCH 182/500:
	Training over batches...
		[batch 4/4] avg loss: 4.938880748311203e-05		[learning rate: 0.0013233]
	Learning Rate: 0.00132333
	LOSS [training: 4.938880748311203e-05 | validation: 4.015715593074565e-05]
	TIME [epoch: 4.17 sec]
EPOCH 183/500:
	Training over batches...
		[batch 4/4] avg loss: 4.560959714916435e-05		[learning rate: 0.0013032]
	Learning Rate: 0.00130317
	LOSS [training: 4.560959714916435e-05 | validation: 5.505038553031439e-05]
	TIME [epoch: 4.18 sec]
EPOCH 184/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00010892842448553663		[learning rate: 0.0012833]
	Learning Rate: 0.00128332
	LOSS [training: 0.00010892842448553663 | validation: 0.00011446123709314282]
	TIME [epoch: 4.17 sec]
EPOCH 185/500:
	Training over batches...
		[batch 4/4] avg loss: 8.393574097968626e-05		[learning rate: 0.0012638]
	Learning Rate: 0.00126377
	LOSS [training: 8.393574097968626e-05 | validation: 4.598582821716435e-05]
	TIME [epoch: 4.18 sec]
EPOCH 186/500:
	Training over batches...
		[batch 4/4] avg loss: 5.6872847060890024e-05		[learning rate: 0.0012445]
	Learning Rate: 0.00124451
	LOSS [training: 5.6872847060890024e-05 | validation: 2.2080518206844645e-05]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_186.pth
	Model improved!!!
EPOCH 187/500:
	Training over batches...
		[batch 4/4] avg loss: 9.939629772844958e-05		[learning rate: 0.0012256]
	Learning Rate: 0.00122556
	LOSS [training: 9.939629772844958e-05 | validation: 8.498761194326932e-05]
	TIME [epoch: 4.2 sec]
EPOCH 188/500:
	Training over batches...
		[batch 4/4] avg loss: 8.519929982490904e-05		[learning rate: 0.0012069]
	Learning Rate: 0.00120689
	LOSS [training: 8.519929982490904e-05 | validation: 4.946739112075061e-05]
	TIME [epoch: 4.18 sec]
EPOCH 189/500:
	Training over batches...
		[batch 4/4] avg loss: 4.6964520047007024e-05		[learning rate: 0.0011885]
	Learning Rate: 0.0011885
	LOSS [training: 4.6964520047007024e-05 | validation: 4.3204208527881426e-05]
	TIME [epoch: 4.18 sec]
EPOCH 190/500:
	Training over batches...
		[batch 4/4] avg loss: 5.62331854992919e-05		[learning rate: 0.0011704]
	Learning Rate: 0.0011704
	LOSS [training: 5.62331854992919e-05 | validation: 8.412418240308516e-05]
	TIME [epoch: 4.18 sec]
EPOCH 191/500:
	Training over batches...
		[batch 4/4] avg loss: 8.851325251564091e-05		[learning rate: 0.0011526]
	Learning Rate: 0.00115257
	LOSS [training: 8.851325251564091e-05 | validation: 9.06467580414303e-05]
	TIME [epoch: 4.2 sec]
EPOCH 192/500:
	Training over batches...
		[batch 4/4] avg loss: 7.716985317005387e-05		[learning rate: 0.001135]
	Learning Rate: 0.00113501
	LOSS [training: 7.716985317005387e-05 | validation: 5.6732672975396084e-05]
	TIME [epoch: 4.17 sec]
EPOCH 193/500:
	Training over batches...
		[batch 4/4] avg loss: 5.8655725772735706e-05		[learning rate: 0.0011177]
	Learning Rate: 0.00111772
	LOSS [training: 5.8655725772735706e-05 | validation: 7.908789139391237e-05]
	TIME [epoch: 4.19 sec]
EPOCH 194/500:
	Training over batches...
		[batch 4/4] avg loss: 8.762485795881125e-05		[learning rate: 0.0011007]
	Learning Rate: 0.00110069
	LOSS [training: 8.762485795881125e-05 | validation: 5.4924737923864874e-05]
	TIME [epoch: 4.18 sec]
EPOCH 195/500:
	Training over batches...
		[batch 4/4] avg loss: 6.5553068158393e-05		[learning rate: 0.0010839]
	Learning Rate: 0.00108393
	LOSS [training: 6.5553068158393e-05 | validation: 4.620823060000823e-05]
	TIME [epoch: 4.17 sec]
EPOCH 196/500:
	Training over batches...
		[batch 4/4] avg loss: 4.074584574123774e-05		[learning rate: 0.0010674]
	Learning Rate: 0.00106741
	LOSS [training: 4.074584574123774e-05 | validation: 4.693408896085182e-05]
	TIME [epoch: 4.17 sec]
EPOCH 197/500:
	Training over batches...
		[batch 4/4] avg loss: 5.1542804880546085e-05		[learning rate: 0.0010512]
	Learning Rate: 0.00105115
	LOSS [training: 5.1542804880546085e-05 | validation: 8.774370362810792e-05]
	TIME [epoch: 4.2 sec]
EPOCH 198/500:
	Training over batches...
		[batch 4/4] avg loss: 6.435723768010248e-05		[learning rate: 0.0010351]
	Learning Rate: 0.00103514
	LOSS [training: 6.435723768010248e-05 | validation: 5.8344698508495e-05]
	TIME [epoch: 4.2 sec]
EPOCH 199/500:
	Training over batches...
		[batch 4/4] avg loss: 4.963758350594971e-05		[learning rate: 0.0010194]
	Learning Rate: 0.00101937
	LOSS [training: 4.963758350594971e-05 | validation: 5.2927005449874726e-05]
	TIME [epoch: 4.18 sec]
EPOCH 200/500:
	Training over batches...
		[batch 4/4] avg loss: 8.119712109879685e-05		[learning rate: 0.0010038]
	Learning Rate: 0.00100384
	LOSS [training: 8.119712109879685e-05 | validation: 4.383770727133718e-05]
	TIME [epoch: 4.17 sec]
EPOCH 201/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7340200728340326e-05		[learning rate: 0.00098855]
	Learning Rate: 0.000988553
	LOSS [training: 3.7340200728340326e-05 | validation: 1.860406076118726e-05]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_201.pth
	Model improved!!!
EPOCH 202/500:
	Training over batches...
		[batch 4/4] avg loss: 4.080185374676793e-05		[learning rate: 0.00097349]
	Learning Rate: 0.000973494
	LOSS [training: 4.080185374676793e-05 | validation: 2.1322672703029786e-05]
	TIME [epoch: 4.21 sec]
EPOCH 203/500:
	Training over batches...
		[batch 4/4] avg loss: 3.902771038794662e-05		[learning rate: 0.00095866]
	Learning Rate: 0.000958664
	LOSS [training: 3.902771038794662e-05 | validation: 2.8438016775651277e-05]
	TIME [epoch: 4.18 sec]
EPOCH 204/500:
	Training over batches...
		[batch 4/4] avg loss: 9.172576058584248e-05		[learning rate: 0.00094406]
	Learning Rate: 0.000944061
	LOSS [training: 9.172576058584248e-05 | validation: 4.545989764933389e-05]
	TIME [epoch: 4.18 sec]
EPOCH 205/500:
	Training over batches...
		[batch 4/4] avg loss: 5.2704936216828375e-05		[learning rate: 0.00092968]
	Learning Rate: 0.00092968
	LOSS [training: 5.2704936216828375e-05 | validation: 1.029918378886685e-05]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_205.pth
	Model improved!!!
EPOCH 206/500:
	Training over batches...
		[batch 4/4] avg loss: 4.0116189146647055e-05		[learning rate: 0.00091552]
	Learning Rate: 0.000915518
	LOSS [training: 4.0116189146647055e-05 | validation: 2.092061760661057e-05]
	TIME [epoch: 4.18 sec]
EPOCH 207/500:
	Training over batches...
		[batch 4/4] avg loss: 4.756685594551147e-05		[learning rate: 0.00090157]
	Learning Rate: 0.000901571
	LOSS [training: 4.756685594551147e-05 | validation: 4.709559067637326e-05]
	TIME [epoch: 4.19 sec]
EPOCH 208/500:
	Training over batches...
		[batch 4/4] avg loss: 5.145765354921639e-05		[learning rate: 0.00088784]
	Learning Rate: 0.000887837
	LOSS [training: 5.145765354921639e-05 | validation: 1.3457376680333643e-05]
	TIME [epoch: 4.21 sec]
EPOCH 209/500:
	Training over batches...
		[batch 4/4] avg loss: 2.965111368746087e-05		[learning rate: 0.00087431]
	Learning Rate: 0.000874312
	LOSS [training: 2.965111368746087e-05 | validation: 3.725705737436868e-05]
	TIME [epoch: 4.19 sec]
EPOCH 210/500:
	Training over batches...
		[batch 4/4] avg loss: 4.8095721820906485e-05		[learning rate: 0.00086099]
	Learning Rate: 0.000860994
	LOSS [training: 4.8095721820906485e-05 | validation: 0.00010997540814325536]
	TIME [epoch: 4.18 sec]
EPOCH 211/500:
	Training over batches...
		[batch 4/4] avg loss: 6.807497547739339e-05		[learning rate: 0.00084788]
	Learning Rate: 0.000847878
	LOSS [training: 6.807497547739339e-05 | validation: 2.5497485036314105e-05]
	TIME [epoch: 4.19 sec]
EPOCH 212/500:
	Training over batches...
		[batch 4/4] avg loss: 3.8673533314904636e-05		[learning rate: 0.00083496]
	Learning Rate: 0.000834962
	LOSS [training: 3.8673533314904636e-05 | validation: 1.9384542521247107e-05]
	TIME [epoch: 4.18 sec]
EPOCH 213/500:
	Training over batches...
		[batch 4/4] avg loss: 3.765244084498909e-05		[learning rate: 0.00082224]
	Learning Rate: 0.000822243
	LOSS [training: 3.765244084498909e-05 | validation: 2.9420344735135996e-05]
	TIME [epoch: 4.18 sec]
EPOCH 214/500:
	Training over batches...
		[batch 4/4] avg loss: 4.4343620173284525e-05		[learning rate: 0.00080972]
	Learning Rate: 0.000809717
	LOSS [training: 4.4343620173284525e-05 | validation: 3.335072546483864e-05]
	TIME [epoch: 4.18 sec]
EPOCH 215/500:
	Training over batches...
		[batch 4/4] avg loss: 3.208612677162403e-05		[learning rate: 0.00079738]
	Learning Rate: 0.000797382
	LOSS [training: 3.208612677162403e-05 | validation: 1.0033394176764077e-05]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_215.pth
	Model improved!!!
EPOCH 216/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4066119948845895e-05		[learning rate: 0.00078524]
	Learning Rate: 0.000785236
	LOSS [training: 3.4066119948845895e-05 | validation: 6.295215224947514e-05]
	TIME [epoch: 4.19 sec]
EPOCH 217/500:
	Training over batches...
		[batch 4/4] avg loss: 6.878421058718887e-05		[learning rate: 0.00077327]
	Learning Rate: 0.000773274
	LOSS [training: 6.878421058718887e-05 | validation: 3.888997077768042e-05]
	TIME [epoch: 4.18 sec]
EPOCH 218/500:
	Training over batches...
		[batch 4/4] avg loss: 2.482951635657893e-05		[learning rate: 0.00076149]
	Learning Rate: 0.000761494
	LOSS [training: 2.482951635657893e-05 | validation: 3.720956969691591e-05]
	TIME [epoch: 4.19 sec]
EPOCH 219/500:
	Training over batches...
		[batch 4/4] avg loss: 3.563429221039171e-05		[learning rate: 0.00074989]
	Learning Rate: 0.000749894
	LOSS [training: 3.563429221039171e-05 | validation: 3.2174200034239546e-05]
	TIME [epoch: 4.21 sec]
EPOCH 220/500:
	Training over batches...
		[batch 4/4] avg loss: 3.1232259422994685e-05		[learning rate: 0.00073847]
	Learning Rate: 0.000738471
	LOSS [training: 3.1232259422994685e-05 | validation: 2.3817027442434968e-05]
	TIME [epoch: 4.19 sec]
EPOCH 221/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9473384742275914e-05		[learning rate: 0.00072722]
	Learning Rate: 0.000727221
	LOSS [training: 3.9473384742275914e-05 | validation: 4.6495498411521115e-05]
	TIME [epoch: 4.19 sec]
EPOCH 222/500:
	Training over batches...
		[batch 4/4] avg loss: 5.840453632849407e-05		[learning rate: 0.00071614]
	Learning Rate: 0.000716143
	LOSS [training: 5.840453632849407e-05 | validation: 1.6240882372901444e-05]
	TIME [epoch: 4.18 sec]
EPOCH 223/500:
	Training over batches...
		[batch 4/4] avg loss: 3.286434175892061e-05		[learning rate: 0.00070523]
	Learning Rate: 0.000705234
	LOSS [training: 3.286434175892061e-05 | validation: 1.6520909061332078e-05]
	TIME [epoch: 4.18 sec]
EPOCH 224/500:
	Training over batches...
		[batch 4/4] avg loss: 2.800458018808294e-05		[learning rate: 0.00069449]
	Learning Rate: 0.000694491
	LOSS [training: 2.800458018808294e-05 | validation: 2.8959869785495806e-05]
	TIME [epoch: 4.18 sec]
EPOCH 225/500:
	Training over batches...
		[batch 4/4] avg loss: 3.249106876801866e-05		[learning rate: 0.00068391]
	Learning Rate: 0.000683912
	LOSS [training: 3.249106876801866e-05 | validation: 2.4404143234105558e-05]
	TIME [epoch: 4.18 sec]
EPOCH 226/500:
	Training over batches...
		[batch 4/4] avg loss: 5.070211868182574e-05		[learning rate: 0.00067349]
	Learning Rate: 0.000673493
	LOSS [training: 5.070211868182574e-05 | validation: 3.9499629130321125e-05]
	TIME [epoch: 4.18 sec]
EPOCH 227/500:
	Training over batches...
		[batch 4/4] avg loss: 3.1660322883930725e-05		[learning rate: 0.00066323]
	Learning Rate: 0.000663234
	LOSS [training: 3.1660322883930725e-05 | validation: 2.3528075550229622e-05]
	TIME [epoch: 4.18 sec]
EPOCH 228/500:
	Training over batches...
		[batch 4/4] avg loss: 2.151808293723989e-05		[learning rate: 0.00065313]
	Learning Rate: 0.00065313
	LOSS [training: 2.151808293723989e-05 | validation: -5.0691902568804636e-06]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_228.pth
	Model improved!!!
EPOCH 229/500:
	Training over batches...
		[batch 4/4] avg loss: 3.537116383265304e-05		[learning rate: 0.00064318]
	Learning Rate: 0.000643181
	LOSS [training: 3.537116383265304e-05 | validation: 1.6408180763566983e-05]
	TIME [epoch: 4.2 sec]
EPOCH 230/500:
	Training over batches...
		[batch 4/4] avg loss: 2.905354139292904e-05		[learning rate: 0.00063338]
	Learning Rate: 0.000633383
	LOSS [training: 2.905354139292904e-05 | validation: 2.370908535319627e-05]
	TIME [epoch: 4.21 sec]
EPOCH 231/500:
	Training over batches...
		[batch 4/4] avg loss: 2.713206601069296e-05		[learning rate: 0.00062373]
	Learning Rate: 0.000623735
	LOSS [training: 2.713206601069296e-05 | validation: 9.740520873676939e-06]
	TIME [epoch: 4.18 sec]
EPOCH 232/500:
	Training over batches...
		[batch 4/4] avg loss: 2.8755233757396084e-05		[learning rate: 0.00061423]
	Learning Rate: 0.000614233
	LOSS [training: 2.8755233757396084e-05 | validation: 1.652917877366944e-05]
	TIME [epoch: 4.18 sec]
EPOCH 233/500:
	Training over batches...
		[batch 4/4] avg loss: 3.103860714813167e-05		[learning rate: 0.00060488]
	Learning Rate: 0.000604876
	LOSS [training: 3.103860714813167e-05 | validation: 6.104446470015645e-05]
	TIME [epoch: 4.18 sec]
EPOCH 234/500:
	Training over batches...
		[batch 4/4] avg loss: 2.8361453980588092e-05		[learning rate: 0.00059566]
	Learning Rate: 0.000595662
	LOSS [training: 2.8361453980588092e-05 | validation: 1.6950089611768915e-05]
	TIME [epoch: 4.18 sec]
EPOCH 235/500:
	Training over batches...
		[batch 4/4] avg loss: 2.2128669954939448e-05		[learning rate: 0.00058659]
	Learning Rate: 0.000586588
	LOSS [training: 2.2128669954939448e-05 | validation: 2.1695610380944032e-05]
	TIME [epoch: 4.19 sec]
EPOCH 236/500:
	Training over batches...
		[batch 4/4] avg loss: 2.7808878887717218e-05		[learning rate: 0.00057765]
	Learning Rate: 0.000577652
	LOSS [training: 2.7808878887717218e-05 | validation: 4.115709835616333e-05]
	TIME [epoch: 4.18 sec]
EPOCH 237/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7750620213437556e-05		[learning rate: 0.00056885]
	Learning Rate: 0.000568853
	LOSS [training: 3.7750620213437556e-05 | validation: 2.9662075439567782e-05]
	TIME [epoch: 4.18 sec]
EPOCH 238/500:
	Training over batches...
		[batch 4/4] avg loss: 2.536987139336988e-05		[learning rate: 0.00056019]
	Learning Rate: 0.000560187
	LOSS [training: 2.536987139336988e-05 | validation: 1.561889866538424e-05]
	TIME [epoch: 4.17 sec]
EPOCH 239/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6628602039752138e-05		[learning rate: 0.00055165]
	Learning Rate: 0.000551654
	LOSS [training: 1.6628602039752138e-05 | validation: 3.7838479334713515e-05]
	TIME [epoch: 4.18 sec]
EPOCH 240/500:
	Training over batches...
		[batch 4/4] avg loss: 3.923504041862391e-05		[learning rate: 0.00054325]
	Learning Rate: 0.00054325
	LOSS [training: 3.923504041862391e-05 | validation: 1.233004122959991e-05]
	TIME [epoch: 4.19 sec]
EPOCH 241/500:
	Training over batches...
		[batch 4/4] avg loss: 2.6197152621745713e-05		[learning rate: 0.00053497]
	Learning Rate: 0.000534975
	LOSS [training: 2.6197152621745713e-05 | validation: 1.5056897694590176e-05]
	TIME [epoch: 4.21 sec]
EPOCH 242/500:
	Training over batches...
		[batch 4/4] avg loss: 2.601000466840875e-05		[learning rate: 0.00052683]
	Learning Rate: 0.000526825
	LOSS [training: 2.601000466840875e-05 | validation: 1.3808792788079227e-06]
	TIME [epoch: 4.18 sec]
EPOCH 243/500:
	Training over batches...
		[batch 4/4] avg loss: 2.224864674349969e-05		[learning rate: 0.0005188]
	Learning Rate: 0.0005188
	LOSS [training: 2.224864674349969e-05 | validation: -1.8541389266268888e-07]
	TIME [epoch: 4.17 sec]
EPOCH 244/500:
	Training over batches...
		[batch 4/4] avg loss: 1.845999130216902e-05		[learning rate: 0.0005109]
	Learning Rate: 0.000510897
	LOSS [training: 1.845999130216902e-05 | validation: 6.256796898210793e-06]
	TIME [epoch: 4.17 sec]
EPOCH 245/500:
	Training over batches...
		[batch 4/4] avg loss: 2.2151032568756833e-05		[learning rate: 0.00050311]
	Learning Rate: 0.000503114
	LOSS [training: 2.2151032568756833e-05 | validation: 7.35285176084699e-06]
	TIME [epoch: 4.18 sec]
EPOCH 246/500:
	Training over batches...
		[batch 4/4] avg loss: 4.1564694657070354e-05		[learning rate: 0.00049545]
	Learning Rate: 0.00049545
	LOSS [training: 4.1564694657070354e-05 | validation: 8.33978231660648e-06]
	TIME [epoch: 4.18 sec]
EPOCH 247/500:
	Training over batches...
		[batch 4/4] avg loss: 2.4572247192931944e-05		[learning rate: 0.0004879]
	Learning Rate: 0.000487903
	LOSS [training: 2.4572247192931944e-05 | validation: 8.394606257837279e-06]
	TIME [epoch: 4.18 sec]
EPOCH 248/500:
	Training over batches...
		[batch 4/4] avg loss: 2.617662599245396e-05		[learning rate: 0.00048047]
	Learning Rate: 0.00048047
	LOSS [training: 2.617662599245396e-05 | validation: 1.8012764897698873e-05]
	TIME [epoch: 4.18 sec]
EPOCH 249/500:
	Training over batches...
		[batch 4/4] avg loss: 2.6354870023964085e-05		[learning rate: 0.00047315]
	Learning Rate: 0.000473151
	LOSS [training: 2.6354870023964085e-05 | validation: 2.279171707896377e-05]
	TIME [epoch: 4.18 sec]
EPOCH 250/500:
	Training over batches...
		[batch 4/4] avg loss: 3.287940324947092e-05		[learning rate: 0.00046594]
	Learning Rate: 0.000465944
	LOSS [training: 3.287940324947092e-05 | validation: 2.0517933615967102e-05]
	TIME [epoch: 4.18 sec]
EPOCH 251/500:
	Training over batches...
		[batch 4/4] avg loss: 1.716052564249271e-05		[learning rate: 0.00045885]
	Learning Rate: 0.000458846
	LOSS [training: 1.716052564249271e-05 | validation: 1.555374729967962e-05]
	TIME [epoch: 4.18 sec]
EPOCH 252/500:
	Training over batches...
		[batch 4/4] avg loss: 2.2064622609659244e-05		[learning rate: 0.00045186]
	Learning Rate: 0.000451856
	LOSS [training: 2.2064622609659244e-05 | validation: 1.2412236314407688e-05]
	TIME [epoch: 4.21 sec]
EPOCH 253/500:
	Training over batches...
		[batch 4/4] avg loss: 1.537125683337792e-05		[learning rate: 0.00044497]
	Learning Rate: 0.000444973
	LOSS [training: 1.537125683337792e-05 | validation: 2.8497232326269996e-06]
	TIME [epoch: 4.18 sec]
EPOCH 254/500:
	Training over batches...
		[batch 4/4] avg loss: 2.4123046347918734e-05		[learning rate: 0.00043819]
	Learning Rate: 0.000438194
	LOSS [training: 2.4123046347918734e-05 | validation: 7.242741645028916e-06]
	TIME [epoch: 4.19 sec]
EPOCH 255/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6874711377115602e-05		[learning rate: 0.00043152]
	Learning Rate: 0.000431519
	LOSS [training: 1.6874711377115602e-05 | validation: 2.4919525377201213e-05]
	TIME [epoch: 4.18 sec]
EPOCH 256/500:
	Training over batches...
		[batch 4/4] avg loss: 2.4648008506778064e-05		[learning rate: 0.00042495]
	Learning Rate: 0.000424946
	LOSS [training: 2.4648008506778064e-05 | validation: 9.773408946434748e-06]
	TIME [epoch: 4.18 sec]
EPOCH 257/500:
	Training over batches...
		[batch 4/4] avg loss: 1.8388285489281176e-05		[learning rate: 0.00041847]
	Learning Rate: 0.000418472
	LOSS [training: 1.8388285489281176e-05 | validation: -3.599984695374926e-07]
	TIME [epoch: 4.18 sec]
EPOCH 258/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7309062656954643e-05		[learning rate: 0.0004121]
	Learning Rate: 0.000412098
	LOSS [training: 1.7309062656954643e-05 | validation: 2.7010558159007214e-05]
	TIME [epoch: 4.18 sec]
EPOCH 259/500:
	Training over batches...
		[batch 4/4] avg loss: 2.7945706319578825e-05		[learning rate: 0.00040582]
	Learning Rate: 0.00040582
	LOSS [training: 2.7945706319578825e-05 | validation: 2.4138533636965276e-05]
	TIME [epoch: 4.18 sec]
EPOCH 260/500:
	Training over batches...
		[batch 4/4] avg loss: 2.3113476685137635e-05		[learning rate: 0.00039964]
	Learning Rate: 0.000399638
	LOSS [training: 2.3113476685137635e-05 | validation: 6.6675214120446304e-06]
	TIME [epoch: 4.18 sec]
EPOCH 261/500:
	Training over batches...
		[batch 4/4] avg loss: 1.57545007895572e-05		[learning rate: 0.00039355]
	Learning Rate: 0.00039355
	LOSS [training: 1.57545007895572e-05 | validation: 1.0619270465904496e-05]
	TIME [epoch: 4.18 sec]
EPOCH 262/500:
	Training over batches...
		[batch 4/4] avg loss: 2.2424658708673606e-05		[learning rate: 0.00038755]
	Learning Rate: 0.000387555
	LOSS [training: 2.2424658708673606e-05 | validation: 3.250276546540753e-06]
	TIME [epoch: 4.18 sec]
EPOCH 263/500:
	Training over batches...
		[batch 4/4] avg loss: 2.5931100054437106e-05		[learning rate: 0.00038165]
	Learning Rate: 0.000381651
	LOSS [training: 2.5931100054437106e-05 | validation: 1.3748802595458854e-05]
	TIME [epoch: 4.21 sec]
EPOCH 264/500:
	Training over batches...
		[batch 4/4] avg loss: 2.5558697266310772e-05		[learning rate: 0.00037584]
	Learning Rate: 0.000375837
	LOSS [training: 2.5558697266310772e-05 | validation: 6.812181376716575e-06]
	TIME [epoch: 4.19 sec]
EPOCH 265/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3299561755970247e-05		[learning rate: 0.00037011]
	Learning Rate: 0.000370112
	LOSS [training: 1.3299561755970247e-05 | validation: 2.313030138869743e-05]
	TIME [epoch: 4.18 sec]
EPOCH 266/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7761017389430148e-05		[learning rate: 0.00036447]
	Learning Rate: 0.000364474
	LOSS [training: 1.7761017389430148e-05 | validation: 3.767552017226672e-06]
	TIME [epoch: 4.17 sec]
EPOCH 267/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9061550494324007e-05		[learning rate: 0.00035892]
	Learning Rate: 0.000358922
	LOSS [training: 1.9061550494324007e-05 | validation: -3.7789446718021757e-06]
	TIME [epoch: 4.17 sec]
EPOCH 268/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5574590678524515e-05		[learning rate: 0.00035345]
	Learning Rate: 0.000353454
	LOSS [training: 1.5574590678524515e-05 | validation: 1.7921933605838935e-05]
	TIME [epoch: 4.18 sec]
EPOCH 269/500:
	Training over batches...
		[batch 4/4] avg loss: 2.6699763112303267e-05		[learning rate: 0.00034807]
	Learning Rate: 0.00034807
	LOSS [training: 2.6699763112303267e-05 | validation: 6.027109174610024e-06]
	TIME [epoch: 4.17 sec]
EPOCH 270/500:
	Training over batches...
		[batch 4/4] avg loss: 1.8893702320343487e-05		[learning rate: 0.00034277]
	Learning Rate: 0.000342768
	LOSS [training: 1.8893702320343487e-05 | validation: 7.300902899326501e-06]
	TIME [epoch: 4.17 sec]
EPOCH 271/500:
	Training over batches...
		[batch 4/4] avg loss: 1.626317441244596e-05		[learning rate: 0.00033755]
	Learning Rate: 0.000337546
	LOSS [training: 1.626317441244596e-05 | validation: 1.2209467992548185e-05]
	TIME [epoch: 4.18 sec]
EPOCH 272/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4713693793480531e-05		[learning rate: 0.0003324]
	Learning Rate: 0.000332404
	LOSS [training: 1.4713693793480531e-05 | validation: 1.537675202262978e-05]
	TIME [epoch: 4.17 sec]
EPOCH 273/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2288459211785453e-05		[learning rate: 0.00032734]
	Learning Rate: 0.000327341
	LOSS [training: 1.2288459211785453e-05 | validation: 1.9055943957484e-05]
	TIME [epoch: 4.18 sec]
EPOCH 274/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3493990478083086e-05		[learning rate: 0.00032235]
	Learning Rate: 0.000322354
	LOSS [training: 1.3493990478083086e-05 | validation: 1.2018376851644863e-05]
	TIME [epoch: 4.2 sec]
EPOCH 275/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7160051821765058e-05		[learning rate: 0.00031744]
	Learning Rate: 0.000317444
	LOSS [training: 1.7160051821765058e-05 | validation: 1.1560492879237438e-05]
	TIME [epoch: 4.19 sec]
EPOCH 276/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6295354319301627e-05		[learning rate: 0.00031261]
	Learning Rate: 0.000312608
	LOSS [training: 1.6295354319301627e-05 | validation: 9.716253057139967e-06]
	TIME [epoch: 4.18 sec]
EPOCH 277/500:
	Training over batches...
		[batch 4/4] avg loss: 1.875116458066384e-05		[learning rate: 0.00030785]
	Learning Rate: 0.000307846
	LOSS [training: 1.875116458066384e-05 | validation: 3.7716513743941696e-06]
	TIME [epoch: 4.17 sec]
EPOCH 278/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1526702358361952e-05		[learning rate: 0.00030316]
	Learning Rate: 0.000303156
	LOSS [training: 1.1526702358361952e-05 | validation: 1.2280584417092122e-05]
	TIME [epoch: 4.18 sec]
EPOCH 279/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5639394501421043e-05		[learning rate: 0.00029854]
	Learning Rate: 0.000298538
	LOSS [training: 1.5639394501421043e-05 | validation: 9.988253506743705e-06]
	TIME [epoch: 4.17 sec]
EPOCH 280/500:
	Training over batches...
		[batch 4/4] avg loss: 2.088755531504494e-05		[learning rate: 0.00029399]
	Learning Rate: 0.000293991
	LOSS [training: 2.088755531504494e-05 | validation: 8.653598837385256e-06]
	TIME [epoch: 4.18 sec]
EPOCH 281/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1862243726380118e-05		[learning rate: 0.00028951]
	Learning Rate: 0.000289512
	LOSS [training: 1.1862243726380118e-05 | validation: 1.2274143087712241e-05]
	TIME [epoch: 4.17 sec]
EPOCH 282/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3927285810183767e-05		[learning rate: 0.0002851]
	Learning Rate: 0.000285102
	LOSS [training: 1.3927285810183767e-05 | validation: 1.9365417664066074e-05]
	TIME [epoch: 4.17 sec]
EPOCH 283/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7004398058709792e-05		[learning rate: 0.00028076]
	Learning Rate: 0.000280759
	LOSS [training: 1.7004398058709792e-05 | validation: 8.538253805590479e-06]
	TIME [epoch: 4.17 sec]
EPOCH 284/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9942388669114796e-05		[learning rate: 0.00027648]
	Learning Rate: 0.000276482
	LOSS [training: 1.9942388669114796e-05 | validation: 1.1022548587924286e-05]
	TIME [epoch: 4.18 sec]
EPOCH 285/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3520124769391128e-05		[learning rate: 0.00027227]
	Learning Rate: 0.00027227
	LOSS [training: 1.3520124769391128e-05 | validation: 1.1293312817934444e-05]
	TIME [epoch: 4.2 sec]
EPOCH 286/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5613714825584712e-05		[learning rate: 0.00026812]
	Learning Rate: 0.000268123
	LOSS [training: 1.5613714825584712e-05 | validation: 1.627423982333065e-06]
	TIME [epoch: 4.2 sec]
EPOCH 287/500:
	Training over batches...
		[batch 4/4] avg loss: 9.514376233542453e-06		[learning rate: 0.00026404]
	Learning Rate: 0.000264038
	LOSS [training: 9.514376233542453e-06 | validation: 1.2188153163870786e-05]
	TIME [epoch: 4.17 sec]
EPOCH 288/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4143437783764479e-05		[learning rate: 0.00026002]
	Learning Rate: 0.000260016
	LOSS [training: 1.4143437783764479e-05 | validation: 8.074816674260577e-06]
	TIME [epoch: 4.18 sec]
EPOCH 289/500:
	Training over batches...
		[batch 4/4] avg loss: 1.240823491023957e-05		[learning rate: 0.00025606]
	Learning Rate: 0.000256055
	LOSS [training: 1.240823491023957e-05 | validation: 1.11316232243297e-06]
	TIME [epoch: 4.19 sec]
EPOCH 290/500:
	Training over batches...
		[batch 4/4] avg loss: 1.408397826982366e-05		[learning rate: 0.00025215]
	Learning Rate: 0.000252154
	LOSS [training: 1.408397826982366e-05 | validation: 2.0078115677134802e-05]
	TIME [epoch: 4.18 sec]
EPOCH 291/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6142651554295504e-05		[learning rate: 0.00024831]
	Learning Rate: 0.000248313
	LOSS [training: 1.6142651554295504e-05 | validation: 1.7483667522275592e-05]
	TIME [epoch: 4.18 sec]
EPOCH 292/500:
	Training over batches...
		[batch 4/4] avg loss: 8.405089468910721e-06		[learning rate: 0.00024453]
	Learning Rate: 0.000244531
	LOSS [training: 8.405089468910721e-06 | validation: 2.0065338652241803e-05]
	TIME [epoch: 4.18 sec]
EPOCH 293/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3233082188127577e-05		[learning rate: 0.00024081]
	Learning Rate: 0.000240806
	LOSS [training: 1.3233082188127577e-05 | validation: 1.1497215333595712e-05]
	TIME [epoch: 4.18 sec]
EPOCH 294/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9930424671827042e-05		[learning rate: 0.00023714]
	Learning Rate: 0.000237137
	LOSS [training: 1.9930424671827042e-05 | validation: 1.605810678752806e-05]
	TIME [epoch: 4.18 sec]
EPOCH 295/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4067963363019633e-05		[learning rate: 0.00023352]
	Learning Rate: 0.000233525
	LOSS [training: 1.4067963363019633e-05 | validation: 3.922218622223192e-07]
	TIME [epoch: 4.19 sec]
EPOCH 296/500:
	Training over batches...
		[batch 4/4] avg loss: 1.410525823848452e-05		[learning rate: 0.00022997]
	Learning Rate: 0.000229968
	LOSS [training: 1.410525823848452e-05 | validation: 2.7248418970384547e-05]
	TIME [epoch: 4.21 sec]
EPOCH 297/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5524294145047636e-05		[learning rate: 0.00022646]
	Learning Rate: 0.000226464
	LOSS [training: 1.5524294145047636e-05 | validation: 2.5329746904396487e-05]
	TIME [epoch: 4.21 sec]
EPOCH 298/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1966655721172727e-05		[learning rate: 0.00022301]
	Learning Rate: 0.000223015
	LOSS [training: 1.1966655721172727e-05 | validation: -3.6831085717576873e-06]
	TIME [epoch: 4.19 sec]
EPOCH 299/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2535961514677175e-05		[learning rate: 0.00021962]
	Learning Rate: 0.000219617
	LOSS [training: 1.2535961514677175e-05 | validation: 7.5543121607879455e-06]
	TIME [epoch: 4.18 sec]
EPOCH 300/500:
	Training over batches...
		[batch 4/4] avg loss: 1.242741408644954e-05		[learning rate: 0.00021627]
	Learning Rate: 0.000216272
	LOSS [training: 1.242741408644954e-05 | validation: 1.673785862887711e-05]
	TIME [epoch: 4.2 sec]
EPOCH 301/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0961098051319396e-05		[learning rate: 0.00021298]
	Learning Rate: 0.000212977
	LOSS [training: 1.0961098051319396e-05 | validation: 1.8956946872788374e-05]
	TIME [epoch: 4.19 sec]
EPOCH 302/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2814022290314254e-05		[learning rate: 0.00020973]
	Learning Rate: 0.000209733
	LOSS [training: 1.2814022290314254e-05 | validation: 5.878632129316719e-06]
	TIME [epoch: 4.18 sec]
EPOCH 303/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5443466915976424e-05		[learning rate: 0.00020654]
	Learning Rate: 0.000206538
	LOSS [training: 1.5443466915976424e-05 | validation: 5.906123425675869e-06]
	TIME [epoch: 4.18 sec]
EPOCH 304/500:
	Training over batches...
		[batch 4/4] avg loss: 1.264327409865007e-05		[learning rate: 0.00020339]
	Learning Rate: 0.000203392
	LOSS [training: 1.264327409865007e-05 | validation: 4.801379738208711e-06]
	TIME [epoch: 4.18 sec]
EPOCH 305/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2859809222520613e-05		[learning rate: 0.00020029]
	Learning Rate: 0.000200293
	LOSS [training: 1.2859809222520613e-05 | validation: 5.861098353846072e-06]
	TIME [epoch: 4.19 sec]
EPOCH 306/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3207397809348987e-05		[learning rate: 0.00019724]
	Learning Rate: 0.000197242
	LOSS [training: 1.3207397809348987e-05 | validation: 2.0070205368333704e-05]
	TIME [epoch: 4.19 sec]
EPOCH 307/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2120346262282313e-05		[learning rate: 0.00019424]
	Learning Rate: 0.000194238
	LOSS [training: 1.2120346262282313e-05 | validation: 1.0268640049274413e-05]
	TIME [epoch: 4.21 sec]
EPOCH 308/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2624558549543141e-05		[learning rate: 0.00019128]
	Learning Rate: 0.000191279
	LOSS [training: 1.2624558549543141e-05 | validation: 1.4072238909493829e-05]
	TIME [epoch: 4.21 sec]
EPOCH 309/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7448547431830664e-05		[learning rate: 0.00018836]
	Learning Rate: 0.000188365
	LOSS [training: 1.7448547431830664e-05 | validation: 8.093906106095617e-06]
	TIME [epoch: 4.18 sec]
EPOCH 310/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6841473738994807e-05		[learning rate: 0.0001855]
	Learning Rate: 0.000185495
	LOSS [training: 1.6841473738994807e-05 | validation: 8.6594824661963e-06]
	TIME [epoch: 4.18 sec]
EPOCH 311/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3091831115410123e-05		[learning rate: 0.00018267]
	Learning Rate: 0.00018267
	LOSS [training: 1.3091831115410123e-05 | validation: 1.8187586719543345e-05]
	TIME [epoch: 4.18 sec]
EPOCH 312/500:
	Training over batches...
		[batch 4/4] avg loss: 1.216580981543036e-05		[learning rate: 0.00017989]
	Learning Rate: 0.000179887
	LOSS [training: 1.216580981543036e-05 | validation: 9.957505361484654e-06]
	TIME [epoch: 4.18 sec]
EPOCH 313/500:
	Training over batches...
		[batch 4/4] avg loss: 7.741417638107207e-06		[learning rate: 0.00017715]
	Learning Rate: 0.000177147
	LOSS [training: 7.741417638107207e-06 | validation: 1.3752476494182097e-05]
	TIME [epoch: 4.19 sec]
EPOCH 314/500:
	Training over batches...
		[batch 4/4] avg loss: 1.748861349625108e-05		[learning rate: 0.00017445]
	Learning Rate: 0.000174448
	LOSS [training: 1.748861349625108e-05 | validation: -6.658400831218714e-07]
	TIME [epoch: 4.19 sec]
EPOCH 315/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4009787211880064e-05		[learning rate: 0.00017179]
	Learning Rate: 0.000171791
	LOSS [training: 1.4009787211880064e-05 | validation: 7.50879268005189e-06]
	TIME [epoch: 4.19 sec]
EPOCH 316/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5155564438371295e-05		[learning rate: 0.00016917]
	Learning Rate: 0.000169174
	LOSS [training: 1.5155564438371295e-05 | validation: 5.627492390693822e-06]
	TIME [epoch: 4.2 sec]
EPOCH 317/500:
	Training over batches...
		[batch 4/4] avg loss: 1.24617617141759e-05		[learning rate: 0.0001666]
	Learning Rate: 0.000166597
	LOSS [training: 1.24617617141759e-05 | validation: 1.383047388797798e-05]
	TIME [epoch: 4.24 sec]
EPOCH 318/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1681061517226699e-05		[learning rate: 0.00016406]
	Learning Rate: 0.000164059
	LOSS [training: 1.1681061517226699e-05 | validation: 6.4376953856006835e-06]
	TIME [epoch: 4.26 sec]
EPOCH 319/500:
	Training over batches...
		[batch 4/4] avg loss: 1.582134754913589e-05		[learning rate: 0.00016156]
	Learning Rate: 0.00016156
	LOSS [training: 1.582134754913589e-05 | validation: 6.647170387548584e-06]
	TIME [epoch: 4.2 sec]
EPOCH 320/500:
	Training over batches...
		[batch 4/4] avg loss: 1.341441538931398e-05		[learning rate: 0.0001591]
	Learning Rate: 0.000159099
	LOSS [training: 1.341441538931398e-05 | validation: 2.9563336224158743e-06]
	TIME [epoch: 4.21 sec]
EPOCH 321/500:
	Training over batches...
		[batch 4/4] avg loss: 7.888662879165786e-06		[learning rate: 0.00015668]
	Learning Rate: 0.000156675
	LOSS [training: 7.888662879165786e-06 | validation: 9.423008016747003e-06]
	TIME [epoch: 4.18 sec]
EPOCH 322/500:
	Training over batches...
		[batch 4/4] avg loss: 8.789576772027186e-06		[learning rate: 0.00015429]
	Learning Rate: 0.000154288
	LOSS [training: 8.789576772027186e-06 | validation: 1.5464623204354712e-05]
	TIME [epoch: 4.19 sec]
EPOCH 323/500:
	Training over batches...
		[batch 4/4] avg loss: 9.902947386525306e-06		[learning rate: 0.00015194]
	Learning Rate: 0.000151938
	LOSS [training: 9.902947386525306e-06 | validation: -1.1495398590926078e-06]
	TIME [epoch: 4.18 sec]
EPOCH 324/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9167712234075206e-05		[learning rate: 0.00014962]
	Learning Rate: 0.000149624
	LOSS [training: 1.9167712234075206e-05 | validation: 5.938195695629256e-06]
	TIME [epoch: 4.18 sec]
EPOCH 325/500:
	Training over batches...
		[batch 4/4] avg loss: 6.081565452020876e-06		[learning rate: 0.00014734]
	Learning Rate: 0.000147344
	LOSS [training: 6.081565452020876e-06 | validation: 2.1288587662536607e-05]
	TIME [epoch: 4.18 sec]
EPOCH 326/500:
	Training over batches...
		[batch 4/4] avg loss: 8.921135345636655e-06		[learning rate: 0.0001451]
	Learning Rate: 0.0001451
	LOSS [training: 8.921135345636655e-06 | validation: -1.3823215099148633e-05]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_326.pth
	Model improved!!!
EPOCH 327/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1842246398434675e-05		[learning rate: 0.00014289]
	Learning Rate: 0.000142889
	LOSS [training: 1.1842246398434675e-05 | validation: 4.65223271246562e-06]
	TIME [epoch: 4.2 sec]
EPOCH 328/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1372098132885976e-05		[learning rate: 0.00014071]
	Learning Rate: 0.000140713
	LOSS [training: 1.1372098132885976e-05 | validation: 2.331269081235154e-06]
	TIME [epoch: 4.19 sec]
EPOCH 329/500:
	Training over batches...
		[batch 4/4] avg loss: 8.613589231224462e-06		[learning rate: 0.00013857]
	Learning Rate: 0.000138569
	LOSS [training: 8.613589231224462e-06 | validation: 7.796245462136974e-06]
	TIME [epoch: 4.22 sec]
EPOCH 330/500:
	Training over batches...
		[batch 4/4] avg loss: 6.75298583247652e-06		[learning rate: 0.00013646]
	Learning Rate: 0.000136458
	LOSS [training: 6.75298583247652e-06 | validation: -1.238094933395662e-07]
	TIME [epoch: 4.2 sec]
EPOCH 331/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2251580644113027e-05		[learning rate: 0.00013438]
	Learning Rate: 0.00013438
	LOSS [training: 1.2251580644113027e-05 | validation: 6.626953846806894e-06]
	TIME [epoch: 4.19 sec]
EPOCH 332/500:
	Training over batches...
		[batch 4/4] avg loss: 7.56856277439122e-06		[learning rate: 0.00013233]
	Learning Rate: 0.000132333
	LOSS [training: 7.56856277439122e-06 | validation: 9.062917053769493e-06]
	TIME [epoch: 4.19 sec]
EPOCH 333/500:
	Training over batches...
		[batch 4/4] avg loss: 1.152095505547368e-05		[learning rate: 0.00013032]
	Learning Rate: 0.000130317
	LOSS [training: 1.152095505547368e-05 | validation: -1.7374662213587304e-07]
	TIME [epoch: 4.19 sec]
EPOCH 334/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1964080699921388e-05		[learning rate: 0.00012833]
	Learning Rate: 0.000128332
	LOSS [training: 1.1964080699921388e-05 | validation: 9.825439986681507e-07]
	TIME [epoch: 4.19 sec]
EPOCH 335/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5644090487649588e-05		[learning rate: 0.00012638]
	Learning Rate: 0.000126377
	LOSS [training: 1.5644090487649588e-05 | validation: -1.8670667347766573e-06]
	TIME [epoch: 4.23 sec]
EPOCH 336/500:
	Training over batches...
		[batch 4/4] avg loss: 9.267239553891082e-06		[learning rate: 0.00012445]
	Learning Rate: 0.000124451
	LOSS [training: 9.267239553891082e-06 | validation: 2.0268434586670158e-05]
	TIME [epoch: 4.21 sec]
EPOCH 337/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5832363258705075e-05		[learning rate: 0.00012256]
	Learning Rate: 0.000122556
	LOSS [training: 1.5832363258705075e-05 | validation: -8.295618088178798e-07]
	TIME [epoch: 4.19 sec]
EPOCH 338/500:
	Training over batches...
		[batch 4/4] avg loss: 1.118827715430637e-05		[learning rate: 0.00012069]
	Learning Rate: 0.000120689
	LOSS [training: 1.118827715430637e-05 | validation: -5.042011456322326e-06]
	TIME [epoch: 4.24 sec]
EPOCH 339/500:
	Training over batches...
		[batch 4/4] avg loss: 9.741740653351316e-06		[learning rate: 0.00011885]
	Learning Rate: 0.00011885
	LOSS [training: 9.741740653351316e-06 | validation: -1.4071244716844513e-06]
	TIME [epoch: 4.19 sec]
EPOCH 340/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4489320958761989e-05		[learning rate: 0.00011704]
	Learning Rate: 0.00011704
	LOSS [training: 1.4489320958761989e-05 | validation: -1.3837616136114896e-05]
	TIME [epoch: 4.23 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_340.pth
	Model improved!!!
EPOCH 341/500:
	Training over batches...
		[batch 4/4] avg loss: 8.082993209389544e-06		[learning rate: 0.00011526]
	Learning Rate: 0.000115257
	LOSS [training: 8.082993209389544e-06 | validation: -8.740712066447775e-06]
	TIME [epoch: 4.21 sec]
EPOCH 342/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2131623544263782e-05		[learning rate: 0.0001135]
	Learning Rate: 0.000113501
	LOSS [training: 1.2131623544263782e-05 | validation: 8.983067847622816e-06]
	TIME [epoch: 4.19 sec]
EPOCH 343/500:
	Training over batches...
		[batch 4/4] avg loss: 7.796568050551069e-06		[learning rate: 0.00011177]
	Learning Rate: 0.000111772
	LOSS [training: 7.796568050551069e-06 | validation: 2.929593477375647e-06]
	TIME [epoch: 4.18 sec]
EPOCH 344/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3908545062937706e-05		[learning rate: 0.00011007]
	Learning Rate: 0.000110069
	LOSS [training: 1.3908545062937706e-05 | validation: -6.25617624656738e-06]
	TIME [epoch: 4.19 sec]
EPOCH 345/500:
	Training over batches...
		[batch 4/4] avg loss: 1.193779811131046e-05		[learning rate: 0.00010839]
	Learning Rate: 0.000108393
	LOSS [training: 1.193779811131046e-05 | validation: -1.1774149996279348e-05]
	TIME [epoch: 4.18 sec]
EPOCH 346/500:
	Training over batches...
		[batch 4/4] avg loss: 8.767584355523716e-06		[learning rate: 0.00010674]
	Learning Rate: 0.000106742
	LOSS [training: 8.767584355523716e-06 | validation: 5.127113261135463e-06]
	TIME [epoch: 4.19 sec]
EPOCH 347/500:
	Training over batches...
		[batch 4/4] avg loss: 8.807608938882773e-06		[learning rate: 0.00010512]
	Learning Rate: 0.000105115
	LOSS [training: 8.807608938882773e-06 | validation: 9.439648839330016e-06]
	TIME [epoch: 4.19 sec]
EPOCH 348/500:
	Training over batches...
		[batch 4/4] avg loss: 9.240626592936452e-06		[learning rate: 0.00010351]
	Learning Rate: 0.000103514
	LOSS [training: 9.240626592936452e-06 | validation: 3.903057899658125e-05]
	TIME [epoch: 4.18 sec]
EPOCH 349/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2323509603143989e-05		[learning rate: 0.00010194]
	Learning Rate: 0.000101937
	LOSS [training: 1.2323509603143989e-05 | validation: -4.703792350501778e-06]
	TIME [epoch: 4.19 sec]
EPOCH 350/500:
	Training over batches...
		[batch 4/4] avg loss: 9.267883913064656e-06		[learning rate: 0.00010038]
	Learning Rate: 0.000100385
	LOSS [training: 9.267883913064656e-06 | validation: 3.1176369120027305e-06]
	TIME [epoch: 4.19 sec]
EPOCH 351/500:
	Training over batches...
		[batch 4/4] avg loss: 1.206265454766764e-05		[learning rate: 9.8855e-05]
	Learning Rate: 9.88553e-05
	LOSS [training: 1.206265454766764e-05 | validation: 1.2723449791489427e-05]
	TIME [epoch: 4.22 sec]
EPOCH 352/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4035580855377166e-06		[learning rate: 9.7349e-05]
	Learning Rate: 9.73494e-05
	LOSS [training: 3.4035580855377166e-06 | validation: 3.4950489069325305e-06]
	TIME [epoch: 4.19 sec]
EPOCH 353/500:
	Training over batches...
		[batch 4/4] avg loss: 6.752298682632274e-06		[learning rate: 9.5866e-05]
	Learning Rate: 9.58665e-05
	LOSS [training: 6.752298682632274e-06 | validation: 3.0085314684848987e-06]
	TIME [epoch: 4.19 sec]
EPOCH 354/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3369153242137277e-05		[learning rate: 9.4406e-05]
	Learning Rate: 9.44061e-05
	LOSS [training: 1.3369153242137277e-05 | validation: 1.0164560260137235e-05]
	TIME [epoch: 4.19 sec]
EPOCH 355/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1236597980512575e-05		[learning rate: 9.2968e-05]
	Learning Rate: 9.2968e-05
	LOSS [training: 1.1236597980512575e-05 | validation: 1.248134747445917e-05]
	TIME [epoch: 4.18 sec]
EPOCH 356/500:
	Training over batches...
		[batch 4/4] avg loss: 4.459709229805875e-06		[learning rate: 9.1552e-05]
	Learning Rate: 9.15518e-05
	LOSS [training: 4.459709229805875e-06 | validation: 1.2299948177389775e-05]
	TIME [epoch: 4.19 sec]
EPOCH 357/500:
	Training over batches...
		[batch 4/4] avg loss: 1.023775539001237e-05		[learning rate: 9.0157e-05]
	Learning Rate: 9.01571e-05
	LOSS [training: 1.023775539001237e-05 | validation: 9.524634075742001e-06]
	TIME [epoch: 4.19 sec]
EPOCH 358/500:
	Training over batches...
		[batch 4/4] avg loss: 8.843595050311293e-06		[learning rate: 8.8784e-05]
	Learning Rate: 8.87837e-05
	LOSS [training: 8.843595050311293e-06 | validation: 3.987108400313577e-06]
	TIME [epoch: 4.19 sec]
EPOCH 359/500:
	Training over batches...
		[batch 4/4] avg loss: 7.2414122475965576e-06		[learning rate: 8.7431e-05]
	Learning Rate: 8.74312e-05
	LOSS [training: 7.2414122475965576e-06 | validation: -2.630592529754061e-06]
	TIME [epoch: 4.18 sec]
EPOCH 360/500:
	Training over batches...
		[batch 4/4] avg loss: 3.291539734895821e-06		[learning rate: 8.6099e-05]
	Learning Rate: 8.60994e-05
	LOSS [training: 3.291539734895821e-06 | validation: -6.263475529933782e-06]
	TIME [epoch: 4.19 sec]
EPOCH 361/500:
	Training over batches...
		[batch 4/4] avg loss: 7.206169637180905e-06		[learning rate: 8.4788e-05]
	Learning Rate: 8.47878e-05
	LOSS [training: 7.206169637180905e-06 | validation: -2.364265844472468e-06]
	TIME [epoch: 4.19 sec]
EPOCH 362/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0610397857933497e-05		[learning rate: 8.3496e-05]
	Learning Rate: 8.34962e-05
	LOSS [training: 1.0610397857933497e-05 | validation: 1.4716445623610853e-05]
	TIME [epoch: 4.21 sec]
EPOCH 363/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3347106365364068e-05		[learning rate: 8.2224e-05]
	Learning Rate: 8.22243e-05
	LOSS [training: 1.3347106365364068e-05 | validation: -1.079826921172744e-05]
	TIME [epoch: 4.19 sec]
EPOCH 364/500:
	Training over batches...
		[batch 4/4] avg loss: 9.203931430908696e-06		[learning rate: 8.0972e-05]
	Learning Rate: 8.09717e-05
	LOSS [training: 9.203931430908696e-06 | validation: 1.761206763813528e-06]
	TIME [epoch: 4.19 sec]
EPOCH 365/500:
	Training over batches...
		[batch 4/4] avg loss: 7.233871207324438e-06		[learning rate: 7.9738e-05]
	Learning Rate: 7.97382e-05
	LOSS [training: 7.233871207324438e-06 | validation: -3.0194011737671977e-07]
	TIME [epoch: 4.18 sec]
EPOCH 366/500:
	Training over batches...
		[batch 4/4] avg loss: 1.174281198415217e-05		[learning rate: 7.8524e-05]
	Learning Rate: 7.85235e-05
	LOSS [training: 1.174281198415217e-05 | validation: -5.481063085682303e-06]
	TIME [epoch: 4.18 sec]
EPOCH 367/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2575348992257874e-05		[learning rate: 7.7327e-05]
	Learning Rate: 7.73274e-05
	LOSS [training: 1.2575348992257874e-05 | validation: 3.0679499001753155e-05]
	TIME [epoch: 4.18 sec]
EPOCH 368/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3015241762674323e-05		[learning rate: 7.6149e-05]
	Learning Rate: 7.61494e-05
	LOSS [training: 1.3015241762674323e-05 | validation: 5.52659105741804e-06]
	TIME [epoch: 4.19 sec]
EPOCH 369/500:
	Training over batches...
		[batch 4/4] avg loss: 1.165192134274984e-05		[learning rate: 7.4989e-05]
	Learning Rate: 7.49894e-05
	LOSS [training: 1.165192134274984e-05 | validation: 2.4664790424988592e-06]
	TIME [epoch: 4.19 sec]
EPOCH 370/500:
	Training over batches...
		[batch 4/4] avg loss: 5.137494424579225e-06		[learning rate: 7.3847e-05]
	Learning Rate: 7.38471e-05
	LOSS [training: 5.137494424579225e-06 | validation: 1.0239343995907868e-05]
	TIME [epoch: 4.18 sec]
EPOCH 371/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2561459048999744e-05		[learning rate: 7.2722e-05]
	Learning Rate: 7.27221e-05
	LOSS [training: 1.2561459048999744e-05 | validation: 7.058857786423912e-08]
	TIME [epoch: 4.18 sec]
EPOCH 372/500:
	Training over batches...
		[batch 4/4] avg loss: 8.210212142057306e-06		[learning rate: 7.1614e-05]
	Learning Rate: 7.16143e-05
	LOSS [training: 8.210212142057306e-06 | validation: 1.760165638282074e-05]
	TIME [epoch: 4.19 sec]
EPOCH 373/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2115604458085573e-05		[learning rate: 7.0523e-05]
	Learning Rate: 7.05234e-05
	LOSS [training: 1.2115604458085573e-05 | validation: -1.4370831902619233e-06]
	TIME [epoch: 4.27 sec]
EPOCH 374/500:
	Training over batches...
		[batch 4/4] avg loss: 5.988698732089958e-06		[learning rate: 6.9449e-05]
	Learning Rate: 6.94491e-05
	LOSS [training: 5.988698732089958e-06 | validation: 4.918103939381524e-06]
	TIME [epoch: 4.2 sec]
EPOCH 375/500:
	Training over batches...
		[batch 4/4] avg loss: 4.747668771787006e-06		[learning rate: 6.8391e-05]
	Learning Rate: 6.83912e-05
	LOSS [training: 4.747668771787006e-06 | validation: 1.8312122816033224e-06]
	TIME [epoch: 4.18 sec]
EPOCH 376/500:
	Training over batches...
		[batch 4/4] avg loss: 9.102397954233731e-06		[learning rate: 6.7349e-05]
	Learning Rate: 6.73493e-05
	LOSS [training: 9.102397954233731e-06 | validation: 1.707956090289442e-05]
	TIME [epoch: 4.18 sec]
EPOCH 377/500:
	Training over batches...
		[batch 4/4] avg loss: 9.22801009100227e-06		[learning rate: 6.6323e-05]
	Learning Rate: 6.63234e-05
	LOSS [training: 9.22801009100227e-06 | validation: 6.78858572064911e-06]
	TIME [epoch: 4.18 sec]
EPOCH 378/500:
	Training over batches...
		[batch 4/4] avg loss: 6.172074177387187e-06		[learning rate: 6.5313e-05]
	Learning Rate: 6.5313e-05
	LOSS [training: 6.172074177387187e-06 | validation: 9.600051034219791e-06]
	TIME [epoch: 4.19 sec]
EPOCH 379/500:
	Training over batches...
		[batch 4/4] avg loss: 9.018748842734348e-06		[learning rate: 6.4318e-05]
	Learning Rate: 6.43181e-05
	LOSS [training: 9.018748842734348e-06 | validation: 2.486935299768067e-06]
	TIME [epoch: 4.18 sec]
EPOCH 380/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2274840766197448e-05		[learning rate: 6.3338e-05]
	Learning Rate: 6.33383e-05
	LOSS [training: 1.2274840766197448e-05 | validation: 1.950518148284508e-05]
	TIME [epoch: 4.18 sec]
EPOCH 381/500:
	Training over batches...
		[batch 4/4] avg loss: 8.600735508039082e-06		[learning rate: 6.2373e-05]
	Learning Rate: 6.23735e-05
	LOSS [training: 8.600735508039082e-06 | validation: 1.3104482218315727e-05]
	TIME [epoch: 4.18 sec]
EPOCH 382/500:
	Training over batches...
		[batch 4/4] avg loss: 8.943263487506403e-06		[learning rate: 6.1423e-05]
	Learning Rate: 6.14233e-05
	LOSS [training: 8.943263487506403e-06 | validation: 3.244229594596027e-06]
	TIME [epoch: 4.18 sec]
EPOCH 383/500:
	Training over batches...
		[batch 4/4] avg loss: 4.303085156777354e-06		[learning rate: 6.0488e-05]
	Learning Rate: 6.04876e-05
	LOSS [training: 4.303085156777354e-06 | validation: 4.828761854978048e-07]
	TIME [epoch: 4.19 sec]
EPOCH 384/500:
	Training over batches...
		[batch 4/4] avg loss: 3.5327140539018755e-06		[learning rate: 5.9566e-05]
	Learning Rate: 5.95662e-05
	LOSS [training: 3.5327140539018755e-06 | validation: 3.629570905606916e-05]
	TIME [epoch: 4.22 sec]
EPOCH 385/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4977988908654316e-06		[learning rate: 5.8659e-05]
	Learning Rate: 5.86588e-05
	LOSS [training: 1.4977988908654316e-06 | validation: 1.2070175106744684e-05]
	TIME [epoch: 4.21 sec]
EPOCH 386/500:
	Training over batches...
		[batch 4/4] avg loss: 6.415606654397222e-06		[learning rate: 5.7765e-05]
	Learning Rate: 5.77652e-05
	LOSS [training: 6.415606654397222e-06 | validation: 8.14320572972349e-06]
	TIME [epoch: 4.19 sec]
EPOCH 387/500:
	Training over batches...
		[batch 4/4] avg loss: 8.137650481896297e-06		[learning rate: 5.6885e-05]
	Learning Rate: 5.68853e-05
	LOSS [training: 8.137650481896297e-06 | validation: 1.1122950230752603e-05]
	TIME [epoch: 4.19 sec]
EPOCH 388/500:
	Training over batches...
		[batch 4/4] avg loss: 9.711971909872607e-06		[learning rate: 5.6019e-05]
	Learning Rate: 5.60187e-05
	LOSS [training: 9.711971909872607e-06 | validation: -1.013230384304742e-05]
	TIME [epoch: 4.19 sec]
EPOCH 389/500:
	Training over batches...
		[batch 4/4] avg loss: 7.086613354583049e-06		[learning rate: 5.5165e-05]
	Learning Rate: 5.51654e-05
	LOSS [training: 7.086613354583049e-06 | validation: 8.694084221172683e-06]
	TIME [epoch: 4.19 sec]
EPOCH 390/500:
	Training over batches...
		[batch 4/4] avg loss: 9.075907276930417e-06		[learning rate: 5.4325e-05]
	Learning Rate: 5.4325e-05
	LOSS [training: 9.075907276930417e-06 | validation: 2.2668571500384706e-05]
	TIME [epoch: 4.19 sec]
EPOCH 391/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0488679556164616e-05		[learning rate: 5.3497e-05]
	Learning Rate: 5.34975e-05
	LOSS [training: 1.0488679556164616e-05 | validation: 8.450069053064003e-06]
	TIME [epoch: 4.19 sec]
EPOCH 392/500:
	Training over batches...
		[batch 4/4] avg loss: 7.6231842023323226e-06		[learning rate: 5.2683e-05]
	Learning Rate: 5.26825e-05
	LOSS [training: 7.6231842023323226e-06 | validation: -4.305558883951565e-06]
	TIME [epoch: 4.24 sec]
EPOCH 393/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1035175647579721e-05		[learning rate: 5.188e-05]
	Learning Rate: 5.188e-05
	LOSS [training: 1.1035175647579721e-05 | validation: 1.6540090430351207e-05]
	TIME [epoch: 4.2 sec]
EPOCH 394/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2180653324259328e-05		[learning rate: 5.109e-05]
	Learning Rate: 5.10897e-05
	LOSS [training: 1.2180653324259328e-05 | validation: -4.828124961500092e-07]
	TIME [epoch: 4.19 sec]
EPOCH 395/500:
	Training over batches...
		[batch 4/4] avg loss: 7.043657729383757e-06		[learning rate: 5.0311e-05]
	Learning Rate: 5.03114e-05
	LOSS [training: 7.043657729383757e-06 | validation: 1.221265827978657e-06]
	TIME [epoch: 4.22 sec]
EPOCH 396/500:
	Training over batches...
		[batch 4/4] avg loss: 2.422360743241536e-06		[learning rate: 4.9545e-05]
	Learning Rate: 4.9545e-05
	LOSS [training: 2.422360743241536e-06 | validation: 9.899911377650783e-06]
	TIME [epoch: 4.23 sec]
EPOCH 397/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7666002947053224e-06		[learning rate: 4.879e-05]
	Learning Rate: 4.87903e-05
	LOSS [training: 1.7666002947053224e-06 | validation: 2.2442826723342524e-05]
	TIME [epoch: 4.23 sec]
EPOCH 398/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5199705712263254e-06		[learning rate: 4.8047e-05]
	Learning Rate: 4.8047e-05
	LOSS [training: 1.5199705712263254e-06 | validation: -6.336016004533818e-06]
	TIME [epoch: 4.19 sec]
EPOCH 399/500:
	Training over batches...
		[batch 4/4] avg loss: 9.418733736158092e-06		[learning rate: 4.7315e-05]
	Learning Rate: 4.73151e-05
	LOSS [training: 9.418733736158092e-06 | validation: 3.844169323451441e-06]
	TIME [epoch: 4.19 sec]
EPOCH 400/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5457302297335663e-05		[learning rate: 4.6594e-05]
	Learning Rate: 4.65944e-05
	LOSS [training: 1.5457302297335663e-05 | validation: -1.2087367804105131e-05]
	TIME [epoch: 4.2 sec]
EPOCH 401/500:
	Training over batches...
		[batch 4/4] avg loss: 6.9667445910635014e-06		[learning rate: 4.5885e-05]
	Learning Rate: 4.58846e-05
	LOSS [training: 6.9667445910635014e-06 | validation: -1.3587504909309267e-05]
	TIME [epoch: 4.19 sec]
EPOCH 402/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2305899189663073e-05		[learning rate: 4.5186e-05]
	Learning Rate: 4.51856e-05
	LOSS [training: 1.2305899189663073e-05 | validation: 2.662190636705475e-06]
	TIME [epoch: 4.18 sec]
EPOCH 403/500:
	Training over batches...
		[batch 4/4] avg loss: 8.3499100391482e-06		[learning rate: 4.4497e-05]
	Learning Rate: 4.44973e-05
	LOSS [training: 8.3499100391482e-06 | validation: 1.90228678001787e-05]
	TIME [epoch: 4.19 sec]
EPOCH 404/500:
	Training over batches...
		[batch 4/4] avg loss: 6.405662949316438e-06		[learning rate: 4.3819e-05]
	Learning Rate: 4.38194e-05
	LOSS [training: 6.405662949316438e-06 | validation: 1.8079246497921807e-05]
	TIME [epoch: 4.18 sec]
EPOCH 405/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3499402204126154e-05		[learning rate: 4.3152e-05]
	Learning Rate: 4.31519e-05
	LOSS [training: 1.3499402204126154e-05 | validation: -2.6890660227600893e-06]
	TIME [epoch: 4.19 sec]
EPOCH 406/500:
	Training over batches...
		[batch 4/4] avg loss: 8.997252679600276e-06		[learning rate: 4.2495e-05]
	Learning Rate: 4.24946e-05
	LOSS [training: 8.997252679600276e-06 | validation: 1.0618351493285384e-05]
	TIME [epoch: 4.27 sec]
EPOCH 407/500:
	Training over batches...
		[batch 4/4] avg loss: 7.32715310818044e-06		[learning rate: 4.1847e-05]
	Learning Rate: 4.18472e-05
	LOSS [training: 7.32715310818044e-06 | validation: -4.422343704495901e-06]
	TIME [epoch: 4.2 sec]
EPOCH 408/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6772027239903014e-06		[learning rate: 4.121e-05]
	Learning Rate: 4.12098e-05
	LOSS [training: 1.6772027239903014e-06 | validation: 1.276710859191943e-05]
	TIME [epoch: 4.18 sec]
EPOCH 409/500:
	Training over batches...
		[batch 4/4] avg loss: 7.271358692325204e-06		[learning rate: 4.0582e-05]
	Learning Rate: 4.0582e-05
	LOSS [training: 7.271358692325204e-06 | validation: 1.0317158761787404e-05]
	TIME [epoch: 4.18 sec]
EPOCH 410/500:
	Training over batches...
		[batch 4/4] avg loss: 1.47536393332951e-05		[learning rate: 3.9964e-05]
	Learning Rate: 3.99638e-05
	LOSS [training: 1.47536393332951e-05 | validation: 6.207199597802804e-06]
	TIME [epoch: 4.18 sec]
EPOCH 411/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1286664563020745e-05		[learning rate: 3.9355e-05]
	Learning Rate: 3.9355e-05
	LOSS [training: 1.1286664563020745e-05 | validation: -1.5306562859651597e-06]
	TIME [epoch: 4.22 sec]
EPOCH 412/500:
	Training over batches...
		[batch 4/4] avg loss: 4.5546885559693e-06		[learning rate: 3.8755e-05]
	Learning Rate: 3.87555e-05
	LOSS [training: 4.5546885559693e-06 | validation: -1.3974026542726393e-05]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_412.pth
	Model improved!!!
EPOCH 413/500:
	Training over batches...
		[batch 4/4] avg loss: 8.484479539988876e-06		[learning rate: 3.8165e-05]
	Learning Rate: 3.81651e-05
	LOSS [training: 8.484479539988876e-06 | validation: 5.38316330106503e-06]
	TIME [epoch: 4.19 sec]
EPOCH 414/500:
	Training over batches...
		[batch 4/4] avg loss: 2.503881127506657e-06		[learning rate: 3.7584e-05]
	Learning Rate: 3.75837e-05
	LOSS [training: 2.503881127506657e-06 | validation: 4.921440727913007e-06]
	TIME [epoch: 4.17 sec]
EPOCH 415/500:
	Training over batches...
		[batch 4/4] avg loss: 6.014701884557328e-06		[learning rate: 3.7011e-05]
	Learning Rate: 3.70112e-05
	LOSS [training: 6.014701884557328e-06 | validation: -8.175523144855923e-06]
	TIME [epoch: 4.17 sec]
EPOCH 416/500:
	Training over batches...
		[batch 4/4] avg loss: 3.0781322444374263e-06		[learning rate: 3.6447e-05]
	Learning Rate: 3.64474e-05
	LOSS [training: 3.0781322444374263e-06 | validation: 1.9066621720775777e-06]
	TIME [epoch: 4.18 sec]
EPOCH 417/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7563298354504654e-05		[learning rate: 3.5892e-05]
	Learning Rate: 3.58922e-05
	LOSS [training: 1.7563298354504654e-05 | validation: -8.485612765882778e-07]
	TIME [epoch: 4.21 sec]
EPOCH 418/500:
	Training over batches...
		[batch 4/4] avg loss: 2.6793076813936833e-06		[learning rate: 3.5345e-05]
	Learning Rate: 3.53454e-05
	LOSS [training: 2.6793076813936833e-06 | validation: 2.4382537731004294e-05]
	TIME [epoch: 4.18 sec]
EPOCH 419/500:
	Training over batches...
		[batch 4/4] avg loss: 6.89132180497376e-06		[learning rate: 3.4807e-05]
	Learning Rate: 3.4807e-05
	LOSS [training: 6.89132180497376e-06 | validation: -5.9272543498420925e-06]
	TIME [epoch: 4.18 sec]
EPOCH 420/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3127057405825537e-05		[learning rate: 3.4277e-05]
	Learning Rate: 3.42768e-05
	LOSS [training: 1.3127057405825537e-05 | validation: 3.858164314286761e-06]
	TIME [epoch: 4.18 sec]
EPOCH 421/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2993143992577894e-06		[learning rate: 3.3755e-05]
	Learning Rate: 3.37546e-05
	LOSS [training: 1.2993143992577894e-06 | validation: -1.9159009349438704e-06]
	TIME [epoch: 4.18 sec]
EPOCH 422/500:
	Training over batches...
		[batch 4/4] avg loss: 7.654122300764344e-06		[learning rate: 3.324e-05]
	Learning Rate: 3.32404e-05
	LOSS [training: 7.654122300764344e-06 | validation: 1.9043976083097822e-05]
	TIME [epoch: 4.18 sec]
EPOCH 423/500:
	Training over batches...
		[batch 4/4] avg loss: 4.8086183319107075e-06		[learning rate: 3.2734e-05]
	Learning Rate: 3.27341e-05
	LOSS [training: 4.8086183319107075e-06 | validation: 1.2184409319061285e-05]
	TIME [epoch: 4.18 sec]
EPOCH 424/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0719370294049459e-05		[learning rate: 3.2235e-05]
	Learning Rate: 3.22354e-05
	LOSS [training: 1.0719370294049459e-05 | validation: 9.291563253130077e-06]
	TIME [epoch: 4.18 sec]
EPOCH 425/500:
	Training over batches...
		[batch 4/4] avg loss: -1.915043895439666e-08		[learning rate: 3.1744e-05]
	Learning Rate: 3.17444e-05
	LOSS [training: -1.915043895439666e-08 | validation: -8.97654624776556e-06]
	TIME [epoch: 4.19 sec]
EPOCH 426/500:
	Training over batches...
		[batch 4/4] avg loss: 7.342680730025531e-06		[learning rate: 3.1261e-05]
	Learning Rate: 3.12608e-05
	LOSS [training: 7.342680730025531e-06 | validation: -4.639674031794439e-06]
	TIME [epoch: 4.18 sec]
EPOCH 427/500:
	Training over batches...
		[batch 4/4] avg loss: 7.5762878995103696e-06		[learning rate: 3.0785e-05]
	Learning Rate: 3.07846e-05
	LOSS [training: 7.5762878995103696e-06 | validation: 5.83928575069681e-06]
	TIME [epoch: 4.19 sec]
EPOCH 428/500:
	Training over batches...
		[batch 4/4] avg loss: 9.734589122505799e-06		[learning rate: 3.0316e-05]
	Learning Rate: 3.03156e-05
	LOSS [training: 9.734589122505799e-06 | validation: -5.897120343091755e-06]
	TIME [epoch: 4.21 sec]
EPOCH 429/500:
	Training over batches...
		[batch 4/4] avg loss: 9.019242254171834e-06		[learning rate: 2.9854e-05]
	Learning Rate: 2.98538e-05
	LOSS [training: 9.019242254171834e-06 | validation: 4.164919274006307e-06]
	TIME [epoch: 4.18 sec]
EPOCH 430/500:
	Training over batches...
		[batch 4/4] avg loss: 8.68039898073192e-06		[learning rate: 2.9399e-05]
	Learning Rate: 2.9399e-05
	LOSS [training: 8.68039898073192e-06 | validation: 2.9841560956027637e-06]
	TIME [epoch: 4.18 sec]
EPOCH 431/500:
	Training over batches...
		[batch 4/4] avg loss: 4.375762299680619e-06		[learning rate: 2.8951e-05]
	Learning Rate: 2.89512e-05
	LOSS [training: 4.375762299680619e-06 | validation: 1.7572923342912273e-06]
	TIME [epoch: 4.18 sec]
EPOCH 432/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3829835312205385e-05		[learning rate: 2.851e-05]
	Learning Rate: 2.85102e-05
	LOSS [training: 1.3829835312205385e-05 | validation: 9.064447358722022e-07]
	TIME [epoch: 4.18 sec]
EPOCH 433/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9175987712280687e-05		[learning rate: 2.8076e-05]
	Learning Rate: 2.80759e-05
	LOSS [training: 1.9175987712280687e-05 | validation: -5.185228533217901e-06]
	TIME [epoch: 4.18 sec]
EPOCH 434/500:
	Training over batches...
		[batch 4/4] avg loss: 4.861587608056217e-06		[learning rate: 2.7648e-05]
	Learning Rate: 2.76482e-05
	LOSS [training: 4.861587608056217e-06 | validation: -8.036076331384256e-06]
	TIME [epoch: 4.17 sec]
EPOCH 435/500:
	Training over batches...
		[batch 4/4] avg loss: 5.910200884469586e-06		[learning rate: 2.7227e-05]
	Learning Rate: 2.7227e-05
	LOSS [training: 5.910200884469586e-06 | validation: -7.464265332357553e-07]
	TIME [epoch: 4.19 sec]
EPOCH 436/500:
	Training over batches...
		[batch 4/4] avg loss: 4.902215065362592e-06		[learning rate: 2.6812e-05]
	Learning Rate: 2.68122e-05
	LOSS [training: 4.902215065362592e-06 | validation: -2.7524992583738416e-07]
	TIME [epoch: 4.18 sec]
EPOCH 437/500:
	Training over batches...
		[batch 4/4] avg loss: 2.5046020373598046e-06		[learning rate: 2.6404e-05]
	Learning Rate: 2.64038e-05
	LOSS [training: 2.5046020373598046e-06 | validation: 5.918432375906502e-06]
	TIME [epoch: 4.18 sec]
EPOCH 438/500:
	Training over batches...
		[batch 4/4] avg loss: 7.352185426118596e-06		[learning rate: 2.6002e-05]
	Learning Rate: 2.60016e-05
	LOSS [training: 7.352185426118596e-06 | validation: 5.429553877285276e-07]
	TIME [epoch: 4.19 sec]
EPOCH 439/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3591719151560989e-05		[learning rate: 2.5605e-05]
	Learning Rate: 2.56055e-05
	LOSS [training: 1.3591719151560989e-05 | validation: -1.2446322864361205e-06]
	TIME [epoch: 4.22 sec]
EPOCH 440/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2082186966115404e-05		[learning rate: 2.5215e-05]
	Learning Rate: 2.52154e-05
	LOSS [training: 1.2082186966115404e-05 | validation: 1.8194555509084332e-05]
	TIME [epoch: 4.18 sec]
EPOCH 441/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1133167125944033e-05		[learning rate: 2.4831e-05]
	Learning Rate: 2.48313e-05
	LOSS [training: 1.1133167125944033e-05 | validation: -1.020150578160406e-05]
	TIME [epoch: 4.18 sec]
EPOCH 442/500:
	Training over batches...
		[batch 4/4] avg loss: 7.294379998190049e-06		[learning rate: 2.4453e-05]
	Learning Rate: 2.44531e-05
	LOSS [training: 7.294379998190049e-06 | validation: 2.5045518936966493e-05]
	TIME [epoch: 4.17 sec]
EPOCH 443/500:
	Training over batches...
		[batch 4/4] avg loss: 9.125383638543317e-06		[learning rate: 2.4081e-05]
	Learning Rate: 2.40806e-05
	LOSS [training: 9.125383638543317e-06 | validation: -1.5955367077926665e-05]
	TIME [epoch: 4.18 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd6_20240711_014044/states/model_algphiq_1a_v_mmd6_443.pth
	Model improved!!!
EPOCH 444/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1858396466217336e-05		[learning rate: 2.3714e-05]
	Learning Rate: 2.37137e-05
	LOSS [training: 1.1858396466217336e-05 | validation: 8.430883463184102e-07]
	TIME [epoch: 4.22 sec]
EPOCH 445/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6465084370737035e-06		[learning rate: 2.3352e-05]
	Learning Rate: 2.33525e-05
	LOSS [training: 3.6465084370737035e-06 | validation: 2.294724406022519e-05]
	TIME [epoch: 4.17 sec]
EPOCH 446/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0606215766675865e-05		[learning rate: 2.2997e-05]
	Learning Rate: 2.29968e-05
	LOSS [training: 1.0606215766675865e-05 | validation: 2.1172690995996395e-06]
	TIME [epoch: 4.17 sec]
EPOCH 447/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4525793358427518e-05		[learning rate: 2.2646e-05]
	Learning Rate: 2.26464e-05
	LOSS [training: 1.4525793358427518e-05 | validation: 8.251049301629676e-06]
	TIME [epoch: 4.18 sec]
EPOCH 448/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2512372366863867e-05		[learning rate: 2.2301e-05]
	Learning Rate: 2.23015e-05
	LOSS [training: 1.2512372366863867e-05 | validation: -4.370136619732622e-06]
	TIME [epoch: 4.18 sec]
EPOCH 449/500:
	Training over batches...
		[batch 4/4] avg loss: 7.75773947322622e-06		[learning rate: 2.1962e-05]
	Learning Rate: 2.19617e-05
	LOSS [training: 7.75773947322622e-06 | validation: -1.5265210621044023e-05]
	TIME [epoch: 4.18 sec]
EPOCH 450/500:
	Training over batches...
		[batch 4/4] avg loss: 8.4234583752254e-06		[learning rate: 2.1627e-05]
	Learning Rate: 2.16272e-05
	LOSS [training: 8.4234583752254e-06 | validation: 4.207263531485594e-06]
	TIME [epoch: 4.2 sec]
EPOCH 451/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5947202881862245e-05		[learning rate: 2.1298e-05]
	Learning Rate: 2.12977e-05
	LOSS [training: 1.5947202881862245e-05 | validation: -4.6882874876577055e-06]
	TIME [epoch: 4.18 sec]
EPOCH 452/500:
	Training over batches...
		[batch 4/4] avg loss: 5.531856035956629e-06		[learning rate: 2.0973e-05]
	Learning Rate: 2.09733e-05
	LOSS [training: 5.531856035956629e-06 | validation: 1.0066369019296362e-05]
	TIME [epoch: 4.18 sec]
EPOCH 453/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1185352004454473e-05		[learning rate: 2.0654e-05]
	Learning Rate: 2.06538e-05
	LOSS [training: 1.1185352004454473e-05 | validation: 9.264992994343801e-06]
	TIME [epoch: 4.17 sec]
EPOCH 454/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6437735662378605e-05		[learning rate: 2.0339e-05]
	Learning Rate: 2.03392e-05
	LOSS [training: 1.6437735662378605e-05 | validation: 5.328691151168519e-06]
	TIME [epoch: 4.17 sec]
EPOCH 455/500:
	Training over batches...
		[batch 4/4] avg loss: 7.549375709138717e-06		[learning rate: 2.0029e-05]
	Learning Rate: 2.00293e-05
	LOSS [training: 7.549375709138717e-06 | validation: 1.1845853058248368e-05]
	TIME [epoch: 4.17 sec]
EPOCH 456/500:
	Training over batches...
		[batch 4/4] avg loss: 5.670664140912219e-06		[learning rate: 1.9724e-05]
	Learning Rate: 1.97242e-05
	LOSS [training: 5.670664140912219e-06 | validation: 7.895237699617813e-09]
	TIME [epoch: 4.17 sec]
EPOCH 457/500:
	Training over batches...
		[batch 4/4] avg loss: 3.733360903726113e-06		[learning rate: 1.9424e-05]
	Learning Rate: 1.94238e-05
	LOSS [training: 3.733360903726113e-06 | validation: -3.5667126518317274e-06]
	TIME [epoch: 4.18 sec]
EPOCH 458/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2164142542247405e-05		[learning rate: 1.9128e-05]
	Learning Rate: 1.91279e-05
	LOSS [training: 1.2164142542247405e-05 | validation: 6.981061524766519e-06]
	TIME [epoch: 4.18 sec]
EPOCH 459/500:
	Training over batches...
		[batch 4/4] avg loss: 9.478612207656667e-06		[learning rate: 1.8836e-05]
	Learning Rate: 1.88365e-05
	LOSS [training: 9.478612207656667e-06 | validation: 8.679262797057774e-06]
	TIME [epoch: 4.18 sec]
EPOCH 460/500:
	Training over batches...
		[batch 4/4] avg loss: 6.83523016890808e-06		[learning rate: 1.855e-05]
	Learning Rate: 1.85495e-05
	LOSS [training: 6.83523016890808e-06 | validation: 1.0247207041948592e-05]
	TIME [epoch: 4.19 sec]
EPOCH 461/500:
	Training over batches...
		[batch 4/4] avg loss: 5.710090786164911e-06		[learning rate: 1.8267e-05]
	Learning Rate: 1.8267e-05
	LOSS [training: 5.710090786164911e-06 | validation: -1.1687662068837757e-06]
	TIME [epoch: 4.21 sec]
EPOCH 462/500:
	Training over batches...
		[batch 4/4] avg loss: 5.936203714495992e-06		[learning rate: 1.7989e-05]
	Learning Rate: 1.79887e-05
	LOSS [training: 5.936203714495992e-06 | validation: -5.329258316675968e-06]
	TIME [epoch: 4.18 sec]
EPOCH 463/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1810052971854002e-05		[learning rate: 1.7715e-05]
	Learning Rate: 1.77147e-05
	LOSS [training: 1.1810052971854002e-05 | validation: -1.9252721755402838e-07]
	TIME [epoch: 4.18 sec]
EPOCH 464/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7524841418643521e-06		[learning rate: 1.7445e-05]
	Learning Rate: 1.74448e-05
	LOSS [training: 1.7524841418643521e-06 | validation: 6.041761865329454e-06]
	TIME [epoch: 4.18 sec]
EPOCH 465/500:
	Training over batches...
		[batch 4/4] avg loss: 6.577428592509427e-06		[learning rate: 1.7179e-05]
	Learning Rate: 1.71791e-05
	LOSS [training: 6.577428592509427e-06 | validation: -1.8025721034708974e-08]
	TIME [epoch: 4.18 sec]
EPOCH 466/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0439805201190811e-05		[learning rate: 1.6917e-05]
	Learning Rate: 1.69174e-05
	LOSS [training: 1.0439805201190811e-05 | validation: 4.715025116877048e-06]
	TIME [epoch: 4.18 sec]
EPOCH 467/500:
	Training over batches...
		[batch 4/4] avg loss: 4.928431848976955e-06		[learning rate: 1.666e-05]
	Learning Rate: 1.66597e-05
	LOSS [training: 4.928431848976955e-06 | validation: -1.1560625593554752e-05]
	TIME [epoch: 4.17 sec]
EPOCH 468/500:
	Training over batches...
		[batch 4/4] avg loss: 8.306656949130798e-06		[learning rate: 1.6406e-05]
	Learning Rate: 1.64059e-05
	LOSS [training: 8.306656949130798e-06 | validation: 1.153383064558855e-05]
	TIME [epoch: 4.17 sec]
EPOCH 469/500:
	Training over batches...
		[batch 4/4] avg loss: 5.450268158133098e-06		[learning rate: 1.6156e-05]
	Learning Rate: 1.6156e-05
	LOSS [training: 5.450268158133098e-06 | validation: 5.5235092693151075e-06]
	TIME [epoch: 4.18 sec]
EPOCH 470/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4545556284651483e-05		[learning rate: 1.591e-05]
	Learning Rate: 1.59099e-05
	LOSS [training: 1.4545556284651483e-05 | validation: -3.5483148795285577e-06]
	TIME [epoch: 4.18 sec]
EPOCH 471/500:
	Training over batches...
		[batch 4/4] avg loss: 9.63592948756875e-06		[learning rate: 1.5668e-05]
	Learning Rate: 1.56675e-05
	LOSS [training: 9.63592948756875e-06 | validation: -8.115195624868399e-06]
	TIME [epoch: 4.19 sec]
EPOCH 472/500:
	Training over batches...
		[batch 4/4] avg loss: 6.301760816050051e-06		[learning rate: 1.5429e-05]
	Learning Rate: 1.54288e-05
	LOSS [training: 6.301760816050051e-06 | validation: -3.1262275477834656e-06]
	TIME [epoch: 4.21 sec]
EPOCH 473/500:
	Training over batches...
		[batch 4/4] avg loss: 8.176076418176748e-06		[learning rate: 1.5194e-05]
	Learning Rate: 1.51938e-05
	LOSS [training: 8.176076418176748e-06 | validation: 9.032735704967983e-07]
	TIME [epoch: 4.19 sec]
EPOCH 474/500:
	Training over batches...
		[batch 4/4] avg loss: 5.547753172634895e-06		[learning rate: 1.4962e-05]
	Learning Rate: 1.49624e-05
	LOSS [training: 5.547753172634895e-06 | validation: -7.998492699954341e-07]
	TIME [epoch: 4.18 sec]
EPOCH 475/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1822554958791033e-05		[learning rate: 1.4734e-05]
	Learning Rate: 1.47344e-05
	LOSS [training: 1.1822554958791033e-05 | validation: -1.4680729289811013e-06]
	TIME [epoch: 4.17 sec]
EPOCH 476/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3356357651133854e-05		[learning rate: 1.451e-05]
	Learning Rate: 1.451e-05
	LOSS [training: 1.3356357651133854e-05 | validation: -5.837896770749307e-06]
	TIME [epoch: 4.18 sec]
EPOCH 477/500:
	Training over batches...
		[batch 4/4] avg loss: 7.5075229966820035e-06		[learning rate: 1.4289e-05]
	Learning Rate: 1.42889e-05
	LOSS [training: 7.5075229966820035e-06 | validation: 9.004149570740027e-06]
	TIME [epoch: 4.18 sec]
EPOCH 478/500:
	Training over batches...
		[batch 4/4] avg loss: 4.112775867925466e-06		[learning rate: 1.4071e-05]
	Learning Rate: 1.40713e-05
	LOSS [training: 4.112775867925466e-06 | validation: 4.993182631421567e-06]
	TIME [epoch: 4.18 sec]
EPOCH 479/500:
	Training over batches...
		[batch 4/4] avg loss: 1.015848040038081e-05		[learning rate: 1.3857e-05]
	Learning Rate: 1.38569e-05
	LOSS [training: 1.015848040038081e-05 | validation: 3.027084842814665e-06]
	TIME [epoch: 4.18 sec]
EPOCH 480/500:
	Training over batches...
		[batch 4/4] avg loss: 1.8653617409564172e-06		[learning rate: 1.3646e-05]
	Learning Rate: 1.36458e-05
	LOSS [training: 1.8653617409564172e-06 | validation: 1.0745057491889253e-05]
	TIME [epoch: 4.19 sec]
EPOCH 481/500:
	Training over batches...
		[batch 4/4] avg loss: 1.813122475496254e-06		[learning rate: 1.3438e-05]
	Learning Rate: 1.3438e-05
	LOSS [training: 1.813122475496254e-06 | validation: 5.577305380323416e-06]
	TIME [epoch: 4.19 sec]
EPOCH 482/500:
	Training over batches...
		[batch 4/4] avg loss: 7.4318482323813e-06		[learning rate: 1.3233e-05]
	Learning Rate: 1.32333e-05
	LOSS [training: 7.4318482323813e-06 | validation: 8.796886089600386e-06]
	TIME [epoch: 4.19 sec]
EPOCH 483/500:
	Training over batches...
		[batch 4/4] avg loss: 5.962386186282198e-06		[learning rate: 1.3032e-05]
	Learning Rate: 1.30317e-05
	LOSS [training: 5.962386186282198e-06 | validation: 1.20704542322716e-05]
	TIME [epoch: 4.22 sec]
EPOCH 484/500:
	Training over batches...
		[batch 4/4] avg loss: 1.252780139466214e-05		[learning rate: 1.2833e-05]
	Learning Rate: 1.28332e-05
	LOSS [training: 1.252780139466214e-05 | validation: -1.7270012085572883e-06]
	TIME [epoch: 4.2 sec]
EPOCH 485/500:
	Training over batches...
		[batch 4/4] avg loss: 6.546066761252467e-06		[learning rate: 1.2638e-05]
	Learning Rate: 1.26377e-05
	LOSS [training: 6.546066761252467e-06 | validation: -4.621699469025354e-06]
	TIME [epoch: 4.19 sec]
EPOCH 486/500:
	Training over batches...
		[batch 4/4] avg loss: 8.075763922178238e-06		[learning rate: 1.2445e-05]
	Learning Rate: 1.24451e-05
	LOSS [training: 8.075763922178238e-06 | validation: 7.296489758967572e-06]
	TIME [epoch: 4.18 sec]
EPOCH 487/500:
	Training over batches...
		[batch 4/4] avg loss: 8.729658328184397e-06		[learning rate: 1.2256e-05]
	Learning Rate: 1.22556e-05
	LOSS [training: 8.729658328184397e-06 | validation: -3.3217803443974507e-06]
	TIME [epoch: 4.19 sec]
EPOCH 488/500:
	Training over batches...
		[batch 4/4] avg loss: 9.802707689459343e-06		[learning rate: 1.2069e-05]
	Learning Rate: 1.20689e-05
	LOSS [training: 9.802707689459343e-06 | validation: -6.4252173610439565e-06]
	TIME [epoch: 4.18 sec]
EPOCH 489/500:
	Training over batches...
		[batch 4/4] avg loss: 6.045161454467985e-06		[learning rate: 1.1885e-05]
	Learning Rate: 1.1885e-05
	LOSS [training: 6.045161454467985e-06 | validation: -1.3238386135472348e-07]
	TIME [epoch: 4.19 sec]
EPOCH 490/500:
	Training over batches...
		[batch 4/4] avg loss: 8.050661907112344e-06		[learning rate: 1.1704e-05]
	Learning Rate: 1.1704e-05
	LOSS [training: 8.050661907112344e-06 | validation: -1.590406816742718e-06]
	TIME [epoch: 4.19 sec]
EPOCH 491/500:
	Training over batches...
		[batch 4/4] avg loss: 5.7037471251850656e-06		[learning rate: 1.1526e-05]
	Learning Rate: 1.15257e-05
	LOSS [training: 5.7037471251850656e-06 | validation: 7.946260528953976e-06]
	TIME [epoch: 4.19 sec]
EPOCH 492/500:
	Training over batches...
		[batch 4/4] avg loss: 7.72583785365577e-06		[learning rate: 1.135e-05]
	Learning Rate: 1.13501e-05
	LOSS [training: 7.72583785365577e-06 | validation: -5.900916307027382e-06]
	TIME [epoch: 4.18 sec]
EPOCH 493/500:
	Training over batches...
		[batch 4/4] avg loss: 6.855606273320713e-06		[learning rate: 1.1177e-05]
	Learning Rate: 1.11772e-05
	LOSS [training: 6.855606273320713e-06 | validation: -3.603760143376933e-06]
	TIME [epoch: 4.19 sec]
EPOCH 494/500:
	Training over batches...
		[batch 4/4] avg loss: 3.356538563441314e-06		[learning rate: 1.1007e-05]
	Learning Rate: 1.10069e-05
	LOSS [training: 3.356538563441314e-06 | validation: 8.512818999606654e-06]
	TIME [epoch: 4.22 sec]
EPOCH 495/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1653816410481023e-05		[learning rate: 1.0839e-05]
	Learning Rate: 1.08393e-05
	LOSS [training: 1.1653816410481023e-05 | validation: -8.444674100686501e-06]
	TIME [epoch: 4.19 sec]
EPOCH 496/500:
	Training over batches...
		[batch 4/4] avg loss: 6.755773238699136e-06		[learning rate: 1.0674e-05]
	Learning Rate: 1.06741e-05
	LOSS [training: 6.755773238699136e-06 | validation: 1.2349837133855735e-05]
	TIME [epoch: 4.18 sec]
EPOCH 497/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0757717962101675e-05		[learning rate: 1.0512e-05]
	Learning Rate: 1.05115e-05
	LOSS [training: 1.0757717962101675e-05 | validation: -6.883532073431727e-07]
	TIME [epoch: 4.17 sec]
EPOCH 498/500:
	Training over batches...
		[batch 4/4] avg loss: 2.506264040031514e-06		[learning rate: 1.0351e-05]
	Learning Rate: 1.03514e-05
	LOSS [training: 2.506264040031514e-06 | validation: 8.893429889291761e-06]
	TIME [epoch: 4.18 sec]
EPOCH 499/500:
	Training over batches...
		[batch 4/4] avg loss: 9.31703605723133e-06		[learning rate: 1.0194e-05]
	Learning Rate: 1.01937e-05
	LOSS [training: 9.31703605723133e-06 | validation: 5.714336943655418e-06]
	TIME [epoch: 4.18 sec]
EPOCH 500/500:
	Training over batches...
		[batch 4/4] avg loss: 4.9774004290937815e-06		[learning rate: 1.0038e-05]
	Learning Rate: 1.00384e-05
	LOSS [training: 4.9774004290937815e-06 | validation: 2.3647408100573977e-05]
	TIME [epoch: 4.19 sec]
Finished training in 2248.686 seconds.
