Args:
Namespace(name='model_phiq_1a_v2', outdir='out/model_training/model_phiq_1a_v2', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.01, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.75, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2282036331

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.283096277161951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.283096277161951 | validation: 6.266959909794812]
	TIME [epoch: 113 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.7649144177747065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7649144177747065 | validation: 5.844895312808811]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.426005627351654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.426005627351654 | validation: 5.378482362232381]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.022946505324423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.022946505324423 | validation: 5.066301568886827]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.831883768010438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.831883768010438 | validation: 4.84288265582059]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.679379750446186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.679379750446186 | validation: 4.766034343590192]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_6.pth
	Model improved!!!
EPOCH 7/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.528829091316998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.528829091316998 | validation: 4.807047232336687]
	TIME [epoch: 6.25 sec]
EPOCH 8/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.517626150705892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.517626150705892 | validation: 4.544598100559135]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.322628108010615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.322628108010615 | validation: 4.337684906503864]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_9.pth
	Model improved!!!
EPOCH 10/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2375870769178965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2375870769178965 | validation: 4.318664813428025]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_10.pth
	Model improved!!!
EPOCH 11/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.1570569201425265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1570569201425265 | validation: 4.237696979457611]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_11.pth
	Model improved!!!
EPOCH 12/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.038670708578179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.038670708578179 | validation: 4.019514942942362]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_12.pth
	Model improved!!!
EPOCH 13/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9813887338848755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9813887338848755 | validation: 3.869280408330246]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_13.pth
	Model improved!!!
EPOCH 14/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8878924143698583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8878924143698583 | validation: 3.8874283113633954]
	TIME [epoch: 6.23 sec]
EPOCH 15/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9074709616086114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9074709616086114 | validation: 5.084147925276092]
	TIME [epoch: 6.23 sec]
EPOCH 16/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.55368767743712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.55368767743712 | validation: 4.151260388935029]
	TIME [epoch: 6.23 sec]
EPOCH 17/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.84319133444347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.84319133444347 | validation: 3.7866659191614502]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_17.pth
	Model improved!!!
EPOCH 18/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6729647847189772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6729647847189772 | validation: 3.5989378035049837]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_18.pth
	Model improved!!!
EPOCH 19/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6252589338505405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6252589338505405 | validation: 3.415810068297366]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_19.pth
	Model improved!!!
EPOCH 20/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.491548281824862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.491548281824862 | validation: 3.319386249453121]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_20.pth
	Model improved!!!
EPOCH 21/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8719048722104743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8719048722104743 | validation: 4.723851329017737]
	TIME [epoch: 6.23 sec]
EPOCH 22/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.498204086497618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.498204086497618 | validation: 3.9946746005341884]
	TIME [epoch: 6.22 sec]
EPOCH 23/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.342727370066572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.342727370066572 | validation: 4.921425736373811]
	TIME [epoch: 6.27 sec]
EPOCH 24/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.451619872121087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.451619872121087 | validation: 4.110295863544004]
	TIME [epoch: 6.25 sec]
EPOCH 25/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.038181071153192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.038181071153192 | validation: 3.7965551599102723]
	TIME [epoch: 6.22 sec]
EPOCH 26/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7109777421504058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7109777421504058 | validation: 3.7108603628737717]
	TIME [epoch: 6.23 sec]
EPOCH 27/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7303382436116044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7303382436116044 | validation: 3.9630767393979642]
	TIME [epoch: 6.23 sec]
EPOCH 28/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.135644798825814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.135644798825814 | validation: 4.20220280113633]
	TIME [epoch: 6.23 sec]
EPOCH 29/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7511255126495415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7511255126495415 | validation: 4.080166892269394]
	TIME [epoch: 6.27 sec]
EPOCH 30/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.798440696243606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.798440696243606 | validation: 3.5888285539809948]
	TIME [epoch: 6.24 sec]
EPOCH 31/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.620588338632126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.620588338632126 | validation: 3.5327001181438478]
	TIME [epoch: 6.23 sec]
EPOCH 32/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.536271090984406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.536271090984406 | validation: 3.4466038369792398]
	TIME [epoch: 6.23 sec]
EPOCH 33/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5205223342651566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5205223342651566 | validation: 3.4069563009764954]
	TIME [epoch: 6.22 sec]
EPOCH 34/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4358594124167423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4358594124167423 | validation: 3.2996095351718986]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_34.pth
	Model improved!!!
EPOCH 35/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.352273267314204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.352273267314204 | validation: 3.142129187454125]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_35.pth
	Model improved!!!
EPOCH 36/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.196684972888364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.196684972888364 | validation: 3.487145778245634]
	TIME [epoch: 6.24 sec]
EPOCH 37/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.749678976415863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.749678976415863 | validation: 6.531672855687503]
	TIME [epoch: 6.22 sec]
EPOCH 38/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.365666829443308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.365666829443308 | validation: 4.2460021237416905]
	TIME [epoch: 6.22 sec]
EPOCH 39/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.092540993638309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.092540993638309 | validation: 3.3271539445073577]
	TIME [epoch: 6.22 sec]
EPOCH 40/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4748300731847204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4748300731847204 | validation: 3.226126220822355]
	TIME [epoch: 6.22 sec]
EPOCH 41/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.475338379027403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.475338379027403 | validation: 3.2992946124893683]
	TIME [epoch: 6.27 sec]
EPOCH 42/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4556607264555055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4556607264555055 | validation: 3.2693830444186105]
	TIME [epoch: 6.24 sec]
EPOCH 43/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.281038603400361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.281038603400361 | validation: 3.112943494878412]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_43.pth
	Model improved!!!
EPOCH 44/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1636967121992163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1636967121992163 | validation: 2.9460807066397185]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_44.pth
	Model improved!!!
EPOCH 45/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0505553968692114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0505553968692114 | validation: 3.2542952319684737]
	TIME [epoch: 6.24 sec]
EPOCH 46/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0762910392122187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0762910392122187 | validation: 2.691305662223952]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_46.pth
	Model improved!!!
EPOCH 47/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8207116762495086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8207116762495086 | validation: 3.0205765032568688]
	TIME [epoch: 6.27 sec]
EPOCH 48/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.044568420512862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.044568420512862 | validation: 2.5988511071134854]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_48.pth
	Model improved!!!
EPOCH 49/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2980801838291245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2980801838291245 | validation: 4.470941491626025]
	TIME [epoch: 6.23 sec]
EPOCH 50/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.142095383683165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.142095383683165 | validation: 2.8776112537926117]
	TIME [epoch: 6.22 sec]
EPOCH 51/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.027391907622649		[learning rate: 0.0099456]
	Learning Rate: 0.00994561
	LOSS [training: 5.027391907622649 | validation: 4.300678304916711]
	TIME [epoch: 6.22 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.669780609244804		[learning rate: 0.0098736]
	Learning Rate: 0.00987356
	LOSS [training: 4.669780609244804 | validation: 3.095902702706604]
	TIME [epoch: 6.23 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.264907210020559		[learning rate: 0.009802]
	Learning Rate: 0.00980202
	LOSS [training: 3.264907210020559 | validation: 3.0447324776738363]
	TIME [epoch: 6.27 sec]
EPOCH 54/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.101921166460266		[learning rate: 0.009731]
	Learning Rate: 0.00973101
	LOSS [training: 3.101921166460266 | validation: 2.8265574874784987]
	TIME [epoch: 6.22 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.830158868099117		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 2.830158868099117 | validation: 2.6273504560569814]
	TIME [epoch: 6.22 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7878123807439033		[learning rate: 0.0095905]
	Learning Rate: 0.00959052
	LOSS [training: 2.7878123807439033 | validation: 2.612821889657262]
	TIME [epoch: 6.22 sec]
EPOCH 57/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6348058233462663		[learning rate: 0.009521]
	Learning Rate: 0.00952104
	LOSS [training: 2.6348058233462663 | validation: 2.4538459932531174]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_57.pth
	Model improved!!!
EPOCH 58/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.611863471358396		[learning rate: 0.0094521]
	Learning Rate: 0.00945206
	LOSS [training: 3.611863471358396 | validation: 3.1180697526170063]
	TIME [epoch: 6.23 sec]
EPOCH 59/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.873501188089804		[learning rate: 0.0093836]
	Learning Rate: 0.00938358
	LOSS [training: 2.873501188089804 | validation: 3.3289788560994538]
	TIME [epoch: 6.26 sec]
EPOCH 60/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3925553893755023		[learning rate: 0.0093156]
	Learning Rate: 0.00931559
	LOSS [training: 3.3925553893755023 | validation: 2.7024967584715993]
	TIME [epoch: 6.23 sec]
EPOCH 61/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8281144453136617		[learning rate: 0.0092481]
	Learning Rate: 0.0092481
	LOSS [training: 2.8281144453136617 | validation: 2.6377754415340187]
	TIME [epoch: 6.23 sec]
EPOCH 62/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.686452296207765		[learning rate: 0.0091811]
	Learning Rate: 0.0091811
	LOSS [training: 2.686452296207765 | validation: 2.5677213164405734]
	TIME [epoch: 6.22 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6887342344350587		[learning rate: 0.0091146]
	Learning Rate: 0.00911458
	LOSS [training: 2.6887342344350587 | validation: 2.8089610934044793]
	TIME [epoch: 6.22 sec]
EPOCH 64/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8423771459726037		[learning rate: 0.0090485]
	Learning Rate: 0.00904855
	LOSS [training: 3.8423771459726037 | validation: 3.8136945539037463]
	TIME [epoch: 6.23 sec]
EPOCH 65/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9545090250289845		[learning rate: 0.008983]
	Learning Rate: 0.00898299
	LOSS [training: 3.9545090250289845 | validation: 3.5026216481613033]
	TIME [epoch: 6.26 sec]
EPOCH 66/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5936526637416435		[learning rate: 0.0089179]
	Learning Rate: 0.00891791
	LOSS [training: 3.5936526637416435 | validation: 2.8443470966767115]
	TIME [epoch: 6.23 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.80814014013361		[learning rate: 0.0088533]
	Learning Rate: 0.0088533
	LOSS [training: 2.80814014013361 | validation: 2.523118725646645]
	TIME [epoch: 6.23 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.760656182593439		[learning rate: 0.0087892]
	Learning Rate: 0.00878916
	LOSS [training: 2.760656182593439 | validation: 3.034334684879073]
	TIME [epoch: 6.23 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9020160701318964		[learning rate: 0.0087255]
	Learning Rate: 0.00872548
	LOSS [training: 2.9020160701318964 | validation: 3.4682873346598977]
	TIME [epoch: 6.22 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8111080700255373		[learning rate: 0.0086623]
	Learning Rate: 0.00866227
	LOSS [training: 2.8111080700255373 | validation: 2.595280743106086]
	TIME [epoch: 6.23 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.497505024479118		[learning rate: 0.0085995]
	Learning Rate: 0.00859951
	LOSS [training: 2.497505024479118 | validation: 2.3253695722486007]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_71.pth
	Model improved!!!
EPOCH 72/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2806836532072348		[learning rate: 0.0085372]
	Learning Rate: 0.00853721
	LOSS [training: 2.2806836532072348 | validation: 2.350298486724008]
	TIME [epoch: 6.23 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3125970941406546		[learning rate: 0.0084754]
	Learning Rate: 0.00847535
	LOSS [training: 2.3125970941406546 | validation: 2.5684907577603027]
	TIME [epoch: 6.23 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.409623542411673		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 2.409623542411673 | validation: 2.4774697908554364]
	TIME [epoch: 6.23 sec]
EPOCH 75/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.636725938006972		[learning rate: 0.008353]
	Learning Rate: 0.00835299
	LOSS [training: 2.636725938006972 | validation: 2.1534007784172102]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_75.pth
	Model improved!!!
EPOCH 76/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.103275091165747		[learning rate: 0.0082925]
	Learning Rate: 0.00829248
	LOSS [training: 2.103275091165747 | validation: 2.437530513707202]
	TIME [epoch: 6.26 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.484055590977907		[learning rate: 0.0082324]
	Learning Rate: 0.0082324
	LOSS [training: 2.484055590977907 | validation: 2.3904038013705176]
	TIME [epoch: 6.24 sec]
EPOCH 78/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.63716289132391		[learning rate: 0.0081728]
	Learning Rate: 0.00817275
	LOSS [training: 2.63716289132391 | validation: 3.9347265662025723]
	TIME [epoch: 6.24 sec]
EPOCH 79/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.932561764698078		[learning rate: 0.0081135]
	Learning Rate: 0.00811354
	LOSS [training: 2.932561764698078 | validation: 2.538247295143088]
	TIME [epoch: 6.23 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.501296538018204		[learning rate: 0.0080548]
	Learning Rate: 0.00805476
	LOSS [training: 2.501296538018204 | validation: 2.4972415020054486]
	TIME [epoch: 6.22 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3141268181272427		[learning rate: 0.0079964]
	Learning Rate: 0.0079964
	LOSS [training: 2.3141268181272427 | validation: 2.740843583217247]
	TIME [epoch: 6.23 sec]
EPOCH 82/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.378567440869332		[learning rate: 0.0079385]
	Learning Rate: 0.00793847
	LOSS [training: 2.378567440869332 | validation: 2.434388901314591]
	TIME [epoch: 6.25 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2580132785487343		[learning rate: 0.007881]
	Learning Rate: 0.00788096
	LOSS [training: 2.2580132785487343 | validation: 2.7160835131744134]
	TIME [epoch: 6.24 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9645457461781546		[learning rate: 0.0078239]
	Learning Rate: 0.00782386
	LOSS [training: 3.9645457461781546 | validation: 4.2540674338828754]
	TIME [epoch: 6.22 sec]
EPOCH 85/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.027067001784127		[learning rate: 0.0077672]
	Learning Rate: 0.00776718
	LOSS [training: 4.027067001784127 | validation: 3.4045549264036405]
	TIME [epoch: 6.21 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.173876092974865		[learning rate: 0.0077109]
	Learning Rate: 0.0077109
	LOSS [training: 3.173876092974865 | validation: 2.456426769944428]
	TIME [epoch: 6.22 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.227664015471907		[learning rate: 0.007655]
	Learning Rate: 0.00765504
	LOSS [training: 2.227664015471907 | validation: 2.068651696649445]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_87.pth
	Model improved!!!
EPOCH 88/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9590985141570794		[learning rate: 0.0075996]
	Learning Rate: 0.00759958
	LOSS [training: 1.9590985141570794 | validation: 2.1230501129882926]
	TIME [epoch: 6.26 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0621979039676837		[learning rate: 0.0075445]
	Learning Rate: 0.00754452
	LOSS [training: 2.0621979039676837 | validation: 2.1065668044930876]
	TIME [epoch: 6.25 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8753778908435854		[learning rate: 0.0074899]
	Learning Rate: 0.00748986
	LOSS [training: 1.8753778908435854 | validation: 2.477195998575258]
	TIME [epoch: 6.23 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1904230623850847		[learning rate: 0.0074356]
	Learning Rate: 0.0074356
	LOSS [training: 2.1904230623850847 | validation: 2.167877902075985]
	TIME [epoch: 6.23 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9673057658570388		[learning rate: 0.0073817]
	Learning Rate: 0.00738173
	LOSS [training: 1.9673057658570388 | validation: 2.057673208831047]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_92.pth
	Model improved!!!
EPOCH 93/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.796754244524881		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 1.796754244524881 | validation: 1.6998597158532736]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_93.pth
	Model improved!!!
EPOCH 94/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7896213120391011		[learning rate: 0.0072752]
	Learning Rate: 0.00727515
	LOSS [training: 1.7896213120391011 | validation: 1.7586348524272686]
	TIME [epoch: 6.26 sec]
EPOCH 95/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3496812222598376		[learning rate: 0.0072224]
	Learning Rate: 0.00722244
	LOSS [training: 2.3496812222598376 | validation: 3.0508582567903932]
	TIME [epoch: 6.23 sec]
EPOCH 96/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0347775903729097		[learning rate: 0.0071701]
	Learning Rate: 0.00717012
	LOSS [training: 2.0347775903729097 | validation: 2.5633559328640025]
	TIME [epoch: 6.22 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.161092013973529		[learning rate: 0.0071182]
	Learning Rate: 0.00711817
	LOSS [training: 2.161092013973529 | validation: 2.1816390860008035]
	TIME [epoch: 6.22 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8367713660231606		[learning rate: 0.0070666]
	Learning Rate: 0.0070666
	LOSS [training: 1.8367713660231606 | validation: 2.1409759167857336]
	TIME [epoch: 6.22 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.541109074464071		[learning rate: 0.0070154]
	Learning Rate: 0.0070154
	LOSS [training: 2.541109074464071 | validation: 3.815189801831025]
	TIME [epoch: 6.22 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.649289031265782		[learning rate: 0.0069646]
	Learning Rate: 0.00696458
	LOSS [training: 2.649289031265782 | validation: 3.169591227220403]
	TIME [epoch: 6.26 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7792855455362675		[learning rate: 0.0069141]
	Learning Rate: 0.00691412
	LOSS [training: 2.7792855455362675 | validation: 2.547197643169124]
	TIME [epoch: 6.23 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.54374603976472		[learning rate: 0.006864]
	Learning Rate: 0.00686403
	LOSS [training: 2.54374603976472 | validation: 2.4166181659816766]
	TIME [epoch: 6.23 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.386595319276668		[learning rate: 0.0068143]
	Learning Rate: 0.0068143
	LOSS [training: 2.386595319276668 | validation: 2.000447640055489]
	TIME [epoch: 6.23 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9044427036077778		[learning rate: 0.0067649]
	Learning Rate: 0.00676493
	LOSS [training: 1.9044427036077778 | validation: 1.8189941458608712]
	TIME [epoch: 6.22 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.948702859892133		[learning rate: 0.0067159]
	Learning Rate: 0.00671592
	LOSS [training: 1.948702859892133 | validation: 2.4442636539763876]
	TIME [epoch: 6.22 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.38581944934747		[learning rate: 0.0066673]
	Learning Rate: 0.00666726
	LOSS [training: 2.38581944934747 | validation: 2.1802937078545472]
	TIME [epoch: 6.26 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9539658344752737		[learning rate: 0.006619]
	Learning Rate: 0.00661896
	LOSS [training: 1.9539658344752737 | validation: 2.0472011682455205]
	TIME [epoch: 6.23 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.847969049886296		[learning rate: 0.006571]
	Learning Rate: 0.006571
	LOSS [training: 1.847969049886296 | validation: 1.7081346616645283]
	TIME [epoch: 6.22 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6167290664300504		[learning rate: 0.0065234]
	Learning Rate: 0.00652339
	LOSS [training: 1.6167290664300504 | validation: 1.5613927595393462]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_109.pth
	Model improved!!!
EPOCH 110/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9961172736528332		[learning rate: 0.0064761]
	Learning Rate: 0.00647613
	LOSS [training: 1.9961172736528332 | validation: 2.4722481325518646]
	TIME [epoch: 6.22 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5890639861325777		[learning rate: 0.0064292]
	Learning Rate: 0.00642921
	LOSS [training: 2.5890639861325777 | validation: 2.531004844964108]
	TIME [epoch: 6.21 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.402079170222217		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 2.402079170222217 | validation: 2.0280277441102297]
	TIME [epoch: 6.27 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.114754686579787		[learning rate: 0.0063364]
	Learning Rate: 0.00633639
	LOSS [training: 2.114754686579787 | validation: 2.6011548344719557]
	TIME [epoch: 6.24 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.225456149919978		[learning rate: 0.0062905]
	Learning Rate: 0.00629049
	LOSS [training: 2.225456149919978 | validation: 1.972506654626011]
	TIME [epoch: 6.22 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8908104531309755		[learning rate: 0.0062449]
	Learning Rate: 0.00624491
	LOSS [training: 1.8908104531309755 | validation: 1.6789797387814684]
	TIME [epoch: 6.22 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7322524325269784		[learning rate: 0.0061997]
	Learning Rate: 0.00619967
	LOSS [training: 1.7322524325269784 | validation: 2.885029543745566]
	TIME [epoch: 6.21 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9715036890067195		[learning rate: 0.0061548]
	Learning Rate: 0.00615475
	LOSS [training: 2.9715036890067195 | validation: 2.5427503543084327]
	TIME [epoch: 6.21 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1410888200234783		[learning rate: 0.0061102]
	Learning Rate: 0.00611016
	LOSS [training: 2.1410888200234783 | validation: 1.7739345453145507]
	TIME [epoch: 6.25 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7315004890897678		[learning rate: 0.0060659]
	Learning Rate: 0.00606589
	LOSS [training: 1.7315004890897678 | validation: 1.6416300008482831]
	TIME [epoch: 6.23 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6288097033773314		[learning rate: 0.0060219]
	Learning Rate: 0.00602195
	LOSS [training: 1.6288097033773314 | validation: 1.6556161318454803]
	TIME [epoch: 6.22 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.342167521054311		[learning rate: 0.0059783]
	Learning Rate: 0.00597832
	LOSS [training: 2.342167521054311 | validation: 2.200694259734382]
	TIME [epoch: 6.22 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9133357304024075		[learning rate: 0.005935]
	Learning Rate: 0.005935
	LOSS [training: 1.9133357304024075 | validation: 2.602995386254964]
	TIME [epoch: 6.22 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.903240072755018		[learning rate: 0.005892]
	Learning Rate: 0.00589201
	LOSS [training: 2.903240072755018 | validation: 3.1057976724490337]
	TIME [epoch: 6.21 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.960856600527241		[learning rate: 0.0058493]
	Learning Rate: 0.00584932
	LOSS [training: 2.960856600527241 | validation: 2.9374136410027774]
	TIME [epoch: 6.26 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7373854180313666		[learning rate: 0.0058069]
	Learning Rate: 0.00580694
	LOSS [training: 2.7373854180313666 | validation: 2.2087765334301093]
	TIME [epoch: 6.24 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0003959200250687		[learning rate: 0.0057649]
	Learning Rate: 0.00576487
	LOSS [training: 2.0003959200250687 | validation: 1.924798845914868]
	TIME [epoch: 6.22 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8241554962255861		[learning rate: 0.0057231]
	Learning Rate: 0.0057231
	LOSS [training: 1.8241554962255861 | validation: 1.6592812324586]
	TIME [epoch: 6.22 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5068167758147744		[learning rate: 0.0056816]
	Learning Rate: 0.00568164
	LOSS [training: 1.5068167758147744 | validation: 1.5788413475950687]
	TIME [epoch: 6.22 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8892282785573706		[learning rate: 0.0056405]
	Learning Rate: 0.00564048
	LOSS [training: 1.8892282785573706 | validation: 1.9719729217831072]
	TIME [epoch: 6.21 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.692213523468588		[learning rate: 0.0055996]
	Learning Rate: 0.00559961
	LOSS [training: 1.692213523468588 | validation: 2.7330324115998774]
	TIME [epoch: 6.25 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.375564287747701		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 2.375564287747701 | validation: 2.1581492135734406]
	TIME [epoch: 6.23 sec]
EPOCH 132/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7111694311756969		[learning rate: 0.0055188]
	Learning Rate: 0.00551877
	LOSS [training: 1.7111694311756969 | validation: 1.8881684817392692]
	TIME [epoch: 6.22 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9634856128427098		[learning rate: 0.0054788]
	Learning Rate: 0.00547878
	LOSS [training: 1.9634856128427098 | validation: 2.729838058332046]
	TIME [epoch: 6.22 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0453935085408137		[learning rate: 0.0054391]
	Learning Rate: 0.00543909
	LOSS [training: 2.0453935085408137 | validation: 1.7366667573369705]
	TIME [epoch: 6.22 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5329912793043492		[learning rate: 0.0053997]
	Learning Rate: 0.00539968
	LOSS [training: 1.5329912793043492 | validation: 1.69865190509387]
	TIME [epoch: 6.22 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6192070423820795		[learning rate: 0.0053606]
	Learning Rate: 0.00536056
	LOSS [training: 1.6192070423820795 | validation: 1.8843644599970224]
	TIME [epoch: 6.27 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8129665145571918		[learning rate: 0.0053217]
	Learning Rate: 0.00532173
	LOSS [training: 1.8129665145571918 | validation: 1.7387775154432399]
	TIME [epoch: 6.23 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.489037654772753		[learning rate: 0.0052832]
	Learning Rate: 0.00528317
	LOSS [training: 1.489037654772753 | validation: 1.626360894263843]
	TIME [epoch: 6.21 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9984986240209093		[learning rate: 0.0052449]
	Learning Rate: 0.0052449
	LOSS [training: 1.9984986240209093 | validation: 4.207041324423515]
	TIME [epoch: 6.21 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.37505353818861		[learning rate: 0.0052069]
	Learning Rate: 0.0052069
	LOSS [training: 3.37505353818861 | validation: 2.522076173180717]
	TIME [epoch: 6.22 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5472023010002918		[learning rate: 0.0051692]
	Learning Rate: 0.00516917
	LOSS [training: 2.5472023010002918 | validation: 2.8031926522542854]
	TIME [epoch: 6.21 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.766637209699808		[learning rate: 0.0051317]
	Learning Rate: 0.00513172
	LOSS [training: 2.766637209699808 | validation: 3.0373891657519625]
	TIME [epoch: 6.25 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.912742910843647		[learning rate: 0.0050945]
	Learning Rate: 0.00509454
	LOSS [training: 2.912742910843647 | validation: 3.0642237577387723]
	TIME [epoch: 6.23 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6107349378215146		[learning rate: 0.0050576]
	Learning Rate: 0.00505763
	LOSS [training: 2.6107349378215146 | validation: 2.0048981137748396]
	TIME [epoch: 6.23 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8649840143998357		[learning rate: 0.005021]
	Learning Rate: 0.00502099
	LOSS [training: 1.8649840143998357 | validation: 1.7237589083938876]
	TIME [epoch: 6.22 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7772457640716808		[learning rate: 0.0049846]
	Learning Rate: 0.00498461
	LOSS [training: 1.7772457640716808 | validation: 1.7642044228850091]
	TIME [epoch: 6.22 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6131695715402876		[learning rate: 0.0049485]
	Learning Rate: 0.0049485
	LOSS [training: 1.6131695715402876 | validation: 2.1027431178575284]
	TIME [epoch: 6.22 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8796412298779739		[learning rate: 0.0049126]
	Learning Rate: 0.00491265
	LOSS [training: 1.8796412298779739 | validation: 1.6943679887638865]
	TIME [epoch: 6.26 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.738475388903859		[learning rate: 0.0048771]
	Learning Rate: 0.00487706
	LOSS [training: 1.738475388903859 | validation: 1.6031591593948273]
	TIME [epoch: 6.23 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5151010179882085		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 1.5151010179882085 | validation: 1.5787368081987296]
	TIME [epoch: 6.22 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5733207283872772		[learning rate: 0.0048066]
	Learning Rate: 0.00480665
	LOSS [training: 1.5733207283872772 | validation: 2.366637248536053]
	TIME [epoch: 6.21 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6793934219460969		[learning rate: 0.0047718]
	Learning Rate: 0.00477182
	LOSS [training: 1.6793934219460969 | validation: 2.1079012422334946]
	TIME [epoch: 6.22 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.598945609504363		[learning rate: 0.0047373]
	Learning Rate: 0.00473725
	LOSS [training: 1.598945609504363 | validation: 3.5018349728634273]
	TIME [epoch: 6.22 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8947761952177826		[learning rate: 0.0047029]
	Learning Rate: 0.00470293
	LOSS [training: 2.8947761952177826 | validation: 2.8172776366133654]
	TIME [epoch: 6.25 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.676167000742333		[learning rate: 0.0046689]
	Learning Rate: 0.00466886
	LOSS [training: 2.676167000742333 | validation: 2.416974549420785]
	TIME [epoch: 6.23 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0878816801076923		[learning rate: 0.004635]
	Learning Rate: 0.00463503
	LOSS [training: 3.0878816801076923 | validation: 2.2668228944533864]
	TIME [epoch: 6.22 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8554429273430673		[learning rate: 0.0046015]
	Learning Rate: 0.00460145
	LOSS [training: 1.8554429273430673 | validation: 1.7041610140430832]
	TIME [epoch: 6.22 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6336168999049128		[learning rate: 0.0045681]
	Learning Rate: 0.00456811
	LOSS [training: 1.6336168999049128 | validation: 1.6457644393790702]
	TIME [epoch: 6.22 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5535841914897788		[learning rate: 0.004535]
	Learning Rate: 0.00453502
	LOSS [training: 1.5535841914897788 | validation: 1.544552016816199]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_159.pth
	Model improved!!!
EPOCH 160/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6050681529223574		[learning rate: 0.0045022]
	Learning Rate: 0.00450216
	LOSS [training: 1.6050681529223574 | validation: 1.476825733049242]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_160.pth
	Model improved!!!
EPOCH 161/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7914446224500795		[learning rate: 0.0044695]
	Learning Rate: 0.00446954
	LOSS [training: 1.7914446224500795 | validation: 1.6113226850327802]
	TIME [epoch: 6.24 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5624557193474988		[learning rate: 0.0044372]
	Learning Rate: 0.00443716
	LOSS [training: 1.5624557193474988 | validation: 1.5774408511768228]
	TIME [epoch: 6.22 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4819688749160398		[learning rate: 0.004405]
	Learning Rate: 0.00440501
	LOSS [training: 1.4819688749160398 | validation: 1.6767090877329922]
	TIME [epoch: 6.22 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.384768376035057		[learning rate: 0.0043731]
	Learning Rate: 0.0043731
	LOSS [training: 1.384768376035057 | validation: 1.4899150696434016]
	TIME [epoch: 6.22 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6314574631727148		[learning rate: 0.0043414]
	Learning Rate: 0.00434142
	LOSS [training: 1.6314574631727148 | validation: 2.744093767672736]
	TIME [epoch: 6.22 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7170547670770977		[learning rate: 0.00431]
	Learning Rate: 0.00430996
	LOSS [training: 1.7170547670770977 | validation: 1.3906066873020522]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_166.pth
	Model improved!!!
EPOCH 167/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8107188431880774		[learning rate: 0.0042787]
	Learning Rate: 0.00427874
	LOSS [training: 1.8107188431880774 | validation: 2.881134872784264]
	TIME [epoch: 6.23 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1939961410437547		[learning rate: 0.0042477]
	Learning Rate: 0.00424774
	LOSS [training: 2.1939961410437547 | validation: 2.982838114649499]
	TIME [epoch: 6.23 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3898426235610484		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 2.3898426235610484 | validation: 1.8250377939105207]
	TIME [epoch: 6.22 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.517740700012505		[learning rate: 0.0041864]
	Learning Rate: 0.00418641
	LOSS [training: 1.517740700012505 | validation: 1.4502457216202076]
	TIME [epoch: 6.22 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.367356757623483		[learning rate: 0.0041561]
	Learning Rate: 0.00415608
	LOSS [training: 1.367356757623483 | validation: 1.5434056289437248]
	TIME [epoch: 6.22 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5418207313434726		[learning rate: 0.004126]
	Learning Rate: 0.00412597
	LOSS [training: 1.5418207313434726 | validation: 1.700123485242305]
	TIME [epoch: 6.27 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7937341573186574		[learning rate: 0.0040961]
	Learning Rate: 0.00409608
	LOSS [training: 1.7937341573186574 | validation: 2.679259940362179]
	TIME [epoch: 6.23 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8158740518687075		[learning rate: 0.0040664]
	Learning Rate: 0.0040664
	LOSS [training: 1.8158740518687075 | validation: 1.4394497085221603]
	TIME [epoch: 6.22 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.377320440045752		[learning rate: 0.0040369]
	Learning Rate: 0.00403694
	LOSS [training: 1.377320440045752 | validation: 1.452589953914018]
	TIME [epoch: 6.22 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2682232598675545		[learning rate: 0.0040077]
	Learning Rate: 0.0040077
	LOSS [training: 1.2682232598675545 | validation: 1.4305435165495295]
	TIME [epoch: 6.23 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5507445619262805		[learning rate: 0.0039787]
	Learning Rate: 0.00397866
	LOSS [training: 1.5507445619262805 | validation: 1.9635484091623332]
	TIME [epoch: 6.23 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.820381437372359		[learning rate: 0.0039498]
	Learning Rate: 0.00394984
	LOSS [training: 1.820381437372359 | validation: 1.555166819219966]
	TIME [epoch: 6.27 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.316813212617279		[learning rate: 0.0039212]
	Learning Rate: 0.00392122
	LOSS [training: 1.316813212617279 | validation: 1.49130514287641]
	TIME [epoch: 6.23 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4306419792453153		[learning rate: 0.0038928]
	Learning Rate: 0.00389281
	LOSS [training: 1.4306419792453153 | validation: 1.8154098281631268]
	TIME [epoch: 6.23 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4095652131212175		[learning rate: 0.0038646]
	Learning Rate: 0.00386461
	LOSS [training: 1.4095652131212175 | validation: 1.3845693104291297]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_181.pth
	Model improved!!!
EPOCH 182/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.719412146573693		[learning rate: 0.0038366]
	Learning Rate: 0.00383661
	LOSS [training: 1.719412146573693 | validation: 1.6808994424329744]
	TIME [epoch: 6.22 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4141693432420057		[learning rate: 0.0038088]
	Learning Rate: 0.00380881
	LOSS [training: 1.4141693432420057 | validation: 1.790152309664696]
	TIME [epoch: 6.22 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3815502493767957		[learning rate: 0.0037812]
	Learning Rate: 0.00378122
	LOSS [training: 1.3815502493767957 | validation: 1.659512053381785]
	TIME [epoch: 6.26 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2822336697642773		[learning rate: 0.0037538]
	Learning Rate: 0.00375382
	LOSS [training: 1.2822336697642773 | validation: 1.622888219450844]
	TIME [epoch: 6.22 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3518202182826369		[learning rate: 0.0037266]
	Learning Rate: 0.00372663
	LOSS [training: 1.3518202182826369 | validation: 1.6290450861547627]
	TIME [epoch: 6.22 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5135929869324183		[learning rate: 0.0036996]
	Learning Rate: 0.00369963
	LOSS [training: 1.5135929869324183 | validation: 1.5835666156814896]
	TIME [epoch: 6.22 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2353699846596535		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 1.2353699846596535 | validation: 1.8664900949644023]
	TIME [epoch: 6.22 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3452751216070435		[learning rate: 0.0036462]
	Learning Rate: 0.00364621
	LOSS [training: 1.3452751216070435 | validation: 1.8453347387284438]
	TIME [epoch: 6.22 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.347500231259295		[learning rate: 0.0036198]
	Learning Rate: 0.0036198
	LOSS [training: 1.347500231259295 | validation: 1.3546048845872467]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phiq_1a_v2_20240503_101338/states/model_phiq_1a_v2_190.pth
	Model improved!!!
EPOCH 191/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3529898293173541		[learning rate: 0.0035936]
	Learning Rate: 0.00359357
	LOSS [training: 1.3529898293173541 | validation: 1.5512701703009348]
	TIME [epoch: 6.22 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3600306005952136		[learning rate: 0.0035675]
	Learning Rate: 0.00356754
	LOSS [training: 1.3600306005952136 | validation: 2.2996597208903014]
	TIME [epoch: 6.24 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.755535944851707		[learning rate: 0.0035417]
	Learning Rate: 0.00354169
	LOSS [training: 2.755535944851707 | validation: 3.209919687374291]
	TIME [epoch: 6.22 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0918645278253343		[learning rate: 0.003516]
	Learning Rate: 0.00351603
	LOSS [training: 3.0918645278253343 | validation: 4.230484379793218]
	TIME [epoch: 6.21 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.317432855428455		[learning rate: 0.0034906]
	Learning Rate: 0.00349056
	LOSS [training: 3.317432855428455 | validation: 3.7394363572241076]
	TIME [epoch: 6.21 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4385537985671073		[learning rate: 0.0034653]
	Learning Rate: 0.00346527
	LOSS [training: 3.4385537985671073 | validation: 3.479361705992563]
	TIME [epoch: 6.26 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0325110155950483		[learning rate: 0.0034402]
	Learning Rate: 0.00344016
	LOSS [training: 3.0325110155950483 | validation: 3.1749139784669245]
	TIME [epoch: 6.22 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6443554961801636		[learning rate: 0.0034152]
	Learning Rate: 0.00341524
	LOSS [training: 2.6443554961801636 | validation: 3.101841015469648]
	TIME [epoch: 6.21 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0567509586765262		[learning rate: 0.0033905]
	Learning Rate: 0.0033905
	LOSS [training: 3.0567509586765262 | validation: 3.5162218079759384]
	TIME [epoch: 6.21 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5600939973149135		[learning rate: 0.0033659]
	Learning Rate: 0.00336593
	LOSS [training: 3.5600939973149135 | validation: 3.3589825873007872]
	TIME [epoch: 6.21 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4044354200327582		[learning rate: 0.0033415]
	Learning Rate: 0.00334155
	LOSS [training: 3.4044354200327582 | validation: 3.452974463666358]
	TIME [epoch: 6.21 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5497272808327462		[learning rate: 0.0033173]
	Learning Rate: 0.00331734
	LOSS [training: 3.5497272808327462 | validation: 4.345006526665095]
	TIME [epoch: 6.26 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.859048705836128		[learning rate: 0.0032933]
	Learning Rate: 0.0032933
	LOSS [training: 3.859048705836128 | validation: 3.3379042307950435]
	TIME [epoch: 6.22 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.11196136325171		[learning rate: 0.0032694]
	Learning Rate: 0.00326944
	LOSS [training: 4.11196136325171 | validation: 5.0695038482514505]
	TIME [epoch: 6.21 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.324518734434029		[learning rate: 0.0032458]
	Learning Rate: 0.00324576
	LOSS [training: 4.324518734434029 | validation: 4.134927131002242]
	TIME [epoch: 6.21 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.522674935306932		[learning rate: 0.0032222]
	Learning Rate: 0.00322224
	LOSS [training: 3.522674935306932 | validation: 4.493431176007373]
	TIME [epoch: 6.21 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6322240014445675		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 3.6322240014445675 | validation: 3.848393129062826]
	TIME [epoch: 6.21 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.099552723418062		[learning rate: 0.0031757]
	Learning Rate: 0.00317572
	LOSS [training: 3.099552723418062 | validation: 2.759160965099639]
	TIME [epoch: 6.26 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8185517306661216		[learning rate: 0.0031527]
	Learning Rate: 0.00315271
	LOSS [training: 2.8185517306661216 | validation: 3.7178586913178533]
	TIME [epoch: 6.22 sec]
EPOCH 210/1000:
	Training over batches...
ERROR:
!!! UPDATED MODEL HAS NAN VALUES IN PHI.W[0] !!!
