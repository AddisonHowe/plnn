Args:
Namespace(name='model_phi2_1a_v2', outdir='out/model_training/model_phi2_1a_v2', training_data='data/training_data/data_phi2_1a/training', validation_data='data/training_data/data_phi2_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=200, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.3, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 477315097

Training model...

Saving initial model state to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.491479903451401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.491479903451401 | validation: 4.142136486686633]
	TIME [epoch: 144 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7801285754803673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7801285754803673 | validation: 2.808586451557485]
	TIME [epoch: 6.54 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.924499374654707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.924499374654707 | validation: 2.3223432360303513]
	TIME [epoch: 6.51 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6081605681835383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6081605681835383 | validation: 2.186156811177071]
	TIME [epoch: 6.49 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3829139431346027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3829139431346027 | validation: 2.228172521598093]
	TIME [epoch: 6.53 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4090852157123903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4090852157123903 | validation: 1.9159551443778708]
	TIME [epoch: 6.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.112313712532409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.112313712532409 | validation: 2.5028789334796135]
	TIME [epoch: 6.48 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.310796085289404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.310796085289404 | validation: 1.8510547035056701]
	TIME [epoch: 6.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.119058445438881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.119058445438881 | validation: 1.806685395689418]
	TIME [epoch: 6.49 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0804417763217167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0804417763217167 | validation: 1.795013595208799]
	TIME [epoch: 6.53 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9797790288620956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9797790288620956 | validation: 1.7456582513534742]
	TIME [epoch: 6.53 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9776414668046045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9776414668046045 | validation: 2.148501018798523]
	TIME [epoch: 6.5 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.042497443941908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.042497443941908 | validation: 1.7856059143981402]
	TIME [epoch: 6.49 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.12533878894826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.12533878894826 | validation: 1.8038763017503254]
	TIME [epoch: 6.48 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9718569360104794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9718569360104794 | validation: 2.0852012324197506]
	TIME [epoch: 6.49 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1210115655014063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1210115655014063 | validation: 2.0020993655617123]
	TIME [epoch: 6.52 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.010227995905101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.010227995905101 | validation: 1.6969260399019854]
	TIME [epoch: 6.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.979829772485943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.979829772485943 | validation: 1.7927808700697883]
	TIME [epoch: 6.49 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9792754101519616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9792754101519616 | validation: 2.0337217812987327]
	TIME [epoch: 6.49 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0335582395069753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0335582395069753 | validation: 1.9106684823993434]
	TIME [epoch: 6.49 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9931822387769276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9931822387769276 | validation: 1.6500445002919122]
	TIME [epoch: 6.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0150765497748084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0150765497748084 | validation: 2.391295011806182]
	TIME [epoch: 6.51 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249799305230641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.249799305230641 | validation: 1.7630418466117856]
	TIME [epoch: 6.47 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9045364990020304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9045364990020304 | validation: 1.709048166994158]
	TIME [epoch: 6.47 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.993402324683089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.993402324683089 | validation: 1.6282056975278238]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7989003662533165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7989003662533165 | validation: 1.544898479729866]
	TIME [epoch: 6.48 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0051827999896754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0051827999896754 | validation: 2.5320048806887803]
	TIME [epoch: 6.53 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.914693883919575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.914693883919575 | validation: 1.5313984317710705]
	TIME [epoch: 6.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7183145571107594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7183145571107594 | validation: 1.58551522498625]
	TIME [epoch: 6.47 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6787388405891117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6787388405891117 | validation: 1.4708199989813446]
	TIME [epoch: 6.49 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0135997116544173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0135997116544173 | validation: 1.5577772064400763]
	TIME [epoch: 6.47 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6432910640957943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6432910640957943 | validation: 1.3622482922403614]
	TIME [epoch: 6.47 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.694949506552817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.694949506552817 | validation: 3.2002309252994814]
	TIME [epoch: 6.52 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2170525957224942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2170525957224942 | validation: 1.3569952145059379]
	TIME [epoch: 6.49 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6940348413332766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6940348413332766 | validation: 1.5749508346062444]
	TIME [epoch: 6.85 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7022637241635283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7022637241635283 | validation: 1.3482103542696413]
	TIME [epoch: 6.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.75440049157952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.75440049157952 | validation: 1.2824948952736723]
	TIME [epoch: 6.51 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5450551201959926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5450551201959926 | validation: 1.3418139549113648]
	TIME [epoch: 6.54 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6494363140897939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6494363140897939 | validation: 1.722743744545689]
	TIME [epoch: 6.52 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8637561756674743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8637561756674743 | validation: 1.4529050185431336]
	TIME [epoch: 6.49 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7390358712526104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7390358712526104 | validation: 1.2863769789327701]
	TIME [epoch: 6.49 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7010004325012311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7010004325012311 | validation: 1.355070326901954]
	TIME [epoch: 6.49 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5720537015241198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5720537015241198 | validation: 1.1998966776066833]
	TIME [epoch: 6.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5366477750240248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5366477750240248 | validation: 1.4752330297082858]
	TIME [epoch: 6.55 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.66566466081116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.66566466081116 | validation: 1.229420993113166]
	TIME [epoch: 6.51 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4496332828312433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4496332828312433 | validation: 1.1264330022896396]
	TIME [epoch: 6.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.394597085393995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.394597085393995 | validation: 1.1205214460788655]
	TIME [epoch: 6.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4498315432551447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4498315432551447 | validation: 1.1589478673527507]
	TIME [epoch: 6.51 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.414908354249755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.414908354249755 | validation: 1.1125374492587345]
	TIME [epoch: 6.51 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.647800137130704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.647800137130704 | validation: 1.0912787287891055]
	TIME [epoch: 6.52 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5073404423630774		[learning rate: 0.0099811]
	Learning Rate: 0.00998112
	LOSS [training: 1.5073404423630774 | validation: 1.2303095216446986]
	TIME [epoch: 6.51 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8808692196253467		[learning rate: 0.0099576]
	Learning Rate: 0.00995758
	LOSS [training: 1.8808692196253467 | validation: 1.2077569652887448]
	TIME [epoch: 6.5 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3544707390462674		[learning rate: 0.0099341]
	Learning Rate: 0.00993409
	LOSS [training: 1.3544707390462674 | validation: 0.9949205067741459]
	TIME [epoch: 6.48 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6002570648900267		[learning rate: 0.0099107]
	Learning Rate: 0.00991066
	LOSS [training: 1.6002570648900267 | validation: 1.6497858806645178]
	TIME [epoch: 6.48 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.652073085142855		[learning rate: 0.0098873]
	Learning Rate: 0.00988728
	LOSS [training: 1.652073085142855 | validation: 1.2154601442068063]
	TIME [epoch: 6.51 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6090700049806017		[learning rate: 0.009864]
	Learning Rate: 0.00986396
	LOSS [training: 1.6090700049806017 | validation: 1.310488153819842]
	TIME [epoch: 6.49 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5179279559870567		[learning rate: 0.0098407]
	Learning Rate: 0.00984069
	LOSS [training: 1.5179279559870567 | validation: 1.1720300724004047]
	TIME [epoch: 6.49 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5538181839122598		[learning rate: 0.0098175]
	Learning Rate: 0.00981748
	LOSS [training: 1.5538181839122598 | validation: 1.0988860637762228]
	TIME [epoch: 6.48 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2618214444855007		[learning rate: 0.0097943]
	Learning Rate: 0.00979432
	LOSS [training: 1.2618214444855007 | validation: 1.8563588938396558]
	TIME [epoch: 6.48 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.561850941195396		[learning rate: 0.0097712]
	Learning Rate: 0.00977122
	LOSS [training: 1.561850941195396 | validation: 1.185578333481743]
	TIME [epoch: 6.5 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3550135318659577		[learning rate: 0.0097482]
	Learning Rate: 0.00974817
	LOSS [training: 1.3550135318659577 | validation: 1.08659157058218]
	TIME [epoch: 6.52 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3506556491133297		[learning rate: 0.0097252]
	Learning Rate: 0.00972517
	LOSS [training: 1.3506556491133297 | validation: 1.0675281563913637]
	TIME [epoch: 6.48 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.355518691816189		[learning rate: 0.0097022]
	Learning Rate: 0.00970223
	LOSS [training: 1.355518691816189 | validation: 0.9867416706253667]
	TIME [epoch: 6.47 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2531138958632397		[learning rate: 0.0096793]
	Learning Rate: 0.00967935
	LOSS [training: 1.2531138958632397 | validation: 1.0050916373224217]
	TIME [epoch: 6.5 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1405857117152784		[learning rate: 0.0096565]
	Learning Rate: 0.00965652
	LOSS [training: 1.1405857117152784 | validation: 1.058068987918973]
	TIME [epoch: 6.49 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.237564687594745		[learning rate: 0.0096337]
	Learning Rate: 0.00963374
	LOSS [training: 1.237564687594745 | validation: 1.7064447899862802]
	TIME [epoch: 6.49 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3211801499191143		[learning rate: 0.009611]
	Learning Rate: 0.00961101
	LOSS [training: 1.3211801499191143 | validation: 0.7980406551894087]
	TIME [epoch: 6.54 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0203080583149664		[learning rate: 0.0095883]
	Learning Rate: 0.00958834
	LOSS [training: 1.0203080583149664 | validation: 0.689541694181879]
	TIME [epoch: 6.48 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0511199507300233		[learning rate: 0.0095657]
	Learning Rate: 0.00956573
	LOSS [training: 1.0511199507300233 | validation: 0.7175735274655209]
	TIME [epoch: 6.47 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.834529015435925		[learning rate: 0.0095432]
	Learning Rate: 0.00954316
	LOSS [training: 1.834529015435925 | validation: 3.565053848274955]
	TIME [epoch: 6.47 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.410043747511055		[learning rate: 0.0095207]
	Learning Rate: 0.00952065
	LOSS [training: 2.410043747511055 | validation: 1.5975694817166106]
	TIME [epoch: 6.47 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.794821550294964		[learning rate: 0.0094982]
	Learning Rate: 0.00949819
	LOSS [training: 2.794821550294964 | validation: 1.460884874626477]
	TIME [epoch: 6.5 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7109497817463446		[learning rate: 0.0094758]
	Learning Rate: 0.00947579
	LOSS [training: 1.7109497817463446 | validation: 1.2573806504617164]
	TIME [epoch: 6.49 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4426930866953227		[learning rate: 0.0094534]
	Learning Rate: 0.00945344
	LOSS [training: 1.4426930866953227 | validation: 0.9230927642535839]
	TIME [epoch: 6.47 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.659710233471421		[learning rate: 0.0094311]
	Learning Rate: 0.00943114
	LOSS [training: 1.659710233471421 | validation: 2.7733614894679812]
	TIME [epoch: 6.48 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.925359024950011		[learning rate: 0.0094089]
	Learning Rate: 0.00940889
	LOSS [training: 1.925359024950011 | validation: 0.9787636805862112]
	TIME [epoch: 6.48 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1574065614786746		[learning rate: 0.0093867]
	Learning Rate: 0.0093867
	LOSS [training: 1.1574065614786746 | validation: 0.7635410884256433]
	TIME [epoch: 6.48 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.705103318783533		[learning rate: 0.0093646]
	Learning Rate: 0.00936456
	LOSS [training: 1.705103318783533 | validation: 0.8880673683400153]
	TIME [epoch: 6.52 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2092791267543819		[learning rate: 0.0093425]
	Learning Rate: 0.00934247
	LOSS [training: 1.2092791267543819 | validation: 0.7908687403803047]
	TIME [epoch: 6.48 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6363275361808927		[learning rate: 0.0093204]
	Learning Rate: 0.00932043
	LOSS [training: 1.6363275361808927 | validation: 1.9307313729313758]
	TIME [epoch: 6.47 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4334250102070143		[learning rate: 0.0092984]
	Learning Rate: 0.00929844
	LOSS [training: 1.4334250102070143 | validation: 1.8951793975911306]
	TIME [epoch: 6.47 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3884365441892854		[learning rate: 0.0092765]
	Learning Rate: 0.00927651
	LOSS [training: 1.3884365441892854 | validation: 0.7808240021522845]
	TIME [epoch: 6.47 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.173403757945995		[learning rate: 0.0092546]
	Learning Rate: 0.00925463
	LOSS [training: 1.173403757945995 | validation: 0.6691455320440607]
	TIME [epoch: 6.47 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9569265254097251		[learning rate: 0.0092328]
	Learning Rate: 0.0092328
	LOSS [training: 0.9569265254097251 | validation: 0.6568103189683192]
	TIME [epoch: 6.53 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9166624020035565		[learning rate: 0.009211]
	Learning Rate: 0.00921102
	LOSS [training: 0.9166624020035565 | validation: 0.7608848770610086]
	TIME [epoch: 6.48 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9646526268508356		[learning rate: 0.0091893]
	Learning Rate: 0.00918929
	LOSS [training: 0.9646526268508356 | validation: 0.862381638846679]
	TIME [epoch: 6.47 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3049717448709606		[learning rate: 0.0091676]
	Learning Rate: 0.00916762
	LOSS [training: 1.3049717448709606 | validation: 0.735487387802868]
	TIME [epoch: 6.47 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9943248601342924		[learning rate: 0.009146]
	Learning Rate: 0.00914599
	LOSS [training: 0.9943248601342924 | validation: 0.9420463256541884]
	TIME [epoch: 6.47 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1238278841711675		[learning rate: 0.0091244]
	Learning Rate: 0.00912442
	LOSS [training: 1.1238278841711675 | validation: 1.847029782497133]
	TIME [epoch: 6.49 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2606879514180496		[learning rate: 0.0091029]
	Learning Rate: 0.00910289
	LOSS [training: 1.2606879514180496 | validation: 0.6650423392154997]
	TIME [epoch: 6.51 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0621793164016842		[learning rate: 0.0090814]
	Learning Rate: 0.00908142
	LOSS [training: 1.0621793164016842 | validation: 0.6664791314243426]
	TIME [epoch: 6.47 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.269483438826081		[learning rate: 0.00906]
	Learning Rate: 0.00906
	LOSS [training: 1.269483438826081 | validation: 2.072210119023743]
	TIME [epoch: 6.47 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.458239471033402		[learning rate: 0.0090386]
	Learning Rate: 0.00903863
	LOSS [training: 1.458239471033402 | validation: 0.9025261149934991]
	TIME [epoch: 6.47 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.131189100764939		[learning rate: 0.0090173]
	Learning Rate: 0.00901731
	LOSS [training: 1.131189100764939 | validation: 0.8820606250684664]
	TIME [epoch: 6.47 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0222824700882909		[learning rate: 0.008996]
	Learning Rate: 0.00899604
	LOSS [training: 1.0222824700882909 | validation: 0.864638917127392]
	TIME [epoch: 6.5 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0303891374009457		[learning rate: 0.0089748]
	Learning Rate: 0.00897482
	LOSS [training: 1.0303891374009457 | validation: 0.6414013766802138]
	TIME [epoch: 6.49 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.04949947015641		[learning rate: 0.0089536]
	Learning Rate: 0.00895365
	LOSS [training: 1.04949947015641 | validation: 0.9184664232716107]
	TIME [epoch: 6.47 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9882864454348377		[learning rate: 0.0089325]
	Learning Rate: 0.00893253
	LOSS [training: 0.9882864454348377 | validation: 0.6258677984049065]
	TIME [epoch: 6.47 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9638144144667464		[learning rate: 0.0089115]
	Learning Rate: 0.00891146
	LOSS [training: 0.9638144144667464 | validation: 0.6999780950926151]
	TIME [epoch: 6.48 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9446438008748311		[learning rate: 0.0088904]
	Learning Rate: 0.00889044
	LOSS [training: 0.9446438008748311 | validation: 0.6629605476907041]
	TIME [epoch: 6.47 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0515931249614012		[learning rate: 0.0088695]
	Learning Rate: 0.00886946
	LOSS [training: 1.0515931249614012 | validation: 0.5982138561045172]
	TIME [epoch: 6.52 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9018783643106174		[learning rate: 0.0088485]
	Learning Rate: 0.00884854
	LOSS [training: 0.9018783643106174 | validation: 0.676041874695417]
	TIME [epoch: 6.5 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9616964870417108		[learning rate: 0.0088277]
	Learning Rate: 0.00882767
	LOSS [training: 0.9616964870417108 | validation: 0.6222220499998945]
	TIME [epoch: 6.49 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9294431348209095		[learning rate: 0.0088068]
	Learning Rate: 0.00880685
	LOSS [training: 0.9294431348209095 | validation: 0.6173065278931888]
	TIME [epoch: 6.49 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9599325193946161		[learning rate: 0.0087861]
	Learning Rate: 0.00878607
	LOSS [training: 0.9599325193946161 | validation: 1.2119258838529032]
	TIME [epoch: 6.48 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0464313946627384		[learning rate: 0.0087653]
	Learning Rate: 0.00876535
	LOSS [training: 1.0464313946627384 | validation: 0.6048522784374737]
	TIME [epoch: 6.49 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9416804573489166		[learning rate: 0.0087447]
	Learning Rate: 0.00874467
	LOSS [training: 0.9416804573489166 | validation: 0.7108796676853774]
	TIME [epoch: 6.53 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9230044797269755		[learning rate: 0.008724]
	Learning Rate: 0.00872405
	LOSS [training: 0.9230044797269755 | validation: 0.6013569168638107]
	TIME [epoch: 6.49 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9071023993892389		[learning rate: 0.0087035]
	Learning Rate: 0.00870347
	LOSS [training: 0.9071023993892389 | validation: 0.8115231461154351]
	TIME [epoch: 6.49 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9896477080963206		[learning rate: 0.0086829]
	Learning Rate: 0.00868294
	LOSS [training: 0.9896477080963206 | validation: 0.6290390166754827]
	TIME [epoch: 6.48 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.908448847236295		[learning rate: 0.0086625]
	Learning Rate: 0.00866246
	LOSS [training: 0.908448847236295 | validation: 0.6869450629492502]
	TIME [epoch: 6.48 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9671162523729757		[learning rate: 0.008642]
	Learning Rate: 0.00864202
	LOSS [training: 0.9671162523729757 | validation: 0.7392640901827803]
	TIME [epoch: 6.51 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0127637273856354		[learning rate: 0.0086216]
	Learning Rate: 0.00862164
	LOSS [training: 1.0127637273856354 | validation: 0.7258763319560826]
	TIME [epoch: 6.51 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.955123721469629		[learning rate: 0.0086013]
	Learning Rate: 0.0086013
	LOSS [training: 0.955123721469629 | validation: 0.7379808250161393]
	TIME [epoch: 6.48 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9997393665704525		[learning rate: 0.008581]
	Learning Rate: 0.00858101
	LOSS [training: 0.9997393665704525 | validation: 0.9271545216336543]
	TIME [epoch: 6.48 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7155168066821099		[learning rate: 0.0085608]
	Learning Rate: 0.00856077
	LOSS [training: 0.7155168066821099 | validation: 0.3972796554288526]
	TIME [epoch: 6.48 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6604749253695259		[learning rate: 0.0085406]
	Learning Rate: 0.00854058
	LOSS [training: 0.6604749253695259 | validation: 0.4113392701301499]
	TIME [epoch: 6.49 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5387762791741771		[learning rate: 0.0085204]
	Learning Rate: 0.00852043
	LOSS [training: 0.5387762791741771 | validation: 0.41290611966082]
	TIME [epoch: 6.53 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5212689113295601		[learning rate: 0.0085003]
	Learning Rate: 0.00850033
	LOSS [training: 0.5212689113295601 | validation: 0.45702638115374017]
	TIME [epoch: 6.49 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4857444596576226		[learning rate: 0.0084803]
	Learning Rate: 0.00848028
	LOSS [training: 0.4857444596576226 | validation: 0.3256393913437623]
	TIME [epoch: 6.48 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34223603268524044		[learning rate: 0.0084603]
	Learning Rate: 0.00846028
	LOSS [training: 0.34223603268524044 | validation: 0.24891825209594948]
	TIME [epoch: 6.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4213088794381415		[learning rate: 0.0084403]
	Learning Rate: 0.00844032
	LOSS [training: 0.4213088794381415 | validation: 0.4096442637823181]
	TIME [epoch: 6.49 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30819747347999404		[learning rate: 0.0084204]
	Learning Rate: 0.00842041
	LOSS [training: 0.30819747347999404 | validation: 0.23817674423742768]
	TIME [epoch: 6.49 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23753162499357577		[learning rate: 0.0084005]
	Learning Rate: 0.00840055
	LOSS [training: 0.23753162499357577 | validation: 0.22716459409771633]
	TIME [epoch: 6.52 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2842266263783857		[learning rate: 0.0083807]
	Learning Rate: 0.00838073
	LOSS [training: 0.2842266263783857 | validation: 0.17063477841580799]
	TIME [epoch: 6.49 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7508183380815114		[learning rate: 0.008361]
	Learning Rate: 0.00836096
	LOSS [training: 0.7508183380815114 | validation: 0.5382499937601621]
	TIME [epoch: 6.49 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9591925560967642		[learning rate: 0.0083412]
	Learning Rate: 0.00834124
	LOSS [training: 0.9591925560967642 | validation: 0.6153655579401268]
	TIME [epoch: 6.48 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8792684976761341		[learning rate: 0.0083216]
	Learning Rate: 0.00832157
	LOSS [training: 0.8792684976761341 | validation: 0.7750519062291137]
	TIME [epoch: 6.48 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9615234256105714		[learning rate: 0.0083019]
	Learning Rate: 0.00830194
	LOSS [training: 0.9615234256105714 | validation: 0.7129939927631882]
	TIME [epoch: 6.51 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9039373892080036		[learning rate: 0.0082824]
	Learning Rate: 0.00828236
	LOSS [training: 0.9039373892080036 | validation: 0.5677693220868597]
	TIME [epoch: 6.5 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8585851099661468		[learning rate: 0.0082628]
	Learning Rate: 0.00826282
	LOSS [training: 0.8585851099661468 | validation: 0.6234229423957744]
	TIME [epoch: 6.48 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9462773440069017		[learning rate: 0.0082433]
	Learning Rate: 0.00824333
	LOSS [training: 0.9462773440069017 | validation: 0.6409806159871796]
	TIME [epoch: 6.48 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8719830211908212		[learning rate: 0.0082239]
	Learning Rate: 0.00822388
	LOSS [training: 0.8719830211908212 | validation: 0.6316118055695263]
	TIME [epoch: 6.48 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0109594021031936		[learning rate: 0.0082045]
	Learning Rate: 0.00820448
	LOSS [training: 1.0109594021031936 | validation: 0.6942902436561897]
	TIME [epoch: 6.47 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8846045214465503		[learning rate: 0.0081851]
	Learning Rate: 0.00818513
	LOSS [training: 0.8846045214465503 | validation: 0.6289610458897404]
	TIME [epoch: 6.52 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8683695134583571		[learning rate: 0.0081658]
	Learning Rate: 0.00816582
	LOSS [training: 0.8683695134583571 | validation: 0.6094692053630012]
	TIME [epoch: 6.48 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9444895672037029		[learning rate: 0.0081466]
	Learning Rate: 0.00814656
	LOSS [training: 0.9444895672037029 | validation: 0.7688653936930352]
	TIME [epoch: 6.48 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9149563100980455		[learning rate: 0.0081273]
	Learning Rate: 0.00812735
	LOSS [training: 0.9149563100980455 | validation: 0.5704923420637255]
	TIME [epoch: 6.47 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8220943773777932		[learning rate: 0.0081082]
	Learning Rate: 0.00810817
	LOSS [training: 0.8220943773777932 | validation: 0.5839721933045453]
	TIME [epoch: 6.48 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8731772579597159		[learning rate: 0.008089]
	Learning Rate: 0.00808905
	LOSS [training: 0.8731772579597159 | validation: 0.5796618477528721]
	TIME [epoch: 6.48 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8435290274506277		[learning rate: 0.00807]
	Learning Rate: 0.00806997
	LOSS [training: 0.8435290274506277 | validation: 0.6877478610710097]
	TIME [epoch: 6.52 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9422955192627569		[learning rate: 0.0080509]
	Learning Rate: 0.00805093
	LOSS [training: 0.9422955192627569 | validation: 0.7194985088066136]
	TIME [epoch: 6.48 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8682766566164289		[learning rate: 0.0080319]
	Learning Rate: 0.00803194
	LOSS [training: 0.8682766566164289 | validation: 0.5443157457630365]
	TIME [epoch: 6.47 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8867600493602147		[learning rate: 0.008013]
	Learning Rate: 0.00801299
	LOSS [training: 0.8867600493602147 | validation: 0.5484820230422879]
	TIME [epoch: 6.48 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5447985770899543		[learning rate: 0.0079941]
	Learning Rate: 0.00799409
	LOSS [training: 0.5447985770899543 | validation: 0.37797387343637645]
	TIME [epoch: 6.48 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4249594767811937		[learning rate: 0.0079752]
	Learning Rate: 0.00797524
	LOSS [training: 1.4249594767811937 | validation: 0.3875324275271407]
	TIME [epoch: 6.55 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4533844274162153		[learning rate: 0.0079564]
	Learning Rate: 0.00795642
	LOSS [training: 1.4533844274162153 | validation: 1.4624942195001311]
	TIME [epoch: 6.52 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9027236608578187		[learning rate: 0.0079377]
	Learning Rate: 0.00793766
	LOSS [training: 0.9027236608578187 | validation: 0.8114853743003484]
	TIME [epoch: 6.48 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.620111356151726		[learning rate: 0.0079189]
	Learning Rate: 0.00791893
	LOSS [training: 0.620111356151726 | validation: 0.5343297936673231]
	TIME [epoch: 6.47 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3204381923430655		[learning rate: 0.0079003]
	Learning Rate: 0.00790025
	LOSS [training: 0.3204381923430655 | validation: 0.14531472523350092]
	TIME [epoch: 6.48 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36644690205305147		[learning rate: 0.0078816]
	Learning Rate: 0.00788162
	LOSS [training: 0.36644690205305147 | validation: 0.31681734404421613]
	TIME [epoch: 6.48 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30707872531796504		[learning rate: 0.007863]
	Learning Rate: 0.00786303
	LOSS [training: 0.30707872531796504 | validation: 0.21178690004438466]
	TIME [epoch: 6.52 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21578791888898907		[learning rate: 0.0078445]
	Learning Rate: 0.00784448
	LOSS [training: 0.21578791888898907 | validation: 0.22599518498352208]
	TIME [epoch: 6.5 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2805139314990626		[learning rate: 0.007826]
	Learning Rate: 0.00782597
	LOSS [training: 0.2805139314990626 | validation: 0.27033257875471317]
	TIME [epoch: 6.48 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2863110888040863		[learning rate: 0.0078075]
	Learning Rate: 0.00780751
	LOSS [training: 0.2863110888040863 | validation: 0.2263550172547183]
	TIME [epoch: 6.48 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2441836456674929		[learning rate: 0.0077891]
	Learning Rate: 0.0077891
	LOSS [training: 0.2441836456674929 | validation: 0.29886317988259914]
	TIME [epoch: 6.48 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2424580814823419		[learning rate: 0.0077707]
	Learning Rate: 0.00777072
	LOSS [training: 0.2424580814823419 | validation: 0.18017278685485985]
	TIME [epoch: 6.47 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20260548310787757		[learning rate: 0.0077524]
	Learning Rate: 0.00775239
	LOSS [training: 0.20260548310787757 | validation: 0.2935830761779352]
	TIME [epoch: 6.52 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24232285279165056		[learning rate: 0.0077341]
	Learning Rate: 0.00773411
	LOSS [training: 0.24232285279165056 | validation: 0.1583983681387809]
	TIME [epoch: 6.48 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24910911136281175		[learning rate: 0.0077159]
	Learning Rate: 0.00771586
	LOSS [training: 0.24910911136281175 | validation: 0.5305609390382668]
	TIME [epoch: 6.48 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.574944905759579		[learning rate: 0.0076977]
	Learning Rate: 0.00769766
	LOSS [training: 0.574944905759579 | validation: 0.27468575653662264]
	TIME [epoch: 6.47 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4570196404208307		[learning rate: 0.0076795]
	Learning Rate: 0.00767951
	LOSS [training: 0.4570196404208307 | validation: 0.5653097734213832]
	TIME [epoch: 6.48 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3648602996190454		[learning rate: 0.0076614]
	Learning Rate: 0.00766139
	LOSS [training: 0.3648602996190454 | validation: 0.20246304738065724]
	TIME [epoch: 6.48 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20782237230597142		[learning rate: 0.0076433]
	Learning Rate: 0.00764332
	LOSS [training: 0.20782237230597142 | validation: 0.23930067787616252]
	TIME [epoch: 6.52 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4163641539369037		[learning rate: 0.0076253]
	Learning Rate: 0.00762529
	LOSS [training: 0.4163641539369037 | validation: 0.39180011326330255]
	TIME [epoch: 6.48 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5324403627056663		[learning rate: 0.0076073]
	Learning Rate: 0.0076073
	LOSS [training: 0.5324403627056663 | validation: 0.18809109076930364]
	TIME [epoch: 6.47 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3068167882248688		[learning rate: 0.0075894]
	Learning Rate: 0.00758936
	LOSS [training: 0.3068167882248688 | validation: 0.25684600373043043]
	TIME [epoch: 6.48 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33492190168121777		[learning rate: 0.0075715]
	Learning Rate: 0.00757146
	LOSS [training: 0.33492190168121777 | validation: 0.49156721498928424]
	TIME [epoch: 6.48 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30528583719809343		[learning rate: 0.0075536]
	Learning Rate: 0.0075536
	LOSS [training: 0.30528583719809343 | validation: 0.13924450672452893]
	TIME [epoch: 6.49 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21985854507469424		[learning rate: 0.0075358]
	Learning Rate: 0.00753578
	LOSS [training: 0.21985854507469424 | validation: 0.20615063882004375]
	TIME [epoch: 6.52 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20584955419624543		[learning rate: 0.007518]
	Learning Rate: 0.007518
	LOSS [training: 0.20584955419624543 | validation: 0.22759937233103322]
	TIME [epoch: 6.48 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25259129072124326		[learning rate: 0.0075003]
	Learning Rate: 0.00750027
	LOSS [training: 0.25259129072124326 | validation: 0.1435559942149526]
	TIME [epoch: 6.48 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2958317152748263		[learning rate: 0.0074826]
	Learning Rate: 0.00748258
	LOSS [training: 0.2958317152748263 | validation: 0.3360500997847722]
	TIME [epoch: 6.48 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8197638068511601		[learning rate: 0.0074649]
	Learning Rate: 0.00746493
	LOSS [training: 0.8197638068511601 | validation: 0.2021237736125463]
	TIME [epoch: 6.48 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19300398358131982		[learning rate: 0.0074473]
	Learning Rate: 0.00744732
	LOSS [training: 0.19300398358131982 | validation: 0.23190846897773476]
	TIME [epoch: 6.5 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2181948919028057		[learning rate: 0.0074298]
	Learning Rate: 0.00742975
	LOSS [training: 0.2181948919028057 | validation: 0.13325638335819542]
	TIME [epoch: 6.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20606975049504456		[learning rate: 0.0074122]
	Learning Rate: 0.00741223
	LOSS [training: 0.20606975049504456 | validation: 0.2217723271384367]
	TIME [epoch: 6.48 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27346034348236614		[learning rate: 0.0073947]
	Learning Rate: 0.00739474
	LOSS [training: 0.27346034348236614 | validation: 0.1588508834683422]
	TIME [epoch: 6.48 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17547317663826895		[learning rate: 0.0073773]
	Learning Rate: 0.0073773
	LOSS [training: 0.17547317663826895 | validation: 0.17009306474740615]
	TIME [epoch: 6.47 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20030707236299108		[learning rate: 0.0073599]
	Learning Rate: 0.0073599
	LOSS [training: 0.20030707236299108 | validation: 0.1743947362287794]
	TIME [epoch: 6.48 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20949406411178892		[learning rate: 0.0073425]
	Learning Rate: 0.00734254
	LOSS [training: 0.20949406411178892 | validation: 0.11457906552291133]
	TIME [epoch: 6.53 sec]
	Saving model to: out/model_training/model_phi2_1a_v2_20240226_114313/states/model_phi2_1a_v2_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3272162951124729		[learning rate: 0.0073252]
	Learning Rate: 0.00732522
	LOSS [training: 0.3272162951124729 | validation: 0.16394437581231125]
	TIME [epoch: 6.5 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19945093405325925		[learning rate: 0.0073079]
	Learning Rate: 0.00730794
	LOSS [training: 0.19945093405325925 | validation: 0.279353371507269]
	TIME [epoch: 6.48 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2545645733879521		[learning rate: 0.0072907]
	Learning Rate: 0.0072907
	LOSS [training: 0.2545645733879521 | validation: 0.2132254976822725]
	TIME [epoch: 6.48 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20299481564923597		[learning rate: 0.0072735]
	Learning Rate: 0.0072735
	LOSS [training: 0.20299481564923597 | validation: 0.16745349540587623]
	TIME [epoch: 6.48 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20149434203766456		[learning rate: 0.0072563]
	Learning Rate: 0.00725635
	LOSS [training: 0.20149434203766456 | validation: 0.24038421900487456]
	TIME [epoch: 6.49 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25264340796160545		[learning rate: 0.0072392]
	Learning Rate: 0.00723923
	LOSS [training: 0.25264340796160545 | validation: 0.20123491560971046]
	TIME [epoch: 6.52 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19890094368749928		[learning rate: 0.0072222]
	Learning Rate: 0.00722215
	LOSS [training: 0.19890094368749928 | validation: 0.13506526711401048]
	TIME [epoch: 6.48 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2215750528190255		[learning rate: 0.0072051]
	Learning Rate: 0.00720512
	LOSS [training: 0.2215750528190255 | validation: 0.18196462355523862]
	TIME [epoch: 6.48 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1940726389642332		[learning rate: 0.0071881]
	Learning Rate: 0.00718812
	LOSS [training: 0.1940726389642332 | validation: 0.13506481450494234]
	TIME [epoch: 6.48 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23667724882513258		[learning rate: 0.0071712]
	Learning Rate: 0.00717117
	LOSS [training: 0.23667724882513258 | validation: 0.14426377541237137]
	TIME [epoch: 6.48 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2018806214317712		[learning rate: 0.0071542]
	Learning Rate: 0.00715425
	LOSS [training: 0.2018806214317712 | validation: 0.13295993457765462]
	TIME [epoch: 6.5 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19617459342192525		[learning rate: 0.0071374]
	Learning Rate: 0.00713737
	LOSS [training: 0.19617459342192525 | validation: 0.19366751304493374]
	TIME [epoch: 6.51 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19454924647870164		[learning rate: 0.0071205]
	Learning Rate: 0.00712054
	LOSS [training: 0.19454924647870164 | validation: 0.1287897258312758]
	TIME [epoch: 6.48 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1964614131332453		[learning rate: 0.0071037]
	Learning Rate: 0.00710374
	LOSS [training: 0.1964614131332453 | validation: 0.24549202386147184]
	TIME [epoch: 6.48 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1945946374749814		[learning rate: 0.007087]
	Learning Rate: 0.00708698
	LOSS [training: 0.1945946374749814 | validation: 0.2581777584087514]
	TIME [epoch: 6.48 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22013199466877684		[learning rate: 0.0070703]
	Learning Rate: 0.00707027
	LOSS [training: 0.22013199466877684 | validation: 0.14636212150411282]
	TIME [epoch: 6.48 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19928469895986972		[learning rate: 0.0070536]
	Learning Rate: 0.00705359
	LOSS [training: 0.19928469895986972 | validation: 0.2466166872417987]
	TIME [epoch: 6.52 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21585481667011114		[learning rate: 0.007037]
	Learning Rate: 0.00703695
	LOSS [training: 0.21585481667011114 | validation: 0.15904419327396135]
	TIME [epoch: 6.5 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19632870032136357		[learning rate: 0.0070204]
	Learning Rate: 0.00702035
	LOSS [training: 0.19632870032136357 | validation: 0.17837015421385005]
	TIME [epoch: 6.48 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18165099228370346		[learning rate: 0.0070038]
	Learning Rate: 0.00700379
	LOSS [training: 0.18165099228370346 | validation: 0.13560403083903122]
	TIME [epoch: 6.48 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22728906219398198		[learning rate: 0.0069873]
	Learning Rate: 0.00698727
	LOSS [training: 0.22728906219398198 | validation: 0.25391201235817173]
	TIME [epoch: 6.48 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22331305618815125		[learning rate: 0.0069708]
	Learning Rate: 0.00697079
	LOSS [training: 0.22331305618815125 | validation: 0.13793216345559556]
	TIME [epoch: 6.48 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21949095961445972		[learning rate: 0.0069543]
	Learning Rate: 0.00695435
	LOSS [training: 0.21949095961445972 | validation: 0.16953430849794318]
	TIME [epoch: 6.53 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31663609163981576		[learning rate: 0.0069379]
	Learning Rate: 0.00693794
	LOSS [training: 0.31663609163981576 | validation: 0.5963820478694731]
	TIME [epoch: 6.49 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8548442598528478		[learning rate: 0.0069216]
	Learning Rate: 0.00692158
	LOSS [training: 0.8548442598528478 | validation: 0.5626875896941123]
	TIME [epoch: 6.48 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8428708740749041		[learning rate: 0.0069053]
	Learning Rate: 0.00690525
	LOSS [training: 0.8428708740749041 | validation: 0.5948526699392123]
	TIME [epoch: 6.47 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8328376389341612		[learning rate: 0.006889]
	Learning Rate: 0.00688896
	LOSS [training: 0.8328376389341612 | validation: 0.5724729642091202]
	TIME [epoch: 6.48 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8731754022651123		[learning rate: 0.0068727]
	Learning Rate: 0.00687271
	LOSS [training: 0.8731754022651123 | validation: 0.6592143866938445]
	TIME [epoch: 6.48 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8481942572546		[learning rate: 0.0068565]
	Learning Rate: 0.0068565
	LOSS [training: 0.8481942572546 | validation: 0.6608874674811074]
	TIME [epoch: 6.53 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8908815235263434		[learning rate: 0.0068403]
	Learning Rate: 0.00684033
	LOSS [training: 0.8908815235263434 | validation: 0.5568144309531731]
	TIME [epoch: 6.48 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8365385280557576		[learning rate: 0.0068242]
	Learning Rate: 0.00682419
	LOSS [training: 0.8365385280557576 | validation: 0.5268664358518609]
	TIME [epoch: 6.48 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.807766868227788		[learning rate: 0.0068081]
	Learning Rate: 0.0068081
	LOSS [training: 0.807766868227788 | validation: 0.5537232489804116]
	TIME [epoch: 6.48 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8386494794263454		[learning rate: 0.006792]
	Learning Rate: 0.00679204
	LOSS [training: 0.8386494794263454 | validation: 0.6607717350469569]
	TIME [epoch: 6.48 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8520206936090833		[learning rate: 0.006776]
	Learning Rate: 0.00677601
	LOSS [training: 0.8520206936090833 | validation: 0.5711684470722415]
	TIME [epoch: 6.49 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8275743623827416		[learning rate: 0.00676]
	Learning Rate: 0.00676003
	LOSS [training: 0.8275743623827416 | validation: 0.6131906023165742]
	TIME [epoch: 6.52 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8420620948997068		[learning rate: 0.0067441]
	Learning Rate: 0.00674409
	LOSS [training: 0.8420620948997068 | validation: 0.5322814410740384]
	TIME [epoch: 6.48 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8050613194637103		[learning rate: 0.0067282]
	Learning Rate: 0.00672818
	LOSS [training: 0.8050613194637103 | validation: 0.5397496372349208]
	TIME [epoch: 6.48 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8478428768944314		[learning rate: 0.0067123]
	Learning Rate: 0.00671231
	LOSS [training: 0.8478428768944314 | validation: 0.5405258343974578]
	TIME [epoch: 6.48 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8839882303430546		[learning rate: 0.0066965]
	Learning Rate: 0.00669647
	LOSS [training: 0.8839882303430546 | validation: 0.6404208770654338]
	TIME [epoch: 6.48 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8507067956097188		[learning rate: 0.0066807]
	Learning Rate: 0.00668068
	LOSS [training: 0.8507067956097188 | validation: 0.536359729681198]
	TIME [epoch: 6.49 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9701833749321447		[learning rate: 0.0066649]
	Learning Rate: 0.00666492
	LOSS [training: 0.9701833749321447 | validation: 0.5679926593103928]
	TIME [epoch: 6.51 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8502820008406093		[learning rate: 0.0066492]
	Learning Rate: 0.0066492
	LOSS [training: 0.8502820008406093 | validation: 0.5421922411916582]
	TIME [epoch: 6.48 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8120055459057696		[learning rate: 0.0066335]
	Learning Rate: 0.00663351
	LOSS [training: 0.8120055459057696 | validation: 0.5191376217976211]
	TIME [epoch: 6.48 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.808157900705296		[learning rate: 0.0066179]
	Learning Rate: 0.00661787
	LOSS [training: 0.808157900705296 | validation: 0.5245899518640751]
	TIME [epoch: 6.47 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8253854543775869		[learning rate: 0.0066023]
	Learning Rate: 0.00660226
	LOSS [training: 0.8253854543775869 | validation: 0.5358612351811209]
	TIME [epoch: 6.48 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5053390064752796		[learning rate: 0.0065867]
	Learning Rate: 0.00658668
	LOSS [training: 0.5053390064752796 | validation: 0.2046852189800486]
	TIME [epoch: 6.52 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22753771588744426		[learning rate: 0.0065711]
	Learning Rate: 0.00657114
	LOSS [training: 0.22753771588744426 | validation: 0.12226391062089903]
	TIME [epoch: 6.49 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1634822684270228		[learning rate: 0.0065556]
	Learning Rate: 0.00655564
	LOSS [training: 0.1634822684270228 | validation: 0.1589966153212563]
	TIME [epoch: 6.48 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16677866822698767		[learning rate: 0.0065402]
	Learning Rate: 0.00654018
	LOSS [training: 0.16677866822698767 | validation: 0.5121625172805233]
	TIME [epoch: 6.47 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22742047391184467		[learning rate: 0.0065248]
	Learning Rate: 0.00652475
	LOSS [training: 0.22742047391184467 | validation: 0.11950010769401763]
	TIME [epoch: 6.48 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14563559041899515		[learning rate: 0.0065094]
	Learning Rate: 0.00650936
	LOSS [training: 0.14563559041899515 | validation: 0.23944245468938208]
	TIME [epoch: 6.48 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22758833458869848		[learning rate: 0.006494]
	Learning Rate: 0.00649401
	LOSS [training: 0.22758833458869848 | validation: 0.25553399841595353]
	TIME [epoch: 6.53 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18481261495910115		[learning rate: 0.0064787]
	Learning Rate: 0.00647869
	LOSS [training: 0.18481261495910115 | validation: 0.1343535545707348]
	TIME [epoch: 6.49 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25405458718169543		[learning rate: 0.0064634]
	Learning Rate: 0.00646341
	LOSS [training: 0.25405458718169543 | validation: 0.4232789819828337]
	TIME [epoch: 6.47 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7462176885654511		[learning rate: 0.0064482]
	Learning Rate: 0.00644816
	LOSS [training: 0.7462176885654511 | validation: 0.7013202156139408]
	TIME [epoch: 6.48 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8718612281958109		[learning rate: 0.006433]
	Learning Rate: 0.00643295
	LOSS [training: 0.8718612281958109 | validation: 0.5380678441452547]
	TIME [epoch: 6.48 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8351613721542052		[learning rate: 0.0064178]
	Learning Rate: 0.00641778
	LOSS [training: 0.8351613721542052 | validation: 0.6047106239889202]
	TIME [epoch: 6.49 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9265860552231084		[learning rate: 0.0064026]
	Learning Rate: 0.00640264
	LOSS [training: 0.9265860552231084 | validation: 0.5488422129076843]
	TIME [epoch: 6.51 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.790869509296939		[learning rate: 0.0063875]
	Learning Rate: 0.00638754
	LOSS [training: 0.790869509296939 | validation: 0.5530191010619299]
	TIME [epoch: 6.49 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8017016257734493		[learning rate: 0.0063725]
	Learning Rate: 0.00637247
	LOSS [training: 0.8017016257734493 | validation: 0.50783707420752]
	TIME [epoch: 6.48 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8068355668249781		[learning rate: 0.0063574]
	Learning Rate: 0.00635744
	LOSS [training: 0.8068355668249781 | validation: 0.5088699345977797]
	TIME [epoch: 6.48 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7915059010248419		[learning rate: 0.0063424]
	Learning Rate: 0.00634244
	LOSS [training: 0.7915059010248419 | validation: 0.5719087328362231]
	TIME [epoch: 6.47 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8703755901052709		[learning rate: 0.0063275]
	Learning Rate: 0.00632748
	LOSS [training: 0.8703755901052709 | validation: 0.731860276800555]
	TIME [epoch: 6.5 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9021867086797097		[learning rate: 0.0063126]
	Learning Rate: 0.00631255
	LOSS [training: 0.9021867086797097 | validation: 0.6767083458710687]
	TIME [epoch: 6.51 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0211999216703718		[learning rate: 0.0062977]
	Learning Rate: 0.00629766
	LOSS [training: 1.0211999216703718 | validation: 0.7459900784581259]
	TIME [epoch: 6.48 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9069406033718168		[learning rate: 0.0062828]
	Learning Rate: 0.00628281
	LOSS [training: 0.9069406033718168 | validation: 0.5515003160507095]
	TIME [epoch: 6.48 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8319433811540721		[learning rate: 0.006268]
	Learning Rate: 0.00626799
	LOSS [training: 0.8319433811540721 | validation: 0.534062053246552]
	TIME [epoch: 6.48 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8238087229271521		[learning rate: 0.0062532]
	Learning Rate: 0.0062532
	LOSS [training: 0.8238087229271521 | validation: 0.5571494819686804]
	TIME [epoch: 6.48 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8215823144768818		[learning rate: 0.0062385]
	Learning Rate: 0.00623845
	LOSS [training: 0.8215823144768818 | validation: 0.506532379500196]
	TIME [epoch: 6.52 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7993460367115537		[learning rate: 0.0062237]
	Learning Rate: 0.00622374
	LOSS [training: 0.7993460367115537 | validation: 0.6183898370314729]
	TIME [epoch: 6.49 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.174100607628016		[learning rate: 0.0062091]
	Learning Rate: 0.00620906
	LOSS [training: 1.174100607628016 | validation: 0.9494571902155928]
	TIME [epoch: 6.48 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.081015715172598		[learning rate: 0.0061944]
	Learning Rate: 0.00619441
	LOSS [training: 1.081015715172598 | validation: 0.8926175936038773]
	TIME [epoch: 6.47 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.074966206499358		[learning rate: 0.0061798]
	Learning Rate: 0.0061798
	LOSS [training: 1.074966206499358 | validation: 0.6181264083564717]
	TIME [epoch: 6.48 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8204036308009253		[learning rate: 0.0061652]
	Learning Rate: 0.00616522
	LOSS [training: 0.8204036308009253 | validation: 0.5241138992069286]
	TIME [epoch: 6.48 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.83081610211879		[learning rate: 0.0061507]
	Learning Rate: 0.00615068
	LOSS [training: 0.83081610211879 | validation: 0.5905825975223701]
	TIME [epoch: 6.53 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8291926993878033		[learning rate: 0.0061362]
	Learning Rate: 0.00613617
	LOSS [training: 0.8291926993878033 | validation: 0.5512099505536057]
	TIME [epoch: 6.49 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8189049902278788		[learning rate: 0.0061217]
	Learning Rate: 0.0061217
	LOSS [training: 0.8189049902278788 | validation: 0.5210682765820035]
	TIME [epoch: 6.48 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8117088719101366		[learning rate: 0.0061073]
	Learning Rate: 0.00610726
	LOSS [training: 0.8117088719101366 | validation: 0.834751128543722]
	TIME [epoch: 6.48 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9565331016771405		[learning rate: 0.0060929]
	Learning Rate: 0.00609285
	LOSS [training: 0.9565331016771405 | validation: 0.8268227443129286]
	TIME [epoch: 6.48 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8903753084647761		[learning rate: 0.0060785]
	Learning Rate: 0.00607848
	LOSS [training: 0.8903753084647761 | validation: 0.5265439610067376]
	TIME [epoch: 6.49 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8310586690707259		[learning rate: 0.0060641]
	Learning Rate: 0.00606414
	LOSS [training: 0.8310586690707259 | validation: 0.5958331674934046]
	TIME [epoch: 6.53 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8012851237900339		[learning rate: 0.0060498]
	Learning Rate: 0.00604984
	LOSS [training: 0.8012851237900339 | validation: 0.5111566477610123]
	TIME [epoch: 6.48 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7958513488135418		[learning rate: 0.0060356]
	Learning Rate: 0.00603556
	LOSS [training: 0.7958513488135418 | validation: 0.5321812422807093]
	TIME [epoch: 6.48 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8299114046482613		[learning rate: 0.0060213]
	Learning Rate: 0.00602133
	LOSS [training: 0.8299114046482613 | validation: 0.623851391861428]
	TIME [epoch: 6.48 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8257712669259002		[learning rate: 0.0060071]
	Learning Rate: 0.00600712
	LOSS [training: 0.8257712669259002 | validation: 0.5317233730212545]
	TIME [epoch: 6.48 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8148429461025687		[learning rate: 0.005993]
	Learning Rate: 0.00599296
	LOSS [training: 0.8148429461025687 | validation: 0.5334824817172905]
	TIME [epoch: 6.49 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8206054845726847		[learning rate: 0.0059788]
	Learning Rate: 0.00597882
	LOSS [training: 0.8206054845726847 | validation: 0.5202944175967552]
	TIME [epoch: 6.52 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8030081205592975		[learning rate: 0.0059647]
	Learning Rate: 0.00596472
	LOSS [training: 0.8030081205592975 | validation: 0.5078327044145441]
	TIME [epoch: 6.48 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8017597402709903		[learning rate: 0.0059506]
	Learning Rate: 0.00595065
	LOSS [training: 0.8017597402709903 | validation: 0.5436778199738634]
	TIME [epoch: 6.48 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8122644112045323		[learning rate: 0.0059366]
	Learning Rate: 0.00593661
	LOSS [training: 0.8122644112045323 | validation: 0.6996510273942123]
	TIME [epoch: 6.48 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8440094418262245		[learning rate: 0.0059226]
	Learning Rate: 0.00592261
	LOSS [training: 0.8440094418262245 | validation: 0.517482809792674]
	TIME [epoch: 6.48 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8111535409114202		[learning rate: 0.0059086]
	Learning Rate: 0.00590863
	LOSS [training: 0.8111535409114202 | validation: 0.5222856303626331]
	TIME [epoch: 6.5 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8102540319878472		[learning rate: 0.0058947]
	Learning Rate: 0.0058947
	LOSS [training: 0.8102540319878472 | validation: 0.577836133602919]
	TIME [epoch: 6.52 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8055315541575112		[learning rate: 0.0058808]
	Learning Rate: 0.00588079
	LOSS [training: 0.8055315541575112 | validation: 0.5284896427721327]
	TIME [epoch: 6.48 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8812593051663091		[learning rate: 0.0058669]
	Learning Rate: 0.00586692
	LOSS [training: 0.8812593051663091 | validation: 0.5244920624766707]
	TIME [epoch: 6.48 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8137122422341815		[learning rate: 0.0058531]
	Learning Rate: 0.00585308
	LOSS [training: 0.8137122422341815 | validation: 0.6329883592856995]
	TIME [epoch: 6.48 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9307117908774286		[learning rate: 0.0058393]
	Learning Rate: 0.00583928
	LOSS [training: 0.9307117908774286 | validation: 0.6048661512000332]
	TIME [epoch: 6.48 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8066235231825065		[learning rate: 0.0058255]
	Learning Rate: 0.0058255
	LOSS [training: 0.8066235231825065 | validation: 0.5193818453498805]
	TIME [epoch: 6.52 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8302314485785726		[learning rate: 0.0058118]
	Learning Rate: 0.00581176
	LOSS [training: 0.8302314485785726 | validation: 0.6076126071044782]
	TIME [epoch: 6.5 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8003030681162364		[learning rate: 0.0057981]
	Learning Rate: 0.00579805
	LOSS [training: 0.8003030681162364 | validation: 0.6011583984885607]
	TIME [epoch: 6.48 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8196664821997157		[learning rate: 0.0057844]
	Learning Rate: 0.00578438
	LOSS [training: 0.8196664821997157 | validation: 0.6295228647806954]
	TIME [epoch: 6.48 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8364209640091753		[learning rate: 0.0057707]
	Learning Rate: 0.00577073
	LOSS [training: 0.8364209640091753 | validation: 0.6405364622193498]
	TIME [epoch: 6.49 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8062596702482692		[learning rate: 0.0057571]
	Learning Rate: 0.00575712
	LOSS [training: 0.8062596702482692 | validation: 0.5220309874944449]
	TIME [epoch: 6.49 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7946847533765214		[learning rate: 0.0057435]
	Learning Rate: 0.00574354
	LOSS [training: 0.7946847533765214 | validation: 0.5225849179014417]
	TIME [epoch: 6.52 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8281788169113572		[learning rate: 0.00573]
	Learning Rate: 0.00572999
	LOSS [training: 0.8281788169113572 | validation: 0.5870408158932511]
	TIME [epoch: 6.48 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9050372594997629		[learning rate: 0.0057165]
	Learning Rate: 0.00571647
	LOSS [training: 0.9050372594997629 | validation: 0.5967987975166287]
	TIME [epoch: 6.48 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8555138896816906		[learning rate: 0.005703]
	Learning Rate: 0.00570299
	LOSS [training: 0.8555138896816906 | validation: 0.6144778932479411]
	TIME [epoch: 6.48 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8913548698151083		[learning rate: 0.0056895]
	Learning Rate: 0.00568954
	LOSS [training: 0.8913548698151083 | validation: 0.6618673620003546]
	TIME [epoch: 6.48 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8629929985019948		[learning rate: 0.0056761]
	Learning Rate: 0.00567612
	LOSS [training: 0.8629929985019948 | validation: 0.5284907948268704]
	TIME [epoch: 6.49 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7951399641170613		[learning rate: 0.0056627]
	Learning Rate: 0.00566273
	LOSS [training: 0.7951399641170613 | validation: 0.5203751975185514]
	TIME [epoch: 6.52 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8316985812689641		[learning rate: 0.0056494]
	Learning Rate: 0.00564937
	LOSS [training: 0.8316985812689641 | validation: 0.5603583624441586]
	TIME [epoch: 6.49 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8453660857574496		[learning rate: 0.005636]
	Learning Rate: 0.00563604
	LOSS [training: 0.8453660857574496 | validation: 0.5539587306590266]
	TIME [epoch: 6.48 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8003210669748773		[learning rate: 0.0056227]
	Learning Rate: 0.00562275
	LOSS [training: 0.8003210669748773 | validation: 0.5243703222253016]
	TIME [epoch: 6.48 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.778908703640745		[learning rate: 0.0056095]
	Learning Rate: 0.00560949
	LOSS [training: 0.778908703640745 | validation: 0.6116980868783175]
	TIME [epoch: 6.47 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8226701652243265		[learning rate: 0.0055963]
	Learning Rate: 0.00559625
	LOSS [training: 0.8226701652243265 | validation: 0.5784681970424754]
	TIME [epoch: 6.5 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8118144568376978		[learning rate: 0.0055831]
	Learning Rate: 0.00558305
	LOSS [training: 0.8118144568376978 | validation: 0.5030037670303983]
	TIME [epoch: 6.5 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7979301220146323		[learning rate: 0.0055699]
	Learning Rate: 0.00556988
	LOSS [training: 0.7979301220146323 | validation: 0.5344677212328436]
	TIME [epoch: 6.48 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.827636829647289		[learning rate: 0.0055567]
	Learning Rate: 0.00555674
	LOSS [training: 0.827636829647289 | validation: 0.5450379152411469]
	TIME [epoch: 6.47 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7926868848935411		[learning rate: 0.0055436]
	Learning Rate: 0.00554364
	LOSS [training: 0.7926868848935411 | validation: 0.5177874730672348]
	TIME [epoch: 6.48 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8293985088925246		[learning rate: 0.0055306]
	Learning Rate: 0.00553056
	LOSS [training: 0.8293985088925246 | validation: 0.5023471679233292]
	TIME [epoch: 6.47 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7931020965578939		[learning rate: 0.0055175]
	Learning Rate: 0.00551752
	LOSS [training: 0.7931020965578939 | validation: 0.5149047992262231]
	TIME [epoch: 6.5 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7986263109566929		[learning rate: 0.0055045]
	Learning Rate: 0.0055045
	LOSS [training: 0.7986263109566929 | validation: 0.4956597722890631]
	TIME [epoch: 6.48 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8045230343603136		[learning rate: 0.0054915]
	Learning Rate: 0.00549152
	LOSS [training: 0.8045230343603136 | validation: 0.6116817018528148]
	TIME [epoch: 6.47 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8485820751825465		[learning rate: 0.0054786]
	Learning Rate: 0.00547856
	LOSS [training: 0.8485820751825465 | validation: 0.5879364905051158]
	TIME [epoch: 6.46 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8627073131441637		[learning rate: 0.0054656]
	Learning Rate: 0.00546564
	LOSS [training: 0.8627073131441637 | validation: 0.5252223161952274]
	TIME [epoch: 6.47 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7956621100896939		[learning rate: 0.0054527]
	Learning Rate: 0.00545275
	LOSS [training: 0.7956621100896939 | validation: 0.5025696690387426]
	TIME [epoch: 6.46 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7777993909040303		[learning rate: 0.0054399]
	Learning Rate: 0.00543988
	LOSS [training: 0.7777993909040303 | validation: 0.5121926983790556]
	TIME [epoch: 6.51 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7840486081415823		[learning rate: 0.0054271]
	Learning Rate: 0.00542705
	LOSS [training: 0.7840486081415823 | validation: 0.5148840358291553]
	TIME [epoch: 6.47 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7778564083264494		[learning rate: 0.0054143]
	Learning Rate: 0.00541425
	LOSS [training: 0.7778564083264494 | validation: 0.5913143726405774]
	TIME [epoch: 6.47 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7888307116519329		[learning rate: 0.0054015]
	Learning Rate: 0.00540148
	LOSS [training: 0.7888307116519329 | validation: 0.5689558019701211]
	TIME [epoch: 6.47 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8126633149131706		[learning rate: 0.0053887]
	Learning Rate: 0.00538874
	LOSS [training: 0.8126633149131706 | validation: 0.5298377245428902]
	TIME [epoch: 6.48 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8254888651747686		[learning rate: 0.005376]
	Learning Rate: 0.00537603
	LOSS [training: 0.8254888651747686 | validation: 0.5037325799097394]
	TIME [epoch: 6.48 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7946646590854145		[learning rate: 0.0053633]
	Learning Rate: 0.00536335
	LOSS [training: 0.7946646590854145 | validation: 0.504969282931681]
	TIME [epoch: 6.52 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7827272067313779		[learning rate: 0.0053507]
	Learning Rate: 0.00535069
	LOSS [training: 0.7827272067313779 | validation: 0.5062027629984767]
	TIME [epoch: 6.48 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7959313880041634		[learning rate: 0.0053381]
	Learning Rate: 0.00533807
	LOSS [training: 0.7959313880041634 | validation: 0.5005743598765052]
	TIME [epoch: 6.49 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7898690428360722		[learning rate: 0.0053255]
	Learning Rate: 0.00532548
	LOSS [training: 0.7898690428360722 | validation: 0.5351215649219516]
	TIME [epoch: 6.47 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8439648616758821		[learning rate: 0.0053129]
	Learning Rate: 0.00531292
	LOSS [training: 0.8439648616758821 | validation: 0.5448909069075576]
	TIME [epoch: 6.47 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7986079845212432		[learning rate: 0.0053004]
	Learning Rate: 0.00530039
	LOSS [training: 0.7986079845212432 | validation: 0.5392717267562278]
	TIME [epoch: 6.49 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7921843865513395		[learning rate: 0.0052879]
	Learning Rate: 0.00528789
	LOSS [training: 0.7921843865513395 | validation: 0.5127828455951748]
	TIME [epoch: 6.52 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0075548725466832		[learning rate: 0.0052754]
	Learning Rate: 0.00527541
	LOSS [training: 1.0075548725466832 | validation: 0.529133526659467]
	TIME [epoch: 6.47 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7827039556116774		[learning rate: 0.005263]
	Learning Rate: 0.00526297
	LOSS [training: 0.7827039556116774 | validation: 0.5264103546379587]
	TIME [epoch: 6.47 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7801607417376275		[learning rate: 0.0052506]
	Learning Rate: 0.00525055
	LOSS [training: 0.7801607417376275 | validation: 0.4990750766973966]
	TIME [epoch: 6.47 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7673009817188419		[learning rate: 0.0052382]
	Learning Rate: 0.00523817
	LOSS [training: 0.7673009817188419 | validation: 0.4808929312227272]
	TIME [epoch: 6.48 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8316274088386552		[learning rate: 0.0052258]
	Learning Rate: 0.00522581
	LOSS [training: 0.8316274088386552 | validation: 0.500255344352657]
	TIME [epoch: 6.51 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7860068376896051		[learning rate: 0.0052135]
	Learning Rate: 0.00521349
	LOSS [training: 0.7860068376896051 | validation: 0.5203637396460296]
	TIME [epoch: 6.51 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7953765782224456		[learning rate: 0.0052012]
	Learning Rate: 0.00520119
	LOSS [training: 0.7953765782224456 | validation: 0.6021604718712602]
	TIME [epoch: 6.48 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7985222923195615		[learning rate: 0.0051889]
	Learning Rate: 0.00518892
	LOSS [training: 0.7985222923195615 | validation: 0.5043834161374209]
	TIME [epoch: 6.49 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7886345583473309		[learning rate: 0.0051767]
	Learning Rate: 0.00517668
	LOSS [training: 0.7886345583473309 | validation: 0.5746372968355109]
	TIME [epoch: 6.49 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7861062629282446		[learning rate: 0.0051645]
	Learning Rate: 0.00516447
	LOSS [training: 0.7861062629282446 | validation: 0.5228996427904167]
	TIME [epoch: 6.49 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8089660184143848		[learning rate: 0.0051523]
	Learning Rate: 0.00515229
	LOSS [training: 0.8089660184143848 | validation: 0.5457521640564152]
	TIME [epoch: 6.52 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8077035345394528		[learning rate: 0.0051401]
	Learning Rate: 0.00514013
	LOSS [training: 0.8077035345394528 | validation: 0.5391062332829837]
	TIME [epoch: 6.5 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.933012323059286		[learning rate: 0.005128]
	Learning Rate: 0.00512801
	LOSS [training: 0.933012323059286 | validation: 0.5940201995535159]
	TIME [epoch: 6.48 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9706350534452113		[learning rate: 0.0051159]
	Learning Rate: 0.00511591
	LOSS [training: 0.9706350534452113 | validation: 0.5294460977039857]
	TIME [epoch: 6.49 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7958035803160424		[learning rate: 0.0051038]
	Learning Rate: 0.00510384
	LOSS [training: 0.7958035803160424 | validation: 0.725413532995626]
	TIME [epoch: 6.49 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.85498075447231		[learning rate: 0.0050918]
	Learning Rate: 0.00509181
	LOSS [training: 0.85498075447231 | validation: 0.5202281705758769]
	TIME [epoch: 6.49 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9513522925652016		[learning rate: 0.0050798]
	Learning Rate: 0.00507979
	LOSS [training: 0.9513522925652016 | validation: 0.5918068569352515]
	TIME [epoch: 6.54 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7949553577074264		[learning rate: 0.0050678]
	Learning Rate: 0.00506781
	LOSS [training: 0.7949553577074264 | validation: 0.4975951299093618]
	TIME [epoch: 6.5 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8211878198386369		[learning rate: 0.0050559]
	Learning Rate: 0.00505586
	LOSS [training: 0.8211878198386369 | validation: 0.5221149930925731]
	TIME [epoch: 6.49 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7685359566864172		[learning rate: 0.0050439]
	Learning Rate: 0.00504393
	LOSS [training: 0.7685359566864172 | validation: 0.4994735387795614]
	TIME [epoch: 6.49 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7710777425745843		[learning rate: 0.005032]
	Learning Rate: 0.00503203
	LOSS [training: 0.7710777425745843 | validation: 0.5366614908872064]
	TIME [epoch: 6.48 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7924166042488722		[learning rate: 0.0050202]
	Learning Rate: 0.00502016
	LOSS [training: 0.7924166042488722 | validation: 0.5622177145643427]
	TIME [epoch: 6.5 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7983630369642927		[learning rate: 0.0050083]
	Learning Rate: 0.00500832
	LOSS [training: 0.7983630369642927 | validation: 0.5293293989648911]
	TIME [epoch: 6.52 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7792681811756814		[learning rate: 0.0049965]
	Learning Rate: 0.00499651
	LOSS [training: 0.7792681811756814 | validation: 0.4948433529735961]
	TIME [epoch: 6.48 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8007157022961803		[learning rate: 0.0049847]
	Learning Rate: 0.00498472
	LOSS [training: 0.8007157022961803 | validation: 0.5037538352524271]
	TIME [epoch: 6.47 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.795077701356904		[learning rate: 0.004973]
	Learning Rate: 0.00497296
	LOSS [training: 0.795077701356904 | validation: 0.49819058635470853]
	TIME [epoch: 6.48 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7808609552177119		[learning rate: 0.0049612]
	Learning Rate: 0.00496123
	LOSS [training: 0.7808609552177119 | validation: 0.5219599061185883]
	TIME [epoch: 6.47 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8484713034418565		[learning rate: 0.0049495]
	Learning Rate: 0.00494953
	LOSS [training: 0.8484713034418565 | validation: 0.6449128749817024]
	TIME [epoch: 6.5 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8584707231004401		[learning rate: 0.0049379]
	Learning Rate: 0.00493786
	LOSS [training: 0.8584707231004401 | validation: 0.663415558292202]
	TIME [epoch: 6.51 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.849779094444106		[learning rate: 0.0049262]
	Learning Rate: 0.00492621
	LOSS [training: 0.849779094444106 | validation: 0.5002043385223935]
	TIME [epoch: 6.47 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7925613959982162		[learning rate: 0.0049146]
	Learning Rate: 0.00491459
	LOSS [training: 0.7925613959982162 | validation: 0.4861763434933049]
	TIME [epoch: 6.47 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.777508011718467		[learning rate: 0.004903]
	Learning Rate: 0.004903
	LOSS [training: 0.777508011718467 | validation: 0.4882428341402251]
	TIME [epoch: 6.47 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7761169694161436		[learning rate: 0.0048914]
	Learning Rate: 0.00489143
	LOSS [training: 0.7761169694161436 | validation: 0.5353116171112638]
	TIME [epoch: 6.47 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7732327986765128		[learning rate: 0.0048799]
	Learning Rate: 0.00487989
	LOSS [training: 0.7732327986765128 | validation: 0.5094120520185264]
	TIME [epoch: 6.5 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7735646272285222		[learning rate: 0.0048684]
	Learning Rate: 0.00486838
	LOSS [training: 0.7735646272285222 | validation: 0.5218621127069147]
	TIME [epoch: 6.5 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7902975309246065		[learning rate: 0.0048569]
	Learning Rate: 0.0048569
	LOSS [training: 0.7902975309246065 | validation: 0.4996925837447888]
	TIME [epoch: 6.48 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7505149797746474		[learning rate: 0.0048454]
	Learning Rate: 0.00484544
	LOSS [training: 0.7505149797746474 | validation: 0.5311041934301307]
	TIME [epoch: 6.48 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7774634430281623		[learning rate: 0.004834]
	Learning Rate: 0.00483401
	LOSS [training: 0.7774634430281623 | validation: 0.4891210430983619]
	TIME [epoch: 6.48 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7757650524093409		[learning rate: 0.0048226]
	Learning Rate: 0.00482261
	LOSS [training: 0.7757650524093409 | validation: 0.5844856091035738]
	TIME [epoch: 6.47 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7748285792224758		[learning rate: 0.0048112]
	Learning Rate: 0.00481123
	LOSS [training: 0.7748285792224758 | validation: 0.4745463929600554]
	TIME [epoch: 6.52 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7692243719713938		[learning rate: 0.0047999]
	Learning Rate: 0.00479988
	LOSS [training: 0.7692243719713938 | validation: 0.4946529095612606]
	TIME [epoch: 6.48 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8008246473182856		[learning rate: 0.0047886]
	Learning Rate: 0.00478856
	LOSS [training: 0.8008246473182856 | validation: 0.47823244901249407]
	TIME [epoch: 6.47 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8512716630026329		[learning rate: 0.0047773]
	Learning Rate: 0.00477727
	LOSS [training: 0.8512716630026329 | validation: 0.5913849103903259]
	TIME [epoch: 6.48 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8945412669103027		[learning rate: 0.004766]
	Learning Rate: 0.004766
	LOSS [training: 0.8945412669103027 | validation: 0.6859497508586678]
	TIME [epoch: 6.48 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8587056696933558		[learning rate: 0.0047548]
	Learning Rate: 0.00475476
	LOSS [training: 0.8587056696933558 | validation: 0.5299953159721086]
	TIME [epoch: 6.49 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7678786997575452		[learning rate: 0.0047435]
	Learning Rate: 0.00474354
	LOSS [training: 0.7678786997575452 | validation: 0.5009542761507007]
	TIME [epoch: 6.52 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.762402136110153		[learning rate: 0.0047324]
	Learning Rate: 0.00473235
	LOSS [training: 0.762402136110153 | validation: 0.5175937497717829]
	TIME [epoch: 6.49 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7794930755998993		[learning rate: 0.0047212]
	Learning Rate: 0.00472119
	LOSS [training: 0.7794930755998993 | validation: 0.4957419969272676]
	TIME [epoch: 6.48 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7802279575703994		[learning rate: 0.0047101]
	Learning Rate: 0.00471005
	LOSS [training: 0.7802279575703994 | validation: 0.6606023054858086]
	TIME [epoch: 6.49 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8002696012632736		[learning rate: 0.0046989]
	Learning Rate: 0.00469894
	LOSS [training: 0.8002696012632736 | validation: 0.49316142061622203]
	TIME [epoch: 6.49 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7707893051275354		[learning rate: 0.0046879]
	Learning Rate: 0.00468786
	LOSS [training: 0.7707893051275354 | validation: 0.47297863321381056]
	TIME [epoch: 6.51 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7919693805614203		[learning rate: 0.0046768]
	Learning Rate: 0.0046768
	LOSS [training: 0.7919693805614203 | validation: 0.64279550965815]
	TIME [epoch: 6.53 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.183133768922274		[learning rate: 0.0046658]
	Learning Rate: 0.00466577
	LOSS [training: 1.183133768922274 | validation: 0.5470257948032303]
	TIME [epoch: 6.5 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8070377120661913		[learning rate: 0.0046548]
	Learning Rate: 0.00465476
	LOSS [training: 0.8070377120661913 | validation: 0.6408668982016499]
	TIME [epoch: 6.49 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8070172191591076		[learning rate: 0.0046438]
	Learning Rate: 0.00464378
	LOSS [training: 0.8070172191591076 | validation: 0.48266165245345516]
	TIME [epoch: 6.48 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.765034045842359		[learning rate: 0.0046328]
	Learning Rate: 0.00463283
	LOSS [training: 0.765034045842359 | validation: 0.4611526643273267]
	TIME [epoch: 6.49 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7601860647023022		[learning rate: 0.0046219]
	Learning Rate: 0.0046219
	LOSS [training: 0.7601860647023022 | validation: 0.4735581093147172]
	TIME [epoch: 6.52 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.79209345427694		[learning rate: 0.004611]
	Learning Rate: 0.004611
	LOSS [training: 0.79209345427694 | validation: 0.4811945402851421]
	TIME [epoch: 6.49 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7632204692041928		[learning rate: 0.0046001]
	Learning Rate: 0.00460012
	LOSS [training: 0.7632204692041928 | validation: 0.4931191881946188]
	TIME [epoch: 6.48 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7576073396278643		[learning rate: 0.0045893]
	Learning Rate: 0.00458927
	LOSS [training: 0.7576073396278643 | validation: 0.524836366274227]
	TIME [epoch: 6.47 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7842111981345125		[learning rate: 0.0045784]
	Learning Rate: 0.00457844
	LOSS [training: 0.7842111981345125 | validation: 0.5131881172621101]
	TIME [epoch: 6.47 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7908352097955944		[learning rate: 0.0045676]
	Learning Rate: 0.00456765
	LOSS [training: 0.7908352097955944 | validation: 0.49841964021234064]
	TIME [epoch: 6.47 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7895159088238463		[learning rate: 0.0045569]
	Learning Rate: 0.00455687
	LOSS [training: 0.7895159088238463 | validation: 0.5300739973560645]
	TIME [epoch: 6.52 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8074079581597893		[learning rate: 0.0045461]
	Learning Rate: 0.00454612
	LOSS [training: 0.8074079581597893 | validation: 0.7694874079702785]
	TIME [epoch: 6.48 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9936248478634683		[learning rate: 0.0045354]
	Learning Rate: 0.0045354
	LOSS [training: 0.9936248478634683 | validation: 0.5256409071939492]
	TIME [epoch: 6.48 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7809127639077368		[learning rate: 0.0045247]
	Learning Rate: 0.0045247
	LOSS [training: 0.7809127639077368 | validation: 0.4671316902259557]
	TIME [epoch: 6.48 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7892639848484149		[learning rate: 0.004514]
	Learning Rate: 0.00451403
	LOSS [training: 0.7892639848484149 | validation: 0.4710548361280055]
	TIME [epoch: 6.47 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8129919059603676		[learning rate: 0.0045034]
	Learning Rate: 0.00450338
	LOSS [training: 0.8129919059603676 | validation: 0.48705515053778814]
	TIME [epoch: 6.48 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.854955069020012		[learning rate: 0.0044928]
	Learning Rate: 0.00449276
	LOSS [training: 0.854955069020012 | validation: 0.6589008210894999]
	TIME [epoch: 6.53 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8192824364375397		[learning rate: 0.0044822]
	Learning Rate: 0.00448216
	LOSS [training: 0.8192824364375397 | validation: 0.48275579931865015]
	TIME [epoch: 6.48 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7669162377249413		[learning rate: 0.0044716]
	Learning Rate: 0.00447159
	LOSS [training: 0.7669162377249413 | validation: 0.5127020059202761]
	TIME [epoch: 6.47 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7523720002043017		[learning rate: 0.004461]
	Learning Rate: 0.00446104
	LOSS [training: 0.7523720002043017 | validation: 0.48968037447602575]
	TIME [epoch: 6.47 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7368124539662011		[learning rate: 0.0044505]
	Learning Rate: 0.00445051
	LOSS [training: 0.7368124539662011 | validation: 0.4918992537504441]
	TIME [epoch: 6.47 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7411678446877933		[learning rate: 0.00444]
	Learning Rate: 0.00444002
	LOSS [training: 0.7411678446877933 | validation: 0.455121298989526]
	TIME [epoch: 6.48 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8180347988083053		[learning rate: 0.0044295]
	Learning Rate: 0.00442954
	LOSS [training: 0.8180347988083053 | validation: 0.487457292598176]
	TIME [epoch: 6.51 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7493444989472231		[learning rate: 0.0044191]
	Learning Rate: 0.00441909
	LOSS [training: 0.7493444989472231 | validation: 0.6238338279844755]
	TIME [epoch: 6.48 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8838789095494155		[learning rate: 0.0044087]
	Learning Rate: 0.00440867
	LOSS [training: 0.8838789095494155 | validation: 0.5305216595475122]
	TIME [epoch: 6.48 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8004075216021475		[learning rate: 0.0043983]
	Learning Rate: 0.00439827
	LOSS [training: 0.8004075216021475 | validation: 0.6365758754491853]
	TIME [epoch: 6.48 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7770933469994215		[learning rate: 0.0043879]
	Learning Rate: 0.0043879
	LOSS [training: 0.7770933469994215 | validation: 0.4659670404576754]
	TIME [epoch: 6.47 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7301056276669461		[learning rate: 0.0043775]
	Learning Rate: 0.00437755
	LOSS [training: 0.7301056276669461 | validation: 0.48703477787827854]
	TIME [epoch: 6.48 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7481140453132469		[learning rate: 0.0043672]
	Learning Rate: 0.00436722
	LOSS [training: 0.7481140453132469 | validation: 0.4835131737817663]
	TIME [epoch: 6.51 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7383301111289526		[learning rate: 0.0043569]
	Learning Rate: 0.00435692
	LOSS [training: 0.7383301111289526 | validation: 0.47479546134987927]
	TIME [epoch: 6.48 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.737824161463097		[learning rate: 0.0043466]
	Learning Rate: 0.00434664
	LOSS [training: 0.737824161463097 | validation: 0.45828423594388046]
	TIME [epoch: 6.47 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7288212000443168		[learning rate: 0.0043364]
	Learning Rate: 0.00433639
	LOSS [training: 0.7288212000443168 | validation: 0.4603818828248015]
	TIME [epoch: 6.47 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7437124595740909		[learning rate: 0.0043262]
	Learning Rate: 0.00432616
	LOSS [training: 0.7437124595740909 | validation: 0.5220974614299004]
	TIME [epoch: 6.48 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9435717815349871		[learning rate: 0.004316]
	Learning Rate: 0.00431596
	LOSS [training: 0.9435717815349871 | validation: 0.8301990754400776]
	TIME [epoch: 6.52 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0851840773705745		[learning rate: 0.0043058]
	Learning Rate: 0.00430577
	LOSS [training: 1.0851840773705745 | validation: 0.5747680314126253]
	TIME [epoch: 6.5 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.767988015837594		[learning rate: 0.0042956]
	Learning Rate: 0.00429562
	LOSS [training: 0.767988015837594 | validation: 0.5189020553347435]
	TIME [epoch: 6.48 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7443892025429046		[learning rate: 0.0042855]
	Learning Rate: 0.00428549
	LOSS [training: 0.7443892025429046 | validation: 0.4643894716680502]
	TIME [epoch: 6.48 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7430685497510894		[learning rate: 0.0042754]
	Learning Rate: 0.00427538
	LOSS [training: 0.7430685497510894 | validation: 0.5061747642449606]
	TIME [epoch: 6.48 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7755413714355492		[learning rate: 0.0042653]
	Learning Rate: 0.00426529
	LOSS [training: 0.7755413714355492 | validation: 0.493718728258386]
	TIME [epoch: 6.49 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.780427714181299		[learning rate: 0.0042552]
	Learning Rate: 0.00425523
	LOSS [training: 0.780427714181299 | validation: 0.5108625935333543]
	TIME [epoch: 6.53 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7615905535753397		[learning rate: 0.0042452]
	Learning Rate: 0.00424519
	LOSS [training: 0.7615905535753397 | validation: 0.5280853113666408]
	TIME [epoch: 6.5 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.776055432849353		[learning rate: 0.0042352]
	Learning Rate: 0.00423518
	LOSS [training: 0.776055432849353 | validation: 0.4933898029517983]
	TIME [epoch: 6.49 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7564508244319089		[learning rate: 0.0042252]
	Learning Rate: 0.00422519
	LOSS [training: 0.7564508244319089 | validation: 0.4821538986051484]
	TIME [epoch: 6.48 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7996847838797296		[learning rate: 0.0042152]
	Learning Rate: 0.00421522
	LOSS [training: 0.7996847838797296 | validation: 0.6711671767762927]
	TIME [epoch: 6.48 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1149103567765404		[learning rate: 0.0042053]
	Learning Rate: 0.00420528
	LOSS [training: 1.1149103567765404 | validation: 1.032919462935095]
	TIME [epoch: 6.5 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1803541284333199		[learning rate: 0.0041954]
	Learning Rate: 0.00419536
	LOSS [training: 1.1803541284333199 | validation: 0.708461473962904]
	TIME [epoch: 6.52 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8692907757569494		[learning rate: 0.0041855]
	Learning Rate: 0.00418546
	LOSS [training: 0.8692907757569494 | validation: 0.7084080530365284]
	TIME [epoch: 6.47 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9390252011345328		[learning rate: 0.0041756]
	Learning Rate: 0.00417559
	LOSS [training: 0.9390252011345328 | validation: 0.5774101778837758]
	TIME [epoch: 6.47 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7788626049348548		[learning rate: 0.0041657]
	Learning Rate: 0.00416574
	LOSS [training: 0.7788626049348548 | validation: 0.49405611430766627]
	TIME [epoch: 6.47 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7588766141834065		[learning rate: 0.0041559]
	Learning Rate: 0.00415592
	LOSS [training: 0.7588766141834065 | validation: 0.47643327835326216]
	TIME [epoch: 6.47 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7548482145520267		[learning rate: 0.0041461]
	Learning Rate: 0.00414611
	LOSS [training: 0.7548482145520267 | validation: 0.5585102135986795]
	TIME [epoch: 6.49 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8129556857203069		[learning rate: 0.0041363]
	Learning Rate: 0.00413633
	LOSS [training: 0.8129556857203069 | validation: 0.4829800367029947]
	TIME [epoch: 6.5 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7385633560423432		[learning rate: 0.0041266]
	Learning Rate: 0.00412657
	LOSS [training: 0.7385633560423432 | validation: 0.4527015906118922]
	TIME [epoch: 6.48 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7225158200797764		[learning rate: 0.0041168]
	Learning Rate: 0.00411684
	LOSS [training: 0.7225158200797764 | validation: 0.5022767987322846]
	TIME [epoch: 6.47 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7630608617050327		[learning rate: 0.0041071]
	Learning Rate: 0.00410713
	LOSS [training: 0.7630608617050327 | validation: 0.5335494331412215]
	TIME [epoch: 6.48 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7554048804344383		[learning rate: 0.0040974]
	Learning Rate: 0.00409744
	LOSS [training: 0.7554048804344383 | validation: 0.459116808204462]
	TIME [epoch: 6.47 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7522350181349611		[learning rate: 0.0040878]
	Learning Rate: 0.00408778
	LOSS [training: 0.7522350181349611 | validation: 0.48158740171998027]
	TIME [epoch: 6.52 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7372023913619734		[learning rate: 0.0040781]
	Learning Rate: 0.00407813
	LOSS [training: 0.7372023913619734 | validation: 0.4571731864915785]
	TIME [epoch: 6.49 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7329675152691981		[learning rate: 0.0040685]
	Learning Rate: 0.00406851
	LOSS [training: 0.7329675152691981 | validation: 0.5330640837915165]
	TIME [epoch: 6.47 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7543332536518675		[learning rate: 0.0040589]
	Learning Rate: 0.00405892
	LOSS [training: 0.7543332536518675 | validation: 0.4528855870917039]
	TIME [epoch: 6.47 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7177955903802691		[learning rate: 0.0040493]
	Learning Rate: 0.00404934
	LOSS [training: 0.7177955903802691 | validation: 0.4821281760675774]
	TIME [epoch: 6.47 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7483510090911263		[learning rate: 0.0040398]
	Learning Rate: 0.00403979
	LOSS [training: 0.7483510090911263 | validation: 0.45936839525972367]
	TIME [epoch: 6.48 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.741973600691664		[learning rate: 0.0040303]
	Learning Rate: 0.00403026
	LOSS [training: 0.741973600691664 | validation: 0.4553360060858418]
	TIME [epoch: 6.51 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7576093522779436		[learning rate: 0.0040208]
	Learning Rate: 0.00402076
	LOSS [training: 0.7576093522779436 | validation: 0.4605641862988184]
	TIME [epoch: 6.47 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7453922258835043		[learning rate: 0.0040113]
	Learning Rate: 0.00401127
	LOSS [training: 0.7453922258835043 | validation: 0.5022189054870582]
	TIME [epoch: 6.48 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7524514527599603		[learning rate: 0.0040018]
	Learning Rate: 0.00400181
	LOSS [training: 0.7524514527599603 | validation: 0.5359827881031293]
	TIME [epoch: 6.48 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7667189329081578		[learning rate: 0.0039924]
	Learning Rate: 0.00399237
	LOSS [training: 0.7667189329081578 | validation: 0.511069011798331]
	TIME [epoch: 6.48 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7584553744271114		[learning rate: 0.003983]
	Learning Rate: 0.00398295
	LOSS [training: 0.7584553744271114 | validation: 0.511373090948652]
	TIME [epoch: 6.48 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.78803228391098		[learning rate: 0.0039736]
	Learning Rate: 0.00397356
	LOSS [training: 0.78803228391098 | validation: 0.46511598547230715]
	TIME [epoch: 6.53 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7461383707039421		[learning rate: 0.0039642]
	Learning Rate: 0.00396418
	LOSS [training: 0.7461383707039421 | validation: 0.5318396527076148]
	TIME [epoch: 6.48 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7529556190883581		[learning rate: 0.0039548]
	Learning Rate: 0.00395483
	LOSS [training: 0.7529556190883581 | validation: 0.5093559958101533]
	TIME [epoch: 6.48 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7704945808083277		[learning rate: 0.0039455]
	Learning Rate: 0.0039455
	LOSS [training: 0.7704945808083277 | validation: 0.5012708068865759]
	TIME [epoch: 6.49 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7681427450699573		[learning rate: 0.0039362]
	Learning Rate: 0.0039362
	LOSS [training: 0.7681427450699573 | validation: 0.46309653180065774]
	TIME [epoch: 6.48 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7398224617484885		[learning rate: 0.0039269]
	Learning Rate: 0.00392691
	LOSS [training: 0.7398224617484885 | validation: 0.47903825895106195]
	TIME [epoch: 6.49 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7398889393856676		[learning rate: 0.0039176]
	Learning Rate: 0.00391765
	LOSS [training: 0.7398889393856676 | validation: 0.4654274281387368]
	TIME [epoch: 6.52 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8053247638044461		[learning rate: 0.0039084]
	Learning Rate: 0.00390841
	LOSS [training: 0.8053247638044461 | validation: 0.4969235519945024]
	TIME [epoch: 6.49 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7524595723098977		[learning rate: 0.0038992]
	Learning Rate: 0.00389919
	LOSS [training: 0.7524595723098977 | validation: 0.6041455946379853]
	TIME [epoch: 6.49 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8054113577987989		[learning rate: 0.00389]
	Learning Rate: 0.00388999
	LOSS [training: 0.8054113577987989 | validation: 0.4786673890169414]
	TIME [epoch: 6.48 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8124817198177798		[learning rate: 0.0038808]
	Learning Rate: 0.00388082
	LOSS [training: 0.8124817198177798 | validation: 0.5130872238197143]
	TIME [epoch: 6.48 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.775365593620631		[learning rate: 0.0038717]
	Learning Rate: 0.00387166
	LOSS [training: 0.775365593620631 | validation: 0.46781275622252416]
	TIME [epoch: 6.51 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7503656416531661		[learning rate: 0.0038625]
	Learning Rate: 0.00386253
	LOSS [training: 0.7503656416531661 | validation: 0.4949370592068236]
	TIME [epoch: 6.53 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7345022702040211		[learning rate: 0.0038534]
	Learning Rate: 0.00385342
	LOSS [training: 0.7345022702040211 | validation: 0.478037505190418]
	TIME [epoch: 6.48 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.752735920707645		[learning rate: 0.0038443]
	Learning Rate: 0.00384433
	LOSS [training: 0.752735920707645 | validation: 0.5370607105958439]
	TIME [epoch: 6.49 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7591178543855963		[learning rate: 0.0038353]
	Learning Rate: 0.00383526
	LOSS [training: 0.7591178543855963 | validation: 0.4823406990514404]
	TIME [epoch: 6.49 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7297511885025681		[learning rate: 0.0038262]
	Learning Rate: 0.00382621
	LOSS [training: 0.7297511885025681 | validation: 0.4652609603402863]
	TIME [epoch: 6.49 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7268119109492737		[learning rate: 0.0038172]
	Learning Rate: 0.00381719
	LOSS [training: 0.7268119109492737 | validation: 0.46858711456917956]
	TIME [epoch: 6.53 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7370784327518093		[learning rate: 0.0038082]
	Learning Rate: 0.00380818
	LOSS [training: 0.7370784327518093 | validation: 0.523228204379226]
	TIME [epoch: 6.48 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7370803141832636		[learning rate: 0.0037992]
	Learning Rate: 0.0037992
	LOSS [training: 0.7370803141832636 | validation: 0.471742992437682]
	TIME [epoch: 6.48 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8343967503354776		[learning rate: 0.0037902]
	Learning Rate: 0.00379024
	LOSS [training: 0.8343967503354776 | validation: 0.5017716953230354]
	TIME [epoch: 6.48 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7857208692480813		[learning rate: 0.0037813]
	Learning Rate: 0.0037813
	LOSS [training: 0.7857208692480813 | validation: 0.5420222131552862]
	TIME [epoch: 6.48 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8825156020371212		[learning rate: 0.0037724]
	Learning Rate: 0.00377238
	LOSS [training: 0.8825156020371212 | validation: 0.519599899778344]
	TIME [epoch: 6.49 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7371494664348742		[learning rate: 0.0037635]
	Learning Rate: 0.00376348
	LOSS [training: 0.7371494664348742 | validation: 0.49367070542708036]
	TIME [epoch: 6.55 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7278318331647672		[learning rate: 0.0037546]
	Learning Rate: 0.0037546
	LOSS [training: 0.7278318331647672 | validation: 0.4807283188798832]
	TIME [epoch: 6.49 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7574445582945438		[learning rate: 0.0037457]
	Learning Rate: 0.00374575
	LOSS [training: 0.7574445582945438 | validation: 0.5105048998745212]
	TIME [epoch: 6.48 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7802958140030798		[learning rate: 0.0037369]
	Learning Rate: 0.00373691
	LOSS [training: 0.7802958140030798 | validation: 0.5481440301309911]
	TIME [epoch: 6.49 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7779934705606618		[learning rate: 0.0037281]
	Learning Rate: 0.0037281
	LOSS [training: 0.7779934705606618 | validation: 0.5391088320907367]
	TIME [epoch: 6.51 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.80200276773367		[learning rate: 0.0037193]
	Learning Rate: 0.0037193
	LOSS [training: 0.80200276773367 | validation: 0.4894475131010412]
	TIME [epoch: 6.5 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7524087284600449		[learning rate: 0.0037105]
	Learning Rate: 0.00371053
	LOSS [training: 0.7524087284600449 | validation: 0.49601821869771934]
	TIME [epoch: 6.54 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7306050481808375		[learning rate: 0.0037018]
	Learning Rate: 0.00370178
	LOSS [training: 0.7306050481808375 | validation: 0.47319395174255957]
	TIME [epoch: 6.5 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7628583278520784		[learning rate: 0.003693]
	Learning Rate: 0.00369304
	LOSS [training: 0.7628583278520784 | validation: 0.4869784719005378]
	TIME [epoch: 6.5 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7635779470565663		[learning rate: 0.0036843]
	Learning Rate: 0.00368433
	LOSS [training: 0.7635779470565663 | validation: 0.5148496016281296]
	TIME [epoch: 6.5 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7909051269740808		[learning rate: 0.0036756]
	Learning Rate: 0.00367564
	LOSS [training: 0.7909051269740808 | validation: 0.4677791182640574]
	TIME [epoch: 6.5 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.732349704352504		[learning rate: 0.003667]
	Learning Rate: 0.00366697
	LOSS [training: 0.732349704352504 | validation: 0.4723289992676009]
	TIME [epoch: 6.51 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7269499883766508		[learning rate: 0.0036583]
	Learning Rate: 0.00365832
	LOSS [training: 0.7269499883766508 | validation: 0.46254079570853146]
	TIME [epoch: 6.54 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7366931570259506		[learning rate: 0.0036497]
	Learning Rate: 0.00364969
	LOSS [training: 0.7366931570259506 | validation: 0.47101170881933974]
	TIME [epoch: 6.51 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7450518774596241		[learning rate: 0.0036411]
	Learning Rate: 0.00364108
	LOSS [training: 0.7450518774596241 | validation: 0.5747571515205634]
	TIME [epoch: 6.52 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8104538703087864		[learning rate: 0.0036325]
	Learning Rate: 0.0036325
	LOSS [training: 0.8104538703087864 | validation: 0.5193991021329388]
	TIME [epoch: 6.5 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7843405032545201		[learning rate: 0.0036239]
	Learning Rate: 0.00362393
	LOSS [training: 0.7843405032545201 | validation: 0.5259784987996833]
	TIME [epoch: 6.52 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7906422463649461		[learning rate: 0.0036154]
	Learning Rate: 0.00361538
	LOSS [training: 0.7906422463649461 | validation: 0.5100432583535378]
	TIME [epoch: 6.54 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7553875854021666		[learning rate: 0.0036069]
	Learning Rate: 0.00360685
	LOSS [training: 0.7553875854021666 | validation: 0.4842005424280261]
	TIME [epoch: 6.52 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.750317385107151		[learning rate: 0.0035983]
	Learning Rate: 0.00359834
	LOSS [training: 0.750317385107151 | validation: 0.5035480778279986]
	TIME [epoch: 6.5 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8133919611459742		[learning rate: 0.0035899]
	Learning Rate: 0.00358986
	LOSS [training: 0.8133919611459742 | validation: 0.6415977303706925]
	TIME [epoch: 6.5 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8422524261001074		[learning rate: 0.0035814]
	Learning Rate: 0.00358139
	LOSS [training: 0.8422524261001074 | validation: 0.5126039113281676]
	TIME [epoch: 6.5 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7477488362573733		[learning rate: 0.0035729]
	Learning Rate: 0.00357294
	LOSS [training: 0.7477488362573733 | validation: 0.47310667057413525]
	TIME [epoch: 6.5 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7375878857679028		[learning rate: 0.0035645]
	Learning Rate: 0.00356451
	LOSS [training: 0.7375878857679028 | validation: 0.49331414679991437]
	TIME [epoch: 6.57 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7471172915527693		[learning rate: 0.0035561]
	Learning Rate: 0.0035561
	LOSS [training: 0.7471172915527693 | validation: 0.4974548142595344]
	TIME [epoch: 6.51 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7646462146161765		[learning rate: 0.0035477]
	Learning Rate: 0.00354771
	LOSS [training: 0.7646462146161765 | validation: 0.5212659926643883]
	TIME [epoch: 6.49 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7718352750377913		[learning rate: 0.0035393]
	Learning Rate: 0.00353935
	LOSS [training: 0.7718352750377913 | validation: 0.4742218513951979]
	TIME [epoch: 6.5 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7764534808880085		[learning rate: 0.003531]
	Learning Rate: 0.003531
	LOSS [training: 0.7764534808880085 | validation: 0.4802194142929012]
	TIME [epoch: 6.5 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7513668414612005		[learning rate: 0.0035227]
	Learning Rate: 0.00352267
	LOSS [training: 0.7513668414612005 | validation: 0.5144657904527816]
	TIME [epoch: 6.53 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7497576039694248		[learning rate: 0.0035144]
	Learning Rate: 0.00351436
	LOSS [training: 0.7497576039694248 | validation: 0.49801289587071684]
	TIME [epoch: 6.56 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7519675490357456		[learning rate: 0.0035061]
	Learning Rate: 0.00350607
	LOSS [training: 0.7519675490357456 | validation: 0.47114621089375286]
	TIME [epoch: 6.51 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7387852500108506		[learning rate: 0.0034978]
	Learning Rate: 0.0034978
	LOSS [training: 0.7387852500108506 | validation: 0.4773879155123516]
	TIME [epoch: 6.51 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7507135794859767		[learning rate: 0.0034895]
	Learning Rate: 0.00348955
	LOSS [training: 0.7507135794859767 | validation: 0.5093517823887913]
	TIME [epoch: 6.51 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7563890983083726		[learning rate: 0.0034813]
	Learning Rate: 0.00348132
	LOSS [training: 0.7563890983083726 | validation: 0.5426282307018025]
	TIME [epoch: 6.5 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7731568984607199		[learning rate: 0.0034731]
	Learning Rate: 0.00347311
	LOSS [training: 0.7731568984607199 | validation: 0.48383158742521054]
	TIME [epoch: 6.53 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7518477186383811		[learning rate: 0.0034649]
	Learning Rate: 0.00346491
	LOSS [training: 0.7518477186383811 | validation: 0.5380147706636569]
	TIME [epoch: 6.54 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7653910350194033		[learning rate: 0.0034567]
	Learning Rate: 0.00345674
	LOSS [training: 0.7653910350194033 | validation: 0.4596828167156619]
	TIME [epoch: 6.5 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7358395795752564		[learning rate: 0.0034486]
	Learning Rate: 0.00344859
	LOSS [training: 0.7358395795752564 | validation: 0.479104273495445]
	TIME [epoch: 6.52 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.735277337391158		[learning rate: 0.0034405]
	Learning Rate: 0.00344045
	LOSS [training: 0.735277337391158 | validation: 0.473970612065518]
	TIME [epoch: 6.51 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7613100205278991		[learning rate: 0.0034323]
	Learning Rate: 0.00343234
	LOSS [training: 0.7613100205278991 | validation: 0.4579708350664645]
	TIME [epoch: 6.51 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7295355643957646		[learning rate: 0.0034242]
	Learning Rate: 0.00342424
	LOSS [training: 0.7295355643957646 | validation: 0.4944537081326731]
	TIME [epoch: 6.55 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7723477186842458		[learning rate: 0.0034162]
	Learning Rate: 0.00341616
	LOSS [training: 0.7723477186842458 | validation: 0.5305125007545539]
	TIME [epoch: 6.52 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7744001544271713		[learning rate: 0.0034081]
	Learning Rate: 0.0034081
	LOSS [training: 0.7744001544271713 | validation: 0.47952000113186133]
	TIME [epoch: 6.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7415195819002064		[learning rate: 0.0034001]
	Learning Rate: 0.00340006
	LOSS [training: 0.7415195819002064 | validation: 0.4810381608471076]
	TIME [epoch: 6.49 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7442852059441767		[learning rate: 0.003392]
	Learning Rate: 0.00339204
	LOSS [training: 0.7442852059441767 | validation: 0.4547740173263516]
	TIME [epoch: 6.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7597900599034926		[learning rate: 0.003384]
	Learning Rate: 0.00338404
	LOSS [training: 0.7597900599034926 | validation: 0.4864041439283497]
	TIME [epoch: 6.49 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7418335476831425		[learning rate: 0.0033761]
	Learning Rate: 0.00337606
	LOSS [training: 0.7418335476831425 | validation: 0.4714783524540212]
	TIME [epoch: 6.54 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7332293308270383		[learning rate: 0.0033681]
	Learning Rate: 0.0033681
	LOSS [training: 0.7332293308270383 | validation: 0.49675716855815566]
	TIME [epoch: 6.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7362986420928475		[learning rate: 0.0033602]
	Learning Rate: 0.00336015
	LOSS [training: 0.7362986420928475 | validation: 0.4691649404928684]
	TIME [epoch: 6.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.728961925722351		[learning rate: 0.0033522]
	Learning Rate: 0.00335223
	LOSS [training: 0.728961925722351 | validation: 0.45742901171459854]
	TIME [epoch: 6.49 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7336188429243109		[learning rate: 0.0033443]
	Learning Rate: 0.00334432
	LOSS [training: 0.7336188429243109 | validation: 0.4626464770639677]
	TIME [epoch: 6.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7213416173987351		[learning rate: 0.0033364]
	Learning Rate: 0.00333643
	LOSS [training: 0.7213416173987351 | validation: 0.4562360552564149]
	TIME [epoch: 6.51 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7716548342232008		[learning rate: 0.0033286]
	Learning Rate: 0.00332856
	LOSS [training: 0.7716548342232008 | validation: 0.593926832739503]
	TIME [epoch: 6.53 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8071954481702452		[learning rate: 0.0033207]
	Learning Rate: 0.00332071
	LOSS [training: 0.8071954481702452 | validation: 0.45163627725014194]
	TIME [epoch: 6.51 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7434649597180796		[learning rate: 0.0033129]
	Learning Rate: 0.00331288
	LOSS [training: 0.7434649597180796 | validation: 0.4715963839575081]
	TIME [epoch: 6.49 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7322726115221649		[learning rate: 0.0033051]
	Learning Rate: 0.00330506
	LOSS [training: 0.7322726115221649 | validation: 0.4677802414430799]
	TIME [epoch: 6.49 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.761774640245136		[learning rate: 0.0032973]
	Learning Rate: 0.00329727
	LOSS [training: 0.761774640245136 | validation: 0.4715692759987976]
	TIME [epoch: 6.48 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7339578854001616		[learning rate: 0.0032895]
	Learning Rate: 0.00328949
	LOSS [training: 0.7339578854001616 | validation: 0.479822006587371]
	TIME [epoch: 6.51 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7307299145986076		[learning rate: 0.0032817]
	Learning Rate: 0.00328173
	LOSS [training: 0.7307299145986076 | validation: 0.4871881398009994]
	TIME [epoch: 6.53 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.748755639270915		[learning rate: 0.003274]
	Learning Rate: 0.00327399
	LOSS [training: 0.748755639270915 | validation: 0.45929329489337045]
	TIME [epoch: 6.48 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7411698442790208		[learning rate: 0.0032663]
	Learning Rate: 0.00326626
	LOSS [training: 0.7411698442790208 | validation: 0.4567675767193861]
	TIME [epoch: 6.5 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7293289902183158		[learning rate: 0.0032586]
	Learning Rate: 0.00325856
	LOSS [training: 0.7293289902183158 | validation: 0.5662771371489455]
	TIME [epoch: 6.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7531850037976279		[learning rate: 0.0032509]
	Learning Rate: 0.00325087
	LOSS [training: 0.7531850037976279 | validation: 0.524401586524113]
	TIME [epoch: 6.51 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7643156713901365		[learning rate: 0.0032432]
	Learning Rate: 0.0032432
	LOSS [training: 0.7643156713901365 | validation: 0.4981439520747547]
	TIME [epoch: 6.54 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7627729938454342		[learning rate: 0.0032356]
	Learning Rate: 0.00323555
	LOSS [training: 0.7627729938454342 | validation: 0.4903763172519301]
	TIME [epoch: 6.52 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7593201089036761		[learning rate: 0.0032279]
	Learning Rate: 0.00322792
	LOSS [training: 0.7593201089036761 | validation: 0.4935835742998812]
	TIME [epoch: 6.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7498558913912292		[learning rate: 0.0032203]
	Learning Rate: 0.00322031
	LOSS [training: 0.7498558913912292 | validation: 0.4966085562355013]
	TIME [epoch: 6.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7861881609635817		[learning rate: 0.0032127]
	Learning Rate: 0.00321271
	LOSS [training: 0.7861881609635817 | validation: 0.5225766308145554]
	TIME [epoch: 6.51 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7804903864916573		[learning rate: 0.0032051]
	Learning Rate: 0.00320513
	LOSS [training: 0.7804903864916573 | validation: 0.4867115407047398]
	TIME [epoch: 6.51 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7714796344558615		[learning rate: 0.0031976]
	Learning Rate: 0.00319757
	LOSS [training: 0.7714796344558615 | validation: 0.45648390739501254]
	TIME [epoch: 6.54 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7391316090622841		[learning rate: 0.00319]
	Learning Rate: 0.00319003
	LOSS [training: 0.7391316090622841 | validation: 0.4669787312832732]
	TIME [epoch: 6.51 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7396243461490087		[learning rate: 0.0031825]
	Learning Rate: 0.00318251
	LOSS [training: 0.7396243461490087 | validation: 0.48272997363175113]
	TIME [epoch: 6.51 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7473435429151594		[learning rate: 0.003175]
	Learning Rate: 0.003175
	LOSS [training: 0.7473435429151594 | validation: 0.4558675726576959]
	TIME [epoch: 6.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7439552420208808		[learning rate: 0.0031675]
	Learning Rate: 0.00316751
	LOSS [training: 0.7439552420208808 | validation: 0.4986565237057746]
	TIME [epoch: 6.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7579250304961784		[learning rate: 0.00316]
	Learning Rate: 0.00316004
	LOSS [training: 0.7579250304961784 | validation: 0.4584259370165425]
	TIME [epoch: 6.52 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7292250578902133		[learning rate: 0.0031526]
	Learning Rate: 0.00315258
	LOSS [training: 0.7292250578902133 | validation: 0.4456139752811841]
	TIME [epoch: 6.55 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7196871849963471		[learning rate: 0.0031451]
	Learning Rate: 0.00314515
	LOSS [training: 0.7196871849963471 | validation: 0.46576677081549406]
	TIME [epoch: 6.51 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7330309725204522		[learning rate: 0.0031377]
	Learning Rate: 0.00313773
	LOSS [training: 0.7330309725204522 | validation: 0.4854333131738122]
	TIME [epoch: 6.52 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7442638190681998		[learning rate: 0.0031303]
	Learning Rate: 0.00313033
	LOSS [training: 0.7442638190681998 | validation: 0.45903258896035876]
	TIME [epoch: 6.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7126958817607301		[learning rate: 0.0031229]
	Learning Rate: 0.00312294
	LOSS [training: 0.7126958817607301 | validation: 0.4603502657705442]
	TIME [epoch: 6.5 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7252266557341043		[learning rate: 0.0031156]
	Learning Rate: 0.00311558
	LOSS [training: 0.7252266557341043 | validation: 0.45316043175122855]
	TIME [epoch: 6.53 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7224019841180382		[learning rate: 0.0031082]
	Learning Rate: 0.00310823
	LOSS [training: 0.7224019841180382 | validation: 0.486096981618084]
	TIME [epoch: 6.53 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7440347912725389		[learning rate: 0.0031009]
	Learning Rate: 0.0031009
	LOSS [training: 0.7440347912725389 | validation: 0.5082874120047949]
	TIME [epoch: 6.49 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7720158942568217		[learning rate: 0.0030936]
	Learning Rate: 0.00309358
	LOSS [training: 0.7720158942568217 | validation: 0.4627296519246458]
	TIME [epoch: 6.5 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7120760307492345		[learning rate: 0.0030863]
	Learning Rate: 0.00308628
	LOSS [training: 0.7120760307492345 | validation: 0.46441841897883057]
	TIME [epoch: 6.5 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7305750284814485		[learning rate: 0.003079]
	Learning Rate: 0.003079
	LOSS [training: 0.7305750284814485 | validation: 0.47363151587019964]
	TIME [epoch: 6.5 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7296888715147822		[learning rate: 0.0030717]
	Learning Rate: 0.00307174
	LOSS [training: 0.7296888715147822 | validation: 0.5876772743992636]
	TIME [epoch: 6.53 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7576180604466018		[learning rate: 0.0030645]
	Learning Rate: 0.0030645
	LOSS [training: 0.7576180604466018 | validation: 0.5184264066699118]
	TIME [epoch: 6.52 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7462333201351214		[learning rate: 0.0030573]
	Learning Rate: 0.00305727
	LOSS [training: 0.7462333201351214 | validation: 0.4514726205325334]
	TIME [epoch: 6.47 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7171101432868747		[learning rate: 0.0030501]
	Learning Rate: 0.00305006
	LOSS [training: 0.7171101432868747 | validation: 0.5154294086924909]
	TIME [epoch: 6.47 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.739067789500996		[learning rate: 0.0030429]
	Learning Rate: 0.00304286
	LOSS [training: 0.739067789500996 | validation: 0.4592432330296584]
	TIME [epoch: 6.49 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7231080783298038		[learning rate: 0.0030357]
	Learning Rate: 0.00303568
	LOSS [training: 0.7231080783298038 | validation: 0.46342247256810565]
	TIME [epoch: 6.5 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7253664701333911		[learning rate: 0.0030285]
	Learning Rate: 0.00302852
	LOSS [training: 0.7253664701333911 | validation: 0.4696790738866972]
	TIME [epoch: 6.54 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7194638834904216		[learning rate: 0.0030214]
	Learning Rate: 0.00302138
	LOSS [training: 0.7194638834904216 | validation: 0.45471897286481844]
	TIME [epoch: 6.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7414692543394288		[learning rate: 0.0030143]
	Learning Rate: 0.00301425
	LOSS [training: 0.7414692543394288 | validation: 0.48889373292338933]
	TIME [epoch: 6.51 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.762147095161828		[learning rate: 0.0030071]
	Learning Rate: 0.00300714
	LOSS [training: 0.762147095161828 | validation: 0.49566708294060957]
	TIME [epoch: 6.51 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.727582337189703		[learning rate: 0.003]
	Learning Rate: 0.00300005
	LOSS [training: 0.727582337189703 | validation: 0.48120654685817876]
	TIME [epoch: 6.5 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7559561720400853		[learning rate: 0.002993]
	Learning Rate: 0.00299297
	LOSS [training: 0.7559561720400853 | validation: 0.5415939667195843]
	TIME [epoch: 6.52 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7476781498640817		[learning rate: 0.0029859]
	Learning Rate: 0.00298591
	LOSS [training: 0.7476781498640817 | validation: 0.4687794235325531]
	TIME [epoch: 6.56 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7460677326363822		[learning rate: 0.0029789]
	Learning Rate: 0.00297887
	LOSS [training: 0.7460677326363822 | validation: 0.46766949918352774]
	TIME [epoch: 6.52 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7605042371755816		[learning rate: 0.0029718]
	Learning Rate: 0.00297184
	LOSS [training: 0.7605042371755816 | validation: 0.46103490237173855]
	TIME [epoch: 6.5 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7418383434301996		[learning rate: 0.0029648]
	Learning Rate: 0.00296483
	LOSS [training: 0.7418383434301996 | validation: 0.4552686404549889]
	TIME [epoch: 6.51 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7399386758337434		[learning rate: 0.0029578]
	Learning Rate: 0.00295784
	LOSS [training: 0.7399386758337434 | validation: 0.46556911124383893]
	TIME [epoch: 6.5 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7692465272845928		[learning rate: 0.0029509]
	Learning Rate: 0.00295086
	LOSS [training: 0.7692465272845928 | validation: 0.5232231592003552]
	TIME [epoch: 6.51 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7995592372663004		[learning rate: 0.0029439]
	Learning Rate: 0.0029439
	LOSS [training: 0.7995592372663004 | validation: 0.5139117110437628]
	TIME [epoch: 6.54 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7519688821308215		[learning rate: 0.002937]
	Learning Rate: 0.00293696
	LOSS [training: 0.7519688821308215 | validation: 0.4640712266733691]
	TIME [epoch: 6.52 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7215866637910253		[learning rate: 0.00293]
	Learning Rate: 0.00293003
	LOSS [training: 0.7215866637910253 | validation: 0.471098477273009]
	TIME [epoch: 6.51 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7222503911585334		[learning rate: 0.0029231]
	Learning Rate: 0.00292312
	LOSS [training: 0.7222503911585334 | validation: 0.46738670787378006]
	TIME [epoch: 6.49 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7240331529494771		[learning rate: 0.0029162]
	Learning Rate: 0.00291622
	LOSS [training: 0.7240331529494771 | validation: 0.4592542673538161]
	TIME [epoch: 6.49 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7515182642324643		[learning rate: 0.0029093]
	Learning Rate: 0.00290934
	LOSS [training: 0.7515182642324643 | validation: 0.5396495911540262]
	TIME [epoch: 6.56 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8154972269469895		[learning rate: 0.0029025]
	Learning Rate: 0.00290248
	LOSS [training: 0.8154972269469895 | validation: 0.5065347592471072]
	TIME [epoch: 6.52 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7435660982875343		[learning rate: 0.0028956]
	Learning Rate: 0.00289563
	LOSS [training: 0.7435660982875343 | validation: 0.48038692135156547]
	TIME [epoch: 6.49 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7557619513985286		[learning rate: 0.0028888]
	Learning Rate: 0.0028888
	LOSS [training: 0.7557619513985286 | validation: 0.5667354681377755]
	TIME [epoch: 6.52 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8538887779215021		[learning rate: 0.002882]
	Learning Rate: 0.00288199
	LOSS [training: 0.8538887779215021 | validation: 0.5544556934876188]
	TIME [epoch: 6.52 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.776772877042214		[learning rate: 0.0028752]
	Learning Rate: 0.00287519
	LOSS [training: 0.776772877042214 | validation: 0.4887117111136686]
	TIME [epoch: 6.49 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.730138094054032		[learning rate: 0.0028684]
	Learning Rate: 0.00286841
	LOSS [training: 0.730138094054032 | validation: 0.46879005317545]
	TIME [epoch: 6.54 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7291104687322072		[learning rate: 0.0028616]
	Learning Rate: 0.00286164
	LOSS [training: 0.7291104687322072 | validation: 0.4531583542920855]
	TIME [epoch: 6.5 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7332888856089159		[learning rate: 0.0028549]
	Learning Rate: 0.00285489
	LOSS [training: 0.7332888856089159 | validation: 0.46348985479754234]
	TIME [epoch: 6.49 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7375524007375373		[learning rate: 0.0028482]
	Learning Rate: 0.00284816
	LOSS [training: 0.7375524007375373 | validation: 0.49544409843611625]
	TIME [epoch: 6.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7287054678310196		[learning rate: 0.0028414]
	Learning Rate: 0.00284144
	LOSS [training: 0.7287054678310196 | validation: 0.4476354734640814]
	TIME [epoch: 6.5 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7198246100502491		[learning rate: 0.0028347]
	Learning Rate: 0.00283474
	LOSS [training: 0.7198246100502491 | validation: 0.4921245371166024]
	TIME [epoch: 6.51 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7405172051548536		[learning rate: 0.0028281]
	Learning Rate: 0.00282805
	LOSS [training: 0.7405172051548536 | validation: 0.44201111971884627]
	TIME [epoch: 6.55 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7191474695938609		[learning rate: 0.0028214]
	Learning Rate: 0.00282138
	LOSS [training: 0.7191474695938609 | validation: 0.444798117023324]
	TIME [epoch: 6.5 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7308725780377527		[learning rate: 0.0028147]
	Learning Rate: 0.00281472
	LOSS [training: 0.7308725780377527 | validation: 0.4522930232267659]
	TIME [epoch: 6.49 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7236616420034371		[learning rate: 0.0028081]
	Learning Rate: 0.00280808
	LOSS [training: 0.7236616420034371 | validation: 0.590818028463528]
	TIME [epoch: 6.5 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.768559047187358		[learning rate: 0.0028015]
	Learning Rate: 0.00280146
	LOSS [training: 0.768559047187358 | validation: 0.48448592783933037]
	TIME [epoch: 6.5 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7402046020309535		[learning rate: 0.0027949]
	Learning Rate: 0.00279485
	LOSS [training: 0.7402046020309535 | validation: 0.4923177318256058]
	TIME [epoch: 6.52 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7470407220964022		[learning rate: 0.0027883]
	Learning Rate: 0.00278826
	LOSS [training: 0.7470407220964022 | validation: 0.48893151062410434]
	TIME [epoch: 6.51 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7348547472103797		[learning rate: 0.0027817]
	Learning Rate: 0.00278168
	LOSS [training: 0.7348547472103797 | validation: 0.509879099723832]
	TIME [epoch: 6.5 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7465451615589827		[learning rate: 0.0027751]
	Learning Rate: 0.00277512
	LOSS [training: 0.7465451615589827 | validation: 0.4795626277073733]
	TIME [epoch: 6.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7323976996649154		[learning rate: 0.0027686]
	Learning Rate: 0.00276858
	LOSS [training: 0.7323976996649154 | validation: 0.4587335059247168]
	TIME [epoch: 6.49 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7263127972649887		[learning rate: 0.002762]
	Learning Rate: 0.00276205
	LOSS [training: 0.7263127972649887 | validation: 0.48133010446910623]
	TIME [epoch: 6.49 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7807180174268724		[learning rate: 0.0027555]
	Learning Rate: 0.00275553
	LOSS [training: 0.7807180174268724 | validation: 0.5111372389449218]
	TIME [epoch: 6.53 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7493656402863336		[learning rate: 0.002749]
	Learning Rate: 0.00274903
	LOSS [training: 0.7493656402863336 | validation: 0.4832385026924221]
	TIME [epoch: 6.51 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7424163735374452		[learning rate: 0.0027425]
	Learning Rate: 0.00274255
	LOSS [training: 0.7424163735374452 | validation: 0.5399533455407922]
	TIME [epoch: 6.49 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7813868906965502		[learning rate: 0.0027361]
	Learning Rate: 0.00273608
	LOSS [training: 0.7813868906965502 | validation: 0.4570212769326104]
	TIME [epoch: 6.51 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7248898029921014		[learning rate: 0.0027296]
	Learning Rate: 0.00272962
	LOSS [training: 0.7248898029921014 | validation: 0.4491750267045407]
	TIME [epoch: 6.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7375925829082005		[learning rate: 0.0027232]
	Learning Rate: 0.00272318
	LOSS [training: 0.7375925829082005 | validation: 0.4879110808397357]
	TIME [epoch: 6.51 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7322098227771803		[learning rate: 0.0027168]
	Learning Rate: 0.00271676
	LOSS [training: 0.7322098227771803 | validation: 0.4430280988622228]
	TIME [epoch: 6.56 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7229639479830422		[learning rate: 0.0027104]
	Learning Rate: 0.00271035
	LOSS [training: 0.7229639479830422 | validation: 0.470281758357306]
	TIME [epoch: 6.5 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7458485681710312		[learning rate: 0.002704]
	Learning Rate: 0.00270396
	LOSS [training: 0.7458485681710312 | validation: 0.46945527855463687]
	TIME [epoch: 6.51 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7366540490055098		[learning rate: 0.0026976]
	Learning Rate: 0.00269758
	LOSS [training: 0.7366540490055098 | validation: 0.4541242794466204]
	TIME [epoch: 6.51 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7339901199105544		[learning rate: 0.0026912]
	Learning Rate: 0.00269122
	LOSS [training: 0.7339901199105544 | validation: 0.45946536710457286]
	TIME [epoch: 6.51 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7303415623611988		[learning rate: 0.0026849]
	Learning Rate: 0.00268487
	LOSS [training: 0.7303415623611988 | validation: 0.460525066091815]
	TIME [epoch: 6.52 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.727358310224304		[learning rate: 0.0026785]
	Learning Rate: 0.00267854
	LOSS [training: 0.727358310224304 | validation: 0.4640490852986152]
	TIME [epoch: 6.55 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7205424687110056		[learning rate: 0.0026722]
	Learning Rate: 0.00267222
	LOSS [training: 0.7205424687110056 | validation: 0.45747127564790885]
	TIME [epoch: 6.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.728765783014192		[learning rate: 0.0026659]
	Learning Rate: 0.00266591
	LOSS [training: 0.728765783014192 | validation: 0.44176573960212834]
	TIME [epoch: 6.49 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7154470378815226		[learning rate: 0.0026596]
	Learning Rate: 0.00265963
	LOSS [training: 0.7154470378815226 | validation: 0.4456742819012079]
	TIME [epoch: 6.51 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7213108912706357		[learning rate: 0.0026534]
	Learning Rate: 0.00265335
	LOSS [training: 0.7213108912706357 | validation: 0.44679865744740055]
	TIME [epoch: 6.51 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7425975885248678		[learning rate: 0.0026471]
	Learning Rate: 0.00264709
	LOSS [training: 0.7425975885248678 | validation: 0.48835023782898596]
	TIME [epoch: 6.53 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7458499412962512		[learning rate: 0.0026408]
	Learning Rate: 0.00264085
	LOSS [training: 0.7458499412962512 | validation: 0.4574067580913927]
	TIME [epoch: 6.54 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7580992918115315		[learning rate: 0.0026346]
	Learning Rate: 0.00263462
	LOSS [training: 0.7580992918115315 | validation: 0.4614729846066486]
	TIME [epoch: 6.52 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7160726181962198		[learning rate: 0.0026284]
	Learning Rate: 0.00262841
	LOSS [training: 0.7160726181962198 | validation: 0.45180602356614286]
	TIME [epoch: 6.52 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7192362098384939		[learning rate: 0.0026222]
	Learning Rate: 0.00262221
	LOSS [training: 0.7192362098384939 | validation: 0.44816608639008587]
	TIME [epoch: 6.51 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7337478423118312		[learning rate: 0.002616]
	Learning Rate: 0.00261602
	LOSS [training: 0.7337478423118312 | validation: 0.46849068687458917]
	TIME [epoch: 6.52 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.724058575336173		[learning rate: 0.0026098]
	Learning Rate: 0.00260985
	LOSS [training: 0.724058575336173 | validation: 0.4578772695233547]
	TIME [epoch: 6.56 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7231751910713469		[learning rate: 0.0026037]
	Learning Rate: 0.00260369
	LOSS [training: 0.7231751910713469 | validation: 0.45258487993499785]
	TIME [epoch: 6.52 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7167883200494939		[learning rate: 0.0025976]
	Learning Rate: 0.00259755
	LOSS [training: 0.7167883200494939 | validation: 0.4848675669101884]
	TIME [epoch: 6.51 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7290681665669647		[learning rate: 0.0025914]
	Learning Rate: 0.00259142
	LOSS [training: 0.7290681665669647 | validation: 0.4525828885523633]
	TIME [epoch: 6.52 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7227666388842394		[learning rate: 0.0025853]
	Learning Rate: 0.00258531
	LOSS [training: 0.7227666388842394 | validation: 0.45934071690878453]
	TIME [epoch: 6.52 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7184377622971357		[learning rate: 0.0025792]
	Learning Rate: 0.00257921
	LOSS [training: 0.7184377622971357 | validation: 0.46499416391013415]
	TIME [epoch: 6.52 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7525449586980727		[learning rate: 0.0025731]
	Learning Rate: 0.00257313
	LOSS [training: 0.7525449586980727 | validation: 0.5353053824013012]
	TIME [epoch: 6.56 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7512648838362501		[learning rate: 0.0025671]
	Learning Rate: 0.00256706
	LOSS [training: 0.7512648838362501 | validation: 0.51472711242564]
	TIME [epoch: 6.52 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7429185128808223		[learning rate: 0.002561]
	Learning Rate: 0.002561
	LOSS [training: 0.7429185128808223 | validation: 0.4724938456789498]
	TIME [epoch: 6.52 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.729861836339154		[learning rate: 0.002555]
	Learning Rate: 0.00255496
	LOSS [training: 0.729861836339154 | validation: 0.4540700224613318]
	TIME [epoch: 6.5 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7269810449144438		[learning rate: 0.0025489]
	Learning Rate: 0.00254894
	LOSS [training: 0.7269810449144438 | validation: 0.4655918404333311]
	TIME [epoch: 6.52 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7285591031318932		[learning rate: 0.0025429]
	Learning Rate: 0.00254292
	LOSS [training: 0.7285591031318932 | validation: 0.4837281778101495]
	TIME [epoch: 6.5 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7246514470854899		[learning rate: 0.0025369]
	Learning Rate: 0.00253693
	LOSS [training: 0.7246514470854899 | validation: 0.4592835546529686]
	TIME [epoch: 6.55 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.723641906182949		[learning rate: 0.0025309]
	Learning Rate: 0.00253094
	LOSS [training: 0.723641906182949 | validation: 0.4546733301337077]
	TIME [epoch: 6.49 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8461439877239082		[learning rate: 0.002525]
	Learning Rate: 0.00252497
	LOSS [training: 0.8461439877239082 | validation: 0.5311854858424299]
	TIME [epoch: 6.51 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7430917895952093		[learning rate: 0.002519]
	Learning Rate: 0.00251901
	LOSS [training: 0.7430917895952093 | validation: 0.4591475198279955]
	TIME [epoch: 6.49 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7204663580926873		[learning rate: 0.0025131]
	Learning Rate: 0.00251307
	LOSS [training: 0.7204663580926873 | validation: 0.45367147547878756]
	TIME [epoch: 6.48 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7149446582151484		[learning rate: 0.0025071]
	Learning Rate: 0.00250715
	LOSS [training: 0.7149446582151484 | validation: 0.4487902348014829]
	TIME [epoch: 6.5 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7109369600159772		[learning rate: 0.0025012]
	Learning Rate: 0.00250123
	LOSS [training: 0.7109369600159772 | validation: 0.4549863444338099]
	TIME [epoch: 6.52 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7197508524894272		[learning rate: 0.0024953]
	Learning Rate: 0.00249533
	LOSS [training: 0.7197508524894272 | validation: 0.44899914126421714]
	TIME [epoch: 6.51 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7531746347939644		[learning rate: 0.0024894]
	Learning Rate: 0.00248945
	LOSS [training: 0.7531746347939644 | validation: 0.49535803335230244]
	TIME [epoch: 6.47 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7336765962021756		[learning rate: 0.0024836]
	Learning Rate: 0.00248357
	LOSS [training: 0.7336765962021756 | validation: 0.46448969852273625]
	TIME [epoch: 6.51 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7215845194281723		[learning rate: 0.0024777]
	Learning Rate: 0.00247771
	LOSS [training: 0.7215845194281723 | validation: 0.4626234255984189]
	TIME [epoch: 6.49 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7228117428775007		[learning rate: 0.0024719]
	Learning Rate: 0.00247187
	LOSS [training: 0.7228117428775007 | validation: 0.5276758904894598]
	TIME [epoch: 6.53 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7385625769442024		[learning rate: 0.002466]
	Learning Rate: 0.00246604
	LOSS [training: 0.7385625769442024 | validation: 0.45347472142233114]
	TIME [epoch: 6.49 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7045973447903089		[learning rate: 0.0024602]
	Learning Rate: 0.00246022
	LOSS [training: 0.7045973447903089 | validation: 0.4444256369402678]
	TIME [epoch: 6.49 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7104237983522212		[learning rate: 0.0024544]
	Learning Rate: 0.00245442
	LOSS [training: 0.7104237983522212 | validation: 0.4428424722995104]
	TIME [epoch: 6.49 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7199640885570932		[learning rate: 0.0024486]
	Learning Rate: 0.00244863
	LOSS [training: 0.7199640885570932 | validation: 0.4531460751010039]
	TIME [epoch: 6.49 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7255563104532312		[learning rate: 0.0024429]
	Learning Rate: 0.00244285
	LOSS [training: 0.7255563104532312 | validation: 0.4613232095251875]
	TIME [epoch: 6.49 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7165130189941548		[learning rate: 0.0024371]
	Learning Rate: 0.00243709
	LOSS [training: 0.7165130189941548 | validation: 0.4653236700226325]
	TIME [epoch: 6.54 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.715459469286706		[learning rate: 0.0024313]
	Learning Rate: 0.00243134
	LOSS [training: 0.715459469286706 | validation: 0.44345920550202594]
	TIME [epoch: 6.49 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7034281103849649		[learning rate: 0.0024256]
	Learning Rate: 0.00242561
	LOSS [training: 0.7034281103849649 | validation: 0.5274329153775307]
	TIME [epoch: 6.49 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7237095581814456		[learning rate: 0.0024199]
	Learning Rate: 0.00241989
	LOSS [training: 0.7237095581814456 | validation: 0.44824440256975534]
	TIME [epoch: 6.5 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7188052234268371		[learning rate: 0.0024142]
	Learning Rate: 0.00241418
	LOSS [training: 0.7188052234268371 | validation: 0.4689305631698075]
	TIME [epoch: 6.49 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7340550319468966		[learning rate: 0.0024085]
	Learning Rate: 0.00240848
	LOSS [training: 0.7340550319468966 | validation: 0.47143078358770785]
	TIME [epoch: 6.5 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7110595097603145		[learning rate: 0.0024028]
	Learning Rate: 0.0024028
	LOSS [training: 0.7110595097603145 | validation: 0.4402892730185806]
	TIME [epoch: 6.54 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7232790530269381		[learning rate: 0.0023971]
	Learning Rate: 0.00239713
	LOSS [training: 0.7232790530269381 | validation: 0.5406792892994605]
	TIME [epoch: 6.5 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7438003748332388		[learning rate: 0.0023915]
	Learning Rate: 0.00239148
	LOSS [training: 0.7438003748332388 | validation: 0.4866467944699422]
	TIME [epoch: 6.5 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7202641827081842		[learning rate: 0.0023858]
	Learning Rate: 0.00238584
	LOSS [training: 0.7202641827081842 | validation: 0.45344404911507924]
	TIME [epoch: 6.51 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7139794033717805		[learning rate: 0.0023802]
	Learning Rate: 0.00238021
	LOSS [training: 0.7139794033717805 | validation: 0.4448792654622305]
	TIME [epoch: 6.5 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7220766558167506		[learning rate: 0.0023746]
	Learning Rate: 0.0023746
	LOSS [training: 0.7220766558167506 | validation: 0.4511166605368541]
	TIME [epoch: 6.52 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7159425985628436		[learning rate: 0.002369]
	Learning Rate: 0.00236899
	LOSS [training: 0.7159425985628436 | validation: 0.4755973008061275]
	TIME [epoch: 6.52 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7171730084151079		[learning rate: 0.0023634]
	Learning Rate: 0.00236341
	LOSS [training: 0.7171730084151079 | validation: 0.4491603681806044]
	TIME [epoch: 6.49 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7254609435503824		[learning rate: 0.0023578]
	Learning Rate: 0.00235783
	LOSS [training: 0.7254609435503824 | validation: 0.48506516979510256]
	TIME [epoch: 6.5 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7178462349846778		[learning rate: 0.0023523]
	Learning Rate: 0.00235227
	LOSS [training: 0.7178462349846778 | validation: 0.4710943468453327]
	TIME [epoch: 6.49 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7517680477075894		[learning rate: 0.0023467]
	Learning Rate: 0.00234672
	LOSS [training: 0.7517680477075894 | validation: 0.49731162223267633]
	TIME [epoch: 6.48 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7719039755167799		[learning rate: 0.0023412]
	Learning Rate: 0.00234119
	LOSS [training: 0.7719039755167799 | validation: 0.4920264922816391]
	TIME [epoch: 6.52 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7359029888385702		[learning rate: 0.0023357]
	Learning Rate: 0.00233566
	LOSS [training: 0.7359029888385702 | validation: 0.4782610546920613]
	TIME [epoch: 6.48 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7449156431629472		[learning rate: 0.0023302]
	Learning Rate: 0.00233015
	LOSS [training: 0.7449156431629472 | validation: 0.47306543989067246]
	TIME [epoch: 6.48 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7562689066966942		[learning rate: 0.0023247]
	Learning Rate: 0.00232466
	LOSS [training: 0.7562689066966942 | validation: 0.5009439956657495]
	TIME [epoch: 6.49 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7399126939926169		[learning rate: 0.0023192]
	Learning Rate: 0.00231917
	LOSS [training: 0.7399126939926169 | validation: 0.47189799186036085]
	TIME [epoch: 6.48 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.728090425458514		[learning rate: 0.0023137]
	Learning Rate: 0.0023137
	LOSS [training: 0.728090425458514 | validation: 0.4887215991228667]
	TIME [epoch: 6.49 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.723169200002304		[learning rate: 0.0023082]
	Learning Rate: 0.00230825
	LOSS [training: 0.723169200002304 | validation: 0.4473474873708124]
	TIME [epoch: 6.54 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7280287721120491		[learning rate: 0.0023028]
	Learning Rate: 0.0023028
	LOSS [training: 0.7280287721120491 | validation: 0.4812414111539418]
	TIME [epoch: 6.51 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7344509852391462		[learning rate: 0.0022974]
	Learning Rate: 0.00229737
	LOSS [training: 0.7344509852391462 | validation: 0.4491143850522633]
	TIME [epoch: 6.48 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7131356344422232		[learning rate: 0.0022919]
	Learning Rate: 0.00229195
	LOSS [training: 0.7131356344422232 | validation: 0.4449939587640898]
	TIME [epoch: 6.48 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7430075482750491		[learning rate: 0.0022865]
	Learning Rate: 0.00228654
	LOSS [training: 0.7430075482750491 | validation: 0.45786732340730685]
	TIME [epoch: 6.5 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7513580353051091		[learning rate: 0.0022811]
	Learning Rate: 0.00228115
	LOSS [training: 0.7513580353051091 | validation: 0.4997699363136595]
	TIME [epoch: 6.5 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7478926550185419		[learning rate: 0.0022758]
	Learning Rate: 0.00227577
	LOSS [training: 0.7478926550185419 | validation: 0.4565079421928744]
	TIME [epoch: 6.53 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7159393841732865		[learning rate: 0.0022704]
	Learning Rate: 0.0022704
	LOSS [training: 0.7159393841732865 | validation: 0.456132927521102]
	TIME [epoch: 6.49 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7190328897466636		[learning rate: 0.002265]
	Learning Rate: 0.00226505
	LOSS [training: 0.7190328897466636 | validation: 0.4483654315043613]
	TIME [epoch: 6.48 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7434621359263888		[learning rate: 0.0022597]
	Learning Rate: 0.0022597
	LOSS [training: 0.7434621359263888 | validation: 0.4681904973583035]
	TIME [epoch: 6.5 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7390771693364544		[learning rate: 0.0022544]
	Learning Rate: 0.00225437
	LOSS [training: 0.7390771693364544 | validation: 0.5074430291904419]
	TIME [epoch: 6.5 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7388791807950549		[learning rate: 0.0022491]
	Learning Rate: 0.00224905
	LOSS [training: 0.7388791807950549 | validation: 0.48447733729846437]
	TIME [epoch: 6.51 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.739345511191903		[learning rate: 0.0022437]
	Learning Rate: 0.00224375
	LOSS [training: 0.739345511191903 | validation: 0.45495110111194925]
	TIME [epoch: 6.53 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7124743911020259		[learning rate: 0.0022385]
	Learning Rate: 0.00223846
	LOSS [training: 0.7124743911020259 | validation: 0.44216412909120273]
	TIME [epoch: 6.5 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7111403165203112		[learning rate: 0.0022332]
	Learning Rate: 0.00223318
	LOSS [training: 0.7111403165203112 | validation: 0.4535711895531315]
	TIME [epoch: 6.49 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7356852468010983		[learning rate: 0.0022279]
	Learning Rate: 0.00222791
	LOSS [training: 0.7356852468010983 | validation: 0.4716034051377766]
	TIME [epoch: 6.5 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7560997472675376		[learning rate: 0.0022227]
	Learning Rate: 0.00222265
	LOSS [training: 0.7560997472675376 | validation: 0.483998279589493]
	TIME [epoch: 6.49 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.735844953048284		[learning rate: 0.0022174]
	Learning Rate: 0.00221741
	LOSS [training: 0.735844953048284 | validation: 0.4555830305495945]
	TIME [epoch: 6.53 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7261397718821853		[learning rate: 0.0022122]
	Learning Rate: 0.00221218
	LOSS [training: 0.7261397718821853 | validation: 0.456522638721252]
	TIME [epoch: 6.51 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7246201588286182		[learning rate: 0.002207]
	Learning Rate: 0.00220696
	LOSS [training: 0.7246201588286182 | validation: 0.4441209000829163]
	TIME [epoch: 6.49 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7050107051567457		[learning rate: 0.0022018]
	Learning Rate: 0.00220176
	LOSS [training: 0.7050107051567457 | validation: 0.4526491499137904]
	TIME [epoch: 6.48 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7160576366134652		[learning rate: 0.0021966]
	Learning Rate: 0.00219656
	LOSS [training: 0.7160576366134652 | validation: 0.44319010067762926]
	TIME [epoch: 6.48 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7069580933311505		[learning rate: 0.0021914]
	Learning Rate: 0.00219138
	LOSS [training: 0.7069580933311505 | validation: 0.4576838989401914]
	TIME [epoch: 6.5 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7085642769030716		[learning rate: 0.0021862]
	Learning Rate: 0.00218621
	LOSS [training: 0.7085642769030716 | validation: 0.4567932719950283]
	TIME [epoch: 6.55 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7127617402569492		[learning rate: 0.0021811]
	Learning Rate: 0.00218106
	LOSS [training: 0.7127617402569492 | validation: 0.45368197808450617]
	TIME [epoch: 6.51 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.709361096652679		[learning rate: 0.0021759]
	Learning Rate: 0.00217591
	LOSS [training: 0.709361096652679 | validation: 0.44154212699016526]
	TIME [epoch: 6.49 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7075687472059429		[learning rate: 0.0021708]
	Learning Rate: 0.00217078
	LOSS [training: 0.7075687472059429 | validation: 0.4451627906933914]
	TIME [epoch: 6.5 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7294811925606682		[learning rate: 0.0021657]
	Learning Rate: 0.00216566
	LOSS [training: 0.7294811925606682 | validation: 0.45636537999207816]
	TIME [epoch: 6.5 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7152730119345042		[learning rate: 0.0021605]
	Learning Rate: 0.00216055
	LOSS [training: 0.7152730119345042 | validation: 0.4740172118286479]
	TIME [epoch: 6.51 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7563306636685683		[learning rate: 0.0021555]
	Learning Rate: 0.00215545
	LOSS [training: 0.7563306636685683 | validation: 0.49952812555617443]
	TIME [epoch: 6.52 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8084205230639864		[learning rate: 0.0021504]
	Learning Rate: 0.00215037
	LOSS [training: 0.8084205230639864 | validation: 0.4946554728809811]
	TIME [epoch: 6.5 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7240866573242477		[learning rate: 0.0021453]
	Learning Rate: 0.0021453
	LOSS [training: 0.7240866573242477 | validation: 0.44532976556754006]
	TIME [epoch: 6.47 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7157812270702486		[learning rate: 0.0021402]
	Learning Rate: 0.00214024
	LOSS [training: 0.7157812270702486 | validation: 0.4594362733746162]
	TIME [epoch: 6.48 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7146984552774204		[learning rate: 0.0021352]
	Learning Rate: 0.00213519
	LOSS [training: 0.7146984552774204 | validation: 0.4781627155426104]
	TIME [epoch: 6.5 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7073463257838402		[learning rate: 0.0021302]
	Learning Rate: 0.00213015
	LOSS [training: 0.7073463257838402 | validation: 0.45499831231534804]
	TIME [epoch: 6.52 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7074130745796567		[learning rate: 0.0021251]
	Learning Rate: 0.00212513
	LOSS [training: 0.7074130745796567 | validation: 0.45193889504991114]
	TIME [epoch: 6.53 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7106187899261058		[learning rate: 0.0021201]
	Learning Rate: 0.00212011
	LOSS [training: 0.7106187899261058 | validation: 0.4601732558346122]
	TIME [epoch: 6.48 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7298783012436292		[learning rate: 0.0021151]
	Learning Rate: 0.00211511
	LOSS [training: 0.7298783012436292 | validation: 0.4397952899007292]
	TIME [epoch: 6.48 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.721815084124133		[learning rate: 0.0021101]
	Learning Rate: 0.00211012
	LOSS [training: 0.721815084124133 | validation: 0.4606325090777982]
	TIME [epoch: 6.48 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7215549257106528		[learning rate: 0.0021051]
	Learning Rate: 0.00210515
	LOSS [training: 0.7215549257106528 | validation: 0.454816064374365]
	TIME [epoch: 6.49 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7150220306215151		[learning rate: 0.0021002]
	Learning Rate: 0.00210018
	LOSS [training: 0.7150220306215151 | validation: 0.4492708706140311]
	TIME [epoch: 6.54 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7040878271879032		[learning rate: 0.0020952]
	Learning Rate: 0.00209523
	LOSS [training: 0.7040878271879032 | validation: 0.4433921028790623]
	TIME [epoch: 6.5 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7369740278992264		[learning rate: 0.0020903]
	Learning Rate: 0.00209028
	LOSS [training: 0.7369740278992264 | validation: 0.43807586171953256]
	TIME [epoch: 6.48 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7053444300939088		[learning rate: 0.0020854]
	Learning Rate: 0.00208535
	LOSS [training: 0.7053444300939088 | validation: 0.44748660662255313]
	TIME [epoch: 6.48 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7148772853085656		[learning rate: 0.0020804]
	Learning Rate: 0.00208043
	LOSS [training: 0.7148772853085656 | validation: 0.4590208581547818]
	TIME [epoch: 6.48 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7104956085223229		[learning rate: 0.0020755]
	Learning Rate: 0.00207553
	LOSS [training: 0.7104956085223229 | validation: 0.4401690567416061]
	TIME [epoch: 6.49 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7076727347741659		[learning rate: 0.0020706]
	Learning Rate: 0.00207063
	LOSS [training: 0.7076727347741659 | validation: 0.4713788815174528]
	TIME [epoch: 6.53 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7167946831999464		[learning rate: 0.0020657]
	Learning Rate: 0.00206575
	LOSS [training: 0.7167946831999464 | validation: 0.49615160448432444]
	TIME [epoch: 6.49 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7482231113874007		[learning rate: 0.0020609]
	Learning Rate: 0.00206087
	LOSS [training: 0.7482231113874007 | validation: 0.460025117456779]
	TIME [epoch: 6.49 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7245467141298568		[learning rate: 0.002056]
	Learning Rate: 0.00205601
	LOSS [training: 0.7245467141298568 | validation: 0.45294055449494847]
	TIME [epoch: 6.49 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7120113430928261		[learning rate: 0.0020512]
	Learning Rate: 0.00205116
	LOSS [training: 0.7120113430928261 | validation: 0.4363423717912812]
	TIME [epoch: 6.49 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7088270332070793		[learning rate: 0.0020463]
	Learning Rate: 0.00204632
	LOSS [training: 0.7088270332070793 | validation: 0.4658821571331622]
	TIME [epoch: 6.5 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7468927215724472		[learning rate: 0.0020415]
	Learning Rate: 0.0020415
	LOSS [training: 0.7468927215724472 | validation: 0.44372278040961144]
	TIME [epoch: 6.55 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7127991737892406		[learning rate: 0.0020367]
	Learning Rate: 0.00203668
	LOSS [training: 0.7127991737892406 | validation: 0.43441914415290966]
	TIME [epoch: 6.52 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7173006819230714		[learning rate: 0.0020319]
	Learning Rate: 0.00203188
	LOSS [training: 0.7173006819230714 | validation: 0.46730633209474465]
	TIME [epoch: 6.5 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7204698737613324		[learning rate: 0.0020271]
	Learning Rate: 0.00202708
	LOSS [training: 0.7204698737613324 | validation: 0.4663131860973475]
	TIME [epoch: 6.51 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7193269029990301		[learning rate: 0.0020223]
	Learning Rate: 0.0020223
	LOSS [training: 0.7193269029990301 | validation: 0.47123922404246177]
	TIME [epoch: 6.5 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7154291132245036		[learning rate: 0.0020175]
	Learning Rate: 0.00201753
	LOSS [training: 0.7154291132245036 | validation: 0.4593869593326091]
	TIME [epoch: 6.52 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7176413007593846		[learning rate: 0.0020128]
	Learning Rate: 0.00201277
	LOSS [training: 0.7176413007593846 | validation: 0.43671961000612036]
	TIME [epoch: 6.54 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7008379812487823		[learning rate: 0.002008]
	Learning Rate: 0.00200803
	LOSS [training: 0.7008379812487823 | validation: 0.4437747907653895]
	TIME [epoch: 6.47 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7001911145518666		[learning rate: 0.0020033]
	Learning Rate: 0.00200329
	LOSS [training: 0.7001911145518666 | validation: 0.4406135296492406]
	TIME [epoch: 6.47 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7305319983329711		[learning rate: 0.0019986]
	Learning Rate: 0.00199856
	LOSS [training: 0.7305319983329711 | validation: 0.44404944445078065]
	TIME [epoch: 6.48 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7062796866680453		[learning rate: 0.0019938]
	Learning Rate: 0.00199385
	LOSS [training: 0.7062796866680453 | validation: 0.4367715216054742]
	TIME [epoch: 6.47 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7102522741571551		[learning rate: 0.0019891]
	Learning Rate: 0.00198915
	LOSS [training: 0.7102522741571551 | validation: 0.45117059101166135]
	TIME [epoch: 6.51 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7206977396062061		[learning rate: 0.0019845]
	Learning Rate: 0.00198445
	LOSS [training: 0.7206977396062061 | validation: 0.43086317792252604]
	TIME [epoch: 6.51 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7045765220404248		[learning rate: 0.0019798]
	Learning Rate: 0.00197977
	LOSS [training: 0.7045765220404248 | validation: 0.45114879786679357]
	TIME [epoch: 6.49 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7161010362595202		[learning rate: 0.0019751]
	Learning Rate: 0.0019751
	LOSS [training: 0.7161010362595202 | validation: 0.44888716895938585]
	TIME [epoch: 6.48 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7248183757395288		[learning rate: 0.0019704]
	Learning Rate: 0.00197044
	LOSS [training: 0.7248183757395288 | validation: 0.4531408969162535]
	TIME [epoch: 6.48 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7193556646230383		[learning rate: 0.0019658]
	Learning Rate: 0.0019658
	LOSS [training: 0.7193556646230383 | validation: 0.4638420024285218]
	TIME [epoch: 6.48 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7171979480721552		[learning rate: 0.0019612]
	Learning Rate: 0.00196116
	LOSS [training: 0.7171979480721552 | validation: 0.461966051434639]
	TIME [epoch: 6.52 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7119527867445198		[learning rate: 0.0019565]
	Learning Rate: 0.00195653
	LOSS [training: 0.7119527867445198 | validation: 0.4856377462357436]
	TIME [epoch: 6.48 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7762478600837615		[learning rate: 0.0019519]
	Learning Rate: 0.00195192
	LOSS [training: 0.7762478600837615 | validation: 0.5956535526070748]
	TIME [epoch: 6.48 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8243256620256896		[learning rate: 0.0019473]
	Learning Rate: 0.00194731
	LOSS [training: 0.8243256620256896 | validation: 0.5172514888776213]
	TIME [epoch: 6.48 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7510897878390974		[learning rate: 0.0019427]
	Learning Rate: 0.00194272
	LOSS [training: 0.7510897878390974 | validation: 0.4560749773786916]
	TIME [epoch: 6.48 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7319721232590173		[learning rate: 0.0019381]
	Learning Rate: 0.00193814
	LOSS [training: 0.7319721232590173 | validation: 0.4731763650556987]
	TIME [epoch: 6.49 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7319328134468663		[learning rate: 0.0019336]
	Learning Rate: 0.00193357
	LOSS [training: 0.7319328134468663 | validation: 0.46064257035113665]
	TIME [epoch: 6.52 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7372791895750733		[learning rate: 0.001929]
	Learning Rate: 0.001929
	LOSS [training: 0.7372791895750733 | validation: 0.4534958651769881]
	TIME [epoch: 6.48 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7168182593598195		[learning rate: 0.0019245]
	Learning Rate: 0.00192445
	LOSS [training: 0.7168182593598195 | validation: 0.4509634646871278]
	TIME [epoch: 6.48 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7163488231514428		[learning rate: 0.0019199]
	Learning Rate: 0.00191992
	LOSS [training: 0.7163488231514428 | validation: 0.44450525027123833]
	TIME [epoch: 6.48 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7021589244119454		[learning rate: 0.0019154]
	Learning Rate: 0.00191539
	LOSS [training: 0.7021589244119454 | validation: 0.44541957276157895]
	TIME [epoch: 6.48 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7181786008515207		[learning rate: 0.0019109]
	Learning Rate: 0.00191087
	LOSS [training: 0.7181786008515207 | validation: 0.4376220756245312]
	TIME [epoch: 6.49 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7114142841578952		[learning rate: 0.0019064]
	Learning Rate: 0.00190636
	LOSS [training: 0.7114142841578952 | validation: 0.4614062810034655]
	TIME [epoch: 6.51 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7134083872612631		[learning rate: 0.0019019]
	Learning Rate: 0.00190186
	LOSS [training: 0.7134083872612631 | validation: 0.431888226615743]
	TIME [epoch: 6.5 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7048926405555644		[learning rate: 0.0018974]
	Learning Rate: 0.00189738
	LOSS [training: 0.7048926405555644 | validation: 0.4709448152846747]
	TIME [epoch: 6.5 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.723950113375605		[learning rate: 0.0018929]
	Learning Rate: 0.0018929
	LOSS [training: 0.723950113375605 | validation: 0.4444207273602061]
	TIME [epoch: 6.5 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7087155399867584		[learning rate: 0.0018884]
	Learning Rate: 0.00188844
	LOSS [training: 0.7087155399867584 | validation: 0.44816237509489987]
	TIME [epoch: 6.51 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7407180143973655		[learning rate: 0.001884]
	Learning Rate: 0.00188398
	LOSS [training: 0.7407180143973655 | validation: 0.4943521028317019]
	TIME [epoch: 6.51 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7377143302804008		[learning rate: 0.0018795]
	Learning Rate: 0.00187954
	LOSS [training: 0.7377143302804008 | validation: 0.4824845678858542]
	TIME [epoch: 6.5 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7241877110515288		[learning rate: 0.0018751]
	Learning Rate: 0.00187511
	LOSS [training: 0.7241877110515288 | validation: 0.4568457245819009]
	TIME [epoch: 6.48 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7276712916225313		[learning rate: 0.0018707]
	Learning Rate: 0.00187068
	LOSS [training: 0.7276712916225313 | validation: 0.4964774790292624]
	TIME [epoch: 6.48 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7592359463559898		[learning rate: 0.0018663]
	Learning Rate: 0.00186627
	LOSS [training: 0.7592359463559898 | validation: 0.45453399483453655]
	TIME [epoch: 6.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7099764490445087		[learning rate: 0.0018619]
	Learning Rate: 0.00186187
	LOSS [training: 0.7099764490445087 | validation: 0.44025840681793693]
	TIME [epoch: 6.48 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7078811599765711		[learning rate: 0.0018575]
	Learning Rate: 0.00185748
	LOSS [training: 0.7078811599765711 | validation: 0.4619386163306827]
	TIME [epoch: 6.52 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7234215070412991		[learning rate: 0.0018531]
	Learning Rate: 0.00185309
	LOSS [training: 0.7234215070412991 | validation: 0.4770948883086499]
	TIME [epoch: 6.49 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7471807248074758		[learning rate: 0.0018487]
	Learning Rate: 0.00184872
	LOSS [training: 0.7471807248074758 | validation: 0.5006637836282615]
	TIME [epoch: 6.5 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7442422619416897		[learning rate: 0.0018444]
	Learning Rate: 0.00184436
	LOSS [training: 0.7442422619416897 | validation: 0.493656971559666]
	TIME [epoch: 6.47 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7281735904739463		[learning rate: 0.00184]
	Learning Rate: 0.00184001
	LOSS [training: 0.7281735904739463 | validation: 0.46500690225273633]
	TIME [epoch: 6.47 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7350077747394504		[learning rate: 0.0018357]
	Learning Rate: 0.00183567
	LOSS [training: 0.7350077747394504 | validation: 0.4891581569490191]
	TIME [epoch: 6.48 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7343916880046968		[learning rate: 0.0018313]
	Learning Rate: 0.00183134
	LOSS [training: 0.7343916880046968 | validation: 0.4565400272026917]
	TIME [epoch: 6.52 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.721425415533928		[learning rate: 0.001827]
	Learning Rate: 0.00182702
	LOSS [training: 0.721425415533928 | validation: 0.44950420187022905]
	TIME [epoch: 6.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7168154030119742		[learning rate: 0.0018227]
	Learning Rate: 0.00182271
	LOSS [training: 0.7168154030119742 | validation: 0.4735606647770522]
	TIME [epoch: 6.49 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7195691262888955		[learning rate: 0.0018184]
	Learning Rate: 0.00181841
	LOSS [training: 0.7195691262888955 | validation: 0.45255350142856654]
	TIME [epoch: 6.48 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7140955976480944		[learning rate: 0.0018141]
	Learning Rate: 0.00181412
	LOSS [training: 0.7140955976480944 | validation: 0.44681783554871324]
	TIME [epoch: 6.5 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7109285997415195		[learning rate: 0.0018098]
	Learning Rate: 0.00180984
	LOSS [training: 0.7109285997415195 | validation: 0.5002900175268604]
	TIME [epoch: 6.49 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7292598211828204		[learning rate: 0.0018056]
	Learning Rate: 0.00180557
	LOSS [training: 0.7292598211828204 | validation: 0.5238956152276096]
	TIME [epoch: 6.54 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7390297136134434		[learning rate: 0.0018013]
	Learning Rate: 0.00180132
	LOSS [training: 0.7390297136134434 | validation: 0.4598397538001898]
	TIME [epoch: 6.48 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7102648460695535		[learning rate: 0.0017971]
	Learning Rate: 0.00179707
	LOSS [training: 0.7102648460695535 | validation: 0.4569033274009961]
	TIME [epoch: 6.47 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7181785304067327		[learning rate: 0.0017928]
	Learning Rate: 0.00179283
	LOSS [training: 0.7181785304067327 | validation: 0.4468702982720609]
	TIME [epoch: 6.47 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7275629308069285		[learning rate: 0.0017886]
	Learning Rate: 0.0017886
	LOSS [training: 0.7275629308069285 | validation: 0.46137217453505447]
	TIME [epoch: 6.48 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7370830808955559		[learning rate: 0.0017844]
	Learning Rate: 0.00178438
	LOSS [training: 0.7370830808955559 | validation: 0.4690556793342774]
	TIME [epoch: 6.51 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7198783024519043		[learning rate: 0.0017802]
	Learning Rate: 0.00178017
	LOSS [training: 0.7198783024519043 | validation: 0.46114055948021787]
	TIME [epoch: 6.5 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.722791110213816		[learning rate: 0.001776]
	Learning Rate: 0.00177597
	LOSS [training: 0.722791110213816 | validation: 0.46138295257937095]
	TIME [epoch: 6.48 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7178957245036178		[learning rate: 0.0017718]
	Learning Rate: 0.00177178
	LOSS [training: 0.7178957245036178 | validation: 0.4648374328507175]
	TIME [epoch: 6.49 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7207295841273598		[learning rate: 0.0017676]
	Learning Rate: 0.0017676
	LOSS [training: 0.7207295841273598 | validation: 0.45361041830762266]
	TIME [epoch: 6.49 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7151529576291418		[learning rate: 0.0017634]
	Learning Rate: 0.00176343
	LOSS [training: 0.7151529576291418 | validation: 0.4480084689997102]
	TIME [epoch: 6.49 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7191451914643958		[learning rate: 0.0017593]
	Learning Rate: 0.00175927
	LOSS [training: 0.7191451914643958 | validation: 0.4421160598936726]
	TIME [epoch: 6.53 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7175135616522704		[learning rate: 0.0017551]
	Learning Rate: 0.00175512
	LOSS [training: 0.7175135616522704 | validation: 0.4550059580716185]
	TIME [epoch: 6.5 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7121074797158811		[learning rate: 0.001751]
	Learning Rate: 0.00175098
	LOSS [training: 0.7121074797158811 | validation: 0.437522461824684]
	TIME [epoch: 6.48 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7110348695467806		[learning rate: 0.0017469]
	Learning Rate: 0.00174685
	LOSS [training: 0.7110348695467806 | validation: 0.448999620112486]
	TIME [epoch: 6.48 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.71774318698092		[learning rate: 0.0017427]
	Learning Rate: 0.00174273
	LOSS [training: 0.71774318698092 | validation: 0.4473252976807758]
	TIME [epoch: 6.47 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7128855635162493		[learning rate: 0.0017386]
	Learning Rate: 0.00173862
	LOSS [training: 0.7128855635162493 | validation: 0.44447633134927]
	TIME [epoch: 6.48 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7128852776362896		[learning rate: 0.0017345]
	Learning Rate: 0.00173452
	LOSS [training: 0.7128852776362896 | validation: 0.44052067748317225]
	TIME [epoch: 6.52 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7134943984270302		[learning rate: 0.0017304]
	Learning Rate: 0.00173043
	LOSS [training: 0.7134943984270302 | validation: 0.47043076651048726]
	TIME [epoch: 6.48 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7226667932265128		[learning rate: 0.0017263]
	Learning Rate: 0.00172635
	LOSS [training: 0.7226667932265128 | validation: 0.45794199934397883]
	TIME [epoch: 6.48 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7125939573398569		[learning rate: 0.0017223]
	Learning Rate: 0.00172228
	LOSS [training: 0.7125939573398569 | validation: 0.4497135560876661]
	TIME [epoch: 6.48 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7053092882974351		[learning rate: 0.0017182]
	Learning Rate: 0.00171821
	LOSS [training: 0.7053092882974351 | validation: 0.4505851955658675]
	TIME [epoch: 6.47 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7461021835108321		[learning rate: 0.0017142]
	Learning Rate: 0.00171416
	LOSS [training: 0.7461021835108321 | validation: 0.5455455247021336]
	TIME [epoch: 6.5 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7570705064290708		[learning rate: 0.0017101]
	Learning Rate: 0.00171012
	LOSS [training: 0.7570705064290708 | validation: 0.4860924991141638]
	TIME [epoch: 6.5 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7231835584278746		[learning rate: 0.0017061]
	Learning Rate: 0.00170608
	LOSS [training: 0.7231835584278746 | validation: 0.43120801982842843]
	TIME [epoch: 6.48 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7073532332523005		[learning rate: 0.0017021]
	Learning Rate: 0.00170206
	LOSS [training: 0.7073532332523005 | validation: 0.43721238842413435]
	TIME [epoch: 6.48 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7115179413485151		[learning rate: 0.001698]
	Learning Rate: 0.00169804
	LOSS [training: 0.7115179413485151 | validation: 0.43161209600763095]
	TIME [epoch: 6.47 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7093291508169318		[learning rate: 0.001694]
	Learning Rate: 0.00169404
	LOSS [training: 0.7093291508169318 | validation: 0.45627235677329475]
	TIME [epoch: 6.47 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7059915198812331		[learning rate: 0.00169]
	Learning Rate: 0.00169004
	LOSS [training: 0.7059915198812331 | validation: 0.4761150634750154]
	TIME [epoch: 6.5 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7174292713697041		[learning rate: 0.0016861]
	Learning Rate: 0.00168606
	LOSS [training: 0.7174292713697041 | validation: 0.4436964701232128]
	TIME [epoch: 6.5 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7103119428094781		[learning rate: 0.0016821]
	Learning Rate: 0.00168208
	LOSS [training: 0.7103119428094781 | validation: 0.4566218927217038]
	TIME [epoch: 6.47 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.70425447489553		[learning rate: 0.0016781]
	Learning Rate: 0.00167811
	LOSS [training: 0.70425447489553 | validation: 0.45253794235287315]
	TIME [epoch: 6.48 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7254145635081743		[learning rate: 0.0016742]
	Learning Rate: 0.00167415
	LOSS [training: 0.7254145635081743 | validation: 0.45102917264425596]
	TIME [epoch: 6.48 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7274636485852863		[learning rate: 0.0016702]
	Learning Rate: 0.0016702
	LOSS [training: 0.7274636485852863 | validation: 0.4518542161605359]
	TIME [epoch: 6.48 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7155770999311024		[learning rate: 0.0016663]
	Learning Rate: 0.00166626
	LOSS [training: 0.7155770999311024 | validation: 0.4517610414927484]
	TIME [epoch: 6.52 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7046629927779153		[learning rate: 0.0016623]
	Learning Rate: 0.00166233
	LOSS [training: 0.7046629927779153 | validation: 0.446930016300457]
	TIME [epoch: 6.49 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.702967101299549		[learning rate: 0.0016584]
	Learning Rate: 0.00165841
	LOSS [training: 0.702967101299549 | validation: 0.4508851002948277]
	TIME [epoch: 6.49 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7094087430003846		[learning rate: 0.0016545]
	Learning Rate: 0.0016545
	LOSS [training: 0.7094087430003846 | validation: 0.443139123560406]
	TIME [epoch: 6.49 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7119976233505512		[learning rate: 0.0016506]
	Learning Rate: 0.0016506
	LOSS [training: 0.7119976233505512 | validation: 0.43557679719669784]
	TIME [epoch: 6.5 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7038707883856002		[learning rate: 0.0016467]
	Learning Rate: 0.0016467
	LOSS [training: 0.7038707883856002 | validation: 0.4769160307918642]
	TIME [epoch: 6.49 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7408814760442942		[learning rate: 0.0016428]
	Learning Rate: 0.00164282
	LOSS [training: 0.7408814760442942 | validation: 0.4502133428937328]
	TIME [epoch: 6.55 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7105157901747063		[learning rate: 0.0016389]
	Learning Rate: 0.00163894
	LOSS [training: 0.7105157901747063 | validation: 0.4663230889402393]
	TIME [epoch: 6.49 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7141120241378701		[learning rate: 0.0016351]
	Learning Rate: 0.00163508
	LOSS [training: 0.7141120241378701 | validation: 0.45205386810411025]
	TIME [epoch: 6.5 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7223913961319527		[learning rate: 0.0016312]
	Learning Rate: 0.00163122
	LOSS [training: 0.7223913961319527 | validation: 0.44996117849921724]
	TIME [epoch: 6.49 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7234908503146419		[learning rate: 0.0016274]
	Learning Rate: 0.00162737
	LOSS [training: 0.7234908503146419 | validation: 0.4639970668024419]
	TIME [epoch: 6.5 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7087985067358078		[learning rate: 0.0016235]
	Learning Rate: 0.00162353
	LOSS [training: 0.7087985067358078 | validation: 0.4396102388096734]
	TIME [epoch: 6.51 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7075201606917876		[learning rate: 0.0016197]
	Learning Rate: 0.0016197
	LOSS [training: 0.7075201606917876 | validation: 0.4412962730978591]
	TIME [epoch: 6.54 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7081939856780327		[learning rate: 0.0016159]
	Learning Rate: 0.00161588
	LOSS [training: 0.7081939856780327 | validation: 0.456695698987751]
	TIME [epoch: 6.51 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7179992136041704		[learning rate: 0.0016121]
	Learning Rate: 0.00161207
	LOSS [training: 0.7179992136041704 | validation: 0.4350856808913837]
	TIME [epoch: 6.49 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7030642921760298		[learning rate: 0.0016083]
	Learning Rate: 0.00160827
	LOSS [training: 0.7030642921760298 | validation: 0.4415165843457527]
	TIME [epoch: 6.51 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7105955015063031		[learning rate: 0.0016045]
	Learning Rate: 0.00160448
	LOSS [training: 0.7105955015063031 | validation: 0.4494031532809879]
	TIME [epoch: 6.49 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7094479410497768		[learning rate: 0.0016007]
	Learning Rate: 0.00160069
	LOSS [training: 0.7094479410497768 | validation: 0.4424043944118029]
	TIME [epoch: 6.52 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7076602540441053		[learning rate: 0.0015969]
	Learning Rate: 0.00159692
	LOSS [training: 0.7076602540441053 | validation: 0.44929041016161825]
	TIME [epoch: 6.51 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7109031667101638		[learning rate: 0.0015931]
	Learning Rate: 0.00159315
	LOSS [training: 0.7109031667101638 | validation: 0.46027163873035026]
	TIME [epoch: 6.5 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7169738925631262		[learning rate: 0.0015894]
	Learning Rate: 0.00158939
	LOSS [training: 0.7169738925631262 | validation: 0.46839183290009656]
	TIME [epoch: 6.5 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.723690704870889		[learning rate: 0.0015856]
	Learning Rate: 0.00158564
	LOSS [training: 0.723690704870889 | validation: 0.4709006226036876]
	TIME [epoch: 6.5 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7192705314519993		[learning rate: 0.0015819]
	Learning Rate: 0.0015819
	LOSS [training: 0.7192705314519993 | validation: 0.44219782472336816]
	TIME [epoch: 6.5 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7136059455211818		[learning rate: 0.0015782]
	Learning Rate: 0.00157817
	LOSS [training: 0.7136059455211818 | validation: 0.46945564110835997]
	TIME [epoch: 6.53 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7447752358781725		[learning rate: 0.0015744]
	Learning Rate: 0.00157445
	LOSS [training: 0.7447752358781725 | validation: 0.44180452725702163]
	TIME [epoch: 6.49 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.70910495516375		[learning rate: 0.0015707]
	Learning Rate: 0.00157073
	LOSS [training: 0.70910495516375 | validation: 0.4449820996425059]
	TIME [epoch: 6.5 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7074669510030478		[learning rate: 0.001567]
	Learning Rate: 0.00156703
	LOSS [training: 0.7074669510030478 | validation: 0.4430181915113255]
	TIME [epoch: 6.49 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7192384193770881		[learning rate: 0.0015633]
	Learning Rate: 0.00156333
	LOSS [training: 0.7192384193770881 | validation: 0.4548486647517954]
	TIME [epoch: 6.48 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7179046348747352		[learning rate: 0.0015596]
	Learning Rate: 0.00155964
	LOSS [training: 0.7179046348747352 | validation: 0.4373446578241364]
	TIME [epoch: 6.5 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7028163696915729		[learning rate: 0.001556]
	Learning Rate: 0.00155597
	LOSS [training: 0.7028163696915729 | validation: 0.4777975494333733]
	TIME [epoch: 6.54 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7136757375473554		[learning rate: 0.0015523]
	Learning Rate: 0.0015523
	LOSS [training: 0.7136757375473554 | validation: 0.4447716487315654]
	TIME [epoch: 6.48 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7168790442608426		[learning rate: 0.0015486]
	Learning Rate: 0.00154863
	LOSS [training: 0.7168790442608426 | validation: 0.44112275204991386]
	TIME [epoch: 6.5 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6993065096311712		[learning rate: 0.001545]
	Learning Rate: 0.00154498
	LOSS [training: 0.6993065096311712 | validation: 0.4407104510577322]
	TIME [epoch: 6.49 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7031308821030244		[learning rate: 0.0015413]
	Learning Rate: 0.00154134
	LOSS [training: 0.7031308821030244 | validation: 0.4510580715481039]
	TIME [epoch: 6.49 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7286781496310385		[learning rate: 0.0015377]
	Learning Rate: 0.0015377
	LOSS [training: 0.7286781496310385 | validation: 0.4887575605039763]
	TIME [epoch: 6.52 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7207773650749234		[learning rate: 0.0015341]
	Learning Rate: 0.00153407
	LOSS [training: 0.7207773650749234 | validation: 0.4383433220555534]
	TIME [epoch: 6.53 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7026765987822533		[learning rate: 0.0015305]
	Learning Rate: 0.00153045
	LOSS [training: 0.7026765987822533 | validation: 0.44042548856366526]
	TIME [epoch: 6.49 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7099164154659892		[learning rate: 0.0015268]
	Learning Rate: 0.00152684
	LOSS [training: 0.7099164154659892 | validation: 0.4655426988552344]
	TIME [epoch: 6.5 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7197967050522994		[learning rate: 0.0015232]
	Learning Rate: 0.00152324
	LOSS [training: 0.7197967050522994 | validation: 0.4298739331031898]
	TIME [epoch: 6.5 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6986786220346379		[learning rate: 0.0015196]
	Learning Rate: 0.00151965
	LOSS [training: 0.6986786220346379 | validation: 0.4364243465525485]
	TIME [epoch: 6.5 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7003065945375185		[learning rate: 0.0015161]
	Learning Rate: 0.00151607
	LOSS [training: 0.7003065945375185 | validation: 0.4511279995690138]
	TIME [epoch: 6.53 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7085951990468682		[learning rate: 0.0015125]
	Learning Rate: 0.00151249
	LOSS [training: 0.7085951990468682 | validation: 0.44889219706109906]
	TIME [epoch: 6.52 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7090429646086813		[learning rate: 0.0015089]
	Learning Rate: 0.00150892
	LOSS [training: 0.7090429646086813 | validation: 0.4265814575875788]
	TIME [epoch: 6.5 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.709284431394877		[learning rate: 0.0015054]
	Learning Rate: 0.00150536
	LOSS [training: 0.709284431394877 | validation: 0.4397240553828765]
	TIME [epoch: 6.5 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7037471783097304		[learning rate: 0.0015018]
	Learning Rate: 0.00150181
	LOSS [training: 0.7037471783097304 | validation: 0.448750959521112]
	TIME [epoch: 6.49 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.716635486872122		[learning rate: 0.0014983]
	Learning Rate: 0.00149827
	LOSS [training: 0.716635486872122 | validation: 0.48335527035067094]
	TIME [epoch: 6.49 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7222016504164752		[learning rate: 0.0014947]
	Learning Rate: 0.00149473
	LOSS [training: 0.7222016504164752 | validation: 0.45172534263504793]
	TIME [epoch: 6.54 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7085635882524042		[learning rate: 0.0014912]
	Learning Rate: 0.00149121
	LOSS [training: 0.7085635882524042 | validation: 0.44439395639470697]
	TIME [epoch: 6.5 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7130768096881712		[learning rate: 0.0014877]
	Learning Rate: 0.00148769
	LOSS [training: 0.7130768096881712 | validation: 0.44734259228062223]
	TIME [epoch: 6.5 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.709446307773597		[learning rate: 0.0014842]
	Learning Rate: 0.00148418
	LOSS [training: 0.709446307773597 | validation: 0.45106068439730873]
	TIME [epoch: 6.48 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7140055637489622		[learning rate: 0.0014807]
	Learning Rate: 0.00148068
	LOSS [training: 0.7140055637489622 | validation: 0.4360473539904964]
	TIME [epoch: 6.5 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7008562236621592		[learning rate: 0.0014772]
	Learning Rate: 0.00147719
	LOSS [training: 0.7008562236621592 | validation: 0.44951603451343874]
	TIME [epoch: 6.5 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7017867575329092		[learning rate: 0.0014737]
	Learning Rate: 0.0014737
	LOSS [training: 0.7017867575329092 | validation: 0.44916925743249214]
	TIME [epoch: 6.55 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7084785477207121		[learning rate: 0.0014702]
	Learning Rate: 0.00147023
	LOSS [training: 0.7084785477207121 | validation: 0.4369520333678974]
	TIME [epoch: 6.48 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7131126677411983		[learning rate: 0.0014668]
	Learning Rate: 0.00146676
	LOSS [training: 0.7131126677411983 | validation: 0.45065135088285824]
	TIME [epoch: 6.48 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7052420484367972		[learning rate: 0.0014633]
	Learning Rate: 0.0014633
	LOSS [training: 0.7052420484367972 | validation: 0.4301997158026494]
	TIME [epoch: 6.48 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7000229789821069		[learning rate: 0.0014598]
	Learning Rate: 0.00145985
	LOSS [training: 0.7000229789821069 | validation: 0.43432147036140434]
	TIME [epoch: 6.49 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7069973018135747		[learning rate: 0.0014564]
	Learning Rate: 0.0014564
	LOSS [training: 0.7069973018135747 | validation: 0.4509009469943473]
	TIME [epoch: 6.49 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7089793706349183		[learning rate: 0.001453]
	Learning Rate: 0.00145297
	LOSS [training: 0.7089793706349183 | validation: 0.44045966108790413]
	TIME [epoch: 6.53 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7083323317613812		[learning rate: 0.0014495]
	Learning Rate: 0.00144954
	LOSS [training: 0.7083323317613812 | validation: 0.4560118247594801]
	TIME [epoch: 6.49 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7053414408375155		[learning rate: 0.0014461]
	Learning Rate: 0.00144612
	LOSS [training: 0.7053414408375155 | validation: 0.42827573247139095]
	TIME [epoch: 6.49 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7029976792655789		[learning rate: 0.0014427]
	Learning Rate: 0.00144271
	LOSS [training: 0.7029976792655789 | validation: 0.46020327572759534]
	TIME [epoch: 6.5 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7201440132130053		[learning rate: 0.0014393]
	Learning Rate: 0.00143931
	LOSS [training: 0.7201440132130053 | validation: 0.4392263948181772]
	TIME [epoch: 6.48 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7405941129221646		[learning rate: 0.0014359]
	Learning Rate: 0.00143591
	LOSS [training: 0.7405941129221646 | validation: 0.4524922030956045]
	TIME [epoch: 6.51 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7036385855197433		[learning rate: 0.0014325]
	Learning Rate: 0.00143253
	LOSS [training: 0.7036385855197433 | validation: 0.46845211615885773]
	TIME [epoch: 6.51 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.70749157707788		[learning rate: 0.0014291]
	Learning Rate: 0.00142915
	LOSS [training: 0.70749157707788 | validation: 0.4327406756990996]
	TIME [epoch: 6.48 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6921272104807872		[learning rate: 0.0014258]
	Learning Rate: 0.00142578
	LOSS [training: 0.6921272104807872 | validation: 0.43706692855586493]
	TIME [epoch: 6.5 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7069319901576276		[learning rate: 0.0014224]
	Learning Rate: 0.00142241
	LOSS [training: 0.7069319901576276 | validation: 0.4554709515454234]
	TIME [epoch: 6.5 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.728318877059326		[learning rate: 0.0014191]
	Learning Rate: 0.00141906
	LOSS [training: 0.728318877059326 | validation: 0.4799842656526901]
	TIME [epoch: 6.48 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7297122680556416		[learning rate: 0.0014157]
	Learning Rate: 0.00141571
	LOSS [training: 0.7297122680556416 | validation: 0.46826597740519693]
	TIME [epoch: 6.55 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7230807655072917		[learning rate: 0.0014124]
	Learning Rate: 0.00141237
	LOSS [training: 0.7230807655072917 | validation: 0.44633632009310764]
	TIME [epoch: 6.5 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7107065015441768		[learning rate: 0.001409]
	Learning Rate: 0.00140904
	LOSS [training: 0.7107065015441768 | validation: 0.45591370547378146]
	TIME [epoch: 6.5 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7221316234401861		[learning rate: 0.0014057]
	Learning Rate: 0.00140572
	LOSS [training: 0.7221316234401861 | validation: 0.46324023213446747]
	TIME [epoch: 6.5 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7117526847640502		[learning rate: 0.0014024]
	Learning Rate: 0.0014024
	LOSS [training: 0.7117526847640502 | validation: 0.43308973857377175]
	TIME [epoch: 6.5 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7053447549082354		[learning rate: 0.0013991]
	Learning Rate: 0.00139909
	LOSS [training: 0.7053447549082354 | validation: 0.4460934386291973]
	TIME [epoch: 6.5 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7247502177442366		[learning rate: 0.0013958]
	Learning Rate: 0.00139579
	LOSS [training: 0.7247502177442366 | validation: 0.5025992905811296]
	TIME [epoch: 6.55 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7965054433956716		[learning rate: 0.0013925]
	Learning Rate: 0.0013925
	LOSS [training: 0.7965054433956716 | validation: 0.5177040202977717]
	TIME [epoch: 6.51 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7618079486484662		[learning rate: 0.0013892]
	Learning Rate: 0.00138921
	LOSS [training: 0.7618079486484662 | validation: 0.49491998364893147]
	TIME [epoch: 6.5 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7860043478944927		[learning rate: 0.0013859]
	Learning Rate: 0.00138594
	LOSS [training: 0.7860043478944927 | validation: 0.5236031876534906]
	TIME [epoch: 6.5 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7598772896061041		[learning rate: 0.0013827]
	Learning Rate: 0.00138267
	LOSS [training: 0.7598772896061041 | validation: 0.4835448987058294]
	TIME [epoch: 6.5 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7594754209251147		[learning rate: 0.0013794]
	Learning Rate: 0.00137941
	LOSS [training: 0.7594754209251147 | validation: 0.48138251380909786]
	TIME [epoch: 6.52 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.738379885445113		[learning rate: 0.0013762]
	Learning Rate: 0.00137615
	LOSS [training: 0.738379885445113 | validation: 0.4633696185882694]
	TIME [epoch: 6.54 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7279106634272052		[learning rate: 0.0013729]
	Learning Rate: 0.00137291
	LOSS [training: 0.7279106634272052 | validation: 0.5269461116979989]
	TIME [epoch: 6.52 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8002271917105659		[learning rate: 0.0013697]
	Learning Rate: 0.00136967
	LOSS [training: 0.8002271917105659 | validation: 0.5043528775379239]
	TIME [epoch: 6.51 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7813567924323725		[learning rate: 0.0013664]
	Learning Rate: 0.00136644
	LOSS [training: 0.7813567924323725 | validation: 0.4849592621683154]
	TIME [epoch: 6.51 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.747713218245298		[learning rate: 0.0013632]
	Learning Rate: 0.00136321
	LOSS [training: 0.747713218245298 | validation: 0.4680244317681344]
	TIME [epoch: 6.5 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7433891293935218		[learning rate: 0.00136]
	Learning Rate: 0.00136
	LOSS [training: 0.7433891293935218 | validation: 0.49455469188534024]
	TIME [epoch: 6.54 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7677284829677944		[learning rate: 0.0013568]
	Learning Rate: 0.00135679
	LOSS [training: 0.7677284829677944 | validation: 0.4997490382813349]
	TIME [epoch: 6.51 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7481656339369099		[learning rate: 0.0013536]
	Learning Rate: 0.00135359
	LOSS [training: 0.7481656339369099 | validation: 0.4655726026604772]
	TIME [epoch: 6.49 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8175108210422403		[learning rate: 0.0013504]
	Learning Rate: 0.0013504
	LOSS [training: 0.8175108210422403 | validation: 0.589863398021859]
	TIME [epoch: 6.49 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7964067499710943		[learning rate: 0.0013472]
	Learning Rate: 0.00134721
	LOSS [training: 0.7964067499710943 | validation: 0.5088134144568519]
	TIME [epoch: 6.49 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7590037048272288		[learning rate: 0.001344]
	Learning Rate: 0.00134403
	LOSS [training: 0.7590037048272288 | validation: 0.47419736956001385]
	TIME [epoch: 6.5 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7263893321907158		[learning rate: 0.0013409]
	Learning Rate: 0.00134086
	LOSS [training: 0.7263893321907158 | validation: 0.49641863017640087]
	TIME [epoch: 6.54 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7375983324130384		[learning rate: 0.0013377]
	Learning Rate: 0.0013377
	LOSS [training: 0.7375983324130384 | validation: 0.450918916549504]
	TIME [epoch: 6.5 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7127730519721913		[learning rate: 0.0013345]
	Learning Rate: 0.00133455
	LOSS [training: 0.7127730519721913 | validation: 0.44934845627432984]
	TIME [epoch: 6.49 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7068826659581722		[learning rate: 0.0013314]
	Learning Rate: 0.0013314
	LOSS [training: 0.7068826659581722 | validation: 0.45698714500673576]
	TIME [epoch: 6.5 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7109521100768633		[learning rate: 0.0013283]
	Learning Rate: 0.00132826
	LOSS [training: 0.7109521100768633 | validation: 0.433500647563581]
	TIME [epoch: 6.5 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7078240261672019		[learning rate: 0.0013251]
	Learning Rate: 0.00132512
	LOSS [training: 0.7078240261672019 | validation: 0.43778689059118403]
	TIME [epoch: 6.5 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6986512714004638		[learning rate: 0.001322]
	Learning Rate: 0.001322
	LOSS [training: 0.6986512714004638 | validation: 0.43975915036481694]
	TIME [epoch: 6.55 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.702589097882959		[learning rate: 0.0013189]
	Learning Rate: 0.00131888
	LOSS [training: 0.702589097882959 | validation: 0.42924853525697204]
	TIME [epoch: 6.52 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7113255423339396		[learning rate: 0.0013158]
	Learning Rate: 0.00131577
	LOSS [training: 0.7113255423339396 | validation: 0.47107579666837585]
	TIME [epoch: 6.5 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7190105826784756		[learning rate: 0.0013127]
	Learning Rate: 0.00131266
	LOSS [training: 0.7190105826784756 | validation: 0.45905831639533695]
	TIME [epoch: 6.49 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7121556389523356		[learning rate: 0.0013096]
	Learning Rate: 0.00130957
	LOSS [training: 0.7121556389523356 | validation: 0.4548490537032833]
	TIME [epoch: 6.49 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7143331343001599		[learning rate: 0.0013065]
	Learning Rate: 0.00130648
	LOSS [training: 0.7143331343001599 | validation: 0.4478014901745415]
	TIME [epoch: 6.52 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7157876543828		[learning rate: 0.0013034]
	Learning Rate: 0.0013034
	LOSS [training: 0.7157876543828 | validation: 0.44483561416009015]
	TIME [epoch: 6.53 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7172217648932986		[learning rate: 0.0013003]
	Learning Rate: 0.00130032
	LOSS [training: 0.7172217648932986 | validation: 0.44555603690604095]
	TIME [epoch: 6.49 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7092576634493891		[learning rate: 0.0012973]
	Learning Rate: 0.00129726
	LOSS [training: 0.7092576634493891 | validation: 0.44632472596539474]
	TIME [epoch: 6.48 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.713174168066361		[learning rate: 0.0012942]
	Learning Rate: 0.0012942
	LOSS [training: 0.713174168066361 | validation: 0.47983371540677167]
	TIME [epoch: 6.5 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7175173043216918		[learning rate: 0.0012911]
	Learning Rate: 0.00129114
	LOSS [training: 0.7175173043216918 | validation: 0.4811063989389289]
	TIME [epoch: 6.48 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7714779275473476		[learning rate: 0.0012881]
	Learning Rate: 0.0012881
	LOSS [training: 0.7714779275473476 | validation: 0.4511267552637595]
	TIME [epoch: 6.54 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7166817529874429		[learning rate: 0.0012851]
	Learning Rate: 0.00128506
	LOSS [training: 0.7166817529874429 | validation: 0.44033170547713635]
	TIME [epoch: 6.5 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7064352142642432		[learning rate: 0.001282]
	Learning Rate: 0.00128203
	LOSS [training: 0.7064352142642432 | validation: 0.4406611913488263]
	TIME [epoch: 6.48 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6997967554363298		[learning rate: 0.001279]
	Learning Rate: 0.001279
	LOSS [training: 0.6997967554363298 | validation: 0.4422368619209859]
	TIME [epoch: 6.48 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7063032456773115		[learning rate: 0.001276]
	Learning Rate: 0.00127599
	LOSS [training: 0.7063032456773115 | validation: 0.4417365842988129]
	TIME [epoch: 6.48 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7020571942639178		[learning rate: 0.001273]
	Learning Rate: 0.00127298
	LOSS [training: 0.7020571942639178 | validation: 0.45057719432701626]
	TIME [epoch: 6.48 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7134165240099202		[learning rate: 0.00127]
	Learning Rate: 0.00126997
	LOSS [training: 0.7134165240099202 | validation: 0.45334482069121185]
	TIME [epoch: 6.52 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7205956747505017		[learning rate: 0.001267]
	Learning Rate: 0.00126698
	LOSS [training: 0.7205956747505017 | validation: 0.4692974738286838]
	TIME [epoch: 6.49 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7599910022390741		[learning rate: 0.001264]
	Learning Rate: 0.00126399
	LOSS [training: 0.7599910022390741 | validation: 0.5368305172255652]
	TIME [epoch: 6.48 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8055666229799796		[learning rate: 0.001261]
	Learning Rate: 0.00126101
	LOSS [training: 0.8055666229799796 | validation: 0.564539335133281]
	TIME [epoch: 6.47 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8041265483671174		[learning rate: 0.001258]
	Learning Rate: 0.00125803
	LOSS [training: 0.8041265483671174 | validation: 0.507480369062039]
	TIME [epoch: 6.49 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7635966644522638		[learning rate: 0.0012551]
	Learning Rate: 0.00125507
	LOSS [training: 0.7635966644522638 | validation: 0.497019513034185]
	TIME [epoch: 6.51 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7457717253817882		[learning rate: 0.0012521]
	Learning Rate: 0.00125211
	LOSS [training: 0.7457717253817882 | validation: 0.4732855383183813]
	TIME [epoch: 6.55 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7446828865628026		[learning rate: 0.0012492]
	Learning Rate: 0.00124915
	LOSS [training: 0.7446828865628026 | validation: 0.4823431003356434]
	TIME [epoch: 6.48 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.741207158810778		[learning rate: 0.0012462]
	Learning Rate: 0.00124621
	LOSS [training: 0.741207158810778 | validation: 0.4643703206095664]
	TIME [epoch: 6.49 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7347452899453123		[learning rate: 0.0012433]
	Learning Rate: 0.00124327
	LOSS [training: 0.7347452899453123 | validation: 0.4611327261374587]
	TIME [epoch: 6.48 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7170409898112002		[learning rate: 0.0012403]
	Learning Rate: 0.00124033
	LOSS [training: 0.7170409898112002 | validation: 0.4479983230250229]
	TIME [epoch: 6.48 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7095473303230468		[learning rate: 0.0012374]
	Learning Rate: 0.00123741
	LOSS [training: 0.7095473303230468 | validation: 0.4594451241322558]
	TIME [epoch: 6.51 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7172668973905368		[learning rate: 0.0012345]
	Learning Rate: 0.00123449
	LOSS [training: 0.7172668973905368 | validation: 0.45273806705086794]
	TIME [epoch: 6.53 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7240165039140581		[learning rate: 0.0012316]
	Learning Rate: 0.00123158
	LOSS [training: 0.7240165039140581 | validation: 0.43288974110922696]
	TIME [epoch: 6.49 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7141636653352892		[learning rate: 0.0012287]
	Learning Rate: 0.00122867
	LOSS [training: 0.7141636653352892 | validation: 0.44427413729099974]
	TIME [epoch: 6.5 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7143436856632635		[learning rate: 0.0012258]
	Learning Rate: 0.00122577
	LOSS [training: 0.7143436856632635 | validation: 0.4681582007212384]
	TIME [epoch: 6.48 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.727512086932		[learning rate: 0.0012229]
	Learning Rate: 0.00122288
	LOSS [training: 0.727512086932 | validation: 0.4809539874271407]
	TIME [epoch: 6.48 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7372513562118185		[learning rate: 0.00122]
	Learning Rate: 0.00122
	LOSS [training: 0.7372513562118185 | validation: 0.47815788091414907]
	TIME [epoch: 6.53 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7355593290429676		[learning rate: 0.0012171]
	Learning Rate: 0.00121712
	LOSS [training: 0.7355593290429676 | validation: 0.4429173498160533]
	TIME [epoch: 6.5 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7145749515091868		[learning rate: 0.0012142]
	Learning Rate: 0.00121425
	LOSS [training: 0.7145749515091868 | validation: 0.4620268169508476]
	TIME [epoch: 6.49 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.717957628944128		[learning rate: 0.0012114]
	Learning Rate: 0.00121138
	LOSS [training: 0.717957628944128 | validation: 0.45649805924777637]
	TIME [epoch: 6.5 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7148020036084856		[learning rate: 0.0012085]
	Learning Rate: 0.00120853
	LOSS [training: 0.7148020036084856 | validation: 0.44872624065877065]
	TIME [epoch: 6.47 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7104472952435336		[learning rate: 0.0012057]
	Learning Rate: 0.00120568
	LOSS [training: 0.7104472952435336 | validation: 0.4437157990228004]
	TIME [epoch: 6.47 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7062823707792824		[learning rate: 0.0012028]
	Learning Rate: 0.00120283
	LOSS [training: 0.7062823707792824 | validation: 0.46672307028689164]
	TIME [epoch: 6.53 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7223658222011743		[learning rate: 0.0012]
	Learning Rate: 0.0012
	LOSS [training: 0.7223658222011743 | validation: 0.4440980593953078]
	TIME [epoch: 6.48 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7222892250287687		[learning rate: 0.0011972]
	Learning Rate: 0.00119716
	LOSS [training: 0.7222892250287687 | validation: 0.4574027424800281]
	TIME [epoch: 6.47 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7136773724120838		[learning rate: 0.0011943]
	Learning Rate: 0.00119434
	LOSS [training: 0.7136773724120838 | validation: 0.4449726204179516]
	TIME [epoch: 6.47 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7074483105481034		[learning rate: 0.0011915]
	Learning Rate: 0.00119152
	LOSS [training: 0.7074483105481034 | validation: 0.44315085832418]
	TIME [epoch: 6.48 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7074642542389259		[learning rate: 0.0011887]
	Learning Rate: 0.00118871
	LOSS [training: 0.7074642542389259 | validation: 0.44770553650601286]
	TIME [epoch: 6.48 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7146501118190491		[learning rate: 0.0011859]
	Learning Rate: 0.00118591
	LOSS [training: 0.7146501118190491 | validation: 0.45797080548117036]
	TIME [epoch: 6.52 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7055936864731471		[learning rate: 0.0011831]
	Learning Rate: 0.00118311
	LOSS [training: 0.7055936864731471 | validation: 0.4462444603959029]
	TIME [epoch: 6.49 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7170190728119097		[learning rate: 0.0011803]
	Learning Rate: 0.00118032
	LOSS [training: 0.7170190728119097 | validation: 0.45671391637245096]
	TIME [epoch: 6.47 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.71982236643906		[learning rate: 0.0011775]
	Learning Rate: 0.00117754
	LOSS [training: 0.71982236643906 | validation: 0.46336429262878376]
	TIME [epoch: 6.47 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7125032380435607		[learning rate: 0.0011748]
	Learning Rate: 0.00117476
	LOSS [training: 0.7125032380435607 | validation: 0.4596975682127251]
	TIME [epoch: 6.47 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7090480995412012		[learning rate: 0.001172]
	Learning Rate: 0.00117199
	LOSS [training: 0.7090480995412012 | validation: 0.43358176057834236]
	TIME [epoch: 6.47 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7042336868243846		[learning rate: 0.0011692]
	Learning Rate: 0.00116922
	LOSS [training: 0.7042336868243846 | validation: 0.44342311199159834]
	TIME [epoch: 6.52 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.712980950392031		[learning rate: 0.0011665]
	Learning Rate: 0.00116646
	LOSS [training: 0.712980950392031 | validation: 0.47816190279990334]
	TIME [epoch: 6.48 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.732436844605133		[learning rate: 0.0011637]
	Learning Rate: 0.00116371
	LOSS [training: 0.732436844605133 | validation: 0.44342802341813503]
	TIME [epoch: 6.46 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7100319236285564		[learning rate: 0.001161]
	Learning Rate: 0.00116097
	LOSS [training: 0.7100319236285564 | validation: 0.46118080471873996]
	TIME [epoch: 6.5 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7167905941408746		[learning rate: 0.0011582]
	Learning Rate: 0.00115823
	LOSS [training: 0.7167905941408746 | validation: 0.4481560561191005]
	TIME [epoch: 6.48 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7011373201608169		[learning rate: 0.0011555]
	Learning Rate: 0.0011555
	LOSS [training: 0.7011373201608169 | validation: 0.4524736467723883]
	TIME [epoch: 6.52 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.729334781100041		[learning rate: 0.0011528]
	Learning Rate: 0.00115277
	LOSS [training: 0.729334781100041 | validation: 0.48022139227895455]
	TIME [epoch: 6.53 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.726231279822785		[learning rate: 0.0011501]
	Learning Rate: 0.00115005
	LOSS [training: 0.726231279822785 | validation: 0.4621490538576362]
	TIME [epoch: 6.49 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7208128504386588		[learning rate: 0.0011473]
	Learning Rate: 0.00114734
	LOSS [training: 0.7208128504386588 | validation: 0.4586732574539036]
	TIME [epoch: 6.5 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7204998610008341		[learning rate: 0.0011446]
	Learning Rate: 0.00114463
	LOSS [training: 0.7204998610008341 | validation: 0.4560284516715866]
	TIME [epoch: 6.49 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7269160446041621		[learning rate: 0.0011419]
	Learning Rate: 0.00114193
	LOSS [training: 0.7269160446041621 | validation: 0.46496796024153275]
	TIME [epoch: 6.49 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7122644395814673		[learning rate: 0.0011392]
	Learning Rate: 0.00113924
	LOSS [training: 0.7122644395814673 | validation: 0.45674893644414666]
	TIME [epoch: 6.54 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.711678838205659		[learning rate: 0.0011366]
	Learning Rate: 0.00113655
	LOSS [training: 0.711678838205659 | validation: 0.4623306944572942]
	TIME [epoch: 6.5 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7145785431694188		[learning rate: 0.0011339]
	Learning Rate: 0.00113387
	LOSS [training: 0.7145785431694188 | validation: 0.4589811095750127]
	TIME [epoch: 6.5 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7178794662799641		[learning rate: 0.0011312]
	Learning Rate: 0.0011312
	LOSS [training: 0.7178794662799641 | validation: 0.452831712986969]
	TIME [epoch: 6.47 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7111008136421326		[learning rate: 0.0011285]
	Learning Rate: 0.00112853
	LOSS [training: 0.7111008136421326 | validation: 0.4411990553384313]
	TIME [epoch: 6.49 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.706681357709826		[learning rate: 0.0011259]
	Learning Rate: 0.00112587
	LOSS [training: 0.706681357709826 | validation: 0.4331161630525466]
	TIME [epoch: 6.5 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.710215514676143		[learning rate: 0.0011232]
	Learning Rate: 0.00112321
	LOSS [training: 0.710215514676143 | validation: 0.45315265317257597]
	TIME [epoch: 6.53 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7119017117890706		[learning rate: 0.0011206]
	Learning Rate: 0.00112056
	LOSS [training: 0.7119017117890706 | validation: 0.4570449052789678]
	TIME [epoch: 6.5 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.709766007476049		[learning rate: 0.0011179]
	Learning Rate: 0.00111792
	LOSS [training: 0.709766007476049 | validation: 0.4476220317472078]
	TIME [epoch: 6.47 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7102168925191524		[learning rate: 0.0011153]
	Learning Rate: 0.00111528
	LOSS [training: 0.7102168925191524 | validation: 0.4391535098167144]
	TIME [epoch: 6.47 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7025967184475993		[learning rate: 0.0011127]
	Learning Rate: 0.00111265
	LOSS [training: 0.7025967184475993 | validation: 0.44820951796377]
	TIME [epoch: 6.48 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7068646986733123		[learning rate: 0.00111]
	Learning Rate: 0.00111003
	LOSS [training: 0.7068646986733123 | validation: 0.44389346791708884]
	TIME [epoch: 6.49 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7062582549228588		[learning rate: 0.0011074]
	Learning Rate: 0.00110741
	LOSS [training: 0.7062582549228588 | validation: 0.43935365921729086]
	TIME [epoch: 6.52 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7070034992915921		[learning rate: 0.0011048]
	Learning Rate: 0.0011048
	LOSS [training: 0.7070034992915921 | validation: 0.4446363436485628]
	TIME [epoch: 6.48 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7153956466442416		[learning rate: 0.0011022]
	Learning Rate: 0.00110219
	LOSS [training: 0.7153956466442416 | validation: 0.45783159688726927]
	TIME [epoch: 6.47 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7080261683267952		[learning rate: 0.0010996]
	Learning Rate: 0.00109959
	LOSS [training: 0.7080261683267952 | validation: 0.438156139787544]
	TIME [epoch: 6.48 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7004448550642992		[learning rate: 0.001097]
	Learning Rate: 0.001097
	LOSS [training: 0.7004448550642992 | validation: 0.4591292362398938]
	TIME [epoch: 6.47 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7215855388547421		[learning rate: 0.0010944]
	Learning Rate: 0.00109441
	LOSS [training: 0.7215855388547421 | validation: 0.44424806476735235]
	TIME [epoch: 6.5 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7130399326045793		[learning rate: 0.0010918]
	Learning Rate: 0.00109183
	LOSS [training: 0.7130399326045793 | validation: 0.45085019877988797]
	TIME [epoch: 6.5 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7093087825474591		[learning rate: 0.0010893]
	Learning Rate: 0.00108925
	LOSS [training: 0.7093087825474591 | validation: 0.4611391174401206]
	TIME [epoch: 6.47 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7064448996361652		[learning rate: 0.0010867]
	Learning Rate: 0.00108668
	LOSS [training: 0.7064448996361652 | validation: 0.44065006846585786]
	TIME [epoch: 6.48 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7094820792664559		[learning rate: 0.0010841]
	Learning Rate: 0.00108412
	LOSS [training: 0.7094820792664559 | validation: 0.45053229023443947]
	TIME [epoch: 6.47 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7048076155956524		[learning rate: 0.0010816]
	Learning Rate: 0.00108156
	LOSS [training: 0.7048076155956524 | validation: 0.4361274092215217]
	TIME [epoch: 6.48 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7005615252752846		[learning rate: 0.001079]
	Learning Rate: 0.00107901
	LOSS [training: 0.7005615252752846 | validation: 0.4363015617977739]
	TIME [epoch: 6.51 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7042175703491418		[learning rate: 0.0010765]
	Learning Rate: 0.00107647
	LOSS [training: 0.7042175703491418 | validation: 0.4545465210912526]
	TIME [epoch: 6.49 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7096854952198867		[learning rate: 0.0010739]
	Learning Rate: 0.00107393
	LOSS [training: 0.7096854952198867 | validation: 0.44082086278925353]
	TIME [epoch: 6.47 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7035208971477088		[learning rate: 0.0010714]
	Learning Rate: 0.00107139
	LOSS [training: 0.7035208971477088 | validation: 0.43998735682753476]
	TIME [epoch: 6.48 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.712968203773952		[learning rate: 0.0010689]
	Learning Rate: 0.00106887
	LOSS [training: 0.712968203773952 | validation: 0.4425158660581017]
	TIME [epoch: 6.5 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7063424960479155		[learning rate: 0.0010663]
	Learning Rate: 0.00106634
	LOSS [training: 0.7063424960479155 | validation: 0.4531327789652574]
	TIME [epoch: 6.47 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7137059937768656		[learning rate: 0.0010638]
	Learning Rate: 0.00106383
	LOSS [training: 0.7137059937768656 | validation: 0.44065394205832786]
	TIME [epoch: 6.51 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6999392048658698		[learning rate: 0.0010613]
	Learning Rate: 0.00106132
	LOSS [training: 0.6999392048658698 | validation: 0.4422624448995761]
	TIME [epoch: 6.47 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7098396928322833		[learning rate: 0.0010588]
	Learning Rate: 0.00105882
	LOSS [training: 0.7098396928322833 | validation: 0.45163127265123126]
	TIME [epoch: 6.47 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7112965123883914		[learning rate: 0.0010563]
	Learning Rate: 0.00105632
	LOSS [training: 0.7112965123883914 | validation: 0.4334161828373592]
	TIME [epoch: 6.47 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6988426950555024		[learning rate: 0.0010538]
	Learning Rate: 0.00105383
	LOSS [training: 0.6988426950555024 | validation: 0.44220582100348405]
	TIME [epoch: 6.46 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7083463438584049		[learning rate: 0.0010513]
	Learning Rate: 0.00105134
	LOSS [training: 0.7083463438584049 | validation: 0.4442138123344226]
	TIME [epoch: 6.47 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7122143455945422		[learning rate: 0.0010489]
	Learning Rate: 0.00104886
	LOSS [training: 0.7122143455945422 | validation: 0.43847739328771995]
	TIME [epoch: 6.51 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7006006934709774		[learning rate: 0.0010464]
	Learning Rate: 0.00104639
	LOSS [training: 0.7006006934709774 | validation: 0.44551910240901543]
	TIME [epoch: 6.47 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7024079849647158		[learning rate: 0.0010439]
	Learning Rate: 0.00104392
	LOSS [training: 0.7024079849647158 | validation: 0.4362987994881431]
	TIME [epoch: 6.48 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7180175898152581		[learning rate: 0.0010415]
	Learning Rate: 0.00104146
	LOSS [training: 0.7180175898152581 | validation: 0.44146177395801167]
	TIME [epoch: 6.47 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7054435957582552		[learning rate: 0.001039]
	Learning Rate: 0.001039
	LOSS [training: 0.7054435957582552 | validation: 0.45058302708735654]
	TIME [epoch: 6.47 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7159454803679696		[learning rate: 0.0010365]
	Learning Rate: 0.00103655
	LOSS [training: 0.7159454803679696 | validation: 0.47630478449257724]
	TIME [epoch: 6.49 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7388553105892923		[learning rate: 0.0010341]
	Learning Rate: 0.0010341
	LOSS [training: 0.7388553105892923 | validation: 0.4979824986915218]
	TIME [epoch: 6.5 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7633726371139257		[learning rate: 0.0010317]
	Learning Rate: 0.00103166
	LOSS [training: 0.7633726371139257 | validation: 0.48982569307836354]
	TIME [epoch: 6.5 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7525844736274745		[learning rate: 0.0010292]
	Learning Rate: 0.00102923
	LOSS [training: 0.7525844736274745 | validation: 0.471881920525725]
	TIME [epoch: 6.47 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.724596959298525		[learning rate: 0.0010268]
	Learning Rate: 0.0010268
	LOSS [training: 0.724596959298525 | validation: 0.45534537999274105]
	TIME [epoch: 6.47 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7121501888850088		[learning rate: 0.0010244]
	Learning Rate: 0.00102438
	LOSS [training: 0.7121501888850088 | validation: 0.43648435058043966]
	TIME [epoch: 6.47 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7044712736461897		[learning rate: 0.001022]
	Learning Rate: 0.00102196
	LOSS [training: 0.7044712736461897 | validation: 0.4533606269629452]
	TIME [epoch: 6.51 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7010916353645417		[learning rate: 0.0010196]
	Learning Rate: 0.00101955
	LOSS [training: 0.7010916353645417 | validation: 0.4443743007628776]
	TIME [epoch: 6.49 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7034337303423225		[learning rate: 0.0010171]
	Learning Rate: 0.00101715
	LOSS [training: 0.7034337303423225 | validation: 0.44402037585400006]
	TIME [epoch: 6.47 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7020911290660763		[learning rate: 0.0010147]
	Learning Rate: 0.00101475
	LOSS [training: 0.7020911290660763 | validation: 0.4294631334818819]
	TIME [epoch: 6.47 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7043315626553939		[learning rate: 0.0010124]
	Learning Rate: 0.00101236
	LOSS [training: 0.7043315626553939 | validation: 0.4383349050286502]
	TIME [epoch: 6.48 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.703641985786861		[learning rate: 0.00101]
	Learning Rate: 0.00100997
	LOSS [training: 0.703641985786861 | validation: 0.4381867925548007]
	TIME [epoch: 6.48 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7015256626214544		[learning rate: 0.0010076]
	Learning Rate: 0.00100759
	LOSS [training: 0.7015256626214544 | validation: 0.43347135944066156]
	TIME [epoch: 6.51 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6919211844735973		[learning rate: 0.0010052]
	Learning Rate: 0.00100521
	LOSS [training: 0.6919211844735973 | validation: 0.4421409103700816]
	TIME [epoch: 6.48 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7022700885192464		[learning rate: 0.0010028]
	Learning Rate: 0.00100284
	LOSS [training: 0.7022700885192464 | validation: 0.44770470071902224]
	TIME [epoch: 6.49 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7035503813611993		[learning rate: 0.0010005]
	Learning Rate: 0.00100047
	LOSS [training: 0.7035503813611993 | validation: 0.45345971280453407]
	TIME [epoch: 6.48 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7021650541872074		[learning rate: 0.00099811]
	Learning Rate: 0.000998112
	LOSS [training: 0.7021650541872074 | validation: 0.4442182840019513]
	TIME [epoch: 6.48 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7024518715154654		[learning rate: 0.00099576]
	Learning Rate: 0.000995758
	LOSS [training: 0.7024518715154654 | validation: 0.43569565242836783]
	TIME [epoch: 6.49 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7092490013006116		[learning rate: 0.00099341]
	Learning Rate: 0.000993409
	LOSS [training: 0.7092490013006116 | validation: 0.4749098062829722]
	TIME [epoch: 6.52 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7261067717788222		[learning rate: 0.00099107]
	Learning Rate: 0.000991066
	LOSS [training: 0.7261067717788222 | validation: 0.4792193140405449]
	TIME [epoch: 6.49 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7459986142604247		[learning rate: 0.00098873]
	Learning Rate: 0.000988728
	LOSS [training: 0.7459986142604247 | validation: 0.4779521595592164]
	TIME [epoch: 6.48 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7339764581514666		[learning rate: 0.0009864]
	Learning Rate: 0.000986396
	LOSS [training: 0.7339764581514666 | validation: 0.4475202911894186]
	TIME [epoch: 6.47 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7118179416515372		[learning rate: 0.00098407]
	Learning Rate: 0.000984069
	LOSS [training: 0.7118179416515372 | validation: 0.4466751422190732]
	TIME [epoch: 6.49 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7063285261003143		[learning rate: 0.00098175]
	Learning Rate: 0.000981748
	LOSS [training: 0.7063285261003143 | validation: 0.4386856280198313]
	TIME [epoch: 6.51 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7078425317151967		[learning rate: 0.00097943]
	Learning Rate: 0.000979432
	LOSS [training: 0.7078425317151967 | validation: 0.4424547839364923]
	TIME [epoch: 6.52 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7071989817240041		[learning rate: 0.00097712]
	Learning Rate: 0.000977122
	LOSS [training: 0.7071989817240041 | validation: 0.44322161703149576]
	TIME [epoch: 6.49 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7077414360833976		[learning rate: 0.00097482]
	Learning Rate: 0.000974817
	LOSS [training: 0.7077414360833976 | validation: 0.428556736386418]
	TIME [epoch: 6.48 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7045221640933681		[learning rate: 0.00097252]
	Learning Rate: 0.000972517
	LOSS [training: 0.7045221640933681 | validation: 0.43830497926876183]
	TIME [epoch: 6.49 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7000881051568473		[learning rate: 0.00097022]
	Learning Rate: 0.000970224
	LOSS [training: 0.7000881051568473 | validation: 0.43926895919634434]
	TIME [epoch: 6.49 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7026230674549618		[learning rate: 0.00096793]
	Learning Rate: 0.000967935
	LOSS [training: 0.7026230674549618 | validation: 0.43818859517999453]
	TIME [epoch: 6.5 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7007247490411848		[learning rate: 0.00096565]
	Learning Rate: 0.000965652
	LOSS [training: 0.7007247490411848 | validation: 0.4330989637281266]
	TIME [epoch: 6.49 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7031409940874365		[learning rate: 0.00096337]
	Learning Rate: 0.000963374
	LOSS [training: 0.7031409940874365 | validation: 0.4430600716752482]
	TIME [epoch: 6.47 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7119055786075036		[learning rate: 0.0009611]
	Learning Rate: 0.000961101
	LOSS [training: 0.7119055786075036 | validation: 0.4513415517209829]
	TIME [epoch: 6.46 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7054961354805032		[learning rate: 0.00095883]
	Learning Rate: 0.000958834
	LOSS [training: 0.7054961354805032 | validation: 0.4337143230250985]
	TIME [epoch: 6.47 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7020024095241751		[learning rate: 0.00095657]
	Learning Rate: 0.000956572
	LOSS [training: 0.7020024095241751 | validation: 0.449614729793762]
	TIME [epoch: 6.48 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.700487958341496		[learning rate: 0.00095432]
	Learning Rate: 0.000954316
	LOSS [training: 0.700487958341496 | validation: 0.44155650898123977]
	TIME [epoch: 6.52 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7034261354408747		[learning rate: 0.00095207]
	Learning Rate: 0.000952065
	LOSS [training: 0.7034261354408747 | validation: 0.4295536191537269]
	TIME [epoch: 6.5 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7012358856469573		[learning rate: 0.00094982]
	Learning Rate: 0.000949819
	LOSS [training: 0.7012358856469573 | validation: 0.4385460846671595]
	TIME [epoch: 6.49 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7078303935708845		[learning rate: 0.00094758]
	Learning Rate: 0.000947579
	LOSS [training: 0.7078303935708845 | validation: 0.4378295123742024]
	TIME [epoch: 6.49 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7011825070384462		[learning rate: 0.00094534]
	Learning Rate: 0.000945344
	LOSS [training: 0.7011825070384462 | validation: 0.44200102486429727]
	TIME [epoch: 6.49 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7015224246241996		[learning rate: 0.00094311]
	Learning Rate: 0.000943114
	LOSS [training: 0.7015224246241996 | validation: 0.44845267976106]
	TIME [epoch: 6.49 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7138577545600822		[learning rate: 0.00094089]
	Learning Rate: 0.000940889
	LOSS [training: 0.7138577545600822 | validation: 0.4511095188316193]
	TIME [epoch: 6.53 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7186269977453296		[learning rate: 0.00093867]
	Learning Rate: 0.00093867
	LOSS [training: 0.7186269977453296 | validation: 0.4614079266924756]
	TIME [epoch: 6.5 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7070963244946371		[learning rate: 0.00093646]
	Learning Rate: 0.000936456
	LOSS [training: 0.7070963244946371 | validation: 0.4421614536726939]
	TIME [epoch: 6.47 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.698930531112288		[learning rate: 0.00093425]
	Learning Rate: 0.000934246
	LOSS [training: 0.698930531112288 | validation: 0.43884724207291825]
	TIME [epoch: 6.49 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7062484491923016		[learning rate: 0.00093204]
	Learning Rate: 0.000932043
	LOSS [training: 0.7062484491923016 | validation: 0.45198769186536475]
	TIME [epoch: 6.5 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7103555550173514		[learning rate: 0.00092984]
	Learning Rate: 0.000929844
	LOSS [training: 0.7103555550173514 | validation: 0.4506435802069756]
	TIME [epoch: 6.52 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7118774112928243		[learning rate: 0.00092765]
	Learning Rate: 0.000927651
	LOSS [training: 0.7118774112928243 | validation: 0.4460755494234639]
	TIME [epoch: 6.51 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7111007278580861		[learning rate: 0.00092546]
	Learning Rate: 0.000925463
	LOSS [training: 0.7111007278580861 | validation: 0.4495231675611018]
	TIME [epoch: 6.47 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7048542926219591		[learning rate: 0.00092328]
	Learning Rate: 0.00092328
	LOSS [training: 0.7048542926219591 | validation: 0.4382124747942152]
	TIME [epoch: 6.5 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.706258957457742		[learning rate: 0.0009211]
	Learning Rate: 0.000921102
	LOSS [training: 0.706258957457742 | validation: 0.4566495907355824]
	TIME [epoch: 6.49 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7100817306417018		[learning rate: 0.00091893]
	Learning Rate: 0.000918929
	LOSS [training: 0.7100817306417018 | validation: 0.43808676338238955]
	TIME [epoch: 6.51 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7069116382457976		[learning rate: 0.00091676]
	Learning Rate: 0.000916762
	LOSS [training: 0.7069116382457976 | validation: 0.4407016054422443]
	TIME [epoch: 6.54 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7088952549885137		[learning rate: 0.0009146]
	Learning Rate: 0.000914599
	LOSS [training: 0.7088952549885137 | validation: 0.4496338189522966]
	TIME [epoch: 6.49 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7066113329850774		[learning rate: 0.00091244]
	Learning Rate: 0.000912442
	LOSS [training: 0.7066113329850774 | validation: 0.4349554147867999]
	TIME [epoch: 6.47 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7063382389998176		[learning rate: 0.00091029]
	Learning Rate: 0.000910289
	LOSS [training: 0.7063382389998176 | validation: 0.44027004893889904]
	TIME [epoch: 6.47 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7032761163269932		[learning rate: 0.00090814]
	Learning Rate: 0.000908142
	LOSS [training: 0.7032761163269932 | validation: 0.44457167328639496]
	TIME [epoch: 6.49 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6994751713096611		[learning rate: 0.000906]
	Learning Rate: 0.000906
	LOSS [training: 0.6994751713096611 | validation: 0.4359352878692774]
	TIME [epoch: 6.49 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7080102291379868		[learning rate: 0.00090386]
	Learning Rate: 0.000903863
	LOSS [training: 0.7080102291379868 | validation: 0.4434372945619768]
	TIME [epoch: 6.53 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7057358815224949		[learning rate: 0.00090173]
	Learning Rate: 0.000901731
	LOSS [training: 0.7057358815224949 | validation: 0.4515679732470169]
	TIME [epoch: 6.5 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7102590179738831		[learning rate: 0.0008996]
	Learning Rate: 0.000899604
	LOSS [training: 0.7102590179738831 | validation: 0.4413403145128635]
	TIME [epoch: 6.49 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7020609235658657		[learning rate: 0.00089748]
	Learning Rate: 0.000897482
	LOSS [training: 0.7020609235658657 | validation: 0.45713871183069926]
	TIME [epoch: 6.49 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7131930059142177		[learning rate: 0.00089536]
	Learning Rate: 0.000895365
	LOSS [training: 0.7131930059142177 | validation: 0.43724842819435733]
	TIME [epoch: 6.48 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7012326701343721		[learning rate: 0.00089325]
	Learning Rate: 0.000893253
	LOSS [training: 0.7012326701343721 | validation: 0.43203966129786864]
	TIME [epoch: 6.51 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7004360463116506		[learning rate: 0.00089115]
	Learning Rate: 0.000891146
	LOSS [training: 0.7004360463116506 | validation: 0.4386144117350258]
	TIME [epoch: 6.51 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6992749141658366		[learning rate: 0.00088904]
	Learning Rate: 0.000889044
	LOSS [training: 0.6992749141658366 | validation: 0.4297140443192169]
	TIME [epoch: 6.5 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.699106231571055		[learning rate: 0.00088695]
	Learning Rate: 0.000886946
	LOSS [training: 0.699106231571055 | validation: 0.44929029754660416]
	TIME [epoch: 6.49 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7194577109385114		[learning rate: 0.00088485]
	Learning Rate: 0.000884854
	LOSS [training: 0.7194577109385114 | validation: 0.46156452816819316]
	TIME [epoch: 6.49 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7209439765679894		[learning rate: 0.00088277]
	Learning Rate: 0.000882767
	LOSS [training: 0.7209439765679894 | validation: 0.43958127016740856]
	TIME [epoch: 6.49 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7032057051719067		[learning rate: 0.00088068]
	Learning Rate: 0.000880685
	LOSS [training: 0.7032057051719067 | validation: 0.44376100382917905]
	TIME [epoch: 6.52 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7059187131341063		[learning rate: 0.00087861]
	Learning Rate: 0.000878607
	LOSS [training: 0.7059187131341063 | validation: 0.4459723108225503]
	TIME [epoch: 6.52 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7079171813034719		[learning rate: 0.00087653]
	Learning Rate: 0.000876535
	LOSS [training: 0.7079171813034719 | validation: 0.43535267042767306]
	TIME [epoch: 6.49 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7069418636683193		[learning rate: 0.00087447]
	Learning Rate: 0.000874467
	LOSS [training: 0.7069418636683193 | validation: 0.4419460115599224]
	TIME [epoch: 6.5 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.71159355670163		[learning rate: 0.0008724]
	Learning Rate: 0.000872405
	LOSS [training: 0.71159355670163 | validation: 0.46009051313319255]
	TIME [epoch: 6.49 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7152751679268187		[learning rate: 0.00087035]
	Learning Rate: 0.000870346
	LOSS [training: 0.7152751679268187 | validation: 0.4510804069317128]
	TIME [epoch: 6.49 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7199649111350677		[learning rate: 0.00086829]
	Learning Rate: 0.000868294
	LOSS [training: 0.7199649111350677 | validation: 0.46569387118645156]
	TIME [epoch: 6.51 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7081653643321562		[learning rate: 0.00086625]
	Learning Rate: 0.000866246
	LOSS [training: 0.7081653643321562 | validation: 0.4441104685119761]
	TIME [epoch: 6.48 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7040390631443517		[learning rate: 0.0008642]
	Learning Rate: 0.000864202
	LOSS [training: 0.7040390631443517 | validation: 0.4460008678393361]
	TIME [epoch: 6.47 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7062628896638049		[learning rate: 0.00086216]
	Learning Rate: 0.000862164
	LOSS [training: 0.7062628896638049 | validation: 0.4460404079320562]
	TIME [epoch: 6.46 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7039212127209492		[learning rate: 0.00086013]
	Learning Rate: 0.00086013
	LOSS [training: 0.7039212127209492 | validation: 0.4336422126890388]
	TIME [epoch: 6.47 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6941299371601778		[learning rate: 0.0008581]
	Learning Rate: 0.000858101
	LOSS [training: 0.6941299371601778 | validation: 0.43879282053731083]
	TIME [epoch: 6.47 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7008288723144098		[learning rate: 0.00085608]
	Learning Rate: 0.000856077
	LOSS [training: 0.7008288723144098 | validation: 0.45920543851111784]
	TIME [epoch: 6.52 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7146449552642492		[learning rate: 0.00085406]
	Learning Rate: 0.000854058
	LOSS [training: 0.7146449552642492 | validation: 0.44024162908146136]
	TIME [epoch: 6.49 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7000629112143878		[learning rate: 0.00085204]
	Learning Rate: 0.000852043
	LOSS [training: 0.7000629112143878 | validation: 0.4389439362140175]
	TIME [epoch: 6.48 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7073534511371531		[learning rate: 0.00085003]
	Learning Rate: 0.000850033
	LOSS [training: 0.7073534511371531 | validation: 0.4336797495585801]
	TIME [epoch: 6.48 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6998338874677263		[learning rate: 0.00084803]
	Learning Rate: 0.000848028
	LOSS [training: 0.6998338874677263 | validation: 0.43402050007907733]
	TIME [epoch: 6.47 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.699565494069208		[learning rate: 0.00084603]
	Learning Rate: 0.000846028
	LOSS [training: 0.699565494069208 | validation: 0.4344075363294597]
	TIME [epoch: 6.48 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7028206168246627		[learning rate: 0.00084403]
	Learning Rate: 0.000844032
	LOSS [training: 0.7028206168246627 | validation: 0.4315213491445118]
	TIME [epoch: 6.54 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7037084729463425		[learning rate: 0.00084204]
	Learning Rate: 0.000842041
	LOSS [training: 0.7037084729463425 | validation: 0.43323360350996676]
	TIME [epoch: 6.47 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6962270917936801		[learning rate: 0.00084005]
	Learning Rate: 0.000840055
	LOSS [training: 0.6962270917936801 | validation: 0.436459040109871]
	TIME [epoch: 6.47 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7006555779008746		[learning rate: 0.00083807]
	Learning Rate: 0.000838073
	LOSS [training: 0.7006555779008746 | validation: 0.4429036622511178]
	TIME [epoch: 6.47 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.703523591240884		[learning rate: 0.0008361]
	Learning Rate: 0.000836096
	LOSS [training: 0.703523591240884 | validation: 0.44706309144152434]
	TIME [epoch: 6.46 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7169279031305494		[learning rate: 0.00083412]
	Learning Rate: 0.000834124
	LOSS [training: 0.7169279031305494 | validation: 0.47108287233091345]
	TIME [epoch: 6.48 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7157224105192226		[learning rate: 0.00083216]
	Learning Rate: 0.000832157
	LOSS [training: 0.7157224105192226 | validation: 0.45766211830465775]
	TIME [epoch: 6.5 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7225656590414395		[learning rate: 0.00083019]
	Learning Rate: 0.000830194
	LOSS [training: 0.7225656590414395 | validation: 0.47130633452167314]
	TIME [epoch: 6.47 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7150045671459078		[learning rate: 0.00082824]
	Learning Rate: 0.000828236
	LOSS [training: 0.7150045671459078 | validation: 0.4534974630608697]
	TIME [epoch: 6.48 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7045275298200208		[learning rate: 0.00082628]
	Learning Rate: 0.000826282
	LOSS [training: 0.7045275298200208 | validation: 0.440422335713287]
	TIME [epoch: 6.47 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7061337420868826		[learning rate: 0.00082433]
	Learning Rate: 0.000824333
	LOSS [training: 0.7061337420868826 | validation: 0.44122415045037755]
	TIME [epoch: 6.47 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7003269906002985		[learning rate: 0.00082239]
	Learning Rate: 0.000822388
	LOSS [training: 0.7003269906002985 | validation: 0.43531205731518097]
	TIME [epoch: 6.49 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.697330476195048		[learning rate: 0.00082045]
	Learning Rate: 0.000820448
	LOSS [training: 0.697330476195048 | validation: 0.4351562091150394]
	TIME [epoch: 6.5 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6978349821958301		[learning rate: 0.00081851]
	Learning Rate: 0.000818513
	LOSS [training: 0.6978349821958301 | validation: 0.43733076691453476]
	TIME [epoch: 6.49 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6949215042561357		[learning rate: 0.00081658]
	Learning Rate: 0.000816582
	LOSS [training: 0.6949215042561357 | validation: 0.43707730713887677]
	TIME [epoch: 6.49 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.699131753827933		[learning rate: 0.00081466]
	Learning Rate: 0.000814656
	LOSS [training: 0.699131753827933 | validation: 0.43202147132636204]
	TIME [epoch: 6.5 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6987528887731961		[learning rate: 0.00081273]
	Learning Rate: 0.000812734
	LOSS [training: 0.6987528887731961 | validation: 0.43944923076310805]
	TIME [epoch: 6.49 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7040335131348828		[learning rate: 0.00081082]
	Learning Rate: 0.000810817
	LOSS [training: 0.7040335131348828 | validation: 0.44703405739825186]
	TIME [epoch: 6.55 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6968990818051863		[learning rate: 0.0008089]
	Learning Rate: 0.000808905
	LOSS [training: 0.6968990818051863 | validation: 0.4322150577714595]
	TIME [epoch: 6.51 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7016555484706709		[learning rate: 0.000807]
	Learning Rate: 0.000806997
	LOSS [training: 0.7016555484706709 | validation: 0.4572127864202859]
	TIME [epoch: 6.48 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7016687061375791		[learning rate: 0.00080509]
	Learning Rate: 0.000805093
	LOSS [training: 0.7016687061375791 | validation: 0.4361767343678958]
	TIME [epoch: 6.49 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7035587800095418		[learning rate: 0.00080319]
	Learning Rate: 0.000803194
	LOSS [training: 0.7035587800095418 | validation: 0.4412514966897807]
	TIME [epoch: 6.49 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6975885190916497		[learning rate: 0.0008013]
	Learning Rate: 0.000801299
	LOSS [training: 0.6975885190916497 | validation: 0.43954628293118114]
	TIME [epoch: 6.49 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6970902145339982		[learning rate: 0.00079941]
	Learning Rate: 0.000799409
	LOSS [training: 0.6970902145339982 | validation: 0.43281205750518964]
	TIME [epoch: 6.53 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6990067609794469		[learning rate: 0.00079752]
	Learning Rate: 0.000797524
	LOSS [training: 0.6990067609794469 | validation: 0.4413420844583006]
	TIME [epoch: 6.5 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7003698316667089		[learning rate: 0.00079564]
	Learning Rate: 0.000795642
	LOSS [training: 0.7003698316667089 | validation: 0.4360687239378795]
	TIME [epoch: 6.49 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7003578089662147		[learning rate: 0.00079377]
	Learning Rate: 0.000793766
	LOSS [training: 0.7003578089662147 | validation: 0.4496397820162239]
	TIME [epoch: 6.49 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7093458150279162		[learning rate: 0.00079189]
	Learning Rate: 0.000791893
	LOSS [training: 0.7093458150279162 | validation: 0.44709120034611954]
	TIME [epoch: 6.47 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7066516148205391		[learning rate: 0.00079003]
	Learning Rate: 0.000790025
	LOSS [training: 0.7066516148205391 | validation: 0.446692024725735]
	TIME [epoch: 6.5 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7100251731091163		[learning rate: 0.00078816]
	Learning Rate: 0.000788162
	LOSS [training: 0.7100251731091163 | validation: 0.45009067577655304]
	TIME [epoch: 6.5 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7093997188131561		[learning rate: 0.0007863]
	Learning Rate: 0.000786302
	LOSS [training: 0.7093997188131561 | validation: 0.4258399993751454]
	TIME [epoch: 6.49 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6971959766949911		[learning rate: 0.00078445]
	Learning Rate: 0.000784448
	LOSS [training: 0.6971959766949911 | validation: 0.44140375609562127]
	TIME [epoch: 6.48 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.702732985874737		[learning rate: 0.0007826]
	Learning Rate: 0.000782598
	LOSS [training: 0.702732985874737 | validation: 0.4429144876197957]
	TIME [epoch: 6.49 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7036377792620897		[learning rate: 0.00078075]
	Learning Rate: 0.000780751
	LOSS [training: 0.7036377792620897 | validation: 0.4341070103279333]
	TIME [epoch: 6.49 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6942662832749968		[learning rate: 0.00077891]
	Learning Rate: 0.00077891
	LOSS [training: 0.6942662832749968 | validation: 0.43942682186240756]
	TIME [epoch: 6.52 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7016484920732801		[learning rate: 0.00077707]
	Learning Rate: 0.000777073
	LOSS [training: 0.7016484920732801 | validation: 0.4303561917459975]
	TIME [epoch: 6.51 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7000112128481054		[learning rate: 0.00077524]
	Learning Rate: 0.000775239
	LOSS [training: 0.7000112128481054 | validation: 0.4388227084621833]
	TIME [epoch: 6.49 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7072773837467324		[learning rate: 0.00077341]
	Learning Rate: 0.000773411
	LOSS [training: 0.7072773837467324 | validation: 0.4488215852916613]
	TIME [epoch: 6.49 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7041838376455611		[learning rate: 0.00077159]
	Learning Rate: 0.000771586
	LOSS [training: 0.7041838376455611 | validation: 0.4568040576610245]
	TIME [epoch: 6.49 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7155369192323419		[learning rate: 0.00076977]
	Learning Rate: 0.000769766
	LOSS [training: 0.7155369192323419 | validation: 0.4428380302331091]
	TIME [epoch: 6.51 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7006044156579325		[learning rate: 0.00076795]
	Learning Rate: 0.000767951
	LOSS [training: 0.7006044156579325 | validation: 0.44968880413452017]
	TIME [epoch: 6.53 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7135791361533033		[learning rate: 0.00076614]
	Learning Rate: 0.000766139
	LOSS [training: 0.7135791361533033 | validation: 0.43714096607981995]
	TIME [epoch: 6.5 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6989737637392961		[learning rate: 0.00076433]
	Learning Rate: 0.000764332
	LOSS [training: 0.6989737637392961 | validation: 0.43674023039041515]
	TIME [epoch: 6.5 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6978174075790861		[learning rate: 0.00076253]
	Learning Rate: 0.000762529
	LOSS [training: 0.6978174075790861 | validation: 0.4424403657843377]
	TIME [epoch: 6.51 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6989084844260159		[learning rate: 0.00076073]
	Learning Rate: 0.00076073
	LOSS [training: 0.6989084844260159 | validation: 0.4399509286008043]
	TIME [epoch: 6.52 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6960300873336547		[learning rate: 0.00075894]
	Learning Rate: 0.000758936
	LOSS [training: 0.6960300873336547 | validation: 0.44261848662715897]
	TIME [epoch: 6.52 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.697527520844961		[learning rate: 0.00075715]
	Learning Rate: 0.000757146
	LOSS [training: 0.697527520844961 | validation: 0.43589428911242345]
	TIME [epoch: 6.56 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6978689744306887		[learning rate: 0.00075536]
	Learning Rate: 0.00075536
	LOSS [training: 0.6978689744306887 | validation: 0.4327520090047083]
	TIME [epoch: 6.51 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6974916521466844		[learning rate: 0.00075358]
	Learning Rate: 0.000753578
	LOSS [training: 0.6974916521466844 | validation: 0.4305660771843687]
	TIME [epoch: 6.52 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6960110599123379		[learning rate: 0.0007518]
	Learning Rate: 0.0007518
	LOSS [training: 0.6960110599123379 | validation: 0.44545014207567857]
	TIME [epoch: 6.51 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.699259724183986		[learning rate: 0.00075003]
	Learning Rate: 0.000750027
	LOSS [training: 0.699259724183986 | validation: 0.4376945760060919]
	TIME [epoch: 6.5 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6990735216835002		[learning rate: 0.00074826]
	Learning Rate: 0.000748258
	LOSS [training: 0.6990735216835002 | validation: 0.4304204988406782]
	TIME [epoch: 6.53 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6931655983854794		[learning rate: 0.00074649]
	Learning Rate: 0.000746493
	LOSS [training: 0.6931655983854794 | validation: 0.4309008058611705]
	TIME [epoch: 6.53 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6973496342250859		[learning rate: 0.00074473]
	Learning Rate: 0.000744732
	LOSS [training: 0.6973496342250859 | validation: 0.43511274588906546]
	TIME [epoch: 6.49 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7027352048997466		[learning rate: 0.00074298]
	Learning Rate: 0.000742975
	LOSS [training: 0.7027352048997466 | validation: 0.44223892878398363]
	TIME [epoch: 6.49 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7011833240534263		[learning rate: 0.00074122]
	Learning Rate: 0.000741223
	LOSS [training: 0.7011833240534263 | validation: 0.4369632998152514]
	TIME [epoch: 6.51 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6961741601794836		[learning rate: 0.00073947]
	Learning Rate: 0.000739474
	LOSS [training: 0.6961741601794836 | validation: 0.42880262317341244]
	TIME [epoch: 6.49 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7046287239170737		[learning rate: 0.00073773]
	Learning Rate: 0.00073773
	LOSS [training: 0.7046287239170737 | validation: 0.4677173899512719]
	TIME [epoch: 6.52 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7122694363936993		[learning rate: 0.00073599]
	Learning Rate: 0.00073599
	LOSS [training: 0.7122694363936993 | validation: 0.441324129762659]
	TIME [epoch: 6.5 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6982913118529721		[learning rate: 0.00073425]
	Learning Rate: 0.000734254
	LOSS [training: 0.6982913118529721 | validation: 0.43908284156115385]
	TIME [epoch: 6.49 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.690388556241674		[learning rate: 0.00073252]
	Learning Rate: 0.000732522
	LOSS [training: 0.690388556241674 | validation: 0.4392932084171164]
	TIME [epoch: 6.49 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7151285016111586		[learning rate: 0.00073079]
	Learning Rate: 0.000730794
	LOSS [training: 0.7151285016111586 | validation: 0.4577825441093012]
	TIME [epoch: 6.49 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.703820899712339		[learning rate: 0.00072907]
	Learning Rate: 0.00072907
	LOSS [training: 0.703820899712339 | validation: 0.4377138530682415]
	TIME [epoch: 6.49 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7041543642971702		[learning rate: 0.00072735]
	Learning Rate: 0.00072735
	LOSS [training: 0.7041543642971702 | validation: 0.4361038491007431]
	TIME [epoch: 6.54 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7079730702302991		[learning rate: 0.00072563]
	Learning Rate: 0.000725634
	LOSS [training: 0.7079730702302991 | validation: 0.4441288296922344]
	TIME [epoch: 6.5 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6974833278788669		[learning rate: 0.00072392]
	Learning Rate: 0.000723923
	LOSS [training: 0.6974833278788669 | validation: 0.43308066347514956]
	TIME [epoch: 6.49 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6995003077945185		[learning rate: 0.00072222]
	Learning Rate: 0.000722215
	LOSS [training: 0.6995003077945185 | validation: 0.4392919547123107]
	TIME [epoch: 6.49 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6979133217251924		[learning rate: 0.00072051]
	Learning Rate: 0.000720512
	LOSS [training: 0.6979133217251924 | validation: 0.43804689231716465]
	TIME [epoch: 6.49 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7039739267438785		[learning rate: 0.00071881]
	Learning Rate: 0.000718812
	LOSS [training: 0.7039739267438785 | validation: 0.4347804976863803]
	TIME [epoch: 6.49 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6985249725599854		[learning rate: 0.00071712]
	Learning Rate: 0.000717117
	LOSS [training: 0.6985249725599854 | validation: 0.43521112130992645]
	TIME [epoch: 6.53 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7080859991312519		[learning rate: 0.00071542]
	Learning Rate: 0.000715425
	LOSS [training: 0.7080859991312519 | validation: 0.44754635773411044]
	TIME [epoch: 6.49 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7022557302830267		[learning rate: 0.00071374]
	Learning Rate: 0.000713738
	LOSS [training: 0.7022557302830267 | validation: 0.44129368940937475]
	TIME [epoch: 6.49 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.720945012207475		[learning rate: 0.00071205]
	Learning Rate: 0.000712054
	LOSS [training: 0.720945012207475 | validation: 0.4767294790461414]
	TIME [epoch: 6.49 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7154953095578785		[learning rate: 0.00071037]
	Learning Rate: 0.000710374
	LOSS [training: 0.7154953095578785 | validation: 0.4451828222433]
	TIME [epoch: 6.48 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7057902269693784		[learning rate: 0.0007087]
	Learning Rate: 0.000708698
	LOSS [training: 0.7057902269693784 | validation: 0.44590338580167493]
	TIME [epoch: 6.51 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7027842787234558		[learning rate: 0.00070703]
	Learning Rate: 0.000707027
	LOSS [training: 0.7027842787234558 | validation: 0.44679059504892393]
	TIME [epoch: 6.52 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7119696951647987		[learning rate: 0.00070536]
	Learning Rate: 0.000705359
	LOSS [training: 0.7119696951647987 | validation: 0.44309104876607425]
	TIME [epoch: 6.49 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7083171414182912		[learning rate: 0.0007037]
	Learning Rate: 0.000703695
	LOSS [training: 0.7083171414182912 | validation: 0.4408875784370119]
	TIME [epoch: 6.49 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7044500841486887		[learning rate: 0.00070204]
	Learning Rate: 0.000702035
	LOSS [training: 0.7044500841486887 | validation: 0.4310874181200398]
	TIME [epoch: 6.49 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6936097538513287		[learning rate: 0.00070038]
	Learning Rate: 0.000700379
	LOSS [training: 0.6936097538513287 | validation: 0.4429327304406145]
	TIME [epoch: 6.49 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7003259090568988		[learning rate: 0.00069873]
	Learning Rate: 0.000698727
	LOSS [training: 0.7003259090568988 | validation: 0.4304313035889592]
	TIME [epoch: 6.53 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7023431188966021		[learning rate: 0.00069708]
	Learning Rate: 0.000697079
	LOSS [training: 0.7023431188966021 | validation: 0.45668336848087776]
	TIME [epoch: 6.5 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7109406039182178		[learning rate: 0.00069543]
	Learning Rate: 0.000695435
	LOSS [training: 0.7109406039182178 | validation: 0.4491671468771566]
	TIME [epoch: 6.49 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7102582273106728		[learning rate: 0.00069379]
	Learning Rate: 0.000693794
	LOSS [training: 0.7102582273106728 | validation: 0.45497698627180905]
	TIME [epoch: 6.49 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7259987746849401		[learning rate: 0.00069216]
	Learning Rate: 0.000692158
	LOSS [training: 0.7259987746849401 | validation: 0.48587530979124544]
	TIME [epoch: 6.51 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7241347553400697		[learning rate: 0.00069053]
	Learning Rate: 0.000690525
	LOSS [training: 0.7241347553400697 | validation: 0.46121855755754104]
	TIME [epoch: 6.49 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7125044605468533		[learning rate: 0.0006889]
	Learning Rate: 0.000688896
	LOSS [training: 0.7125044605468533 | validation: 0.45798776090881455]
	TIME [epoch: 6.53 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7159937344721506		[learning rate: 0.00068727]
	Learning Rate: 0.000687271
	LOSS [training: 0.7159937344721506 | validation: 0.45391475829874267]
	TIME [epoch: 6.5 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7071863267383505		[learning rate: 0.00068565]
	Learning Rate: 0.00068565
	LOSS [training: 0.7071863267383505 | validation: 0.44379297099575676]
	TIME [epoch: 6.49 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7130836788092083		[learning rate: 0.00068403]
	Learning Rate: 0.000684033
	LOSS [training: 0.7130836788092083 | validation: 0.4662165095862257]
	TIME [epoch: 6.49 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7163468172088874		[learning rate: 0.00068242]
	Learning Rate: 0.000682419
	LOSS [training: 0.7163468172088874 | validation: 0.460584150052538]
	TIME [epoch: 6.49 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.712063599279249		[learning rate: 0.00068081]
	Learning Rate: 0.00068081
	LOSS [training: 0.712063599279249 | validation: 0.4574342076265282]
	TIME [epoch: 6.5 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7164979828500948		[learning rate: 0.0006792]
	Learning Rate: 0.000679204
	LOSS [training: 0.7164979828500948 | validation: 0.4407145826524812]
	TIME [epoch: 6.53 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7006480969777625		[learning rate: 0.0006776]
	Learning Rate: 0.000677601
	LOSS [training: 0.7006480969777625 | validation: 0.44333980175452525]
	TIME [epoch: 6.49 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7058010142086789		[learning rate: 0.000676]
	Learning Rate: 0.000676003
	LOSS [training: 0.7058010142086789 | validation: 0.43701383927665327]
	TIME [epoch: 6.49 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7043722013277026		[learning rate: 0.00067441]
	Learning Rate: 0.000674409
	LOSS [training: 0.7043722013277026 | validation: 0.4421868735579915]
	TIME [epoch: 6.49 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7007079986686705		[learning rate: 0.00067282]
	Learning Rate: 0.000672818
	LOSS [training: 0.7007079986686705 | validation: 0.4415188336610605]
	TIME [epoch: 6.49 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7030809711169338		[learning rate: 0.00067123]
	Learning Rate: 0.000671231
	LOSS [training: 0.7030809711169338 | validation: 0.438406968772567]
	TIME [epoch: 6.5 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7011565614628094		[learning rate: 0.00066965]
	Learning Rate: 0.000669647
	LOSS [training: 0.7011565614628094 | validation: 0.44127601530225524]
	TIME [epoch: 6.52 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7087406463331336		[learning rate: 0.00066807]
	Learning Rate: 0.000668068
	LOSS [training: 0.7087406463331336 | validation: 0.43290189637504944]
	TIME [epoch: 6.48 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7001525421567905		[learning rate: 0.00066649]
	Learning Rate: 0.000666492
	LOSS [training: 0.7001525421567905 | validation: 0.43654572506855904]
	TIME [epoch: 6.48 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7034102555550554		[learning rate: 0.00066492]
	Learning Rate: 0.00066492
	LOSS [training: 0.7034102555550554 | validation: 0.44123671208632037]
	TIME [epoch: 6.49 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7065781044150163		[learning rate: 0.00066335]
	Learning Rate: 0.000663351
	LOSS [training: 0.7065781044150163 | validation: 0.446918556143255]
	TIME [epoch: 6.51 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.704282368497001		[learning rate: 0.00066179]
	Learning Rate: 0.000661786
	LOSS [training: 0.704282368497001 | validation: 0.44296879112217513]
	TIME [epoch: 6.53 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7079283292201097		[learning rate: 0.00066023]
	Learning Rate: 0.000660225
	LOSS [training: 0.7079283292201097 | validation: 0.45288027033648504]
	TIME [epoch: 6.51 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7206029248150718		[learning rate: 0.00065867]
	Learning Rate: 0.000658668
	LOSS [training: 0.7206029248150718 | validation: 0.45919551624260013]
	TIME [epoch: 6.49 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7085321462672051		[learning rate: 0.00065711]
	Learning Rate: 0.000657114
	LOSS [training: 0.7085321462672051 | validation: 0.45161013754609325]
	TIME [epoch: 6.5 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7075805893437426		[learning rate: 0.00065556]
	Learning Rate: 0.000655564
	LOSS [training: 0.7075805893437426 | validation: 0.4502032616884487]
	TIME [epoch: 6.51 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.715504086209432		[learning rate: 0.00065402]
	Learning Rate: 0.000654018
	LOSS [training: 0.715504086209432 | validation: 0.4544884788531049]
	TIME [epoch: 6.49 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7114411035207978		[learning rate: 0.00065248]
	Learning Rate: 0.000652475
	LOSS [training: 0.7114411035207978 | validation: 0.4405520164283483]
	TIME [epoch: 6.55 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7081475397883039		[learning rate: 0.00065094]
	Learning Rate: 0.000650936
	LOSS [training: 0.7081475397883039 | validation: 0.4441117220316265]
	TIME [epoch: 6.51 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7031771940230428		[learning rate: 0.0006494]
	Learning Rate: 0.000649401
	LOSS [training: 0.7031771940230428 | validation: 0.443371445234141]
	TIME [epoch: 6.48 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6944839199821822		[learning rate: 0.00064787]
	Learning Rate: 0.000647869
	LOSS [training: 0.6944839199821822 | validation: 0.4425667280119932]
	TIME [epoch: 6.49 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6959988784107507		[learning rate: 0.00064634]
	Learning Rate: 0.000646341
	LOSS [training: 0.6959988784107507 | validation: 0.43182103641314995]
	TIME [epoch: 6.49 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6952324054609408		[learning rate: 0.00064482]
	Learning Rate: 0.000644816
	LOSS [training: 0.6952324054609408 | validation: 0.43050610010730095]
	TIME [epoch: 6.52 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.695763388388429		[learning rate: 0.0006433]
	Learning Rate: 0.000643295
	LOSS [training: 0.695763388388429 | validation: 0.43117567682064534]
	TIME [epoch: 6.54 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6965984377282083		[learning rate: 0.00064178]
	Learning Rate: 0.000641778
	LOSS [training: 0.6965984377282083 | validation: 0.43570826950867364]
	TIME [epoch: 6.51 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.702008909822885		[learning rate: 0.00064026]
	Learning Rate: 0.000640264
	LOSS [training: 0.702008909822885 | validation: 0.4384236407704175]
	TIME [epoch: 6.52 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6999356559278184		[learning rate: 0.00063875]
	Learning Rate: 0.000638754
	LOSS [training: 0.6999356559278184 | validation: 0.44048562064330066]
	TIME [epoch: 6.51 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7000816116853004		[learning rate: 0.00063725]
	Learning Rate: 0.000637247
	LOSS [training: 0.7000816116853004 | validation: 0.4364331493712407]
	TIME [epoch: 6.51 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7043625069032728		[learning rate: 0.00063574]
	Learning Rate: 0.000635744
	LOSS [training: 0.7043625069032728 | validation: 0.44263461023821027]
	TIME [epoch: 6.53 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6995016023189685		[learning rate: 0.00063424]
	Learning Rate: 0.000634244
	LOSS [training: 0.6995016023189685 | validation: 0.4467488285374092]
	TIME [epoch: 6.54 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7116238512349335		[learning rate: 0.00063275]
	Learning Rate: 0.000632748
	LOSS [training: 0.7116238512349335 | validation: 0.4513335799052024]
	TIME [epoch: 6.51 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7134858683435923		[learning rate: 0.00063126]
	Learning Rate: 0.000631255
	LOSS [training: 0.7134858683435923 | validation: 0.47841726090754505]
	TIME [epoch: 6.51 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7337650882510067		[learning rate: 0.00062977]
	Learning Rate: 0.000629766
	LOSS [training: 0.7337650882510067 | validation: 0.4589476112494653]
	TIME [epoch: 6.51 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7233431566399066		[learning rate: 0.00062828]
	Learning Rate: 0.000628281
	LOSS [training: 0.7233431566399066 | validation: 0.46548423950725104]
	TIME [epoch: 6.51 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.730012399744056		[learning rate: 0.0006268]
	Learning Rate: 0.000626799
	LOSS [training: 0.730012399744056 | validation: 0.46798873953241626]
	TIME [epoch: 6.55 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7208704923829503		[learning rate: 0.00062532]
	Learning Rate: 0.00062532
	LOSS [training: 0.7208704923829503 | validation: 0.4456210903359619]
	TIME [epoch: 6.52 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7053591380930362		[learning rate: 0.00062385]
	Learning Rate: 0.000623845
	LOSS [training: 0.7053591380930362 | validation: 0.4388623568147959]
	TIME [epoch: 6.52 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6982800304718596		[learning rate: 0.00062237]
	Learning Rate: 0.000622374
	LOSS [training: 0.6982800304718596 | validation: 0.4360612395250554]
	TIME [epoch: 6.51 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7025245259241808		[learning rate: 0.00062091]
	Learning Rate: 0.000620906
	LOSS [training: 0.7025245259241808 | validation: 0.4421485979003918]
	TIME [epoch: 6.52 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7053549066605752		[learning rate: 0.00061944]
	Learning Rate: 0.000619441
	LOSS [training: 0.7053549066605752 | validation: 0.4399359050319702]
	TIME [epoch: 6.49 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6970902815589426		[learning rate: 0.00061798]
	Learning Rate: 0.00061798
	LOSS [training: 0.6970902815589426 | validation: 0.44385253255244556]
	TIME [epoch: 6.57 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6998474774776889		[learning rate: 0.00061652]
	Learning Rate: 0.000616522
	LOSS [training: 0.6998474774776889 | validation: 0.43476764731415496]
	TIME [epoch: 6.51 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.697700468108054		[learning rate: 0.00061507]
	Learning Rate: 0.000615068
	LOSS [training: 0.697700468108054 | validation: 0.4430563589519707]
	TIME [epoch: 6.51 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7091197588377096		[learning rate: 0.00061362]
	Learning Rate: 0.000613617
	LOSS [training: 0.7091197588377096 | validation: 0.4504361600013212]
	TIME [epoch: 6.51 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.70601370228047		[learning rate: 0.00061217]
	Learning Rate: 0.00061217
	LOSS [training: 0.70601370228047 | validation: 0.4442723812692633]
	TIME [epoch: 6.52 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.699950337025358		[learning rate: 0.00061073]
	Learning Rate: 0.000610726
	LOSS [training: 0.699950337025358 | validation: 0.43915361049099705]
	TIME [epoch: 6.53 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6979005332996919		[learning rate: 0.00060929]
	Learning Rate: 0.000609285
	LOSS [training: 0.6979005332996919 | validation: 0.43709488330993584]
	TIME [epoch: 6.53 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6973879865753156		[learning rate: 0.00060785]
	Learning Rate: 0.000607848
	LOSS [training: 0.6973879865753156 | validation: 0.44474031337221015]
	TIME [epoch: 6.52 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7019457285940514		[learning rate: 0.00060641]
	Learning Rate: 0.000606414
	LOSS [training: 0.7019457285940514 | validation: 0.44928305438899085]
	TIME [epoch: 6.51 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6978660738960176		[learning rate: 0.00060498]
	Learning Rate: 0.000604983
	LOSS [training: 0.6978660738960176 | validation: 0.43886918551899856]
	TIME [epoch: 6.5 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6908299825303594		[learning rate: 0.00060356]
	Learning Rate: 0.000603557
	LOSS [training: 0.6908299825303594 | validation: 0.440589579261254]
	TIME [epoch: 6.51 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6999794438653912		[learning rate: 0.00060213]
	Learning Rate: 0.000602133
	LOSS [training: 0.6999794438653912 | validation: 0.4435067391239942]
	TIME [epoch: 6.55 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.708858545160083		[learning rate: 0.00060071]
	Learning Rate: 0.000600712
	LOSS [training: 0.708858545160083 | validation: 0.43888245666002046]
	TIME [epoch: 6.52 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7010406350035355		[learning rate: 0.0005993]
	Learning Rate: 0.000599296
	LOSS [training: 0.7010406350035355 | validation: 0.43985893265255216]
	TIME [epoch: 6.49 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7041141847424846		[learning rate: 0.00059788]
	Learning Rate: 0.000597882
	LOSS [training: 0.7041141847424846 | validation: 0.45079507614511855]
	TIME [epoch: 6.49 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7093639236939978		[learning rate: 0.00059647]
	Learning Rate: 0.000596471
	LOSS [training: 0.7093639236939978 | validation: 0.43776749450745045]
	TIME [epoch: 6.5 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7030402310272778		[learning rate: 0.00059506]
	Learning Rate: 0.000595065
	LOSS [training: 0.7030402310272778 | validation: 0.434441672647328]
	TIME [epoch: 6.49 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7027366868138005		[learning rate: 0.00059366]
	Learning Rate: 0.000593661
	LOSS [training: 0.7027366868138005 | validation: 0.4488579186127267]
	TIME [epoch: 6.53 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.705773740425711		[learning rate: 0.00059226]
	Learning Rate: 0.000592261
	LOSS [training: 0.705773740425711 | validation: 0.4384593962066466]
	TIME [epoch: 6.49 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7029919315777585		[learning rate: 0.00059086]
	Learning Rate: 0.000590863
	LOSS [training: 0.7029919315777585 | validation: 0.43898101945114737]
	TIME [epoch: 6.5 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6986933138545687		[learning rate: 0.00058947]
	Learning Rate: 0.00058947
	LOSS [training: 0.6986933138545687 | validation: 0.44272872454452683]
	TIME [epoch: 6.49 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7002450574747829		[learning rate: 0.00058808]
	Learning Rate: 0.000588079
	LOSS [training: 0.7002450574747829 | validation: 0.4514492463239669]
	TIME [epoch: 6.48 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7026100862463585		[learning rate: 0.00058669]
	Learning Rate: 0.000586692
	LOSS [training: 0.7026100862463585 | validation: 0.44428701770316925]
	TIME [epoch: 6.49 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7076023748123795		[learning rate: 0.00058531]
	Learning Rate: 0.000585308
	LOSS [training: 0.7076023748123795 | validation: 0.4477833423526876]
	TIME [epoch: 6.53 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7139175530913114		[learning rate: 0.00058393]
	Learning Rate: 0.000583927
	LOSS [training: 0.7139175530913114 | validation: 0.44335355386823466]
	TIME [epoch: 6.49 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7101894740738401		[learning rate: 0.00058255]
	Learning Rate: 0.00058255
	LOSS [training: 0.7101894740738401 | validation: 0.43788764649347567]
	TIME [epoch: 6.49 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7035386130504463		[learning rate: 0.00058118]
	Learning Rate: 0.000581176
	LOSS [training: 0.7035386130504463 | validation: 0.4458893485675008]
	TIME [epoch: 6.49 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7032853382567293		[learning rate: 0.00057981]
	Learning Rate: 0.000579805
	LOSS [training: 0.7032853382567293 | validation: 0.4397683058389793]
	TIME [epoch: 6.49 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7006737718017743		[learning rate: 0.00057844]
	Learning Rate: 0.000578437
	LOSS [training: 0.7006737718017743 | validation: 0.4346672515955692]
	TIME [epoch: 6.51 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7010233904118479		[learning rate: 0.00057707]
	Learning Rate: 0.000577073
	LOSS [training: 0.7010233904118479 | validation: 0.4410003346309128]
	TIME [epoch: 6.52 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6993102254091843		[learning rate: 0.00057571]
	Learning Rate: 0.000575712
	LOSS [training: 0.6993102254091843 | validation: 0.4362299708828362]
	TIME [epoch: 6.51 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6974526637071723		[learning rate: 0.00057435]
	Learning Rate: 0.000574354
	LOSS [training: 0.6974526637071723 | validation: 0.4458075919470194]
	TIME [epoch: 6.5 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6965404652851342		[learning rate: 0.000573]
	Learning Rate: 0.000572999
	LOSS [training: 0.6965404652851342 | validation: 0.4317933299207301]
	TIME [epoch: 6.48 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6985843579171214		[learning rate: 0.00057165]
	Learning Rate: 0.000571647
	LOSS [training: 0.6985843579171214 | validation: 0.4326123550313275]
	TIME [epoch: 6.5 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694379565896875		[learning rate: 0.0005703]
	Learning Rate: 0.000570299
	LOSS [training: 0.694379565896875 | validation: 0.43397453974136607]
	TIME [epoch: 6.53 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.697373795308463		[learning rate: 0.00056895]
	Learning Rate: 0.000568954
	LOSS [training: 0.697373795308463 | validation: 0.436636075369506]
	TIME [epoch: 6.51 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6984203903190502		[learning rate: 0.00056761]
	Learning Rate: 0.000567612
	LOSS [training: 0.6984203903190502 | validation: 0.4381920062197834]
	TIME [epoch: 6.47 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6979895498251458		[learning rate: 0.00056627]
	Learning Rate: 0.000566273
	LOSS [training: 0.6979895498251458 | validation: 0.4326078929212084]
	TIME [epoch: 6.49 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6975048389861432		[learning rate: 0.00056494]
	Learning Rate: 0.000564937
	LOSS [training: 0.6975048389861432 | validation: 0.43549767362165]
	TIME [epoch: 6.49 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7006385637382271		[learning rate: 0.0005636]
	Learning Rate: 0.000563604
	LOSS [training: 0.7006385637382271 | validation: 0.4381804022739624]
	TIME [epoch: 6.48 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7042191624075878		[learning rate: 0.00056227]
	Learning Rate: 0.000562275
	LOSS [training: 0.7042191624075878 | validation: 0.43648912330575285]
	TIME [epoch: 6.52 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6983263893602268		[learning rate: 0.00056095]
	Learning Rate: 0.000560949
	LOSS [training: 0.6983263893602268 | validation: 0.4353741471568568]
	TIME [epoch: 6.49 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6975560769544302		[learning rate: 0.00055963]
	Learning Rate: 0.000559625
	LOSS [training: 0.6975560769544302 | validation: 0.4309795308132462]
	TIME [epoch: 6.51 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6953468270854586		[learning rate: 0.00055831]
	Learning Rate: 0.000558305
	LOSS [training: 0.6953468270854586 | validation: 0.44062814344909684]
	TIME [epoch: 6.51 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7128547561854377		[learning rate: 0.00055699]
	Learning Rate: 0.000556988
	LOSS [training: 0.7128547561854377 | validation: 0.45359747398328365]
	TIME [epoch: 6.48 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7018229485321704		[learning rate: 0.00055567]
	Learning Rate: 0.000555674
	LOSS [training: 0.7018229485321704 | validation: 0.43883575909651407]
	TIME [epoch: 6.48 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6969216280711464		[learning rate: 0.00055436]
	Learning Rate: 0.000554364
	LOSS [training: 0.6969216280711464 | validation: 0.4383596122552347]
	TIME [epoch: 6.53 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6936364439163306		[learning rate: 0.00055306]
	Learning Rate: 0.000553056
	LOSS [training: 0.6936364439163306 | validation: 0.42484670310714734]
	TIME [epoch: 6.49 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7010969796159124		[learning rate: 0.00055175]
	Learning Rate: 0.000551752
	LOSS [training: 0.7010969796159124 | validation: 0.42303216791234427]
	TIME [epoch: 6.49 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6977272646339441		[learning rate: 0.00055045]
	Learning Rate: 0.00055045
	LOSS [training: 0.6977272646339441 | validation: 0.4312466773216193]
	TIME [epoch: 6.49 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6942151425269002		[learning rate: 0.00054915]
	Learning Rate: 0.000549152
	LOSS [training: 0.6942151425269002 | validation: 0.432944433314429]
	TIME [epoch: 6.49 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6978517765761432		[learning rate: 0.00054786]
	Learning Rate: 0.000547856
	LOSS [training: 0.6978517765761432 | validation: 0.45027635644037056]
	TIME [epoch: 6.5 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6974539700709046		[learning rate: 0.00054656]
	Learning Rate: 0.000546564
	LOSS [training: 0.6974539700709046 | validation: 0.43987303906962905]
	TIME [epoch: 6.5 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6981461459239504		[learning rate: 0.00054527]
	Learning Rate: 0.000545275
	LOSS [training: 0.6981461459239504 | validation: 0.4467551732658892]
	TIME [epoch: 6.49 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7057991707994399		[learning rate: 0.00054399]
	Learning Rate: 0.000543988
	LOSS [training: 0.7057991707994399 | validation: 0.44072432930710015]
	TIME [epoch: 6.49 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7016047038523776		[learning rate: 0.00054271]
	Learning Rate: 0.000542705
	LOSS [training: 0.7016047038523776 | validation: 0.4306947573231662]
	TIME [epoch: 6.49 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6940328461232522		[learning rate: 0.00054143]
	Learning Rate: 0.000541425
	LOSS [training: 0.6940328461232522 | validation: 0.43608651036954676]
	TIME [epoch: 6.48 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6944065739138665		[learning rate: 0.00054015]
	Learning Rate: 0.000540148
	LOSS [training: 0.6944065739138665 | validation: 0.4326448592809673]
	TIME [epoch: 6.52 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6954164136433963		[learning rate: 0.00053887]
	Learning Rate: 0.000538874
	LOSS [training: 0.6954164136433963 | validation: 0.4413051856115232]
	TIME [epoch: 6.49 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6980317204652846		[learning rate: 0.0005376]
	Learning Rate: 0.000537603
	LOSS [training: 0.6980317204652846 | validation: 0.4283423719255067]
	TIME [epoch: 6.47 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6979550577854182		[learning rate: 0.00053633]
	Learning Rate: 0.000536335
	LOSS [training: 0.6979550577854182 | validation: 0.43595718253908583]
	TIME [epoch: 6.49 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6967961916301427		[learning rate: 0.00053507]
	Learning Rate: 0.00053507
	LOSS [training: 0.6967961916301427 | validation: 0.4361643888590827]
	TIME [epoch: 6.47 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6935335680515827		[learning rate: 0.00053381]
	Learning Rate: 0.000533807
	LOSS [training: 0.6935335680515827 | validation: 0.4354795007755775]
	TIME [epoch: 6.49 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6958990536344795		[learning rate: 0.00053255]
	Learning Rate: 0.000532548
	LOSS [training: 0.6958990536344795 | validation: 0.4323311180130078]
	TIME [epoch: 6.54 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6953127360707916		[learning rate: 0.00053129]
	Learning Rate: 0.000531292
	LOSS [training: 0.6953127360707916 | validation: 0.4363360576915245]
	TIME [epoch: 6.49 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6975267471873074		[learning rate: 0.00053004]
	Learning Rate: 0.000530039
	LOSS [training: 0.6975267471873074 | validation: 0.4332337279214827]
	TIME [epoch: 6.47 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6903375109578596		[learning rate: 0.00052879]
	Learning Rate: 0.000528789
	LOSS [training: 0.6903375109578596 | validation: 0.4353549358325409]
	TIME [epoch: 6.47 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.695614822024611		[learning rate: 0.00052754]
	Learning Rate: 0.000527541
	LOSS [training: 0.695614822024611 | validation: 0.4237367988811131]
	TIME [epoch: 6.47 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6980237992597379		[learning rate: 0.0005263]
	Learning Rate: 0.000526297
	LOSS [training: 0.6980237992597379 | validation: 0.4513609316458458]
	TIME [epoch: 6.49 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7067940272159496		[learning rate: 0.00052506]
	Learning Rate: 0.000525055
	LOSS [training: 0.7067940272159496 | validation: 0.4354025357903767]
	TIME [epoch: 6.54 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6968106088955606		[learning rate: 0.00052382]
	Learning Rate: 0.000523817
	LOSS [training: 0.6968106088955606 | validation: 0.44253819171698455]
	TIME [epoch: 6.49 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.706440946812106		[learning rate: 0.00052258]
	Learning Rate: 0.000522581
	LOSS [training: 0.706440946812106 | validation: 0.43832249145741287]
	TIME [epoch: 6.48 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6963305172154518		[learning rate: 0.00052135]
	Learning Rate: 0.000521348
	LOSS [training: 0.6963305172154518 | validation: 0.4426629773354016]
	TIME [epoch: 6.5 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7005774809456784		[learning rate: 0.00052012]
	Learning Rate: 0.000520119
	LOSS [training: 0.7005774809456784 | validation: 0.4530287364467487]
	TIME [epoch: 6.48 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7095138217653467		[learning rate: 0.00051889]
	Learning Rate: 0.000518892
	LOSS [training: 0.7095138217653467 | validation: 0.4459022036322882]
	TIME [epoch: 6.5 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.708116873964938		[learning rate: 0.00051767]
	Learning Rate: 0.000517668
	LOSS [training: 0.708116873964938 | validation: 0.44671169526270094]
	TIME [epoch: 6.52 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7027395233531737		[learning rate: 0.00051645]
	Learning Rate: 0.000516447
	LOSS [training: 0.7027395233531737 | validation: 0.44926986091273097]
	TIME [epoch: 6.47 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7066450654441659		[learning rate: 0.00051523]
	Learning Rate: 0.000515229
	LOSS [training: 0.7066450654441659 | validation: 0.44981010475806776]
	TIME [epoch: 6.48 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.704010062818089		[learning rate: 0.00051401]
	Learning Rate: 0.000514013
	LOSS [training: 0.704010062818089 | validation: 0.4402442560797926]
	TIME [epoch: 6.48 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7015063864392811		[learning rate: 0.0005128]
	Learning Rate: 0.000512801
	LOSS [training: 0.7015063864392811 | validation: 0.4311313409802122]
	TIME [epoch: 6.47 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7000629963487549		[learning rate: 0.00051159]
	Learning Rate: 0.000511591
	LOSS [training: 0.7000629963487549 | validation: 0.43550435732840853]
	TIME [epoch: 6.52 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6995028105581353		[learning rate: 0.00051038]
	Learning Rate: 0.000510384
	LOSS [training: 0.6995028105581353 | validation: 0.4241178769782217]
	TIME [epoch: 6.5 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6969234824939926		[learning rate: 0.00050918]
	Learning Rate: 0.00050918
	LOSS [training: 0.6969234824939926 | validation: 0.4403550292161029]
	TIME [epoch: 6.47 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6980779517736657		[learning rate: 0.00050798]
	Learning Rate: 0.000507979
	LOSS [training: 0.6980779517736657 | validation: 0.4342103550336267]
	TIME [epoch: 6.49 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7013341955607089		[learning rate: 0.00050678]
	Learning Rate: 0.000506781
	LOSS [training: 0.7013341955607089 | validation: 0.4449150225525213]
	TIME [epoch: 6.5 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7063562008057992		[learning rate: 0.00050559]
	Learning Rate: 0.000505586
	LOSS [training: 0.7063562008057992 | validation: 0.43359015612968843]
	TIME [epoch: 6.49 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6965637921695907		[learning rate: 0.00050439]
	Learning Rate: 0.000504393
	LOSS [training: 0.6965637921695907 | validation: 0.4380037465161614]
	TIME [epoch: 6.52 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6980274340592654		[learning rate: 0.0005032]
	Learning Rate: 0.000503203
	LOSS [training: 0.6980274340592654 | validation: 0.432130980034956]
	TIME [epoch: 6.49 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7027277097241533		[learning rate: 0.00050202]
	Learning Rate: 0.000502016
	LOSS [training: 0.7027277097241533 | validation: 0.4344445211407559]
	TIME [epoch: 6.48 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7071666052527187		[learning rate: 0.00050083]
	Learning Rate: 0.000500832
	LOSS [training: 0.7071666052527187 | validation: 0.44179238491204725]
	TIME [epoch: 6.48 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6956616890936516		[learning rate: 0.00049965]
	Learning Rate: 0.000499651
	LOSS [training: 0.6956616890936516 | validation: 0.44311376836553507]
	TIME [epoch: 6.47 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.701491940437297		[learning rate: 0.00049847]
	Learning Rate: 0.000498472
	LOSS [training: 0.701491940437297 | validation: 0.4549480475700507]
	TIME [epoch: 6.48 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7023805633282294		[learning rate: 0.0004973]
	Learning Rate: 0.000497296
	LOSS [training: 0.7023805633282294 | validation: 0.4320971331873672]
	TIME [epoch: 6.52 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7005815906141784		[learning rate: 0.00049612]
	Learning Rate: 0.000496123
	LOSS [training: 0.7005815906141784 | validation: 0.4420284540720429]
	TIME [epoch: 6.48 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6995759818982343		[learning rate: 0.00049495]
	Learning Rate: 0.000494953
	LOSS [training: 0.6995759818982343 | validation: 0.4392862480207739]
	TIME [epoch: 6.49 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7035916639629615		[learning rate: 0.00049379]
	Learning Rate: 0.000493786
	LOSS [training: 0.7035916639629615 | validation: 0.44107116930991463]
	TIME [epoch: 6.48 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7118166731216471		[learning rate: 0.00049262]
	Learning Rate: 0.000492621
	LOSS [training: 0.7118166731216471 | validation: 0.4475571189993652]
	TIME [epoch: 6.49 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7036488066574015		[learning rate: 0.00049146]
	Learning Rate: 0.000491459
	LOSS [training: 0.7036488066574015 | validation: 0.43840412705889853]
	TIME [epoch: 6.49 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6995699776874081		[learning rate: 0.0004903]
	Learning Rate: 0.0004903
	LOSS [training: 0.6995699776874081 | validation: 0.43445269049578533]
	TIME [epoch: 6.51 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6953265115049065		[learning rate: 0.00048914]
	Learning Rate: 0.000489143
	LOSS [training: 0.6953265115049065 | validation: 0.4357966390996444]
	TIME [epoch: 6.48 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694953577025242		[learning rate: 0.00048799]
	Learning Rate: 0.000487989
	LOSS [training: 0.694953577025242 | validation: 0.4305885347028763]
	TIME [epoch: 6.48 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6989073839541736		[learning rate: 0.00048684]
	Learning Rate: 0.000486838
	LOSS [training: 0.6989073839541736 | validation: 0.4340825213420577]
	TIME [epoch: 6.48 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6972232820266847		[learning rate: 0.00048569]
	Learning Rate: 0.00048569
	LOSS [training: 0.6972232820266847 | validation: 0.43559777497216234]
	TIME [epoch: 6.49 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7025118315182617		[learning rate: 0.00048454]
	Learning Rate: 0.000484544
	LOSS [training: 0.7025118315182617 | validation: 0.43663585767108853]
	TIME [epoch: 6.52 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6961218901418265		[learning rate: 0.0004834]
	Learning Rate: 0.000483401
	LOSS [training: 0.6961218901418265 | validation: 0.42955251380506887]
	TIME [epoch: 6.49 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6948478538461798		[learning rate: 0.00048226]
	Learning Rate: 0.000482261
	LOSS [training: 0.6948478538461798 | validation: 0.4364142463219276]
	TIME [epoch: 6.48 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6968958386358033		[learning rate: 0.00048112]
	Learning Rate: 0.000481123
	LOSS [training: 0.6968958386358033 | validation: 0.44104541900811056]
	TIME [epoch: 6.48 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7123201056071469		[learning rate: 0.00047999]
	Learning Rate: 0.000479988
	LOSS [training: 0.7123201056071469 | validation: 0.4517198907321229]
	TIME [epoch: 6.47 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7064953827143992		[learning rate: 0.00047886]
	Learning Rate: 0.000478856
	LOSS [training: 0.7064953827143992 | validation: 0.4290566057885649]
	TIME [epoch: 6.48 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6939020379575885		[learning rate: 0.00047773]
	Learning Rate: 0.000477727
	LOSS [training: 0.6939020379575885 | validation: 0.4360573404503233]
	TIME [epoch: 6.56 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6973019836668561		[learning rate: 0.0004766]
	Learning Rate: 0.0004766
	LOSS [training: 0.6973019836668561 | validation: 0.435738253725081]
	TIME [epoch: 6.49 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.693532262473857		[learning rate: 0.00047548]
	Learning Rate: 0.000475476
	LOSS [training: 0.693532262473857 | validation: 0.4274676093620699]
	TIME [epoch: 6.48 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6975089149378195		[learning rate: 0.00047435]
	Learning Rate: 0.000474354
	LOSS [training: 0.6975089149378195 | validation: 0.4324228797267345]
	TIME [epoch: 6.48 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6945459630416835		[learning rate: 0.00047324]
	Learning Rate: 0.000473235
	LOSS [training: 0.6945459630416835 | validation: 0.44163631152034843]
	TIME [epoch: 6.48 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7015497940249681		[learning rate: 0.00047212]
	Learning Rate: 0.000472119
	LOSS [training: 0.7015497940249681 | validation: 0.44004997567323223]
	TIME [epoch: 6.49 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7064957399726131		[learning rate: 0.00047101]
	Learning Rate: 0.000471005
	LOSS [training: 0.7064957399726131 | validation: 0.44850982344443513]
	TIME [epoch: 6.52 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7007453392981604		[learning rate: 0.00046989]
	Learning Rate: 0.000469894
	LOSS [training: 0.7007453392981604 | validation: 0.4407877469029873]
	TIME [epoch: 6.48 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7079669880878713		[learning rate: 0.00046879]
	Learning Rate: 0.000468786
	LOSS [training: 0.7079669880878713 | validation: 0.4410501085447516]
	TIME [epoch: 6.48 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7134595567231005		[learning rate: 0.00046768]
	Learning Rate: 0.00046768
	LOSS [training: 0.7134595567231005 | validation: 0.4386196882537586]
	TIME [epoch: 6.49 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7029635100902072		[learning rate: 0.00046658]
	Learning Rate: 0.000466577
	LOSS [training: 0.7029635100902072 | validation: 0.44430568890262767]
	TIME [epoch: 6.48 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6961135885386109		[learning rate: 0.00046548]
	Learning Rate: 0.000465476
	LOSS [training: 0.6961135885386109 | validation: 0.44095317915624105]
	TIME [epoch: 6.51 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7039294953338032		[learning rate: 0.00046438]
	Learning Rate: 0.000464378
	LOSS [training: 0.7039294953338032 | validation: 0.4463146732483494]
	TIME [epoch: 6.53 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.702824222942397		[learning rate: 0.00046328]
	Learning Rate: 0.000463283
	LOSS [training: 0.702824222942397 | validation: 0.435836703166952]
	TIME [epoch: 6.49 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6962373187037332		[learning rate: 0.00046219]
	Learning Rate: 0.00046219
	LOSS [training: 0.6962373187037332 | validation: 0.42958459236665714]
	TIME [epoch: 6.5 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.69438062718487		[learning rate: 0.0004611]
	Learning Rate: 0.0004611
	LOSS [training: 0.69438062718487 | validation: 0.43416888338169823]
	TIME [epoch: 6.48 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6966071211921652		[learning rate: 0.00046001]
	Learning Rate: 0.000460012
	LOSS [training: 0.6966071211921652 | validation: 0.43700485591816973]
	TIME [epoch: 6.5 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933250497463861		[learning rate: 0.00045893]
	Learning Rate: 0.000458927
	LOSS [training: 0.6933250497463861 | validation: 0.42963716075362846]
	TIME [epoch: 6.52 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934164992525578		[learning rate: 0.00045784]
	Learning Rate: 0.000457844
	LOSS [training: 0.6934164992525578 | validation: 0.43212548934836037]
	TIME [epoch: 6.51 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6939610589045856		[learning rate: 0.00045676]
	Learning Rate: 0.000456764
	LOSS [training: 0.6939610589045856 | validation: 0.4314468409411232]
	TIME [epoch: 6.49 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6916720689050182		[learning rate: 0.00045569]
	Learning Rate: 0.000455687
	LOSS [training: 0.6916720689050182 | validation: 0.43379013484216145]
	TIME [epoch: 6.52 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6940763704056558		[learning rate: 0.00045461]
	Learning Rate: 0.000454612
	LOSS [training: 0.6940763704056558 | validation: 0.4351362927311032]
	TIME [epoch: 6.57 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7043630503818484		[learning rate: 0.00045354]
	Learning Rate: 0.00045354
	LOSS [training: 0.7043630503818484 | validation: 0.45303940785998575]
	TIME [epoch: 6.58 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7058956961256937		[learning rate: 0.00045247]
	Learning Rate: 0.00045247
	LOSS [training: 0.7058956961256937 | validation: 0.4365891498444764]
	TIME [epoch: 6.62 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.696183604696758		[learning rate: 0.0004514]
	Learning Rate: 0.000451403
	LOSS [training: 0.696183604696758 | validation: 0.4356645295693664]
	TIME [epoch: 6.56 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6917770421903772		[learning rate: 0.00045034]
	Learning Rate: 0.000450338
	LOSS [training: 0.6917770421903772 | validation: 0.43155093958016233]
	TIME [epoch: 6.55 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6952717853888116		[learning rate: 0.00044928]
	Learning Rate: 0.000449276
	LOSS [training: 0.6952717853888116 | validation: 0.43088588847251097]
	TIME [epoch: 6.55 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6994779254012252		[learning rate: 0.00044822]
	Learning Rate: 0.000448216
	LOSS [training: 0.6994779254012252 | validation: 0.43564112231760777]
	TIME [epoch: 6.55 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6980079096913		[learning rate: 0.00044716]
	Learning Rate: 0.000447159
	LOSS [training: 0.6980079096913 | validation: 0.43965171309024925]
	TIME [epoch: 6.58 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7014535425569826		[learning rate: 0.0004461]
	Learning Rate: 0.000446104
	LOSS [training: 0.7014535425569826 | validation: 0.43086564075562955]
	TIME [epoch: 6.59 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6938912652591582		[learning rate: 0.00044505]
	Learning Rate: 0.000445051
	LOSS [training: 0.6938912652591582 | validation: 0.42964126127362195]
	TIME [epoch: 6.55 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6879965849351627		[learning rate: 0.000444]
	Learning Rate: 0.000444002
	LOSS [training: 0.6879965849351627 | validation: 0.43493148615697236]
	TIME [epoch: 6.57 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6936558635545951		[learning rate: 0.00044295]
	Learning Rate: 0.000442954
	LOSS [training: 0.6936558635545951 | validation: 0.43272642744351175]
	TIME [epoch: 6.57 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.691919128009914		[learning rate: 0.00044191]
	Learning Rate: 0.000441909
	LOSS [training: 0.691919128009914 | validation: 0.43744143133894514]
	TIME [epoch: 6.55 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6962077581016792		[learning rate: 0.00044087]
	Learning Rate: 0.000440867
	LOSS [training: 0.6962077581016792 | validation: 0.4334849650276207]
	TIME [epoch: 6.54 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6967264744926748		[learning rate: 0.00043983]
	Learning Rate: 0.000439827
	LOSS [training: 0.6967264744926748 | validation: 0.4458470250712165]
	TIME [epoch: 6.54 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6970841894928845		[learning rate: 0.00043879]
	Learning Rate: 0.00043879
	LOSS [training: 0.6970841894928845 | validation: 0.42709274818017795]
	TIME [epoch: 6.51 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933586155442089		[learning rate: 0.00043775]
	Learning Rate: 0.000437755
	LOSS [training: 0.6933586155442089 | validation: 0.4397992038954882]
	TIME [epoch: 6.5 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6930520455292435		[learning rate: 0.00043672]
	Learning Rate: 0.000436722
	LOSS [training: 0.6930520455292435 | validation: 0.4246618043062779]
	TIME [epoch: 6.5 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6931683440941733		[learning rate: 0.00043569]
	Learning Rate: 0.000435692
	LOSS [training: 0.6931683440941733 | validation: 0.4277841645838568]
	TIME [epoch: 6.5 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6930541285137232		[learning rate: 0.00043466]
	Learning Rate: 0.000434664
	LOSS [training: 0.6930541285137232 | validation: 0.4237192788352667]
	TIME [epoch: 6.53 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933239157542139		[learning rate: 0.00043364]
	Learning Rate: 0.000433639
	LOSS [training: 0.6933239157542139 | validation: 0.4415583265381482]
	TIME [epoch: 6.51 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6971986804046368		[learning rate: 0.00043262]
	Learning Rate: 0.000432616
	LOSS [training: 0.6971986804046368 | validation: 0.44867163746947136]
	TIME [epoch: 6.49 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7058290435819193		[learning rate: 0.0004316]
	Learning Rate: 0.000431595
	LOSS [training: 0.7058290435819193 | validation: 0.4518457186847436]
	TIME [epoch: 6.48 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7081064720837582		[learning rate: 0.00043058]
	Learning Rate: 0.000430577
	LOSS [training: 0.7081064720837582 | validation: 0.4437601410817999]
	TIME [epoch: 6.5 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7000891565945866		[learning rate: 0.00042956]
	Learning Rate: 0.000429562
	LOSS [training: 0.7000891565945866 | validation: 0.43129129726496623]
	TIME [epoch: 6.49 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7007240510850405		[learning rate: 0.00042855]
	Learning Rate: 0.000428548
	LOSS [training: 0.7007240510850405 | validation: 0.44339324928989654]
	TIME [epoch: 6.53 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6972265654758527		[learning rate: 0.00042754]
	Learning Rate: 0.000427538
	LOSS [training: 0.6972265654758527 | validation: 0.4291437418793439]
	TIME [epoch: 6.5 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6922947981002789		[learning rate: 0.00042653]
	Learning Rate: 0.000426529
	LOSS [training: 0.6922947981002789 | validation: 0.42650520152626453]
	TIME [epoch: 6.49 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6983306931994709		[learning rate: 0.00042552]
	Learning Rate: 0.000425523
	LOSS [training: 0.6983306931994709 | validation: 0.43210525979015296]
	TIME [epoch: 6.5 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6985186784169087		[learning rate: 0.00042452]
	Learning Rate: 0.000424519
	LOSS [training: 0.6985186784169087 | validation: 0.4377156624851424]
	TIME [epoch: 6.47 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6992313874829545		[learning rate: 0.00042352]
	Learning Rate: 0.000423518
	LOSS [training: 0.6992313874829545 | validation: 0.4346454026971813]
	TIME [epoch: 6.5 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7005804867308402		[learning rate: 0.00042252]
	Learning Rate: 0.000422519
	LOSS [training: 0.7005804867308402 | validation: 0.4419387503921885]
	TIME [epoch: 6.51 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7020783350583042		[learning rate: 0.00042152]
	Learning Rate: 0.000421522
	LOSS [training: 0.7020783350583042 | validation: 0.43315974070432645]
	TIME [epoch: 6.48 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6951521224448076		[learning rate: 0.00042053]
	Learning Rate: 0.000420528
	LOSS [training: 0.6951521224448076 | validation: 0.4384049495470744]
	TIME [epoch: 6.49 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6954574159597973		[learning rate: 0.00041954]
	Learning Rate: 0.000419536
	LOSS [training: 0.6954574159597973 | validation: 0.4373371980815387]
	TIME [epoch: 6.46 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7019955405334615		[learning rate: 0.00041855]
	Learning Rate: 0.000418546
	LOSS [training: 0.7019955405334615 | validation: 0.44895086875236734]
	TIME [epoch: 6.49 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6972470732816891		[learning rate: 0.00041756]
	Learning Rate: 0.000417559
	LOSS [training: 0.6972470732816891 | validation: 0.43703621107288154]
	TIME [epoch: 6.52 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6949068453091707		[learning rate: 0.00041657]
	Learning Rate: 0.000416574
	LOSS [training: 0.6949068453091707 | validation: 0.44201885124703644]
	TIME [epoch: 6.54 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6981091975264355		[learning rate: 0.00041559]
	Learning Rate: 0.000415591
	LOSS [training: 0.6981091975264355 | validation: 0.4319527552849048]
	TIME [epoch: 6.5 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6985190172777701		[learning rate: 0.00041461]
	Learning Rate: 0.000414611
	LOSS [training: 0.6985190172777701 | validation: 0.43453153039182757]
	TIME [epoch: 6.49 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6990964365481454		[learning rate: 0.00041363]
	Learning Rate: 0.000413633
	LOSS [training: 0.6990964365481454 | validation: 0.4386566397027001]
	TIME [epoch: 6.49 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7010456837062962		[learning rate: 0.00041266]
	Learning Rate: 0.000412657
	LOSS [training: 0.7010456837062962 | validation: 0.44357288555021546]
	TIME [epoch: 6.49 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7070062018536103		[learning rate: 0.00041168]
	Learning Rate: 0.000411684
	LOSS [training: 0.7070062018536103 | validation: 0.44074118193458567]
	TIME [epoch: 6.54 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7024864380827794		[learning rate: 0.00041071]
	Learning Rate: 0.000410713
	LOSS [training: 0.7024864380827794 | validation: 0.44233030780435806]
	TIME [epoch: 6.51 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6959764223003112		[learning rate: 0.00040974]
	Learning Rate: 0.000409744
	LOSS [training: 0.6959764223003112 | validation: 0.43570444509417117]
	TIME [epoch: 6.47 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7017631643192956		[learning rate: 0.00040878]
	Learning Rate: 0.000408778
	LOSS [training: 0.7017631643192956 | validation: 0.4406692215779046]
	TIME [epoch: 6.48 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6995228163695941		[learning rate: 0.00040781]
	Learning Rate: 0.000407813
	LOSS [training: 0.6995228163695941 | validation: 0.4349420667796778]
	TIME [epoch: 6.5 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7007016280392107		[learning rate: 0.00040685]
	Learning Rate: 0.000406851
	LOSS [training: 0.7007016280392107 | validation: 0.441581357878917]
	TIME [epoch: 6.5 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6993375170035336		[learning rate: 0.00040589]
	Learning Rate: 0.000405892
	LOSS [training: 0.6993375170035336 | validation: 0.439017083295695]
	TIME [epoch: 6.53 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6982263226884058		[learning rate: 0.00040493]
	Learning Rate: 0.000404934
	LOSS [training: 0.6982263226884058 | validation: 0.4301011195157457]
	TIME [epoch: 6.49 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6967078099955433		[learning rate: 0.00040398]
	Learning Rate: 0.000403979
	LOSS [training: 0.6967078099955433 | validation: 0.43509816883916486]
	TIME [epoch: 6.49 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6930223547006295		[learning rate: 0.00040303]
	Learning Rate: 0.000403026
	LOSS [training: 0.6930223547006295 | validation: 0.4459921749177942]
	TIME [epoch: 6.49 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6978316803711202		[learning rate: 0.00040208]
	Learning Rate: 0.000402076
	LOSS [training: 0.6978316803711202 | validation: 0.43097128708459514]
	TIME [epoch: 6.49 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6994193554610163		[learning rate: 0.00040113]
	Learning Rate: 0.000401127
	LOSS [training: 0.6994193554610163 | validation: 0.43899834758484424]
	TIME [epoch: 6.5 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6976249157799564		[learning rate: 0.00040018]
	Learning Rate: 0.000400181
	LOSS [training: 0.6976249157799564 | validation: 0.4348395791065646]
	TIME [epoch: 6.53 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6949231207009007		[learning rate: 0.00039924]
	Learning Rate: 0.000399237
	LOSS [training: 0.6949231207009007 | validation: 0.43146989220029336]
	TIME [epoch: 6.49 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.699621539535038		[learning rate: 0.0003983]
	Learning Rate: 0.000398295
	LOSS [training: 0.699621539535038 | validation: 0.4418695111245647]
	TIME [epoch: 6.49 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6969518469371596		[learning rate: 0.00039736]
	Learning Rate: 0.000397356
	LOSS [training: 0.6969518469371596 | validation: 0.43130766614819604]
	TIME [epoch: 6.48 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6942542933977427		[learning rate: 0.00039642]
	Learning Rate: 0.000396418
	LOSS [training: 0.6942542933977427 | validation: 0.4420855691991414]
	TIME [epoch: 6.49 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7005050393302451		[learning rate: 0.00039548]
	Learning Rate: 0.000395483
	LOSS [training: 0.7005050393302451 | validation: 0.4407445764601785]
	TIME [epoch: 6.53 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6999933839340373		[learning rate: 0.00039455]
	Learning Rate: 0.00039455
	LOSS [training: 0.6999933839340373 | validation: 0.4367095988220888]
	TIME [epoch: 6.51 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6961414491063476		[learning rate: 0.00039362]
	Learning Rate: 0.00039362
	LOSS [training: 0.6961414491063476 | validation: 0.43751983751035883]
	TIME [epoch: 6.5 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6914773524941276		[learning rate: 0.00039269]
	Learning Rate: 0.000392691
	LOSS [training: 0.6914773524941276 | validation: 0.4435838700946758]
	TIME [epoch: 6.5 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.69672979915654		[learning rate: 0.00039177]
	Learning Rate: 0.000391765
	LOSS [training: 0.69672979915654 | validation: 0.43176547548829686]
	TIME [epoch: 6.5 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934714641054898		[learning rate: 0.00039084]
	Learning Rate: 0.000390841
	LOSS [training: 0.6934714641054898 | validation: 0.4378398908484581]
	TIME [epoch: 6.5 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6955855832428111		[learning rate: 0.00038992]
	Learning Rate: 0.000389919
	LOSS [training: 0.6955855832428111 | validation: 0.4338254630118471]
	TIME [epoch: 6.53 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.692355592846212		[learning rate: 0.000389]
	Learning Rate: 0.000388999
	LOSS [training: 0.692355592846212 | validation: 0.4388446544722109]
	TIME [epoch: 6.49 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6942318654429276		[learning rate: 0.00038808]
	Learning Rate: 0.000388082
	LOSS [training: 0.6942318654429276 | validation: 0.42765239627683116]
	TIME [epoch: 6.48 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7003147931754992		[learning rate: 0.00038717]
	Learning Rate: 0.000387166
	LOSS [training: 0.7003147931754992 | validation: 0.4457174639648281]
	TIME [epoch: 6.5 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7077534027009097		[learning rate: 0.00038625]
	Learning Rate: 0.000386253
	LOSS [training: 0.7077534027009097 | validation: 0.4442354851341002]
	TIME [epoch: 6.5 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7003667326001304		[learning rate: 0.00038534]
	Learning Rate: 0.000385342
	LOSS [training: 0.7003667326001304 | validation: 0.4364143483012248]
	TIME [epoch: 6.49 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6993108325893822		[learning rate: 0.00038443]
	Learning Rate: 0.000384433
	LOSS [training: 0.6993108325893822 | validation: 0.4299518863048949]
	TIME [epoch: 6.53 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6918254088647332		[learning rate: 0.00038353]
	Learning Rate: 0.000383526
	LOSS [training: 0.6918254088647332 | validation: 0.4401418529121012]
	TIME [epoch: 6.49 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6958694317753429		[learning rate: 0.00038262]
	Learning Rate: 0.000382621
	LOSS [training: 0.6958694317753429 | validation: 0.4350361573763377]
	TIME [epoch: 6.49 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694172568775237		[learning rate: 0.00038172]
	Learning Rate: 0.000381719
	LOSS [training: 0.694172568775237 | validation: 0.43312957613191966]
	TIME [epoch: 6.5 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6948317105080328		[learning rate: 0.00038082]
	Learning Rate: 0.000380818
	LOSS [training: 0.6948317105080328 | validation: 0.43913223949079994]
	TIME [epoch: 6.48 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6941698479703121		[learning rate: 0.00037992]
	Learning Rate: 0.00037992
	LOSS [training: 0.6941698479703121 | validation: 0.42750753544815545]
	TIME [epoch: 6.52 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6941337334165479		[learning rate: 0.00037902]
	Learning Rate: 0.000379024
	LOSS [training: 0.6941337334165479 | validation: 0.4389738831639386]
	TIME [epoch: 6.51 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6948915947238852		[learning rate: 0.00037813]
	Learning Rate: 0.00037813
	LOSS [training: 0.6948915947238852 | validation: 0.4301583149821899]
	TIME [epoch: 6.49 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933352748007074		[learning rate: 0.00037724]
	Learning Rate: 0.000377238
	LOSS [training: 0.6933352748007074 | validation: 0.44033364461409735]
	TIME [epoch: 6.5 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6963559862897858		[learning rate: 0.00037635]
	Learning Rate: 0.000376348
	LOSS [training: 0.6963559862897858 | validation: 0.4394479395668489]
	TIME [epoch: 6.48 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6997654236239279		[learning rate: 0.00037546]
	Learning Rate: 0.00037546
	LOSS [training: 0.6997654236239279 | validation: 0.43496460211889354]
	TIME [epoch: 6.47 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6997334870931479		[learning rate: 0.00037457]
	Learning Rate: 0.000374575
	LOSS [training: 0.6997334870931479 | validation: 0.43932797986647365]
	TIME [epoch: 6.52 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6971155756309111		[learning rate: 0.00037369]
	Learning Rate: 0.000373691
	LOSS [training: 0.6971155756309111 | validation: 0.4361491533306995]
	TIME [epoch: 6.5 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6976219601366411		[learning rate: 0.00037281]
	Learning Rate: 0.00037281
	LOSS [training: 0.6976219601366411 | validation: 0.437141276333178]
	TIME [epoch: 6.49 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6938002027601197		[learning rate: 0.00037193]
	Learning Rate: 0.00037193
	LOSS [training: 0.6938002027601197 | validation: 0.4375887185808513]
	TIME [epoch: 6.47 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6979292605457503		[learning rate: 0.00037105]
	Learning Rate: 0.000371053
	LOSS [training: 0.6979292605457503 | validation: 0.4341675340527442]
	TIME [epoch: 6.5 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6968409313009262		[learning rate: 0.00037018]
	Learning Rate: 0.000370178
	LOSS [training: 0.6968409313009262 | validation: 0.44172386456162727]
	TIME [epoch: 6.5 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.702171957848418		[learning rate: 0.0003693]
	Learning Rate: 0.000369305
	LOSS [training: 0.702171957848418 | validation: 0.44461918950691204]
	TIME [epoch: 6.54 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7018591142659014		[learning rate: 0.00036843]
	Learning Rate: 0.000368433
	LOSS [training: 0.7018591142659014 | validation: 0.43551081062982605]
	TIME [epoch: 6.5 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6961876114592801		[learning rate: 0.00036756]
	Learning Rate: 0.000367564
	LOSS [training: 0.6961876114592801 | validation: 0.43606248648626916]
	TIME [epoch: 6.5 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6951174995195619		[learning rate: 0.0003667]
	Learning Rate: 0.000366697
	LOSS [training: 0.6951174995195619 | validation: 0.4346574642670066]
	TIME [epoch: 6.48 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6943714911517824		[learning rate: 0.00036583]
	Learning Rate: 0.000365832
	LOSS [training: 0.6943714911517824 | validation: 0.44297159345037784]
	TIME [epoch: 6.49 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6973472470197997		[learning rate: 0.00036497]
	Learning Rate: 0.000364969
	LOSS [training: 0.6973472470197997 | validation: 0.441568278931475]
	TIME [epoch: 6.49 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7010400672822393		[learning rate: 0.00036411]
	Learning Rate: 0.000364108
	LOSS [training: 0.7010400672822393 | validation: 0.4457046160089039]
	TIME [epoch: 6.53 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.700632098254754		[learning rate: 0.00036325]
	Learning Rate: 0.00036325
	LOSS [training: 0.700632098254754 | validation: 0.4380809288931622]
	TIME [epoch: 6.5 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6954059209203496		[learning rate: 0.00036239]
	Learning Rate: 0.000362393
	LOSS [training: 0.6954059209203496 | validation: 0.432967172380657]
	TIME [epoch: 6.5 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6983343118264289		[learning rate: 0.00036154]
	Learning Rate: 0.000361538
	LOSS [training: 0.6983343118264289 | validation: 0.4381245950234659]
	TIME [epoch: 6.49 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6942383432707204		[learning rate: 0.00036069]
	Learning Rate: 0.000360685
	LOSS [training: 0.6942383432707204 | validation: 0.4254375045638624]
	TIME [epoch: 6.49 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6964025383043391		[learning rate: 0.00035983]
	Learning Rate: 0.000359834
	LOSS [training: 0.6964025383043391 | validation: 0.42211262840405234]
	TIME [epoch: 6.51 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6919849898241215		[learning rate: 0.00035899]
	Learning Rate: 0.000358986
	LOSS [training: 0.6919849898241215 | validation: 0.4351191082442491]
	TIME [epoch: 6.52 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6975092545887545		[learning rate: 0.00035814]
	Learning Rate: 0.000358139
	LOSS [training: 0.6975092545887545 | validation: 0.4382321019021648]
	TIME [epoch: 6.49 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6927873088698121		[learning rate: 0.00035729]
	Learning Rate: 0.000357294
	LOSS [training: 0.6927873088698121 | validation: 0.43394008178355403]
	TIME [epoch: 6.49 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6967604842582332		[learning rate: 0.00035645]
	Learning Rate: 0.000356451
	LOSS [training: 0.6967604842582332 | validation: 0.4360464013422747]
	TIME [epoch: 6.48 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6957125117940548		[learning rate: 0.00035561]
	Learning Rate: 0.00035561
	LOSS [training: 0.6957125117940548 | validation: 0.43526220597211573]
	TIME [epoch: 6.47 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6938161871461515		[learning rate: 0.00035477]
	Learning Rate: 0.000354771
	LOSS [training: 0.6938161871461515 | validation: 0.4292901205627451]
	TIME [epoch: 6.51 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6896900651232739		[learning rate: 0.00035393]
	Learning Rate: 0.000353935
	LOSS [training: 0.6896900651232739 | validation: 0.42669708638784204]
	TIME [epoch: 6.51 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6932442370601698		[learning rate: 0.0003531]
	Learning Rate: 0.0003531
	LOSS [training: 0.6932442370601698 | validation: 0.4396554201699686]
	TIME [epoch: 6.48 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934941546769442		[learning rate: 0.00035227]
	Learning Rate: 0.000352267
	LOSS [training: 0.6934941546769442 | validation: 0.42780437952097305]
	TIME [epoch: 6.47 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7010902382441146		[learning rate: 0.00035144]
	Learning Rate: 0.000351436
	LOSS [training: 0.7010902382441146 | validation: 0.4349195401389857]
	TIME [epoch: 6.49 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7000874598206566		[learning rate: 0.00035061]
	Learning Rate: 0.000350607
	LOSS [training: 0.7000874598206566 | validation: 0.43032354854803007]
	TIME [epoch: 6.49 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6970897550446049		[learning rate: 0.00034978]
	Learning Rate: 0.00034978
	LOSS [training: 0.6970897550446049 | validation: 0.43166257144060904]
	TIME [epoch: 6.52 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6969118583805229		[learning rate: 0.00034895]
	Learning Rate: 0.000348955
	LOSS [training: 0.6969118583805229 | validation: 0.4271434487698819]
	TIME [epoch: 6.5 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6942366565716693		[learning rate: 0.00034813]
	Learning Rate: 0.000348132
	LOSS [training: 0.6942366565716693 | validation: 0.4251892384387007]
	TIME [epoch: 6.49 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6958270077308942		[learning rate: 0.00034731]
	Learning Rate: 0.000347311
	LOSS [training: 0.6958270077308942 | validation: 0.4416858433650192]
	TIME [epoch: 6.49 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7029080584819088		[learning rate: 0.00034649]
	Learning Rate: 0.000346491
	LOSS [training: 0.7029080584819088 | validation: 0.43021190793650493]
	TIME [epoch: 6.48 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6962866588070791		[learning rate: 0.00034567]
	Learning Rate: 0.000345674
	LOSS [training: 0.6962866588070791 | validation: 0.4374112683720671]
	TIME [epoch: 6.5 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6931850626040781		[learning rate: 0.00034486]
	Learning Rate: 0.000344859
	LOSS [training: 0.6931850626040781 | validation: 0.43479473438065075]
	TIME [epoch: 6.52 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.692877286167762		[learning rate: 0.00034405]
	Learning Rate: 0.000344045
	LOSS [training: 0.692877286167762 | validation: 0.43379882445521506]
	TIME [epoch: 6.5 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6947559400231929		[learning rate: 0.00034323]
	Learning Rate: 0.000343233
	LOSS [training: 0.6947559400231929 | validation: 0.4295348599010394]
	TIME [epoch: 6.48 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6951999589798916		[learning rate: 0.00034242]
	Learning Rate: 0.000342424
	LOSS [training: 0.6951999589798916 | validation: 0.4303949658775872]
	TIME [epoch: 6.47 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6917580427635184		[learning rate: 0.00034162]
	Learning Rate: 0.000341616
	LOSS [training: 0.6917580427635184 | validation: 0.4395876229344748]
	TIME [epoch: 6.49 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6980869378631687		[learning rate: 0.00034081]
	Learning Rate: 0.00034081
	LOSS [training: 0.6980869378631687 | validation: 0.4361502449277637]
	TIME [epoch: 6.51 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6958994993557429		[learning rate: 0.00034001]
	Learning Rate: 0.000340006
	LOSS [training: 0.6958994993557429 | validation: 0.4432627867909332]
	TIME [epoch: 6.51 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7015232054764023		[learning rate: 0.0003392]
	Learning Rate: 0.000339204
	LOSS [training: 0.7015232054764023 | validation: 0.44633415662098364]
	TIME [epoch: 6.46 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7010035533243372		[learning rate: 0.0003384]
	Learning Rate: 0.000338404
	LOSS [training: 0.7010035533243372 | validation: 0.42626871506587966]
	TIME [epoch: 6.49 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.696053161733755		[learning rate: 0.00033761]
	Learning Rate: 0.000337606
	LOSS [training: 0.696053161733755 | validation: 0.43368162732965276]
	TIME [epoch: 6.47 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7000685430118858		[learning rate: 0.00033681]
	Learning Rate: 0.00033681
	LOSS [training: 0.7000685430118858 | validation: 0.43438008999319483]
	TIME [epoch: 6.49 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6979280978253188		[learning rate: 0.00033602]
	Learning Rate: 0.000336015
	LOSS [training: 0.6979280978253188 | validation: 0.43411229698306264]
	TIME [epoch: 6.53 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7093090812375793		[learning rate: 0.00033522]
	Learning Rate: 0.000335223
	LOSS [training: 0.7093090812375793 | validation: 0.4513239370399181]
	TIME [epoch: 6.49 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7130605556619951		[learning rate: 0.00033443]
	Learning Rate: 0.000334432
	LOSS [training: 0.7130605556619951 | validation: 0.4489752918461531]
	TIME [epoch: 6.49 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7010511714982268		[learning rate: 0.00033364]
	Learning Rate: 0.000333643
	LOSS [training: 0.7010511714982268 | validation: 0.4363864468293631]
	TIME [epoch: 6.49 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6966115943136071		[learning rate: 0.00033286]
	Learning Rate: 0.000332856
	LOSS [training: 0.6966115943136071 | validation: 0.4390292501659185]
	TIME [epoch: 6.47 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6928640486007162		[learning rate: 0.00033207]
	Learning Rate: 0.000332071
	LOSS [training: 0.6928640486007162 | validation: 0.4425828313106109]
	TIME [epoch: 6.49 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6991018860393574		[learning rate: 0.00033129]
	Learning Rate: 0.000331288
	LOSS [training: 0.6991018860393574 | validation: 0.42948311052114546]
	TIME [epoch: 6.52 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6960952617105092		[learning rate: 0.00033051]
	Learning Rate: 0.000330506
	LOSS [training: 0.6960952617105092 | validation: 0.4381909380907763]
	TIME [epoch: 6.49 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6964164995179931		[learning rate: 0.00032973]
	Learning Rate: 0.000329726
	LOSS [training: 0.6964164995179931 | validation: 0.42729858482083605]
	TIME [epoch: 6.48 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6930326457200886		[learning rate: 0.00032895]
	Learning Rate: 0.000328949
	LOSS [training: 0.6930326457200886 | validation: 0.4335994623718462]
	TIME [epoch: 6.47 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6973572249708047		[learning rate: 0.00032817]
	Learning Rate: 0.000328173
	LOSS [training: 0.6973572249708047 | validation: 0.4366966030266577]
	TIME [epoch: 6.49 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6958305470918342		[learning rate: 0.0003274]
	Learning Rate: 0.000327399
	LOSS [training: 0.6958305470918342 | validation: 0.4298059901760482]
	TIME [epoch: 6.51 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6950437479052319		[learning rate: 0.00032663]
	Learning Rate: 0.000326626
	LOSS [training: 0.6950437479052319 | validation: 0.43627608483314884]
	TIME [epoch: 6.53 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.692616355341159		[learning rate: 0.00032586]
	Learning Rate: 0.000325856
	LOSS [training: 0.692616355341159 | validation: 0.43752389348241655]
	TIME [epoch: 6.49 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6940475016214546		[learning rate: 0.00032509]
	Learning Rate: 0.000325087
	LOSS [training: 0.6940475016214546 | validation: 0.44033268870491393]
	TIME [epoch: 6.49 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6998334232892196		[learning rate: 0.00032432]
	Learning Rate: 0.00032432
	LOSS [training: 0.6998334232892196 | validation: 0.43286288023701214]
	TIME [epoch: 6.47 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6942614844605948		[learning rate: 0.00032356]
	Learning Rate: 0.000323555
	LOSS [training: 0.6942614844605948 | validation: 0.43447414119409683]
	TIME [epoch: 6.49 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6932939631469688		[learning rate: 0.00032279]
	Learning Rate: 0.000322792
	LOSS [training: 0.6932939631469688 | validation: 0.4278648961075361]
	TIME [epoch: 6.5 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6907794162026475		[learning rate: 0.00032203]
	Learning Rate: 0.000322031
	LOSS [training: 0.6907794162026475 | validation: 0.43938273145335793]
	TIME [epoch: 6.5 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6923523602547611		[learning rate: 0.00032127]
	Learning Rate: 0.000321271
	LOSS [training: 0.6923523602547611 | validation: 0.4336395735091497]
	TIME [epoch: 6.47 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6923421570976009		[learning rate: 0.00032051]
	Learning Rate: 0.000320513
	LOSS [training: 0.6923421570976009 | validation: 0.4337797024112789]
	TIME [epoch: 6.46 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6947398811329499		[learning rate: 0.00031976]
	Learning Rate: 0.000319757
	LOSS [training: 0.6947398811329499 | validation: 0.43357954814063476]
	TIME [epoch: 6.49 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6931796591336786		[learning rate: 0.000319]
	Learning Rate: 0.000319003
	LOSS [training: 0.6931796591336786 | validation: 0.4260945838615505]
	TIME [epoch: 6.49 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6918224389217046		[learning rate: 0.00031825]
	Learning Rate: 0.000318251
	LOSS [training: 0.6918224389217046 | validation: 0.4337461716814772]
	TIME [epoch: 6.53 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.691146210311316		[learning rate: 0.0003175]
	Learning Rate: 0.0003175
	LOSS [training: 0.691146210311316 | validation: 0.42652791120209366]
	TIME [epoch: 6.48 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6912987586155741		[learning rate: 0.00031675]
	Learning Rate: 0.000316751
	LOSS [training: 0.6912987586155741 | validation: 0.434288784063694]
	TIME [epoch: 6.47 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6976271634201655		[learning rate: 0.000316]
	Learning Rate: 0.000316004
	LOSS [training: 0.6976271634201655 | validation: 0.43130268554353113]
	TIME [epoch: 6.48 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6935276274799915		[learning rate: 0.00031526]
	Learning Rate: 0.000315258
	LOSS [training: 0.6935276274799915 | validation: 0.4342631995001285]
	TIME [epoch: 6.46 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6970739332559933		[learning rate: 0.00031451]
	Learning Rate: 0.000314515
	LOSS [training: 0.6970739332559933 | validation: 0.43421843328238574]
	TIME [epoch: 6.48 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6908455072871784		[learning rate: 0.00031377]
	Learning Rate: 0.000313773
	LOSS [training: 0.6908455072871784 | validation: 0.4351925632154374]
	TIME [epoch: 6.54 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6941143650426221		[learning rate: 0.00031303]
	Learning Rate: 0.000313033
	LOSS [training: 0.6941143650426221 | validation: 0.4299518900359773]
	TIME [epoch: 6.48 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6936232089574331		[learning rate: 0.00031229]
	Learning Rate: 0.000312294
	LOSS [training: 0.6936232089574331 | validation: 0.4350955056562029]
	TIME [epoch: 6.49 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6966968359175368		[learning rate: 0.00031156]
	Learning Rate: 0.000311558
	LOSS [training: 0.6966968359175368 | validation: 0.42552879476962174]
	TIME [epoch: 6.48 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6937031961686527		[learning rate: 0.00031082]
	Learning Rate: 0.000310823
	LOSS [training: 0.6937031961686527 | validation: 0.4392067956940313]
	TIME [epoch: 6.49 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6948205233248311		[learning rate: 0.00031009]
	Learning Rate: 0.00031009
	LOSS [training: 0.6948205233248311 | validation: 0.4298645750226634]
	TIME [epoch: 6.5 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6927408822224331		[learning rate: 0.00030936]
	Learning Rate: 0.000309358
	LOSS [training: 0.6927408822224331 | validation: 0.4291873402199436]
	TIME [epoch: 6.53 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6941470576878833		[learning rate: 0.00030863]
	Learning Rate: 0.000308628
	LOSS [training: 0.6941470576878833 | validation: 0.4283406454031702]
	TIME [epoch: 6.5 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.692670076397695		[learning rate: 0.0003079]
	Learning Rate: 0.0003079
	LOSS [training: 0.692670076397695 | validation: 0.4319381988468775]
	TIME [epoch: 6.49 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6957032936420594		[learning rate: 0.00030717]
	Learning Rate: 0.000307174
	LOSS [training: 0.6957032936420594 | validation: 0.4371863471058297]
	TIME [epoch: 6.47 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6977778262773038		[learning rate: 0.00030645]
	Learning Rate: 0.00030645
	LOSS [training: 0.6977778262773038 | validation: 0.4315344575554301]
	TIME [epoch: 6.47 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6936169672503996		[learning rate: 0.00030573]
	Learning Rate: 0.000305727
	LOSS [training: 0.6936169672503996 | validation: 0.42700263934156096]
	TIME [epoch: 6.5 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6955961976511337		[learning rate: 0.00030501]
	Learning Rate: 0.000305005
	LOSS [training: 0.6955961976511337 | validation: 0.42428739312346136]
	TIME [epoch: 6.51 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6946127475429419		[learning rate: 0.00030429]
	Learning Rate: 0.000304286
	LOSS [training: 0.6946127475429419 | validation: 0.4366282774663765]
	TIME [epoch: 6.49 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6895478717640164		[learning rate: 0.00030357]
	Learning Rate: 0.000303568
	LOSS [training: 0.6895478717640164 | validation: 0.42772028847067356]
	TIME [epoch: 6.47 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.69314732738874		[learning rate: 0.00030285]
	Learning Rate: 0.000302852
	LOSS [training: 0.69314732738874 | validation: 0.42692410074964704]
	TIME [epoch: 6.49 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6915085580804774		[learning rate: 0.00030214]
	Learning Rate: 0.000302138
	LOSS [training: 0.6915085580804774 | validation: 0.43509181640379085]
	TIME [epoch: 6.47 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6895585694306569		[learning rate: 0.00030143]
	Learning Rate: 0.000301425
	LOSS [training: 0.6895585694306569 | validation: 0.4247462333448564]
	TIME [epoch: 6.51 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6957447230356341		[learning rate: 0.00030071]
	Learning Rate: 0.000300714
	LOSS [training: 0.6957447230356341 | validation: 0.43569660370372265]
	TIME [epoch: 6.49 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.693409773277039		[learning rate: 0.0003]
	Learning Rate: 0.000300005
	LOSS [training: 0.693409773277039 | validation: 0.427239766752799]
	TIME [epoch: 6.47 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.693815414443802		[learning rate: 0.0002993]
	Learning Rate: 0.000299297
	LOSS [training: 0.693815414443802 | validation: 0.4365711449410131]
	TIME [epoch: 6.47 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6926611639082416		[learning rate: 0.00029859]
	Learning Rate: 0.000298591
	LOSS [training: 0.6926611639082416 | validation: 0.4361558202899215]
	TIME [epoch: 6.47 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6936417702933302		[learning rate: 0.00029789]
	Learning Rate: 0.000297887
	LOSS [training: 0.6936417702933302 | validation: 0.427390650291344]
	TIME [epoch: 6.48 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6949549011971615		[learning rate: 0.00029718]
	Learning Rate: 0.000297184
	LOSS [training: 0.6949549011971615 | validation: 0.4345830655354864]
	TIME [epoch: 6.52 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6944946102940938		[learning rate: 0.00029648]
	Learning Rate: 0.000296483
	LOSS [training: 0.6944946102940938 | validation: 0.439285951011571]
	TIME [epoch: 6.48 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6938405764855539		[learning rate: 0.00029578]
	Learning Rate: 0.000295784
	LOSS [training: 0.6938405764855539 | validation: 0.4297008406637366]
	TIME [epoch: 6.5 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6935987898359841		[learning rate: 0.00029509]
	Learning Rate: 0.000295086
	LOSS [training: 0.6935987898359841 | validation: 0.4247885753389219]
	TIME [epoch: 6.47 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6925355778384032		[learning rate: 0.00029439]
	Learning Rate: 0.00029439
	LOSS [training: 0.6925355778384032 | validation: 0.43519042416102116]
	TIME [epoch: 6.49 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6928179176359548		[learning rate: 0.0002937]
	Learning Rate: 0.000293696
	LOSS [training: 0.6928179176359548 | validation: 0.43731827431984327]
	TIME [epoch: 6.5 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6905029090670486		[learning rate: 0.000293]
	Learning Rate: 0.000293003
	LOSS [training: 0.6905029090670486 | validation: 0.432331438012782]
	TIME [epoch: 6.53 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6913137769506783		[learning rate: 0.00029231]
	Learning Rate: 0.000292312
	LOSS [training: 0.6913137769506783 | validation: 0.4364461642198938]
	TIME [epoch: 6.48 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6958331982189819		[learning rate: 0.00029162]
	Learning Rate: 0.000291622
	LOSS [training: 0.6958331982189819 | validation: 0.43230965347040506]
	TIME [epoch: 6.47 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6940469669631141		[learning rate: 0.00029093]
	Learning Rate: 0.000290934
	LOSS [training: 0.6940469669631141 | validation: 0.43153043158667526]
	TIME [epoch: 6.47 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.690799401832314		[learning rate: 0.00029025]
	Learning Rate: 0.000290248
	LOSS [training: 0.690799401832314 | validation: 0.43196227651788527]
	TIME [epoch: 6.47 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694921171974991		[learning rate: 0.00028956]
	Learning Rate: 0.000289563
	LOSS [training: 0.694921171974991 | validation: 0.4346631095272603]
	TIME [epoch: 6.49 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6930586192616506		[learning rate: 0.00028888]
	Learning Rate: 0.00028888
	LOSS [training: 0.6930586192616506 | validation: 0.4339425997776192]
	TIME [epoch: 6.5 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6944839372300018		[learning rate: 0.0002882]
	Learning Rate: 0.000288199
	LOSS [training: 0.6944839372300018 | validation: 0.44147956275754896]
	TIME [epoch: 6.48 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6936920298486757		[learning rate: 0.00028752]
	Learning Rate: 0.000287519
	LOSS [training: 0.6936920298486757 | validation: 0.4331178136172088]
	TIME [epoch: 6.47 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6939845674722898		[learning rate: 0.00028684]
	Learning Rate: 0.000286841
	LOSS [training: 0.6939845674722898 | validation: 0.42465596534076944]
	TIME [epoch: 6.47 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6950734936293094		[learning rate: 0.00028616]
	Learning Rate: 0.000286164
	LOSS [training: 0.6950734936293094 | validation: 0.4381109435043046]
	TIME [epoch: 6.47 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.698952398410947		[learning rate: 0.00028549]
	Learning Rate: 0.000285489
	LOSS [training: 0.698952398410947 | validation: 0.43738616146778203]
	TIME [epoch: 6.51 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6955901748368969		[learning rate: 0.00028482]
	Learning Rate: 0.000284816
	LOSS [training: 0.6955901748368969 | validation: 0.43348366872855365]
	TIME [epoch: 6.51 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6943858385648852		[learning rate: 0.00028414]
	Learning Rate: 0.000284144
	LOSS [training: 0.6943858385648852 | validation: 0.4363865709102523]
	TIME [epoch: 6.47 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6950996348531049		[learning rate: 0.00028347]
	Learning Rate: 0.000283474
	LOSS [training: 0.6950996348531049 | validation: 0.438278705101977]
	TIME [epoch: 6.47 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6952945721269572		[learning rate: 0.00028281]
	Learning Rate: 0.000282805
	LOSS [training: 0.6952945721269572 | validation: 0.43220012196020297]
	TIME [epoch: 6.47 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7010109685527472		[learning rate: 0.00028214]
	Learning Rate: 0.000282138
	LOSS [training: 0.7010109685527472 | validation: 0.4384175460943554]
	TIME [epoch: 6.47 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6973273428415876		[learning rate: 0.00028147]
	Learning Rate: 0.000281472
	LOSS [training: 0.6973273428415876 | validation: 0.43675360002783337]
	TIME [epoch: 6.52 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.698402594518468		[learning rate: 0.00028081]
	Learning Rate: 0.000280808
	LOSS [training: 0.698402594518468 | validation: 0.42873700086942734]
	TIME [epoch: 6.48 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6925031825761672		[learning rate: 0.00028015]
	Learning Rate: 0.000280146
	LOSS [training: 0.6925031825761672 | validation: 0.4377065430150908]
	TIME [epoch: 6.47 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6994184196399598		[learning rate: 0.00027949]
	Learning Rate: 0.000279485
	LOSS [training: 0.6994184196399598 | validation: 0.4459622410238302]
	TIME [epoch: 6.47 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6958104765462741		[learning rate: 0.00027883]
	Learning Rate: 0.000278826
	LOSS [training: 0.6958104765462741 | validation: 0.4289164236064451]
	TIME [epoch: 6.47 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6910035808665309		[learning rate: 0.00027817]
	Learning Rate: 0.000278168
	LOSS [training: 0.6910035808665309 | validation: 0.4335881379881803]
	TIME [epoch: 6.49 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6948036788097877		[learning rate: 0.00027751]
	Learning Rate: 0.000277512
	LOSS [training: 0.6948036788097877 | validation: 0.4290807002798755]
	TIME [epoch: 6.54 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6952535339066978		[learning rate: 0.00027686]
	Learning Rate: 0.000276858
	LOSS [training: 0.6952535339066978 | validation: 0.4364243782143962]
	TIME [epoch: 6.5 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6950612014319076		[learning rate: 0.0002762]
	Learning Rate: 0.000276204
	LOSS [training: 0.6950612014319076 | validation: 0.42827013304434014]
	TIME [epoch: 6.49 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6967373322496032		[learning rate: 0.00027555]
	Learning Rate: 0.000275553
	LOSS [training: 0.6967373322496032 | validation: 0.43785264604404184]
	TIME [epoch: 6.47 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6973645353650193		[learning rate: 0.0002749]
	Learning Rate: 0.000274903
	LOSS [training: 0.6973645353650193 | validation: 0.43611944227947724]
	TIME [epoch: 6.47 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6917343426032259		[learning rate: 0.00027425]
	Learning Rate: 0.000274255
	LOSS [training: 0.6917343426032259 | validation: 0.43299331313718986]
	TIME [epoch: 6.49 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6928943297628398		[learning rate: 0.00027361]
	Learning Rate: 0.000273608
	LOSS [training: 0.6928943297628398 | validation: 0.43261365373205773]
	TIME [epoch: 6.53 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6972396753349839		[learning rate: 0.00027296]
	Learning Rate: 0.000272962
	LOSS [training: 0.6972396753349839 | validation: 0.4324218945460422]
	TIME [epoch: 6.5 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.699787458656456		[learning rate: 0.00027232]
	Learning Rate: 0.000272318
	LOSS [training: 0.699787458656456 | validation: 0.44065924970655773]
	TIME [epoch: 6.48 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6980577393644576		[learning rate: 0.00027168]
	Learning Rate: 0.000271676
	LOSS [training: 0.6980577393644576 | validation: 0.428792314815028]
	TIME [epoch: 6.47 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7006201901213405		[learning rate: 0.00027104]
	Learning Rate: 0.000271035
	LOSS [training: 0.7006201901213405 | validation: 0.43617724568322136]
	TIME [epoch: 6.47 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6968629020048963		[learning rate: 0.0002704]
	Learning Rate: 0.000270396
	LOSS [training: 0.6968629020048963 | validation: 0.44190175551870553]
	TIME [epoch: 6.51 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.697218362826861		[learning rate: 0.00026976]
	Learning Rate: 0.000269758
	LOSS [training: 0.697218362826861 | validation: 0.43883438501333333]
	TIME [epoch: 6.49 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7018326476812863		[learning rate: 0.00026912]
	Learning Rate: 0.000269122
	LOSS [training: 0.7018326476812863 | validation: 0.43822721599679487]
	TIME [epoch: 6.48 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7008082712874437		[learning rate: 0.00026849]
	Learning Rate: 0.000268487
	LOSS [training: 0.7008082712874437 | validation: 0.4450910551587791]
	TIME [epoch: 6.48 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7016499099937012		[learning rate: 0.00026785]
	Learning Rate: 0.000267854
	LOSS [training: 0.7016499099937012 | validation: 0.4397025892681377]
	TIME [epoch: 6.48 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6997419589466001		[learning rate: 0.00026722]
	Learning Rate: 0.000267222
	LOSS [training: 0.6997419589466001 | validation: 0.43710231057622223]
	TIME [epoch: 6.48 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6981437270288258		[learning rate: 0.00026659]
	Learning Rate: 0.000266591
	LOSS [training: 0.6981437270288258 | validation: 0.44060420894822133]
	TIME [epoch: 6.52 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7030055749875646		[learning rate: 0.00026596]
	Learning Rate: 0.000265963
	LOSS [training: 0.7030055749875646 | validation: 0.44007829642613217]
	TIME [epoch: 6.47 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7020331394300843		[learning rate: 0.00026534]
	Learning Rate: 0.000265335
	LOSS [training: 0.7020331394300843 | validation: 0.44251127043124605]
	TIME [epoch: 6.48 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7033245109536661		[learning rate: 0.00026471]
	Learning Rate: 0.000264709
	LOSS [training: 0.7033245109536661 | validation: 0.44316472753205605]
	TIME [epoch: 6.47 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7059199301392609		[learning rate: 0.00026408]
	Learning Rate: 0.000264085
	LOSS [training: 0.7059199301392609 | validation: 0.44125759771882156]
	TIME [epoch: 6.48 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7051659394453216		[learning rate: 0.00026346]
	Learning Rate: 0.000263462
	LOSS [training: 0.7051659394453216 | validation: 0.4496402280074585]
	TIME [epoch: 6.48 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7053640332467951		[learning rate: 0.00026284]
	Learning Rate: 0.00026284
	LOSS [training: 0.7053640332467951 | validation: 0.44581969488764084]
	TIME [epoch: 6.51 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7043551330681802		[learning rate: 0.00026222]
	Learning Rate: 0.00026222
	LOSS [training: 0.7043551330681802 | validation: 0.4459063710968822]
	TIME [epoch: 6.47 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7018590809880042		[learning rate: 0.0002616]
	Learning Rate: 0.000261602
	LOSS [training: 0.7018590809880042 | validation: 0.4415304939601217]
	TIME [epoch: 6.48 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7045221076574356		[learning rate: 0.00026098]
	Learning Rate: 0.000260985
	LOSS [training: 0.7045221076574356 | validation: 0.43404735594046245]
	TIME [epoch: 6.47 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7022913267204105		[learning rate: 0.00026037]
	Learning Rate: 0.000260369
	LOSS [training: 0.7022913267204105 | validation: 0.4395524046495534]
	TIME [epoch: 6.48 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7023291871555541		[learning rate: 0.00025976]
	Learning Rate: 0.000259755
	LOSS [training: 0.7023291871555541 | validation: 0.4473278169740796]
	TIME [epoch: 6.52 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7081414631304849		[learning rate: 0.00025914]
	Learning Rate: 0.000259142
	LOSS [training: 0.7081414631304849 | validation: 0.44448007467016953]
	TIME [epoch: 6.51 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6991570649719702		[learning rate: 0.00025853]
	Learning Rate: 0.000258531
	LOSS [training: 0.6991570649719702 | validation: 0.4463958164184165]
	TIME [epoch: 6.48 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6981308637805487		[learning rate: 0.00025792]
	Learning Rate: 0.000257921
	LOSS [training: 0.6981308637805487 | validation: 0.42924571260516775]
	TIME [epoch: 6.48 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.699134620641651		[learning rate: 0.00025731]
	Learning Rate: 0.000257313
	LOSS [training: 0.699134620641651 | validation: 0.4330321125553443]
	TIME [epoch: 6.49 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7048205805542922		[learning rate: 0.00025671]
	Learning Rate: 0.000256706
	LOSS [training: 0.7048205805542922 | validation: 0.43472054074213295]
	TIME [epoch: 6.48 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7031711181341158		[learning rate: 0.0002561]
	Learning Rate: 0.0002561
	LOSS [training: 0.7031711181341158 | validation: 0.44097171351357517]
	TIME [epoch: 6.51 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7020725045521716		[learning rate: 0.0002555]
	Learning Rate: 0.000255496
	LOSS [training: 0.7020725045521716 | validation: 0.4400050851776506]
	TIME [epoch: 6.49 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7023088959339081		[learning rate: 0.00025489]
	Learning Rate: 0.000254894
	LOSS [training: 0.7023088959339081 | validation: 0.4423982030925387]
	TIME [epoch: 6.48 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.701554157154843		[learning rate: 0.00025429]
	Learning Rate: 0.000254292
	LOSS [training: 0.701554157154843 | validation: 0.44015924414303476]
	TIME [epoch: 6.5 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7016605508599041		[learning rate: 0.00025369]
	Learning Rate: 0.000253693
	LOSS [training: 0.7016605508599041 | validation: 0.4308573778150415]
	TIME [epoch: 6.49 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6998553405245942		[learning rate: 0.00025309]
	Learning Rate: 0.000253094
	LOSS [training: 0.6998553405245942 | validation: 0.4279296417722346]
	TIME [epoch: 6.49 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6993617929826271		[learning rate: 0.0002525]
	Learning Rate: 0.000252497
	LOSS [training: 0.6993617929826271 | validation: 0.4309909285504665]
	TIME [epoch: 6.54 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.698522112759159		[learning rate: 0.0002519]
	Learning Rate: 0.000251902
	LOSS [training: 0.698522112759159 | validation: 0.4275742739269726]
	TIME [epoch: 6.49 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6937842592977158		[learning rate: 0.00025131]
	Learning Rate: 0.000251307
	LOSS [training: 0.6937842592977158 | validation: 0.44545483131120717]
	TIME [epoch: 6.49 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6988481728529793		[learning rate: 0.00025071]
	Learning Rate: 0.000250714
	LOSS [training: 0.6988481728529793 | validation: 0.44046892190471654]
	TIME [epoch: 6.49 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7000926122939323		[learning rate: 0.00025012]
	Learning Rate: 0.000250123
	LOSS [training: 0.7000926122939323 | validation: 0.4359393873897395]
	TIME [epoch: 6.5 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694771092348432		[learning rate: 0.00024953]
	Learning Rate: 0.000249533
	LOSS [training: 0.694771092348432 | validation: 0.43207857512111325]
	TIME [epoch: 6.48 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6956199841511191		[learning rate: 0.00024894]
	Learning Rate: 0.000248945
	LOSS [training: 0.6956199841511191 | validation: 0.4361466023324858]
	TIME [epoch: 6.53 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6991750438732873		[learning rate: 0.00024836]
	Learning Rate: 0.000248357
	LOSS [training: 0.6991750438732873 | validation: 0.4340342549172264]
	TIME [epoch: 6.5 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6950425137822578		[learning rate: 0.00024777]
	Learning Rate: 0.000247771
	LOSS [training: 0.6950425137822578 | validation: 0.4349154247183928]
	TIME [epoch: 6.49 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6955580738956153		[learning rate: 0.00024719]
	Learning Rate: 0.000247187
	LOSS [training: 0.6955580738956153 | validation: 0.44018799782600593]
	TIME [epoch: 6.49 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6949001156027416		[learning rate: 0.0002466]
	Learning Rate: 0.000246604
	LOSS [training: 0.6949001156027416 | validation: 0.4362916652579825]
	TIME [epoch: 6.49 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6975514564043297		[learning rate: 0.00024602]
	Learning Rate: 0.000246022
	LOSS [training: 0.6975514564043297 | validation: 0.43981803699843064]
	TIME [epoch: 6.53 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6966130676268927		[learning rate: 0.00024544]
	Learning Rate: 0.000245442
	LOSS [training: 0.6966130676268927 | validation: 0.4326440044923885]
	TIME [epoch: 6.49 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.696609115421696		[learning rate: 0.00024486]
	Learning Rate: 0.000244863
	LOSS [training: 0.696609115421696 | validation: 0.43306512360705707]
	TIME [epoch: 6.47 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6959626310578597		[learning rate: 0.00024429]
	Learning Rate: 0.000244285
	LOSS [training: 0.6959626310578597 | validation: 0.4413357775463082]
	TIME [epoch: 6.48 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6958294790580085		[learning rate: 0.00024371]
	Learning Rate: 0.000243709
	LOSS [training: 0.6958294790580085 | validation: 0.4394672489008307]
	TIME [epoch: 6.49 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6957901252539506		[learning rate: 0.00024313]
	Learning Rate: 0.000243134
	LOSS [training: 0.6957901252539506 | validation: 0.434183534453047]
	TIME [epoch: 6.49 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6940875529033427		[learning rate: 0.00024256]
	Learning Rate: 0.000242561
	LOSS [training: 0.6940875529033427 | validation: 0.4279710743391948]
	TIME [epoch: 6.54 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6975231656304605		[learning rate: 0.00024199]
	Learning Rate: 0.000241989
	LOSS [training: 0.6975231656304605 | validation: 0.4383594296238218]
	TIME [epoch: 6.5 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.70197823734693		[learning rate: 0.00024142]
	Learning Rate: 0.000241418
	LOSS [training: 0.70197823734693 | validation: 0.4326083905307883]
	TIME [epoch: 6.48 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6979572491867316		[learning rate: 0.00024085]
	Learning Rate: 0.000240848
	LOSS [training: 0.6979572491867316 | validation: 0.43408342363802965]
	TIME [epoch: 6.49 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6960985442879711		[learning rate: 0.00024028]
	Learning Rate: 0.00024028
	LOSS [training: 0.6960985442879711 | validation: 0.43142497998264623]
	TIME [epoch: 6.49 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6958045811377983		[learning rate: 0.00023971]
	Learning Rate: 0.000239713
	LOSS [training: 0.6958045811377983 | validation: 0.4308541803282954]
	TIME [epoch: 6.49 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933298597800971		[learning rate: 0.00023915]
	Learning Rate: 0.000239148
	LOSS [training: 0.6933298597800971 | validation: 0.43832045396200225]
	TIME [epoch: 6.54 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.697443647413326		[learning rate: 0.00023858]
	Learning Rate: 0.000238584
	LOSS [training: 0.697443647413326 | validation: 0.4334383784006652]
	TIME [epoch: 6.48 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6925356791500766		[learning rate: 0.00023802]
	Learning Rate: 0.000238021
	LOSS [training: 0.6925356791500766 | validation: 0.4259698766082214]
	TIME [epoch: 6.5 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933563320523632		[learning rate: 0.00023746]
	Learning Rate: 0.00023746
	LOSS [training: 0.6933563320523632 | validation: 0.43472915563920944]
	TIME [epoch: 6.46 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6976277138419421		[learning rate: 0.0002369]
	Learning Rate: 0.000236899
	LOSS [training: 0.6976277138419421 | validation: 0.4316977895235206]
	TIME [epoch: 6.49 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6962302894487399		[learning rate: 0.00023634]
	Learning Rate: 0.000236341
	LOSS [training: 0.6962302894487399 | validation: 0.43327224293914535]
	TIME [epoch: 6.51 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6980556717584429		[learning rate: 0.00023578]
	Learning Rate: 0.000235783
	LOSS [training: 0.6980556717584429 | validation: 0.4307500414975977]
	TIME [epoch: 6.51 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6991492288834641		[learning rate: 0.00023523]
	Learning Rate: 0.000235227
	LOSS [training: 0.6991492288834641 | validation: 0.43379230292177884]
	TIME [epoch: 6.49 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6983186454366992		[learning rate: 0.00023467]
	Learning Rate: 0.000234672
	LOSS [training: 0.6983186454366992 | validation: 0.437914231614233]
	TIME [epoch: 6.49 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6976703200055644		[learning rate: 0.00023412]
	Learning Rate: 0.000234119
	LOSS [training: 0.6976703200055644 | validation: 0.4307652524810971]
	TIME [epoch: 6.49 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6993315158907866		[learning rate: 0.00023357]
	Learning Rate: 0.000233566
	LOSS [training: 0.6993315158907866 | validation: 0.4404554111858593]
	TIME [epoch: 6.47 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6987534317018702		[learning rate: 0.00023302]
	Learning Rate: 0.000233015
	LOSS [training: 0.6987534317018702 | validation: 0.42535193071360944]
	TIME [epoch: 6.53 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6971524704586061		[learning rate: 0.00023247]
	Learning Rate: 0.000232466
	LOSS [training: 0.6971524704586061 | validation: 0.4270488661824987]
	TIME [epoch: 6.51 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.699190295180437		[learning rate: 0.00023192]
	Learning Rate: 0.000231917
	LOSS [training: 0.699190295180437 | validation: 0.4333566602514034]
	TIME [epoch: 6.49 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6985755518956538		[learning rate: 0.00023137]
	Learning Rate: 0.00023137
	LOSS [training: 0.6985755518956538 | validation: 0.43068198031991767]
	TIME [epoch: 6.49 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7028696181501555		[learning rate: 0.00023082]
	Learning Rate: 0.000230825
	LOSS [training: 0.7028696181501555 | validation: 0.4380868976041242]
	TIME [epoch: 6.49 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694809642254375		[learning rate: 0.00023028]
	Learning Rate: 0.00023028
	LOSS [training: 0.694809642254375 | validation: 0.42787882425565343]
	TIME [epoch: 6.49 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6973540748783814		[learning rate: 0.00022974]
	Learning Rate: 0.000229737
	LOSS [training: 0.6973540748783814 | validation: 0.4414506604962532]
	TIME [epoch: 6.54 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694356828677987		[learning rate: 0.00022919]
	Learning Rate: 0.000229195
	LOSS [training: 0.694356828677987 | validation: 0.43337949506648865]
	TIME [epoch: 6.49 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6966787430037991		[learning rate: 0.00022865]
	Learning Rate: 0.000228654
	LOSS [training: 0.6966787430037991 | validation: 0.4374584298885183]
	TIME [epoch: 6.48 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6920101338700286		[learning rate: 0.00022811]
	Learning Rate: 0.000228115
	LOSS [training: 0.6920101338700286 | validation: 0.4295026356005379]
	TIME [epoch: 6.48 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6926083421606413		[learning rate: 0.00022758]
	Learning Rate: 0.000227577
	LOSS [training: 0.6926083421606413 | validation: 0.43139127816089173]
	TIME [epoch: 6.48 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6945049071893716		[learning rate: 0.00022704]
	Learning Rate: 0.00022704
	LOSS [training: 0.6945049071893716 | validation: 0.43407711231905866]
	TIME [epoch: 6.49 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6966767304239092		[learning rate: 0.0002265]
	Learning Rate: 0.000226505
	LOSS [training: 0.6966767304239092 | validation: 0.4218133877952598]
	TIME [epoch: 6.54 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6911265296832563		[learning rate: 0.00022597]
	Learning Rate: 0.00022597
	LOSS [training: 0.6911265296832563 | validation: 0.43618325389538787]
	TIME [epoch: 6.47 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6950516669462131		[learning rate: 0.00022544]
	Learning Rate: 0.000225437
	LOSS [training: 0.6950516669462131 | validation: 0.4307119186649273]
	TIME [epoch: 6.47 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6920448047541317		[learning rate: 0.00022491]
	Learning Rate: 0.000224905
	LOSS [training: 0.6920448047541317 | validation: 0.4342918617785142]
	TIME [epoch: 6.47 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6921260405584062		[learning rate: 0.00022437]
	Learning Rate: 0.000224375
	LOSS [training: 0.6921260405584062 | validation: 0.4308672742263087]
	TIME [epoch: 6.47 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6914287367198224		[learning rate: 0.00022385]
	Learning Rate: 0.000223846
	LOSS [training: 0.6914287367198224 | validation: 0.4291597789952649]
	TIME [epoch: 6.5 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6937277155704241		[learning rate: 0.00022332]
	Learning Rate: 0.000223318
	LOSS [training: 0.6937277155704241 | validation: 0.43003274150183174]
	TIME [epoch: 6.51 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6962345996526738		[learning rate: 0.00022279]
	Learning Rate: 0.000222791
	LOSS [training: 0.6962345996526738 | validation: 0.436021926720006]
	TIME [epoch: 6.47 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6928884753314232		[learning rate: 0.00022227]
	Learning Rate: 0.000222265
	LOSS [training: 0.6928884753314232 | validation: 0.42570981447418865]
	TIME [epoch: 6.49 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6957415714412807		[learning rate: 0.00022174]
	Learning Rate: 0.000221741
	LOSS [training: 0.6957415714412807 | validation: 0.4355930869646025]
	TIME [epoch: 6.5 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6936502912525404		[learning rate: 0.00022122]
	Learning Rate: 0.000221218
	LOSS [training: 0.6936502912525404 | validation: 0.4279885689097051]
	TIME [epoch: 6.46 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6936810975688761		[learning rate: 0.0002207]
	Learning Rate: 0.000220696
	LOSS [training: 0.6936810975688761 | validation: 0.4402576924827372]
	TIME [epoch: 6.52 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6974071661528304		[learning rate: 0.00022018]
	Learning Rate: 0.000220176
	LOSS [training: 0.6974071661528304 | validation: 0.43053304892216937]
	TIME [epoch: 6.5 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6958586623978538		[learning rate: 0.00021966]
	Learning Rate: 0.000219656
	LOSS [training: 0.6958586623978538 | validation: 0.43821336092359375]
	TIME [epoch: 6.49 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6954389529013802		[learning rate: 0.00021914]
	Learning Rate: 0.000219138
	LOSS [training: 0.6954389529013802 | validation: 0.4309337467899687]
	TIME [epoch: 6.48 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7002300098095058		[learning rate: 0.00021862]
	Learning Rate: 0.000218621
	LOSS [training: 0.7002300098095058 | validation: 0.4342887273430635]
	TIME [epoch: 6.5 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6943972008035522		[learning rate: 0.00021811]
	Learning Rate: 0.000218106
	LOSS [training: 0.6943972008035522 | validation: 0.43639569594722694]
	TIME [epoch: 6.5 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6930119348437916		[learning rate: 0.00021759]
	Learning Rate: 0.000217591
	LOSS [training: 0.6930119348437916 | validation: 0.4295573798364118]
	TIME [epoch: 6.55 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.698081873906807		[learning rate: 0.00021708]
	Learning Rate: 0.000217078
	LOSS [training: 0.698081873906807 | validation: 0.4344279941191286]
	TIME [epoch: 6.51 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6921277828718071		[learning rate: 0.00021657]
	Learning Rate: 0.000216566
	LOSS [training: 0.6921277828718071 | validation: 0.4251226998722843]
	TIME [epoch: 6.5 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6925106613848071		[learning rate: 0.00021605]
	Learning Rate: 0.000216055
	LOSS [training: 0.6925106613848071 | validation: 0.4369153907349416]
	TIME [epoch: 6.49 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6945764027792654		[learning rate: 0.00021555]
	Learning Rate: 0.000215545
	LOSS [training: 0.6945764027792654 | validation: 0.43125239415537586]
	TIME [epoch: 6.49 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934328222492813		[learning rate: 0.00021504]
	Learning Rate: 0.000215037
	LOSS [training: 0.6934328222492813 | validation: 0.4355609488115705]
	TIME [epoch: 6.48 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6919368284848373		[learning rate: 0.00021453]
	Learning Rate: 0.00021453
	LOSS [training: 0.6919368284848373 | validation: 0.4312546355892211]
	TIME [epoch: 6.53 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6912054907388068		[learning rate: 0.00021402]
	Learning Rate: 0.000214024
	LOSS [training: 0.6912054907388068 | validation: 0.4383270255167555]
	TIME [epoch: 6.51 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6940199055122409		[learning rate: 0.00021352]
	Learning Rate: 0.000213519
	LOSS [training: 0.6940199055122409 | validation: 0.4281695073517351]
	TIME [epoch: 6.5 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933880240496751		[learning rate: 0.00021302]
	Learning Rate: 0.000213015
	LOSS [training: 0.6933880240496751 | validation: 0.42799170004990794]
	TIME [epoch: 6.48 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6898684420301289		[learning rate: 0.00021251]
	Learning Rate: 0.000212513
	LOSS [training: 0.6898684420301289 | validation: 0.4360417705412722]
	TIME [epoch: 6.48 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.696724740789994		[learning rate: 0.00021201]
	Learning Rate: 0.000212011
	LOSS [training: 0.696724740789994 | validation: 0.43505726276598505]
	TIME [epoch: 6.52 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6915884523297176		[learning rate: 0.00021151]
	Learning Rate: 0.000211511
	LOSS [training: 0.6915884523297176 | validation: 0.4233174798997952]
	TIME [epoch: 6.51 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6954430903695311		[learning rate: 0.00021101]
	Learning Rate: 0.000211012
	LOSS [training: 0.6954430903695311 | validation: 0.4252140462610347]
	TIME [epoch: 6.48 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6907852436260236		[learning rate: 0.00021051]
	Learning Rate: 0.000210514
	LOSS [training: 0.6907852436260236 | validation: 0.4363336210658553]
	TIME [epoch: 6.5 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6919200303863107		[learning rate: 0.00021002]
	Learning Rate: 0.000210018
	LOSS [training: 0.6919200303863107 | validation: 0.4315931196664254]
	TIME [epoch: 6.49 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6917920032250124		[learning rate: 0.00020952]
	Learning Rate: 0.000209523
	LOSS [training: 0.6917920032250124 | validation: 0.4252264948987404]
	TIME [epoch: 6.48 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6931997696212149		[learning rate: 0.00020903]
	Learning Rate: 0.000209028
	LOSS [training: 0.6931997696212149 | validation: 0.42893356887054856]
	TIME [epoch: 6.53 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6919898296523443		[learning rate: 0.00020854]
	Learning Rate: 0.000208535
	LOSS [training: 0.6919898296523443 | validation: 0.43092494824488453]
	TIME [epoch: 6.5 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6884025004351552		[learning rate: 0.00020804]
	Learning Rate: 0.000208043
	LOSS [training: 0.6884025004351552 | validation: 0.4300024956395299]
	TIME [epoch: 6.5 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6925951209373312		[learning rate: 0.00020755]
	Learning Rate: 0.000207553
	LOSS [training: 0.6925951209373312 | validation: 0.4278397526824266]
	TIME [epoch: 6.5 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6946901132989526		[learning rate: 0.00020706]
	Learning Rate: 0.000207063
	LOSS [training: 0.6946901132989526 | validation: 0.4320486824601809]
	TIME [epoch: 6.5 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6943807921562367		[learning rate: 0.00020657]
	Learning Rate: 0.000206575
	LOSS [training: 0.6943807921562367 | validation: 0.42965419680501826]
	TIME [epoch: 6.5 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6874479282671353		[learning rate: 0.00020609]
	Learning Rate: 0.000206087
	LOSS [training: 0.6874479282671353 | validation: 0.4339247367751557]
	TIME [epoch: 6.54 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6926218813472065		[learning rate: 0.0002056]
	Learning Rate: 0.000205601
	LOSS [training: 0.6926218813472065 | validation: 0.42807455095226815]
	TIME [epoch: 6.5 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6913396323345908		[learning rate: 0.00020512]
	Learning Rate: 0.000205116
	LOSS [training: 0.6913396323345908 | validation: 0.43809796580987076]
	TIME [epoch: 6.5 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6971676291356592		[learning rate: 0.00020463]
	Learning Rate: 0.000204632
	LOSS [training: 0.6971676291356592 | validation: 0.42772643244854425]
	TIME [epoch: 6.5 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6919056297154026		[learning rate: 0.00020415]
	Learning Rate: 0.00020415
	LOSS [training: 0.6919056297154026 | validation: 0.4286420133658476]
	TIME [epoch: 6.49 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6888349283042079		[learning rate: 0.00020367]
	Learning Rate: 0.000203668
	LOSS [training: 0.6888349283042079 | validation: 0.4322554515742567]
	TIME [epoch: 6.52 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6869054940508841		[learning rate: 0.00020319]
	Learning Rate: 0.000203188
	LOSS [training: 0.6869054940508841 | validation: 0.43670857342652364]
	TIME [epoch: 6.55 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6924090712598723		[learning rate: 0.00020271]
	Learning Rate: 0.000202708
	LOSS [training: 0.6924090712598723 | validation: 0.4265212661254512]
	TIME [epoch: 6.51 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.691387114012193		[learning rate: 0.00020223]
	Learning Rate: 0.00020223
	LOSS [training: 0.691387114012193 | validation: 0.43276638942120355]
	TIME [epoch: 6.51 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6905132711640343		[learning rate: 0.00020175]
	Learning Rate: 0.000201753
	LOSS [training: 0.6905132711640343 | validation: 0.42979445412821815]
	TIME [epoch: 6.5 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6892276547635802		[learning rate: 0.00020128]
	Learning Rate: 0.000201277
	LOSS [training: 0.6892276547635802 | validation: 0.438960004098856]
	TIME [epoch: 6.5 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6916587945809869		[learning rate: 0.0002008]
	Learning Rate: 0.000200803
	LOSS [training: 0.6916587945809869 | validation: 0.4380259039135739]
	TIME [epoch: 6.5 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6957478246897356		[learning rate: 0.00020033]
	Learning Rate: 0.000200329
	LOSS [training: 0.6957478246897356 | validation: 0.42583077722431917]
	TIME [epoch: 6.51 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6941602851320379		[learning rate: 0.00019986]
	Learning Rate: 0.000199856
	LOSS [training: 0.6941602851320379 | validation: 0.4339173225088069]
	TIME [epoch: 6.51 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694914262448064		[learning rate: 0.00019938]
	Learning Rate: 0.000199385
	LOSS [training: 0.694914262448064 | validation: 0.43465704700092817]
	TIME [epoch: 6.5 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6912514969652829		[learning rate: 0.00019891]
	Learning Rate: 0.000198915
	LOSS [training: 0.6912514969652829 | validation: 0.42410921275251356]
	TIME [epoch: 6.51 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6901700669226081		[learning rate: 0.00019845]
	Learning Rate: 0.000198445
	LOSS [training: 0.6901700669226081 | validation: 0.43241537481162595]
	TIME [epoch: 6.51 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6931925978967918		[learning rate: 0.00019798]
	Learning Rate: 0.000197977
	LOSS [training: 0.6931925978967918 | validation: 0.4239294180309794]
	TIME [epoch: 6.55 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934567355195592		[learning rate: 0.00019751]
	Learning Rate: 0.00019751
	LOSS [training: 0.6934567355195592 | validation: 0.43924271154822175]
	TIME [epoch: 6.52 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.696272526012868		[learning rate: 0.00019704]
	Learning Rate: 0.000197044
	LOSS [training: 0.696272526012868 | validation: 0.4282898759589462]
	TIME [epoch: 6.5 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6946633219059434		[learning rate: 0.00019658]
	Learning Rate: 0.00019658
	LOSS [training: 0.6946633219059434 | validation: 0.44237116020040207]
	TIME [epoch: 6.47 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6965984248190256		[learning rate: 0.00019612]
	Learning Rate: 0.000196116
	LOSS [training: 0.6965984248190256 | validation: 0.44004467600873376]
	TIME [epoch: 6.49 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6963995393405378		[learning rate: 0.00019565]
	Learning Rate: 0.000195653
	LOSS [training: 0.6963995393405378 | validation: 0.436328752988089]
	TIME [epoch: 6.5 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933374610309511		[learning rate: 0.00019519]
	Learning Rate: 0.000195192
	LOSS [training: 0.6933374610309511 | validation: 0.435061159666301]
	TIME [epoch: 6.52 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6932141475229106		[learning rate: 0.00019473]
	Learning Rate: 0.000194731
	LOSS [training: 0.6932141475229106 | validation: 0.4287918795465998]
	TIME [epoch: 6.49 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6922910688563063		[learning rate: 0.00019427]
	Learning Rate: 0.000194272
	LOSS [training: 0.6922910688563063 | validation: 0.4378765630899836]
	TIME [epoch: 6.5 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6938873771381007		[learning rate: 0.00019381]
	Learning Rate: 0.000193814
	LOSS [training: 0.6938873771381007 | validation: 0.43212821879806956]
	TIME [epoch: 6.49 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6922132421036429		[learning rate: 0.00019336]
	Learning Rate: 0.000193357
	LOSS [training: 0.6922132421036429 | validation: 0.4325952679895675]
	TIME [epoch: 6.47 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6910491110382437		[learning rate: 0.0001929]
	Learning Rate: 0.0001929
	LOSS [training: 0.6910491110382437 | validation: 0.42552573211810024]
	TIME [epoch: 6.51 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6919268348105756		[learning rate: 0.00019245]
	Learning Rate: 0.000192445
	LOSS [training: 0.6919268348105756 | validation: 0.43907322940114435]
	TIME [epoch: 6.5 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6908468865898967		[learning rate: 0.00019199]
	Learning Rate: 0.000191992
	LOSS [training: 0.6908468865898967 | validation: 0.42800132488038073]
	TIME [epoch: 6.48 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6961690513694434		[learning rate: 0.00019154]
	Learning Rate: 0.000191539
	LOSS [training: 0.6961690513694434 | validation: 0.42584443127234933]
	TIME [epoch: 6.47 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6899320389533258		[learning rate: 0.00019109]
	Learning Rate: 0.000191087
	LOSS [training: 0.6899320389533258 | validation: 0.42638263507190644]
	TIME [epoch: 6.46 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6921690518122865		[learning rate: 0.00019064]
	Learning Rate: 0.000190636
	LOSS [training: 0.6921690518122865 | validation: 0.43485436906472397]
	TIME [epoch: 6.46 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6929121983359061		[learning rate: 0.00019019]
	Learning Rate: 0.000190186
	LOSS [training: 0.6929121983359061 | validation: 0.4373749403749594]
	TIME [epoch: 6.5 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934874223558889		[learning rate: 0.00018974]
	Learning Rate: 0.000189738
	LOSS [training: 0.6934874223558889 | validation: 0.4279932192932836]
	TIME [epoch: 6.49 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6946138045158243		[learning rate: 0.00018929]
	Learning Rate: 0.00018929
	LOSS [training: 0.6946138045158243 | validation: 0.4328775279941425]
	TIME [epoch: 6.46 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6914482224571052		[learning rate: 0.00018884]
	Learning Rate: 0.000188844
	LOSS [training: 0.6914482224571052 | validation: 0.42964949564466315]
	TIME [epoch: 6.47 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6924893132919483		[learning rate: 0.0001884]
	Learning Rate: 0.000188398
	LOSS [training: 0.6924893132919483 | validation: 0.43860360651475117]
	TIME [epoch: 6.48 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6931039649570154		[learning rate: 0.00018795]
	Learning Rate: 0.000187954
	LOSS [training: 0.6931039649570154 | validation: 0.440149790110969]
	TIME [epoch: 6.48 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6946980892290023		[learning rate: 0.00018751]
	Learning Rate: 0.000187511
	LOSS [training: 0.6946980892290023 | validation: 0.4253497310363724]
	TIME [epoch: 6.51 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.690902995856818		[learning rate: 0.00018707]
	Learning Rate: 0.000187068
	LOSS [training: 0.690902995856818 | validation: 0.43598373827118053]
	TIME [epoch: 6.48 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6928009873211938		[learning rate: 0.00018663]
	Learning Rate: 0.000186627
	LOSS [training: 0.6928009873211938 | validation: 0.4350796956712435]
	TIME [epoch: 6.49 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933856106964598		[learning rate: 0.00018619]
	Learning Rate: 0.000186187
	LOSS [training: 0.6933856106964598 | validation: 0.43300894529893347]
	TIME [epoch: 6.49 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6961903940348354		[learning rate: 0.00018575]
	Learning Rate: 0.000185748
	LOSS [training: 0.6961903940348354 | validation: 0.42877014782073775]
	TIME [epoch: 6.47 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6942887385903602		[learning rate: 0.00018531]
	Learning Rate: 0.000185309
	LOSS [training: 0.6942887385903602 | validation: 0.43400901664827524]
	TIME [epoch: 6.47 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6948697129542842		[learning rate: 0.00018487]
	Learning Rate: 0.000184872
	LOSS [training: 0.6948697129542842 | validation: 0.4331600858447458]
	TIME [epoch: 6.51 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6930358028931797		[learning rate: 0.00018444]
	Learning Rate: 0.000184436
	LOSS [training: 0.6930358028931797 | validation: 0.4212370716824464]
	TIME [epoch: 6.48 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6955231392427801		[learning rate: 0.000184]
	Learning Rate: 0.000184001
	LOSS [training: 0.6955231392427801 | validation: 0.438394815558213]
	TIME [epoch: 6.47 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6950655772676876		[learning rate: 0.00018357]
	Learning Rate: 0.000183567
	LOSS [training: 0.6950655772676876 | validation: 0.4348024420358235]
	TIME [epoch: 6.48 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6912872426292729		[learning rate: 0.00018313]
	Learning Rate: 0.000183134
	LOSS [training: 0.6912872426292729 | validation: 0.427912634556243]
	TIME [epoch: 6.5 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6945214035130071		[learning rate: 0.0001827]
	Learning Rate: 0.000182702
	LOSS [training: 0.6945214035130071 | validation: 0.428178219798902]
	TIME [epoch: 6.49 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6937805268896573		[learning rate: 0.00018227]
	Learning Rate: 0.000182271
	LOSS [training: 0.6937805268896573 | validation: 0.43337894628468965]
	TIME [epoch: 6.49 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6959568818827317		[learning rate: 0.00018184]
	Learning Rate: 0.000181841
	LOSS [training: 0.6959568818827317 | validation: 0.4352230983296334]
	TIME [epoch: 6.48 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6954692402161166		[learning rate: 0.00018141]
	Learning Rate: 0.000181412
	LOSS [training: 0.6954692402161166 | validation: 0.4180570946690576]
	TIME [epoch: 6.47 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6946372455513378		[learning rate: 0.00018098]
	Learning Rate: 0.000180984
	LOSS [training: 0.6946372455513378 | validation: 0.4388514845711604]
	TIME [epoch: 6.47 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6964348614858846		[learning rate: 0.00018056]
	Learning Rate: 0.000180557
	LOSS [training: 0.6964348614858846 | validation: 0.43553765024877567]
	TIME [epoch: 6.48 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6946888297926618		[learning rate: 0.00018013]
	Learning Rate: 0.000180132
	LOSS [training: 0.6946888297926618 | validation: 0.434463621381009]
	TIME [epoch: 6.54 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6954589811979737		[learning rate: 0.00017971]
	Learning Rate: 0.000179707
	LOSS [training: 0.6954589811979737 | validation: 0.4269483819912806]
	TIME [epoch: 6.49 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6939960654244144		[learning rate: 0.00017928]
	Learning Rate: 0.000179283
	LOSS [training: 0.6939960654244144 | validation: 0.43413142360566415]
	TIME [epoch: 6.51 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6950511541253497		[learning rate: 0.00017886]
	Learning Rate: 0.00017886
	LOSS [training: 0.6950511541253497 | validation: 0.43475630974847485]
	TIME [epoch: 6.46 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6919818346495695		[learning rate: 0.00017844]
	Learning Rate: 0.000178438
	LOSS [training: 0.6919818346495695 | validation: 0.4298577303206146]
	TIME [epoch: 6.47 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.693555781817647		[learning rate: 0.00017802]
	Learning Rate: 0.000178017
	LOSS [training: 0.693555781817647 | validation: 0.4343450032426603]
	TIME [epoch: 6.46 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6913405516764218		[learning rate: 0.0001776]
	Learning Rate: 0.000177597
	LOSS [training: 0.6913405516764218 | validation: 0.4345432557887652]
	TIME [epoch: 6.51 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6958762884431223		[learning rate: 0.00017718]
	Learning Rate: 0.000177178
	LOSS [training: 0.6958762884431223 | validation: 0.43006875248380527]
	TIME [epoch: 6.51 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.690153181701638		[learning rate: 0.00017676]
	Learning Rate: 0.00017676
	LOSS [training: 0.690153181701638 | validation: 0.42790945803435826]
	TIME [epoch: 6.47 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934936863989536		[learning rate: 0.00017634]
	Learning Rate: 0.000176343
	LOSS [training: 0.6934936863989536 | validation: 0.42825873551597166]
	TIME [epoch: 6.46 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6919943336725131		[learning rate: 0.00017593]
	Learning Rate: 0.000175927
	LOSS [training: 0.6919943336725131 | validation: 0.4323407038110054]
	TIME [epoch: 6.47 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6885242228073535		[learning rate: 0.00017551]
	Learning Rate: 0.000175512
	LOSS [training: 0.6885242228073535 | validation: 0.43218092188050505]
	TIME [epoch: 6.49 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.693539942084139		[learning rate: 0.0001751]
	Learning Rate: 0.000175098
	LOSS [training: 0.693539942084139 | validation: 0.4333889472726094]
	TIME [epoch: 6.54 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6927279165446778		[learning rate: 0.00017469]
	Learning Rate: 0.000174685
	LOSS [training: 0.6927279165446778 | validation: 0.4283192012990071]
	TIME [epoch: 6.48 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6899908481143944		[learning rate: 0.00017427]
	Learning Rate: 0.000174273
	LOSS [training: 0.6899908481143944 | validation: 0.4277232031259254]
	TIME [epoch: 6.47 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6943355463264923		[learning rate: 0.00017386]
	Learning Rate: 0.000173862
	LOSS [training: 0.6943355463264923 | validation: 0.43020410843134005]
	TIME [epoch: 6.47 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6881287414485491		[learning rate: 0.00017345]
	Learning Rate: 0.000173452
	LOSS [training: 0.6881287414485491 | validation: 0.43481441314633185]
	TIME [epoch: 6.47 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.691809779846435		[learning rate: 0.00017304]
	Learning Rate: 0.000173043
	LOSS [training: 0.691809779846435 | validation: 0.4255823102037719]
	TIME [epoch: 6.49 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6925743054317922		[learning rate: 0.00017263]
	Learning Rate: 0.000172635
	LOSS [training: 0.6925743054317922 | validation: 0.4298340165412212]
	TIME [epoch: 6.49 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6913000489995212		[learning rate: 0.00017223]
	Learning Rate: 0.000172228
	LOSS [training: 0.6913000489995212 | validation: 0.44004665958932776]
	TIME [epoch: 6.47 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6981289241686642		[learning rate: 0.00017182]
	Learning Rate: 0.000171821
	LOSS [training: 0.6981289241686642 | validation: 0.4337426131961544]
	TIME [epoch: 6.47 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6938362163684266		[learning rate: 0.00017142]
	Learning Rate: 0.000171416
	LOSS [training: 0.6938362163684266 | validation: 0.4329284277331932]
	TIME [epoch: 6.47 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6973977523105324		[learning rate: 0.00017101]
	Learning Rate: 0.000171012
	LOSS [training: 0.6973977523105324 | validation: 0.42877596946854]
	TIME [epoch: 6.48 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6918326279894706		[learning rate: 0.00017061]
	Learning Rate: 0.000170608
	LOSS [training: 0.6918326279894706 | validation: 0.430150295193886]
	TIME [epoch: 6.54 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6919756681502113		[learning rate: 0.00017021]
	Learning Rate: 0.000170206
	LOSS [training: 0.6919756681502113 | validation: 0.43500562500996354]
	TIME [epoch: 6.48 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6923511664522872		[learning rate: 0.0001698]
	Learning Rate: 0.000169804
	LOSS [training: 0.6923511664522872 | validation: 0.4280803464362471]
	TIME [epoch: 6.47 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6943956102172678		[learning rate: 0.0001694]
	Learning Rate: 0.000169404
	LOSS [training: 0.6943956102172678 | validation: 0.4353059967762737]
	TIME [epoch: 6.47 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6947975161147584		[learning rate: 0.000169]
	Learning Rate: 0.000169004
	LOSS [training: 0.6947975161147584 | validation: 0.43230749059709744]
	TIME [epoch: 6.46 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6916910551376544		[learning rate: 0.00016861]
	Learning Rate: 0.000168606
	LOSS [training: 0.6916910551376544 | validation: 0.42913706177328015]
	TIME [epoch: 6.49 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933458556608953		[learning rate: 0.00016821]
	Learning Rate: 0.000168208
	LOSS [training: 0.6933458556608953 | validation: 0.42901260885311837]
	TIME [epoch: 6.52 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694108835549086		[learning rate: 0.00016781]
	Learning Rate: 0.000167811
	LOSS [training: 0.694108835549086 | validation: 0.42894777829489084]
	TIME [epoch: 6.5 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6943882443988302		[learning rate: 0.00016742]
	Learning Rate: 0.000167415
	LOSS [training: 0.6943882443988302 | validation: 0.42975171978836024]
	TIME [epoch: 6.47 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6935301329555457		[learning rate: 0.00016702]
	Learning Rate: 0.00016702
	LOSS [training: 0.6935301329555457 | validation: 0.4260112308965152]
	TIME [epoch: 6.49 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6935043036608942		[learning rate: 0.00016663]
	Learning Rate: 0.000166626
	LOSS [training: 0.6935043036608942 | validation: 0.43186425929161376]
	TIME [epoch: 6.47 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6883203910463189		[learning rate: 0.00016623]
	Learning Rate: 0.000166233
	LOSS [training: 0.6883203910463189 | validation: 0.4276967038436765]
	TIME [epoch: 6.49 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6963861359011825		[learning rate: 0.00016584]
	Learning Rate: 0.000165841
	LOSS [training: 0.6963861359011825 | validation: 0.4302859475584773]
	TIME [epoch: 6.52 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6955228453438634		[learning rate: 0.00016545]
	Learning Rate: 0.00016545
	LOSS [training: 0.6955228453438634 | validation: 0.42859795604599793]
	TIME [epoch: 6.48 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6923966349770575		[learning rate: 0.00016506]
	Learning Rate: 0.00016506
	LOSS [training: 0.6923966349770575 | validation: 0.42678018300887743]
	TIME [epoch: 6.47 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6919635486912465		[learning rate: 0.00016467]
	Learning Rate: 0.00016467
	LOSS [training: 0.6919635486912465 | validation: 0.4288059961512056]
	TIME [epoch: 6.48 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6878418051903024		[learning rate: 0.00016428]
	Learning Rate: 0.000164282
	LOSS [training: 0.6878418051903024 | validation: 0.4272780029090939]
	TIME [epoch: 6.49 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.69093546955439		[learning rate: 0.00016389]
	Learning Rate: 0.000163894
	LOSS [training: 0.69093546955439 | validation: 0.4283738943052342]
	TIME [epoch: 6.51 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6895473297956265		[learning rate: 0.00016351]
	Learning Rate: 0.000163508
	LOSS [training: 0.6895473297956265 | validation: 0.43545981901190833]
	TIME [epoch: 6.52 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6928434699380979		[learning rate: 0.00016312]
	Learning Rate: 0.000163122
	LOSS [training: 0.6928434699380979 | validation: 0.43046206183942326]
	TIME [epoch: 6.48 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6962188199421612		[learning rate: 0.00016274]
	Learning Rate: 0.000162737
	LOSS [training: 0.6962188199421612 | validation: 0.43114288175864146]
	TIME [epoch: 6.47 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6922415212572475		[learning rate: 0.00016235]
	Learning Rate: 0.000162353
	LOSS [training: 0.6922415212572475 | validation: 0.4287897276102425]
	TIME [epoch: 6.47 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6940437674278949		[learning rate: 0.00016197]
	Learning Rate: 0.00016197
	LOSS [training: 0.6940437674278949 | validation: 0.429636718504546]
	TIME [epoch: 6.48 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6931058532922055		[learning rate: 0.00016159]
	Learning Rate: 0.000161588
	LOSS [training: 0.6931058532922055 | validation: 0.43540948640788835]
	TIME [epoch: 6.53 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694584994031313		[learning rate: 0.00016121]
	Learning Rate: 0.000161207
	LOSS [training: 0.694584994031313 | validation: 0.4277507626368347]
	TIME [epoch: 6.5 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6896009932024709		[learning rate: 0.00016083]
	Learning Rate: 0.000160827
	LOSS [training: 0.6896009932024709 | validation: 0.4305001759424518]
	TIME [epoch: 6.49 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6948035784800138		[learning rate: 0.00016045]
	Learning Rate: 0.000160448
	LOSS [training: 0.6948035784800138 | validation: 0.43039892446047545]
	TIME [epoch: 6.47 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6946774430215129		[learning rate: 0.00016007]
	Learning Rate: 0.000160069
	LOSS [training: 0.6946774430215129 | validation: 0.4324651078166192]
	TIME [epoch: 6.49 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6946313102140376		[learning rate: 0.00015969]
	Learning Rate: 0.000159692
	LOSS [training: 0.6946313102140376 | validation: 0.431354423191411]
	TIME [epoch: 6.47 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6932525240139266		[learning rate: 0.00015931]
	Learning Rate: 0.000159315
	LOSS [training: 0.6932525240139266 | validation: 0.43599309302646266]
	TIME [epoch: 6.53 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934493933706147		[learning rate: 0.00015894]
	Learning Rate: 0.000158939
	LOSS [training: 0.6934493933706147 | validation: 0.42696418908213174]
	TIME [epoch: 6.5 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6922536173967373		[learning rate: 0.00015856]
	Learning Rate: 0.000158564
	LOSS [training: 0.6922536173967373 | validation: 0.43368738155360065]
	TIME [epoch: 6.49 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6988655285523727		[learning rate: 0.00015819]
	Learning Rate: 0.00015819
	LOSS [training: 0.6988655285523727 | validation: 0.4303259623385748]
	TIME [epoch: 6.49 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6973567252863739		[learning rate: 0.00015782]
	Learning Rate: 0.000157817
	LOSS [training: 0.6973567252863739 | validation: 0.43357933916765307]
	TIME [epoch: 6.47 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6970889360608423		[learning rate: 0.00015744]
	Learning Rate: 0.000157445
	LOSS [training: 0.6970889360608423 | validation: 0.43122924917776084]
	TIME [epoch: 6.49 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6918783223296863		[learning rate: 0.00015707]
	Learning Rate: 0.000157073
	LOSS [training: 0.6918783223296863 | validation: 0.43155988771435283]
	TIME [epoch: 6.51 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6943602076246431		[learning rate: 0.0001567]
	Learning Rate: 0.000156703
	LOSS [training: 0.6943602076246431 | validation: 0.43372168597781513]
	TIME [epoch: 6.47 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6951607828820178		[learning rate: 0.00015633]
	Learning Rate: 0.000156333
	LOSS [training: 0.6951607828820178 | validation: 0.4320220553528751]
	TIME [epoch: 6.47 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934589542630031		[learning rate: 0.00015596]
	Learning Rate: 0.000155964
	LOSS [training: 0.6934589542630031 | validation: 0.4252713157601472]
	TIME [epoch: 6.48 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6935374388896829		[learning rate: 0.0001556]
	Learning Rate: 0.000155597
	LOSS [training: 0.6935374388896829 | validation: 0.43680361709052706]
	TIME [epoch: 6.47 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6917756897324054		[learning rate: 0.00015523]
	Learning Rate: 0.00015523
	LOSS [training: 0.6917756897324054 | validation: 0.43146271727007385]
	TIME [epoch: 6.5 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6946479677064371		[learning rate: 0.00015486]
	Learning Rate: 0.000154863
	LOSS [training: 0.6946479677064371 | validation: 0.4271719592074843]
	TIME [epoch: 6.51 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6922783862746947		[learning rate: 0.0001545]
	Learning Rate: 0.000154498
	LOSS [training: 0.6922783862746947 | validation: 0.42495963631064]
	TIME [epoch: 6.47 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6929127556458177		[learning rate: 0.00015413]
	Learning Rate: 0.000154134
	LOSS [training: 0.6929127556458177 | validation: 0.4415201697946945]
	TIME [epoch: 6.46 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6935388272816279		[learning rate: 0.00015377]
	Learning Rate: 0.00015377
	LOSS [training: 0.6935388272816279 | validation: 0.4339940030782527]
	TIME [epoch: 6.48 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6923879823107584		[learning rate: 0.00015341]
	Learning Rate: 0.000153407
	LOSS [training: 0.6923879823107584 | validation: 0.42801002132318866]
	TIME [epoch: 6.46 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6895168900720023		[learning rate: 0.00015305]
	Learning Rate: 0.000153045
	LOSS [training: 0.6895168900720023 | validation: 0.42940985212916655]
	TIME [epoch: 6.51 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6942700333452836		[learning rate: 0.00015268]
	Learning Rate: 0.000152684
	LOSS [training: 0.6942700333452836 | validation: 0.4374890018905761]
	TIME [epoch: 6.48 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934982031248527		[learning rate: 0.00015232]
	Learning Rate: 0.000152324
	LOSS [training: 0.6934982031248527 | validation: 0.4389526902756695]
	TIME [epoch: 6.48 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934965496103074		[learning rate: 0.00015196]
	Learning Rate: 0.000151965
	LOSS [training: 0.6934965496103074 | validation: 0.4368713194990853]
	TIME [epoch: 6.49 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.693835952938061		[learning rate: 0.00015161]
	Learning Rate: 0.000151607
	LOSS [training: 0.693835952938061 | validation: 0.43574876354152603]
	TIME [epoch: 6.47 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6976076619922484		[learning rate: 0.00015125]
	Learning Rate: 0.000151249
	LOSS [training: 0.6976076619922484 | validation: 0.4347859868266256]
	TIME [epoch: 6.47 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6972636709978665		[learning rate: 0.00015089]
	Learning Rate: 0.000150892
	LOSS [training: 0.6972636709978665 | validation: 0.43493528586123587]
	TIME [epoch: 6.53 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6930227375500677		[learning rate: 0.00015054]
	Learning Rate: 0.000150536
	LOSS [training: 0.6930227375500677 | validation: 0.432237782205225]
	TIME [epoch: 6.49 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6896885277129191		[learning rate: 0.00015018]
	Learning Rate: 0.000150181
	LOSS [training: 0.6896885277129191 | validation: 0.4288838109617444]
	TIME [epoch: 6.47 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6908480082118609		[learning rate: 0.00014983]
	Learning Rate: 0.000149827
	LOSS [training: 0.6908480082118609 | validation: 0.4345700523046224]
	TIME [epoch: 6.49 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933586224825324		[learning rate: 0.00014947]
	Learning Rate: 0.000149473
	LOSS [training: 0.6933586224825324 | validation: 0.43366034538096854]
	TIME [epoch: 6.48 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6924965063651406		[learning rate: 0.00014912]
	Learning Rate: 0.000149121
	LOSS [training: 0.6924965063651406 | validation: 0.432808738582459]
	TIME [epoch: 6.5 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933734182314784		[learning rate: 0.00014877]
	Learning Rate: 0.000148769
	LOSS [training: 0.6933734182314784 | validation: 0.42438098334958196]
	TIME [epoch: 6.54 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694227283483132		[learning rate: 0.00014842]
	Learning Rate: 0.000148418
	LOSS [training: 0.694227283483132 | validation: 0.4210471686156754]
	TIME [epoch: 6.48 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6909010483314224		[learning rate: 0.00014807]
	Learning Rate: 0.000148068
	LOSS [training: 0.6909010483314224 | validation: 0.4326178026074405]
	TIME [epoch: 6.49 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.691637447392758		[learning rate: 0.00014772]
	Learning Rate: 0.000147719
	LOSS [training: 0.691637447392758 | validation: 0.4319450273962366]
	TIME [epoch: 6.48 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6914394379798393		[learning rate: 0.00014737]
	Learning Rate: 0.00014737
	LOSS [training: 0.6914394379798393 | validation: 0.4256527519300698]
	TIME [epoch: 6.49 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6943030754915026		[learning rate: 0.00014702]
	Learning Rate: 0.000147023
	LOSS [training: 0.6943030754915026 | validation: 0.42604468430040127]
	TIME [epoch: 6.5 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6915544119700915		[learning rate: 0.00014668]
	Learning Rate: 0.000146676
	LOSS [training: 0.6915544119700915 | validation: 0.42894683225771935]
	TIME [epoch: 6.53 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6906183538154147		[learning rate: 0.00014633]
	Learning Rate: 0.00014633
	LOSS [training: 0.6906183538154147 | validation: 0.42726442383537216]
	TIME [epoch: 6.47 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6955262651690992		[learning rate: 0.00014598]
	Learning Rate: 0.000145985
	LOSS [training: 0.6955262651690992 | validation: 0.4318767994561479]
	TIME [epoch: 6.5 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.690736354122077		[learning rate: 0.00014564]
	Learning Rate: 0.00014564
	LOSS [training: 0.690736354122077 | validation: 0.4332340432684101]
	TIME [epoch: 6.48 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933957752043584		[learning rate: 0.0001453]
	Learning Rate: 0.000145297
	LOSS [training: 0.6933957752043584 | validation: 0.4308430194131304]
	TIME [epoch: 6.49 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6939165915228376		[learning rate: 0.00014495]
	Learning Rate: 0.000144954
	LOSS [training: 0.6939165915228376 | validation: 0.42954160127465246]
	TIME [epoch: 6.53 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6903789093242466		[learning rate: 0.00014461]
	Learning Rate: 0.000144612
	LOSS [training: 0.6903789093242466 | validation: 0.4322638068524238]
	TIME [epoch: 6.49 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6894409562783116		[learning rate: 0.00014427]
	Learning Rate: 0.000144271
	LOSS [training: 0.6894409562783116 | validation: 0.4213890212338952]
	TIME [epoch: 6.5 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.690343613561281		[learning rate: 0.00014393]
	Learning Rate: 0.000143931
	LOSS [training: 0.690343613561281 | validation: 0.4270921359792811]
	TIME [epoch: 6.47 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.691961033394284		[learning rate: 0.00014359]
	Learning Rate: 0.000143591
	LOSS [training: 0.691961033394284 | validation: 0.4291665067190475]
	TIME [epoch: 6.47 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6921112116193736		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 0.6921112116193736 | validation: 0.4300265440212314]
	TIME [epoch: 6.48 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6917906644878699		[learning rate: 0.00014291]
	Learning Rate: 0.000142915
	LOSS [training: 0.6917906644878699 | validation: 0.42943268002445856]
	TIME [epoch: 6.52 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.693344407568497		[learning rate: 0.00014258]
	Learning Rate: 0.000142578
	LOSS [training: 0.693344407568497 | validation: 0.43563389678826714]
	TIME [epoch: 6.48 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6916318613896887		[learning rate: 0.00014224]
	Learning Rate: 0.000142241
	LOSS [training: 0.6916318613896887 | validation: 0.43610550881372667]
	TIME [epoch: 6.47 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6926749541971163		[learning rate: 0.00014191]
	Learning Rate: 0.000141906
	LOSS [training: 0.6926749541971163 | validation: 0.4279169910511395]
	TIME [epoch: 6.48 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6898809528608286		[learning rate: 0.00014157]
	Learning Rate: 0.000141571
	LOSS [training: 0.6898809528608286 | validation: 0.4350218663282279]
	TIME [epoch: 6.48 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6959544514990111		[learning rate: 0.00014124]
	Learning Rate: 0.000141237
	LOSS [training: 0.6959544514990111 | validation: 0.42657210775616505]
	TIME [epoch: 6.49 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6954424955231371		[learning rate: 0.0001409]
	Learning Rate: 0.000140904
	LOSS [training: 0.6954424955231371 | validation: 0.4354851413750733]
	TIME [epoch: 6.53 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6938620721873443		[learning rate: 0.00014057]
	Learning Rate: 0.000140572
	LOSS [training: 0.6938620721873443 | validation: 0.4328228978301871]
	TIME [epoch: 6.49 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.69230670794278		[learning rate: 0.00014024]
	Learning Rate: 0.00014024
	LOSS [training: 0.69230670794278 | validation: 0.43459333035706615]
	TIME [epoch: 6.49 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.691979118160796		[learning rate: 0.00013991]
	Learning Rate: 0.000139909
	LOSS [training: 0.691979118160796 | validation: 0.43302833654001616]
	TIME [epoch: 6.47 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.692962668227182		[learning rate: 0.00013958]
	Learning Rate: 0.000139579
	LOSS [training: 0.692962668227182 | validation: 0.43064870618321494]
	TIME [epoch: 6.48 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6955171543176202		[learning rate: 0.00013925]
	Learning Rate: 0.00013925
	LOSS [training: 0.6955171543176202 | validation: 0.4291700095947852]
	TIME [epoch: 6.51 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6935170003386006		[learning rate: 0.00013892]
	Learning Rate: 0.000138921
	LOSS [training: 0.6935170003386006 | validation: 0.43390033423569135]
	TIME [epoch: 6.51 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.691971306510146		[learning rate: 0.00013859]
	Learning Rate: 0.000138594
	LOSS [training: 0.691971306510146 | validation: 0.4328366793592229]
	TIME [epoch: 6.47 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6943962425144659		[learning rate: 0.00013827]
	Learning Rate: 0.000138267
	LOSS [training: 0.6943962425144659 | validation: 0.42900840731003925]
	TIME [epoch: 6.47 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6922770823211721		[learning rate: 0.00013794]
	Learning Rate: 0.000137941
	LOSS [training: 0.6922770823211721 | validation: 0.43760605368530303]
	TIME [epoch: 6.48 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6913446331983888		[learning rate: 0.00013762]
	Learning Rate: 0.000137615
	LOSS [training: 0.6913446331983888 | validation: 0.4293581852196128]
	TIME [epoch: 6.48 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.69214688100336		[learning rate: 0.00013729]
	Learning Rate: 0.000137291
	LOSS [training: 0.69214688100336 | validation: 0.4320169466415865]
	TIME [epoch: 6.51 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6924201202933945		[learning rate: 0.00013697]
	Learning Rate: 0.000136967
	LOSS [training: 0.6924201202933945 | validation: 0.42346122541250175]
	TIME [epoch: 6.49 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6950660923195199		[learning rate: 0.00013664]
	Learning Rate: 0.000136644
	LOSS [training: 0.6950660923195199 | validation: 0.4334818293953891]
	TIME [epoch: 6.47 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6942987283313755		[learning rate: 0.00013632]
	Learning Rate: 0.000136321
	LOSS [training: 0.6942987283313755 | validation: 0.42613907851085886]
	TIME [epoch: 6.47 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933842491495037		[learning rate: 0.000136]
	Learning Rate: 0.000136
	LOSS [training: 0.6933842491495037 | validation: 0.428605086024118]
	TIME [epoch: 6.47 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6950553905177438		[learning rate: 0.00013568]
	Learning Rate: 0.000135679
	LOSS [training: 0.6950553905177438 | validation: 0.429215127778977]
	TIME [epoch: 6.48 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6975887744953653		[learning rate: 0.00013536]
	Learning Rate: 0.000135359
	LOSS [training: 0.6975887744953653 | validation: 0.42975262483327137]
	TIME [epoch: 6.52 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6984938531709807		[learning rate: 0.00013504]
	Learning Rate: 0.00013504
	LOSS [training: 0.6984938531709807 | validation: 0.4378261551560358]
	TIME [epoch: 6.48 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6939757410065763		[learning rate: 0.00013472]
	Learning Rate: 0.000134721
	LOSS [training: 0.6939757410065763 | validation: 0.4303807333093803]
	TIME [epoch: 6.47 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6928975964729971		[learning rate: 0.0001344]
	Learning Rate: 0.000134403
	LOSS [training: 0.6928975964729971 | validation: 0.431554063026933]
	TIME [epoch: 6.47 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6915357377822715		[learning rate: 0.00013409]
	Learning Rate: 0.000134086
	LOSS [training: 0.6915357377822715 | validation: 0.4275641266654604]
	TIME [epoch: 6.47 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6921134078072284		[learning rate: 0.00013377]
	Learning Rate: 0.00013377
	LOSS [training: 0.6921134078072284 | validation: 0.4260432441918882]
	TIME [epoch: 6.49 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6938279133159886		[learning rate: 0.00013345]
	Learning Rate: 0.000133455
	LOSS [training: 0.6938279133159886 | validation: 0.4309093859712256]
	TIME [epoch: 6.53 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6908037585640008		[learning rate: 0.00013314]
	Learning Rate: 0.00013314
	LOSS [training: 0.6908037585640008 | validation: 0.4324518458263515]
	TIME [epoch: 6.48 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6909165946820821		[learning rate: 0.00013283]
	Learning Rate: 0.000132826
	LOSS [training: 0.6909165946820821 | validation: 0.43369458304603675]
	TIME [epoch: 6.49 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6957363379482422		[learning rate: 0.00013251]
	Learning Rate: 0.000132512
	LOSS [training: 0.6957363379482422 | validation: 0.4384078373527013]
	TIME [epoch: 6.48 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6930210348435404		[learning rate: 0.0001322]
	Learning Rate: 0.0001322
	LOSS [training: 0.6930210348435404 | validation: 0.43734791048641625]
	TIME [epoch: 6.47 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6937387176488102		[learning rate: 0.00013189]
	Learning Rate: 0.000131888
	LOSS [training: 0.6937387176488102 | validation: 0.42530448917147856]
	TIME [epoch: 6.5 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6943285517771993		[learning rate: 0.00013158]
	Learning Rate: 0.000131577
	LOSS [training: 0.6943285517771993 | validation: 0.43254072079384226]
	TIME [epoch: 6.52 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6952273016620447		[learning rate: 0.00013127]
	Learning Rate: 0.000131266
	LOSS [training: 0.6952273016620447 | validation: 0.42780052929391843]
	TIME [epoch: 6.48 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6944827198843571		[learning rate: 0.00013096]
	Learning Rate: 0.000130957
	LOSS [training: 0.6944827198843571 | validation: 0.43133365686067715]
	TIME [epoch: 6.49 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6918384539773067		[learning rate: 0.00013065]
	Learning Rate: 0.000130648
	LOSS [training: 0.6918384539773067 | validation: 0.43815959004604405]
	TIME [epoch: 6.48 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6952111787979722		[learning rate: 0.00013034]
	Learning Rate: 0.00013034
	LOSS [training: 0.6952111787979722 | validation: 0.43186057061713795]
	TIME [epoch: 6.49 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6949479255260582		[learning rate: 0.00013003]
	Learning Rate: 0.000130032
	LOSS [training: 0.6949479255260582 | validation: 0.4353945403249655]
	TIME [epoch: 6.52 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6921213372324571		[learning rate: 0.00012973]
	Learning Rate: 0.000129726
	LOSS [training: 0.6921213372324571 | validation: 0.43135736929061386]
	TIME [epoch: 6.48 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6925300882381877		[learning rate: 0.00012942]
	Learning Rate: 0.00012942
	LOSS [training: 0.6925300882381877 | validation: 0.4299752757681171]
	TIME [epoch: 6.48 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.690939139911739		[learning rate: 0.00012911]
	Learning Rate: 0.000129114
	LOSS [training: 0.690939139911739 | validation: 0.4374744235195862]
	TIME [epoch: 6.49 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6898005943927152		[learning rate: 0.00012881]
	Learning Rate: 0.00012881
	LOSS [training: 0.6898005943927152 | validation: 0.4298740456075617]
	TIME [epoch: 6.49 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6980384853752849		[learning rate: 0.00012851]
	Learning Rate: 0.000128506
	LOSS [training: 0.6980384853752849 | validation: 0.43320561421795173]
	TIME [epoch: 6.5 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694224616882568		[learning rate: 0.0001282]
	Learning Rate: 0.000128203
	LOSS [training: 0.694224616882568 | validation: 0.4359200136931076]
	TIME [epoch: 6.53 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6920109547330794		[learning rate: 0.0001279]
	Learning Rate: 0.0001279
	LOSS [training: 0.6920109547330794 | validation: 0.4364702795621519]
	TIME [epoch: 6.5 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6922102145684212		[learning rate: 0.0001276]
	Learning Rate: 0.000127599
	LOSS [training: 0.6922102145684212 | validation: 0.4330398086292937]
	TIME [epoch: 6.47 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6941968886417317		[learning rate: 0.0001273]
	Learning Rate: 0.000127298
	LOSS [training: 0.6941968886417317 | validation: 0.4334686723632484]
	TIME [epoch: 6.47 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6944134374047259		[learning rate: 0.000127]
	Learning Rate: 0.000126997
	LOSS [training: 0.6944134374047259 | validation: 0.42727597584784116]
	TIME [epoch: 6.47 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933199651845905		[learning rate: 0.0001267]
	Learning Rate: 0.000126698
	LOSS [training: 0.6933199651845905 | validation: 0.4254886668807867]
	TIME [epoch: 6.51 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6908582514656162		[learning rate: 0.0001264]
	Learning Rate: 0.000126399
	LOSS [training: 0.6908582514656162 | validation: 0.4303953702923282]
	TIME [epoch: 6.53 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6898762903302487		[learning rate: 0.0001261]
	Learning Rate: 0.000126101
	LOSS [training: 0.6898762903302487 | validation: 0.43512806397343695]
	TIME [epoch: 6.48 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6909728660288847		[learning rate: 0.0001258]
	Learning Rate: 0.000125803
	LOSS [training: 0.6909728660288847 | validation: 0.4333492990731653]
	TIME [epoch: 6.49 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6930102566257285		[learning rate: 0.00012551]
	Learning Rate: 0.000125507
	LOSS [training: 0.6930102566257285 | validation: 0.42883613836320844]
	TIME [epoch: 6.49 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.690994420979897		[learning rate: 0.00012521]
	Learning Rate: 0.000125211
	LOSS [training: 0.690994420979897 | validation: 0.4325046311198406]
	TIME [epoch: 6.49 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6902018440443414		[learning rate: 0.00012492]
	Learning Rate: 0.000124915
	LOSS [training: 0.6902018440443414 | validation: 0.43417597544681136]
	TIME [epoch: 6.51 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6908279406988143		[learning rate: 0.00012462]
	Learning Rate: 0.000124621
	LOSS [training: 0.6908279406988143 | validation: 0.4324809183190724]
	TIME [epoch: 6.5 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6922306141311736		[learning rate: 0.00012433]
	Learning Rate: 0.000124327
	LOSS [training: 0.6922306141311736 | validation: 0.42800197774283577]
	TIME [epoch: 6.48 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6959178820555685		[learning rate: 0.00012403]
	Learning Rate: 0.000124033
	LOSS [training: 0.6959178820555685 | validation: 0.42859736590892633]
	TIME [epoch: 6.5 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6905775745614535		[learning rate: 0.00012374]
	Learning Rate: 0.000123741
	LOSS [training: 0.6905775745614535 | validation: 0.42675482501104334]
	TIME [epoch: 6.47 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6943358350192921		[learning rate: 0.00012345]
	Learning Rate: 0.000123449
	LOSS [training: 0.6943358350192921 | validation: 0.4273826552617594]
	TIME [epoch: 6.5 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6928804933624081		[learning rate: 0.00012316]
	Learning Rate: 0.000123158
	LOSS [training: 0.6928804933624081 | validation: 0.43677267308463386]
	TIME [epoch: 6.52 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6875261166320406		[learning rate: 0.00012287]
	Learning Rate: 0.000122867
	LOSS [training: 0.6875261166320406 | validation: 0.43468945697475336]
	TIME [epoch: 6.48 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6935241924144064		[learning rate: 0.00012258]
	Learning Rate: 0.000122577
	LOSS [training: 0.6935241924144064 | validation: 0.42707695783421035]
	TIME [epoch: 6.47 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6951728455890919		[learning rate: 0.00012229]
	Learning Rate: 0.000122288
	LOSS [training: 0.6951728455890919 | validation: 0.4310666180604951]
	TIME [epoch: 6.48 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6947346898599073		[learning rate: 0.000122]
	Learning Rate: 0.000122
	LOSS [training: 0.6947346898599073 | validation: 0.4283129110930714]
	TIME [epoch: 6.47 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6927179693909579		[learning rate: 0.00012171]
	Learning Rate: 0.000121712
	LOSS [training: 0.6927179693909579 | validation: 0.43265644286284805]
	TIME [epoch: 6.49 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6959288919847431		[learning rate: 0.00012142]
	Learning Rate: 0.000121425
	LOSS [training: 0.6959288919847431 | validation: 0.43110551400657815]
	TIME [epoch: 6.51 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6952007331256146		[learning rate: 0.00012114]
	Learning Rate: 0.000121138
	LOSS [training: 0.6952007331256146 | validation: 0.43038890854893497]
	TIME [epoch: 6.47 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6905402284028719		[learning rate: 0.00012085]
	Learning Rate: 0.000120853
	LOSS [training: 0.6905402284028719 | validation: 0.4328751822243323]
	TIME [epoch: 6.47 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.691229515743432		[learning rate: 0.00012057]
	Learning Rate: 0.000120568
	LOSS [training: 0.691229515743432 | validation: 0.4259132763635962]
	TIME [epoch: 6.48 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6911759465537479		[learning rate: 0.00012028]
	Learning Rate: 0.000120283
	LOSS [training: 0.6911759465537479 | validation: 0.4255414440363377]
	TIME [epoch: 6.47 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934601694618076		[learning rate: 0.00012]
	Learning Rate: 0.00012
	LOSS [training: 0.6934601694618076 | validation: 0.43797097889153735]
	TIME [epoch: 6.49 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6910440599277925		[learning rate: 0.00011972]
	Learning Rate: 0.000119716
	LOSS [training: 0.6910440599277925 | validation: 0.4321672768920768]
	TIME [epoch: 6.5 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6941966174300783		[learning rate: 0.00011943]
	Learning Rate: 0.000119434
	LOSS [training: 0.6941966174300783 | validation: 0.42683220260979376]
	TIME [epoch: 6.48 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934795352817071		[learning rate: 0.00011915]
	Learning Rate: 0.000119152
	LOSS [training: 0.6934795352817071 | validation: 0.424553662179303]
	TIME [epoch: 6.47 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6975861757023247		[learning rate: 0.00011887]
	Learning Rate: 0.000118871
	LOSS [training: 0.6975861757023247 | validation: 0.4280844908267134]
	TIME [epoch: 6.47 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6944201445741511		[learning rate: 0.00011859]
	Learning Rate: 0.000118591
	LOSS [training: 0.6944201445741511 | validation: 0.42877270376308907]
	TIME [epoch: 6.48 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933248811538812		[learning rate: 0.00011831]
	Learning Rate: 0.000118311
	LOSS [training: 0.6933248811538812 | validation: 0.42825020922787105]
	TIME [epoch: 6.53 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6903064052732256		[learning rate: 0.00011803]
	Learning Rate: 0.000118032
	LOSS [training: 0.6903064052732256 | validation: 0.4293702946682402]
	TIME [epoch: 6.48 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6928168251963208		[learning rate: 0.00011775]
	Learning Rate: 0.000117754
	LOSS [training: 0.6928168251963208 | validation: 0.42222690071633445]
	TIME [epoch: 6.51 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6889105827144437		[learning rate: 0.00011748]
	Learning Rate: 0.000117476
	LOSS [training: 0.6889105827144437 | validation: 0.4305870271569267]
	TIME [epoch: 6.47 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6895101460156112		[learning rate: 0.0001172]
	Learning Rate: 0.000117199
	LOSS [training: 0.6895101460156112 | validation: 0.4276509969212023]
	TIME [epoch: 6.47 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6912245328308046		[learning rate: 0.00011692]
	Learning Rate: 0.000116922
	LOSS [training: 0.6912245328308046 | validation: 0.4268303487886793]
	TIME [epoch: 6.48 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6915605699015791		[learning rate: 0.00011665]
	Learning Rate: 0.000116647
	LOSS [training: 0.6915605699015791 | validation: 0.43073979990194117]
	TIME [epoch: 6.52 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.692860229022703		[learning rate: 0.00011637]
	Learning Rate: 0.000116371
	LOSS [training: 0.692860229022703 | validation: 0.42840442599415085]
	TIME [epoch: 6.49 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.696784069317348		[learning rate: 0.0001161]
	Learning Rate: 0.000116097
	LOSS [training: 0.696784069317348 | validation: 0.4283469252484543]
	TIME [epoch: 6.48 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6946944274098804		[learning rate: 0.00011582]
	Learning Rate: 0.000115823
	LOSS [training: 0.6946944274098804 | validation: 0.4401472961751667]
	TIME [epoch: 6.47 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6922377737578767		[learning rate: 0.00011555]
	Learning Rate: 0.00011555
	LOSS [training: 0.6922377737578767 | validation: 0.4273215323144341]
	TIME [epoch: 6.47 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6936796883388368		[learning rate: 0.00011528]
	Learning Rate: 0.000115277
	LOSS [training: 0.6936796883388368 | validation: 0.42619671007684057]
	TIME [epoch: 6.49 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6921293519176412		[learning rate: 0.00011501]
	Learning Rate: 0.000115005
	LOSS [training: 0.6921293519176412 | validation: 0.4274370991408862]
	TIME [epoch: 6.51 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6941289488300401		[learning rate: 0.00011473]
	Learning Rate: 0.000114734
	LOSS [training: 0.6941289488300401 | validation: 0.434843474124222]
	TIME [epoch: 6.48 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6865030006673555		[learning rate: 0.00011446]
	Learning Rate: 0.000114463
	LOSS [training: 0.6865030006673555 | validation: 0.4325633796973431]
	TIME [epoch: 6.48 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6950602355956697		[learning rate: 0.00011419]
	Learning Rate: 0.000114193
	LOSS [training: 0.6950602355956697 | validation: 0.43186207235663304]
	TIME [epoch: 6.47 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6918388357909047		[learning rate: 0.00011392]
	Learning Rate: 0.000113924
	LOSS [training: 0.6918388357909047 | validation: 0.4306469258703164]
	TIME [epoch: 6.48 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6964458154111963		[learning rate: 0.00011366]
	Learning Rate: 0.000113655
	LOSS [training: 0.6964458154111963 | validation: 0.4254967746628558]
	TIME [epoch: 6.49 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6925114345693815		[learning rate: 0.00011339]
	Learning Rate: 0.000113387
	LOSS [training: 0.6925114345693815 | validation: 0.4306989070035615]
	TIME [epoch: 6.53 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6923428498231681		[learning rate: 0.00011312]
	Learning Rate: 0.00011312
	LOSS [training: 0.6923428498231681 | validation: 0.43364084360954136]
	TIME [epoch: 6.48 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6887987037583222		[learning rate: 0.00011285]
	Learning Rate: 0.000112853
	LOSS [training: 0.6887987037583222 | validation: 0.42700737291849994]
	TIME [epoch: 6.49 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6877675443781838		[learning rate: 0.00011259]
	Learning Rate: 0.000112587
	LOSS [training: 0.6877675443781838 | validation: 0.4341435846970972]
	TIME [epoch: 6.48 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6891260880263343		[learning rate: 0.00011232]
	Learning Rate: 0.000112321
	LOSS [training: 0.6891260880263343 | validation: 0.4307273579260268]
	TIME [epoch: 6.49 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6928692377986458		[learning rate: 0.00011206]
	Learning Rate: 0.000112056
	LOSS [training: 0.6928692377986458 | validation: 0.4362378394494582]
	TIME [epoch: 6.53 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6922211225896825		[learning rate: 0.00011179]
	Learning Rate: 0.000111792
	LOSS [training: 0.6922211225896825 | validation: 0.4373965088514962]
	TIME [epoch: 6.49 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6965078986024882		[learning rate: 0.00011153]
	Learning Rate: 0.000111528
	LOSS [training: 0.6965078986024882 | validation: 0.4322213781154854]
	TIME [epoch: 6.48 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6899961779917458		[learning rate: 0.00011127]
	Learning Rate: 0.000111265
	LOSS [training: 0.6899961779917458 | validation: 0.427262926030762]
	TIME [epoch: 6.49 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6916908491254066		[learning rate: 0.000111]
	Learning Rate: 0.000111003
	LOSS [training: 0.6916908491254066 | validation: 0.43505144358097353]
	TIME [epoch: 6.49 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.693735058882887		[learning rate: 0.00011074]
	Learning Rate: 0.000110741
	LOSS [training: 0.693735058882887 | validation: 0.4283118558368012]
	TIME [epoch: 6.48 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6912058139586776		[learning rate: 0.00011048]
	Learning Rate: 0.00011048
	LOSS [training: 0.6912058139586776 | validation: 0.4323129249403672]
	TIME [epoch: 6.53 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.691052795410242		[learning rate: 0.00011022]
	Learning Rate: 0.000110219
	LOSS [training: 0.691052795410242 | validation: 0.4260890795619084]
	TIME [epoch: 6.48 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6900628845244984		[learning rate: 0.00010996]
	Learning Rate: 0.000109959
	LOSS [training: 0.6900628845244984 | validation: 0.4319796014959223]
	TIME [epoch: 6.47 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6910416399091155		[learning rate: 0.0001097]
	Learning Rate: 0.0001097
	LOSS [training: 0.6910416399091155 | validation: 0.43196760695541525]
	TIME [epoch: 6.48 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6911268234411856		[learning rate: 0.00010944]
	Learning Rate: 0.000109441
	LOSS [training: 0.6911268234411856 | validation: 0.43510100351001085]
	TIME [epoch: 6.49 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6900625551903199		[learning rate: 0.00010918]
	Learning Rate: 0.000109183
	LOSS [training: 0.6900625551903199 | validation: 0.43271724801900024]
	TIME [epoch: 6.49 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6925717660739608		[learning rate: 0.00010893]
	Learning Rate: 0.000108925
	LOSS [training: 0.6925717660739608 | validation: 0.4340137060951307]
	TIME [epoch: 6.53 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6901578768270324		[learning rate: 0.00010867]
	Learning Rate: 0.000108668
	LOSS [training: 0.6901578768270324 | validation: 0.4223739603617594]
	TIME [epoch: 6.48 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6930359784107637		[learning rate: 0.00010841]
	Learning Rate: 0.000108412
	LOSS [training: 0.6930359784107637 | validation: 0.43246597197723924]
	TIME [epoch: 6.48 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.691530788280841		[learning rate: 0.00010816]
	Learning Rate: 0.000108156
	LOSS [training: 0.691530788280841 | validation: 0.4300002308972503]
	TIME [epoch: 6.47 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6923207380502114		[learning rate: 0.0001079]
	Learning Rate: 0.000107901
	LOSS [training: 0.6923207380502114 | validation: 0.43077169429395124]
	TIME [epoch: 6.48 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6927981654978398		[learning rate: 0.00010765]
	Learning Rate: 0.000107647
	LOSS [training: 0.6927981654978398 | validation: 0.431153534511655]
	TIME [epoch: 6.51 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6909181344700233		[learning rate: 0.00010739]
	Learning Rate: 0.000107393
	LOSS [training: 0.6909181344700233 | validation: 0.4374043392326349]
	TIME [epoch: 6.52 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6929750150938914		[learning rate: 0.00010714]
	Learning Rate: 0.000107139
	LOSS [training: 0.6929750150938914 | validation: 0.42590674789227173]
	TIME [epoch: 6.49 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6944651686595558		[learning rate: 0.00010689]
	Learning Rate: 0.000106887
	LOSS [training: 0.6944651686595558 | validation: 0.43043970788076985]
	TIME [epoch: 6.48 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6921233761609042		[learning rate: 0.00010663]
	Learning Rate: 0.000106634
	LOSS [training: 0.6921233761609042 | validation: 0.435079399302476]
	TIME [epoch: 6.48 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6943096595452036		[learning rate: 0.00010638]
	Learning Rate: 0.000106383
	LOSS [training: 0.6943096595452036 | validation: 0.4303597315846243]
	TIME [epoch: 6.49 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6976726053818629		[learning rate: 0.00010613]
	Learning Rate: 0.000106132
	LOSS [training: 0.6976726053818629 | validation: 0.42434667291777145]
	TIME [epoch: 6.53 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6921113884225008		[learning rate: 0.00010588]
	Learning Rate: 0.000105882
	LOSS [training: 0.6921113884225008 | validation: 0.43436266422221215]
	TIME [epoch: 6.51 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6933118879249832		[learning rate: 0.00010563]
	Learning Rate: 0.000105632
	LOSS [training: 0.6933118879249832 | validation: 0.439604342877587]
	TIME [epoch: 6.49 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6929156741461215		[learning rate: 0.00010538]
	Learning Rate: 0.000105383
	LOSS [training: 0.6929156741461215 | validation: 0.4321859170351666]
	TIME [epoch: 6.48 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6914207132659254		[learning rate: 0.00010513]
	Learning Rate: 0.000105134
	LOSS [training: 0.6914207132659254 | validation: 0.42987935380028286]
	TIME [epoch: 6.48 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6872705649345356		[learning rate: 0.00010489]
	Learning Rate: 0.000104886
	LOSS [training: 0.6872705649345356 | validation: 0.42396209056784967]
	TIME [epoch: 6.48 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6917738354080597		[learning rate: 0.00010464]
	Learning Rate: 0.000104639
	LOSS [training: 0.6917738354080597 | validation: 0.42811122438756083]
	TIME [epoch: 6.53 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6960062652668063		[learning rate: 0.00010439]
	Learning Rate: 0.000104392
	LOSS [training: 0.6960062652668063 | validation: 0.43494347411692835]
	TIME [epoch: 6.51 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.696979635313369		[learning rate: 0.00010415]
	Learning Rate: 0.000104146
	LOSS [training: 0.696979635313369 | validation: 0.431282050274543]
	TIME [epoch: 6.48 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6921580542531822		[learning rate: 0.0001039]
	Learning Rate: 0.0001039
	LOSS [training: 0.6921580542531822 | validation: 0.43628036776215817]
	TIME [epoch: 6.47 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.695227707291244		[learning rate: 0.00010365]
	Learning Rate: 0.000103655
	LOSS [training: 0.695227707291244 | validation: 0.4292658345304421]
	TIME [epoch: 6.48 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.690396077464555		[learning rate: 0.00010341]
	Learning Rate: 0.00010341
	LOSS [training: 0.690396077464555 | validation: 0.4324325780174606]
	TIME [epoch: 6.49 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6919920287490028		[learning rate: 0.00010317]
	Learning Rate: 0.000103166
	LOSS [training: 0.6919920287490028 | validation: 0.42535848546780375]
	TIME [epoch: 6.52 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6949453290788269		[learning rate: 0.00010292]
	Learning Rate: 0.000102923
	LOSS [training: 0.6949453290788269 | validation: 0.43671828396927903]
	TIME [epoch: 6.48 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.69384341928261		[learning rate: 0.00010268]
	Learning Rate: 0.00010268
	LOSS [training: 0.69384341928261 | validation: 0.42632290146059554]
	TIME [epoch: 6.5 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6904892201112119		[learning rate: 0.00010244]
	Learning Rate: 0.000102438
	LOSS [training: 0.6904892201112119 | validation: 0.4350705477798193]
	TIME [epoch: 6.48 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6932672737881368		[learning rate: 0.0001022]
	Learning Rate: 0.000102196
	LOSS [training: 0.6932672737881368 | validation: 0.4334172401528973]
	TIME [epoch: 6.48 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.692119471844799		[learning rate: 0.00010196]
	Learning Rate: 0.000101955
	LOSS [training: 0.692119471844799 | validation: 0.42961365015351405]
	TIME [epoch: 6.49 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6922366957808819		[learning rate: 0.00010171]
	Learning Rate: 0.000101715
	LOSS [training: 0.6922366957808819 | validation: 0.4319268585102032]
	TIME [epoch: 6.51 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6935691825912824		[learning rate: 0.00010147]
	Learning Rate: 0.000101475
	LOSS [training: 0.6935691825912824 | validation: 0.42825180443929056]
	TIME [epoch: 6.48 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6928550810696906		[learning rate: 0.00010124]
	Learning Rate: 0.000101236
	LOSS [training: 0.6928550810696906 | validation: 0.4280146567614606]
	TIME [epoch: 6.48 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.693202043286256		[learning rate: 0.000101]
	Learning Rate: 0.000100997
	LOSS [training: 0.693202043286256 | validation: 0.43090041545221575]
	TIME [epoch: 6.47 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.694453863090951		[learning rate: 0.00010076]
	Learning Rate: 0.000100759
	LOSS [training: 0.694453863090951 | validation: 0.427921210467545]
	TIME [epoch: 6.47 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6929023052538358		[learning rate: 0.00010052]
	Learning Rate: 0.000100521
	LOSS [training: 0.6929023052538358 | validation: 0.4292333782212521]
	TIME [epoch: 6.53 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6900609565053888		[learning rate: 0.00010028]
	Learning Rate: 0.000100284
	LOSS [training: 0.6900609565053888 | validation: 0.4298141048811661]
	TIME [epoch: 6.49 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6921557417388502		[learning rate: 0.00010005]
	Learning Rate: 0.000100047
	LOSS [training: 0.6921557417388502 | validation: 0.43518498092579583]
	TIME [epoch: 6.48 sec]
Finished training in 13219.749 seconds.
