Args:
Namespace(name='model_phi1_1a_v2', outdir='out/model_training/model_phi1_1a_v2', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=200, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2774159062

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.634646127506421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.634646127506421 | validation: 12.004099773577423]
	TIME [epoch: 150 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.123773685491221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.123773685491221 | validation: 11.808338227266965]
	TIME [epoch: 6.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.776101331969404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.776101331969404 | validation: 11.446893880192867]
	TIME [epoch: 6.66 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.338557533072711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.338557533072711 | validation: 11.319368939314998]
	TIME [epoch: 6.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.027563014192019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.027563014192019 | validation: 11.302257878414313]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.953418193595484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.953418193595484 | validation: 11.132190562119401]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.64540521708966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.64540521708966 | validation: 10.85006996648199]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.43081344623797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.43081344623797 | validation: 10.829854457513829]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.320872413212365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.320872413212365 | validation: 10.887272617921866]
	TIME [epoch: 6.66 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.097137689614911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.097137689614911 | validation: 10.655286864207149]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.061667305767005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.061667305767005 | validation: 10.543825200073005]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.854993916775335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.854993916775335 | validation: 10.420965600607344]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.02003394516037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.02003394516037 | validation: 10.507806873573443]
	TIME [epoch: 6.61 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.51676590814024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.51676590814024 | validation: 9.501631331535041]
	TIME [epoch: 6.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.325196382996463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.325196382996463 | validation: 9.965680833977885]
	TIME [epoch: 6.64 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.254544630225563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.254544630225563 | validation: 8.83432564100234]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.005520051733773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.005520051733773 | validation: 9.524896461977978]
	TIME [epoch: 6.6 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.034170620367867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.034170620367867 | validation: 9.016102136785747]
	TIME [epoch: 6.6 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.646051079086678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.646051079086678 | validation: 9.384799997692607]
	TIME [epoch: 6.61 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.026544101107334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.026544101107334 | validation: 9.13985313308439]
	TIME [epoch: 6.64 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.895811659071822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.895811659071822 | validation: 9.03210337851257]
	TIME [epoch: 6.6 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.484994294703209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.484994294703209 | validation: 8.48284854364556]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.614255923403379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.614255923403379 | validation: 8.429675371408749]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.507562897125534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.507562897125534 | validation: 8.813180262439056]
	TIME [epoch: 6.6 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.629933834779917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.629933834779917 | validation: 8.960881964051465]
	TIME [epoch: 6.62 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.467695889953629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.467695889953629 | validation: 8.780984154136185]
	TIME [epoch: 6.61 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.248247705146551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.248247705146551 | validation: 9.064703363468336]
	TIME [epoch: 6.6 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.538237306790142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.538237306790142 | validation: 9.092388210370263]
	TIME [epoch: 6.61 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.278251263092464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.278251263092464 | validation: 9.149019390767364]
	TIME [epoch: 6.6 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.105381679126022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.105381679126022 | validation: 8.096158200543112]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.636899081430532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.636899081430532 | validation: 8.79148956208764]
	TIME [epoch: 6.64 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.877250295664188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.877250295664188 | validation: 7.879169967351868]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.776754944047346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.776754944047346 | validation: 8.507427434264056]
	TIME [epoch: 6.6 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.852079509613935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.852079509613935 | validation: 8.932703533642023]
	TIME [epoch: 6.6 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.998358619058121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.998358619058121 | validation: 7.798246377976907]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.4386283227160375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.4386283227160375 | validation: 8.130318783910415]
	TIME [epoch: 6.68 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.6352868929484305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.6352868929484305 | validation: 7.6956319123942105]
	TIME [epoch: 6.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.329388966853458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.329388966853458 | validation: 8.113296038501304]
	TIME [epoch: 6.65 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.269995038106058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.269995038106058 | validation: 7.006400769768723]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.387159637092262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.387159637092262 | validation: 6.769711712047242]
	TIME [epoch: 6.66 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.191833600901453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.191833600901453 | validation: 9.690834709489973]
	TIME [epoch: 6.65 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.377864697800643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.377864697800643 | validation: 10.764323608236216]
	TIME [epoch: 6.68 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.333837290378821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.333837290378821 | validation: 10.55170086833392]
	TIME [epoch: 6.64 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.733025255766043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.733025255766043 | validation: 7.669569203704804]
	TIME [epoch: 6.64 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.077381363643793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.077381363643793 | validation: 7.001941202340157]
	TIME [epoch: 6.64 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.318409764715265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.318409764715265 | validation: 6.739711920049806]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.679351443885613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.679351443885613 | validation: 6.625517598764167]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.3011944017138894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3011944017138894 | validation: 10.368516685241376]
	TIME [epoch: 6.68 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.42456403853471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.42456403853471 | validation: 7.070229036150163]
	TIME [epoch: 6.65 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.753544633826399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.753544633826399 | validation: 6.739609483838258]
	TIME [epoch: 6.65 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.155409654130157		[learning rate: 0.0099811]
	Learning Rate: 0.00998112
	LOSS [training: 6.155409654130157 | validation: 6.510335215655992]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.530282910926856		[learning rate: 0.0099576]
	Learning Rate: 0.00995758
	LOSS [training: 5.530282910926856 | validation: 6.371585728682043]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.148505413295077		[learning rate: 0.0099341]
	Learning Rate: 0.00993409
	LOSS [training: 6.148505413295077 | validation: 5.739105514474372]
	TIME [epoch: 6.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.831224463974954		[learning rate: 0.0099107]
	Learning Rate: 0.00991066
	LOSS [training: 5.831224463974954 | validation: 7.691859396888096]
	TIME [epoch: 6.66 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.363011314047393		[learning rate: 0.0098873]
	Learning Rate: 0.00988728
	LOSS [training: 6.363011314047393 | validation: 6.624126261534391]
	TIME [epoch: 6.64 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.9692320374540495		[learning rate: 0.009864]
	Learning Rate: 0.00986396
	LOSS [training: 5.9692320374540495 | validation: 6.710747878918495]
	TIME [epoch: 6.65 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.772480600006101		[learning rate: 0.0098407]
	Learning Rate: 0.00984069
	LOSS [training: 5.772480600006101 | validation: 6.858420148817159]
	TIME [epoch: 6.65 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.193725411863645		[learning rate: 0.0098175]
	Learning Rate: 0.00981748
	LOSS [training: 6.193725411863645 | validation: 6.436084816556312]
	TIME [epoch: 6.65 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.273371169904451		[learning rate: 0.0097943]
	Learning Rate: 0.00979432
	LOSS [training: 5.273371169904451 | validation: 5.818639629244765]
	TIME [epoch: 6.67 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.035966426555926		[learning rate: 0.0097712]
	Learning Rate: 0.00977122
	LOSS [training: 5.035966426555926 | validation: 5.74859499891482]
	TIME [epoch: 6.65 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.816155911980337		[learning rate: 0.0097482]
	Learning Rate: 0.00974817
	LOSS [training: 4.816155911980337 | validation: 5.491875726089157]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.744351580894394		[learning rate: 0.0097252]
	Learning Rate: 0.00972517
	LOSS [training: 4.744351580894394 | validation: 5.707931804758847]
	TIME [epoch: 6.64 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7345041515533195		[learning rate: 0.0097022]
	Learning Rate: 0.00970223
	LOSS [training: 4.7345041515533195 | validation: 5.565888298533608]
	TIME [epoch: 6.64 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.639943555768475		[learning rate: 0.0096793]
	Learning Rate: 0.00967935
	LOSS [training: 4.639943555768475 | validation: 6.027710869337944]
	TIME [epoch: 6.67 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.89131154348871		[learning rate: 0.0096565]
	Learning Rate: 0.00965652
	LOSS [training: 4.89131154348871 | validation: 5.724855464185952]
	TIME [epoch: 6.65 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.750384868277129		[learning rate: 0.0096337]
	Learning Rate: 0.00963374
	LOSS [training: 4.750384868277129 | validation: 5.278482401320552]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.309739551794088		[learning rate: 0.009611]
	Learning Rate: 0.00961101
	LOSS [training: 4.309739551794088 | validation: 5.544794051668472]
	TIME [epoch: 6.63 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.320422024042732		[learning rate: 0.0095883]
	Learning Rate: 0.00958834
	LOSS [training: 4.320422024042732 | validation: 5.207411219455365]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.316856499623358		[learning rate: 0.0095657]
	Learning Rate: 0.00956573
	LOSS [training: 4.316856499623358 | validation: 5.613602316010766]
	TIME [epoch: 6.65 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.193882716312357		[learning rate: 0.0095432]
	Learning Rate: 0.00954316
	LOSS [training: 4.193882716312357 | validation: 5.211237380270603]
	TIME [epoch: 6.67 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.407788238749207		[learning rate: 0.0095207]
	Learning Rate: 0.00952065
	LOSS [training: 4.407788238749207 | validation: 5.173787077796853]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.42981882246674		[learning rate: 0.0094982]
	Learning Rate: 0.00949819
	LOSS [training: 4.42981882246674 | validation: 5.170511459617307]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.100666917256096		[learning rate: 0.0094758]
	Learning Rate: 0.00947579
	LOSS [training: 4.100666917256096 | validation: 6.397974311358477]
	TIME [epoch: 6.63 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.343617434646252		[learning rate: 0.0094534]
	Learning Rate: 0.00945344
	LOSS [training: 4.343617434646252 | validation: 5.324687570833154]
	TIME [epoch: 6.63 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.049155288796134		[learning rate: 0.0094311]
	Learning Rate: 0.00943114
	LOSS [training: 4.049155288796134 | validation: 5.297240102291379]
	TIME [epoch: 6.67 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.041003365023771		[learning rate: 0.0094089]
	Learning Rate: 0.00940889
	LOSS [training: 4.041003365023771 | validation: 5.34266528239255]
	TIME [epoch: 6.64 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9468933556450687		[learning rate: 0.0093867]
	Learning Rate: 0.0093867
	LOSS [training: 3.9468933556450687 | validation: 5.330352291232481]
	TIME [epoch: 6.63 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.211572651504383		[learning rate: 0.0093646]
	Learning Rate: 0.00936456
	LOSS [training: 4.211572651504383 | validation: 5.366228244729248]
	TIME [epoch: 6.62 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.016827425641307		[learning rate: 0.0093425]
	Learning Rate: 0.00934247
	LOSS [training: 4.016827425641307 | validation: 5.418846974002982]
	TIME [epoch: 6.61 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.903291485285996		[learning rate: 0.0093204]
	Learning Rate: 0.00932043
	LOSS [training: 3.903291485285996 | validation: 5.038713966776907]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.113870443992378		[learning rate: 0.0092984]
	Learning Rate: 0.00929844
	LOSS [training: 4.113870443992378 | validation: 5.0019891222011825]
	TIME [epoch: 6.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8251117731655357		[learning rate: 0.0092765]
	Learning Rate: 0.00927651
	LOSS [training: 3.8251117731655357 | validation: 5.154427812131466]
	TIME [epoch: 6.63 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.100760576254149		[learning rate: 0.0092546]
	Learning Rate: 0.00925463
	LOSS [training: 4.100760576254149 | validation: 5.073296659390879]
	TIME [epoch: 6.64 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9858358715598365		[learning rate: 0.0092328]
	Learning Rate: 0.0092328
	LOSS [training: 3.9858358715598365 | validation: 5.227331368253177]
	TIME [epoch: 6.63 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8852245919389583		[learning rate: 0.009211]
	Learning Rate: 0.00921102
	LOSS [training: 3.8852245919389583 | validation: 4.812411635079155]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.709248748631068		[learning rate: 0.0091893]
	Learning Rate: 0.00918929
	LOSS [training: 3.709248748631068 | validation: 5.0522906392863405]
	TIME [epoch: 6.66 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6897738797140875		[learning rate: 0.0091676]
	Learning Rate: 0.00916762
	LOSS [training: 3.6897738797140875 | validation: 4.589869639609924]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515979566453052		[learning rate: 0.009146]
	Learning Rate: 0.00914599
	LOSS [training: 3.515979566453052 | validation: 4.640482179147353]
	TIME [epoch: 6.63 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5480333088803384		[learning rate: 0.0091244]
	Learning Rate: 0.00912442
	LOSS [training: 3.5480333088803384 | validation: 4.423850505630812]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4605604393988		[learning rate: 0.0091029]
	Learning Rate: 0.00910289
	LOSS [training: 3.4605604393988 | validation: 4.1693639515875125]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3765629553679566		[learning rate: 0.0090814]
	Learning Rate: 0.00908142
	LOSS [training: 3.3765629553679566 | validation: 4.009939822375215]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2539420046993692		[learning rate: 0.00906]
	Learning Rate: 0.00906
	LOSS [training: 3.2539420046993692 | validation: 4.241687118482444]
	TIME [epoch: 6.65 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3323593115567363		[learning rate: 0.0090386]
	Learning Rate: 0.00903863
	LOSS [training: 3.3323593115567363 | validation: 4.3870858048847]
	TIME [epoch: 6.62 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6512684929462713		[learning rate: 0.0090173]
	Learning Rate: 0.00901731
	LOSS [training: 3.6512684929462713 | validation: 3.0721848226980972]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5590879619955276		[learning rate: 0.008996]
	Learning Rate: 0.00899604
	LOSS [training: 2.5590879619955276 | validation: 2.7414430568325514]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.609966130755237		[learning rate: 0.0089748]
	Learning Rate: 0.00897482
	LOSS [training: 2.609966130755237 | validation: 2.790855039251105]
	TIME [epoch: 6.62 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8157709017781065		[learning rate: 0.0089536]
	Learning Rate: 0.00895365
	LOSS [training: 2.8157709017781065 | validation: 3.7937581856569533]
	TIME [epoch: 6.62 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8630489419776217		[learning rate: 0.0089325]
	Learning Rate: 0.00893253
	LOSS [training: 2.8630489419776217 | validation: 2.384072525775667]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4585433213122294		[learning rate: 0.0089115]
	Learning Rate: 0.00891146
	LOSS [training: 2.4585433213122294 | validation: 3.185082660834285]
	TIME [epoch: 6.61 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.464019276720921		[learning rate: 0.0088904]
	Learning Rate: 0.00889044
	LOSS [training: 2.464019276720921 | validation: 3.0471184752196283]
	TIME [epoch: 6.61 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.533937622985952		[learning rate: 0.0088695]
	Learning Rate: 0.00886946
	LOSS [training: 2.533937622985952 | validation: 3.6706976821384636]
	TIME [epoch: 6.61 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5354441405469395		[learning rate: 0.0088485]
	Learning Rate: 0.00884854
	LOSS [training: 2.5354441405469395 | validation: 2.7431664365491644]
	TIME [epoch: 6.61 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3214895261816446		[learning rate: 0.0088277]
	Learning Rate: 0.00882767
	LOSS [training: 2.3214895261816446 | validation: 2.2198128640054264]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2353251070152877		[learning rate: 0.0088068]
	Learning Rate: 0.00880685
	LOSS [training: 2.2353251070152877 | validation: 2.0841546577024896]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5319551665079993		[learning rate: 0.0087861]
	Learning Rate: 0.00878607
	LOSS [training: 2.5319551665079993 | validation: 2.348510237228586]
	TIME [epoch: 6.61 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.635987294208332		[learning rate: 0.0087653]
	Learning Rate: 0.00876535
	LOSS [training: 2.635987294208332 | validation: 2.671924480019417]
	TIME [epoch: 6.61 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2254438127679004		[learning rate: 0.0087447]
	Learning Rate: 0.00874467
	LOSS [training: 2.2254438127679004 | validation: 2.112864313615928]
	TIME [epoch: 6.61 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2858681358898187		[learning rate: 0.008724]
	Learning Rate: 0.00872405
	LOSS [training: 2.2858681358898187 | validation: 2.180356537675491]
	TIME [epoch: 6.61 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.085882239875626		[learning rate: 0.0087035]
	Learning Rate: 0.00870347
	LOSS [training: 2.085882239875626 | validation: 2.6661190510869766]
	TIME [epoch: 6.61 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.35384644530048		[learning rate: 0.0086829]
	Learning Rate: 0.00868294
	LOSS [training: 2.35384644530048 | validation: 1.8490567725773772]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2378988653902745		[learning rate: 0.0086625]
	Learning Rate: 0.00866246
	LOSS [training: 2.2378988653902745 | validation: 1.8166438038002362]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3827648969937805		[learning rate: 0.008642]
	Learning Rate: 0.00864202
	LOSS [training: 2.3827648969937805 | validation: 3.7566869388123187]
	TIME [epoch: 6.61 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1263277669627074		[learning rate: 0.0086216]
	Learning Rate: 0.00862164
	LOSS [training: 2.1263277669627074 | validation: 1.9018756734869011]
	TIME [epoch: 6.61 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.263295019017824		[learning rate: 0.0086013]
	Learning Rate: 0.0086013
	LOSS [training: 2.263295019017824 | validation: 2.2136745369062654]
	TIME [epoch: 6.61 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.379192803385524		[learning rate: 0.008581]
	Learning Rate: 0.00858101
	LOSS [training: 2.379192803385524 | validation: 2.075738681763289]
	TIME [epoch: 6.62 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9857430536356493		[learning rate: 0.0085608]
	Learning Rate: 0.00856077
	LOSS [training: 1.9857430536356493 | validation: 2.29918480411981]
	TIME [epoch: 6.65 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1335269747457635		[learning rate: 0.0085406]
	Learning Rate: 0.00854058
	LOSS [training: 2.1335269747457635 | validation: 1.8273056745029466]
	TIME [epoch: 6.61 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0311741468174533		[learning rate: 0.0085204]
	Learning Rate: 0.00852043
	LOSS [training: 2.0311741468174533 | validation: 2.2227222918291676]
	TIME [epoch: 6.61 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0685551192396137		[learning rate: 0.0085003]
	Learning Rate: 0.00850033
	LOSS [training: 2.0685551192396137 | validation: 1.7145827498808766]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.144894974082649		[learning rate: 0.0084803]
	Learning Rate: 0.00848028
	LOSS [training: 2.144894974082649 | validation: 1.9475359349742918]
	TIME [epoch: 6.6 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.89172245252137		[learning rate: 0.0084603]
	Learning Rate: 0.00846028
	LOSS [training: 1.89172245252137 | validation: 1.84706763079167]
	TIME [epoch: 6.61 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2032722821348796		[learning rate: 0.0084403]
	Learning Rate: 0.00844032
	LOSS [training: 2.2032722821348796 | validation: 2.7748182421947396]
	TIME [epoch: 6.65 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2516625573272604		[learning rate: 0.0084204]
	Learning Rate: 0.00842041
	LOSS [training: 2.2516625573272604 | validation: 2.6236314376142653]
	TIME [epoch: 6.6 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.027120977155115		[learning rate: 0.0084005]
	Learning Rate: 0.00840055
	LOSS [training: 2.027120977155115 | validation: 1.8929330791164503]
	TIME [epoch: 6.6 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7311204249627914		[learning rate: 0.0083807]
	Learning Rate: 0.00838073
	LOSS [training: 1.7311204249627914 | validation: 2.6497132472153555]
	TIME [epoch: 6.6 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.57619942728963		[learning rate: 0.008361]
	Learning Rate: 0.00836096
	LOSS [training: 2.57619942728963 | validation: 3.888220310232106]
	TIME [epoch: 6.61 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.487758345485271		[learning rate: 0.0083412]
	Learning Rate: 0.00834124
	LOSS [training: 2.487758345485271 | validation: 1.7792369774581482]
	TIME [epoch: 6.61 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3096745741764724		[learning rate: 0.0083216]
	Learning Rate: 0.00832157
	LOSS [training: 2.3096745741764724 | validation: 2.0308502327521207]
	TIME [epoch: 6.65 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7217125770987232		[learning rate: 0.0083019]
	Learning Rate: 0.00830194
	LOSS [training: 1.7217125770987232 | validation: 2.164173243087222]
	TIME [epoch: 6.6 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9833947929908402		[learning rate: 0.0082824]
	Learning Rate: 0.00828236
	LOSS [training: 1.9833947929908402 | validation: 1.7461678038367627]
	TIME [epoch: 6.61 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.247497866996086		[learning rate: 0.0082628]
	Learning Rate: 0.00826282
	LOSS [training: 2.247497866996086 | validation: 3.003286955967266]
	TIME [epoch: 6.6 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.87578992484931		[learning rate: 0.0082433]
	Learning Rate: 0.00824333
	LOSS [training: 1.87578992484931 | validation: 1.7995006287918702]
	TIME [epoch: 6.6 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.147569538229246		[learning rate: 0.0082239]
	Learning Rate: 0.00822388
	LOSS [training: 2.147569538229246 | validation: 2.628603761902532]
	TIME [epoch: 6.61 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8584662051982304		[learning rate: 0.0082045]
	Learning Rate: 0.00820448
	LOSS [training: 1.8584662051982304 | validation: 2.7433016349523576]
	TIME [epoch: 6.64 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.893766718194657		[learning rate: 0.0081851]
	Learning Rate: 0.00818513
	LOSS [training: 1.893766718194657 | validation: 1.8983103645759956]
	TIME [epoch: 6.61 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6925811613132837		[learning rate: 0.0081658]
	Learning Rate: 0.00816582
	LOSS [training: 1.6925811613132837 | validation: 1.845274824383587]
	TIME [epoch: 6.61 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7087182712598643		[learning rate: 0.0081466]
	Learning Rate: 0.00814656
	LOSS [training: 1.7087182712598643 | validation: 2.0768591434115313]
	TIME [epoch: 6.6 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8083772221655692		[learning rate: 0.0081273]
	Learning Rate: 0.00812735
	LOSS [training: 1.8083772221655692 | validation: 1.525659078707898]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7229387262130533		[learning rate: 0.0081082]
	Learning Rate: 0.00810817
	LOSS [training: 1.7229387262130533 | validation: 2.346847252220493]
	TIME [epoch: 6.62 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.227735403086565		[learning rate: 0.008089]
	Learning Rate: 0.00808905
	LOSS [training: 2.227735403086565 | validation: 2.5283213071409723]
	TIME [epoch: 6.63 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8760074767732866		[learning rate: 0.00807]
	Learning Rate: 0.00806997
	LOSS [training: 1.8760074767732866 | validation: 2.910167926938956]
	TIME [epoch: 6.59 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8656742276645961		[learning rate: 0.0080509]
	Learning Rate: 0.00805093
	LOSS [training: 1.8656742276645961 | validation: 1.9492917817539426]
	TIME [epoch: 6.6 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6539113113226667		[learning rate: 0.0080319]
	Learning Rate: 0.00803194
	LOSS [training: 1.6539113113226667 | validation: 2.0168541812665284]
	TIME [epoch: 6.6 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6475613434153775		[learning rate: 0.008013]
	Learning Rate: 0.00801299
	LOSS [training: 1.6475613434153775 | validation: 1.4166290184733263]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7390154761895267		[learning rate: 0.0079941]
	Learning Rate: 0.00799409
	LOSS [training: 1.7390154761895267 | validation: 7.107147213980996]
	TIME [epoch: 6.67 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.321934196666833		[learning rate: 0.0079752]
	Learning Rate: 0.00797524
	LOSS [training: 5.321934196666833 | validation: 2.532779617539566]
	TIME [epoch: 6.63 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0014603506414796		[learning rate: 0.0079564]
	Learning Rate: 0.00795642
	LOSS [training: 2.0014603506414796 | validation: 2.037931900748144]
	TIME [epoch: 6.63 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2943850050400085		[learning rate: 0.0079377]
	Learning Rate: 0.00793766
	LOSS [training: 3.2943850050400085 | validation: 1.9626777575023713]
	TIME [epoch: 6.62 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8144193667231896		[learning rate: 0.0079189]
	Learning Rate: 0.00791893
	LOSS [training: 1.8144193667231896 | validation: 1.47332170712036]
	TIME [epoch: 6.63 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4703230943244727		[learning rate: 0.0079003]
	Learning Rate: 0.00790025
	LOSS [training: 1.4703230943244727 | validation: 1.800382941732381]
	TIME [epoch: 6.62 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5830952588590574		[learning rate: 0.0078816]
	Learning Rate: 0.00788162
	LOSS [training: 1.5830952588590574 | validation: 1.5911045964304154]
	TIME [epoch: 6.67 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5949391626267437		[learning rate: 0.007863]
	Learning Rate: 0.00786303
	LOSS [training: 1.5949391626267437 | validation: 1.3752908403626827]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7145787244329682		[learning rate: 0.0078445]
	Learning Rate: 0.00784448
	LOSS [training: 1.7145787244329682 | validation: 2.190837011076207]
	TIME [epoch: 6.62 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9647198141882132		[learning rate: 0.007826]
	Learning Rate: 0.00782597
	LOSS [training: 1.9647198141882132 | validation: 1.9852760632729476]
	TIME [epoch: 6.63 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5547235910445727		[learning rate: 0.0078075]
	Learning Rate: 0.00780751
	LOSS [training: 1.5547235910445727 | validation: 2.5840093205504626]
	TIME [epoch: 6.63 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.80309646633209		[learning rate: 0.0077891]
	Learning Rate: 0.0077891
	LOSS [training: 1.80309646633209 | validation: 1.8162747871996134]
	TIME [epoch: 6.63 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7039028674745356		[learning rate: 0.0077707]
	Learning Rate: 0.00777072
	LOSS [training: 1.7039028674745356 | validation: 1.6556696783740965]
	TIME [epoch: 6.67 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7375366805267625		[learning rate: 0.0077524]
	Learning Rate: 0.00775239
	LOSS [training: 1.7375366805267625 | validation: 1.6215277385904645]
	TIME [epoch: 6.64 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7036199019239127		[learning rate: 0.0077341]
	Learning Rate: 0.00773411
	LOSS [training: 1.7036199019239127 | validation: 1.757160074343276]
	TIME [epoch: 6.63 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5339237427003454		[learning rate: 0.0077159]
	Learning Rate: 0.00771586
	LOSS [training: 1.5339237427003454 | validation: 1.3756034424594694]
	TIME [epoch: 6.63 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6808033984119937		[learning rate: 0.0076977]
	Learning Rate: 0.00769766
	LOSS [training: 1.6808033984119937 | validation: 1.7071023247323085]
	TIME [epoch: 6.63 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6755489933105132		[learning rate: 0.0076795]
	Learning Rate: 0.00767951
	LOSS [training: 1.6755489933105132 | validation: 1.3961376000921328]
	TIME [epoch: 6.63 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5818287111180442		[learning rate: 0.0076614]
	Learning Rate: 0.00766139
	LOSS [training: 1.5818287111180442 | validation: 1.59659588802048]
	TIME [epoch: 6.66 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3781482098037316		[learning rate: 0.0076433]
	Learning Rate: 0.00764332
	LOSS [training: 2.3781482098037316 | validation: 2.029722810183696]
	TIME [epoch: 6.63 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1213057126949115		[learning rate: 0.0076253]
	Learning Rate: 0.00762529
	LOSS [training: 2.1213057126949115 | validation: 2.0712335403729103]
	TIME [epoch: 6.62 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6400464163643744		[learning rate: 0.0076073]
	Learning Rate: 0.0076073
	LOSS [training: 1.6400464163643744 | validation: 1.6920676317433214]
	TIME [epoch: 6.63 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5761269624329959		[learning rate: 0.0075894]
	Learning Rate: 0.00758936
	LOSS [training: 1.5761269624329959 | validation: 2.0342126122312063]
	TIME [epoch: 6.63 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4790935553919355		[learning rate: 0.0075715]
	Learning Rate: 0.00757146
	LOSS [training: 1.4790935553919355 | validation: 1.3034356489717318]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4883994112818608		[learning rate: 0.0075536]
	Learning Rate: 0.0075536
	LOSS [training: 1.4883994112818608 | validation: 1.5848789666335654]
	TIME [epoch: 6.67 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7609663178721786		[learning rate: 0.0075358]
	Learning Rate: 0.00753578
	LOSS [training: 1.7609663178721786 | validation: 1.9844879686591796]
	TIME [epoch: 6.64 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.514339514336017		[learning rate: 0.007518]
	Learning Rate: 0.007518
	LOSS [training: 1.514339514336017 | validation: 2.3140394106438595]
	TIME [epoch: 6.64 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6202011252019777		[learning rate: 0.0075003]
	Learning Rate: 0.00750027
	LOSS [training: 1.6202011252019777 | validation: 1.5200701171439033]
	TIME [epoch: 6.63 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4152981955901975		[learning rate: 0.0074826]
	Learning Rate: 0.00748258
	LOSS [training: 1.4152981955901975 | validation: 1.5457134570550142]
	TIME [epoch: 6.64 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4691286375465358		[learning rate: 0.0074649]
	Learning Rate: 0.00746493
	LOSS [training: 1.4691286375465358 | validation: 1.2200784451013234]
	TIME [epoch: 6.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4585360849044253		[learning rate: 0.0074473]
	Learning Rate: 0.00744732
	LOSS [training: 1.4585360849044253 | validation: 2.3814090990172554]
	TIME [epoch: 6.64 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8484874963431637		[learning rate: 0.0074298]
	Learning Rate: 0.00742975
	LOSS [training: 1.8484874963431637 | validation: 1.7849718333313747]
	TIME [epoch: 6.61 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.458418985535977		[learning rate: 0.0074122]
	Learning Rate: 0.00741223
	LOSS [training: 1.458418985535977 | validation: 1.326218926504389]
	TIME [epoch: 6.61 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.668338813058923		[learning rate: 0.0073947]
	Learning Rate: 0.00739474
	LOSS [training: 1.668338813058923 | validation: 1.4415687936896404]
	TIME [epoch: 6.61 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.497009443328643		[learning rate: 0.0073773]
	Learning Rate: 0.0073773
	LOSS [training: 1.497009443328643 | validation: 1.967001752842092]
	TIME [epoch: 6.61 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6247861901456737		[learning rate: 0.0073599]
	Learning Rate: 0.0073599
	LOSS [training: 1.6247861901456737 | validation: 1.316998869379045]
	TIME [epoch: 6.64 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3660128047839646		[learning rate: 0.0073425]
	Learning Rate: 0.00734254
	LOSS [training: 1.3660128047839646 | validation: 1.3334487679678353]
	TIME [epoch: 6.63 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3089820059647401		[learning rate: 0.0073252]
	Learning Rate: 0.00732522
	LOSS [training: 1.3089820059647401 | validation: 2.6231160336209394]
	TIME [epoch: 6.61 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1587097710871586		[learning rate: 0.0073079]
	Learning Rate: 0.00730794
	LOSS [training: 2.1587097710871586 | validation: 2.8762517823436027]
	TIME [epoch: 6.61 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9580762569138415		[learning rate: 0.0072907]
	Learning Rate: 0.0072907
	LOSS [training: 1.9580762569138415 | validation: 1.634978058630451]
	TIME [epoch: 6.61 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.552248226902246		[learning rate: 0.0072735]
	Learning Rate: 0.0072735
	LOSS [training: 1.552248226902246 | validation: 2.0817801307291433]
	TIME [epoch: 6.62 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4811043582849264		[learning rate: 0.0072563]
	Learning Rate: 0.00725635
	LOSS [training: 1.4811043582849264 | validation: 1.7048164724746815]
	TIME [epoch: 6.64 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4105178509331808		[learning rate: 0.0072392]
	Learning Rate: 0.00723923
	LOSS [training: 1.4105178509331808 | validation: 1.6149535983530847]
	TIME [epoch: 6.63 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3997360091174573		[learning rate: 0.0072222]
	Learning Rate: 0.00722215
	LOSS [training: 1.3997360091174573 | validation: 1.421285756940299]
	TIME [epoch: 6.61 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7256495230531783		[learning rate: 0.0072051]
	Learning Rate: 0.00720512
	LOSS [training: 1.7256495230531783 | validation: 1.444311990765344]
	TIME [epoch: 6.61 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.441044121035178		[learning rate: 0.0071881]
	Learning Rate: 0.00718812
	LOSS [training: 1.441044121035178 | validation: 1.5515827577017172]
	TIME [epoch: 6.61 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9401930348507144		[learning rate: 0.0071712]
	Learning Rate: 0.00717117
	LOSS [training: 1.9401930348507144 | validation: 2.231199875838732]
	TIME [epoch: 6.61 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6269309099423683		[learning rate: 0.0071542]
	Learning Rate: 0.00715425
	LOSS [training: 1.6269309099423683 | validation: 1.2099327223739105]
	TIME [epoch: 6.66 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2351741441489046		[learning rate: 0.0071374]
	Learning Rate: 0.00713737
	LOSS [training: 1.2351741441489046 | validation: 1.2392073423289476]
	TIME [epoch: 6.62 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.332627197727522		[learning rate: 0.0071205]
	Learning Rate: 0.00712054
	LOSS [training: 1.332627197727522 | validation: 1.3602906547010256]
	TIME [epoch: 6.62 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2293644152641499		[learning rate: 0.0071037]
	Learning Rate: 0.00710374
	LOSS [training: 1.2293644152641499 | validation: 1.4283894426877115]
	TIME [epoch: 6.62 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3411150378787215		[learning rate: 0.007087]
	Learning Rate: 0.00708698
	LOSS [training: 1.3411150378787215 | validation: 1.167997874491942]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6252169135553038		[learning rate: 0.0070703]
	Learning Rate: 0.00707027
	LOSS [training: 1.6252169135553038 | validation: 2.6941395783147377]
	TIME [epoch: 6.63 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8596677482390205		[learning rate: 0.0070536]
	Learning Rate: 0.00705359
	LOSS [training: 1.8596677482390205 | validation: 1.2191600518141514]
	TIME [epoch: 6.64 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2330100151352945		[learning rate: 0.007037]
	Learning Rate: 0.00703695
	LOSS [training: 1.2330100151352945 | validation: 1.0691405054553178]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1267607559948485		[learning rate: 0.0070204]
	Learning Rate: 0.00702035
	LOSS [training: 1.1267607559948485 | validation: 1.4834883020054979]
	TIME [epoch: 6.62 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2797735245684398		[learning rate: 0.0070038]
	Learning Rate: 0.00700379
	LOSS [training: 1.2797735245684398 | validation: 1.322023176228257]
	TIME [epoch: 6.61 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3660289797901046		[learning rate: 0.0069873]
	Learning Rate: 0.00698727
	LOSS [training: 1.3660289797901046 | validation: 1.2609009408030056]
	TIME [epoch: 6.62 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2123913929152375		[learning rate: 0.0069708]
	Learning Rate: 0.00697079
	LOSS [training: 1.2123913929152375 | validation: 1.175047480658644]
	TIME [epoch: 6.64 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9730241451023225		[learning rate: 0.0069543]
	Learning Rate: 0.00695435
	LOSS [training: 1.9730241451023225 | validation: 1.5236382064609248]
	TIME [epoch: 6.63 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2275918734464215		[learning rate: 0.0069379]
	Learning Rate: 0.00693794
	LOSS [training: 1.2275918734464215 | validation: 1.2297586193766439]
	TIME [epoch: 6.62 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2468234940366447		[learning rate: 0.0069216]
	Learning Rate: 0.00692158
	LOSS [training: 1.2468234940366447 | validation: 1.3116727582077548]
	TIME [epoch: 6.62 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5812228702709377		[learning rate: 0.0069053]
	Learning Rate: 0.00690525
	LOSS [training: 1.5812228702709377 | validation: 1.6003710149981116]
	TIME [epoch: 6.61 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.297196368043855		[learning rate: 0.006889]
	Learning Rate: 0.00688896
	LOSS [training: 1.297196368043855 | validation: 1.0832313404774194]
	TIME [epoch: 6.61 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3031530026851899		[learning rate: 0.0068727]
	Learning Rate: 0.00687271
	LOSS [training: 1.3031530026851899 | validation: 1.0430391504000787]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0836398162034082		[learning rate: 0.0068565]
	Learning Rate: 0.0068565
	LOSS [training: 1.0836398162034082 | validation: 1.6214671559231346]
	TIME [epoch: 6.64 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5003675806103192		[learning rate: 0.0068403]
	Learning Rate: 0.00684033
	LOSS [training: 1.5003675806103192 | validation: 1.3567792650581425]
	TIME [epoch: 6.62 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1357962560012154		[learning rate: 0.0068242]
	Learning Rate: 0.00682419
	LOSS [training: 1.1357962560012154 | validation: 1.044288092386592]
	TIME [epoch: 6.61 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2191558171234678		[learning rate: 0.0068081]
	Learning Rate: 0.0068081
	LOSS [training: 1.2191558171234678 | validation: 1.0231970171441587]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3984747867287906		[learning rate: 0.006792]
	Learning Rate: 0.00679204
	LOSS [training: 1.3984747867287906 | validation: 1.2251250556869508]
	TIME [epoch: 6.6 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4677377088700652		[learning rate: 0.006776]
	Learning Rate: 0.00677601
	LOSS [training: 1.4677377088700652 | validation: 1.57999906737086]
	TIME [epoch: 6.61 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4269677770131346		[learning rate: 0.00676]
	Learning Rate: 0.00676003
	LOSS [training: 1.4269677770131346 | validation: 1.7761786667746793]
	TIME [epoch: 6.63 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.476626013957365		[learning rate: 0.0067441]
	Learning Rate: 0.00674409
	LOSS [training: 1.476626013957365 | validation: 1.5984366112131652]
	TIME [epoch: 6.61 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2740059146224838		[learning rate: 0.0067282]
	Learning Rate: 0.00672818
	LOSS [training: 1.2740059146224838 | validation: 1.1166749578594197]
	TIME [epoch: 6.61 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1344363895654244		[learning rate: 0.0067123]
	Learning Rate: 0.00671231
	LOSS [training: 1.1344363895654244 | validation: 1.5898187400174106]
	TIME [epoch: 6.6 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3360233892059241		[learning rate: 0.0066965]
	Learning Rate: 0.00669647
	LOSS [training: 1.3360233892059241 | validation: 0.8094483595443751]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2779029001503797		[learning rate: 0.0066807]
	Learning Rate: 0.00668068
	LOSS [training: 1.2779029001503797 | validation: 1.0114368021187186]
	TIME [epoch: 6.63 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0814571833649638		[learning rate: 0.0066649]
	Learning Rate: 0.00666492
	LOSS [training: 1.0814571833649638 | validation: 1.4011779204079486]
	TIME [epoch: 6.64 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1620257402454994		[learning rate: 0.0066492]
	Learning Rate: 0.0066492
	LOSS [training: 1.1620257402454994 | validation: 1.6330541129659002]
	TIME [epoch: 6.63 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2892290435179414		[learning rate: 0.0066335]
	Learning Rate: 0.00663351
	LOSS [training: 1.2892290435179414 | validation: 1.1708908778495775]
	TIME [epoch: 6.63 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1427006173780492		[learning rate: 0.0066179]
	Learning Rate: 0.00661787
	LOSS [training: 1.1427006173780492 | validation: 0.8896250950621344]
	TIME [epoch: 6.6 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9715202255762664		[learning rate: 0.0066023]
	Learning Rate: 0.00660226
	LOSS [training: 0.9715202255762664 | validation: 0.9156232041576144]
	TIME [epoch: 6.62 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0404134010623665		[learning rate: 0.0065867]
	Learning Rate: 0.00658668
	LOSS [training: 1.0404134010623665 | validation: 1.5337265975211118]
	TIME [epoch: 6.63 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3609016350796508		[learning rate: 0.0065711]
	Learning Rate: 0.00657114
	LOSS [training: 1.3609016350796508 | validation: 1.0020074122486171]
	TIME [epoch: 6.66 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2299899598914752		[learning rate: 0.0065556]
	Learning Rate: 0.00655564
	LOSS [training: 1.2299899598914752 | validation: 1.1017687503995286]
	TIME [epoch: 6.62 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0644759961561663		[learning rate: 0.0065402]
	Learning Rate: 0.00654018
	LOSS [training: 1.0644759961561663 | validation: 1.2116086283961882]
	TIME [epoch: 6.63 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1074179374649737		[learning rate: 0.0065248]
	Learning Rate: 0.00652475
	LOSS [training: 1.1074179374649737 | validation: 1.214737956391621]
	TIME [epoch: 6.63 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5342547032606924		[learning rate: 0.0065094]
	Learning Rate: 0.00650936
	LOSS [training: 1.5342547032606924 | validation: 1.011919279518111]
	TIME [epoch: 6.63 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1192437040535146		[learning rate: 0.006494]
	Learning Rate: 0.00649401
	LOSS [training: 1.1192437040535146 | validation: 1.0027905430790331]
	TIME [epoch: 6.65 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7305061195635205		[learning rate: 0.0064787]
	Learning Rate: 0.00647869
	LOSS [training: 1.7305061195635205 | validation: 1.3738371432827594]
	TIME [epoch: 6.66 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.559815912242627		[learning rate: 0.0064634]
	Learning Rate: 0.00646341
	LOSS [training: 2.559815912242627 | validation: 1.5991932532586497]
	TIME [epoch: 6.63 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4272337742415337		[learning rate: 0.0064482]
	Learning Rate: 0.00644816
	LOSS [training: 1.4272337742415337 | validation: 1.1720373663717383]
	TIME [epoch: 6.63 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0753552482366038		[learning rate: 0.006433]
	Learning Rate: 0.00643295
	LOSS [training: 1.0753552482366038 | validation: 1.0154696987330112]
	TIME [epoch: 6.63 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0540916817768369		[learning rate: 0.0064178]
	Learning Rate: 0.00641778
	LOSS [training: 1.0540916817768369 | validation: 1.0581223686877785]
	TIME [epoch: 6.61 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1806029606201875		[learning rate: 0.0064026]
	Learning Rate: 0.00640264
	LOSS [training: 1.1806029606201875 | validation: 0.9094835139474164]
	TIME [epoch: 6.66 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9645611186107775		[learning rate: 0.0063875]
	Learning Rate: 0.00638754
	LOSS [training: 0.9645611186107775 | validation: 0.9256575488372625]
	TIME [epoch: 6.63 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9276435267399818		[learning rate: 0.0063725]
	Learning Rate: 0.00637247
	LOSS [training: 0.9276435267399818 | validation: 1.2617043315377399]
	TIME [epoch: 6.63 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.989671453293178		[learning rate: 0.0063574]
	Learning Rate: 0.00635744
	LOSS [training: 0.989671453293178 | validation: 0.8099165583990678]
	TIME [epoch: 6.63 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1182148496869204		[learning rate: 0.0063424]
	Learning Rate: 0.00634244
	LOSS [training: 1.1182148496869204 | validation: 1.1661065261163435]
	TIME [epoch: 6.63 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9059283163118828		[learning rate: 0.0063275]
	Learning Rate: 0.00632748
	LOSS [training: 0.9059283163118828 | validation: 1.1804095693123453]
	TIME [epoch: 6.63 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.890866207099766		[learning rate: 0.0063126]
	Learning Rate: 0.00631255
	LOSS [training: 0.890866207099766 | validation: 1.3204869059600592]
	TIME [epoch: 6.66 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3508827228620202		[learning rate: 0.0062977]
	Learning Rate: 0.00629766
	LOSS [training: 1.3508827228620202 | validation: 1.2591316212977244]
	TIME [epoch: 6.63 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9684436523410962		[learning rate: 0.0062828]
	Learning Rate: 0.00628281
	LOSS [training: 0.9684436523410962 | validation: 1.0494071684312944]
	TIME [epoch: 6.63 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9038506068426919		[learning rate: 0.006268]
	Learning Rate: 0.00626799
	LOSS [training: 0.9038506068426919 | validation: 1.122024902316367]
	TIME [epoch: 6.63 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0481639488459102		[learning rate: 0.0062532]
	Learning Rate: 0.0062532
	LOSS [training: 1.0481639488459102 | validation: 0.9481821620346502]
	TIME [epoch: 6.62 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0213639005397848		[learning rate: 0.0062385]
	Learning Rate: 0.00623845
	LOSS [training: 1.0213639005397848 | validation: 0.8603227757398321]
	TIME [epoch: 6.64 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8996430861727818		[learning rate: 0.0062237]
	Learning Rate: 0.00622374
	LOSS [training: 0.8996430861727818 | validation: 1.5772864089371392]
	TIME [epoch: 6.66 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.416934354749788		[learning rate: 0.0062091]
	Learning Rate: 0.00620906
	LOSS [training: 1.416934354749788 | validation: 0.9775509212923297]
	TIME [epoch: 6.63 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1019163385544508		[learning rate: 0.0061944]
	Learning Rate: 0.00619441
	LOSS [training: 1.1019163385544508 | validation: 0.8574773655646571]
	TIME [epoch: 6.63 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.132892160209504		[learning rate: 0.0061798]
	Learning Rate: 0.0061798
	LOSS [training: 1.132892160209504 | validation: 0.926919669615899]
	TIME [epoch: 6.62 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.016437078676358		[learning rate: 0.0061652]
	Learning Rate: 0.00616522
	LOSS [training: 1.016437078676358 | validation: 1.156486545301942]
	TIME [epoch: 6.63 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0937574731315123		[learning rate: 0.0061507]
	Learning Rate: 0.00615068
	LOSS [training: 1.0937574731315123 | validation: 0.8793982846121393]
	TIME [epoch: 6.66 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9870823941153478		[learning rate: 0.0061362]
	Learning Rate: 0.00613617
	LOSS [training: 0.9870823941153478 | validation: 1.6408952971175066]
	TIME [epoch: 6.64 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4553374979202762		[learning rate: 0.0061217]
	Learning Rate: 0.0061217
	LOSS [training: 1.4553374979202762 | validation: 1.853491887880434]
	TIME [epoch: 6.63 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3117859447621343		[learning rate: 0.0061073]
	Learning Rate: 0.00610726
	LOSS [training: 1.3117859447621343 | validation: 1.0901581473597626]
	TIME [epoch: 6.63 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9189115819975061		[learning rate: 0.0060929]
	Learning Rate: 0.00609285
	LOSS [training: 0.9189115819975061 | validation: 0.9128976024508404]
	TIME [epoch: 6.63 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0967902733290533		[learning rate: 0.0060785]
	Learning Rate: 0.00607848
	LOSS [training: 1.0967902733290533 | validation: 1.0936153487753246]
	TIME [epoch: 6.63 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4642642511995883		[learning rate: 0.0060641]
	Learning Rate: 0.00606414
	LOSS [training: 1.4642642511995883 | validation: 0.9308678607680294]
	TIME [epoch: 6.66 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9975416893857375		[learning rate: 0.0060498]
	Learning Rate: 0.00604984
	LOSS [training: 0.9975416893857375 | validation: 0.9514892255849379]
	TIME [epoch: 6.63 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9265887496949826		[learning rate: 0.0060356]
	Learning Rate: 0.00603556
	LOSS [training: 0.9265887496949826 | validation: 0.9968749883463819]
	TIME [epoch: 6.62 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0724455716515027		[learning rate: 0.0060213]
	Learning Rate: 0.00602133
	LOSS [training: 1.0724455716515027 | validation: 1.3176734146965252]
	TIME [epoch: 6.62 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.192802507445094		[learning rate: 0.0060071]
	Learning Rate: 0.00600712
	LOSS [training: 1.192802507445094 | validation: 0.7755316576193534]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7644161055933852		[learning rate: 0.005993]
	Learning Rate: 0.00599296
	LOSS [training: 0.7644161055933852 | validation: 1.1686122460924213]
	TIME [epoch: 6.65 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4781528062176112		[learning rate: 0.0059788]
	Learning Rate: 0.00597882
	LOSS [training: 1.4781528062176112 | validation: 0.991685766015976]
	TIME [epoch: 6.64 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.021673603932388		[learning rate: 0.0059647]
	Learning Rate: 0.00596472
	LOSS [training: 1.021673603932388 | validation: 0.988471338062671]
	TIME [epoch: 6.6 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0114950587171232		[learning rate: 0.0059506]
	Learning Rate: 0.00595065
	LOSS [training: 1.0114950587171232 | validation: 1.12510989956495]
	TIME [epoch: 6.62 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0531955150969337		[learning rate: 0.0059366]
	Learning Rate: 0.00593661
	LOSS [training: 1.0531955150969337 | validation: 0.8141883043515192]
	TIME [epoch: 6.62 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8593879528916417		[learning rate: 0.0059226]
	Learning Rate: 0.00592261
	LOSS [training: 0.8593879528916417 | validation: 1.231905096468642]
	TIME [epoch: 6.62 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8218294973988899		[learning rate: 0.0059086]
	Learning Rate: 0.00590863
	LOSS [training: 0.8218294973988899 | validation: 1.0418451507933528]
	TIME [epoch: 6.67 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8161882918715422		[learning rate: 0.0058947]
	Learning Rate: 0.0058947
	LOSS [training: 0.8161882918715422 | validation: 0.804118795375696]
	TIME [epoch: 6.61 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0686694164319772		[learning rate: 0.0058808]
	Learning Rate: 0.00588079
	LOSS [training: 1.0686694164319772 | validation: 2.0073378165372224]
	TIME [epoch: 6.62 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1148566822800468		[learning rate: 0.0058669]
	Learning Rate: 0.00586692
	LOSS [training: 1.1148566822800468 | validation: 0.9542938593911876]
	TIME [epoch: 6.62 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4732439803656434		[learning rate: 0.0058531]
	Learning Rate: 0.00585308
	LOSS [training: 1.4732439803656434 | validation: 1.5297864293711378]
	TIME [epoch: 6.62 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1931333064945668		[learning rate: 0.0058393]
	Learning Rate: 0.00583928
	LOSS [training: 1.1931333064945668 | validation: 0.8952317517313169]
	TIME [epoch: 6.62 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9937541394547897		[learning rate: 0.0058255]
	Learning Rate: 0.0058255
	LOSS [training: 0.9937541394547897 | validation: 0.7456771310931871]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3510723035493482		[learning rate: 0.0058118]
	Learning Rate: 0.00581176
	LOSS [training: 1.3510723035493482 | validation: 0.9775762265448665]
	TIME [epoch: 6.62 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9653657439777608		[learning rate: 0.0057981]
	Learning Rate: 0.00579805
	LOSS [training: 0.9653657439777608 | validation: 1.0180497232860528]
	TIME [epoch: 6.62 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9029183044226838		[learning rate: 0.0057844]
	Learning Rate: 0.00578438
	LOSS [training: 0.9029183044226838 | validation: 0.9421510642304869]
	TIME [epoch: 6.62 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8965110109683362		[learning rate: 0.0057707]
	Learning Rate: 0.00577073
	LOSS [training: 0.8965110109683362 | validation: 1.1595290429960512]
	TIME [epoch: 6.61 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9937541021014293		[learning rate: 0.0057571]
	Learning Rate: 0.00575712
	LOSS [training: 0.9937541021014293 | validation: 0.5983203395843989]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9909501052085018		[learning rate: 0.0057435]
	Learning Rate: 0.00574354
	LOSS [training: 0.9909501052085018 | validation: 0.9011293022198958]
	TIME [epoch: 6.65 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9073565844793562		[learning rate: 0.00573]
	Learning Rate: 0.00572999
	LOSS [training: 0.9073565844793562 | validation: 0.8539869803117373]
	TIME [epoch: 6.62 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.063793895197059		[learning rate: 0.0057165]
	Learning Rate: 0.00571647
	LOSS [training: 1.063793895197059 | validation: 1.4163628568806685]
	TIME [epoch: 6.62 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3443279921507405		[learning rate: 0.005703]
	Learning Rate: 0.00570299
	LOSS [training: 1.3443279921507405 | validation: 1.1544507834901716]
	TIME [epoch: 6.62 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0107821892322881		[learning rate: 0.0056895]
	Learning Rate: 0.00568954
	LOSS [training: 1.0107821892322881 | validation: 1.1550798483372848]
	TIME [epoch: 6.62 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9874588251491684		[learning rate: 0.0056761]
	Learning Rate: 0.00567612
	LOSS [training: 0.9874588251491684 | validation: 1.0356947107462253]
	TIME [epoch: 6.63 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.882867493783904		[learning rate: 0.0056627]
	Learning Rate: 0.00566273
	LOSS [training: 0.882867493783904 | validation: 0.9031961992678957]
	TIME [epoch: 6.65 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8834327400793274		[learning rate: 0.0056494]
	Learning Rate: 0.00564937
	LOSS [training: 0.8834327400793274 | validation: 0.7220914412196716]
	TIME [epoch: 6.62 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9149150456476288		[learning rate: 0.005636]
	Learning Rate: 0.00563604
	LOSS [training: 0.9149150456476288 | validation: 1.2811086590467236]
	TIME [epoch: 6.61 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0634523580064055		[learning rate: 0.0056227]
	Learning Rate: 0.00562275
	LOSS [training: 1.0634523580064055 | validation: 1.0308126206521298]
	TIME [epoch: 6.61 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.196447375495413		[learning rate: 0.0056095]
	Learning Rate: 0.00560949
	LOSS [training: 1.196447375495413 | validation: 1.1357765817302359]
	TIME [epoch: 6.61 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9421252849979757		[learning rate: 0.0055963]
	Learning Rate: 0.00559625
	LOSS [training: 0.9421252849979757 | validation: 0.8898128124530182]
	TIME [epoch: 6.63 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9040760864577398		[learning rate: 0.0055831]
	Learning Rate: 0.00558305
	LOSS [training: 0.9040760864577398 | validation: 0.7719509276927209]
	TIME [epoch: 6.65 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9713784826594984		[learning rate: 0.0055699]
	Learning Rate: 0.00556988
	LOSS [training: 0.9713784826594984 | validation: 1.3304046654868553]
	TIME [epoch: 6.62 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.104263877095613		[learning rate: 0.0055567]
	Learning Rate: 0.00555674
	LOSS [training: 1.104263877095613 | validation: 0.6273730959916015]
	TIME [epoch: 6.61 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7224229549431389		[learning rate: 0.0055436]
	Learning Rate: 0.00554364
	LOSS [training: 0.7224229549431389 | validation: 0.7011122744859595]
	TIME [epoch: 6.61 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.685880730144677		[learning rate: 0.0055306]
	Learning Rate: 0.00553056
	LOSS [training: 0.685880730144677 | validation: 0.802966531320014]
	TIME [epoch: 6.62 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7632064182792313		[learning rate: 0.0055175]
	Learning Rate: 0.00551752
	LOSS [training: 0.7632064182792313 | validation: 0.9792939776215247]
	TIME [epoch: 6.64 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8075610741141359		[learning rate: 0.0055045]
	Learning Rate: 0.0055045
	LOSS [training: 0.8075610741141359 | validation: 0.7171922867806224]
	TIME [epoch: 6.64 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8567249284619738		[learning rate: 0.0054915]
	Learning Rate: 0.00549152
	LOSS [training: 0.8567249284619738 | validation: 0.6849309562033187]
	TIME [epoch: 6.61 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8183845636321226		[learning rate: 0.0054786]
	Learning Rate: 0.00547856
	LOSS [training: 0.8183845636321226 | validation: 0.651290844346338]
	TIME [epoch: 6.61 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9309048806313849		[learning rate: 0.0054656]
	Learning Rate: 0.00546564
	LOSS [training: 0.9309048806313849 | validation: 1.5026969174962514]
	TIME [epoch: 6.62 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1420508024846794		[learning rate: 0.0054527]
	Learning Rate: 0.00545275
	LOSS [training: 1.1420508024846794 | validation: 1.4066291226254566]
	TIME [epoch: 6.62 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.030271205849289		[learning rate: 0.0054399]
	Learning Rate: 0.00543988
	LOSS [training: 1.030271205849289 | validation: 0.7254140094044315]
	TIME [epoch: 6.64 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8706050400540771		[learning rate: 0.0054271]
	Learning Rate: 0.00542705
	LOSS [training: 0.8706050400540771 | validation: 0.796845539336509]
	TIME [epoch: 6.62 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9471113541475671		[learning rate: 0.0054143]
	Learning Rate: 0.00541425
	LOSS [training: 0.9471113541475671 | validation: 1.0799130481509596]
	TIME [epoch: 6.62 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2568420982754387		[learning rate: 0.0054015]
	Learning Rate: 0.00540148
	LOSS [training: 1.2568420982754387 | validation: 0.9763798610070404]
	TIME [epoch: 6.61 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0208536348062835		[learning rate: 0.0053887]
	Learning Rate: 0.00538874
	LOSS [training: 1.0208536348062835 | validation: 0.8105179998403558]
	TIME [epoch: 6.61 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7005233893545645		[learning rate: 0.005376]
	Learning Rate: 0.00537603
	LOSS [training: 0.7005233893545645 | validation: 0.8703018228244528]
	TIME [epoch: 6.63 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9116752877989921		[learning rate: 0.0053633]
	Learning Rate: 0.00536335
	LOSS [training: 0.9116752877989921 | validation: 1.0878919116095624]
	TIME [epoch: 6.65 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8343097743371777		[learning rate: 0.0053507]
	Learning Rate: 0.00535069
	LOSS [training: 0.8343097743371777 | validation: 0.6450631577619474]
	TIME [epoch: 6.62 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6552339054372799		[learning rate: 0.0053381]
	Learning Rate: 0.00533807
	LOSS [training: 0.6552339054372799 | validation: 0.5562554362039027]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9224613883479555		[learning rate: 0.0053255]
	Learning Rate: 0.00532548
	LOSS [training: 0.9224613883479555 | validation: 0.8329256599648173]
	TIME [epoch: 6.62 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8703444411287332		[learning rate: 0.0053129]
	Learning Rate: 0.00531292
	LOSS [training: 0.8703444411287332 | validation: 0.9243444730982748]
	TIME [epoch: 6.61 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9917385284913504		[learning rate: 0.0053004]
	Learning Rate: 0.00530039
	LOSS [training: 0.9917385284913504 | validation: 0.8373870899516775]
	TIME [epoch: 6.65 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.712952795425873		[learning rate: 0.0052879]
	Learning Rate: 0.00528789
	LOSS [training: 0.712952795425873 | validation: 0.9072008024580147]
	TIME [epoch: 6.62 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8003126541752478		[learning rate: 0.0052754]
	Learning Rate: 0.00527541
	LOSS [training: 0.8003126541752478 | validation: 0.9449851933936309]
	TIME [epoch: 6.62 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7868748526091368		[learning rate: 0.005263]
	Learning Rate: 0.00526297
	LOSS [training: 0.7868748526091368 | validation: 0.7183441182963674]
	TIME [epoch: 6.62 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8481512639552597		[learning rate: 0.0052506]
	Learning Rate: 0.00525055
	LOSS [training: 0.8481512639552597 | validation: 0.8328704343429123]
	TIME [epoch: 6.62 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7195016791658506		[learning rate: 0.0052382]
	Learning Rate: 0.00523817
	LOSS [training: 0.7195016791658506 | validation: 0.5352856778310888]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6621224051119268		[learning rate: 0.0052258]
	Learning Rate: 0.00522581
	LOSS [training: 0.6621224051119268 | validation: 0.5969072585807909]
	TIME [epoch: 6.68 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6668271642360574		[learning rate: 0.0052135]
	Learning Rate: 0.00521349
	LOSS [training: 0.6668271642360574 | validation: 0.8474282023050472]
	TIME [epoch: 6.65 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6791788789164405		[learning rate: 0.0052012]
	Learning Rate: 0.00520119
	LOSS [training: 0.6791788789164405 | validation: 1.153473553199884]
	TIME [epoch: 6.65 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.999675646860608		[learning rate: 0.0051889]
	Learning Rate: 0.00518892
	LOSS [training: 0.999675646860608 | validation: 0.625816945394968]
	TIME [epoch: 6.64 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7165213580965195		[learning rate: 0.0051767]
	Learning Rate: 0.00517668
	LOSS [training: 0.7165213580965195 | validation: 0.5777908456011746]
	TIME [epoch: 6.64 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8518933027697544		[learning rate: 0.0051645]
	Learning Rate: 0.00516447
	LOSS [training: 0.8518933027697544 | validation: 0.49980059633366664]
	TIME [epoch: 6.66 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.654213086225919		[learning rate: 0.0051523]
	Learning Rate: 0.00515229
	LOSS [training: 0.654213086225919 | validation: 0.6974803205709484]
	TIME [epoch: 6.68 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8559013789003924		[learning rate: 0.0051401]
	Learning Rate: 0.00514013
	LOSS [training: 0.8559013789003924 | validation: 0.7713779301422946]
	TIME [epoch: 6.64 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.844868003442128		[learning rate: 0.005128]
	Learning Rate: 0.00512801
	LOSS [training: 0.844868003442128 | validation: 1.277529416251899]
	TIME [epoch: 6.64 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8477005479849937		[learning rate: 0.0051159]
	Learning Rate: 0.00511591
	LOSS [training: 0.8477005479849937 | validation: 0.7554690768229098]
	TIME [epoch: 6.64 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8339400392761277		[learning rate: 0.0051038]
	Learning Rate: 0.00510384
	LOSS [training: 0.8339400392761277 | validation: 0.628645823849429]
	TIME [epoch: 6.64 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6682455165306873		[learning rate: 0.0050918]
	Learning Rate: 0.00509181
	LOSS [training: 0.6682455165306873 | validation: 1.1945991398162983]
	TIME [epoch: 6.65 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0973598296353728		[learning rate: 0.0050798]
	Learning Rate: 0.00507979
	LOSS [training: 1.0973598296353728 | validation: 0.6852451942814771]
	TIME [epoch: 6.68 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8066013699017592		[learning rate: 0.0050678]
	Learning Rate: 0.00506781
	LOSS [training: 0.8066013699017592 | validation: 0.8532496999799398]
	TIME [epoch: 6.65 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8412485332614013		[learning rate: 0.0050559]
	Learning Rate: 0.00505586
	LOSS [training: 0.8412485332614013 | validation: 0.5494162163758559]
	TIME [epoch: 6.65 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6087285541068045		[learning rate: 0.0050439]
	Learning Rate: 0.00504393
	LOSS [training: 0.6087285541068045 | validation: 0.5345274413294325]
	TIME [epoch: 6.64 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6608465068273754		[learning rate: 0.005032]
	Learning Rate: 0.00503203
	LOSS [training: 0.6608465068273754 | validation: 0.6281582893744992]
	TIME [epoch: 6.64 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8074897432661269		[learning rate: 0.0050202]
	Learning Rate: 0.00502016
	LOSS [training: 0.8074897432661269 | validation: 1.5294964866620544]
	TIME [epoch: 6.66 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9586440535114724		[learning rate: 0.0050083]
	Learning Rate: 0.00500832
	LOSS [training: 0.9586440535114724 | validation: 0.6184089078044923]
	TIME [epoch: 6.68 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6619867276128206		[learning rate: 0.0049965]
	Learning Rate: 0.00499651
	LOSS [training: 0.6619867276128206 | validation: 1.2418807738966382]
	TIME [epoch: 6.65 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7251586270124036		[learning rate: 0.0049847]
	Learning Rate: 0.00498472
	LOSS [training: 0.7251586270124036 | validation: 0.5920738717072022]
	TIME [epoch: 6.64 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.992982695993811		[learning rate: 0.004973]
	Learning Rate: 0.00497296
	LOSS [training: 0.992982695993811 | validation: 0.7758739013575919]
	TIME [epoch: 6.64 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5920505241809705		[learning rate: 0.0049612]
	Learning Rate: 0.00496123
	LOSS [training: 0.5920505241809705 | validation: 0.7021906787761127]
	TIME [epoch: 6.64 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8347460605525235		[learning rate: 0.0049495]
	Learning Rate: 0.00494953
	LOSS [training: 0.8347460605525235 | validation: 0.7583437051676445]
	TIME [epoch: 6.66 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8215098815742918		[learning rate: 0.0049379]
	Learning Rate: 0.00493786
	LOSS [training: 0.8215098815742918 | validation: 0.6098783367383402]
	TIME [epoch: 6.65 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5378460920069387		[learning rate: 0.0049262]
	Learning Rate: 0.00492621
	LOSS [training: 0.5378460920069387 | validation: 0.6194481252963919]
	TIME [epoch: 6.64 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7380644288779562		[learning rate: 0.0049146]
	Learning Rate: 0.00491459
	LOSS [training: 0.7380644288779562 | validation: 0.7308995465036566]
	TIME [epoch: 6.65 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9244251942840755		[learning rate: 0.004903]
	Learning Rate: 0.004903
	LOSS [training: 0.9244251942840755 | validation: 0.9900744642663799]
	TIME [epoch: 6.64 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.981329455926501		[learning rate: 0.0048914]
	Learning Rate: 0.00489143
	LOSS [training: 0.981329455926501 | validation: 0.8440508015886836]
	TIME [epoch: 6.65 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7004478298042095		[learning rate: 0.0048799]
	Learning Rate: 0.00487989
	LOSS [training: 0.7004478298042095 | validation: 0.5090577878131592]
	TIME [epoch: 6.68 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5872427338548263		[learning rate: 0.0048684]
	Learning Rate: 0.00486838
	LOSS [training: 0.5872427338548263 | validation: 0.9281474498574309]
	TIME [epoch: 6.64 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9976530766809691		[learning rate: 0.0048569]
	Learning Rate: 0.0048569
	LOSS [training: 0.9976530766809691 | validation: 0.8206248552454345]
	TIME [epoch: 6.64 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6952000506164318		[learning rate: 0.0048454]
	Learning Rate: 0.00484544
	LOSS [training: 0.6952000506164318 | validation: 0.5495567325202607]
	TIME [epoch: 6.64 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6969405579727811		[learning rate: 0.004834]
	Learning Rate: 0.00483401
	LOSS [training: 0.6969405579727811 | validation: 0.6590746058407898]
	TIME [epoch: 6.64 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6971601011922275		[learning rate: 0.0048226]
	Learning Rate: 0.00482261
	LOSS [training: 0.6971601011922275 | validation: 0.6966976908990469]
	TIME [epoch: 6.65 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8748187841561986		[learning rate: 0.0048112]
	Learning Rate: 0.00481123
	LOSS [training: 0.8748187841561986 | validation: 1.063067157355385]
	TIME [epoch: 6.68 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8488851358242119		[learning rate: 0.0047999]
	Learning Rate: 0.00479988
	LOSS [training: 0.8488851358242119 | validation: 0.49116356784979465]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6202101496736174		[learning rate: 0.0047886]
	Learning Rate: 0.00478856
	LOSS [training: 0.6202101496736174 | validation: 0.6310841751688563]
	TIME [epoch: 6.64 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6939906364776491		[learning rate: 0.0047773]
	Learning Rate: 0.00477727
	LOSS [training: 0.6939906364776491 | validation: 0.6685163993532918]
	TIME [epoch: 6.64 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6878755376701745		[learning rate: 0.004766]
	Learning Rate: 0.004766
	LOSS [training: 0.6878755376701745 | validation: 0.9750481763976143]
	TIME [epoch: 6.64 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8808856223550331		[learning rate: 0.0047548]
	Learning Rate: 0.00475476
	LOSS [training: 0.8808856223550331 | validation: 0.8105541042843903]
	TIME [epoch: 6.68 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.196498434749604		[learning rate: 0.0047435]
	Learning Rate: 0.00474354
	LOSS [training: 1.196498434749604 | validation: 1.3301267335360807]
	TIME [epoch: 6.64 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0787048762513187		[learning rate: 0.0047324]
	Learning Rate: 0.00473235
	LOSS [training: 1.0787048762513187 | validation: 0.974743178867346]
	TIME [epoch: 6.64 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9805197474790871		[learning rate: 0.0047212]
	Learning Rate: 0.00472119
	LOSS [training: 0.9805197474790871 | validation: 1.0944643914931218]
	TIME [epoch: 6.63 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9178276269235104		[learning rate: 0.0047101]
	Learning Rate: 0.00471005
	LOSS [training: 0.9178276269235104 | validation: 1.0878638588053253]
	TIME [epoch: 6.63 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8609278866845609		[learning rate: 0.0046989]
	Learning Rate: 0.00469894
	LOSS [training: 0.8609278866845609 | validation: 1.1498430190508853]
	TIME [epoch: 6.65 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8415805955655807		[learning rate: 0.0046879]
	Learning Rate: 0.00468786
	LOSS [training: 0.8415805955655807 | validation: 0.9946511698861569]
	TIME [epoch: 6.67 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8507750542958201		[learning rate: 0.0046768]
	Learning Rate: 0.0046768
	LOSS [training: 0.8507750542958201 | validation: 0.813763554936785]
	TIME [epoch: 6.63 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.724718104385303		[learning rate: 0.0046658]
	Learning Rate: 0.00466577
	LOSS [training: 0.724718104385303 | validation: 0.5925707334074284]
	TIME [epoch: 6.63 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6153325921247448		[learning rate: 0.0046548]
	Learning Rate: 0.00465476
	LOSS [training: 0.6153325921247448 | validation: 0.8285862651659515]
	TIME [epoch: 6.64 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1157021262723712		[learning rate: 0.0046438]
	Learning Rate: 0.00464378
	LOSS [training: 1.1157021262723712 | validation: 0.8205626891136549]
	TIME [epoch: 6.63 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7607393881624785		[learning rate: 0.0046328]
	Learning Rate: 0.00463283
	LOSS [training: 0.7607393881624785 | validation: 0.48208552701331486]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6439809310283381		[learning rate: 0.0046219]
	Learning Rate: 0.0046219
	LOSS [training: 0.6439809310283381 | validation: 0.49345341813012866]
	TIME [epoch: 6.67 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6099846548821746		[learning rate: 0.004611]
	Learning Rate: 0.004611
	LOSS [training: 0.6099846548821746 | validation: 0.5644439676536672]
	TIME [epoch: 6.63 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.678694300380313		[learning rate: 0.0046001]
	Learning Rate: 0.00460012
	LOSS [training: 0.678694300380313 | validation: 0.8015983615516026]
	TIME [epoch: 6.63 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8201397603359937		[learning rate: 0.0045893]
	Learning Rate: 0.00458927
	LOSS [training: 0.8201397603359937 | validation: 0.6097685224783647]
	TIME [epoch: 6.62 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5933753849033822		[learning rate: 0.0045784]
	Learning Rate: 0.00457844
	LOSS [training: 0.5933753849033822 | validation: 0.8629063130229007]
	TIME [epoch: 6.62 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.827325045446439		[learning rate: 0.0045676]
	Learning Rate: 0.00456765
	LOSS [training: 0.827325045446439 | validation: 0.7753441291955778]
	TIME [epoch: 6.65 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7308605035584133		[learning rate: 0.0045569]
	Learning Rate: 0.00455687
	LOSS [training: 0.7308605035584133 | validation: 0.6386384954758482]
	TIME [epoch: 6.65 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6958400824160192		[learning rate: 0.0045461]
	Learning Rate: 0.00454612
	LOSS [training: 0.6958400824160192 | validation: 0.5576656616281639]
	TIME [epoch: 6.64 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7121959141099048		[learning rate: 0.0045354]
	Learning Rate: 0.0045354
	LOSS [training: 0.7121959141099048 | validation: 0.4747896269410756]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6335807261642765		[learning rate: 0.0045247]
	Learning Rate: 0.0045247
	LOSS [training: 0.6335807261642765 | validation: 0.5304376745078481]
	TIME [epoch: 6.63 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5764795923331545		[learning rate: 0.004514]
	Learning Rate: 0.00451403
	LOSS [training: 0.5764795923331545 | validation: 0.9113678293138043]
	TIME [epoch: 6.62 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9550600406195866		[learning rate: 0.0045034]
	Learning Rate: 0.00450338
	LOSS [training: 0.9550600406195866 | validation: 1.0411552254135596]
	TIME [epoch: 6.66 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7602849999658774		[learning rate: 0.0044928]
	Learning Rate: 0.00449276
	LOSS [training: 0.7602849999658774 | validation: 0.6544247271540924]
	TIME [epoch: 6.63 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5973561267295013		[learning rate: 0.0044822]
	Learning Rate: 0.00448216
	LOSS [training: 0.5973561267295013 | validation: 0.8981789649940787]
	TIME [epoch: 6.62 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6921369943480103		[learning rate: 0.0044716]
	Learning Rate: 0.00447159
	LOSS [training: 0.6921369943480103 | validation: 0.6663600122234249]
	TIME [epoch: 6.63 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7155668647056517		[learning rate: 0.004461]
	Learning Rate: 0.00446104
	LOSS [training: 0.7155668647056517 | validation: 0.47786390216251196]
	TIME [epoch: 6.63 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6979321154584491		[learning rate: 0.0044505]
	Learning Rate: 0.00445051
	LOSS [training: 0.6979321154584491 | validation: 0.5956779239557601]
	TIME [epoch: 6.63 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6271185237420116		[learning rate: 0.00444]
	Learning Rate: 0.00444002
	LOSS [training: 0.6271185237420116 | validation: 0.6363637665494588]
	TIME [epoch: 6.65 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6093094372212271		[learning rate: 0.0044295]
	Learning Rate: 0.00442954
	LOSS [training: 0.6093094372212271 | validation: 0.6298949566259093]
	TIME [epoch: 6.64 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6445367623255807		[learning rate: 0.0044191]
	Learning Rate: 0.00441909
	LOSS [training: 0.6445367623255807 | validation: 0.5438661876932849]
	TIME [epoch: 6.62 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5583560940394232		[learning rate: 0.0044087]
	Learning Rate: 0.00440867
	LOSS [training: 0.5583560940394232 | validation: 0.4559247310061877]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5127677047789232		[learning rate: 0.0043983]
	Learning Rate: 0.00439827
	LOSS [training: 0.5127677047789232 | validation: 0.7694080898772686]
	TIME [epoch: 6.62 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5560284104289164		[learning rate: 0.0043879]
	Learning Rate: 0.0043879
	LOSS [training: 0.5560284104289164 | validation: 0.3888066947530818]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5258439878832795		[learning rate: 0.0043775]
	Learning Rate: 0.00437755
	LOSS [training: 0.5258439878832795 | validation: 0.4966431666476997]
	TIME [epoch: 6.65 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5190863728021428		[learning rate: 0.0043672]
	Learning Rate: 0.00436722
	LOSS [training: 0.5190863728021428 | validation: 0.9734783428420666]
	TIME [epoch: 6.63 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7257501236289967		[learning rate: 0.0043569]
	Learning Rate: 0.00435692
	LOSS [training: 0.7257501236289967 | validation: 0.4494854479853866]
	TIME [epoch: 6.62 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.506171682462		[learning rate: 0.0043466]
	Learning Rate: 0.00434664
	LOSS [training: 0.506171682462 | validation: 0.8694003950080366]
	TIME [epoch: 6.62 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.882911781682421		[learning rate: 0.0043364]
	Learning Rate: 0.00433639
	LOSS [training: 0.882911781682421 | validation: 0.575506217479665]
	TIME [epoch: 6.62 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5554267531382591		[learning rate: 0.0043262]
	Learning Rate: 0.00432616
	LOSS [training: 0.5554267531382591 | validation: 0.5285090987760225]
	TIME [epoch: 6.62 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5457385911485895		[learning rate: 0.004316]
	Learning Rate: 0.00431596
	LOSS [training: 0.5457385911485895 | validation: 0.3878372429503961]
	TIME [epoch: 6.66 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.47133428058578125		[learning rate: 0.0043058]
	Learning Rate: 0.00430577
	LOSS [training: 0.47133428058578125 | validation: 0.5034250296270628]
	TIME [epoch: 6.62 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5450860158091916		[learning rate: 0.0042956]
	Learning Rate: 0.00429562
	LOSS [training: 0.5450860158091916 | validation: 0.4642902471426684]
	TIME [epoch: 6.61 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5103687947310006		[learning rate: 0.0042855]
	Learning Rate: 0.00428549
	LOSS [training: 0.5103687947310006 | validation: 0.5343986198366127]
	TIME [epoch: 6.61 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5154232480676846		[learning rate: 0.0042754]
	Learning Rate: 0.00427538
	LOSS [training: 0.5154232480676846 | validation: 0.8382440294051856]
	TIME [epoch: 6.61 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2383996258595198		[learning rate: 0.0042653]
	Learning Rate: 0.00426529
	LOSS [training: 1.2383996258595198 | validation: 1.0642835954757457]
	TIME [epoch: 6.61 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9602986693385163		[learning rate: 0.0042552]
	Learning Rate: 0.00425523
	LOSS [training: 0.9602986693385163 | validation: 0.5220403280399172]
	TIME [epoch: 6.65 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5593833309008178		[learning rate: 0.0042452]
	Learning Rate: 0.00424519
	LOSS [training: 0.5593833309008178 | validation: 0.5216731263606565]
	TIME [epoch: 6.61 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5162374381263309		[learning rate: 0.0042352]
	Learning Rate: 0.00423518
	LOSS [training: 0.5162374381263309 | validation: 0.5273692678149136]
	TIME [epoch: 6.61 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5712526999669967		[learning rate: 0.0042252]
	Learning Rate: 0.00422519
	LOSS [training: 0.5712526999669967 | validation: 0.4061458676885481]
	TIME [epoch: 6.61 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5464702862044819		[learning rate: 0.0042152]
	Learning Rate: 0.00421522
	LOSS [training: 0.5464702862044819 | validation: 0.4699858399932937]
	TIME [epoch: 6.61 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5427881493761533		[learning rate: 0.0042053]
	Learning Rate: 0.00420528
	LOSS [training: 0.5427881493761533 | validation: 0.7057000902070345]
	TIME [epoch: 6.61 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5516553835386527		[learning rate: 0.0041954]
	Learning Rate: 0.00419536
	LOSS [training: 0.5516553835386527 | validation: 0.7165540205452372]
	TIME [epoch: 6.65 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6364786205499396		[learning rate: 0.0041855]
	Learning Rate: 0.00418546
	LOSS [training: 0.6364786205499396 | validation: 0.49855272983301224]
	TIME [epoch: 6.61 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5240380331281964		[learning rate: 0.0041756]
	Learning Rate: 0.00417559
	LOSS [training: 0.5240380331281964 | validation: 0.4648492891324823]
	TIME [epoch: 6.61 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5097853166881515		[learning rate: 0.0041657]
	Learning Rate: 0.00416574
	LOSS [training: 0.5097853166881515 | validation: 0.4260133178982811]
	TIME [epoch: 6.62 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4775094709036221		[learning rate: 0.0041559]
	Learning Rate: 0.00415592
	LOSS [training: 0.4775094709036221 | validation: 0.4488125877294101]
	TIME [epoch: 6.62 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.566433479722263		[learning rate: 0.0041461]
	Learning Rate: 0.00414611
	LOSS [training: 0.566433479722263 | validation: 0.4315720674872072]
	TIME [epoch: 6.63 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6237987359548879		[learning rate: 0.0041363]
	Learning Rate: 0.00413633
	LOSS [training: 0.6237987359548879 | validation: 0.5342902894269562]
	TIME [epoch: 6.65 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.587072824742557		[learning rate: 0.0041266]
	Learning Rate: 0.00412657
	LOSS [training: 0.587072824742557 | validation: 0.5572582298289309]
	TIME [epoch: 6.61 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5717715652521328		[learning rate: 0.0041168]
	Learning Rate: 0.00411684
	LOSS [training: 0.5717715652521328 | validation: 0.4846899073389926]
	TIME [epoch: 6.61 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5681383787092658		[learning rate: 0.0041071]
	Learning Rate: 0.00410713
	LOSS [training: 0.5681383787092658 | validation: 0.4418628206307783]
	TIME [epoch: 6.62 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7873866457635552		[learning rate: 0.0040974]
	Learning Rate: 0.00409744
	LOSS [training: 0.7873866457635552 | validation: 0.6985477936135057]
	TIME [epoch: 6.61 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.757271408914983		[learning rate: 0.0040878]
	Learning Rate: 0.00408778
	LOSS [training: 0.757271408914983 | validation: 0.6425110235685009]
	TIME [epoch: 6.66 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6677384163095611		[learning rate: 0.0040781]
	Learning Rate: 0.00407813
	LOSS [training: 0.6677384163095611 | validation: 0.5270241033648578]
	TIME [epoch: 6.63 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6874636122285663		[learning rate: 0.0040685]
	Learning Rate: 0.00406851
	LOSS [training: 0.6874636122285663 | validation: 0.5770473842165991]
	TIME [epoch: 6.62 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5307206507060115		[learning rate: 0.0040589]
	Learning Rate: 0.00405892
	LOSS [training: 0.5307206507060115 | validation: 0.44372216543492576]
	TIME [epoch: 6.61 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5247723740449374		[learning rate: 0.0040493]
	Learning Rate: 0.00404934
	LOSS [training: 0.5247723740449374 | validation: 0.48907667636460794]
	TIME [epoch: 6.62 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5020319339205708		[learning rate: 0.0040398]
	Learning Rate: 0.00403979
	LOSS [training: 0.5020319339205708 | validation: 0.578551862695006]
	TIME [epoch: 6.62 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5665516250761897		[learning rate: 0.0040303]
	Learning Rate: 0.00403026
	LOSS [training: 0.5665516250761897 | validation: 0.4514002652465396]
	TIME [epoch: 6.65 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4606760862547278		[learning rate: 0.0040208]
	Learning Rate: 0.00402076
	LOSS [training: 0.4606760862547278 | validation: 0.45617405710826375]
	TIME [epoch: 6.62 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.610012212275588		[learning rate: 0.0040113]
	Learning Rate: 0.00401127
	LOSS [training: 0.610012212275588 | validation: 0.6373887655429289]
	TIME [epoch: 6.62 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5321536097180499		[learning rate: 0.0040018]
	Learning Rate: 0.00400181
	LOSS [training: 0.5321536097180499 | validation: 0.4433079535469044]
	TIME [epoch: 6.61 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.629955148060718		[learning rate: 0.0039924]
	Learning Rate: 0.00399237
	LOSS [training: 0.629955148060718 | validation: 0.5562531677863295]
	TIME [epoch: 6.61 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6301079467631028		[learning rate: 0.003983]
	Learning Rate: 0.00398295
	LOSS [training: 0.6301079467631028 | validation: 0.464270258789774]
	TIME [epoch: 6.64 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6977855380600733		[learning rate: 0.0039736]
	Learning Rate: 0.00397356
	LOSS [training: 0.6977855380600733 | validation: 0.42883124117196636]
	TIME [epoch: 6.63 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.567460321818733		[learning rate: 0.0039642]
	Learning Rate: 0.00396418
	LOSS [training: 0.567460321818733 | validation: 0.4146108170593589]
	TIME [epoch: 6.62 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6912086162402375		[learning rate: 0.0039548]
	Learning Rate: 0.00395483
	LOSS [training: 0.6912086162402375 | validation: 0.4270128859246406]
	TIME [epoch: 6.62 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7445868199736052		[learning rate: 0.0039455]
	Learning Rate: 0.0039455
	LOSS [training: 0.7445868199736052 | validation: 0.7887927793992144]
	TIME [epoch: 6.61 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7613411291809418		[learning rate: 0.0039362]
	Learning Rate: 0.0039362
	LOSS [training: 0.7613411291809418 | validation: 0.5803706135860112]
	TIME [epoch: 6.61 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5458489810422397		[learning rate: 0.0039269]
	Learning Rate: 0.00392691
	LOSS [training: 0.5458489810422397 | validation: 0.5748222152974943]
	TIME [epoch: 6.65 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5394559880963754		[learning rate: 0.0039176]
	Learning Rate: 0.00391765
	LOSS [training: 0.5394559880963754 | validation: 0.6518383957537782]
	TIME [epoch: 6.63 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5117164231815918		[learning rate: 0.0039084]
	Learning Rate: 0.00390841
	LOSS [training: 0.5117164231815918 | validation: 0.4422087035075771]
	TIME [epoch: 6.6 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5626744842191609		[learning rate: 0.0038992]
	Learning Rate: 0.00389919
	LOSS [training: 0.5626744842191609 | validation: 0.7416250110723037]
	TIME [epoch: 6.61 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6516650507737813		[learning rate: 0.00389]
	Learning Rate: 0.00388999
	LOSS [training: 0.6516650507737813 | validation: 0.48140930795022735]
	TIME [epoch: 6.7 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5269466450412332		[learning rate: 0.0038808]
	Learning Rate: 0.00388082
	LOSS [training: 0.5269466450412332 | validation: 0.4052581653640047]
	TIME [epoch: 6.61 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5796441507721346		[learning rate: 0.0038717]
	Learning Rate: 0.00387166
	LOSS [training: 0.5796441507721346 | validation: 0.8135461236940601]
	TIME [epoch: 6.64 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7126779438962941		[learning rate: 0.0038625]
	Learning Rate: 0.00386253
	LOSS [training: 0.7126779438962941 | validation: 0.48954274299271394]
	TIME [epoch: 6.62 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5384555967313127		[learning rate: 0.0038534]
	Learning Rate: 0.00385342
	LOSS [training: 0.5384555967313127 | validation: 0.5947321247848874]
	TIME [epoch: 6.62 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4823648673950348		[learning rate: 0.0038443]
	Learning Rate: 0.00384433
	LOSS [training: 0.4823648673950348 | validation: 0.37044559750893785]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4837518235251082		[learning rate: 0.0038353]
	Learning Rate: 0.00383526
	LOSS [training: 0.4837518235251082 | validation: 0.44332295527608995]
	TIME [epoch: 6.6 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5181315568413771		[learning rate: 0.0038262]
	Learning Rate: 0.00382621
	LOSS [training: 0.5181315568413771 | validation: 0.6698478574510383]
	TIME [epoch: 6.63 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46136977327602136		[learning rate: 0.0038172]
	Learning Rate: 0.00381719
	LOSS [training: 0.46136977327602136 | validation: 0.5002778868120759]
	TIME [epoch: 6.61 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5092913140672464		[learning rate: 0.0038082]
	Learning Rate: 0.00380818
	LOSS [training: 0.5092913140672464 | validation: 0.5346490705733864]
	TIME [epoch: 6.61 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4836775006933852		[learning rate: 0.0037992]
	Learning Rate: 0.0037992
	LOSS [training: 0.4836775006933852 | validation: 0.371180508670257]
	TIME [epoch: 6.59 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5345473653805286		[learning rate: 0.0037902]
	Learning Rate: 0.00379024
	LOSS [training: 0.5345473653805286 | validation: 0.42066860346808277]
	TIME [epoch: 6.61 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4952551312192835		[learning rate: 0.0037813]
	Learning Rate: 0.0037813
	LOSS [training: 0.4952551312192835 | validation: 0.4333740156287299]
	TIME [epoch: 6.59 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4849876445240368		[learning rate: 0.0037724]
	Learning Rate: 0.00377238
	LOSS [training: 0.4849876445240368 | validation: 0.5015313598591753]
	TIME [epoch: 6.66 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5399745901565627		[learning rate: 0.0037635]
	Learning Rate: 0.00376348
	LOSS [training: 0.5399745901565627 | validation: 1.1125496903590975]
	TIME [epoch: 6.6 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8021496544767464		[learning rate: 0.0037546]
	Learning Rate: 0.0037546
	LOSS [training: 0.8021496544767464 | validation: 0.6361306206199716]
	TIME [epoch: 6.62 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.928067506925921		[learning rate: 0.0037457]
	Learning Rate: 0.00374575
	LOSS [training: 0.928067506925921 | validation: 0.678461788406622]
	TIME [epoch: 6.6 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5528539876909745		[learning rate: 0.0037369]
	Learning Rate: 0.00373691
	LOSS [training: 0.5528539876909745 | validation: 0.6365385325392577]
	TIME [epoch: 6.6 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6575571132333057		[learning rate: 0.0037281]
	Learning Rate: 0.0037281
	LOSS [training: 0.6575571132333057 | validation: 1.0619717575231096]
	TIME [epoch: 6.6 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6654070098898163		[learning rate: 0.0037193]
	Learning Rate: 0.0037193
	LOSS [training: 0.6654070098898163 | validation: 0.5000456508452579]
	TIME [epoch: 6.66 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6123953930823962		[learning rate: 0.0037105]
	Learning Rate: 0.00371053
	LOSS [training: 0.6123953930823962 | validation: 0.4581163417887997]
	TIME [epoch: 6.61 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5522990413444265		[learning rate: 0.0037018]
	Learning Rate: 0.00370178
	LOSS [training: 0.5522990413444265 | validation: 0.5290909631442657]
	TIME [epoch: 6.61 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5429028450321692		[learning rate: 0.003693]
	Learning Rate: 0.00369304
	LOSS [training: 0.5429028450321692 | validation: 0.5775222910082438]
	TIME [epoch: 6.59 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5446960781127288		[learning rate: 0.0036843]
	Learning Rate: 0.00368433
	LOSS [training: 0.5446960781127288 | validation: 0.5487543390809105]
	TIME [epoch: 6.61 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48482633405028785		[learning rate: 0.0036756]
	Learning Rate: 0.00367564
	LOSS [training: 0.48482633405028785 | validation: 0.3948477065251245]
	TIME [epoch: 6.62 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6985966683684903		[learning rate: 0.003667]
	Learning Rate: 0.00366697
	LOSS [training: 0.6985966683684903 | validation: 0.35007397827216397]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5035661456757502		[learning rate: 0.0036583]
	Learning Rate: 0.00365832
	LOSS [training: 0.5035661456757502 | validation: 0.6850433580822264]
	TIME [epoch: 6.61 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.552764215575774		[learning rate: 0.0036497]
	Learning Rate: 0.00364969
	LOSS [training: 0.552764215575774 | validation: 0.45451659039485787]
	TIME [epoch: 6.62 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43744904289431297		[learning rate: 0.0036411]
	Learning Rate: 0.00364108
	LOSS [training: 0.43744904289431297 | validation: 0.5397099900399098]
	TIME [epoch: 6.61 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5330472065823731		[learning rate: 0.0036325]
	Learning Rate: 0.0036325
	LOSS [training: 0.5330472065823731 | validation: 0.3920864009906965]
	TIME [epoch: 6.59 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4631382071385827		[learning rate: 0.0036239]
	Learning Rate: 0.00362393
	LOSS [training: 0.4631382071385827 | validation: 0.6037675478278482]
	TIME [epoch: 6.62 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4897472426139885		[learning rate: 0.0036154]
	Learning Rate: 0.00361538
	LOSS [training: 0.4897472426139885 | validation: 0.6467097166916108]
	TIME [epoch: 6.61 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5393959394917066		[learning rate: 0.0036069]
	Learning Rate: 0.00360685
	LOSS [training: 0.5393959394917066 | validation: 0.5024450092736296]
	TIME [epoch: 6.59 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5484511350609895		[learning rate: 0.0035983]
	Learning Rate: 0.00359834
	LOSS [training: 0.5484511350609895 | validation: 0.6576367681529752]
	TIME [epoch: 6.58 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5231522953508033		[learning rate: 0.0035899]
	Learning Rate: 0.00358986
	LOSS [training: 0.5231522953508033 | validation: 0.48994769184466114]
	TIME [epoch: 6.6 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4756689410642937		[learning rate: 0.0035814]
	Learning Rate: 0.00358139
	LOSS [training: 0.4756689410642937 | validation: 0.4215540754153272]
	TIME [epoch: 6.62 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4785497640185329		[learning rate: 0.0035729]
	Learning Rate: 0.00357294
	LOSS [training: 0.4785497640185329 | validation: 0.5206384257064386]
	TIME [epoch: 6.66 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4455477146870577		[learning rate: 0.0035645]
	Learning Rate: 0.00356451
	LOSS [training: 0.4455477146870577 | validation: 0.4425031154328927]
	TIME [epoch: 6.6 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4576592944549439		[learning rate: 0.0035561]
	Learning Rate: 0.0035561
	LOSS [training: 0.4576592944549439 | validation: 0.32816588819182746]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.367679633134809		[learning rate: 0.0035477]
	Learning Rate: 0.00354771
	LOSS [training: 0.367679633134809 | validation: 0.42548811229291483]
	TIME [epoch: 6.62 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5528875339307189		[learning rate: 0.0035393]
	Learning Rate: 0.00353935
	LOSS [training: 0.5528875339307189 | validation: 0.4677426595597999]
	TIME [epoch: 6.62 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4953761444567519		[learning rate: 0.003531]
	Learning Rate: 0.003531
	LOSS [training: 0.4953761444567519 | validation: 0.35098005050767167]
	TIME [epoch: 6.61 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5227731586531579		[learning rate: 0.0035227]
	Learning Rate: 0.00352267
	LOSS [training: 0.5227731586531579 | validation: 0.5797446010769642]
	TIME [epoch: 6.65 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.536170307121289		[learning rate: 0.0035144]
	Learning Rate: 0.00351436
	LOSS [training: 0.536170307121289 | validation: 0.3976874007414902]
	TIME [epoch: 6.62 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4214729972315121		[learning rate: 0.0035061]
	Learning Rate: 0.00350607
	LOSS [training: 0.4214729972315121 | validation: 0.453658766476165]
	TIME [epoch: 6.62 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4399129948229465		[learning rate: 0.0034978]
	Learning Rate: 0.0034978
	LOSS [training: 0.4399129948229465 | validation: 0.4612499370603337]
	TIME [epoch: 6.62 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43380923836549246		[learning rate: 0.0034895]
	Learning Rate: 0.00348955
	LOSS [training: 0.43380923836549246 | validation: 0.5345228989100135]
	TIME [epoch: 6.62 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.46423857398571283		[learning rate: 0.0034813]
	Learning Rate: 0.00348132
	LOSS [training: 0.46423857398571283 | validation: 0.4788557215649355]
	TIME [epoch: 6.63 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4914766800225746		[learning rate: 0.0034731]
	Learning Rate: 0.00347311
	LOSS [training: 0.4914766800225746 | validation: 0.45408251154312684]
	TIME [epoch: 6.64 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5280277724966803		[learning rate: 0.0034649]
	Learning Rate: 0.00346491
	LOSS [training: 0.5280277724966803 | validation: 0.4464746644220313]
	TIME [epoch: 6.6 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5194204364556543		[learning rate: 0.0034567]
	Learning Rate: 0.00345674
	LOSS [training: 0.5194204364556543 | validation: 0.3933075522540184]
	TIME [epoch: 6.6 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5125500182100963		[learning rate: 0.0034486]
	Learning Rate: 0.00344859
	LOSS [training: 0.5125500182100963 | validation: 0.4102475952240484]
	TIME [epoch: 6.62 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3985375598481397		[learning rate: 0.0034405]
	Learning Rate: 0.00344045
	LOSS [training: 0.3985375598481397 | validation: 0.32378454443773025]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3644599391271849		[learning rate: 0.0034323]
	Learning Rate: 0.00343234
	LOSS [training: 0.3644599391271849 | validation: 0.40692643357734437]
	TIME [epoch: 6.61 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4397595199505758		[learning rate: 0.0034242]
	Learning Rate: 0.00342424
	LOSS [training: 0.4397595199505758 | validation: 0.6187805562623292]
	TIME [epoch: 6.63 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.515504320334349		[learning rate: 0.0034162]
	Learning Rate: 0.00341616
	LOSS [training: 0.515504320334349 | validation: 0.47484114331369975]
	TIME [epoch: 6.62 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4770603690535837		[learning rate: 0.0034081]
	Learning Rate: 0.0034081
	LOSS [training: 0.4770603690535837 | validation: 0.3450996953803249]
	TIME [epoch: 6.61 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40532514162430255		[learning rate: 0.0034001]
	Learning Rate: 0.00340006
	LOSS [training: 0.40532514162430255 | validation: 1.057631213355274]
	TIME [epoch: 6.61 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.60394440769101		[learning rate: 0.003392]
	Learning Rate: 0.00339204
	LOSS [training: 0.60394440769101 | validation: 0.45175353289504083]
	TIME [epoch: 6.62 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43138225143907405		[learning rate: 0.003384]
	Learning Rate: 0.00338404
	LOSS [training: 0.43138225143907405 | validation: 0.5796021948973122]
	TIME [epoch: 6.64 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4538193474949822		[learning rate: 0.0033761]
	Learning Rate: 0.00337606
	LOSS [training: 0.4538193474949822 | validation: 0.3186508808941897]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5513748067717688		[learning rate: 0.0033681]
	Learning Rate: 0.0033681
	LOSS [training: 0.5513748067717688 | validation: 0.5620355694639455]
	TIME [epoch: 6.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4710125091658236		[learning rate: 0.0033602]
	Learning Rate: 0.00336015
	LOSS [training: 0.4710125091658236 | validation: 0.4015380856915477]
	TIME [epoch: 6.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44671380448546305		[learning rate: 0.0033522]
	Learning Rate: 0.00335223
	LOSS [training: 0.44671380448546305 | validation: 0.4136497706640625]
	TIME [epoch: 6.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36551425827048745		[learning rate: 0.0033443]
	Learning Rate: 0.00334432
	LOSS [training: 0.36551425827048745 | validation: 0.48603409262583774]
	TIME [epoch: 6.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45200114830190474		[learning rate: 0.0033364]
	Learning Rate: 0.00333643
	LOSS [training: 0.45200114830190474 | validation: 0.46302073866848403]
	TIME [epoch: 6.62 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41996710051159275		[learning rate: 0.0033286]
	Learning Rate: 0.00332856
	LOSS [training: 0.41996710051159275 | validation: 0.4382047699063152]
	TIME [epoch: 6.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3774620310340774		[learning rate: 0.0033207]
	Learning Rate: 0.00332071
	LOSS [training: 0.3774620310340774 | validation: 0.4870891290866477]
	TIME [epoch: 6.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4207823296379843		[learning rate: 0.0033129]
	Learning Rate: 0.00331288
	LOSS [training: 0.4207823296379843 | validation: 0.4785975173470985]
	TIME [epoch: 6.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3818140803150511		[learning rate: 0.0033051]
	Learning Rate: 0.00330506
	LOSS [training: 0.3818140803150511 | validation: 0.3273772129565333]
	TIME [epoch: 6.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.515464568932783		[learning rate: 0.0032973]
	Learning Rate: 0.00329727
	LOSS [training: 0.515464568932783 | validation: 0.6097612918213857]
	TIME [epoch: 6.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5375674480492266		[learning rate: 0.0032895]
	Learning Rate: 0.00328949
	LOSS [training: 0.5375674480492266 | validation: 0.4614441999992962]
	TIME [epoch: 6.64 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.558010198815311		[learning rate: 0.0032817]
	Learning Rate: 0.00328173
	LOSS [training: 0.558010198815311 | validation: 0.4935002783166645]
	TIME [epoch: 6.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6403999381085125		[learning rate: 0.003274]
	Learning Rate: 0.00327399
	LOSS [training: 0.6403999381085125 | validation: 0.5899228916891446]
	TIME [epoch: 6.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5352697219148268		[learning rate: 0.0032663]
	Learning Rate: 0.00326626
	LOSS [training: 0.5352697219148268 | validation: 0.4238252663310907]
	TIME [epoch: 6.58 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41744531867274076		[learning rate: 0.0032586]
	Learning Rate: 0.00325856
	LOSS [training: 0.41744531867274076 | validation: 0.31795474794943623]
	TIME [epoch: 6.57 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3884247467119355		[learning rate: 0.0032509]
	Learning Rate: 0.00325087
	LOSS [training: 0.3884247467119355 | validation: 0.37261203609105137]
	TIME [epoch: 6.63 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4121104600096256		[learning rate: 0.0032432]
	Learning Rate: 0.0032432
	LOSS [training: 0.4121104600096256 | validation: 0.40262006131215733]
	TIME [epoch: 6.66 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3461922642148586		[learning rate: 0.0032356]
	Learning Rate: 0.00323555
	LOSS [training: 0.3461922642148586 | validation: 0.47889271934585886]
	TIME [epoch: 6.63 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3769437021837912		[learning rate: 0.0032279]
	Learning Rate: 0.00322792
	LOSS [training: 0.3769437021837912 | validation: 0.5275446390436613]
	TIME [epoch: 6.63 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4584510209438091		[learning rate: 0.0032203]
	Learning Rate: 0.00322031
	LOSS [training: 0.4584510209438091 | validation: 0.453501616227559]
	TIME [epoch: 6.63 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4041272506323173		[learning rate: 0.0032127]
	Learning Rate: 0.00321271
	LOSS [training: 0.4041272506323173 | validation: 0.5156452392259319]
	TIME [epoch: 6.63 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41751522065511304		[learning rate: 0.0032051]
	Learning Rate: 0.00320513
	LOSS [training: 0.41751522065511304 | validation: 0.46922366618889977]
	TIME [epoch: 6.65 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4390505994921973		[learning rate: 0.0031976]
	Learning Rate: 0.00319757
	LOSS [training: 0.4390505994921973 | validation: 0.37102433793372025]
	TIME [epoch: 6.65 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.418946286105575		[learning rate: 0.00319]
	Learning Rate: 0.00319003
	LOSS [training: 0.418946286105575 | validation: 0.34789617041207094]
	TIME [epoch: 6.63 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4029776147313152		[learning rate: 0.0031825]
	Learning Rate: 0.00318251
	LOSS [training: 0.4029776147313152 | validation: 0.32412262723682206]
	TIME [epoch: 6.62 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41209612202922064		[learning rate: 0.003175]
	Learning Rate: 0.003175
	LOSS [training: 0.41209612202922064 | validation: 0.5491875756155431]
	TIME [epoch: 6.63 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45765888898056345		[learning rate: 0.0031675]
	Learning Rate: 0.00316751
	LOSS [training: 0.45765888898056345 | validation: 0.3681541606859242]
	TIME [epoch: 6.63 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3663640264299112		[learning rate: 0.00316]
	Learning Rate: 0.00316004
	LOSS [training: 0.3663640264299112 | validation: 0.3821176765333056]
	TIME [epoch: 6.66 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44685752486183716		[learning rate: 0.0031526]
	Learning Rate: 0.00315258
	LOSS [training: 0.44685752486183716 | validation: 0.371396562126436]
	TIME [epoch: 6.65 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.378369119800339		[learning rate: 0.0031451]
	Learning Rate: 0.00314515
	LOSS [training: 0.378369119800339 | validation: 0.4270702213920129]
	TIME [epoch: 6.63 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6815823980891513		[learning rate: 0.0031377]
	Learning Rate: 0.00313773
	LOSS [training: 0.6815823980891513 | validation: 0.4791534622993152]
	TIME [epoch: 6.63 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4729744451338308		[learning rate: 0.0031303]
	Learning Rate: 0.00313033
	LOSS [training: 0.4729744451338308 | validation: 0.3700285361136364]
	TIME [epoch: 6.63 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.368151927746493		[learning rate: 0.0031229]
	Learning Rate: 0.00312294
	LOSS [training: 0.368151927746493 | validation: 0.3399024140750406]
	TIME [epoch: 6.63 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38742949137930494		[learning rate: 0.0031156]
	Learning Rate: 0.00311558
	LOSS [training: 0.38742949137930494 | validation: 0.3852982595751135]
	TIME [epoch: 6.67 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.416628741746084		[learning rate: 0.0031082]
	Learning Rate: 0.00310823
	LOSS [training: 0.416628741746084 | validation: 0.49456138080378376]
	TIME [epoch: 6.63 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5065944666498361		[learning rate: 0.0031009]
	Learning Rate: 0.0031009
	LOSS [training: 0.5065944666498361 | validation: 0.38389082525592155]
	TIME [epoch: 6.63 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3972845904792862		[learning rate: 0.0030936]
	Learning Rate: 0.00309358
	LOSS [training: 0.3972845904792862 | validation: 0.633333584208341]
	TIME [epoch: 6.62 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4956657696696388		[learning rate: 0.0030863]
	Learning Rate: 0.00308628
	LOSS [training: 0.4956657696696388 | validation: 0.5123830102907112]
	TIME [epoch: 6.63 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.556282411713517		[learning rate: 0.003079]
	Learning Rate: 0.003079
	LOSS [training: 0.556282411713517 | validation: 0.5636413471213086]
	TIME [epoch: 6.64 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5655746104844552		[learning rate: 0.0030717]
	Learning Rate: 0.00307174
	LOSS [training: 0.5655746104844552 | validation: 0.6665474783951997]
	TIME [epoch: 6.66 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.724029743707697		[learning rate: 0.0030645]
	Learning Rate: 0.0030645
	LOSS [training: 0.724029743707697 | validation: 0.624834752987679]
	TIME [epoch: 6.63 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5765547882334144		[learning rate: 0.0030573]
	Learning Rate: 0.00305727
	LOSS [training: 0.5765547882334144 | validation: 0.5252507232044906]
	TIME [epoch: 6.63 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4928350740510402		[learning rate: 0.0030501]
	Learning Rate: 0.00305006
	LOSS [training: 0.4928350740510402 | validation: 0.35023571998082037]
	TIME [epoch: 6.63 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43039433743718486		[learning rate: 0.0030429]
	Learning Rate: 0.00304286
	LOSS [training: 0.43039433743718486 | validation: 0.3637437568422121]
	TIME [epoch: 6.63 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38270139328358643		[learning rate: 0.0030357]
	Learning Rate: 0.00303568
	LOSS [training: 0.38270139328358643 | validation: 0.3996747254256943]
	TIME [epoch: 6.67 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3672956915014204		[learning rate: 0.0030285]
	Learning Rate: 0.00302852
	LOSS [training: 0.3672956915014204 | validation: 0.27123491470220756]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33770839166931765		[learning rate: 0.0030214]
	Learning Rate: 0.00302138
	LOSS [training: 0.33770839166931765 | validation: 0.37451567313482137]
	TIME [epoch: 6.62 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36294028154512653		[learning rate: 0.0030143]
	Learning Rate: 0.00301425
	LOSS [training: 0.36294028154512653 | validation: 0.3597437777992907]
	TIME [epoch: 6.63 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4159632151002478		[learning rate: 0.0030071]
	Learning Rate: 0.00300714
	LOSS [training: 0.4159632151002478 | validation: 0.49155546317785137]
	TIME [epoch: 6.63 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41630368397251916		[learning rate: 0.003]
	Learning Rate: 0.00300005
	LOSS [training: 0.41630368397251916 | validation: 0.3947216968488607]
	TIME [epoch: 6.63 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48812895768201187		[learning rate: 0.002993]
	Learning Rate: 0.00299297
	LOSS [training: 0.48812895768201187 | validation: 0.5244573680004561]
	TIME [epoch: 6.66 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40433535351865435		[learning rate: 0.0029859]
	Learning Rate: 0.00298591
	LOSS [training: 0.40433535351865435 | validation: 0.4012786154255079]
	TIME [epoch: 6.63 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39282001734756283		[learning rate: 0.0029789]
	Learning Rate: 0.00297887
	LOSS [training: 0.39282001734756283 | validation: 0.33845667911043453]
	TIME [epoch: 6.63 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.375351702361885		[learning rate: 0.0029718]
	Learning Rate: 0.00297184
	LOSS [training: 0.375351702361885 | validation: 0.4743401310999711]
	TIME [epoch: 6.63 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6013659203103103		[learning rate: 0.0029648]
	Learning Rate: 0.00296483
	LOSS [training: 0.6013659203103103 | validation: 0.6736012857619591]
	TIME [epoch: 6.63 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.586490168081782		[learning rate: 0.0029578]
	Learning Rate: 0.00295784
	LOSS [training: 0.586490168081782 | validation: 0.5929891531237692]
	TIME [epoch: 6.66 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5059508671471852		[learning rate: 0.0029509]
	Learning Rate: 0.00295086
	LOSS [training: 0.5059508671471852 | validation: 0.3894851693059536]
	TIME [epoch: 6.65 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44682854870171945		[learning rate: 0.0029439]
	Learning Rate: 0.0029439
	LOSS [training: 0.44682854870171945 | validation: 0.4452693607845623]
	TIME [epoch: 6.63 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.431412212702285		[learning rate: 0.002937]
	Learning Rate: 0.00293696
	LOSS [training: 0.431412212702285 | validation: 0.4187687881962268]
	TIME [epoch: 6.63 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4037870815047371		[learning rate: 0.00293]
	Learning Rate: 0.00293003
	LOSS [training: 0.4037870815047371 | validation: 0.29524208355942994]
	TIME [epoch: 6.63 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34782320004370426		[learning rate: 0.0029231]
	Learning Rate: 0.00292312
	LOSS [training: 0.34782320004370426 | validation: 0.3801817333889736]
	TIME [epoch: 6.63 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4534727905890378		[learning rate: 0.0029162]
	Learning Rate: 0.00291622
	LOSS [training: 0.4534727905890378 | validation: 0.4495183718245681]
	TIME [epoch: 6.65 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35948201938358576		[learning rate: 0.0029093]
	Learning Rate: 0.00290934
	LOSS [training: 0.35948201938358576 | validation: 0.48883964172232774]
	TIME [epoch: 6.66 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.458817954998538		[learning rate: 0.0029025]
	Learning Rate: 0.00290248
	LOSS [training: 0.458817954998538 | validation: 0.37271258277472435]
	TIME [epoch: 6.63 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41052765145454984		[learning rate: 0.0028956]
	Learning Rate: 0.00289563
	LOSS [training: 0.41052765145454984 | validation: 0.3026436781252799]
	TIME [epoch: 6.63 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37706709022075546		[learning rate: 0.0028888]
	Learning Rate: 0.0028888
	LOSS [training: 0.37706709022075546 | validation: 0.3801619176617491]
	TIME [epoch: 6.63 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3586055153497697		[learning rate: 0.002882]
	Learning Rate: 0.00288199
	LOSS [training: 0.3586055153497697 | validation: 0.3500681741901115]
	TIME [epoch: 6.63 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.346912583546656		[learning rate: 0.0028752]
	Learning Rate: 0.00287519
	LOSS [training: 0.346912583546656 | validation: 0.492807671883412]
	TIME [epoch: 6.67 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4543483890162142		[learning rate: 0.0028684]
	Learning Rate: 0.00286841
	LOSS [training: 0.4543483890162142 | validation: 0.5055704411750275]
	TIME [epoch: 6.64 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.48127047535451767		[learning rate: 0.0028616]
	Learning Rate: 0.00286164
	LOSS [training: 0.48127047535451767 | validation: 0.47007622712376773]
	TIME [epoch: 6.63 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4111201890365116		[learning rate: 0.0028549]
	Learning Rate: 0.00285489
	LOSS [training: 0.4111201890365116 | validation: 0.5129220700407968]
	TIME [epoch: 6.63 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4132336917370919		[learning rate: 0.0028482]
	Learning Rate: 0.00284816
	LOSS [training: 0.4132336917370919 | validation: 0.4618034723397566]
	TIME [epoch: 6.63 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4219745836025212		[learning rate: 0.0028414]
	Learning Rate: 0.00284144
	LOSS [training: 0.4219745836025212 | validation: 0.390844966563391]
	TIME [epoch: 6.64 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41187459589529035		[learning rate: 0.0028347]
	Learning Rate: 0.00283474
	LOSS [training: 0.41187459589529035 | validation: 0.4299197029405092]
	TIME [epoch: 6.66 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.493512257659106		[learning rate: 0.0028281]
	Learning Rate: 0.00282805
	LOSS [training: 0.493512257659106 | validation: 0.4642932098661742]
	TIME [epoch: 6.63 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5371892627126208		[learning rate: 0.0028214]
	Learning Rate: 0.00282138
	LOSS [training: 0.5371892627126208 | validation: 0.44148861463602385]
	TIME [epoch: 6.63 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39322344385578684		[learning rate: 0.0028147]
	Learning Rate: 0.00281472
	LOSS [training: 0.39322344385578684 | validation: 0.3877029507623989]
	TIME [epoch: 6.63 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41243337793725204		[learning rate: 0.0028081]
	Learning Rate: 0.00280808
	LOSS [training: 0.41243337793725204 | validation: 0.43392451510159497]
	TIME [epoch: 6.63 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.410625621194143		[learning rate: 0.0028015]
	Learning Rate: 0.00280146
	LOSS [training: 0.410625621194143 | validation: 0.43056981681646]
	TIME [epoch: 6.66 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4275167195816036		[learning rate: 0.0027949]
	Learning Rate: 0.00279485
	LOSS [training: 0.4275167195816036 | validation: 0.33403042436817465]
	TIME [epoch: 6.65 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3078506880049744		[learning rate: 0.0027883]
	Learning Rate: 0.00278826
	LOSS [training: 0.3078506880049744 | validation: 0.4013742417813598]
	TIME [epoch: 6.63 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34934740951020476		[learning rate: 0.0027817]
	Learning Rate: 0.00278168
	LOSS [training: 0.34934740951020476 | validation: 0.29319303406509456]
	TIME [epoch: 6.63 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3341810397193293		[learning rate: 0.0027751]
	Learning Rate: 0.00277512
	LOSS [training: 0.3341810397193293 | validation: 0.3596646073159985]
	TIME [epoch: 6.63 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32513378199477855		[learning rate: 0.0027686]
	Learning Rate: 0.00276858
	LOSS [training: 0.32513378199477855 | validation: 0.4086189543310741]
	TIME [epoch: 6.63 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36375423817502206		[learning rate: 0.002762]
	Learning Rate: 0.00276205
	LOSS [training: 0.36375423817502206 | validation: 0.4386621292022548]
	TIME [epoch: 6.67 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38300947470936375		[learning rate: 0.0027555]
	Learning Rate: 0.00275553
	LOSS [training: 0.38300947470936375 | validation: 0.3567603840905855]
	TIME [epoch: 6.63 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5076992795306614		[learning rate: 0.002749]
	Learning Rate: 0.00274903
	LOSS [training: 0.5076992795306614 | validation: 0.4128103340922886]
	TIME [epoch: 6.63 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5223040417615415		[learning rate: 0.0027425]
	Learning Rate: 0.00274255
	LOSS [training: 0.5223040417615415 | validation: 0.43394284702901653]
	TIME [epoch: 6.63 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43592910799506035		[learning rate: 0.0027361]
	Learning Rate: 0.00273608
	LOSS [training: 0.43592910799506035 | validation: 0.3849896696690046]
	TIME [epoch: 6.63 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.353983132756035		[learning rate: 0.0027296]
	Learning Rate: 0.00272962
	LOSS [training: 0.353983132756035 | validation: 0.33625018713730725]
	TIME [epoch: 6.64 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38854979654528066		[learning rate: 0.0027232]
	Learning Rate: 0.00272318
	LOSS [training: 0.38854979654528066 | validation: 0.36243660969507785]
	TIME [epoch: 6.66 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3156405796815497		[learning rate: 0.0027168]
	Learning Rate: 0.00271676
	LOSS [training: 0.3156405796815497 | validation: 0.2700144886399034]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30596840385636936		[learning rate: 0.0027104]
	Learning Rate: 0.00271035
	LOSS [training: 0.30596840385636936 | validation: 0.31379587047236646]
	TIME [epoch: 6.62 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37240792095875996		[learning rate: 0.002704]
	Learning Rate: 0.00270396
	LOSS [training: 0.37240792095875996 | validation: 0.4040028847853405]
	TIME [epoch: 6.62 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43826335260124283		[learning rate: 0.0026976]
	Learning Rate: 0.00269758
	LOSS [training: 0.43826335260124283 | validation: 0.39063219350848355]
	TIME [epoch: 6.63 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36826043915206486		[learning rate: 0.0026912]
	Learning Rate: 0.00269122
	LOSS [training: 0.36826043915206486 | validation: 0.5367710414878173]
	TIME [epoch: 6.66 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3803076452477172		[learning rate: 0.0026849]
	Learning Rate: 0.00268487
	LOSS [training: 0.3803076452477172 | validation: 0.3445942026584492]
	TIME [epoch: 6.63 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3733278336012492		[learning rate: 0.0026785]
	Learning Rate: 0.00267854
	LOSS [training: 0.3733278336012492 | validation: 0.3037457098544003]
	TIME [epoch: 6.62 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3360511763867366		[learning rate: 0.0026722]
	Learning Rate: 0.00267222
	LOSS [training: 0.3360511763867366 | validation: 0.38886518465835296]
	TIME [epoch: 6.62 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39144768902514016		[learning rate: 0.0026659]
	Learning Rate: 0.00266591
	LOSS [training: 0.39144768902514016 | validation: 0.47248236144137906]
	TIME [epoch: 6.63 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6127783600559474		[learning rate: 0.0026596]
	Learning Rate: 0.00265963
	LOSS [training: 0.6127783600559474 | validation: 0.36340731815766625]
	TIME [epoch: 6.63 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41388574634075487		[learning rate: 0.0026534]
	Learning Rate: 0.00265335
	LOSS [training: 0.41388574634075487 | validation: 0.46449171486582386]
	TIME [epoch: 6.66 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4622557991871548		[learning rate: 0.0026471]
	Learning Rate: 0.00264709
	LOSS [training: 0.4622557991871548 | validation: 0.5148171123460352]
	TIME [epoch: 6.63 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4981014982865986		[learning rate: 0.0026408]
	Learning Rate: 0.00264085
	LOSS [training: 0.4981014982865986 | validation: 0.3420729339305272]
	TIME [epoch: 6.62 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4419534448188376		[learning rate: 0.0026346]
	Learning Rate: 0.00263462
	LOSS [training: 0.4419534448188376 | validation: 0.42187481051439957]
	TIME [epoch: 6.63 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3904768840070913		[learning rate: 0.0026284]
	Learning Rate: 0.00262841
	LOSS [training: 0.3904768840070913 | validation: 0.3965047036452195]
	TIME [epoch: 6.63 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36839057568727185		[learning rate: 0.0026222]
	Learning Rate: 0.00262221
	LOSS [training: 0.36839057568727185 | validation: 0.34353504912056354]
	TIME [epoch: 6.64 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31503762939158586		[learning rate: 0.002616]
	Learning Rate: 0.00261602
	LOSS [training: 0.31503762939158586 | validation: 0.45885699261485435]
	TIME [epoch: 6.67 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3736686794539362		[learning rate: 0.0026098]
	Learning Rate: 0.00260985
	LOSS [training: 0.3736686794539362 | validation: 0.4215969993438411]
	TIME [epoch: 6.64 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43717563820549954		[learning rate: 0.0026037]
	Learning Rate: 0.00260369
	LOSS [training: 0.43717563820549954 | validation: 0.6716597887904788]
	TIME [epoch: 6.63 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41239050078423434		[learning rate: 0.0025976]
	Learning Rate: 0.00259755
	LOSS [training: 0.41239050078423434 | validation: 0.28771037880472067]
	TIME [epoch: 6.62 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.388817283504137		[learning rate: 0.0025914]
	Learning Rate: 0.00259142
	LOSS [training: 0.388817283504137 | validation: 0.5165529175086021]
	TIME [epoch: 6.62 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3831503848297931		[learning rate: 0.0025853]
	Learning Rate: 0.00258531
	LOSS [training: 0.3831503848297931 | validation: 0.3352409943723168]
	TIME [epoch: 6.65 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3390319269318399		[learning rate: 0.0025792]
	Learning Rate: 0.00257921
	LOSS [training: 0.3390319269318399 | validation: 0.4354171298238664]
	TIME [epoch: 6.65 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3866852939086599		[learning rate: 0.0025731]
	Learning Rate: 0.00257313
	LOSS [training: 0.3866852939086599 | validation: 0.3265251232225712]
	TIME [epoch: 6.63 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32836329429668076		[learning rate: 0.0025671]
	Learning Rate: 0.00256706
	LOSS [training: 0.32836329429668076 | validation: 0.3692274714720297]
	TIME [epoch: 6.62 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36056172417646737		[learning rate: 0.002561]
	Learning Rate: 0.002561
	LOSS [training: 0.36056172417646737 | validation: 0.5055245947618506]
	TIME [epoch: 6.62 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4099250248513603		[learning rate: 0.002555]
	Learning Rate: 0.00255496
	LOSS [training: 0.4099250248513603 | validation: 0.37813631765920624]
	TIME [epoch: 6.62 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3494777549816636		[learning rate: 0.0025489]
	Learning Rate: 0.00254894
	LOSS [training: 0.3494777549816636 | validation: 0.3703937621463236]
	TIME [epoch: 6.67 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3779308203750513		[learning rate: 0.0025429]
	Learning Rate: 0.00254292
	LOSS [training: 0.3779308203750513 | validation: 0.3066980443555975]
	TIME [epoch: 6.62 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32241372794301254		[learning rate: 0.0025369]
	Learning Rate: 0.00253693
	LOSS [training: 0.32241372794301254 | validation: 0.3827806325333254]
	TIME [epoch: 6.62 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37522362328845094		[learning rate: 0.0025309]
	Learning Rate: 0.00253094
	LOSS [training: 0.37522362328845094 | validation: 0.3350917156739416]
	TIME [epoch: 6.63 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35048738465100604		[learning rate: 0.002525]
	Learning Rate: 0.00252497
	LOSS [training: 0.35048738465100604 | validation: 0.3595017535861168]
	TIME [epoch: 6.62 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4284009620043568		[learning rate: 0.002519]
	Learning Rate: 0.00251901
	LOSS [training: 0.4284009620043568 | validation: 0.5473161121677442]
	TIME [epoch: 6.63 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.44955518486132584		[learning rate: 0.0025131]
	Learning Rate: 0.00251307
	LOSS [training: 0.44955518486132584 | validation: 0.43533584702998585]
	TIME [epoch: 6.65 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4140565474521443		[learning rate: 0.0025071]
	Learning Rate: 0.00250715
	LOSS [training: 0.4140565474521443 | validation: 0.4375772248941625]
	TIME [epoch: 6.62 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37388186053282835		[learning rate: 0.0025012]
	Learning Rate: 0.00250123
	LOSS [training: 0.37388186053282835 | validation: 0.3470214251126759]
	TIME [epoch: 6.62 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29851655356512297		[learning rate: 0.0024953]
	Learning Rate: 0.00249533
	LOSS [training: 0.29851655356512297 | validation: 0.32122573690449124]
	TIME [epoch: 6.62 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4172702891941927		[learning rate: 0.0024894]
	Learning Rate: 0.00248945
	LOSS [training: 0.4172702891941927 | validation: 0.2801142519151181]
	TIME [epoch: 6.62 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35562527586173726		[learning rate: 0.0024836]
	Learning Rate: 0.00248357
	LOSS [training: 0.35562527586173726 | validation: 0.39701495184801805]
	TIME [epoch: 6.66 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4008707717360081		[learning rate: 0.0024777]
	Learning Rate: 0.00247771
	LOSS [training: 0.4008707717360081 | validation: 0.39187612896200674]
	TIME [epoch: 6.63 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33844942257750904		[learning rate: 0.0024719]
	Learning Rate: 0.00247187
	LOSS [training: 0.33844942257750904 | validation: 0.3346651520714021]
	TIME [epoch: 6.62 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34661934089694424		[learning rate: 0.002466]
	Learning Rate: 0.00246604
	LOSS [training: 0.34661934089694424 | validation: 0.4604788346439258]
	TIME [epoch: 6.62 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3759257688681994		[learning rate: 0.0024602]
	Learning Rate: 0.00246022
	LOSS [training: 0.3759257688681994 | validation: 0.3848229602566021]
	TIME [epoch: 6.62 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33704178123344075		[learning rate: 0.0024544]
	Learning Rate: 0.00245442
	LOSS [training: 0.33704178123344075 | validation: 0.4684805841307711]
	TIME [epoch: 6.63 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4185427592634733		[learning rate: 0.0024486]
	Learning Rate: 0.00244863
	LOSS [training: 0.4185427592634733 | validation: 0.34534809224744595]
	TIME [epoch: 6.66 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33305511498620577		[learning rate: 0.0024429]
	Learning Rate: 0.00244285
	LOSS [training: 0.33305511498620577 | validation: 0.3232895446786518]
	TIME [epoch: 6.62 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30185335223707666		[learning rate: 0.0024371]
	Learning Rate: 0.00243709
	LOSS [training: 0.30185335223707666 | validation: 0.3360616846566372]
	TIME [epoch: 6.62 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3881261582480546		[learning rate: 0.0024313]
	Learning Rate: 0.00243134
	LOSS [training: 0.3881261582480546 | validation: 0.37268460732689357]
	TIME [epoch: 6.62 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3894332799555153		[learning rate: 0.0024256]
	Learning Rate: 0.00242561
	LOSS [training: 0.3894332799555153 | validation: 0.5254086160649536]
	TIME [epoch: 6.62 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37755819632814247		[learning rate: 0.0024199]
	Learning Rate: 0.00241989
	LOSS [training: 0.37755819632814247 | validation: 0.282372976160932]
	TIME [epoch: 6.64 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3352893172423933		[learning rate: 0.0024142]
	Learning Rate: 0.00241418
	LOSS [training: 0.3352893172423933 | validation: 0.26712658186311344]
	TIME [epoch: 6.66 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_652.pth
	Model improved!!!
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4048700998672972		[learning rate: 0.0024085]
	Learning Rate: 0.00240848
	LOSS [training: 0.4048700998672972 | validation: 0.5140196475336968]
	TIME [epoch: 6.62 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39268711116147337		[learning rate: 0.0024028]
	Learning Rate: 0.0024028
	LOSS [training: 0.39268711116147337 | validation: 0.3078790559053365]
	TIME [epoch: 6.61 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34271605195616817		[learning rate: 0.0023971]
	Learning Rate: 0.00239713
	LOSS [training: 0.34271605195616817 | validation: 0.3859231139224363]
	TIME [epoch: 6.62 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3784519192446095		[learning rate: 0.0023915]
	Learning Rate: 0.00239148
	LOSS [training: 0.3784519192446095 | validation: 0.26697787226097897]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_656.pth
	Model improved!!!
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3199968464743527		[learning rate: 0.0023858]
	Learning Rate: 0.00238584
	LOSS [training: 0.3199968464743527 | validation: 0.2746395455436972]
	TIME [epoch: 6.65 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31687148819022776		[learning rate: 0.0023802]
	Learning Rate: 0.00238021
	LOSS [training: 0.31687148819022776 | validation: 0.315364075340809]
	TIME [epoch: 6.61 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32425350780968565		[learning rate: 0.0023746]
	Learning Rate: 0.0023746
	LOSS [training: 0.32425350780968565 | validation: 0.30742711517147625]
	TIME [epoch: 6.61 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35801002241385316		[learning rate: 0.002369]
	Learning Rate: 0.00236899
	LOSS [training: 0.35801002241385316 | validation: 0.292853710423867]
	TIME [epoch: 6.61 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30390133585096785		[learning rate: 0.0023634]
	Learning Rate: 0.00236341
	LOSS [training: 0.30390133585096785 | validation: 0.33755559212540687]
	TIME [epoch: 6.62 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3131076534521182		[learning rate: 0.0023578]
	Learning Rate: 0.00235783
	LOSS [training: 0.3131076534521182 | validation: 0.3626396231569543]
	TIME [epoch: 6.62 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3781153001557419		[learning rate: 0.0023523]
	Learning Rate: 0.00235227
	LOSS [training: 0.3781153001557419 | validation: 0.3424174693840144]
	TIME [epoch: 6.64 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36378863388389165		[learning rate: 0.0023467]
	Learning Rate: 0.00234672
	LOSS [training: 0.36378863388389165 | validation: 0.428538237094322]
	TIME [epoch: 6.61 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4470935559119466		[learning rate: 0.0023412]
	Learning Rate: 0.00234119
	LOSS [training: 0.4470935559119466 | validation: 0.5435811799519372]
	TIME [epoch: 6.61 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36210054410961096		[learning rate: 0.0023357]
	Learning Rate: 0.00233566
	LOSS [training: 0.36210054410961096 | validation: 0.3761472068878809]
	TIME [epoch: 6.61 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30411990083896734		[learning rate: 0.0023302]
	Learning Rate: 0.00233015
	LOSS [training: 0.30411990083896734 | validation: 0.36289506324749043]
	TIME [epoch: 6.61 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.38423444807108564		[learning rate: 0.0023247]
	Learning Rate: 0.00232466
	LOSS [training: 0.38423444807108564 | validation: 0.381424092724307]
	TIME [epoch: 6.62 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.41321896655285784		[learning rate: 0.0023192]
	Learning Rate: 0.00231917
	LOSS [training: 0.41321896655285784 | validation: 0.47137772626822894]
	TIME [epoch: 6.64 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3544636820236377		[learning rate: 0.0023137]
	Learning Rate: 0.0023137
	LOSS [training: 0.3544636820236377 | validation: 0.2817791848698615]
	TIME [epoch: 6.61 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31577970307253406		[learning rate: 0.0023082]
	Learning Rate: 0.00230825
	LOSS [training: 0.31577970307253406 | validation: 0.31846770351376275]
	TIME [epoch: 6.61 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.382593251459057		[learning rate: 0.0023028]
	Learning Rate: 0.0023028
	LOSS [training: 0.382593251459057 | validation: 0.5304352924473719]
	TIME [epoch: 6.61 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4523196856511936		[learning rate: 0.0022974]
	Learning Rate: 0.00229737
	LOSS [training: 0.4523196856511936 | validation: 0.3086314768604664]
	TIME [epoch: 6.61 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30781115601428777		[learning rate: 0.0022919]
	Learning Rate: 0.00229195
	LOSS [training: 0.30781115601428777 | validation: 0.28007612595001763]
	TIME [epoch: 6.64 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3280893072558445		[learning rate: 0.0022865]
	Learning Rate: 0.00228654
	LOSS [training: 0.3280893072558445 | validation: 0.36607765076234655]
	TIME [epoch: 6.63 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33832826091993456		[learning rate: 0.0022811]
	Learning Rate: 0.00228115
	LOSS [training: 0.33832826091993456 | validation: 0.3075972561825552]
	TIME [epoch: 6.61 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31517312825870303		[learning rate: 0.0022758]
	Learning Rate: 0.00227577
	LOSS [training: 0.31517312825870303 | validation: 0.260722153827617]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_677.pth
	Model improved!!!
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.42039068939517865		[learning rate: 0.0022704]
	Learning Rate: 0.0022704
	LOSS [training: 0.42039068939517865 | validation: 0.4409724795473012]
	TIME [epoch: 6.61 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36578346855552446		[learning rate: 0.002265]
	Learning Rate: 0.00226505
	LOSS [training: 0.36578346855552446 | validation: 0.35906150772402967]
	TIME [epoch: 6.62 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33558605539418696		[learning rate: 0.0022597]
	Learning Rate: 0.0022597
	LOSS [training: 0.33558605539418696 | validation: 0.4123044398298804]
	TIME [epoch: 6.65 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3894075762736717		[learning rate: 0.0022544]
	Learning Rate: 0.00225437
	LOSS [training: 0.3894075762736717 | validation: 0.5040503377490225]
	TIME [epoch: 6.61 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3752466130652904		[learning rate: 0.0022491]
	Learning Rate: 0.00224905
	LOSS [training: 0.3752466130652904 | validation: 0.3695487183946338]
	TIME [epoch: 6.62 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36069290477283167		[learning rate: 0.0022437]
	Learning Rate: 0.00224375
	LOSS [training: 0.36069290477283167 | validation: 0.3811533432968351]
	TIME [epoch: 6.61 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3714658610500272		[learning rate: 0.0022385]
	Learning Rate: 0.00223846
	LOSS [training: 0.3714658610500272 | validation: 0.546487095596179]
	TIME [epoch: 6.61 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3816025118083946		[learning rate: 0.0022332]
	Learning Rate: 0.00223318
	LOSS [training: 0.3816025118083946 | validation: 0.28756510724991824]
	TIME [epoch: 6.63 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31031159194089125		[learning rate: 0.0022279]
	Learning Rate: 0.00222791
	LOSS [training: 0.31031159194089125 | validation: 0.30184803390967563]
	TIME [epoch: 6.63 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3776659518878753		[learning rate: 0.0022227]
	Learning Rate: 0.00222265
	LOSS [training: 0.3776659518878753 | validation: 0.27288352490142276]
	TIME [epoch: 6.61 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3569241003751236		[learning rate: 0.0022174]
	Learning Rate: 0.00221741
	LOSS [training: 0.3569241003751236 | validation: 0.2613691915671983]
	TIME [epoch: 6.61 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33213002585527285		[learning rate: 0.0022122]
	Learning Rate: 0.00221218
	LOSS [training: 0.33213002585527285 | validation: 0.38605692261030444]
	TIME [epoch: 6.61 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31380860748016975		[learning rate: 0.002207]
	Learning Rate: 0.00220696
	LOSS [training: 0.31380860748016975 | validation: 0.36825152217641877]
	TIME [epoch: 6.61 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3316693148991862		[learning rate: 0.0022018]
	Learning Rate: 0.00220176
	LOSS [training: 0.3316693148991862 | validation: 0.4734413767949003]
	TIME [epoch: 6.63 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3753469345370686		[learning rate: 0.0021966]
	Learning Rate: 0.00219656
	LOSS [training: 0.3753469345370686 | validation: 0.33571490308261687]
	TIME [epoch: 6.64 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3359550619985097		[learning rate: 0.0021914]
	Learning Rate: 0.00219138
	LOSS [training: 0.3359550619985097 | validation: 0.32260575032993]
	TIME [epoch: 6.62 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3175289839224476		[learning rate: 0.0021862]
	Learning Rate: 0.00218621
	LOSS [training: 0.3175289839224476 | validation: 0.26653635852169666]
	TIME [epoch: 6.61 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.548795039646832		[learning rate: 0.0021811]
	Learning Rate: 0.00218106
	LOSS [training: 0.548795039646832 | validation: 0.5615218808015536]
	TIME [epoch: 6.61 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4302764778254045		[learning rate: 0.0021759]
	Learning Rate: 0.00217591
	LOSS [training: 0.4302764778254045 | validation: 0.45035129570562116]
	TIME [epoch: 6.61 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4215160294432449		[learning rate: 0.0021708]
	Learning Rate: 0.00217078
	LOSS [training: 0.4215160294432449 | validation: 0.46597735569572374]
	TIME [epoch: 6.65 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36625978877685206		[learning rate: 0.0021657]
	Learning Rate: 0.00216566
	LOSS [training: 0.36625978877685206 | validation: 0.32496894059583126]
	TIME [epoch: 6.62 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3084416322130816		[learning rate: 0.0021605]
	Learning Rate: 0.00216055
	LOSS [training: 0.3084416322130816 | validation: 0.37182319987673884]
	TIME [epoch: 6.62 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31456050628037396		[learning rate: 0.0021555]
	Learning Rate: 0.00215545
	LOSS [training: 0.31456050628037396 | validation: 0.295198196679692]
	TIME [epoch: 6.61 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4169321038287016		[learning rate: 0.0021504]
	Learning Rate: 0.00215037
	LOSS [training: 0.4169321038287016 | validation: 0.35696384194162745]
	TIME [epoch: 6.61 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3791469453944587		[learning rate: 0.0021453]
	Learning Rate: 0.0021453
	LOSS [training: 0.3791469453944587 | validation: 0.31179403279326195]
	TIME [epoch: 6.62 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3315486545618344		[learning rate: 0.0021402]
	Learning Rate: 0.00214024
	LOSS [training: 0.3315486545618344 | validation: 0.25585425663714545]
	TIME [epoch: 6.65 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_703.pth
	Model improved!!!
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2747154927077362		[learning rate: 0.0021352]
	Learning Rate: 0.00213519
	LOSS [training: 0.2747154927077362 | validation: 0.3077181459880365]
	TIME [epoch: 6.63 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.37878149392366806		[learning rate: 0.0021302]
	Learning Rate: 0.00213015
	LOSS [training: 0.37878149392366806 | validation: 0.3790124774420634]
	TIME [epoch: 6.62 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3549186876838955		[learning rate: 0.0021251]
	Learning Rate: 0.00212513
	LOSS [training: 0.3549186876838955 | validation: 0.3176237830498855]
	TIME [epoch: 6.62 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30820305273067505		[learning rate: 0.0021201]
	Learning Rate: 0.00212011
	LOSS [training: 0.30820305273067505 | validation: 0.2821586569891471]
	TIME [epoch: 6.61 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34569553500051203		[learning rate: 0.0021151]
	Learning Rate: 0.00211511
	LOSS [training: 0.34569553500051203 | validation: 0.29446602807471084]
	TIME [epoch: 6.64 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3422245447141704		[learning rate: 0.0021101]
	Learning Rate: 0.00211012
	LOSS [training: 0.3422245447141704 | validation: 0.3282542752025229]
	TIME [epoch: 6.63 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3156478103905732		[learning rate: 0.0021051]
	Learning Rate: 0.00210515
	LOSS [training: 0.3156478103905732 | validation: 0.2996186124069816]
	TIME [epoch: 6.61 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3426949188368864		[learning rate: 0.0021002]
	Learning Rate: 0.00210018
	LOSS [training: 0.3426949188368864 | validation: 0.3771338202976155]
	TIME [epoch: 6.62 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3313888212725181		[learning rate: 0.0020952]
	Learning Rate: 0.00209523
	LOSS [training: 0.3313888212725181 | validation: 0.41124088632963585]
	TIME [epoch: 6.61 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3393484371519678		[learning rate: 0.0020903]
	Learning Rate: 0.00209028
	LOSS [training: 0.3393484371519678 | validation: 0.5210966044079594]
	TIME [epoch: 6.61 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.4054425373831254		[learning rate: 0.0020854]
	Learning Rate: 0.00208535
	LOSS [training: 0.4054425373831254 | validation: 0.31132192641157175]
	TIME [epoch: 6.63 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.36135364793905944		[learning rate: 0.0020804]
	Learning Rate: 0.00208043
	LOSS [training: 0.36135364793905944 | validation: 0.4072135627601843]
	TIME [epoch: 6.62 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32487495285372747		[learning rate: 0.0020755]
	Learning Rate: 0.00207553
	LOSS [training: 0.32487495285372747 | validation: 0.2851681253967753]
	TIME [epoch: 6.61 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35325728041677334		[learning rate: 0.0020706]
	Learning Rate: 0.00207063
	LOSS [training: 0.35325728041677334 | validation: 0.33520578140691065]
	TIME [epoch: 6.61 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35505248328632444		[learning rate: 0.0020657]
	Learning Rate: 0.00206575
	LOSS [training: 0.35505248328632444 | validation: 0.320664912635788]
	TIME [epoch: 6.61 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35767315091952423		[learning rate: 0.0020609]
	Learning Rate: 0.00206087
	LOSS [training: 0.35767315091952423 | validation: 0.2919475704010113]
	TIME [epoch: 6.6 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34568185883432234		[learning rate: 0.002056]
	Learning Rate: 0.00205601
	LOSS [training: 0.34568185883432234 | validation: 0.35760527850259627]
	TIME [epoch: 6.62 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3268018232964984		[learning rate: 0.0020512]
	Learning Rate: 0.00205116
	LOSS [training: 0.3268018232964984 | validation: 0.31545600294432347]
	TIME [epoch: 6.63 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3001880641456989		[learning rate: 0.0020463]
	Learning Rate: 0.00204632
	LOSS [training: 0.3001880641456989 | validation: 0.40723086774365636]
	TIME [epoch: 6.61 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3072062067155046		[learning rate: 0.0020415]
	Learning Rate: 0.0020415
	LOSS [training: 0.3072062067155046 | validation: 0.39100416048969294]
	TIME [epoch: 6.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3052045516889734		[learning rate: 0.0020367]
	Learning Rate: 0.00203668
	LOSS [training: 0.3052045516889734 | validation: 0.35737120151070634]
	TIME [epoch: 6.6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39689643938200175		[learning rate: 0.0020319]
	Learning Rate: 0.00203188
	LOSS [training: 0.39689643938200175 | validation: 0.3399487993464106]
	TIME [epoch: 6.61 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3441022856966275		[learning rate: 0.0020271]
	Learning Rate: 0.00202708
	LOSS [training: 0.3441022856966275 | validation: 0.4201259837776511]
	TIME [epoch: 6.65 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34215489689142214		[learning rate: 0.0020223]
	Learning Rate: 0.0020223
	LOSS [training: 0.34215489689142214 | validation: 0.3585880061403157]
	TIME [epoch: 6.62 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2950909946435698		[learning rate: 0.0020175]
	Learning Rate: 0.00201753
	LOSS [training: 0.2950909946435698 | validation: 0.3212083851351823]
	TIME [epoch: 6.61 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31449219571417486		[learning rate: 0.0020128]
	Learning Rate: 0.00201277
	LOSS [training: 0.31449219571417486 | validation: 0.2596511212098379]
	TIME [epoch: 6.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29175523237863726		[learning rate: 0.002008]
	Learning Rate: 0.00200803
	LOSS [training: 0.29175523237863726 | validation: 0.3545795833297458]
	TIME [epoch: 6.61 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3061928112270076		[learning rate: 0.0020033]
	Learning Rate: 0.00200329
	LOSS [training: 0.3061928112270076 | validation: 0.3810712552122505]
	TIME [epoch: 6.61 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3101692752925856		[learning rate: 0.0019986]
	Learning Rate: 0.00199856
	LOSS [training: 0.3101692752925856 | validation: 0.4078875280961515]
	TIME [epoch: 6.63 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3560799102962534		[learning rate: 0.0019938]
	Learning Rate: 0.00199385
	LOSS [training: 0.3560799102962534 | validation: 0.28100218089225715]
	TIME [epoch: 6.61 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32537783808489334		[learning rate: 0.0019891]
	Learning Rate: 0.00198915
	LOSS [training: 0.32537783808489334 | validation: 0.365291729780141]
	TIME [epoch: 6.6 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30176347692728356		[learning rate: 0.0019845]
	Learning Rate: 0.00198445
	LOSS [training: 0.30176347692728356 | validation: 0.2567608465106932]
	TIME [epoch: 6.6 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32236235955580483		[learning rate: 0.0019798]
	Learning Rate: 0.00197977
	LOSS [training: 0.32236235955580483 | validation: 0.3610251967100704]
	TIME [epoch: 6.6 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3073699069530898		[learning rate: 0.0019751]
	Learning Rate: 0.0019751
	LOSS [training: 0.3073699069530898 | validation: 0.2702832536144571]
	TIME [epoch: 6.63 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28902863294882347		[learning rate: 0.0019704]
	Learning Rate: 0.00197044
	LOSS [training: 0.28902863294882347 | validation: 0.3816386233806519]
	TIME [epoch: 6.62 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30974192018071545		[learning rate: 0.0019658]
	Learning Rate: 0.0019658
	LOSS [training: 0.30974192018071545 | validation: 0.3568315333071971]
	TIME [epoch: 6.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39691580374392804		[learning rate: 0.0019612]
	Learning Rate: 0.00196116
	LOSS [training: 0.39691580374392804 | validation: 0.4814698094866198]
	TIME [epoch: 6.6 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33181421776394365		[learning rate: 0.0019565]
	Learning Rate: 0.00195653
	LOSS [training: 0.33181421776394365 | validation: 0.30141613653863175]
	TIME [epoch: 6.6 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2933740915324744		[learning rate: 0.0019519]
	Learning Rate: 0.00195192
	LOSS [training: 0.2933740915324744 | validation: 0.31257417582381175]
	TIME [epoch: 6.6 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29948975079904916		[learning rate: 0.0019473]
	Learning Rate: 0.00194731
	LOSS [training: 0.29948975079904916 | validation: 0.3018807488059856]
	TIME [epoch: 6.64 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2695551130329413		[learning rate: 0.0019427]
	Learning Rate: 0.00194272
	LOSS [training: 0.2695551130329413 | validation: 0.2931852011730697]
	TIME [epoch: 6.61 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3799407849448005		[learning rate: 0.0019381]
	Learning Rate: 0.00193814
	LOSS [training: 0.3799407849448005 | validation: 0.38786230078094586]
	TIME [epoch: 6.6 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3811586247341213		[learning rate: 0.0019336]
	Learning Rate: 0.00193357
	LOSS [training: 0.3811586247341213 | validation: 0.3634375012249778]
	TIME [epoch: 6.6 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31364131762431474		[learning rate: 0.001929]
	Learning Rate: 0.001929
	LOSS [training: 0.31364131762431474 | validation: 0.286961960650211]
	TIME [epoch: 6.6 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2829920633423981		[learning rate: 0.0019245]
	Learning Rate: 0.00192445
	LOSS [training: 0.2829920633423981 | validation: 0.274955583883475]
	TIME [epoch: 6.61 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29624686933648486		[learning rate: 0.0019199]
	Learning Rate: 0.00191992
	LOSS [training: 0.29624686933648486 | validation: 0.2550326321355672]
	TIME [epoch: 6.63 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.288920039374348		[learning rate: 0.0019154]
	Learning Rate: 0.00191539
	LOSS [training: 0.288920039374348 | validation: 0.29107479819753657]
	TIME [epoch: 6.61 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2779745330768021		[learning rate: 0.0019109]
	Learning Rate: 0.00191087
	LOSS [training: 0.2779745330768021 | validation: 0.3021737999862487]
	TIME [epoch: 6.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27978792514102463		[learning rate: 0.0019064]
	Learning Rate: 0.00190636
	LOSS [training: 0.27978792514102463 | validation: 0.26084627662654425]
	TIME [epoch: 6.6 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2908914770327846		[learning rate: 0.0019019]
	Learning Rate: 0.00190186
	LOSS [training: 0.2908914770327846 | validation: 0.3370064985704624]
	TIME [epoch: 6.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28249316790915213		[learning rate: 0.0018974]
	Learning Rate: 0.00189738
	LOSS [training: 0.28249316790915213 | validation: 0.31620948089935885]
	TIME [epoch: 6.64 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2879412025671724		[learning rate: 0.0018929]
	Learning Rate: 0.0018929
	LOSS [training: 0.2879412025671724 | validation: 0.2679855103545381]
	TIME [epoch: 6.6 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2695946183140129		[learning rate: 0.0018884]
	Learning Rate: 0.00188844
	LOSS [training: 0.2695946183140129 | validation: 0.26174716531831216]
	TIME [epoch: 6.6 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2687298714096465		[learning rate: 0.001884]
	Learning Rate: 0.00188398
	LOSS [training: 0.2687298714096465 | validation: 0.3105879879619535]
	TIME [epoch: 6.6 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25812181502739884		[learning rate: 0.0018795]
	Learning Rate: 0.00187954
	LOSS [training: 0.25812181502739884 | validation: 0.26347378067918464]
	TIME [epoch: 6.6 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28936268145118527		[learning rate: 0.0018751]
	Learning Rate: 0.00187511
	LOSS [training: 0.28936268145118527 | validation: 0.3666659353998948]
	TIME [epoch: 6.6 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2788380358150322		[learning rate: 0.0018707]
	Learning Rate: 0.00187068
	LOSS [training: 0.2788380358150322 | validation: 0.2592809632366997]
	TIME [epoch: 6.62 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2684728388733363		[learning rate: 0.0018663]
	Learning Rate: 0.00186627
	LOSS [training: 0.2684728388733363 | validation: 0.26225543586614253]
	TIME [epoch: 6.61 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2529466198827838		[learning rate: 0.0018619]
	Learning Rate: 0.00186187
	LOSS [training: 0.2529466198827838 | validation: 0.30413957702126004]
	TIME [epoch: 6.59 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28666346345468885		[learning rate: 0.0018575]
	Learning Rate: 0.00185748
	LOSS [training: 0.28666346345468885 | validation: 0.32847958896153717]
	TIME [epoch: 6.59 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33222156430648025		[learning rate: 0.0018531]
	Learning Rate: 0.00185309
	LOSS [training: 0.33222156430648025 | validation: 0.3827701556087756]
	TIME [epoch: 6.59 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.310565836816726		[learning rate: 0.0018487]
	Learning Rate: 0.00184872
	LOSS [training: 0.310565836816726 | validation: 0.30325227032850277]
	TIME [epoch: 6.59 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29777260472775774		[learning rate: 0.0018444]
	Learning Rate: 0.00184436
	LOSS [training: 0.29777260472775774 | validation: 0.3025309329603216]
	TIME [epoch: 6.62 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2717228279836576		[learning rate: 0.00184]
	Learning Rate: 0.00184001
	LOSS [training: 0.2717228279836576 | validation: 0.31282057088958637]
	TIME [epoch: 6.62 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.43710399362573976		[learning rate: 0.0018357]
	Learning Rate: 0.00183567
	LOSS [training: 0.43710399362573976 | validation: 0.3161195245005675]
	TIME [epoch: 6.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30052450784035717		[learning rate: 0.0018313]
	Learning Rate: 0.00183134
	LOSS [training: 0.30052450784035717 | validation: 0.2692563279921921]
	TIME [epoch: 6.6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25532072394264105		[learning rate: 0.001827]
	Learning Rate: 0.00182702
	LOSS [training: 0.25532072394264105 | validation: 0.3141962764879762]
	TIME [epoch: 6.6 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2741881393580082		[learning rate: 0.0018227]
	Learning Rate: 0.00182271
	LOSS [training: 0.2741881393580082 | validation: 0.3124220068666646]
	TIME [epoch: 6.6 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2946522644916463		[learning rate: 0.0018184]
	Learning Rate: 0.00181841
	LOSS [training: 0.2946522644916463 | validation: 0.37820098772098665]
	TIME [epoch: 6.64 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2920391079429835		[learning rate: 0.0018141]
	Learning Rate: 0.00181412
	LOSS [training: 0.2920391079429835 | validation: 0.27119987252176814]
	TIME [epoch: 6.6 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28942808657653757		[learning rate: 0.0018098]
	Learning Rate: 0.00180984
	LOSS [training: 0.28942808657653757 | validation: 0.3192570526624119]
	TIME [epoch: 6.6 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28219018022934533		[learning rate: 0.0018056]
	Learning Rate: 0.00180557
	LOSS [training: 0.28219018022934533 | validation: 0.26886943776168354]
	TIME [epoch: 6.59 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30326818218591195		[learning rate: 0.0018013]
	Learning Rate: 0.00180132
	LOSS [training: 0.30326818218591195 | validation: 0.4034359205980883]
	TIME [epoch: 6.59 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40405560249973493		[learning rate: 0.0017971]
	Learning Rate: 0.00179707
	LOSS [training: 0.40405560249973493 | validation: 0.567799915396753]
	TIME [epoch: 6.6 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5743965474117062		[learning rate: 0.0017928]
	Learning Rate: 0.00179283
	LOSS [training: 0.5743965474117062 | validation: 0.5850753355949114]
	TIME [epoch: 6.62 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.5031288075638374		[learning rate: 0.0017886]
	Learning Rate: 0.0017886
	LOSS [training: 0.5031288075638374 | validation: 0.3922302112285291]
	TIME [epoch: 6.6 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.39660629477516257		[learning rate: 0.0017844]
	Learning Rate: 0.00178438
	LOSS [training: 0.39660629477516257 | validation: 0.40321557863498714]
	TIME [epoch: 6.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33183044251004085		[learning rate: 0.0017802]
	Learning Rate: 0.00178017
	LOSS [training: 0.33183044251004085 | validation: 0.33613140141793274]
	TIME [epoch: 6.6 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3249956283665275		[learning rate: 0.001776]
	Learning Rate: 0.00177597
	LOSS [training: 0.3249956283665275 | validation: 0.35985575282314775]
	TIME [epoch: 6.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31911482919296785		[learning rate: 0.0017718]
	Learning Rate: 0.00177178
	LOSS [training: 0.31911482919296785 | validation: 0.3192977867132221]
	TIME [epoch: 6.63 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.33008109705666183		[learning rate: 0.0017676]
	Learning Rate: 0.0017676
	LOSS [training: 0.33008109705666183 | validation: 0.2854460393093858]
	TIME [epoch: 6.59 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26896904367704605		[learning rate: 0.0017634]
	Learning Rate: 0.00176343
	LOSS [training: 0.26896904367704605 | validation: 0.2828071731166133]
	TIME [epoch: 6.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32055633142303713		[learning rate: 0.0017593]
	Learning Rate: 0.00175927
	LOSS [training: 0.32055633142303713 | validation: 0.4826690914903402]
	TIME [epoch: 6.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.45263814026212845		[learning rate: 0.0017551]
	Learning Rate: 0.00175512
	LOSS [training: 0.45263814026212845 | validation: 0.49706130831206263]
	TIME [epoch: 6.6 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.40977545929406584		[learning rate: 0.001751]
	Learning Rate: 0.00175098
	LOSS [training: 0.40977545929406584 | validation: 0.32845714987358676]
	TIME [epoch: 6.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30537867999005447		[learning rate: 0.0017469]
	Learning Rate: 0.00174685
	LOSS [training: 0.30537867999005447 | validation: 0.3520194901276214]
	TIME [epoch: 6.63 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3039265506518087		[learning rate: 0.0017427]
	Learning Rate: 0.00174273
	LOSS [training: 0.3039265506518087 | validation: 0.2577243333586673]
	TIME [epoch: 6.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2692911821289234		[learning rate: 0.0017386]
	Learning Rate: 0.00173862
	LOSS [training: 0.2692911821289234 | validation: 0.22687727502114535]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_791.pth
	Model improved!!!
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24546032181111385		[learning rate: 0.0017345]
	Learning Rate: 0.00173452
	LOSS [training: 0.24546032181111385 | validation: 0.23514195238322744]
	TIME [epoch: 6.6 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.280305855932789		[learning rate: 0.0017304]
	Learning Rate: 0.00173043
	LOSS [training: 0.280305855932789 | validation: 0.2714657700201633]
	TIME [epoch: 6.6 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3067994096680531		[learning rate: 0.0017263]
	Learning Rate: 0.00172635
	LOSS [training: 0.3067994096680531 | validation: 0.2954859894422879]
	TIME [epoch: 6.61 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3012554443568905		[learning rate: 0.0017223]
	Learning Rate: 0.00172228
	LOSS [training: 0.3012554443568905 | validation: 0.3110113719113361]
	TIME [epoch: 6.62 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2635406265208334		[learning rate: 0.0017182]
	Learning Rate: 0.00171821
	LOSS [training: 0.2635406265208334 | validation: 0.26612221231670796]
	TIME [epoch: 6.59 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27572872327008374		[learning rate: 0.0017142]
	Learning Rate: 0.00171416
	LOSS [training: 0.27572872327008374 | validation: 0.2737081298099152]
	TIME [epoch: 6.6 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26893902066290004		[learning rate: 0.0017101]
	Learning Rate: 0.00171012
	LOSS [training: 0.26893902066290004 | validation: 0.28420389711898125]
	TIME [epoch: 6.58 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2492125184450679		[learning rate: 0.0017061]
	Learning Rate: 0.00170608
	LOSS [training: 0.2492125184450679 | validation: 0.2834006303415796]
	TIME [epoch: 6.59 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3153606192579431		[learning rate: 0.0017021]
	Learning Rate: 0.00170206
	LOSS [training: 0.3153606192579431 | validation: 0.28917549482022764]
	TIME [epoch: 6.63 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2708226856811316		[learning rate: 0.001698]
	Learning Rate: 0.00169804
	LOSS [training: 0.2708226856811316 | validation: 0.2979409416106155]
	TIME [epoch: 6.59 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2940488873684097		[learning rate: 0.001694]
	Learning Rate: 0.00169404
	LOSS [training: 0.2940488873684097 | validation: 0.3448046425501431]
	TIME [epoch: 6.57 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2811001419716296		[learning rate: 0.00169]
	Learning Rate: 0.00169004
	LOSS [training: 0.2811001419716296 | validation: 0.26428225114635223]
	TIME [epoch: 6.58 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2616739122913109		[learning rate: 0.0016861]
	Learning Rate: 0.00168606
	LOSS [training: 0.2616739122913109 | validation: 0.274607710043095]
	TIME [epoch: 6.6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2800092210724141		[learning rate: 0.0016821]
	Learning Rate: 0.00168208
	LOSS [training: 0.2800092210724141 | validation: 0.2503824097314218]
	TIME [epoch: 6.59 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2690917375235638		[learning rate: 0.0016781]
	Learning Rate: 0.00167811
	LOSS [training: 0.2690917375235638 | validation: 0.32457245897357356]
	TIME [epoch: 6.63 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2979507956086539		[learning rate: 0.0016742]
	Learning Rate: 0.00167415
	LOSS [training: 0.2979507956086539 | validation: 0.28885474307036546]
	TIME [epoch: 6.59 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30921323870726747		[learning rate: 0.0016702]
	Learning Rate: 0.0016702
	LOSS [training: 0.30921323870726747 | validation: 0.2664808949510318]
	TIME [epoch: 6.58 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2930244389005083		[learning rate: 0.0016663]
	Learning Rate: 0.00166626
	LOSS [training: 0.2930244389005083 | validation: 0.2503475400760013]
	TIME [epoch: 6.6 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28258160392489795		[learning rate: 0.0016623]
	Learning Rate: 0.00166233
	LOSS [training: 0.28258160392489795 | validation: 0.3200372307115027]
	TIME [epoch: 6.59 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28173812763950484		[learning rate: 0.0016584]
	Learning Rate: 0.00165841
	LOSS [training: 0.28173812763950484 | validation: 0.2688853374357351]
	TIME [epoch: 6.6 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2777343012194766		[learning rate: 0.0016545]
	Learning Rate: 0.0016545
	LOSS [training: 0.2777343012194766 | validation: 0.32193741785701474]
	TIME [epoch: 6.62 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3140074410377839		[learning rate: 0.0016506]
	Learning Rate: 0.0016506
	LOSS [training: 0.3140074410377839 | validation: 0.280001938274426]
	TIME [epoch: 6.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3521870448268526		[learning rate: 0.0016467]
	Learning Rate: 0.0016467
	LOSS [training: 0.3521870448268526 | validation: 0.2949171262038395]
	TIME [epoch: 6.58 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3324670877081904		[learning rate: 0.0016428]
	Learning Rate: 0.00164282
	LOSS [training: 0.3324670877081904 | validation: 0.2461261503932855]
	TIME [epoch: 6.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23973735989590078		[learning rate: 0.0016389]
	Learning Rate: 0.00163894
	LOSS [training: 0.23973735989590078 | validation: 0.2637485951323286]
	TIME [epoch: 6.59 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2850766809316949		[learning rate: 0.0016351]
	Learning Rate: 0.00163508
	LOSS [training: 0.2850766809316949 | validation: 0.2696472658432597]
	TIME [epoch: 6.6 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30469183168872627		[learning rate: 0.0016312]
	Learning Rate: 0.00163122
	LOSS [training: 0.30469183168872627 | validation: 0.2316246423681415]
	TIME [epoch: 6.63 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24051964049897387		[learning rate: 0.0016274]
	Learning Rate: 0.00162737
	LOSS [training: 0.24051964049897387 | validation: 0.27281412640916686]
	TIME [epoch: 6.59 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26228316195298074		[learning rate: 0.0016235]
	Learning Rate: 0.00162353
	LOSS [training: 0.26228316195298074 | validation: 0.276547086060423]
	TIME [epoch: 6.6 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2591895456912938		[learning rate: 0.0016197]
	Learning Rate: 0.0016197
	LOSS [training: 0.2591895456912938 | validation: 0.2282896515278675]
	TIME [epoch: 6.6 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23557473135683052		[learning rate: 0.0016159]
	Learning Rate: 0.00161588
	LOSS [training: 0.23557473135683052 | validation: 0.2313996774193307]
	TIME [epoch: 6.58 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2683860011022513		[learning rate: 0.0016121]
	Learning Rate: 0.00161207
	LOSS [training: 0.2683860011022513 | validation: 0.2605507796597125]
	TIME [epoch: 6.64 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27291883302322606		[learning rate: 0.0016083]
	Learning Rate: 0.00160827
	LOSS [training: 0.27291883302322606 | validation: 0.2644941164302813]
	TIME [epoch: 6.59 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3498806083909406		[learning rate: 0.0016045]
	Learning Rate: 0.00160448
	LOSS [training: 0.3498806083909406 | validation: 0.36808967916798885]
	TIME [epoch: 6.6 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32932210438460274		[learning rate: 0.0016007]
	Learning Rate: 0.00160069
	LOSS [training: 0.32932210438460274 | validation: 0.2536814673772686]
	TIME [epoch: 6.58 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2750266909968569		[learning rate: 0.0015969]
	Learning Rate: 0.00159692
	LOSS [training: 0.2750266909968569 | validation: 0.23744413391598232]
	TIME [epoch: 6.59 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2751674177990833		[learning rate: 0.0015931]
	Learning Rate: 0.00159315
	LOSS [training: 0.2751674177990833 | validation: 0.25804778691721475]
	TIME [epoch: 6.6 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24728469750462886		[learning rate: 0.0015894]
	Learning Rate: 0.00158939
	LOSS [training: 0.24728469750462886 | validation: 0.3076735623595432]
	TIME [epoch: 6.63 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.284520135471155		[learning rate: 0.0015856]
	Learning Rate: 0.00158564
	LOSS [training: 0.284520135471155 | validation: 0.2924137065112702]
	TIME [epoch: 6.6 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27264229226108316		[learning rate: 0.0015819]
	Learning Rate: 0.0015819
	LOSS [training: 0.27264229226108316 | validation: 0.23398637351775003]
	TIME [epoch: 6.59 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24355057647025177		[learning rate: 0.0015782]
	Learning Rate: 0.00157817
	LOSS [training: 0.24355057647025177 | validation: 0.2539609385962398]
	TIME [epoch: 6.6 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.283817474714514		[learning rate: 0.0015744]
	Learning Rate: 0.00157445
	LOSS [training: 0.283817474714514 | validation: 0.2694780334372865]
	TIME [epoch: 6.6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.31467872331768504		[learning rate: 0.0015707]
	Learning Rate: 0.00157073
	LOSS [training: 0.31467872331768504 | validation: 0.2669221906738911]
	TIME [epoch: 6.61 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27797480381767525		[learning rate: 0.001567]
	Learning Rate: 0.00156703
	LOSS [training: 0.27797480381767525 | validation: 0.2570379961852266]
	TIME [epoch: 6.62 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2544390832528431		[learning rate: 0.0015633]
	Learning Rate: 0.00156333
	LOSS [training: 0.2544390832528431 | validation: 0.31490799731402563]
	TIME [epoch: 6.6 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2616583953393798		[learning rate: 0.0015596]
	Learning Rate: 0.00155964
	LOSS [training: 0.2616583953393798 | validation: 0.35427004953005686]
	TIME [epoch: 6.6 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.351005038266308		[learning rate: 0.001556]
	Learning Rate: 0.00155597
	LOSS [training: 0.351005038266308 | validation: 0.29448430912309354]
	TIME [epoch: 6.59 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2530263295753158		[learning rate: 0.0015523]
	Learning Rate: 0.0015523
	LOSS [training: 0.2530263295753158 | validation: 0.2674311714897246]
	TIME [epoch: 6.58 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23330847728999898		[learning rate: 0.0015486]
	Learning Rate: 0.00154863
	LOSS [training: 0.23330847728999898 | validation: 0.2865752519503044]
	TIME [epoch: 6.62 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.254815649622144		[learning rate: 0.001545]
	Learning Rate: 0.00154498
	LOSS [training: 0.254815649622144 | validation: 0.24494385093583598]
	TIME [epoch: 6.59 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2640641467876046		[learning rate: 0.0015413]
	Learning Rate: 0.00154134
	LOSS [training: 0.2640641467876046 | validation: 0.21177107094598413]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_842.pth
	Model improved!!!
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23432890491242162		[learning rate: 0.0015377]
	Learning Rate: 0.0015377
	LOSS [training: 0.23432890491242162 | validation: 0.30459856775644845]
	TIME [epoch: 6.61 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25447870776481574		[learning rate: 0.0015341]
	Learning Rate: 0.00153407
	LOSS [training: 0.25447870776481574 | validation: 0.28795773346830666]
	TIME [epoch: 6.6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26742554986619826		[learning rate: 0.0015305]
	Learning Rate: 0.00153045
	LOSS [training: 0.26742554986619826 | validation: 0.3579331701006785]
	TIME [epoch: 6.62 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32660118758952383		[learning rate: 0.0015268]
	Learning Rate: 0.00152684
	LOSS [training: 0.32660118758952383 | validation: 0.2842760966440978]
	TIME [epoch: 6.63 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2590565329937046		[learning rate: 0.0015232]
	Learning Rate: 0.00152324
	LOSS [training: 0.2590565329937046 | validation: 0.2606486976221618]
	TIME [epoch: 6.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23484082848914065		[learning rate: 0.0015196]
	Learning Rate: 0.00151965
	LOSS [training: 0.23484082848914065 | validation: 0.2890986084256861]
	TIME [epoch: 6.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24113407763895234		[learning rate: 0.0015161]
	Learning Rate: 0.00151607
	LOSS [training: 0.24113407763895234 | validation: 0.2692661765265048]
	TIME [epoch: 6.6 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24210946792932062		[learning rate: 0.0015125]
	Learning Rate: 0.00151249
	LOSS [training: 0.24210946792932062 | validation: 0.2801838021143358]
	TIME [epoch: 6.6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25696246226976194		[learning rate: 0.0015089]
	Learning Rate: 0.00150892
	LOSS [training: 0.25696246226976194 | validation: 0.2938412721708916]
	TIME [epoch: 6.63 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2580459385725705		[learning rate: 0.0015054]
	Learning Rate: 0.00150536
	LOSS [training: 0.2580459385725705 | validation: 0.2381864579950324]
	TIME [epoch: 6.61 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25320450515569515		[learning rate: 0.0015018]
	Learning Rate: 0.00150181
	LOSS [training: 0.25320450515569515 | validation: 0.24515853299604057]
	TIME [epoch: 6.57 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26029573411841084		[learning rate: 0.0014983]
	Learning Rate: 0.00149827
	LOSS [training: 0.26029573411841084 | validation: 0.3629339867979823]
	TIME [epoch: 6.58 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2527708632066509		[learning rate: 0.0014947]
	Learning Rate: 0.00149473
	LOSS [training: 0.2527708632066509 | validation: 0.2551894905397652]
	TIME [epoch: 6.58 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2499758745224602		[learning rate: 0.0014912]
	Learning Rate: 0.00149121
	LOSS [training: 0.2499758745224602 | validation: 0.2450881640805196]
	TIME [epoch: 6.59 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2545640203844831		[learning rate: 0.0014877]
	Learning Rate: 0.00148769
	LOSS [training: 0.2545640203844831 | validation: 0.3174911758979024]
	TIME [epoch: 6.61 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28523172540301645		[learning rate: 0.0014842]
	Learning Rate: 0.00148418
	LOSS [training: 0.28523172540301645 | validation: 0.24402113068584197]
	TIME [epoch: 6.61 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24420072573960733		[learning rate: 0.0014807]
	Learning Rate: 0.00148068
	LOSS [training: 0.24420072573960733 | validation: 0.23927385033908702]
	TIME [epoch: 6.58 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2667702572570553		[learning rate: 0.0014772]
	Learning Rate: 0.00147719
	LOSS [training: 0.2667702572570553 | validation: 0.25842630779178405]
	TIME [epoch: 6.58 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24554389334031224		[learning rate: 0.0014737]
	Learning Rate: 0.0014737
	LOSS [training: 0.24554389334031224 | validation: 0.2140399002067223]
	TIME [epoch: 6.59 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2396293454082933		[learning rate: 0.0014702]
	Learning Rate: 0.00147023
	LOSS [training: 0.2396293454082933 | validation: 0.260872412013333]
	TIME [epoch: 6.59 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2385473292185519		[learning rate: 0.0014668]
	Learning Rate: 0.00146676
	LOSS [training: 0.2385473292185519 | validation: 0.22601383834122615]
	TIME [epoch: 6.63 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2624847030062011		[learning rate: 0.0014633]
	Learning Rate: 0.0014633
	LOSS [training: 0.2624847030062011 | validation: 0.2618575808955473]
	TIME [epoch: 6.57 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26367589042496226		[learning rate: 0.0014598]
	Learning Rate: 0.00145985
	LOSS [training: 0.26367589042496226 | validation: 0.6367521594429542]
	TIME [epoch: 6.58 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3674089548798772		[learning rate: 0.0014564]
	Learning Rate: 0.0014564
	LOSS [training: 0.3674089548798772 | validation: 0.34451704931604776]
	TIME [epoch: 6.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28644851199835164		[learning rate: 0.001453]
	Learning Rate: 0.00145297
	LOSS [training: 0.28644851199835164 | validation: 0.25460546573247056]
	TIME [epoch: 6.57 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2590478617700172		[learning rate: 0.0014495]
	Learning Rate: 0.00144954
	LOSS [training: 0.2590478617700172 | validation: 0.26399090608762765]
	TIME [epoch: 6.61 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24582296102121798		[learning rate: 0.0014461]
	Learning Rate: 0.00144612
	LOSS [training: 0.24582296102121798 | validation: 0.2511442645420388]
	TIME [epoch: 6.61 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2894246462033312		[learning rate: 0.0014427]
	Learning Rate: 0.00144271
	LOSS [training: 0.2894246462033312 | validation: 0.22593092771294054]
	TIME [epoch: 6.6 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2503966563633391		[learning rate: 0.0014393]
	Learning Rate: 0.00143931
	LOSS [training: 0.2503966563633391 | validation: 0.24363473496200258]
	TIME [epoch: 6.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.28651991217105144		[learning rate: 0.0014359]
	Learning Rate: 0.00143591
	LOSS [training: 0.28651991217105144 | validation: 0.24052731354017304]
	TIME [epoch: 6.59 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22566381412347009		[learning rate: 0.0014325]
	Learning Rate: 0.00143253
	LOSS [training: 0.22566381412347009 | validation: 0.22188893817273952]
	TIME [epoch: 6.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26056927947303793		[learning rate: 0.0014291]
	Learning Rate: 0.00142915
	LOSS [training: 0.26056927947303793 | validation: 0.22915025230772185]
	TIME [epoch: 6.63 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2509407639909726		[learning rate: 0.0014258]
	Learning Rate: 0.00142578
	LOSS [training: 0.2509407639909726 | validation: 0.23149155317361805]
	TIME [epoch: 6.62 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23315778369475057		[learning rate: 0.0014224]
	Learning Rate: 0.00142241
	LOSS [training: 0.23315778369475057 | validation: 0.25938702363880034]
	TIME [epoch: 6.59 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2532305171930712		[learning rate: 0.0014191]
	Learning Rate: 0.00141906
	LOSS [training: 0.2532305171930712 | validation: 0.20875331233991445]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_877.pth
	Model improved!!!
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24004473771497983		[learning rate: 0.0014157]
	Learning Rate: 0.00141571
	LOSS [training: 0.24004473771497983 | validation: 0.27482323259193553]
	TIME [epoch: 6.58 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2590818416201014		[learning rate: 0.0014124]
	Learning Rate: 0.00141237
	LOSS [training: 0.2590818416201014 | validation: 0.21286128814917848]
	TIME [epoch: 6.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2803173242479108		[learning rate: 0.001409]
	Learning Rate: 0.00140904
	LOSS [training: 0.2803173242479108 | validation: 0.2749613724733846]
	TIME [epoch: 6.63 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3161856641864603		[learning rate: 0.0014057]
	Learning Rate: 0.00140572
	LOSS [training: 0.3161856641864603 | validation: 0.28465511798200166]
	TIME [epoch: 6.58 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3012026926812238		[learning rate: 0.0014024]
	Learning Rate: 0.0014024
	LOSS [training: 0.3012026926812238 | validation: 0.25838820402332585]
	TIME [epoch: 6.58 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26000365524705693		[learning rate: 0.0013991]
	Learning Rate: 0.00139909
	LOSS [training: 0.26000365524705693 | validation: 0.2448306818001463]
	TIME [epoch: 6.57 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2450567481481413		[learning rate: 0.0013958]
	Learning Rate: 0.00139579
	LOSS [training: 0.2450567481481413 | validation: 0.3409726549329905]
	TIME [epoch: 6.58 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2903436083131615		[learning rate: 0.0013925]
	Learning Rate: 0.0013925
	LOSS [training: 0.2903436083131615 | validation: 0.3168897433240552]
	TIME [epoch: 6.59 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29776222059102964		[learning rate: 0.0013892]
	Learning Rate: 0.00138921
	LOSS [training: 0.29776222059102964 | validation: 0.2575634637082988]
	TIME [epoch: 6.61 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30385132791993874		[learning rate: 0.0013859]
	Learning Rate: 0.00138594
	LOSS [training: 0.30385132791993874 | validation: 0.3450285554826812]
	TIME [epoch: 6.58 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3375716665841127		[learning rate: 0.0013827]
	Learning Rate: 0.00138267
	LOSS [training: 0.3375716665841127 | validation: 0.4089165587959648]
	TIME [epoch: 6.58 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3791130338070733		[learning rate: 0.0013794]
	Learning Rate: 0.00137941
	LOSS [training: 0.3791130338070733 | validation: 0.4252213108664984]
	TIME [epoch: 6.58 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34842759797232115		[learning rate: 0.0013762]
	Learning Rate: 0.00137615
	LOSS [training: 0.34842759797232115 | validation: 0.2704328324067067]
	TIME [epoch: 6.57 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3296689237262788		[learning rate: 0.0013729]
	Learning Rate: 0.00137291
	LOSS [training: 0.3296689237262788 | validation: 0.31127935160579123]
	TIME [epoch: 6.59 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3622030869905721		[learning rate: 0.0013697]
	Learning Rate: 0.00136967
	LOSS [training: 0.3622030869905721 | validation: 0.3154092685223426]
	TIME [epoch: 6.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3212590553718327		[learning rate: 0.0013664]
	Learning Rate: 0.00136644
	LOSS [training: 0.3212590553718327 | validation: 0.35597566147833914]
	TIME [epoch: 6.58 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.34862419856868787		[learning rate: 0.0013632]
	Learning Rate: 0.00136321
	LOSS [training: 0.34862419856868787 | validation: 0.39613076645210177]
	TIME [epoch: 6.57 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.35125122847433454		[learning rate: 0.00136]
	Learning Rate: 0.00136
	LOSS [training: 0.35125122847433454 | validation: 0.2977415963206944]
	TIME [epoch: 6.58 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3226498778347629		[learning rate: 0.0013568]
	Learning Rate: 0.00135679
	LOSS [training: 0.3226498778347629 | validation: 0.3631736233250129]
	TIME [epoch: 6.59 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3662520993091762		[learning rate: 0.0013536]
	Learning Rate: 0.00135359
	LOSS [training: 0.3662520993091762 | validation: 0.44747124082631146]
	TIME [epoch: 6.61 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3709367939031357		[learning rate: 0.0013504]
	Learning Rate: 0.0013504
	LOSS [training: 0.3709367939031357 | validation: 0.4001833430653871]
	TIME [epoch: 6.62 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3885788318831028		[learning rate: 0.0013472]
	Learning Rate: 0.00134721
	LOSS [training: 0.3885788318831028 | validation: 0.2914176236095018]
	TIME [epoch: 6.59 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3109826787005467		[learning rate: 0.001344]
	Learning Rate: 0.00134403
	LOSS [training: 0.3109826787005467 | validation: 0.3633233208139556]
	TIME [epoch: 6.58 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3460105570351857		[learning rate: 0.0013409]
	Learning Rate: 0.00134086
	LOSS [training: 0.3460105570351857 | validation: 0.3914072708871703]
	TIME [epoch: 6.57 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3447137959386537		[learning rate: 0.0013377]
	Learning Rate: 0.0013377
	LOSS [training: 0.3447137959386537 | validation: 0.28484003260932916]
	TIME [epoch: 6.57 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27548948762327174		[learning rate: 0.0013345]
	Learning Rate: 0.00133455
	LOSS [training: 0.27548948762327174 | validation: 0.2692408236661101]
	TIME [epoch: 6.62 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2477736703582988		[learning rate: 0.0013314]
	Learning Rate: 0.0013314
	LOSS [training: 0.2477736703582988 | validation: 0.2406418300705546]
	TIME [epoch: 6.57 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24494928199536448		[learning rate: 0.0013283]
	Learning Rate: 0.00132826
	LOSS [training: 0.24494928199536448 | validation: 0.2212320000394366]
	TIME [epoch: 6.58 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2582609145342148		[learning rate: 0.0013251]
	Learning Rate: 0.00132512
	LOSS [training: 0.2582609145342148 | validation: 0.20262947059412834]
	TIME [epoch: 6.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_906.pth
	Model improved!!!
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2373505073533846		[learning rate: 0.001322]
	Learning Rate: 0.001322
	LOSS [training: 0.2373505073533846 | validation: 0.23155434904946792]
	TIME [epoch: 6.59 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2674425673738912		[learning rate: 0.0013189]
	Learning Rate: 0.00131888
	LOSS [training: 0.2674425673738912 | validation: 0.2092207029597461]
	TIME [epoch: 6.59 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24658240216188024		[learning rate: 0.0013158]
	Learning Rate: 0.00131577
	LOSS [training: 0.24658240216188024 | validation: 0.24044165527119327]
	TIME [epoch: 6.61 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2704598828562882		[learning rate: 0.0013127]
	Learning Rate: 0.00131266
	LOSS [training: 0.2704598828562882 | validation: 0.26731915533565714]
	TIME [epoch: 6.58 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25595716252872464		[learning rate: 0.0013096]
	Learning Rate: 0.00130957
	LOSS [training: 0.25595716252872464 | validation: 0.27199257305092983]
	TIME [epoch: 6.58 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24818951703184117		[learning rate: 0.0013065]
	Learning Rate: 0.00130648
	LOSS [training: 0.24818951703184117 | validation: 0.2724665046522545]
	TIME [epoch: 6.58 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2704844459104673		[learning rate: 0.0013034]
	Learning Rate: 0.0013034
	LOSS [training: 0.2704844459104673 | validation: 0.23159110275092715]
	TIME [epoch: 6.58 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2635492446347688		[learning rate: 0.0013003]
	Learning Rate: 0.00130032
	LOSS [training: 0.2635492446347688 | validation: 0.23713939738513928]
	TIME [epoch: 6.61 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2546374111531131		[learning rate: 0.0012973]
	Learning Rate: 0.00129726
	LOSS [training: 0.2546374111531131 | validation: 0.2552376377745477]
	TIME [epoch: 6.59 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2640036822930801		[learning rate: 0.0012942]
	Learning Rate: 0.0012942
	LOSS [training: 0.2640036822930801 | validation: 0.25012326284373343]
	TIME [epoch: 6.58 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2924448046429393		[learning rate: 0.0012911]
	Learning Rate: 0.00129114
	LOSS [training: 0.2924448046429393 | validation: 0.25665554575896715]
	TIME [epoch: 6.58 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29535547118560157		[learning rate: 0.0012881]
	Learning Rate: 0.0012881
	LOSS [training: 0.29535547118560157 | validation: 0.2721653185418224]
	TIME [epoch: 6.57 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3223564208542456		[learning rate: 0.0012851]
	Learning Rate: 0.00128506
	LOSS [training: 0.3223564208542456 | validation: 0.27621327853970157]
	TIME [epoch: 6.58 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26622015979441965		[learning rate: 0.001282]
	Learning Rate: 0.00128203
	LOSS [training: 0.26622015979441965 | validation: 0.21018919616235046]
	TIME [epoch: 6.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2480156099627206		[learning rate: 0.001279]
	Learning Rate: 0.001279
	LOSS [training: 0.2480156099627206 | validation: 0.22348283377036843]
	TIME [epoch: 6.6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24439504901347425		[learning rate: 0.001276]
	Learning Rate: 0.00127599
	LOSS [training: 0.24439504901347425 | validation: 0.21511584154435792]
	TIME [epoch: 6.57 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2744129593740467		[learning rate: 0.001273]
	Learning Rate: 0.00127298
	LOSS [training: 0.2744129593740467 | validation: 0.2051756078425716]
	TIME [epoch: 6.57 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24264206994167264		[learning rate: 0.00127]
	Learning Rate: 0.00126997
	LOSS [training: 0.24264206994167264 | validation: 0.25618633749398473]
	TIME [epoch: 6.57 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25140529405976036		[learning rate: 0.001267]
	Learning Rate: 0.00126698
	LOSS [training: 0.25140529405976036 | validation: 0.2073579320143333]
	TIME [epoch: 6.58 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2606592445889353		[learning rate: 0.001264]
	Learning Rate: 0.00126399
	LOSS [training: 0.2606592445889353 | validation: 0.2235120365660418]
	TIME [epoch: 6.62 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2467305435727413		[learning rate: 0.001261]
	Learning Rate: 0.00126101
	LOSS [training: 0.2467305435727413 | validation: 0.21505277943800438]
	TIME [epoch: 6.58 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2963590518581769		[learning rate: 0.001258]
	Learning Rate: 0.00125803
	LOSS [training: 0.2963590518581769 | validation: 0.1911816994738812]
	TIME [epoch: 6.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_928.pth
	Model improved!!!
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21781738534142256		[learning rate: 0.0012551]
	Learning Rate: 0.00125507
	LOSS [training: 0.21781738534142256 | validation: 0.19539428005800547]
	TIME [epoch: 6.59 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22237133745395687		[learning rate: 0.0012521]
	Learning Rate: 0.00125211
	LOSS [training: 0.22237133745395687 | validation: 0.1980590564674855]
	TIME [epoch: 6.58 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22333390990546262		[learning rate: 0.0012492]
	Learning Rate: 0.00124915
	LOSS [training: 0.22333390990546262 | validation: 0.2778978756029814]
	TIME [epoch: 6.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26060574413062326		[learning rate: 0.0012462]
	Learning Rate: 0.00124621
	LOSS [training: 0.26060574413062326 | validation: 0.289800108038777]
	TIME [epoch: 6.63 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25783490392540503		[learning rate: 0.0012433]
	Learning Rate: 0.00124327
	LOSS [training: 0.25783490392540503 | validation: 0.2475817252656169]
	TIME [epoch: 6.58 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26168134019034345		[learning rate: 0.0012403]
	Learning Rate: 0.00124033
	LOSS [training: 0.26168134019034345 | validation: 0.253332165696202]
	TIME [epoch: 6.6 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24142645730538356		[learning rate: 0.0012374]
	Learning Rate: 0.00123741
	LOSS [training: 0.24142645730538356 | validation: 0.2452842484708418]
	TIME [epoch: 6.58 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.29001270878750596		[learning rate: 0.0012345]
	Learning Rate: 0.00123449
	LOSS [training: 0.29001270878750596 | validation: 0.2484075380909575]
	TIME [epoch: 6.58 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23794071564866112		[learning rate: 0.0012316]
	Learning Rate: 0.00123158
	LOSS [training: 0.23794071564866112 | validation: 0.23196403816653888]
	TIME [epoch: 6.62 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22826290558592124		[learning rate: 0.0012287]
	Learning Rate: 0.00122867
	LOSS [training: 0.22826290558592124 | validation: 0.25665186879160273]
	TIME [epoch: 6.6 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23791042173626725		[learning rate: 0.0012258]
	Learning Rate: 0.00122577
	LOSS [training: 0.23791042173626725 | validation: 0.23217076538796602]
	TIME [epoch: 6.58 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23389542659905738		[learning rate: 0.0012229]
	Learning Rate: 0.00122288
	LOSS [training: 0.23389542659905738 | validation: 0.2407609786306674]
	TIME [epoch: 6.57 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23155434267956151		[learning rate: 0.00122]
	Learning Rate: 0.00122
	LOSS [training: 0.23155434267956151 | validation: 0.24434409857520747]
	TIME [epoch: 6.58 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23798495938629932		[learning rate: 0.0012171]
	Learning Rate: 0.00121712
	LOSS [training: 0.23798495938629932 | validation: 0.2277296656602153]
	TIME [epoch: 6.58 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25029214965159635		[learning rate: 0.0012142]
	Learning Rate: 0.00121425
	LOSS [training: 0.25029214965159635 | validation: 0.22298701302497467]
	TIME [epoch: 6.63 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23301256065388892		[learning rate: 0.0012114]
	Learning Rate: 0.00121138
	LOSS [training: 0.23301256065388892 | validation: 0.23077112280748271]
	TIME [epoch: 6.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2300905110648813		[learning rate: 0.0012085]
	Learning Rate: 0.00120853
	LOSS [training: 0.2300905110648813 | validation: 0.25659246474964253]
	TIME [epoch: 6.58 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24733227906172212		[learning rate: 0.0012057]
	Learning Rate: 0.00120568
	LOSS [training: 0.24733227906172212 | validation: 0.20888780563222095]
	TIME [epoch: 6.59 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24001871366472805		[learning rate: 0.0012028]
	Learning Rate: 0.00120283
	LOSS [training: 0.24001871366472805 | validation: 0.24153109958056687]
	TIME [epoch: 6.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24042816009912626		[learning rate: 0.0012]
	Learning Rate: 0.0012
	LOSS [training: 0.24042816009912626 | validation: 0.2117535765501926]
	TIME [epoch: 6.6 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25432422611823524		[learning rate: 0.0011972]
	Learning Rate: 0.00119716
	LOSS [training: 0.25432422611823524 | validation: 0.2352537521031251]
	TIME [epoch: 6.62 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23669332450798128		[learning rate: 0.0011943]
	Learning Rate: 0.00119434
	LOSS [training: 0.23669332450798128 | validation: 0.3290603032307279]
	TIME [epoch: 6.59 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30688117122758723		[learning rate: 0.0011915]
	Learning Rate: 0.00119152
	LOSS [training: 0.30688117122758723 | validation: 0.21514260252668457]
	TIME [epoch: 6.58 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2446895862959114		[learning rate: 0.0011887]
	Learning Rate: 0.00118871
	LOSS [training: 0.2446895862959114 | validation: 0.261037241960772]
	TIME [epoch: 6.58 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2654770420688582		[learning rate: 0.0011859]
	Learning Rate: 0.00118591
	LOSS [training: 0.2654770420688582 | validation: 0.2839346747211555]
	TIME [epoch: 6.57 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24547186046208275		[learning rate: 0.0011831]
	Learning Rate: 0.00118311
	LOSS [training: 0.24547186046208275 | validation: 0.23773044962943302]
	TIME [epoch: 6.6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2269222997305591		[learning rate: 0.0011803]
	Learning Rate: 0.00118032
	LOSS [training: 0.2269222997305591 | validation: 0.25059295032711515]
	TIME [epoch: 6.62 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2276429994323311		[learning rate: 0.0011775]
	Learning Rate: 0.00117754
	LOSS [training: 0.2276429994323311 | validation: 0.21548857852226094]
	TIME [epoch: 6.57 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2290347744117341		[learning rate: 0.0011748]
	Learning Rate: 0.00117476
	LOSS [training: 0.2290347744117341 | validation: 0.22346442290761429]
	TIME [epoch: 6.57 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23550583453636045		[learning rate: 0.001172]
	Learning Rate: 0.00117199
	LOSS [training: 0.23550583453636045 | validation: 0.2758828637805378]
	TIME [epoch: 6.59 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23673742528991557		[learning rate: 0.0011692]
	Learning Rate: 0.00116922
	LOSS [training: 0.23673742528991557 | validation: 0.2222435644534447]
	TIME [epoch: 6.57 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2322334690563029		[learning rate: 0.0011665]
	Learning Rate: 0.00116646
	LOSS [training: 0.2322334690563029 | validation: 0.22584967972502784]
	TIME [epoch: 6.61 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22206731767248974		[learning rate: 0.0011637]
	Learning Rate: 0.00116371
	LOSS [training: 0.22206731767248974 | validation: 0.20724496763492645]
	TIME [epoch: 6.58 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25031082240493846		[learning rate: 0.001161]
	Learning Rate: 0.00116097
	LOSS [training: 0.25031082240493846 | validation: 0.23458313017701954]
	TIME [epoch: 6.57 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24270446757444514		[learning rate: 0.0011582]
	Learning Rate: 0.00115823
	LOSS [training: 0.24270446757444514 | validation: 0.21083329845325605]
	TIME [epoch: 6.57 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23538763322943196		[learning rate: 0.0011555]
	Learning Rate: 0.0011555
	LOSS [training: 0.23538763322943196 | validation: 0.25175747756615974]
	TIME [epoch: 6.56 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25826419994580313		[learning rate: 0.0011528]
	Learning Rate: 0.00115277
	LOSS [training: 0.25826419994580313 | validation: 0.2153066875293782]
	TIME [epoch: 6.58 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21794797069301577		[learning rate: 0.0011501]
	Learning Rate: 0.00115005
	LOSS [training: 0.21794797069301577 | validation: 0.2472074771539943]
	TIME [epoch: 6.6 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2638929414613874		[learning rate: 0.0011473]
	Learning Rate: 0.00114734
	LOSS [training: 0.2638929414613874 | validation: 0.25023610544533603]
	TIME [epoch: 6.58 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.266727183009258		[learning rate: 0.0011446]
	Learning Rate: 0.00114463
	LOSS [training: 0.266727183009258 | validation: 0.2599695700033001]
	TIME [epoch: 6.57 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2743814170062558		[learning rate: 0.0011419]
	Learning Rate: 0.00114193
	LOSS [training: 0.2743814170062558 | validation: 0.2759704475183653]
	TIME [epoch: 6.58 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30272958869933425		[learning rate: 0.0011392]
	Learning Rate: 0.00113924
	LOSS [training: 0.30272958869933425 | validation: 0.269441851307795]
	TIME [epoch: 6.58 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3000515604610997		[learning rate: 0.0011366]
	Learning Rate: 0.00113655
	LOSS [training: 0.3000515604610997 | validation: 0.2695772410499629]
	TIME [epoch: 6.6 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.27992150501138496		[learning rate: 0.0011339]
	Learning Rate: 0.00113387
	LOSS [training: 0.27992150501138496 | validation: 0.2414518013121542]
	TIME [epoch: 6.6 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2760070350007039		[learning rate: 0.0011312]
	Learning Rate: 0.0011312
	LOSS [training: 0.2760070350007039 | validation: 0.2720292710405424]
	TIME [epoch: 6.58 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.30497870562779245		[learning rate: 0.0011285]
	Learning Rate: 0.00112853
	LOSS [training: 0.30497870562779245 | validation: 0.33581157695296393]
	TIME [epoch: 6.58 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.32312633678391073		[learning rate: 0.0011259]
	Learning Rate: 0.00112587
	LOSS [training: 0.32312633678391073 | validation: 0.21177023890701807]
	TIME [epoch: 6.57 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23662199001375067		[learning rate: 0.0011232]
	Learning Rate: 0.00112321
	LOSS [training: 0.23662199001375067 | validation: 0.2134394998392257]
	TIME [epoch: 6.57 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22993076964342168		[learning rate: 0.0011206]
	Learning Rate: 0.00112056
	LOSS [training: 0.22993076964342168 | validation: 0.25297118212775943]
	TIME [epoch: 6.6 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26783236492554896		[learning rate: 0.0011179]
	Learning Rate: 0.00111792
	LOSS [training: 0.26783236492554896 | validation: 0.25825712113786464]
	TIME [epoch: 6.58 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2552017207169429		[learning rate: 0.0011153]
	Learning Rate: 0.00111528
	LOSS [training: 0.2552017207169429 | validation: 0.2448348888630858]
	TIME [epoch: 6.57 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2891533061332353		[learning rate: 0.0011127]
	Learning Rate: 0.00111265
	LOSS [training: 0.2891533061332353 | validation: 0.21939257544497673]
	TIME [epoch: 6.57 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2726295102906059		[learning rate: 0.00111]
	Learning Rate: 0.00111003
	LOSS [training: 0.2726295102906059 | validation: 0.293443008248486]
	TIME [epoch: 6.57 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2887522318171993		[learning rate: 0.0011074]
	Learning Rate: 0.00110741
	LOSS [training: 0.2887522318171993 | validation: 0.22969279115035743]
	TIME [epoch: 6.58 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2441474585340841		[learning rate: 0.0011048]
	Learning Rate: 0.0011048
	LOSS [training: 0.2441474585340841 | validation: 0.2292106519210571]
	TIME [epoch: 6.61 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2397854873444501		[learning rate: 0.0011022]
	Learning Rate: 0.00110219
	LOSS [training: 0.2397854873444501 | validation: 0.21443745013446533]
	TIME [epoch: 6.58 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2660502065934528		[learning rate: 0.0010996]
	Learning Rate: 0.00109959
	LOSS [training: 0.2660502065934528 | validation: 0.2293050693616364]
	TIME [epoch: 6.57 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.216671395351256		[learning rate: 0.001097]
	Learning Rate: 0.001097
	LOSS [training: 0.216671395351256 | validation: 0.2438325513607904]
	TIME [epoch: 6.57 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22465773050369053		[learning rate: 0.0010944]
	Learning Rate: 0.00109441
	LOSS [training: 0.22465773050369053 | validation: 0.19870821979420297]
	TIME [epoch: 6.58 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.259287103129977		[learning rate: 0.0010918]
	Learning Rate: 0.00109183
	LOSS [training: 0.259287103129977 | validation: 0.19724650016096373]
	TIME [epoch: 6.61 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2244463402869231		[learning rate: 0.0010893]
	Learning Rate: 0.00108925
	LOSS [training: 0.2244463402869231 | validation: 0.19606100441799973]
	TIME [epoch: 6.61 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2136427389465075		[learning rate: 0.0010867]
	Learning Rate: 0.00108668
	LOSS [training: 0.2136427389465075 | validation: 0.19036452047770686]
	TIME [epoch: 6.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_990.pth
	Model improved!!!
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2133558455425324		[learning rate: 0.0010841]
	Learning Rate: 0.00108412
	LOSS [training: 0.2133558455425324 | validation: 0.2146735460546788]
	TIME [epoch: 6.61 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2220708993843218		[learning rate: 0.0010816]
	Learning Rate: 0.00108156
	LOSS [training: 0.2220708993843218 | validation: 0.22582498895060826]
	TIME [epoch: 6.6 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23948500935424075		[learning rate: 0.001079]
	Learning Rate: 0.00107901
	LOSS [training: 0.23948500935424075 | validation: 0.24825232337461828]
	TIME [epoch: 6.6 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21709255102851338		[learning rate: 0.0010765]
	Learning Rate: 0.00107647
	LOSS [training: 0.21709255102851338 | validation: 0.18935024543857326]
	TIME [epoch: 6.64 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_994.pth
	Model improved!!!
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21630075827724154		[learning rate: 0.0010739]
	Learning Rate: 0.00107393
	LOSS [training: 0.21630075827724154 | validation: 0.19628917450756606]
	TIME [epoch: 6.61 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2143105500682596		[learning rate: 0.0010714]
	Learning Rate: 0.00107139
	LOSS [training: 0.2143105500682596 | validation: 0.18549617569691068]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_996.pth
	Model improved!!!
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21576607472263234		[learning rate: 0.0010689]
	Learning Rate: 0.00106887
	LOSS [training: 0.21576607472263234 | validation: 0.22258296061564783]
	TIME [epoch: 6.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20957531441877758		[learning rate: 0.0010663]
	Learning Rate: 0.00106634
	LOSS [training: 0.20957531441877758 | validation: 0.2158067713427002]
	TIME [epoch: 6.61 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20960303210777537		[learning rate: 0.0010638]
	Learning Rate: 0.00106383
	LOSS [training: 0.20960303210777537 | validation: 0.21198957325567108]
	TIME [epoch: 6.62 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21981547588048875		[learning rate: 0.0010613]
	Learning Rate: 0.00106132
	LOSS [training: 0.21981547588048875 | validation: 0.20807638381292654]
	TIME [epoch: 6.63 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20849650403908596		[learning rate: 0.0010588]
	Learning Rate: 0.00105882
	LOSS [training: 0.20849650403908596 | validation: 0.21550340975346327]
	TIME [epoch: 6.6 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24106087652567187		[learning rate: 0.0010563]
	Learning Rate: 0.00105632
	LOSS [training: 0.24106087652567187 | validation: 0.2160245249567862]
	TIME [epoch: 6.61 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21263012006956727		[learning rate: 0.0010538]
	Learning Rate: 0.00105383
	LOSS [training: 0.21263012006956727 | validation: 0.31651888643658116]
	TIME [epoch: 6.6 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26391547915447433		[learning rate: 0.0010513]
	Learning Rate: 0.00105134
	LOSS [training: 0.26391547915447433 | validation: 0.23752666179123175]
	TIME [epoch: 6.6 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23869288352019077		[learning rate: 0.0010489]
	Learning Rate: 0.00104886
	LOSS [training: 0.23869288352019077 | validation: 0.2240890057729078]
	TIME [epoch: 6.62 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23167372513238566		[learning rate: 0.0010464]
	Learning Rate: 0.00104639
	LOSS [training: 0.23167372513238566 | validation: 0.22632439065578874]
	TIME [epoch: 6.64 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24689351968639342		[learning rate: 0.0010439]
	Learning Rate: 0.00104392
	LOSS [training: 0.24689351968639342 | validation: 0.2638553616662919]
	TIME [epoch: 6.61 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.3011514256398587		[learning rate: 0.0010415]
	Learning Rate: 0.00104146
	LOSS [training: 0.3011514256398587 | validation: 0.21704787238567486]
	TIME [epoch: 6.61 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2076499821932889		[learning rate: 0.001039]
	Learning Rate: 0.001039
	LOSS [training: 0.2076499821932889 | validation: 0.19362535036857814]
	TIME [epoch: 6.6 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19951245004208293		[learning rate: 0.0010365]
	Learning Rate: 0.00103655
	LOSS [training: 0.19951245004208293 | validation: 0.24480403302204956]
	TIME [epoch: 6.6 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2214389237455952		[learning rate: 0.0010341]
	Learning Rate: 0.0010341
	LOSS [training: 0.2214389237455952 | validation: 0.1972465131196121]
	TIME [epoch: 6.62 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22286014978975527		[learning rate: 0.0010317]
	Learning Rate: 0.00103166
	LOSS [training: 0.22286014978975527 | validation: 0.2053077326674727]
	TIME [epoch: 6.65 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22007037450683825		[learning rate: 0.0010292]
	Learning Rate: 0.00102923
	LOSS [training: 0.22007037450683825 | validation: 0.23034669610820868]
	TIME [epoch: 6.6 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21400365995642384		[learning rate: 0.0010268]
	Learning Rate: 0.0010268
	LOSS [training: 0.21400365995642384 | validation: 0.19173105468797458]
	TIME [epoch: 6.61 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2116514060983989		[learning rate: 0.0010244]
	Learning Rate: 0.00102438
	LOSS [training: 0.2116514060983989 | validation: 0.2076995550316117]
	TIME [epoch: 6.6 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22001181302102218		[learning rate: 0.001022]
	Learning Rate: 0.00102196
	LOSS [training: 0.22001181302102218 | validation: 0.18460551789301682]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1016.pth
	Model improved!!!
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21831590967318068		[learning rate: 0.0010196]
	Learning Rate: 0.00101955
	LOSS [training: 0.21831590967318068 | validation: 0.20322585859108327]
	TIME [epoch: 6.64 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20026864918538867		[learning rate: 0.0010171]
	Learning Rate: 0.00101715
	LOSS [training: 0.20026864918538867 | validation: 0.18952464203057762]
	TIME [epoch: 6.61 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21411095948082143		[learning rate: 0.0010147]
	Learning Rate: 0.00101475
	LOSS [training: 0.21411095948082143 | validation: 0.1968405071711255]
	TIME [epoch: 6.6 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23753748735719102		[learning rate: 0.0010124]
	Learning Rate: 0.00101236
	LOSS [training: 0.23753748735719102 | validation: 0.20661986071939664]
	TIME [epoch: 6.6 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2289862840859775		[learning rate: 0.00101]
	Learning Rate: 0.00100997
	LOSS [training: 0.2289862840859775 | validation: 0.20853704852786586]
	TIME [epoch: 6.6 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2236662449504941		[learning rate: 0.0010076]
	Learning Rate: 0.00100759
	LOSS [training: 0.2236662449504941 | validation: 0.1913189903098504]
	TIME [epoch: 6.61 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2114843442250936		[learning rate: 0.0010052]
	Learning Rate: 0.00100521
	LOSS [training: 0.2114843442250936 | validation: 0.21135250013578677]
	TIME [epoch: 6.64 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20303564184687373		[learning rate: 0.0010028]
	Learning Rate: 0.00100284
	LOSS [training: 0.20303564184687373 | validation: 0.19079922322248222]
	TIME [epoch: 6.6 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2165691839397894		[learning rate: 0.0010005]
	Learning Rate: 0.00100047
	LOSS [training: 0.2165691839397894 | validation: 0.23932980562386438]
	TIME [epoch: 6.61 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21387841177282105		[learning rate: 0.00099811]
	Learning Rate: 0.000998112
	LOSS [training: 0.21387841177282105 | validation: 0.1774141865876747]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1026.pth
	Model improved!!!
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21924204760824625		[learning rate: 0.00099576]
	Learning Rate: 0.000995758
	LOSS [training: 0.21924204760824625 | validation: 0.21457513846254614]
	TIME [epoch: 6.59 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2079731371842469		[learning rate: 0.00099341]
	Learning Rate: 0.000993409
	LOSS [training: 0.2079731371842469 | validation: 0.197081102433121]
	TIME [epoch: 6.6 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19995623019439665		[learning rate: 0.00099107]
	Learning Rate: 0.000991066
	LOSS [training: 0.19995623019439665 | validation: 0.17754328649862036]
	TIME [epoch: 6.62 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20765298893572132		[learning rate: 0.00098873]
	Learning Rate: 0.000988728
	LOSS [training: 0.20765298893572132 | validation: 0.2171178674074726]
	TIME [epoch: 6.59 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2415748065953073		[learning rate: 0.0009864]
	Learning Rate: 0.000986396
	LOSS [training: 0.2415748065953073 | validation: 0.19743952726940683]
	TIME [epoch: 6.59 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22173890103753865		[learning rate: 0.00098407]
	Learning Rate: 0.000984069
	LOSS [training: 0.22173890103753865 | validation: 0.23357086939723304]
	TIME [epoch: 6.58 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24830928856419518		[learning rate: 0.00098175]
	Learning Rate: 0.000981748
	LOSS [training: 0.24830928856419518 | validation: 0.2806133013381079]
	TIME [epoch: 6.59 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26928864537355834		[learning rate: 0.00097943]
	Learning Rate: 0.000979432
	LOSS [training: 0.26928864537355834 | validation: 0.32000339082870977]
	TIME [epoch: 6.61 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2637809322868573		[learning rate: 0.00097712]
	Learning Rate: 0.000977122
	LOSS [training: 0.2637809322868573 | validation: 0.2172037472326489]
	TIME [epoch: 6.6 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21358873274442294		[learning rate: 0.00097482]
	Learning Rate: 0.000974817
	LOSS [training: 0.21358873274442294 | validation: 0.21281353737878256]
	TIME [epoch: 6.59 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2109878708914746		[learning rate: 0.00097252]
	Learning Rate: 0.000972517
	LOSS [training: 0.2109878708914746 | validation: 0.22587913286378392]
	TIME [epoch: 6.59 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21836242922513374		[learning rate: 0.00097022]
	Learning Rate: 0.000970224
	LOSS [training: 0.21836242922513374 | validation: 0.2579724310810886]
	TIME [epoch: 6.59 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22740349506609747		[learning rate: 0.00096793]
	Learning Rate: 0.000967935
	LOSS [training: 0.22740349506609747 | validation: 0.19594694944927402]
	TIME [epoch: 6.59 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.26828574607464245		[learning rate: 0.00096565]
	Learning Rate: 0.000965652
	LOSS [training: 0.26828574607464245 | validation: 0.26550084654937006]
	TIME [epoch: 6.61 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22732290173259498		[learning rate: 0.00096337]
	Learning Rate: 0.000963374
	LOSS [training: 0.22732290173259498 | validation: 0.24646548321188294]
	TIME [epoch: 6.62 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2332677726295739		[learning rate: 0.0009611]
	Learning Rate: 0.000961101
	LOSS [training: 0.2332677726295739 | validation: 0.2139345870515268]
	TIME [epoch: 6.59 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20687844071759437		[learning rate: 0.00095883]
	Learning Rate: 0.000958834
	LOSS [training: 0.20687844071759437 | validation: 0.20221844803120362]
	TIME [epoch: 6.59 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19980663539081206		[learning rate: 0.00095657]
	Learning Rate: 0.000956572
	LOSS [training: 0.19980663539081206 | validation: 0.20198578488462315]
	TIME [epoch: 6.58 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23011666109621892		[learning rate: 0.00095432]
	Learning Rate: 0.000954316
	LOSS [training: 0.23011666109621892 | validation: 0.24436255660276668]
	TIME [epoch: 6.59 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22192310815136276		[learning rate: 0.00095207]
	Learning Rate: 0.000952065
	LOSS [training: 0.22192310815136276 | validation: 0.22310650632993473]
	TIME [epoch: 6.64 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20171532985123228		[learning rate: 0.00094982]
	Learning Rate: 0.000949819
	LOSS [training: 0.20171532985123228 | validation: 0.24541607269384325]
	TIME [epoch: 6.6 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2130379788514011		[learning rate: 0.00094758]
	Learning Rate: 0.000947579
	LOSS [training: 0.2130379788514011 | validation: 0.23366575777022597]
	TIME [epoch: 6.59 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20329640264217647		[learning rate: 0.00094534]
	Learning Rate: 0.000945344
	LOSS [training: 0.20329640264217647 | validation: 0.22081775710364185]
	TIME [epoch: 6.59 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21956199420089484		[learning rate: 0.00094311]
	Learning Rate: 0.000943114
	LOSS [training: 0.21956199420089484 | validation: 0.18316843362975688]
	TIME [epoch: 6.59 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1979543076642954		[learning rate: 0.00094089]
	Learning Rate: 0.000940889
	LOSS [training: 0.1979543076642954 | validation: 0.18695535515617728]
	TIME [epoch: 6.6 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20419330785224057		[learning rate: 0.00093867]
	Learning Rate: 0.00093867
	LOSS [training: 0.20419330785224057 | validation: 0.2147572183189666]
	TIME [epoch: 6.63 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23526384849495088		[learning rate: 0.00093646]
	Learning Rate: 0.000936456
	LOSS [training: 0.23526384849495088 | validation: 0.23856160106992158]
	TIME [epoch: 6.6 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21715882906162962		[learning rate: 0.00093425]
	Learning Rate: 0.000934246
	LOSS [training: 0.21715882906162962 | validation: 0.20845284753708707]
	TIME [epoch: 6.59 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22174526575239314		[learning rate: 0.00093204]
	Learning Rate: 0.000932043
	LOSS [training: 0.22174526575239314 | validation: 0.22044849566978844]
	TIME [epoch: 6.59 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21366068836921331		[learning rate: 0.00092984]
	Learning Rate: 0.000929844
	LOSS [training: 0.21366068836921331 | validation: 0.23261326646290686]
	TIME [epoch: 6.58 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22600247580519		[learning rate: 0.00092765]
	Learning Rate: 0.000927651
	LOSS [training: 0.22600247580519 | validation: 0.24133728097909435]
	TIME [epoch: 6.62 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22332103635039405		[learning rate: 0.00092546]
	Learning Rate: 0.000925463
	LOSS [training: 0.22332103635039405 | validation: 0.2074654989191423]
	TIME [epoch: 6.61 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20681764391496377		[learning rate: 0.00092328]
	Learning Rate: 0.00092328
	LOSS [training: 0.20681764391496377 | validation: 0.21233654639314964]
	TIME [epoch: 6.59 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21976478415669196		[learning rate: 0.0009211]
	Learning Rate: 0.000921102
	LOSS [training: 0.21976478415669196 | validation: 0.2642581588585223]
	TIME [epoch: 6.58 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23080042355989092		[learning rate: 0.00091893]
	Learning Rate: 0.000918929
	LOSS [training: 0.23080042355989092 | validation: 0.2580582756214644]
	TIME [epoch: 6.58 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23498630530458203		[learning rate: 0.00091676]
	Learning Rate: 0.000916762
	LOSS [training: 0.23498630530458203 | validation: 0.20718649498611708]
	TIME [epoch: 6.58 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2338624790217964		[learning rate: 0.0009146]
	Learning Rate: 0.000914599
	LOSS [training: 0.2338624790217964 | validation: 0.20484811127153366]
	TIME [epoch: 6.62 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20856575304946348		[learning rate: 0.00091244]
	Learning Rate: 0.000912442
	LOSS [training: 0.20856575304946348 | validation: 0.2347715646657142]
	TIME [epoch: 6.58 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2189700765152532		[learning rate: 0.00091029]
	Learning Rate: 0.000910289
	LOSS [training: 0.2189700765152532 | validation: 0.19790532162494856]
	TIME [epoch: 6.58 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21038874322821677		[learning rate: 0.00090814]
	Learning Rate: 0.000908142
	LOSS [training: 0.21038874322821677 | validation: 0.19281377886932072]
	TIME [epoch: 6.58 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20977712761320264		[learning rate: 0.000906]
	Learning Rate: 0.000906
	LOSS [training: 0.20977712761320264 | validation: 0.18556068541391563]
	TIME [epoch: 6.58 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18667318107817357		[learning rate: 0.00090386]
	Learning Rate: 0.000903863
	LOSS [training: 0.18667318107817357 | validation: 0.19770865075130278]
	TIME [epoch: 6.6 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19357328668454749		[learning rate: 0.00090173]
	Learning Rate: 0.000901731
	LOSS [training: 0.19357328668454749 | validation: 0.20762605085971508]
	TIME [epoch: 6.61 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22843384856984295		[learning rate: 0.0008996]
	Learning Rate: 0.000899604
	LOSS [training: 0.22843384856984295 | validation: 0.19382827984408796]
	TIME [epoch: 6.6 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2097148540240516		[learning rate: 0.00089748]
	Learning Rate: 0.000897482
	LOSS [training: 0.2097148540240516 | validation: 0.19957062336071907]
	TIME [epoch: 6.59 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19602976774328648		[learning rate: 0.00089536]
	Learning Rate: 0.000895365
	LOSS [training: 0.19602976774328648 | validation: 0.20448706963885738]
	TIME [epoch: 6.59 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19702817988831606		[learning rate: 0.00089325]
	Learning Rate: 0.000893253
	LOSS [training: 0.19702817988831606 | validation: 0.19571583451346275]
	TIME [epoch: 6.59 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20965775016671598		[learning rate: 0.00089115]
	Learning Rate: 0.000891146
	LOSS [training: 0.20965775016671598 | validation: 0.22138288604294842]
	TIME [epoch: 6.61 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22072815756944322		[learning rate: 0.00088904]
	Learning Rate: 0.000889044
	LOSS [training: 0.22072815756944322 | validation: 0.21478569344387255]
	TIME [epoch: 6.6 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20905296780488017		[learning rate: 0.00088695]
	Learning Rate: 0.000886946
	LOSS [training: 0.20905296780488017 | validation: 0.269416936460511]
	TIME [epoch: 6.59 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23889639975587226		[learning rate: 0.00088485]
	Learning Rate: 0.000884854
	LOSS [training: 0.23889639975587226 | validation: 0.2296491369756537]
	TIME [epoch: 6.58 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23318098140384605		[learning rate: 0.00088277]
	Learning Rate: 0.000882767
	LOSS [training: 0.23318098140384605 | validation: 0.22307761204643045]
	TIME [epoch: 6.59 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25728298278296385		[learning rate: 0.00088068]
	Learning Rate: 0.000880685
	LOSS [training: 0.25728298278296385 | validation: 0.24491899555202307]
	TIME [epoch: 6.58 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22839917228459344		[learning rate: 0.00087861]
	Learning Rate: 0.000878607
	LOSS [training: 0.22839917228459344 | validation: 0.2435015765161376]
	TIME [epoch: 6.62 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24263145599394473		[learning rate: 0.00087653]
	Learning Rate: 0.000876535
	LOSS [training: 0.24263145599394473 | validation: 0.3092765632316195]
	TIME [epoch: 6.59 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2861387248487789		[learning rate: 0.00087447]
	Learning Rate: 0.000874467
	LOSS [training: 0.2861387248487789 | validation: 0.2810310339029948]
	TIME [epoch: 6.58 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2833275931424882		[learning rate: 0.0008724]
	Learning Rate: 0.000872405
	LOSS [training: 0.2833275931424882 | validation: 0.25531184897848]
	TIME [epoch: 6.59 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25581958912892694		[learning rate: 0.00087035]
	Learning Rate: 0.000870346
	LOSS [training: 0.25581958912892694 | validation: 0.22583641380013297]
	TIME [epoch: 6.59 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21775293748965954		[learning rate: 0.00086829]
	Learning Rate: 0.000868294
	LOSS [training: 0.21775293748965954 | validation: 0.26165692034027577]
	TIME [epoch: 6.6 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21862505913235827		[learning rate: 0.00086625]
	Learning Rate: 0.000866246
	LOSS [training: 0.21862505913235827 | validation: 0.24865216096878623]
	TIME [epoch: 6.61 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24358796692870888		[learning rate: 0.0008642]
	Learning Rate: 0.000864202
	LOSS [training: 0.24358796692870888 | validation: 0.23858631609858477]
	TIME [epoch: 6.59 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2305115191451073		[learning rate: 0.00086216]
	Learning Rate: 0.000862164
	LOSS [training: 0.2305115191451073 | validation: 0.22846109014288185]
	TIME [epoch: 6.58 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23568440207510064		[learning rate: 0.00086013]
	Learning Rate: 0.00086013
	LOSS [training: 0.23568440207510064 | validation: 0.20920454547460324]
	TIME [epoch: 6.58 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22384988277227214		[learning rate: 0.0008581]
	Learning Rate: 0.000858101
	LOSS [training: 0.22384988277227214 | validation: 0.2304475227457286]
	TIME [epoch: 6.58 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20747844910373212		[learning rate: 0.00085608]
	Learning Rate: 0.000856077
	LOSS [training: 0.20747844910373212 | validation: 0.2187261767405349]
	TIME [epoch: 6.61 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22799133092539167		[learning rate: 0.00085406]
	Learning Rate: 0.000854058
	LOSS [training: 0.22799133092539167 | validation: 0.23081875454734743]
	TIME [epoch: 6.61 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23143532156813137		[learning rate: 0.00085204]
	Learning Rate: 0.000852043
	LOSS [training: 0.23143532156813137 | validation: 0.19165120042110212]
	TIME [epoch: 6.58 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19826491263975027		[learning rate: 0.00085003]
	Learning Rate: 0.000850033
	LOSS [training: 0.19826491263975027 | validation: 0.1958057670678089]
	TIME [epoch: 6.59 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19569509233706758		[learning rate: 0.00084803]
	Learning Rate: 0.000848028
	LOSS [training: 0.19569509233706758 | validation: 0.2360738468551974]
	TIME [epoch: 6.59 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20242201762815443		[learning rate: 0.00084603]
	Learning Rate: 0.000846028
	LOSS [training: 0.20242201762815443 | validation: 0.18491838014051853]
	TIME [epoch: 6.6 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22465117038267568		[learning rate: 0.00084403]
	Learning Rate: 0.000844032
	LOSS [training: 0.22465117038267568 | validation: 0.21400059865305554]
	TIME [epoch: 6.62 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2144005084910789		[learning rate: 0.00084204]
	Learning Rate: 0.000842041
	LOSS [training: 0.2144005084910789 | validation: 0.1930701222885461]
	TIME [epoch: 6.58 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19498647650916148		[learning rate: 0.00084005]
	Learning Rate: 0.000840055
	LOSS [training: 0.19498647650916148 | validation: 0.21867548046059632]
	TIME [epoch: 6.59 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20096605746571106		[learning rate: 0.00083807]
	Learning Rate: 0.000838073
	LOSS [training: 0.20096605746571106 | validation: 0.1815459525275161]
	TIME [epoch: 6.6 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19364671478274054		[learning rate: 0.0008361]
	Learning Rate: 0.000836096
	LOSS [training: 0.19364671478274054 | validation: 0.2381643731509146]
	TIME [epoch: 6.58 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21311142436792627		[learning rate: 0.00083412]
	Learning Rate: 0.000834124
	LOSS [training: 0.21311142436792627 | validation: 0.20744844980874105]
	TIME [epoch: 6.61 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22406520094328494		[learning rate: 0.00083216]
	Learning Rate: 0.000832157
	LOSS [training: 0.22406520094328494 | validation: 0.219224484632157]
	TIME [epoch: 6.63 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21326850093150643		[learning rate: 0.00083019]
	Learning Rate: 0.000830194
	LOSS [training: 0.21326850093150643 | validation: 0.23195087402127718]
	TIME [epoch: 6.61 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.209181290096973		[learning rate: 0.00082824]
	Learning Rate: 0.000828236
	LOSS [training: 0.209181290096973 | validation: 0.18270687585671025]
	TIME [epoch: 6.61 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20400028343489143		[learning rate: 0.00082628]
	Learning Rate: 0.000826282
	LOSS [training: 0.20400028343489143 | validation: 0.191443244967739]
	TIME [epoch: 6.61 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20159258400323027		[learning rate: 0.00082433]
	Learning Rate: 0.000824333
	LOSS [training: 0.20159258400323027 | validation: 0.19409786169210397]
	TIME [epoch: 6.6 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19646806101015887		[learning rate: 0.00082239]
	Learning Rate: 0.000822388
	LOSS [training: 0.19646806101015887 | validation: 0.1824994014910459]
	TIME [epoch: 6.64 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20434478911210663		[learning rate: 0.00082045]
	Learning Rate: 0.000820448
	LOSS [training: 0.20434478911210663 | validation: 0.25873144018544103]
	TIME [epoch: 6.62 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22283229385146347		[learning rate: 0.00081851]
	Learning Rate: 0.000818513
	LOSS [training: 0.22283229385146347 | validation: 0.1765051634904795]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1110.pth
	Model improved!!!
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19833226244328914		[learning rate: 0.00081658]
	Learning Rate: 0.000816582
	LOSS [training: 0.19833226244328914 | validation: 0.23657070877010983]
	TIME [epoch: 6.6 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2146691398281159		[learning rate: 0.00081466]
	Learning Rate: 0.000814656
	LOSS [training: 0.2146691398281159 | validation: 0.20965577439469352]
	TIME [epoch: 6.6 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21040686017370885		[learning rate: 0.00081273]
	Learning Rate: 0.000812734
	LOSS [training: 0.21040686017370885 | validation: 0.18220885306037027]
	TIME [epoch: 6.59 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20131327473648106		[learning rate: 0.00081082]
	Learning Rate: 0.000810817
	LOSS [training: 0.20131327473648106 | validation: 0.19204389081358827]
	TIME [epoch: 6.64 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2006091113792198		[learning rate: 0.0008089]
	Learning Rate: 0.000808905
	LOSS [training: 0.2006091113792198 | validation: 0.21587522808866977]
	TIME [epoch: 6.6 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21574212410597035		[learning rate: 0.000807]
	Learning Rate: 0.000806997
	LOSS [training: 0.21574212410597035 | validation: 0.20161811175273292]
	TIME [epoch: 6.6 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21388454364257647		[learning rate: 0.00080509]
	Learning Rate: 0.000805093
	LOSS [training: 0.21388454364257647 | validation: 0.19258398942777077]
	TIME [epoch: 6.6 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21337416967774825		[learning rate: 0.00080319]
	Learning Rate: 0.000803194
	LOSS [training: 0.21337416967774825 | validation: 0.1784947890051496]
	TIME [epoch: 6.6 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19448459581731564		[learning rate: 0.0008013]
	Learning Rate: 0.000801299
	LOSS [training: 0.19448459581731564 | validation: 0.1893158299161979]
	TIME [epoch: 6.62 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22640029589063557		[learning rate: 0.00079941]
	Learning Rate: 0.000799409
	LOSS [training: 0.22640029589063557 | validation: 0.1882714334240544]
	TIME [epoch: 6.62 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1874953804142459		[learning rate: 0.00079752]
	Learning Rate: 0.000797524
	LOSS [training: 0.1874953804142459 | validation: 0.16274288683705163]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1121.pth
	Model improved!!!
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21285522914449456		[learning rate: 0.00079564]
	Learning Rate: 0.000795642
	LOSS [training: 0.21285522914449456 | validation: 0.19269903417951034]
	TIME [epoch: 6.59 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2612151689169925		[learning rate: 0.00079377]
	Learning Rate: 0.000793766
	LOSS [training: 0.2612151689169925 | validation: 0.1859736187963378]
	TIME [epoch: 6.59 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2045835370260372		[learning rate: 0.00079189]
	Learning Rate: 0.000791893
	LOSS [training: 0.2045835370260372 | validation: 0.1941351820261823]
	TIME [epoch: 6.59 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2121970265400332		[learning rate: 0.00079003]
	Learning Rate: 0.000790025
	LOSS [training: 0.2121970265400332 | validation: 0.19074221268170954]
	TIME [epoch: 6.61 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19186204874277749		[learning rate: 0.00078816]
	Learning Rate: 0.000788162
	LOSS [training: 0.19186204874277749 | validation: 0.1644356190759071]
	TIME [epoch: 6.61 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19560426272741185		[learning rate: 0.0007863]
	Learning Rate: 0.000786302
	LOSS [training: 0.19560426272741185 | validation: 0.2089990823102603]
	TIME [epoch: 6.59 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2313195577290613		[learning rate: 0.00078445]
	Learning Rate: 0.000784448
	LOSS [training: 0.2313195577290613 | validation: 0.18068814585891063]
	TIME [epoch: 6.58 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21782517856221845		[learning rate: 0.0007826]
	Learning Rate: 0.000782598
	LOSS [training: 0.21782517856221845 | validation: 0.19158239951295453]
	TIME [epoch: 6.58 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21499475636471868		[learning rate: 0.00078075]
	Learning Rate: 0.000780751
	LOSS [training: 0.21499475636471868 | validation: 0.19577875977857107]
	TIME [epoch: 6.58 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1949164358111218		[learning rate: 0.00077891]
	Learning Rate: 0.00077891
	LOSS [training: 0.1949164358111218 | validation: 0.207877751185981]
	TIME [epoch: 6.63 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18637810410193328		[learning rate: 0.00077707]
	Learning Rate: 0.000777073
	LOSS [training: 0.18637810410193328 | validation: 0.17450398651190868]
	TIME [epoch: 6.59 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19229641516498197		[learning rate: 0.00077524]
	Learning Rate: 0.000775239
	LOSS [training: 0.19229641516498197 | validation: 0.2169121483581165]
	TIME [epoch: 6.59 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20870981341051129		[learning rate: 0.00077341]
	Learning Rate: 0.000773411
	LOSS [training: 0.20870981341051129 | validation: 0.21122667528593392]
	TIME [epoch: 6.59 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19682242892473303		[learning rate: 0.00077159]
	Learning Rate: 0.000771586
	LOSS [training: 0.19682242892473303 | validation: 0.18191524104755014]
	TIME [epoch: 6.59 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19821853850110766		[learning rate: 0.00076977]
	Learning Rate: 0.000769766
	LOSS [training: 0.19821853850110766 | validation: 0.1811554348600768]
	TIME [epoch: 6.58 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19400307851966653		[learning rate: 0.00076795]
	Learning Rate: 0.000767951
	LOSS [training: 0.19400307851966653 | validation: 0.16180732607892234]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1137.pth
	Model improved!!!
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19082233855416378		[learning rate: 0.00076614]
	Learning Rate: 0.000766139
	LOSS [training: 0.19082233855416378 | validation: 0.17878523836465193]
	TIME [epoch: 6.61 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18696893983179336		[learning rate: 0.00076433]
	Learning Rate: 0.000764332
	LOSS [training: 0.18696893983179336 | validation: 0.2118268245201233]
	TIME [epoch: 6.6 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.209648850822362		[learning rate: 0.00076253]
	Learning Rate: 0.000762529
	LOSS [training: 0.209648850822362 | validation: 0.16437029993086544]
	TIME [epoch: 6.6 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18372287372245427		[learning rate: 0.00076073]
	Learning Rate: 0.00076073
	LOSS [training: 0.18372287372245427 | validation: 0.1654223117160745]
	TIME [epoch: 6.6 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19408973009917613		[learning rate: 0.00075894]
	Learning Rate: 0.000758936
	LOSS [training: 0.19408973009917613 | validation: 0.19436855270983078]
	TIME [epoch: 6.6 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1995461666562189		[learning rate: 0.00075715]
	Learning Rate: 0.000757146
	LOSS [training: 0.1995461666562189 | validation: 0.17823976303920758]
	TIME [epoch: 6.63 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18505784670713793		[learning rate: 0.00075536]
	Learning Rate: 0.00075536
	LOSS [training: 0.18505784670713793 | validation: 0.1643503616382878]
	TIME [epoch: 6.59 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20198091100672055		[learning rate: 0.00075358]
	Learning Rate: 0.000753578
	LOSS [training: 0.20198091100672055 | validation: 0.18353164093486674]
	TIME [epoch: 6.59 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19377203048132183		[learning rate: 0.0007518]
	Learning Rate: 0.0007518
	LOSS [training: 0.19377203048132183 | validation: 0.18479537732017065]
	TIME [epoch: 6.59 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1931158773291528		[learning rate: 0.00075003]
	Learning Rate: 0.000750027
	LOSS [training: 0.1931158773291528 | validation: 0.2003626373741]
	TIME [epoch: 6.59 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2136240002120579		[learning rate: 0.00074826]
	Learning Rate: 0.000748258
	LOSS [training: 0.2136240002120579 | validation: 0.18163864753163234]
	TIME [epoch: 6.6 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19302170668430368		[learning rate: 0.00074649]
	Learning Rate: 0.000746493
	LOSS [training: 0.19302170668430368 | validation: 0.2164975775351058]
	TIME [epoch: 6.63 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19748132684830877		[learning rate: 0.00074473]
	Learning Rate: 0.000744732
	LOSS [training: 0.19748132684830877 | validation: 0.17837350422882398]
	TIME [epoch: 6.59 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19288923384519777		[learning rate: 0.00074298]
	Learning Rate: 0.000742975
	LOSS [training: 0.19288923384519777 | validation: 0.1660915590180721]
	TIME [epoch: 6.59 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18693309296655466		[learning rate: 0.00074122]
	Learning Rate: 0.000741223
	LOSS [training: 0.18693309296655466 | validation: 0.1896332340151091]
	TIME [epoch: 6.58 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1992696801012806		[learning rate: 0.00073947]
	Learning Rate: 0.000739474
	LOSS [training: 0.1992696801012806 | validation: 0.2323591701341236]
	TIME [epoch: 6.58 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2228541838657169		[learning rate: 0.00073773]
	Learning Rate: 0.00073773
	LOSS [training: 0.2228541838657169 | validation: 0.1913947174147883]
	TIME [epoch: 6.61 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18830211894794555		[learning rate: 0.00073599]
	Learning Rate: 0.00073599
	LOSS [training: 0.18830211894794555 | validation: 0.18300405015394633]
	TIME [epoch: 6.61 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1881032821757537		[learning rate: 0.00073425]
	Learning Rate: 0.000734254
	LOSS [training: 0.1881032821757537 | validation: 0.185663541296941]
	TIME [epoch: 6.58 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19238718224841167		[learning rate: 0.00073252]
	Learning Rate: 0.000732522
	LOSS [training: 0.19238718224841167 | validation: 0.2001326110488904]
	TIME [epoch: 6.58 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19031161993417448		[learning rate: 0.00073079]
	Learning Rate: 0.000730794
	LOSS [training: 0.19031161993417448 | validation: 0.18886849544200668]
	TIME [epoch: 6.58 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18461768972968345		[learning rate: 0.00072907]
	Learning Rate: 0.00072907
	LOSS [training: 0.18461768972968345 | validation: 0.167733996532683]
	TIME [epoch: 6.58 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17581729339503355		[learning rate: 0.00072735]
	Learning Rate: 0.00072735
	LOSS [training: 0.17581729339503355 | validation: 0.1548220324793192]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1160.pth
	Model improved!!!
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16864738152985492		[learning rate: 0.00072563]
	Learning Rate: 0.000725634
	LOSS [training: 0.16864738152985492 | validation: 0.1708055606515237]
	TIME [epoch: 6.6 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19389098192273208		[learning rate: 0.00072392]
	Learning Rate: 0.000723923
	LOSS [training: 0.19389098192273208 | validation: 0.2031828543669323]
	TIME [epoch: 6.59 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1913413758805891		[learning rate: 0.00072222]
	Learning Rate: 0.000722215
	LOSS [training: 0.1913413758805891 | validation: 0.22346105693960905]
	TIME [epoch: 6.58 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21084846439961402		[learning rate: 0.00072051]
	Learning Rate: 0.000720512
	LOSS [training: 0.21084846439961402 | validation: 0.1877505020723942]
	TIME [epoch: 6.58 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24230006740337767		[learning rate: 0.00071881]
	Learning Rate: 0.000718812
	LOSS [training: 0.24230006740337767 | validation: 0.21265074936082703]
	TIME [epoch: 6.58 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19195863826138732		[learning rate: 0.00071712]
	Learning Rate: 0.000717117
	LOSS [training: 0.19195863826138732 | validation: 0.16843341865637823]
	TIME [epoch: 6.61 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18257333457000735		[learning rate: 0.00071542]
	Learning Rate: 0.000715425
	LOSS [training: 0.18257333457000735 | validation: 0.18438103774012896]
	TIME [epoch: 6.58 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18171359718095964		[learning rate: 0.00071374]
	Learning Rate: 0.000713738
	LOSS [training: 0.18171359718095964 | validation: 0.21024134549086676]
	TIME [epoch: 6.58 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19752323180234704		[learning rate: 0.00071205]
	Learning Rate: 0.000712054
	LOSS [training: 0.19752323180234704 | validation: 0.17933352490251747]
	TIME [epoch: 6.58 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19278516114784322		[learning rate: 0.00071037]
	Learning Rate: 0.000710374
	LOSS [training: 0.19278516114784322 | validation: 0.18136965166953456]
	TIME [epoch: 6.59 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21959116999398343		[learning rate: 0.0007087]
	Learning Rate: 0.000708698
	LOSS [training: 0.21959116999398343 | validation: 0.2066623290539169]
	TIME [epoch: 6.59 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22625450695869995		[learning rate: 0.00070703]
	Learning Rate: 0.000707027
	LOSS [training: 0.22625450695869995 | validation: 0.2186008708745752]
	TIME [epoch: 6.62 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22504550329393389		[learning rate: 0.00070536]
	Learning Rate: 0.000705359
	LOSS [training: 0.22504550329393389 | validation: 0.211611919257728]
	TIME [epoch: 6.58 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20461000120272138		[learning rate: 0.0007037]
	Learning Rate: 0.000703695
	LOSS [training: 0.20461000120272138 | validation: 0.1769116953576497]
	TIME [epoch: 6.58 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20061166997019844		[learning rate: 0.00070204]
	Learning Rate: 0.000702035
	LOSS [training: 0.20061166997019844 | validation: 0.20240224608569046]
	TIME [epoch: 6.58 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21553310722949734		[learning rate: 0.00070038]
	Learning Rate: 0.000700379
	LOSS [training: 0.21553310722949734 | validation: 0.1828761327299154]
	TIME [epoch: 6.59 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21175801536087668		[learning rate: 0.00069873]
	Learning Rate: 0.000698727
	LOSS [training: 0.21175801536087668 | validation: 0.21150042059024052]
	TIME [epoch: 6.61 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20058789863104542		[learning rate: 0.00069708]
	Learning Rate: 0.000697079
	LOSS [training: 0.20058789863104542 | validation: 0.1620628702327892]
	TIME [epoch: 6.63 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18785398404385342		[learning rate: 0.00069543]
	Learning Rate: 0.000695435
	LOSS [training: 0.18785398404385342 | validation: 0.1844759124808359]
	TIME [epoch: 6.59 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20457149873504082		[learning rate: 0.00069379]
	Learning Rate: 0.000693794
	LOSS [training: 0.20457149873504082 | validation: 0.2054898670427689]
	TIME [epoch: 6.59 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24879213426770908		[learning rate: 0.00069216]
	Learning Rate: 0.000692158
	LOSS [training: 0.24879213426770908 | validation: 0.17594369741724938]
	TIME [epoch: 6.58 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1841838636727117		[learning rate: 0.00069053]
	Learning Rate: 0.000690525
	LOSS [training: 0.1841838636727117 | validation: 0.17586916710170147]
	TIME [epoch: 6.58 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18721600354747947		[learning rate: 0.0006889]
	Learning Rate: 0.000688896
	LOSS [training: 0.18721600354747947 | validation: 0.165142472758336]
	TIME [epoch: 6.61 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18177656430261652		[learning rate: 0.00068727]
	Learning Rate: 0.000687271
	LOSS [training: 0.18177656430261652 | validation: 0.17251727590268895]
	TIME [epoch: 6.6 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18649662270953155		[learning rate: 0.00068565]
	Learning Rate: 0.00068565
	LOSS [training: 0.18649662270953155 | validation: 0.1792154783560554]
	TIME [epoch: 6.59 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1795657693152973		[learning rate: 0.00068403]
	Learning Rate: 0.000684033
	LOSS [training: 0.1795657693152973 | validation: 0.16645428685615765]
	TIME [epoch: 6.6 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2008770552183722		[learning rate: 0.00068242]
	Learning Rate: 0.000682419
	LOSS [training: 0.2008770552183722 | validation: 0.19319033788572246]
	TIME [epoch: 6.59 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20096493761532522		[learning rate: 0.00068081]
	Learning Rate: 0.00068081
	LOSS [training: 0.20096493761532522 | validation: 0.19333371894576964]
	TIME [epoch: 6.58 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2141553155270716		[learning rate: 0.0006792]
	Learning Rate: 0.000679204
	LOSS [training: 0.2141553155270716 | validation: 0.19505795667328196]
	TIME [epoch: 6.62 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19157030727797003		[learning rate: 0.0006776]
	Learning Rate: 0.000677601
	LOSS [training: 0.19157030727797003 | validation: 0.1615893967242587]
	TIME [epoch: 6.58 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18116130408675532		[learning rate: 0.000676]
	Learning Rate: 0.000676003
	LOSS [training: 0.18116130408675532 | validation: 0.18558748638393013]
	TIME [epoch: 6.58 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2007228713930652		[learning rate: 0.00067441]
	Learning Rate: 0.000674409
	LOSS [training: 0.2007228713930652 | validation: 0.25917853570364136]
	TIME [epoch: 6.6 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2230169069057943		[learning rate: 0.00067282]
	Learning Rate: 0.000672818
	LOSS [training: 0.2230169069057943 | validation: 0.1903522656340373]
	TIME [epoch: 6.6 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19743539512309285		[learning rate: 0.00067123]
	Learning Rate: 0.000671231
	LOSS [training: 0.19743539512309285 | validation: 0.19891551936471402]
	TIME [epoch: 6.59 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21488729447180485		[learning rate: 0.00066965]
	Learning Rate: 0.000669647
	LOSS [training: 0.21488729447180485 | validation: 0.1840233048880548]
	TIME [epoch: 6.61 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2147653582993169		[learning rate: 0.00066807]
	Learning Rate: 0.000668068
	LOSS [training: 0.2147653582993169 | validation: 0.170372364771656]
	TIME [epoch: 6.58 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1794730928843284		[learning rate: 0.00066649]
	Learning Rate: 0.000666492
	LOSS [training: 0.1794730928843284 | validation: 0.16446010579433365]
	TIME [epoch: 6.58 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17940474750996235		[learning rate: 0.00066492]
	Learning Rate: 0.00066492
	LOSS [training: 0.17940474750996235 | validation: 0.16707461695918396]
	TIME [epoch: 6.58 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18457481752868507		[learning rate: 0.00066335]
	Learning Rate: 0.000663351
	LOSS [training: 0.18457481752868507 | validation: 0.1587756447977998]
	TIME [epoch: 6.58 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1758736561262025		[learning rate: 0.00066179]
	Learning Rate: 0.000661786
	LOSS [training: 0.1758736561262025 | validation: 0.17147158215913821]
	TIME [epoch: 6.6 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17784303087211278		[learning rate: 0.00066023]
	Learning Rate: 0.000660225
	LOSS [training: 0.17784303087211278 | validation: 0.19250594830543394]
	TIME [epoch: 6.6 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18509856319802206		[learning rate: 0.00065867]
	Learning Rate: 0.000658668
	LOSS [training: 0.18509856319802206 | validation: 0.1949052145308544]
	TIME [epoch: 6.58 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19525536478857206		[learning rate: 0.00065711]
	Learning Rate: 0.000657114
	LOSS [training: 0.19525536478857206 | validation: 0.2022148720020807]
	TIME [epoch: 6.59 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22164960025930974		[learning rate: 0.00065556]
	Learning Rate: 0.000655564
	LOSS [training: 0.22164960025930974 | validation: 0.2245590722186428]
	TIME [epoch: 6.58 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21033496047533423		[learning rate: 0.00065402]
	Learning Rate: 0.000654018
	LOSS [training: 0.21033496047533423 | validation: 0.22194823338730907]
	TIME [epoch: 6.59 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20262859318127693		[learning rate: 0.00065248]
	Learning Rate: 0.000652475
	LOSS [training: 0.20262859318127693 | validation: 0.2397039008977918]
	TIME [epoch: 6.62 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.25187360144995935		[learning rate: 0.00065094]
	Learning Rate: 0.000650936
	LOSS [training: 0.25187360144995935 | validation: 0.22789892895821245]
	TIME [epoch: 6.59 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22038459608443897		[learning rate: 0.0006494]
	Learning Rate: 0.000649401
	LOSS [training: 0.22038459608443897 | validation: 0.25295416628349104]
	TIME [epoch: 6.58 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22957179547416123		[learning rate: 0.00064787]
	Learning Rate: 0.000647869
	LOSS [training: 0.22957179547416123 | validation: 0.2711516630940338]
	TIME [epoch: 6.59 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.24336367823199515		[learning rate: 0.00064634]
	Learning Rate: 0.000646341
	LOSS [training: 0.24336367823199515 | validation: 0.16882368318274177]
	TIME [epoch: 6.58 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1894698906620249		[learning rate: 0.00064482]
	Learning Rate: 0.000644816
	LOSS [training: 0.1894698906620249 | validation: 0.1712537710848886]
	TIME [epoch: 6.6 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19199313804645157		[learning rate: 0.0006433]
	Learning Rate: 0.000643295
	LOSS [training: 0.19199313804645157 | validation: 0.20329038943183703]
	TIME [epoch: 6.62 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19339313620628554		[learning rate: 0.00064178]
	Learning Rate: 0.000641778
	LOSS [training: 0.19339313620628554 | validation: 0.17819717840874078]
	TIME [epoch: 6.61 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18298832400992546		[learning rate: 0.00064026]
	Learning Rate: 0.000640264
	LOSS [training: 0.18298832400992546 | validation: 0.18325995188232203]
	TIME [epoch: 6.59 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1808853731188093		[learning rate: 0.00063875]
	Learning Rate: 0.000638754
	LOSS [training: 0.1808853731188093 | validation: 0.2099839197095375]
	TIME [epoch: 6.6 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19817022603222162		[learning rate: 0.00063725]
	Learning Rate: 0.000637247
	LOSS [training: 0.19817022603222162 | validation: 0.1764911459207867]
	TIME [epoch: 6.58 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19237684447328235		[learning rate: 0.00063574]
	Learning Rate: 0.000635744
	LOSS [training: 0.19237684447328235 | validation: 0.19437341082156046]
	TIME [epoch: 6.62 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1821814973696272		[learning rate: 0.00063424]
	Learning Rate: 0.000634244
	LOSS [training: 0.1821814973696272 | validation: 0.16227824049075112]
	TIME [epoch: 6.59 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16842203788899296		[learning rate: 0.00063275]
	Learning Rate: 0.000632748
	LOSS [training: 0.16842203788899296 | validation: 0.17618790108783722]
	TIME [epoch: 6.59 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18882797914431296		[learning rate: 0.00063126]
	Learning Rate: 0.000631255
	LOSS [training: 0.18882797914431296 | validation: 0.16506924230764938]
	TIME [epoch: 6.58 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20848019951883798		[learning rate: 0.00062977]
	Learning Rate: 0.000629766
	LOSS [training: 0.20848019951883798 | validation: 0.17151692126351148]
	TIME [epoch: 6.59 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18085711467821783		[learning rate: 0.00062828]
	Learning Rate: 0.000628281
	LOSS [training: 0.18085711467821783 | validation: 0.18128301303585703]
	TIME [epoch: 6.58 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20125067390686496		[learning rate: 0.0006268]
	Learning Rate: 0.000626799
	LOSS [training: 0.20125067390686496 | validation: 0.15724385073242295]
	TIME [epoch: 6.63 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18366769388242316		[learning rate: 0.00062532]
	Learning Rate: 0.00062532
	LOSS [training: 0.18366769388242316 | validation: 0.18090633551731672]
	TIME [epoch: 6.57 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21953475121893878		[learning rate: 0.00062385]
	Learning Rate: 0.000623845
	LOSS [training: 0.21953475121893878 | validation: 0.18809278030361015]
	TIME [epoch: 6.58 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1983626253192909		[learning rate: 0.00062237]
	Learning Rate: 0.000622374
	LOSS [training: 0.1983626253192909 | validation: 0.17560064852625618]
	TIME [epoch: 6.59 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1809499770992931		[learning rate: 0.00062091]
	Learning Rate: 0.000620906
	LOSS [training: 0.1809499770992931 | validation: 0.17504903589484025]
	TIME [epoch: 6.58 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1740290485611104		[learning rate: 0.00061944]
	Learning Rate: 0.000619441
	LOSS [training: 0.1740290485611104 | validation: 0.16776178674923217]
	TIME [epoch: 6.6 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1921294243371937		[learning rate: 0.00061798]
	Learning Rate: 0.00061798
	LOSS [training: 0.1921294243371937 | validation: 0.17681515497004532]
	TIME [epoch: 6.6 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18841108999225076		[learning rate: 0.00061652]
	Learning Rate: 0.000616522
	LOSS [training: 0.18841108999225076 | validation: 0.17505815616542697]
	TIME [epoch: 6.58 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.180689004626804		[learning rate: 0.00061507]
	Learning Rate: 0.000615068
	LOSS [training: 0.180689004626804 | validation: 0.19541334743091432]
	TIME [epoch: 6.6 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20386276734697378		[learning rate: 0.00061362]
	Learning Rate: 0.000613617
	LOSS [training: 0.20386276734697378 | validation: 0.21376257753125724]
	TIME [epoch: 6.6 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21847029659511225		[learning rate: 0.00061217]
	Learning Rate: 0.00061217
	LOSS [training: 0.21847029659511225 | validation: 0.2005482306031529]
	TIME [epoch: 6.58 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20760368078196406		[learning rate: 0.00061073]
	Learning Rate: 0.000610726
	LOSS [training: 0.20760368078196406 | validation: 0.2451859486602282]
	TIME [epoch: 6.62 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.22811092356364013		[learning rate: 0.00060929]
	Learning Rate: 0.000609285
	LOSS [training: 0.22811092356364013 | validation: 0.2179060397942652]
	TIME [epoch: 6.59 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2001198882888414		[learning rate: 0.00060785]
	Learning Rate: 0.000607848
	LOSS [training: 0.2001198882888414 | validation: 0.19046519126695857]
	TIME [epoch: 6.58 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20206095614292624		[learning rate: 0.00060641]
	Learning Rate: 0.000606414
	LOSS [training: 0.20206095614292624 | validation: 0.2194017935144368]
	TIME [epoch: 6.58 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19868557961801572		[learning rate: 0.00060498]
	Learning Rate: 0.000604983
	LOSS [training: 0.19868557961801572 | validation: 0.19739410259092585]
	TIME [epoch: 6.57 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18701201156694927		[learning rate: 0.00060356]
	Learning Rate: 0.000603557
	LOSS [training: 0.18701201156694927 | validation: 0.19229152104654734]
	TIME [epoch: 6.58 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19958656073874073		[learning rate: 0.00060213]
	Learning Rate: 0.000602133
	LOSS [training: 0.19958656073874073 | validation: 0.1972942497405432]
	TIME [epoch: 6.6 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2080371111469316		[learning rate: 0.00060071]
	Learning Rate: 0.000600712
	LOSS [training: 0.2080371111469316 | validation: 0.19427723842672867]
	TIME [epoch: 6.57 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2091342368246391		[learning rate: 0.0005993]
	Learning Rate: 0.000599296
	LOSS [training: 0.2091342368246391 | validation: 0.21265813834167613]
	TIME [epoch: 6.57 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20604532195722972		[learning rate: 0.00059788]
	Learning Rate: 0.000597882
	LOSS [training: 0.20604532195722972 | validation: 0.22285396060434243]
	TIME [epoch: 6.57 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20474100746191937		[learning rate: 0.00059647]
	Learning Rate: 0.000596471
	LOSS [training: 0.20474100746191937 | validation: 0.2125620872693107]
	TIME [epoch: 6.57 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21456632462274156		[learning rate: 0.00059506]
	Learning Rate: 0.000595065
	LOSS [training: 0.21456632462274156 | validation: 0.1979740821610122]
	TIME [epoch: 6.59 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1955496089118544		[learning rate: 0.00059366]
	Learning Rate: 0.000593661
	LOSS [training: 0.1955496089118544 | validation: 0.19148105913937372]
	TIME [epoch: 6.59 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1964761343728444		[learning rate: 0.00059226]
	Learning Rate: 0.000592261
	LOSS [training: 0.1964761343728444 | validation: 0.19783131932017384]
	TIME [epoch: 6.57 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19508857619315473		[learning rate: 0.00059086]
	Learning Rate: 0.000590863
	LOSS [training: 0.19508857619315473 | validation: 0.19352468181987872]
	TIME [epoch: 6.57 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20292926215225418		[learning rate: 0.00058947]
	Learning Rate: 0.00058947
	LOSS [training: 0.20292926215225418 | validation: 0.22127638589105753]
	TIME [epoch: 6.59 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20060735442660085		[learning rate: 0.00058808]
	Learning Rate: 0.000588079
	LOSS [training: 0.20060735442660085 | validation: 0.1819864036110845]
	TIME [epoch: 6.57 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1929427016692434		[learning rate: 0.00058669]
	Learning Rate: 0.000586692
	LOSS [training: 0.1929427016692434 | validation: 0.18870743533806197]
	TIME [epoch: 6.6 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17667518833593432		[learning rate: 0.00058531]
	Learning Rate: 0.000585308
	LOSS [training: 0.17667518833593432 | validation: 0.18813058707599276]
	TIME [epoch: 6.58 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19655249321014007		[learning rate: 0.00058393]
	Learning Rate: 0.000583927
	LOSS [training: 0.19655249321014007 | validation: 0.20419316506923546]
	TIME [epoch: 6.57 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21556457188493652		[learning rate: 0.00058255]
	Learning Rate: 0.00058255
	LOSS [training: 0.21556457188493652 | validation: 0.2069527760742228]
	TIME [epoch: 6.57 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21341472810854328		[learning rate: 0.00058118]
	Learning Rate: 0.000581176
	LOSS [training: 0.21341472810854328 | validation: 0.23055615755082465]
	TIME [epoch: 6.57 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2110116715706043		[learning rate: 0.00057981]
	Learning Rate: 0.000579805
	LOSS [training: 0.2110116715706043 | validation: 0.22767662317512652]
	TIME [epoch: 6.58 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2326471316949727		[learning rate: 0.00057844]
	Learning Rate: 0.000578437
	LOSS [training: 0.2326471316949727 | validation: 0.2086686973395685]
	TIME [epoch: 6.62 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20545479644335912		[learning rate: 0.00057707]
	Learning Rate: 0.000577073
	LOSS [training: 0.20545479644335912 | validation: 0.20537052787679172]
	TIME [epoch: 6.58 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20969057302684924		[learning rate: 0.00057571]
	Learning Rate: 0.000575712
	LOSS [training: 0.20969057302684924 | validation: 0.25064006060632005]
	TIME [epoch: 6.58 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2237584527401948		[learning rate: 0.00057435]
	Learning Rate: 0.000574354
	LOSS [training: 0.2237584527401948 | validation: 0.22522445849484476]
	TIME [epoch: 6.57 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20832210502161203		[learning rate: 0.000573]
	Learning Rate: 0.000572999
	LOSS [training: 0.20832210502161203 | validation: 0.19590584457697627]
	TIME [epoch: 6.57 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17751958919043284		[learning rate: 0.00057165]
	Learning Rate: 0.000571647
	LOSS [training: 0.17751958919043284 | validation: 0.15866837634647357]
	TIME [epoch: 6.6 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1745443483228708		[learning rate: 0.0005703]
	Learning Rate: 0.000570299
	LOSS [training: 0.1745443483228708 | validation: 0.16794732460947331]
	TIME [epoch: 6.6 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1778787884820484		[learning rate: 0.00056895]
	Learning Rate: 0.000568954
	LOSS [training: 0.1778787884820484 | validation: 0.18259658144508933]
	TIME [epoch: 6.57 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17124891967182504		[learning rate: 0.00056761]
	Learning Rate: 0.000567612
	LOSS [training: 0.17124891967182504 | validation: 0.17343759087043198]
	TIME [epoch: 6.58 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19470912545621463		[learning rate: 0.00056627]
	Learning Rate: 0.000566273
	LOSS [training: 0.19470912545621463 | validation: 0.16907237183684204]
	TIME [epoch: 6.57 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17536980115095463		[learning rate: 0.00056494]
	Learning Rate: 0.000564937
	LOSS [training: 0.17536980115095463 | validation: 0.15514739983878423]
	TIME [epoch: 6.58 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16854020770167338		[learning rate: 0.0005636]
	Learning Rate: 0.000563604
	LOSS [training: 0.16854020770167338 | validation: 0.16892097086733304]
	TIME [epoch: 6.62 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1889737876245326		[learning rate: 0.00056227]
	Learning Rate: 0.000562275
	LOSS [training: 0.1889737876245326 | validation: 0.15180501223575305]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1269.pth
	Model improved!!!
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17584957861764966		[learning rate: 0.00056095]
	Learning Rate: 0.000560949
	LOSS [training: 0.17584957861764966 | validation: 0.17035440060941398]
	TIME [epoch: 6.58 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18033292286765584		[learning rate: 0.00055963]
	Learning Rate: 0.000559625
	LOSS [training: 0.18033292286765584 | validation: 0.18548459298011447]
	TIME [epoch: 6.58 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18806812936965628		[learning rate: 0.00055831]
	Learning Rate: 0.000558305
	LOSS [training: 0.18806812936965628 | validation: 0.1625785175302298]
	TIME [epoch: 6.57 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16927970694519515		[learning rate: 0.00055699]
	Learning Rate: 0.000556988
	LOSS [training: 0.16927970694519515 | validation: 0.16308042060946926]
	TIME [epoch: 6.6 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17075763138308472		[learning rate: 0.00055567]
	Learning Rate: 0.000555674
	LOSS [training: 0.17075763138308472 | validation: 0.18149082135610456]
	TIME [epoch: 6.61 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18060892012212887		[learning rate: 0.00055436]
	Learning Rate: 0.000554364
	LOSS [training: 0.18060892012212887 | validation: 0.1700636775953637]
	TIME [epoch: 6.58 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17914708409443508		[learning rate: 0.00055306]
	Learning Rate: 0.000553056
	LOSS [training: 0.17914708409443508 | validation: 0.17646259931722738]
	TIME [epoch: 6.57 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18163243850822414		[learning rate: 0.00055175]
	Learning Rate: 0.000551752
	LOSS [training: 0.18163243850822414 | validation: 0.1695185313541271]
	TIME [epoch: 6.58 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17151479764863173		[learning rate: 0.00055045]
	Learning Rate: 0.00055045
	LOSS [training: 0.17151479764863173 | validation: 0.16146843350380172]
	TIME [epoch: 6.59 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18303082558569334		[learning rate: 0.00054915]
	Learning Rate: 0.000549152
	LOSS [training: 0.18303082558569334 | validation: 0.17821431388981374]
	TIME [epoch: 6.63 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17239172722976356		[learning rate: 0.00054786]
	Learning Rate: 0.000547856
	LOSS [training: 0.17239172722976356 | validation: 0.1586902176412667]
	TIME [epoch: 6.59 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17625118220086539		[learning rate: 0.00054656]
	Learning Rate: 0.000546564
	LOSS [training: 0.17625118220086539 | validation: 0.15669525178524424]
	TIME [epoch: 6.6 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17098625508116555		[learning rate: 0.00054527]
	Learning Rate: 0.000545275
	LOSS [training: 0.17098625508116555 | validation: 0.14521912779985413]
	TIME [epoch: 6.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1282.pth
	Model improved!!!
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17448818614262115		[learning rate: 0.00054399]
	Learning Rate: 0.000543988
	LOSS [training: 0.17448818614262115 | validation: 0.15704626922707793]
	TIME [epoch: 6.6 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20894193477124162		[learning rate: 0.00054271]
	Learning Rate: 0.000542705
	LOSS [training: 0.20894193477124162 | validation: 0.20744452288176038]
	TIME [epoch: 6.59 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18133884590213584		[learning rate: 0.00054143]
	Learning Rate: 0.000541425
	LOSS [training: 0.18133884590213584 | validation: 0.17508625318807391]
	TIME [epoch: 6.63 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18347052566714103		[learning rate: 0.00054015]
	Learning Rate: 0.000540148
	LOSS [training: 0.18347052566714103 | validation: 0.1630221926038981]
	TIME [epoch: 6.6 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1710755901437969		[learning rate: 0.00053887]
	Learning Rate: 0.000538874
	LOSS [training: 0.1710755901437969 | validation: 0.2025506025749061]
	TIME [epoch: 6.58 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18352461767182532		[learning rate: 0.0005376]
	Learning Rate: 0.000537603
	LOSS [training: 0.18352461767182532 | validation: 0.19275288120725476]
	TIME [epoch: 6.59 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17003941210504445		[learning rate: 0.00053633]
	Learning Rate: 0.000536335
	LOSS [training: 0.17003941210504445 | validation: 0.15965794439682568]
	TIME [epoch: 6.59 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19126152568913063		[learning rate: 0.00053507]
	Learning Rate: 0.00053507
	LOSS [training: 0.19126152568913063 | validation: 0.16865654292824117]
	TIME [epoch: 6.6 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1787716427529468		[learning rate: 0.00053381]
	Learning Rate: 0.000533807
	LOSS [training: 0.1787716427529468 | validation: 0.18137294552620264]
	TIME [epoch: 6.62 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18503420511568175		[learning rate: 0.00053255]
	Learning Rate: 0.000532548
	LOSS [training: 0.18503420511568175 | validation: 0.17158598486999999]
	TIME [epoch: 6.6 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17992915676189264		[learning rate: 0.00053129]
	Learning Rate: 0.000531292
	LOSS [training: 0.17992915676189264 | validation: 0.1643942718623898]
	TIME [epoch: 6.58 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17536507490687128		[learning rate: 0.00053004]
	Learning Rate: 0.000530039
	LOSS [training: 0.17536507490687128 | validation: 0.15458390417640272]
	TIME [epoch: 6.58 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1971983423388582		[learning rate: 0.00052879]
	Learning Rate: 0.000528789
	LOSS [training: 0.1971983423388582 | validation: 0.15810441694890182]
	TIME [epoch: 6.57 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18549709674674633		[learning rate: 0.00052754]
	Learning Rate: 0.000527541
	LOSS [training: 0.18549709674674633 | validation: 0.1637820412357649]
	TIME [epoch: 6.57 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20310633863763705		[learning rate: 0.0005263]
	Learning Rate: 0.000526297
	LOSS [training: 0.20310633863763705 | validation: 0.1947683146885354]
	TIME [epoch: 6.61 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20277444707584155		[learning rate: 0.00052506]
	Learning Rate: 0.000525055
	LOSS [training: 0.20277444707584155 | validation: 0.1838817776156148]
	TIME [epoch: 6.58 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19756740690946126		[learning rate: 0.00052382]
	Learning Rate: 0.000523817
	LOSS [training: 0.19756740690946126 | validation: 0.18593242987540776]
	TIME [epoch: 6.59 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1967864681175636		[learning rate: 0.00052258]
	Learning Rate: 0.000522581
	LOSS [training: 0.1967864681175636 | validation: 0.17218738200275424]
	TIME [epoch: 6.59 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21354901358945097		[learning rate: 0.00052135]
	Learning Rate: 0.000521348
	LOSS [training: 0.21354901358945097 | validation: 0.1741772022520403]
	TIME [epoch: 6.59 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20458472647238982		[learning rate: 0.00052012]
	Learning Rate: 0.000520119
	LOSS [training: 0.20458472647238982 | validation: 0.17015763541493448]
	TIME [epoch: 6.61 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17542936642310727		[learning rate: 0.00051889]
	Learning Rate: 0.000518892
	LOSS [training: 0.17542936642310727 | validation: 0.16636200165394932]
	TIME [epoch: 6.63 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16711087842514014		[learning rate: 0.00051767]
	Learning Rate: 0.000517668
	LOSS [training: 0.16711087842514014 | validation: 0.15264396528056084]
	TIME [epoch: 6.6 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17629195632502154		[learning rate: 0.00051645]
	Learning Rate: 0.000516447
	LOSS [training: 0.17629195632502154 | validation: 0.1757868755499066]
	TIME [epoch: 6.59 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19159825034235548		[learning rate: 0.00051523]
	Learning Rate: 0.000515229
	LOSS [training: 0.19159825034235548 | validation: 0.20546669431875753]
	TIME [epoch: 6.59 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1891830504718114		[learning rate: 0.00051401]
	Learning Rate: 0.000514013
	LOSS [training: 0.1891830504718114 | validation: 0.1754247208810077]
	TIME [epoch: 6.6 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20401989209696123		[learning rate: 0.0005128]
	Learning Rate: 0.000512801
	LOSS [training: 0.20401989209696123 | validation: 0.1872728466438459]
	TIME [epoch: 6.61 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19224203404740167		[learning rate: 0.00051159]
	Learning Rate: 0.000511591
	LOSS [training: 0.19224203404740167 | validation: 0.19299427055227278]
	TIME [epoch: 6.6 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20592545386306765		[learning rate: 0.00051038]
	Learning Rate: 0.000510384
	LOSS [training: 0.20592545386306765 | validation: 0.16820482959892]
	TIME [epoch: 6.61 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16753864211992403		[learning rate: 0.00050918]
	Learning Rate: 0.00050918
	LOSS [training: 0.16753864211992403 | validation: 0.15072157727378147]
	TIME [epoch: 6.58 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18814214152250208		[learning rate: 0.00050798]
	Learning Rate: 0.000507979
	LOSS [training: 0.18814214152250208 | validation: 0.19200162998115528]
	TIME [epoch: 6.6 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17598723200361371		[learning rate: 0.00050678]
	Learning Rate: 0.000506781
	LOSS [training: 0.17598723200361371 | validation: 0.15041821238435887]
	TIME [epoch: 6.58 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16658964432098228		[learning rate: 0.00050559]
	Learning Rate: 0.000505586
	LOSS [training: 0.16658964432098228 | validation: 0.15701430958278256]
	TIME [epoch: 6.61 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19644838460352135		[learning rate: 0.00050439]
	Learning Rate: 0.000504393
	LOSS [training: 0.19644838460352135 | validation: 0.22650209543225713]
	TIME [epoch: 6.57 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2194405080909913		[learning rate: 0.0005032]
	Learning Rate: 0.000503203
	LOSS [training: 0.2194405080909913 | validation: 0.15909375673264176]
	TIME [epoch: 6.58 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1741427751339797		[learning rate: 0.00050202]
	Learning Rate: 0.000502016
	LOSS [training: 0.1741427751339797 | validation: 0.15648771140641415]
	TIME [epoch: 6.59 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17729679980051005		[learning rate: 0.00050083]
	Learning Rate: 0.000500832
	LOSS [training: 0.17729679980051005 | validation: 0.1565565788801354]
	TIME [epoch: 6.59 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17309555878851274		[learning rate: 0.00049965]
	Learning Rate: 0.000499651
	LOSS [training: 0.17309555878851274 | validation: 0.14795296484598083]
	TIME [epoch: 6.6 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1627166083566731		[learning rate: 0.00049847]
	Learning Rate: 0.000498472
	LOSS [training: 0.1627166083566731 | validation: 0.16723613499816298]
	TIME [epoch: 6.62 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19190238066562443		[learning rate: 0.0004973]
	Learning Rate: 0.000497296
	LOSS [training: 0.19190238066562443 | validation: 0.1786158902358348]
	TIME [epoch: 6.59 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18903223404159497		[learning rate: 0.00049612]
	Learning Rate: 0.000496123
	LOSS [training: 0.18903223404159497 | validation: 0.17226741054572037]
	TIME [epoch: 6.59 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1784473861897637		[learning rate: 0.00049495]
	Learning Rate: 0.000494953
	LOSS [training: 0.1784473861897637 | validation: 0.17760915971835184]
	TIME [epoch: 6.59 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17937317037606923		[learning rate: 0.00049379]
	Learning Rate: 0.000493786
	LOSS [training: 0.17937317037606923 | validation: 0.16523303464586714]
	TIME [epoch: 6.6 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1746033563480909		[learning rate: 0.00049262]
	Learning Rate: 0.000492621
	LOSS [training: 0.1746033563480909 | validation: 0.18781736694150272]
	TIME [epoch: 6.62 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18820323611367834		[learning rate: 0.00049146]
	Learning Rate: 0.000491459
	LOSS [training: 0.18820323611367834 | validation: 0.1622684089346048]
	TIME [epoch: 6.58 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16624394892788694		[learning rate: 0.0004903]
	Learning Rate: 0.0004903
	LOSS [training: 0.16624394892788694 | validation: 0.14760435428702295]
	TIME [epoch: 6.6 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16586861686645268		[learning rate: 0.00048914]
	Learning Rate: 0.000489143
	LOSS [training: 0.16586861686645268 | validation: 0.17309433953515227]
	TIME [epoch: 6.58 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.170603314333157		[learning rate: 0.00048799]
	Learning Rate: 0.000487989
	LOSS [training: 0.170603314333157 | validation: 0.14897425015677068]
	TIME [epoch: 6.57 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16432005915469922		[learning rate: 0.00048684]
	Learning Rate: 0.000486838
	LOSS [training: 0.16432005915469922 | validation: 0.14718344437165395]
	TIME [epoch: 6.61 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17039030914173037		[learning rate: 0.00048569]
	Learning Rate: 0.00048569
	LOSS [training: 0.17039030914173037 | validation: 0.17499756840685576]
	TIME [epoch: 6.64 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18432664655432127		[learning rate: 0.00048454]
	Learning Rate: 0.000484544
	LOSS [training: 0.18432664655432127 | validation: 0.15631105094322217]
	TIME [epoch: 6.58 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.176934840498998		[learning rate: 0.0004834]
	Learning Rate: 0.000483401
	LOSS [training: 0.176934840498998 | validation: 0.1655530037045494]
	TIME [epoch: 6.57 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18415983596091592		[learning rate: 0.00048226]
	Learning Rate: 0.000482261
	LOSS [training: 0.18415983596091592 | validation: 0.1837813989726499]
	TIME [epoch: 6.58 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1939603403343796		[learning rate: 0.00048112]
	Learning Rate: 0.000481123
	LOSS [training: 0.1939603403343796 | validation: 0.16149679290605212]
	TIME [epoch: 6.57 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1837761405321709		[learning rate: 0.00047999]
	Learning Rate: 0.000479988
	LOSS [training: 0.1837761405321709 | validation: 0.17378479062984714]
	TIME [epoch: 6.6 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19860756364627735		[learning rate: 0.00047886]
	Learning Rate: 0.000478856
	LOSS [training: 0.19860756364627735 | validation: 0.1538753691708865]
	TIME [epoch: 6.59 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16620259162461187		[learning rate: 0.00047773]
	Learning Rate: 0.000477727
	LOSS [training: 0.16620259162461187 | validation: 0.16671228114165146]
	TIME [epoch: 6.58 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1890976338164391		[learning rate: 0.0004766]
	Learning Rate: 0.0004766
	LOSS [training: 0.1890976338164391 | validation: 0.153794164597796]
	TIME [epoch: 6.6 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17626242876398074		[learning rate: 0.00047548]
	Learning Rate: 0.000475476
	LOSS [training: 0.17626242876398074 | validation: 0.16987347278446666]
	TIME [epoch: 6.6 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17816846943384607		[learning rate: 0.00047435]
	Learning Rate: 0.000474354
	LOSS [training: 0.17816846943384607 | validation: 0.1614905896923306]
	TIME [epoch: 6.57 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.180084814499089		[learning rate: 0.00047324]
	Learning Rate: 0.000473235
	LOSS [training: 0.180084814499089 | validation: 0.17658296623255576]
	TIME [epoch: 6.61 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17111038817868396		[learning rate: 0.00047212]
	Learning Rate: 0.000472119
	LOSS [training: 0.17111038817868396 | validation: 0.14183846417339568]
	TIME [epoch: 6.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1343.pth
	Model improved!!!
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16414236811713373		[learning rate: 0.00047101]
	Learning Rate: 0.000471005
	LOSS [training: 0.16414236811713373 | validation: 0.15542724848901823]
	TIME [epoch: 6.58 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17902176381677581		[learning rate: 0.00046989]
	Learning Rate: 0.000469894
	LOSS [training: 0.17902176381677581 | validation: 0.1677668781095393]
	TIME [epoch: 6.58 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18422155413863645		[learning rate: 0.00046879]
	Learning Rate: 0.000468786
	LOSS [training: 0.18422155413863645 | validation: 0.17575899558429375]
	TIME [epoch: 6.6 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1968168906556687		[learning rate: 0.00046768]
	Learning Rate: 0.00046768
	LOSS [training: 0.1968168906556687 | validation: 0.22005957532246712]
	TIME [epoch: 6.59 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19616391534094357		[learning rate: 0.00046658]
	Learning Rate: 0.000466577
	LOSS [training: 0.19616391534094357 | validation: 0.1641416856798693]
	TIME [epoch: 6.64 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1797072330933906		[learning rate: 0.00046548]
	Learning Rate: 0.000465476
	LOSS [training: 0.1797072330933906 | validation: 0.1633277923304563]
	TIME [epoch: 6.58 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1835990238382451		[learning rate: 0.00046438]
	Learning Rate: 0.000464378
	LOSS [training: 0.1835990238382451 | validation: 0.16761378917588618]
	TIME [epoch: 6.58 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16484069987021202		[learning rate: 0.00046328]
	Learning Rate: 0.000463283
	LOSS [training: 0.16484069987021202 | validation: 0.1590184333053254]
	TIME [epoch: 6.58 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17328511880416675		[learning rate: 0.00046219]
	Learning Rate: 0.00046219
	LOSS [training: 0.17328511880416675 | validation: 0.1576713721247315]
	TIME [epoch: 6.58 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17507161083521022		[learning rate: 0.0004611]
	Learning Rate: 0.0004611
	LOSS [training: 0.17507161083521022 | validation: 0.16514413358146765]
	TIME [epoch: 6.62 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1836948606534544		[learning rate: 0.00046001]
	Learning Rate: 0.000460012
	LOSS [training: 0.1836948606534544 | validation: 0.16412071052306973]
	TIME [epoch: 6.59 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18246825980207496		[learning rate: 0.00045893]
	Learning Rate: 0.000458927
	LOSS [training: 0.18246825980207496 | validation: 0.197412863063832]
	TIME [epoch: 6.58 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1850490840759606		[learning rate: 0.00045784]
	Learning Rate: 0.000457844
	LOSS [training: 0.1850490840759606 | validation: 0.1449525277965421]
	TIME [epoch: 6.58 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16112091131217832		[learning rate: 0.00045676]
	Learning Rate: 0.000456764
	LOSS [training: 0.16112091131217832 | validation: 0.1397283528150663]
	TIME [epoch: 6.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1357.pth
	Model improved!!!
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16429219968722472		[learning rate: 0.00045569]
	Learning Rate: 0.000455687
	LOSS [training: 0.16429219968722472 | validation: 0.1528832641046105]
	TIME [epoch: 6.58 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16380150062287377		[learning rate: 0.00045461]
	Learning Rate: 0.000454612
	LOSS [training: 0.16380150062287377 | validation: 0.14706351657420824]
	TIME [epoch: 6.63 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16289104115998604		[learning rate: 0.00045354]
	Learning Rate: 0.00045354
	LOSS [training: 0.16289104115998604 | validation: 0.1487906769840016]
	TIME [epoch: 6.6 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15877926382956606		[learning rate: 0.00045247]
	Learning Rate: 0.00045247
	LOSS [training: 0.15877926382956606 | validation: 0.15916308331215695]
	TIME [epoch: 6.58 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19904029337103676		[learning rate: 0.0004514]
	Learning Rate: 0.000451403
	LOSS [training: 0.19904029337103676 | validation: 0.16952949306170711]
	TIME [epoch: 6.58 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18072349877213173		[learning rate: 0.00045034]
	Learning Rate: 0.000450338
	LOSS [training: 0.18072349877213173 | validation: 0.1476189272565694]
	TIME [epoch: 6.58 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16378639722090366		[learning rate: 0.00044928]
	Learning Rate: 0.000449276
	LOSS [training: 0.16378639722090366 | validation: 0.16108841486558528]
	TIME [epoch: 6.58 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1846757494841868		[learning rate: 0.00044822]
	Learning Rate: 0.000448216
	LOSS [training: 0.1846757494841868 | validation: 0.1918762949992355]
	TIME [epoch: 6.61 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18265159270900472		[learning rate: 0.00044716]
	Learning Rate: 0.000447159
	LOSS [training: 0.18265159270900472 | validation: 0.1604023448308059]
	TIME [epoch: 6.58 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1713845147780791		[learning rate: 0.0004461]
	Learning Rate: 0.000446104
	LOSS [training: 0.1713845147780791 | validation: 0.1464782998374643]
	TIME [epoch: 6.58 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17560407071644674		[learning rate: 0.00044505]
	Learning Rate: 0.000445051
	LOSS [training: 0.17560407071644674 | validation: 0.1572808237701196]
	TIME [epoch: 6.6 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19255782387233006		[learning rate: 0.000444]
	Learning Rate: 0.000444002
	LOSS [training: 0.19255782387233006 | validation: 0.15815732188758114]
	TIME [epoch: 6.6 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17566487237933898		[learning rate: 0.00044295]
	Learning Rate: 0.000442954
	LOSS [training: 0.17566487237933898 | validation: 0.16626982112794125]
	TIME [epoch: 6.59 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18103991360171634		[learning rate: 0.00044191]
	Learning Rate: 0.000441909
	LOSS [training: 0.18103991360171634 | validation: 0.20169277976936784]
	TIME [epoch: 6.61 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.21598956217062143		[learning rate: 0.00044087]
	Learning Rate: 0.000440867
	LOSS [training: 0.21598956217062143 | validation: 0.1809632049377378]
	TIME [epoch: 6.58 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19512634009168936		[learning rate: 0.00043983]
	Learning Rate: 0.000439827
	LOSS [training: 0.19512634009168936 | validation: 0.15378971314823564]
	TIME [epoch: 6.6 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1727505522066556		[learning rate: 0.00043879]
	Learning Rate: 0.00043879
	LOSS [training: 0.1727505522066556 | validation: 0.15505436589098448]
	TIME [epoch: 6.57 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20830134915949633		[learning rate: 0.00043775]
	Learning Rate: 0.000437755
	LOSS [training: 0.20830134915949633 | validation: 0.19726642548014]
	TIME [epoch: 6.58 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18811392246459685		[learning rate: 0.00043672]
	Learning Rate: 0.000436722
	LOSS [training: 0.18811392246459685 | validation: 0.14753031112519768]
	TIME [epoch: 6.6 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17285425090908071		[learning rate: 0.00043569]
	Learning Rate: 0.000435692
	LOSS [training: 0.17285425090908071 | validation: 0.16228511251377073]
	TIME [epoch: 6.62 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19527828175101872		[learning rate: 0.00043466]
	Learning Rate: 0.000434664
	LOSS [training: 0.19527828175101872 | validation: 0.17656740162936502]
	TIME [epoch: 6.58 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20230055201357294		[learning rate: 0.00043364]
	Learning Rate: 0.000433639
	LOSS [training: 0.20230055201357294 | validation: 0.1775993370270212]
	TIME [epoch: 6.58 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.20351219378969834		[learning rate: 0.00043262]
	Learning Rate: 0.000432616
	LOSS [training: 0.20351219378969834 | validation: 0.16438890283838475]
	TIME [epoch: 6.6 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1792908067650422		[learning rate: 0.0004316]
	Learning Rate: 0.000431595
	LOSS [training: 0.1792908067650422 | validation: 0.15040234122829496]
	TIME [epoch: 6.59 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18080326235192035		[learning rate: 0.00043058]
	Learning Rate: 0.000430577
	LOSS [training: 0.18080326235192035 | validation: 0.17051317731897075]
	TIME [epoch: 6.62 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18462889470869243		[learning rate: 0.00042956]
	Learning Rate: 0.000429562
	LOSS [training: 0.18462889470869243 | validation: 0.1574524733356904]
	TIME [epoch: 6.59 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1770667741254725		[learning rate: 0.00042855]
	Learning Rate: 0.000428548
	LOSS [training: 0.1770667741254725 | validation: 0.16966110939683837]
	TIME [epoch: 6.57 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17328963941836867		[learning rate: 0.00042754]
	Learning Rate: 0.000427538
	LOSS [training: 0.17328963941836867 | validation: 0.15032111065474935]
	TIME [epoch: 6.58 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16280350168343766		[learning rate: 0.00042653]
	Learning Rate: 0.000426529
	LOSS [training: 0.16280350168343766 | validation: 0.15134074815397683]
	TIME [epoch: 6.58 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17057638505480882		[learning rate: 0.00042552]
	Learning Rate: 0.000425523
	LOSS [training: 0.17057638505480882 | validation: 0.1503976201566248]
	TIME [epoch: 6.58 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16597126539849555		[learning rate: 0.00042452]
	Learning Rate: 0.000424519
	LOSS [training: 0.16597126539849555 | validation: 0.14526693081055486]
	TIME [epoch: 6.64 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1873007925190428		[learning rate: 0.00042352]
	Learning Rate: 0.000423518
	LOSS [training: 0.1873007925190428 | validation: 0.1639862870121956]
	TIME [epoch: 6.58 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17532870724564248		[learning rate: 0.00042252]
	Learning Rate: 0.000422519
	LOSS [training: 0.17532870724564248 | validation: 0.17705028581807272]
	TIME [epoch: 6.6 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1848254642697346		[learning rate: 0.00042152]
	Learning Rate: 0.000421522
	LOSS [training: 0.1848254642697346 | validation: 0.17775399151479146]
	TIME [epoch: 6.59 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1876740285570099		[learning rate: 0.00042053]
	Learning Rate: 0.000420528
	LOSS [training: 0.1876740285570099 | validation: 0.15250743166628647]
	TIME [epoch: 6.6 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16965878153799765		[learning rate: 0.00041954]
	Learning Rate: 0.000419536
	LOSS [training: 0.16965878153799765 | validation: 0.1503623740354933]
	TIME [epoch: 6.61 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1666301145869819		[learning rate: 0.00041855]
	Learning Rate: 0.000418546
	LOSS [training: 0.1666301145869819 | validation: 0.14742109244275695]
	TIME [epoch: 6.62 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1572642836908204		[learning rate: 0.00041756]
	Learning Rate: 0.000417559
	LOSS [training: 0.1572642836908204 | validation: 0.1398741010540726]
	TIME [epoch: 6.6 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15824040695700964		[learning rate: 0.00041657]
	Learning Rate: 0.000416574
	LOSS [training: 0.15824040695700964 | validation: 0.15284377030498664]
	TIME [epoch: 6.6 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17414670690733042		[learning rate: 0.00041559]
	Learning Rate: 0.000415591
	LOSS [training: 0.17414670690733042 | validation: 0.17228789828455704]
	TIME [epoch: 6.6 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18976430559534407		[learning rate: 0.00041461]
	Learning Rate: 0.000414611
	LOSS [training: 0.18976430559534407 | validation: 0.19999670277025478]
	TIME [epoch: 6.6 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17810549564016304		[learning rate: 0.00041363]
	Learning Rate: 0.000413633
	LOSS [training: 0.17810549564016304 | validation: 0.14184662545918963]
	TIME [epoch: 6.63 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1616732646726407		[learning rate: 0.00041266]
	Learning Rate: 0.000412657
	LOSS [training: 0.1616732646726407 | validation: 0.15414561643418453]
	TIME [epoch: 6.61 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1723561715030292		[learning rate: 0.00041168]
	Learning Rate: 0.000411684
	LOSS [training: 0.1723561715030292 | validation: 0.15063146997816146]
	TIME [epoch: 6.6 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15931016991526958		[learning rate: 0.00041071]
	Learning Rate: 0.000410713
	LOSS [training: 0.15931016991526958 | validation: 0.1309619912731067]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1402.pth
	Model improved!!!
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1639908020415789		[learning rate: 0.00040974]
	Learning Rate: 0.000409744
	LOSS [training: 0.1639908020415789 | validation: 0.1525115324249292]
	TIME [epoch: 6.62 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16265956259241834		[learning rate: 0.00040878]
	Learning Rate: 0.000408778
	LOSS [training: 0.16265956259241834 | validation: 0.16160784376733112]
	TIME [epoch: 6.63 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18807652146837067		[learning rate: 0.00040781]
	Learning Rate: 0.000407813
	LOSS [training: 0.18807652146837067 | validation: 0.169007604633767]
	TIME [epoch: 6.66 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17940141017739158		[learning rate: 0.00040685]
	Learning Rate: 0.000406851
	LOSS [training: 0.17940141017739158 | validation: 0.1508724607171723]
	TIME [epoch: 6.62 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19622540980488884		[learning rate: 0.00040589]
	Learning Rate: 0.000405892
	LOSS [training: 0.19622540980488884 | validation: 0.20461157207184694]
	TIME [epoch: 6.62 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19633457927197298		[learning rate: 0.00040493]
	Learning Rate: 0.000404934
	LOSS [training: 0.19633457927197298 | validation: 0.1713703647226963]
	TIME [epoch: 6.62 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19172183192039777		[learning rate: 0.00040398]
	Learning Rate: 0.000403979
	LOSS [training: 0.19172183192039777 | validation: 0.15844378251811597]
	TIME [epoch: 6.62 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.167846811336486		[learning rate: 0.00040303]
	Learning Rate: 0.000403026
	LOSS [training: 0.167846811336486 | validation: 0.14644352676582206]
	TIME [epoch: 6.66 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1720716221985005		[learning rate: 0.00040208]
	Learning Rate: 0.000402076
	LOSS [training: 0.1720716221985005 | validation: 0.16795638763575166]
	TIME [epoch: 6.63 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16984008880962234		[learning rate: 0.00040113]
	Learning Rate: 0.000401127
	LOSS [training: 0.16984008880962234 | validation: 0.1486862630341726]
	TIME [epoch: 6.62 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17510462075940153		[learning rate: 0.00040018]
	Learning Rate: 0.000400181
	LOSS [training: 0.17510462075940153 | validation: 0.1587846456753424]
	TIME [epoch: 6.62 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16616880135720052		[learning rate: 0.00039924]
	Learning Rate: 0.000399237
	LOSS [training: 0.16616880135720052 | validation: 0.16301439799070266]
	TIME [epoch: 6.62 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16998717221420115		[learning rate: 0.0003983]
	Learning Rate: 0.000398295
	LOSS [training: 0.16998717221420115 | validation: 0.14652355442476783]
	TIME [epoch: 6.62 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1800048271039024		[learning rate: 0.00039736]
	Learning Rate: 0.000397356
	LOSS [training: 0.1800048271039024 | validation: 0.1750924202604423]
	TIME [epoch: 6.65 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1925852294126565		[learning rate: 0.00039642]
	Learning Rate: 0.000396418
	LOSS [training: 0.1925852294126565 | validation: 0.17769003676022357]
	TIME [epoch: 6.64 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18025070162245474		[learning rate: 0.00039548]
	Learning Rate: 0.000395483
	LOSS [training: 0.18025070162245474 | validation: 0.14786852303366305]
	TIME [epoch: 6.62 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1664408982340919		[learning rate: 0.00039455]
	Learning Rate: 0.00039455
	LOSS [training: 0.1664408982340919 | validation: 0.15263093081645818]
	TIME [epoch: 6.61 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16677730085198098		[learning rate: 0.00039362]
	Learning Rate: 0.00039362
	LOSS [training: 0.16677730085198098 | validation: 0.16490928673209695]
	TIME [epoch: 6.62 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1800278351653784		[learning rate: 0.00039269]
	Learning Rate: 0.000392691
	LOSS [training: 0.1800278351653784 | validation: 0.18047642607414344]
	TIME [epoch: 6.62 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19822440744834038		[learning rate: 0.00039177]
	Learning Rate: 0.000391765
	LOSS [training: 0.19822440744834038 | validation: 0.18951884002508307]
	TIME [epoch: 6.66 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19192087278187123		[learning rate: 0.00039084]
	Learning Rate: 0.000390841
	LOSS [training: 0.19192087278187123 | validation: 0.17514832763367633]
	TIME [epoch: 6.62 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19040434846692295		[learning rate: 0.00038992]
	Learning Rate: 0.000389919
	LOSS [training: 0.19040434846692295 | validation: 0.1690857249217725]
	TIME [epoch: 6.62 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17883807822052228		[learning rate: 0.000389]
	Learning Rate: 0.000388999
	LOSS [training: 0.17883807822052228 | validation: 0.1539104389151597]
	TIME [epoch: 6.61 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16635472152324693		[learning rate: 0.00038808]
	Learning Rate: 0.000388082
	LOSS [training: 0.16635472152324693 | validation: 0.16571450198123727]
	TIME [epoch: 6.62 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17462476568130936		[learning rate: 0.00038717]
	Learning Rate: 0.000387166
	LOSS [training: 0.17462476568130936 | validation: 0.15579943890381123]
	TIME [epoch: 6.63 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16607190328764948		[learning rate: 0.00038625]
	Learning Rate: 0.000386253
	LOSS [training: 0.16607190328764948 | validation: 0.1433810234826292]
	TIME [epoch: 6.64 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17450075808563031		[learning rate: 0.00038534]
	Learning Rate: 0.000385342
	LOSS [training: 0.17450075808563031 | validation: 0.14826399269557666]
	TIME [epoch: 6.62 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16387917401751734		[learning rate: 0.00038443]
	Learning Rate: 0.000384433
	LOSS [training: 0.16387917401751734 | validation: 0.15166029747902618]
	TIME [epoch: 6.61 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15828815244285985		[learning rate: 0.00038353]
	Learning Rate: 0.000383526
	LOSS [training: 0.15828815244285985 | validation: 0.14911002234912987]
	TIME [epoch: 6.62 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1673712953034079		[learning rate: 0.00038262]
	Learning Rate: 0.000382621
	LOSS [training: 0.1673712953034079 | validation: 0.14636206899003396]
	TIME [epoch: 6.62 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15708006099204394		[learning rate: 0.00038172]
	Learning Rate: 0.000381719
	LOSS [training: 0.15708006099204394 | validation: 0.1472248928346464]
	TIME [epoch: 6.66 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16549644144061326		[learning rate: 0.00038082]
	Learning Rate: 0.000380818
	LOSS [training: 0.16549644144061326 | validation: 0.25543225544541026]
	TIME [epoch: 6.62 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.23124111475069334		[learning rate: 0.00037992]
	Learning Rate: 0.00037992
	LOSS [training: 0.23124111475069334 | validation: 0.14887358353639818]
	TIME [epoch: 6.62 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17030118029317737		[learning rate: 0.00037902]
	Learning Rate: 0.000379024
	LOSS [training: 0.17030118029317737 | validation: 0.15872745207240652]
	TIME [epoch: 6.62 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1712160175311453		[learning rate: 0.00037813]
	Learning Rate: 0.00037813
	LOSS [training: 0.1712160175311453 | validation: 0.15404529731209493]
	TIME [epoch: 6.62 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16643759112987355		[learning rate: 0.00037724]
	Learning Rate: 0.000377238
	LOSS [training: 0.16643759112987355 | validation: 0.15271929704805942]
	TIME [epoch: 6.62 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1625203431966901		[learning rate: 0.00037635]
	Learning Rate: 0.000376348
	LOSS [training: 0.1625203431966901 | validation: 0.15722794704123544]
	TIME [epoch: 6.65 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17289947020488078		[learning rate: 0.00037546]
	Learning Rate: 0.00037546
	LOSS [training: 0.17289947020488078 | validation: 0.15586541709108687]
	TIME [epoch: 6.62 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17420331630956357		[learning rate: 0.00037457]
	Learning Rate: 0.000374575
	LOSS [training: 0.17420331630956357 | validation: 0.1734103070973383]
	TIME [epoch: 6.62 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1733224424183381		[learning rate: 0.00037369]
	Learning Rate: 0.000373691
	LOSS [training: 0.1733224424183381 | validation: 0.1705051745107613]
	TIME [epoch: 6.62 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1668903780993021		[learning rate: 0.00037281]
	Learning Rate: 0.00037281
	LOSS [training: 0.1668903780993021 | validation: 0.1692752147789584]
	TIME [epoch: 6.62 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1659171555780859		[learning rate: 0.00037193]
	Learning Rate: 0.00037193
	LOSS [training: 0.1659171555780859 | validation: 0.17065710722293143]
	TIME [epoch: 6.64 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16524513670868543		[learning rate: 0.00037105]
	Learning Rate: 0.000371053
	LOSS [training: 0.16524513670868543 | validation: 0.16488875848854118]
	TIME [epoch: 6.64 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1693368057429903		[learning rate: 0.00037018]
	Learning Rate: 0.000370178
	LOSS [training: 0.1693368057429903 | validation: 0.1593123445346511]
	TIME [epoch: 6.61 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16849754846486895		[learning rate: 0.0003693]
	Learning Rate: 0.000369305
	LOSS [training: 0.16849754846486895 | validation: 0.18302077712549564]
	TIME [epoch: 6.62 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18177994060208		[learning rate: 0.00036843]
	Learning Rate: 0.000368433
	LOSS [training: 0.18177994060208 | validation: 0.16481843464358475]
	TIME [epoch: 6.61 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17620023608939084		[learning rate: 0.00036756]
	Learning Rate: 0.000367564
	LOSS [training: 0.17620023608939084 | validation: 0.15828022357704985]
	TIME [epoch: 6.61 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1772558922465981		[learning rate: 0.0003667]
	Learning Rate: 0.000366697
	LOSS [training: 0.1772558922465981 | validation: 0.14271420430325749]
	TIME [epoch: 6.66 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16363687520958417		[learning rate: 0.00036583]
	Learning Rate: 0.000365832
	LOSS [training: 0.16363687520958417 | validation: 0.1616636629656799]
	TIME [epoch: 6.62 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16420379063082607		[learning rate: 0.00036497]
	Learning Rate: 0.000364969
	LOSS [training: 0.16420379063082607 | validation: 0.16566106594194818]
	TIME [epoch: 6.62 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16594276100274458		[learning rate: 0.00036411]
	Learning Rate: 0.000364108
	LOSS [training: 0.16594276100274458 | validation: 0.1629777855110619]
	TIME [epoch: 6.61 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1746009418656606		[learning rate: 0.00036325]
	Learning Rate: 0.00036325
	LOSS [training: 0.1746009418656606 | validation: 0.16332396834817037]
	TIME [epoch: 6.61 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17557252954143787		[learning rate: 0.00036239]
	Learning Rate: 0.000362393
	LOSS [training: 0.17557252954143787 | validation: 0.16200393975985192]
	TIME [epoch: 6.63 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1743079525907774		[learning rate: 0.00036154]
	Learning Rate: 0.000361538
	LOSS [training: 0.1743079525907774 | validation: 0.16754465782189326]
	TIME [epoch: 6.65 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1760498989512441		[learning rate: 0.00036069]
	Learning Rate: 0.000360685
	LOSS [training: 0.1760498989512441 | validation: 0.19255597367147392]
	TIME [epoch: 6.61 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1779768863854505		[learning rate: 0.00035983]
	Learning Rate: 0.000359834
	LOSS [training: 0.1779768863854505 | validation: 0.17815736649036795]
	TIME [epoch: 6.61 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17498775126860894		[learning rate: 0.00035899]
	Learning Rate: 0.000358986
	LOSS [training: 0.17498775126860894 | validation: 0.18216824651651897]
	TIME [epoch: 6.61 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17674003367536034		[learning rate: 0.00035814]
	Learning Rate: 0.000358139
	LOSS [training: 0.17674003367536034 | validation: 0.16546090641336186]
	TIME [epoch: 6.61 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17718657255752007		[learning rate: 0.00035729]
	Learning Rate: 0.000357294
	LOSS [training: 0.17718657255752007 | validation: 0.17431346577063733]
	TIME [epoch: 6.65 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1731383383405773		[learning rate: 0.00035645]
	Learning Rate: 0.000356451
	LOSS [training: 0.1731383383405773 | validation: 0.16367274252584219]
	TIME [epoch: 6.63 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17833388125153146		[learning rate: 0.00035561]
	Learning Rate: 0.00035561
	LOSS [training: 0.17833388125153146 | validation: 0.18117927023249542]
	TIME [epoch: 6.61 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19648398102117148		[learning rate: 0.00035477]
	Learning Rate: 0.000354771
	LOSS [training: 0.19648398102117148 | validation: 0.24291348067725374]
	TIME [epoch: 6.62 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.2028219179019616		[learning rate: 0.00035393]
	Learning Rate: 0.000353935
	LOSS [training: 0.2028219179019616 | validation: 0.19950679778488156]
	TIME [epoch: 6.62 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19520855758408234		[learning rate: 0.0003531]
	Learning Rate: 0.0003531
	LOSS [training: 0.19520855758408234 | validation: 0.1899582364846943]
	TIME [epoch: 6.63 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1920505474638806		[learning rate: 0.00035227]
	Learning Rate: 0.000352267
	LOSS [training: 0.1920505474638806 | validation: 0.2004559805315646]
	TIME [epoch: 6.66 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19344325941361837		[learning rate: 0.00035144]
	Learning Rate: 0.000351436
	LOSS [training: 0.19344325941361837 | validation: 0.1976848511137423]
	TIME [epoch: 6.62 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18550662163372922		[learning rate: 0.00035061]
	Learning Rate: 0.000350607
	LOSS [training: 0.18550662163372922 | validation: 0.16835443777334202]
	TIME [epoch: 6.62 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18361384826744126		[learning rate: 0.00034978]
	Learning Rate: 0.00034978
	LOSS [training: 0.18361384826744126 | validation: 0.21243859532672957]
	TIME [epoch: 6.62 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1884222455354444		[learning rate: 0.00034895]
	Learning Rate: 0.000348955
	LOSS [training: 0.1884222455354444 | validation: 0.18803499232626267]
	TIME [epoch: 6.62 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18807718781447685		[learning rate: 0.00034813]
	Learning Rate: 0.000348132
	LOSS [training: 0.18807718781447685 | validation: 0.18172488334113035]
	TIME [epoch: 6.65 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18603892409432105		[learning rate: 0.00034731]
	Learning Rate: 0.000347311
	LOSS [training: 0.18603892409432105 | validation: 0.18040992321580573]
	TIME [epoch: 6.64 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1888325771509146		[learning rate: 0.00034649]
	Learning Rate: 0.000346491
	LOSS [training: 0.1888325771509146 | validation: 0.16982747717048718]
	TIME [epoch: 6.62 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17822805006741618		[learning rate: 0.00034567]
	Learning Rate: 0.000345674
	LOSS [training: 0.17822805006741618 | validation: 0.17035698894915083]
	TIME [epoch: 6.62 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18889020948593566		[learning rate: 0.00034486]
	Learning Rate: 0.000344859
	LOSS [training: 0.18889020948593566 | validation: 0.18777247948258477]
	TIME [epoch: 6.62 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1866821575155194		[learning rate: 0.00034405]
	Learning Rate: 0.000344045
	LOSS [training: 0.1866821575155194 | validation: 0.16510348588144538]
	TIME [epoch: 6.62 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18646192630184866		[learning rate: 0.00034323]
	Learning Rate: 0.000343233
	LOSS [training: 0.18646192630184866 | validation: 0.17636492604416504]
	TIME [epoch: 6.66 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17243866126943438		[learning rate: 0.00034242]
	Learning Rate: 0.000342424
	LOSS [training: 0.17243866126943438 | validation: 0.15060913441752588]
	TIME [epoch: 6.62 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17354923166293668		[learning rate: 0.00034162]
	Learning Rate: 0.000341616
	LOSS [training: 0.17354923166293668 | validation: 0.1609797571789108]
	TIME [epoch: 6.62 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18841160371482718		[learning rate: 0.00034081]
	Learning Rate: 0.00034081
	LOSS [training: 0.18841160371482718 | validation: 0.19726994580226348]
	TIME [epoch: 6.62 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19555503445580497		[learning rate: 0.00034001]
	Learning Rate: 0.000340006
	LOSS [training: 0.19555503445580497 | validation: 0.16409505348412506]
	TIME [epoch: 6.62 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17255423242835286		[learning rate: 0.0003392]
	Learning Rate: 0.000339204
	LOSS [training: 0.17255423242835286 | validation: 0.17643025721683336]
	TIME [epoch: 6.63 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17736252746616138		[learning rate: 0.0003384]
	Learning Rate: 0.000338404
	LOSS [training: 0.17736252746616138 | validation: 0.17677891540936166]
	TIME [epoch: 6.66 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17775224786093205		[learning rate: 0.00033761]
	Learning Rate: 0.000337606
	LOSS [training: 0.17775224786093205 | validation: 0.1863814466800709]
	TIME [epoch: 6.63 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1872169931855343		[learning rate: 0.00033681]
	Learning Rate: 0.00033681
	LOSS [training: 0.1872169931855343 | validation: 0.17639958481112233]
	TIME [epoch: 6.62 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.19369632544993967		[learning rate: 0.00033602]
	Learning Rate: 0.000336015
	LOSS [training: 0.19369632544993967 | validation: 0.172890888963609]
	TIME [epoch: 6.62 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1883130364170317		[learning rate: 0.00033522]
	Learning Rate: 0.000335223
	LOSS [training: 0.1883130364170317 | validation: 0.17575642205954903]
	TIME [epoch: 6.62 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17792640668402634		[learning rate: 0.00033443]
	Learning Rate: 0.000334432
	LOSS [training: 0.17792640668402634 | validation: 0.18085799728804586]
	TIME [epoch: 6.66 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17792755105525696		[learning rate: 0.00033364]
	Learning Rate: 0.000333643
	LOSS [training: 0.17792755105525696 | validation: 0.18091393627707092]
	TIME [epoch: 6.63 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1876054609893156		[learning rate: 0.00033286]
	Learning Rate: 0.000332856
	LOSS [training: 0.1876054609893156 | validation: 0.1754846339173394]
	TIME [epoch: 6.62 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18515393859081503		[learning rate: 0.00033207]
	Learning Rate: 0.000332071
	LOSS [training: 0.18515393859081503 | validation: 0.18716949117847492]
	TIME [epoch: 6.62 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1876114637344589		[learning rate: 0.00033129]
	Learning Rate: 0.000331288
	LOSS [training: 0.1876114637344589 | validation: 0.16282481436527094]
	TIME [epoch: 6.62 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1642036445136555		[learning rate: 0.00033051]
	Learning Rate: 0.000330506
	LOSS [training: 0.1642036445136555 | validation: 0.15263913217530128]
	TIME [epoch: 6.62 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15960165653628952		[learning rate: 0.00032973]
	Learning Rate: 0.000329726
	LOSS [training: 0.15960165653628952 | validation: 0.1569289264366993]
	TIME [epoch: 6.66 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17250492121815048		[learning rate: 0.00032895]
	Learning Rate: 0.000328949
	LOSS [training: 0.17250492121815048 | validation: 0.16552439880123437]
	TIME [epoch: 6.62 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16624264807284306		[learning rate: 0.00032817]
	Learning Rate: 0.000328173
	LOSS [training: 0.16624264807284306 | validation: 0.15874227672341634]
	TIME [epoch: 6.62 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16023674769974758		[learning rate: 0.0003274]
	Learning Rate: 0.000327399
	LOSS [training: 0.16023674769974758 | validation: 0.14441489535361424]
	TIME [epoch: 6.61 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18165839218670646		[learning rate: 0.00032663]
	Learning Rate: 0.000326626
	LOSS [training: 0.18165839218670646 | validation: 0.17757176408996295]
	TIME [epoch: 6.62 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1737964344279257		[learning rate: 0.00032586]
	Learning Rate: 0.000325856
	LOSS [training: 0.1737964344279257 | validation: 0.1631597979351161]
	TIME [epoch: 6.64 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18085008965930865		[learning rate: 0.00032509]
	Learning Rate: 0.000325087
	LOSS [training: 0.18085008965930865 | validation: 0.17090808531826868]
	TIME [epoch: 6.63 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1929032445100561		[learning rate: 0.00032432]
	Learning Rate: 0.00032432
	LOSS [training: 0.1929032445100561 | validation: 0.18761620067794677]
	TIME [epoch: 6.61 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17604526461698908		[learning rate: 0.00032356]
	Learning Rate: 0.000323555
	LOSS [training: 0.17604526461698908 | validation: 0.16881561753744734]
	TIME [epoch: 6.61 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16743683044962995		[learning rate: 0.00032279]
	Learning Rate: 0.000322792
	LOSS [training: 0.16743683044962995 | validation: 0.1788331971157623]
	TIME [epoch: 6.61 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1881917007973182		[learning rate: 0.00032203]
	Learning Rate: 0.000322031
	LOSS [training: 0.1881917007973182 | validation: 0.16446924605869373]
	TIME [epoch: 6.61 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16469913684684645		[learning rate: 0.00032127]
	Learning Rate: 0.000321271
	LOSS [training: 0.16469913684684645 | validation: 0.16390227191010165]
	TIME [epoch: 6.66 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16947591471618637		[learning rate: 0.00032051]
	Learning Rate: 0.000320513
	LOSS [training: 0.16947591471618637 | validation: 0.15366610174801193]
	TIME [epoch: 6.62 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15520029649252504		[learning rate: 0.00031976]
	Learning Rate: 0.000319757
	LOSS [training: 0.15520029649252504 | validation: 0.1598764160810096]
	TIME [epoch: 6.61 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16858421943948695		[learning rate: 0.000319]
	Learning Rate: 0.000319003
	LOSS [training: 0.16858421943948695 | validation: 0.1705951546664466]
	TIME [epoch: 6.61 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1715641845104367		[learning rate: 0.00031825]
	Learning Rate: 0.000318251
	LOSS [training: 0.1715641845104367 | validation: 0.17034567356660663]
	TIME [epoch: 6.61 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1736210506417049		[learning rate: 0.0003175]
	Learning Rate: 0.0003175
	LOSS [training: 0.1736210506417049 | validation: 0.15837215738849048]
	TIME [epoch: 6.63 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15801185976683546		[learning rate: 0.00031675]
	Learning Rate: 0.000316751
	LOSS [training: 0.15801185976683546 | validation: 0.15634714148322204]
	TIME [epoch: 6.65 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15892178699684728		[learning rate: 0.000316]
	Learning Rate: 0.000316004
	LOSS [training: 0.15892178699684728 | validation: 0.15927313497942117]
	TIME [epoch: 6.62 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1596821138745271		[learning rate: 0.00031526]
	Learning Rate: 0.000315258
	LOSS [training: 0.1596821138745271 | validation: 0.16250442823155048]
	TIME [epoch: 6.61 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16270527438597363		[learning rate: 0.00031451]
	Learning Rate: 0.000314515
	LOSS [training: 0.16270527438597363 | validation: 0.15500363742447637]
	TIME [epoch: 6.62 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15822249417230133		[learning rate: 0.00031377]
	Learning Rate: 0.000313773
	LOSS [training: 0.15822249417230133 | validation: 0.1734637869360704]
	TIME [epoch: 6.61 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17908095078930159		[learning rate: 0.00031303]
	Learning Rate: 0.000313033
	LOSS [training: 0.17908095078930159 | validation: 0.1661487698565633]
	TIME [epoch: 6.65 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1613159290589629		[learning rate: 0.00031229]
	Learning Rate: 0.000312294
	LOSS [training: 0.1613159290589629 | validation: 0.14584302340126937]
	TIME [epoch: 6.62 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16093046301968783		[learning rate: 0.00031156]
	Learning Rate: 0.000311558
	LOSS [training: 0.16093046301968783 | validation: 0.15671437367565688]
	TIME [epoch: 6.62 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16797545122853136		[learning rate: 0.00031082]
	Learning Rate: 0.000310823
	LOSS [training: 0.16797545122853136 | validation: 0.17883900958239907]
	TIME [epoch: 6.61 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1780090611978077		[learning rate: 0.00031009]
	Learning Rate: 0.00031009
	LOSS [training: 0.1780090611978077 | validation: 0.19297049671517535]
	TIME [epoch: 6.61 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18234512539348757		[learning rate: 0.00030936]
	Learning Rate: 0.000309358
	LOSS [training: 0.18234512539348757 | validation: 0.18550388400798856]
	TIME [epoch: 6.62 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18373338094932995		[learning rate: 0.00030863]
	Learning Rate: 0.000308628
	LOSS [training: 0.18373338094932995 | validation: 0.1745999463140197]
	TIME [epoch: 6.65 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.18370468642000656		[learning rate: 0.0003079]
	Learning Rate: 0.0003079
	LOSS [training: 0.18370468642000656 | validation: 0.19537397969373527]
	TIME [epoch: 6.62 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1783684401476499		[learning rate: 0.00030717]
	Learning Rate: 0.000307174
	LOSS [training: 0.1783684401476499 | validation: 0.17493721206609486]
	TIME [epoch: 6.62 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1746622195150716		[learning rate: 0.00030645]
	Learning Rate: 0.00030645
	LOSS [training: 0.1746622195150716 | validation: 0.16824179267529032]
	TIME [epoch: 6.62 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17219428584880908		[learning rate: 0.00030573]
	Learning Rate: 0.000305727
	LOSS [training: 0.17219428584880908 | validation: 0.17171151476597185]
	TIME [epoch: 6.62 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17099954592528224		[learning rate: 0.00030501]
	Learning Rate: 0.000305005
	LOSS [training: 0.17099954592528224 | validation: 0.17002719606472255]
	TIME [epoch: 6.65 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16153706962300135		[learning rate: 0.00030429]
	Learning Rate: 0.000304286
	LOSS [training: 0.16153706962300135 | validation: 0.1632604448896057]
	TIME [epoch: 6.64 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16584851974668108		[learning rate: 0.00030357]
	Learning Rate: 0.000303568
	LOSS [training: 0.16584851974668108 | validation: 0.17082714532333504]
	TIME [epoch: 6.62 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1653999114755666		[learning rate: 0.00030285]
	Learning Rate: 0.000302852
	LOSS [training: 0.1653999114755666 | validation: 0.15502977642619276]
	TIME [epoch: 6.61 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16386483203741312		[learning rate: 0.00030214]
	Learning Rate: 0.000302138
	LOSS [training: 0.16386483203741312 | validation: 0.1646202547734446]
	TIME [epoch: 6.62 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1649418521447923		[learning rate: 0.00030143]
	Learning Rate: 0.000301425
	LOSS [training: 0.1649418521447923 | validation: 0.16249481941566632]
	TIME [epoch: 6.62 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17221690004649925		[learning rate: 0.00030071]
	Learning Rate: 0.000300714
	LOSS [training: 0.17221690004649925 | validation: 0.1606515278018097]
	TIME [epoch: 6.66 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16087656241023912		[learning rate: 0.0003]
	Learning Rate: 0.000300005
	LOSS [training: 0.16087656241023912 | validation: 0.14510513166462638]
	TIME [epoch: 6.62 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15480443204648378		[learning rate: 0.0002993]
	Learning Rate: 0.000299297
	LOSS [training: 0.15480443204648378 | validation: 0.15736487585951808]
	TIME [epoch: 6.62 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16580931697389506		[learning rate: 0.00029859]
	Learning Rate: 0.000298591
	LOSS [training: 0.16580931697389506 | validation: 0.16791126204995455]
	TIME [epoch: 6.62 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16225364970075196		[learning rate: 0.00029789]
	Learning Rate: 0.000297887
	LOSS [training: 0.16225364970075196 | validation: 0.13547293601496804]
	TIME [epoch: 6.61 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14942043210287315		[learning rate: 0.00029718]
	Learning Rate: 0.000297184
	LOSS [training: 0.14942043210287315 | validation: 0.14285298488873147]
	TIME [epoch: 6.63 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15327042101048596		[learning rate: 0.00029648]
	Learning Rate: 0.000296483
	LOSS [training: 0.15327042101048596 | validation: 0.152740401965281]
	TIME [epoch: 6.65 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1647991837914824		[learning rate: 0.00029578]
	Learning Rate: 0.000295784
	LOSS [training: 0.1647991837914824 | validation: 0.14934509034958138]
	TIME [epoch: 6.61 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1518320278453864		[learning rate: 0.00029509]
	Learning Rate: 0.000295086
	LOSS [training: 0.1518320278453864 | validation: 0.1469825173710307]
	TIME [epoch: 6.61 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.151080389504944		[learning rate: 0.00029439]
	Learning Rate: 0.00029439
	LOSS [training: 0.151080389504944 | validation: 0.151814453918581]
	TIME [epoch: 6.61 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16004880774730434		[learning rate: 0.0002937]
	Learning Rate: 0.000293696
	LOSS [training: 0.16004880774730434 | validation: 0.1747768161604528]
	TIME [epoch: 6.61 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17275338213478836		[learning rate: 0.000293]
	Learning Rate: 0.000293003
	LOSS [training: 0.17275338213478836 | validation: 0.15090765530588346]
	TIME [epoch: 6.65 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16252544880054165		[learning rate: 0.00029231]
	Learning Rate: 0.000292312
	LOSS [training: 0.16252544880054165 | validation: 0.1700191807157061]
	TIME [epoch: 6.62 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16315080182546512		[learning rate: 0.00029162]
	Learning Rate: 0.000291622
	LOSS [training: 0.16315080182546512 | validation: 0.15224292945345919]
	TIME [epoch: 6.61 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14953031120328036		[learning rate: 0.00029093]
	Learning Rate: 0.000290934
	LOSS [training: 0.14953031120328036 | validation: 0.1551258560249476]
	TIME [epoch: 6.62 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15960177668326994		[learning rate: 0.00029025]
	Learning Rate: 0.000290248
	LOSS [training: 0.15960177668326994 | validation: 0.15751787706737597]
	TIME [epoch: 6.61 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1607248432964626		[learning rate: 0.00028956]
	Learning Rate: 0.000289563
	LOSS [training: 0.1607248432964626 | validation: 0.15756466777353278]
	TIME [epoch: 6.62 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16018437590434093		[learning rate: 0.00028888]
	Learning Rate: 0.00028888
	LOSS [training: 0.16018437590434093 | validation: 0.15470525439704697]
	TIME [epoch: 6.65 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15333767388983374		[learning rate: 0.0002882]
	Learning Rate: 0.000288199
	LOSS [training: 0.15333767388983374 | validation: 0.1501672423192139]
	TIME [epoch: 6.62 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15328083937924925		[learning rate: 0.00028752]
	Learning Rate: 0.000287519
	LOSS [training: 0.15328083937924925 | validation: 0.14307033090105756]
	TIME [epoch: 6.62 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15639241033515633		[learning rate: 0.00028684]
	Learning Rate: 0.000286841
	LOSS [training: 0.15639241033515633 | validation: 0.14912073131685394]
	TIME [epoch: 6.62 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16928560107745258		[learning rate: 0.00028616]
	Learning Rate: 0.000286164
	LOSS [training: 0.16928560107745258 | validation: 0.16046305843611508]
	TIME [epoch: 6.62 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17010580938880315		[learning rate: 0.00028549]
	Learning Rate: 0.000285489
	LOSS [training: 0.17010580938880315 | validation: 0.1526905570548088]
	TIME [epoch: 6.65 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15501170856193214		[learning rate: 0.00028482]
	Learning Rate: 0.000284816
	LOSS [training: 0.15501170856193214 | validation: 0.16815462999473946]
	TIME [epoch: 6.65 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16170282608140565		[learning rate: 0.00028414]
	Learning Rate: 0.000284144
	LOSS [training: 0.16170282608140565 | validation: 0.16792015461127777]
	TIME [epoch: 6.62 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.168267988463963		[learning rate: 0.00028347]
	Learning Rate: 0.000283474
	LOSS [training: 0.168267988463963 | validation: 0.17025267801345748]
	TIME [epoch: 6.62 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16126493538485814		[learning rate: 0.00028281]
	Learning Rate: 0.000282805
	LOSS [training: 0.16126493538485814 | validation: 0.16272347889096855]
	TIME [epoch: 6.62 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.170226767029145		[learning rate: 0.00028214]
	Learning Rate: 0.000282138
	LOSS [training: 0.170226767029145 | validation: 0.16424045929526826]
	TIME [epoch: 6.62 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1658484577699487		[learning rate: 0.00028147]
	Learning Rate: 0.000281472
	LOSS [training: 0.1658484577699487 | validation: 0.1518465989575411]
	TIME [epoch: 6.66 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1592795528498099		[learning rate: 0.00028081]
	Learning Rate: 0.000280808
	LOSS [training: 0.1592795528498099 | validation: 0.1504181699359815]
	TIME [epoch: 6.63 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14873154637296615		[learning rate: 0.00028015]
	Learning Rate: 0.000280146
	LOSS [training: 0.14873154637296615 | validation: 0.1533643821247591]
	TIME [epoch: 6.62 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15496947330661667		[learning rate: 0.00027949]
	Learning Rate: 0.000279485
	LOSS [training: 0.15496947330661667 | validation: 0.1487687653928001]
	TIME [epoch: 6.62 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1530505188665881		[learning rate: 0.00027883]
	Learning Rate: 0.000278826
	LOSS [training: 0.1530505188665881 | validation: 0.14602794342314052]
	TIME [epoch: 6.62 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15199496820325326		[learning rate: 0.00027817]
	Learning Rate: 0.000278168
	LOSS [training: 0.15199496820325326 | validation: 0.14676810778253419]
	TIME [epoch: 6.63 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15283662842795526		[learning rate: 0.00027751]
	Learning Rate: 0.000277512
	LOSS [training: 0.15283662842795526 | validation: 0.13750832442583105]
	TIME [epoch: 6.65 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15605736498939443		[learning rate: 0.00027686]
	Learning Rate: 0.000276858
	LOSS [training: 0.15605736498939443 | validation: 0.14372608430908618]
	TIME [epoch: 6.62 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17729463278586785		[learning rate: 0.0002762]
	Learning Rate: 0.000276204
	LOSS [training: 0.17729463278586785 | validation: 0.1655004745124538]
	TIME [epoch: 6.62 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15473743669403578		[learning rate: 0.00027555]
	Learning Rate: 0.000275553
	LOSS [training: 0.15473743669403578 | validation: 0.1426532799794818]
	TIME [epoch: 6.62 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16527921110526417		[learning rate: 0.0002749]
	Learning Rate: 0.000274903
	LOSS [training: 0.16527921110526417 | validation: 0.15442234551983622]
	TIME [epoch: 6.62 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16121336385986662		[learning rate: 0.00027425]
	Learning Rate: 0.000274255
	LOSS [training: 0.16121336385986662 | validation: 0.1480234835277281]
	TIME [epoch: 6.65 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15800385067739095		[learning rate: 0.00027361]
	Learning Rate: 0.000273608
	LOSS [training: 0.15800385067739095 | validation: 0.13989135812959952]
	TIME [epoch: 6.64 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1623482953875976		[learning rate: 0.00027296]
	Learning Rate: 0.000272962
	LOSS [training: 0.1623482953875976 | validation: 0.1414604758070283]
	TIME [epoch: 6.62 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15748642813237335		[learning rate: 0.00027232]
	Learning Rate: 0.000272318
	LOSS [training: 0.15748642813237335 | validation: 0.16005420695295425]
	TIME [epoch: 6.62 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.156252092585488		[learning rate: 0.00027168]
	Learning Rate: 0.000271676
	LOSS [training: 0.156252092585488 | validation: 0.13979029286211597]
	TIME [epoch: 6.62 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15775136563173983		[learning rate: 0.00027104]
	Learning Rate: 0.000271035
	LOSS [training: 0.15775136563173983 | validation: 0.15336267702853912]
	TIME [epoch: 6.63 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1690926856909062		[learning rate: 0.0002704]
	Learning Rate: 0.000270396
	LOSS [training: 0.1690926856909062 | validation: 0.15055898554129027]
	TIME [epoch: 6.66 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16388163296151054		[learning rate: 0.00026976]
	Learning Rate: 0.000269758
	LOSS [training: 0.16388163296151054 | validation: 0.15730892317225076]
	TIME [epoch: 6.63 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1564475444496288		[learning rate: 0.00026912]
	Learning Rate: 0.000269122
	LOSS [training: 0.1564475444496288 | validation: 0.14208296815523086]
	TIME [epoch: 6.63 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14948585689871974		[learning rate: 0.00026849]
	Learning Rate: 0.000268487
	LOSS [training: 0.14948585689871974 | validation: 0.14783196012964223]
	TIME [epoch: 6.62 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15393470702916007		[learning rate: 0.00026785]
	Learning Rate: 0.000267854
	LOSS [training: 0.15393470702916007 | validation: 0.14218094404279866]
	TIME [epoch: 6.62 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16847160682406837		[learning rate: 0.00026722]
	Learning Rate: 0.000267222
	LOSS [training: 0.16847160682406837 | validation: 0.1714224283444416]
	TIME [epoch: 6.64 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15362396295134362		[learning rate: 0.00026659]
	Learning Rate: 0.000266591
	LOSS [training: 0.15362396295134362 | validation: 0.13890739538181887]
	TIME [epoch: 6.65 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15348653815336716		[learning rate: 0.00026596]
	Learning Rate: 0.000265963
	LOSS [training: 0.15348653815336716 | validation: 0.15740488598412472]
	TIME [epoch: 6.62 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15834382269834996		[learning rate: 0.00026534]
	Learning Rate: 0.000265335
	LOSS [training: 0.15834382269834996 | validation: 0.14287007123475332]
	TIME [epoch: 6.63 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15635173809641445		[learning rate: 0.00026471]
	Learning Rate: 0.000264709
	LOSS [training: 0.15635173809641445 | validation: 0.14009283344190782]
	TIME [epoch: 6.63 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15955953683503665		[learning rate: 0.00026408]
	Learning Rate: 0.000264085
	LOSS [training: 0.15955953683503665 | validation: 0.13826265730762624]
	TIME [epoch: 6.63 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15132518431955352		[learning rate: 0.00026346]
	Learning Rate: 0.000263462
	LOSS [training: 0.15132518431955352 | validation: 0.14355326286262274]
	TIME [epoch: 6.66 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1540084788436109		[learning rate: 0.00026284]
	Learning Rate: 0.00026284
	LOSS [training: 0.1540084788436109 | validation: 0.14207708894456506]
	TIME [epoch: 6.63 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1575225501555699		[learning rate: 0.00026222]
	Learning Rate: 0.00026222
	LOSS [training: 0.1575225501555699 | validation: 0.1542666639384559]
	TIME [epoch: 6.63 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15660171886663363		[learning rate: 0.0002616]
	Learning Rate: 0.000261602
	LOSS [training: 0.15660171886663363 | validation: 0.13862361053399014]
	TIME [epoch: 6.62 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1605002621122308		[learning rate: 0.00026098]
	Learning Rate: 0.000260985
	LOSS [training: 0.1605002621122308 | validation: 0.14298323436899105]
	TIME [epoch: 6.62 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15838321052862323		[learning rate: 0.00026037]
	Learning Rate: 0.000260369
	LOSS [training: 0.15838321052862323 | validation: 0.131263063091863]
	TIME [epoch: 6.64 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15033132297489143		[learning rate: 0.00025976]
	Learning Rate: 0.000259755
	LOSS [training: 0.15033132297489143 | validation: 0.13815142705338856]
	TIME [epoch: 6.66 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15108150628803763		[learning rate: 0.00025914]
	Learning Rate: 0.000259142
	LOSS [training: 0.15108150628803763 | validation: 0.147110542733409]
	TIME [epoch: 6.63 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1564132292019544		[learning rate: 0.00025853]
	Learning Rate: 0.000258531
	LOSS [training: 0.1564132292019544 | validation: 0.1513591333119476]
	TIME [epoch: 6.63 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15287340335104335		[learning rate: 0.00025792]
	Learning Rate: 0.000257921
	LOSS [training: 0.15287340335104335 | validation: 0.1363676869709387]
	TIME [epoch: 6.62 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15445860649598392		[learning rate: 0.00025731]
	Learning Rate: 0.000257313
	LOSS [training: 0.15445860649598392 | validation: 0.1519594901555392]
	TIME [epoch: 6.63 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15662753949526012		[learning rate: 0.00025671]
	Learning Rate: 0.000256706
	LOSS [training: 0.15662753949526012 | validation: 0.15018255172307565]
	TIME [epoch: 6.65 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16118834400389956		[learning rate: 0.0002561]
	Learning Rate: 0.0002561
	LOSS [training: 0.16118834400389956 | validation: 0.15944573561849532]
	TIME [epoch: 6.65 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15514472956579015		[learning rate: 0.0002555]
	Learning Rate: 0.000255496
	LOSS [training: 0.15514472956579015 | validation: 0.15755765371195773]
	TIME [epoch: 6.62 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15278221382675083		[learning rate: 0.00025489]
	Learning Rate: 0.000254894
	LOSS [training: 0.15278221382675083 | validation: 0.14639695190295193]
	TIME [epoch: 6.62 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1528675847690407		[learning rate: 0.00025429]
	Learning Rate: 0.000254292
	LOSS [training: 0.1528675847690407 | validation: 0.13287004670420482]
	TIME [epoch: 6.63 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15109942574765606		[learning rate: 0.00025369]
	Learning Rate: 0.000253693
	LOSS [training: 0.15109942574765606 | validation: 0.14254926719428798]
	TIME [epoch: 6.63 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15161158952050416		[learning rate: 0.00025309]
	Learning Rate: 0.000253094
	LOSS [training: 0.15161158952050416 | validation: 0.14356345434272216]
	TIME [epoch: 6.66 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14620547742921708		[learning rate: 0.0002525]
	Learning Rate: 0.000252497
	LOSS [training: 0.14620547742921708 | validation: 0.1498562256160285]
	TIME [epoch: 6.62 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15270830434260507		[learning rate: 0.0002519]
	Learning Rate: 0.000251902
	LOSS [training: 0.15270830434260507 | validation: 0.137004418021605]
	TIME [epoch: 6.62 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15466113898804515		[learning rate: 0.00025131]
	Learning Rate: 0.000251307
	LOSS [training: 0.15466113898804515 | validation: 0.15134407845529307]
	TIME [epoch: 6.62 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1620515203871437		[learning rate: 0.00025071]
	Learning Rate: 0.000250714
	LOSS [training: 0.1620515203871437 | validation: 0.15406243622039578]
	TIME [epoch: 6.62 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15889873386505843		[learning rate: 0.00025012]
	Learning Rate: 0.000250123
	LOSS [training: 0.15889873386505843 | validation: 0.15053479089778052]
	TIME [epoch: 6.65 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.156400125216934		[learning rate: 0.00024953]
	Learning Rate: 0.000249533
	LOSS [training: 0.156400125216934 | validation: 0.1461481703435315]
	TIME [epoch: 6.65 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15195789319119002		[learning rate: 0.00024894]
	Learning Rate: 0.000248945
	LOSS [training: 0.15195789319119002 | validation: 0.15047132514658984]
	TIME [epoch: 6.62 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1620616718753613		[learning rate: 0.00024836]
	Learning Rate: 0.000248357
	LOSS [training: 0.1620616718753613 | validation: 0.16195993416764454]
	TIME [epoch: 6.63 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16241428916333672		[learning rate: 0.00024777]
	Learning Rate: 0.000247771
	LOSS [training: 0.16241428916333672 | validation: 0.1598238495633708]
	TIME [epoch: 6.63 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15593949985165662		[learning rate: 0.00024719]
	Learning Rate: 0.000247187
	LOSS [training: 0.15593949985165662 | validation: 0.16305666384458195]
	TIME [epoch: 6.63 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15819274124184873		[learning rate: 0.0002466]
	Learning Rate: 0.000246604
	LOSS [training: 0.15819274124184873 | validation: 0.15060612810106694]
	TIME [epoch: 6.66 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15741770272565592		[learning rate: 0.00024602]
	Learning Rate: 0.000246022
	LOSS [training: 0.15741770272565592 | validation: 0.16401245372075263]
	TIME [epoch: 6.64 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16254305507118397		[learning rate: 0.00024544]
	Learning Rate: 0.000245442
	LOSS [training: 0.16254305507118397 | validation: 0.15135943128459245]
	TIME [epoch: 6.63 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1617827890054517		[learning rate: 0.00024486]
	Learning Rate: 0.000244863
	LOSS [training: 0.1617827890054517 | validation: 0.1476639694910364]
	TIME [epoch: 6.63 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15524006295695533		[learning rate: 0.00024429]
	Learning Rate: 0.000244285
	LOSS [training: 0.15524006295695533 | validation: 0.1425067841039003]
	TIME [epoch: 6.63 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15521489004404748		[learning rate: 0.00024371]
	Learning Rate: 0.000243709
	LOSS [training: 0.15521489004404748 | validation: 0.1519294044867746]
	TIME [epoch: 6.64 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1656297296197565		[learning rate: 0.00024313]
	Learning Rate: 0.000243134
	LOSS [training: 0.1656297296197565 | validation: 0.1579497361061798]
	TIME [epoch: 6.66 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1581835574158191		[learning rate: 0.00024256]
	Learning Rate: 0.000242561
	LOSS [training: 0.1581835574158191 | validation: 0.1513199396427747]
	TIME [epoch: 6.62 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15317081138911076		[learning rate: 0.00024199]
	Learning Rate: 0.000241989
	LOSS [training: 0.15317081138911076 | validation: 0.13453423442285972]
	TIME [epoch: 6.63 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14973645429487817		[learning rate: 0.00024142]
	Learning Rate: 0.000241418
	LOSS [training: 0.14973645429487817 | validation: 0.1580907773178818]
	TIME [epoch: 6.63 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1593788642827614		[learning rate: 0.00024085]
	Learning Rate: 0.000240848
	LOSS [training: 0.1593788642827614 | validation: 0.15218833660384007]
	TIME [epoch: 6.63 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16125105752942712		[learning rate: 0.00024028]
	Learning Rate: 0.00024028
	LOSS [training: 0.16125105752942712 | validation: 0.14386275586790942]
	TIME [epoch: 6.67 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.154846000285985		[learning rate: 0.00023971]
	Learning Rate: 0.000239713
	LOSS [training: 0.154846000285985 | validation: 0.1424845953338572]
	TIME [epoch: 6.64 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16704263948300513		[learning rate: 0.00023915]
	Learning Rate: 0.000239148
	LOSS [training: 0.16704263948300513 | validation: 0.1694638137421946]
	TIME [epoch: 6.63 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16042705568844967		[learning rate: 0.00023858]
	Learning Rate: 0.000238584
	LOSS [training: 0.16042705568844967 | validation: 0.1449411018212678]
	TIME [epoch: 6.63 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15219553399990893		[learning rate: 0.00023802]
	Learning Rate: 0.000238021
	LOSS [training: 0.15219553399990893 | validation: 0.1478391779520176]
	TIME [epoch: 6.62 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14853984566527387		[learning rate: 0.00023746]
	Learning Rate: 0.00023746
	LOSS [training: 0.14853984566527387 | validation: 0.14013557860940085]
	TIME [epoch: 6.64 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1574790175275186		[learning rate: 0.0002369]
	Learning Rate: 0.000236899
	LOSS [training: 0.1574790175275186 | validation: 0.14279923665081287]
	TIME [epoch: 6.67 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15424170851557434		[learning rate: 0.00023634]
	Learning Rate: 0.000236341
	LOSS [training: 0.15424170851557434 | validation: 0.14761223562327888]
	TIME [epoch: 6.63 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15633675307451594		[learning rate: 0.00023578]
	Learning Rate: 0.000235783
	LOSS [training: 0.15633675307451594 | validation: 0.15357102739480472]
	TIME [epoch: 6.62 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16199193262724437		[learning rate: 0.00023523]
	Learning Rate: 0.000235227
	LOSS [training: 0.16199193262724437 | validation: 0.14135671692360866]
	TIME [epoch: 6.62 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14931273589461716		[learning rate: 0.00023467]
	Learning Rate: 0.000234672
	LOSS [training: 0.14931273589461716 | validation: 0.15401502780377957]
	TIME [epoch: 6.63 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1539701197948965		[learning rate: 0.00023412]
	Learning Rate: 0.000234119
	LOSS [training: 0.1539701197948965 | validation: 0.14346303764393237]
	TIME [epoch: 6.65 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15857817250986178		[learning rate: 0.00023357]
	Learning Rate: 0.000233566
	LOSS [training: 0.15857817250986178 | validation: 0.16499381215192588]
	TIME [epoch: 6.66 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16187976049696756		[learning rate: 0.00023302]
	Learning Rate: 0.000233015
	LOSS [training: 0.16187976049696756 | validation: 0.1588807348766899]
	TIME [epoch: 6.63 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15814597795724403		[learning rate: 0.00023247]
	Learning Rate: 0.000232466
	LOSS [training: 0.15814597795724403 | validation: 0.1503238908098759]
	TIME [epoch: 6.63 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15586522596134694		[learning rate: 0.00023192]
	Learning Rate: 0.000231917
	LOSS [training: 0.15586522596134694 | validation: 0.14666344919554275]
	TIME [epoch: 6.63 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1597471120740598		[learning rate: 0.00023137]
	Learning Rate: 0.00023137
	LOSS [training: 0.1597471120740598 | validation: 0.15587377545858253]
	TIME [epoch: 6.62 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16426932740022632		[learning rate: 0.00023082]
	Learning Rate: 0.000230825
	LOSS [training: 0.16426932740022632 | validation: 0.16811697984488855]
	TIME [epoch: 6.67 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17748798710982672		[learning rate: 0.00023028]
	Learning Rate: 0.00023028
	LOSS [training: 0.17748798710982672 | validation: 0.17011571490787247]
	TIME [epoch: 6.63 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1725165170766999		[learning rate: 0.00022974]
	Learning Rate: 0.000229737
	LOSS [training: 0.1725165170766999 | validation: 0.16255379603921777]
	TIME [epoch: 6.62 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1817445685957601		[learning rate: 0.00022919]
	Learning Rate: 0.000229195
	LOSS [training: 0.1817445685957601 | validation: 0.17005450744189624]
	TIME [epoch: 6.62 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1720224518727596		[learning rate: 0.00022865]
	Learning Rate: 0.000228654
	LOSS [training: 0.1720224518727596 | validation: 0.16683268515118369]
	TIME [epoch: 6.63 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16690262209178156		[learning rate: 0.00022811]
	Learning Rate: 0.000228115
	LOSS [training: 0.16690262209178156 | validation: 0.16006038990356047]
	TIME [epoch: 6.64 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1685255836264953		[learning rate: 0.00022758]
	Learning Rate: 0.000227577
	LOSS [training: 0.1685255836264953 | validation: 0.16712414471388923]
	TIME [epoch: 6.66 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16152834278474953		[learning rate: 0.00022704]
	Learning Rate: 0.00022704
	LOSS [training: 0.16152834278474953 | validation: 0.15601840691700902]
	TIME [epoch: 6.62 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16215781827775022		[learning rate: 0.0002265]
	Learning Rate: 0.000226505
	LOSS [training: 0.16215781827775022 | validation: 0.15660922744741557]
	TIME [epoch: 6.62 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16143110801727784		[learning rate: 0.00022597]
	Learning Rate: 0.00022597
	LOSS [training: 0.16143110801727784 | validation: 0.17211774126466856]
	TIME [epoch: 6.62 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15784591169343418		[learning rate: 0.00022544]
	Learning Rate: 0.000225437
	LOSS [training: 0.15784591169343418 | validation: 0.14823624028194846]
	TIME [epoch: 6.62 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1590174274745972		[learning rate: 0.00022491]
	Learning Rate: 0.000224905
	LOSS [training: 0.1590174274745972 | validation: 0.16644503089383395]
	TIME [epoch: 6.67 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15777027957166037		[learning rate: 0.00022437]
	Learning Rate: 0.000224375
	LOSS [training: 0.15777027957166037 | validation: 0.14357017762064023]
	TIME [epoch: 6.64 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15538864498278143		[learning rate: 0.00022385]
	Learning Rate: 0.000223846
	LOSS [training: 0.15538864498278143 | validation: 0.14803542760047933]
	TIME [epoch: 6.62 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15483078779540574		[learning rate: 0.00022332]
	Learning Rate: 0.000223318
	LOSS [training: 0.15483078779540574 | validation: 0.15483955264842242]
	TIME [epoch: 6.62 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15435890937059477		[learning rate: 0.00022279]
	Learning Rate: 0.000222791
	LOSS [training: 0.15435890937059477 | validation: 0.15809217159124023]
	TIME [epoch: 6.63 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15776320144232245		[learning rate: 0.00022227]
	Learning Rate: 0.000222265
	LOSS [training: 0.15776320144232245 | validation: 0.14957640337160433]
	TIME [epoch: 6.63 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1614119336382817		[learning rate: 0.00022174]
	Learning Rate: 0.000221741
	LOSS [training: 0.1614119336382817 | validation: 0.15921981407975064]
	TIME [epoch: 6.66 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16681471444514245		[learning rate: 0.00022122]
	Learning Rate: 0.000221218
	LOSS [training: 0.16681471444514245 | validation: 0.15379256000592867]
	TIME [epoch: 6.63 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16525113931584728		[learning rate: 0.0002207]
	Learning Rate: 0.000220696
	LOSS [training: 0.16525113931584728 | validation: 0.1649775179669412]
	TIME [epoch: 6.63 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15951081587360003		[learning rate: 0.00022018]
	Learning Rate: 0.000220176
	LOSS [training: 0.15951081587360003 | validation: 0.1541394830068205]
	TIME [epoch: 6.62 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16195807228969655		[learning rate: 0.00021966]
	Learning Rate: 0.000219656
	LOSS [training: 0.16195807228969655 | validation: 0.16347684132656498]
	TIME [epoch: 6.62 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16687781951628958		[learning rate: 0.00021914]
	Learning Rate: 0.000219138
	LOSS [training: 0.16687781951628958 | validation: 0.14861097597479744]
	TIME [epoch: 6.66 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16061063296153424		[learning rate: 0.00021862]
	Learning Rate: 0.000218621
	LOSS [training: 0.16061063296153424 | validation: 0.1517425412055359]
	TIME [epoch: 6.64 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16295081990270188		[learning rate: 0.00021811]
	Learning Rate: 0.000218106
	LOSS [training: 0.16295081990270188 | validation: 0.16147530926221323]
	TIME [epoch: 6.62 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15756346460242449		[learning rate: 0.00021759]
	Learning Rate: 0.000217591
	LOSS [training: 0.15756346460242449 | validation: 0.152707446306107]
	TIME [epoch: 6.62 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16289361497567784		[learning rate: 0.00021708]
	Learning Rate: 0.000217078
	LOSS [training: 0.16289361497567784 | validation: 0.1581862768250257]
	TIME [epoch: 6.62 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1548669314750702		[learning rate: 0.00021657]
	Learning Rate: 0.000216566
	LOSS [training: 0.1548669314750702 | validation: 0.15813110592913393]
	TIME [epoch: 6.63 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16203219187786494		[learning rate: 0.00021605]
	Learning Rate: 0.000216055
	LOSS [training: 0.16203219187786494 | validation: 0.1532784312301404]
	TIME [epoch: 6.67 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1655675487537525		[learning rate: 0.00021555]
	Learning Rate: 0.000215545
	LOSS [training: 0.1655675487537525 | validation: 0.1613981286207769]
	TIME [epoch: 6.63 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17309216689185822		[learning rate: 0.00021504]
	Learning Rate: 0.000215037
	LOSS [training: 0.17309216689185822 | validation: 0.15721901089127707]
	TIME [epoch: 6.63 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16209070200440728		[learning rate: 0.00021453]
	Learning Rate: 0.00021453
	LOSS [training: 0.16209070200440728 | validation: 0.1649504081410142]
	TIME [epoch: 6.62 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16546444156581025		[learning rate: 0.00021402]
	Learning Rate: 0.000214024
	LOSS [training: 0.16546444156581025 | validation: 0.14416584716940534]
	TIME [epoch: 6.62 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15709455563413433		[learning rate: 0.00021352]
	Learning Rate: 0.000213519
	LOSS [training: 0.15709455563413433 | validation: 0.14992287127455894]
	TIME [epoch: 6.64 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1541318896428437		[learning rate: 0.00021302]
	Learning Rate: 0.000213015
	LOSS [training: 0.1541318896428437 | validation: 0.13931729969690124]
	TIME [epoch: 6.66 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15573416551397332		[learning rate: 0.00021251]
	Learning Rate: 0.000212513
	LOSS [training: 0.15573416551397332 | validation: 0.14398891725864238]
	TIME [epoch: 6.63 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15940856958173666		[learning rate: 0.00021201]
	Learning Rate: 0.000212011
	LOSS [training: 0.15940856958173666 | validation: 0.14307101897131844]
	TIME [epoch: 6.62 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16048973243977338		[learning rate: 0.00021151]
	Learning Rate: 0.000211511
	LOSS [training: 0.16048973243977338 | validation: 0.14248356764307468]
	TIME [epoch: 6.62 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15910583496659897		[learning rate: 0.00021101]
	Learning Rate: 0.000211012
	LOSS [training: 0.15910583496659897 | validation: 0.13967129147741864]
	TIME [epoch: 6.62 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15852319366938522		[learning rate: 0.00021051]
	Learning Rate: 0.000210514
	LOSS [training: 0.15852319366938522 | validation: 0.1651725399367699]
	TIME [epoch: 6.66 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16281277972652508		[learning rate: 0.00021002]
	Learning Rate: 0.000210018
	LOSS [training: 0.16281277972652508 | validation: 0.15308659511717065]
	TIME [epoch: 6.63 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16011484949557075		[learning rate: 0.00020952]
	Learning Rate: 0.000209523
	LOSS [training: 0.16011484949557075 | validation: 0.15178884995077008]
	TIME [epoch: 6.62 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16283512295706742		[learning rate: 0.00020903]
	Learning Rate: 0.000209028
	LOSS [training: 0.16283512295706742 | validation: 0.15280201099149004]
	TIME [epoch: 6.62 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16264608549308995		[learning rate: 0.00020854]
	Learning Rate: 0.000208535
	LOSS [training: 0.16264608549308995 | validation: 0.15994204708869114]
	TIME [epoch: 6.63 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16137172973097214		[learning rate: 0.00020804]
	Learning Rate: 0.000208043
	LOSS [training: 0.16137172973097214 | validation: 0.14561402230988124]
	TIME [epoch: 6.63 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16340447363097393		[learning rate: 0.00020755]
	Learning Rate: 0.000207553
	LOSS [training: 0.16340447363097393 | validation: 0.1577525942713412]
	TIME [epoch: 6.66 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1602908026382185		[learning rate: 0.00020706]
	Learning Rate: 0.000207063
	LOSS [training: 0.1602908026382185 | validation: 0.13682865989256623]
	TIME [epoch: 6.62 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15714303386354989		[learning rate: 0.00020657]
	Learning Rate: 0.000206575
	LOSS [training: 0.15714303386354989 | validation: 0.15357241202866603]
	TIME [epoch: 6.63 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15602663365920305		[learning rate: 0.00020609]
	Learning Rate: 0.000206087
	LOSS [training: 0.15602663365920305 | validation: 0.14823841797558485]
	TIME [epoch: 6.62 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15289601381738932		[learning rate: 0.0002056]
	Learning Rate: 0.000205601
	LOSS [training: 0.15289601381738932 | validation: 0.15247551346181112]
	TIME [epoch: 6.62 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1634776456105346		[learning rate: 0.00020512]
	Learning Rate: 0.000205116
	LOSS [training: 0.1634776456105346 | validation: 0.1527001379805229]
	TIME [epoch: 6.65 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1616703224497346		[learning rate: 0.00020463]
	Learning Rate: 0.000204632
	LOSS [training: 0.1616703224497346 | validation: 0.16448673829333463]
	TIME [epoch: 6.65 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15554085219278024		[learning rate: 0.00020415]
	Learning Rate: 0.00020415
	LOSS [training: 0.15554085219278024 | validation: 0.14022647085619963]
	TIME [epoch: 6.62 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1533715603350096		[learning rate: 0.00020367]
	Learning Rate: 0.000203668
	LOSS [training: 0.1533715603350096 | validation: 0.14390376028268814]
	TIME [epoch: 6.63 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1515393948943795		[learning rate: 0.00020319]
	Learning Rate: 0.000203188
	LOSS [training: 0.1515393948943795 | validation: 0.15188806484566847]
	TIME [epoch: 6.62 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15018771948514978		[learning rate: 0.00020271]
	Learning Rate: 0.000202708
	LOSS [training: 0.15018771948514978 | validation: 0.1426272143414905]
	TIME [epoch: 6.63 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15934226101934865		[learning rate: 0.00020223]
	Learning Rate: 0.00020223
	LOSS [training: 0.15934226101934865 | validation: 0.14358327869050716]
	TIME [epoch: 6.67 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1594885645206034		[learning rate: 0.00020175]
	Learning Rate: 0.000201753
	LOSS [training: 0.1594885645206034 | validation: 0.1512363533318521]
	TIME [epoch: 6.62 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1579934510824285		[learning rate: 0.00020128]
	Learning Rate: 0.000201277
	LOSS [training: 0.1579934510824285 | validation: 0.14688188492812196]
	TIME [epoch: 6.63 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1591115722552113		[learning rate: 0.0002008]
	Learning Rate: 0.000200803
	LOSS [training: 0.1591115722552113 | validation: 0.1390041519452173]
	TIME [epoch: 6.62 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15911138308882017		[learning rate: 0.00020033]
	Learning Rate: 0.000200329
	LOSS [training: 0.15911138308882017 | validation: 0.13659772949270232]
	TIME [epoch: 6.62 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15219828085343656		[learning rate: 0.00019986]
	Learning Rate: 0.000199856
	LOSS [training: 0.15219828085343656 | validation: 0.15054189580860022]
	TIME [epoch: 6.64 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1567643803872619		[learning rate: 0.00019938]
	Learning Rate: 0.000199385
	LOSS [training: 0.1567643803872619 | validation: 0.14477709300815098]
	TIME [epoch: 6.66 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1649531701591918		[learning rate: 0.00019891]
	Learning Rate: 0.000198915
	LOSS [training: 0.1649531701591918 | validation: 0.14733095286788184]
	TIME [epoch: 6.63 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.163924513496797		[learning rate: 0.00019845]
	Learning Rate: 0.000198445
	LOSS [training: 0.163924513496797 | validation: 0.13954081343845678]
	TIME [epoch: 6.62 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15555319758326674		[learning rate: 0.00019798]
	Learning Rate: 0.000197977
	LOSS [training: 0.15555319758326674 | validation: 0.13294718867384503]
	TIME [epoch: 6.62 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1572588919841207		[learning rate: 0.00019751]
	Learning Rate: 0.00019751
	LOSS [training: 0.1572588919841207 | validation: 0.13626250165621542]
	TIME [epoch: 6.62 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15080907489889495		[learning rate: 0.00019704]
	Learning Rate: 0.000197044
	LOSS [training: 0.15080907489889495 | validation: 0.13879229222147435]
	TIME [epoch: 6.66 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14760509809271521		[learning rate: 0.00019658]
	Learning Rate: 0.00019658
	LOSS [training: 0.14760509809271521 | validation: 0.15174812394613607]
	TIME [epoch: 6.63 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1474101925530778		[learning rate: 0.00019612]
	Learning Rate: 0.000196116
	LOSS [training: 0.1474101925530778 | validation: 0.13495500743393252]
	TIME [epoch: 6.62 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1517250017220538		[learning rate: 0.00019565]
	Learning Rate: 0.000195653
	LOSS [training: 0.1517250017220538 | validation: 0.1477298426615581]
	TIME [epoch: 6.62 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15340402368553524		[learning rate: 0.00019519]
	Learning Rate: 0.000195192
	LOSS [training: 0.15340402368553524 | validation: 0.13543744419867193]
	TIME [epoch: 6.62 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14895352052807467		[learning rate: 0.00019473]
	Learning Rate: 0.000194731
	LOSS [training: 0.14895352052807467 | validation: 0.1329600500328781]
	TIME [epoch: 6.63 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16258549501288502		[learning rate: 0.00019427]
	Learning Rate: 0.000194272
	LOSS [training: 0.16258549501288502 | validation: 0.15340121950077795]
	TIME [epoch: 6.66 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16305565585175205		[learning rate: 0.00019381]
	Learning Rate: 0.000193814
	LOSS [training: 0.16305565585175205 | validation: 0.14341989036390906]
	TIME [epoch: 6.63 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16486208162391422		[learning rate: 0.00019336]
	Learning Rate: 0.000193357
	LOSS [training: 0.16486208162391422 | validation: 0.15073099542404558]
	TIME [epoch: 6.63 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1572949765736556		[learning rate: 0.0001929]
	Learning Rate: 0.0001929
	LOSS [training: 0.1572949765736556 | validation: 0.1362181681367381]
	TIME [epoch: 6.62 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14876695996904793		[learning rate: 0.00019245]
	Learning Rate: 0.000192445
	LOSS [training: 0.14876695996904793 | validation: 0.1437188321822962]
	TIME [epoch: 6.62 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14613462676255623		[learning rate: 0.00019199]
	Learning Rate: 0.000191992
	LOSS [training: 0.14613462676255623 | validation: 0.13342067286991643]
	TIME [epoch: 6.65 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14996876812847448		[learning rate: 0.00019154]
	Learning Rate: 0.000191539
	LOSS [training: 0.14996876812847448 | validation: 0.1484538324973325]
	TIME [epoch: 6.64 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1509388591709328		[learning rate: 0.00019109]
	Learning Rate: 0.000191087
	LOSS [training: 0.1509388591709328 | validation: 0.14139450250763275]
	TIME [epoch: 6.63 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15687821940994048		[learning rate: 0.00019064]
	Learning Rate: 0.000190636
	LOSS [training: 0.15687821940994048 | validation: 0.1353063556825375]
	TIME [epoch: 6.62 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14963554516891517		[learning rate: 0.00019019]
	Learning Rate: 0.000190186
	LOSS [training: 0.14963554516891517 | validation: 0.1404005749729239]
	TIME [epoch: 6.63 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15531319962980072		[learning rate: 0.00018974]
	Learning Rate: 0.000189738
	LOSS [training: 0.15531319962980072 | validation: 0.13861882963759764]
	TIME [epoch: 6.62 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15137844296343292		[learning rate: 0.00018929]
	Learning Rate: 0.00018929
	LOSS [training: 0.15137844296343292 | validation: 0.1354664270007659]
	TIME [epoch: 6.67 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16132450057355616		[learning rate: 0.00018884]
	Learning Rate: 0.000188844
	LOSS [training: 0.16132450057355616 | validation: 0.13928169982514949]
	TIME [epoch: 6.63 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1541840380604864		[learning rate: 0.0001884]
	Learning Rate: 0.000188398
	LOSS [training: 0.1541840380604864 | validation: 0.1467308169214891]
	TIME [epoch: 6.63 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15681704318030776		[learning rate: 0.00018795]
	Learning Rate: 0.000187954
	LOSS [training: 0.15681704318030776 | validation: 0.13889877914620466]
	TIME [epoch: 6.62 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15736161299077264		[learning rate: 0.00018751]
	Learning Rate: 0.000187511
	LOSS [training: 0.15736161299077264 | validation: 0.13675887996065633]
	TIME [epoch: 6.63 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15006987463367705		[learning rate: 0.00018707]
	Learning Rate: 0.000187068
	LOSS [training: 0.15006987463367705 | validation: 0.13692198916668147]
	TIME [epoch: 6.64 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15419377895966996		[learning rate: 0.00018663]
	Learning Rate: 0.000186627
	LOSS [training: 0.15419377895966996 | validation: 0.1388323155265977]
	TIME [epoch: 6.67 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15839636542389393		[learning rate: 0.00018619]
	Learning Rate: 0.000186187
	LOSS [training: 0.15839636542389393 | validation: 0.139426993602834]
	TIME [epoch: 6.62 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15775004274261778		[learning rate: 0.00018575]
	Learning Rate: 0.000185748
	LOSS [training: 0.15775004274261778 | validation: 0.13679247362612929]
	TIME [epoch: 6.62 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15160556348689708		[learning rate: 0.00018531]
	Learning Rate: 0.000185309
	LOSS [training: 0.15160556348689708 | validation: 0.14285717323043864]
	TIME [epoch: 6.62 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15529676866977798		[learning rate: 0.00018487]
	Learning Rate: 0.000184872
	LOSS [training: 0.15529676866977798 | validation: 0.1491232713792699]
	TIME [epoch: 6.62 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15771121888831366		[learning rate: 0.00018444]
	Learning Rate: 0.000184436
	LOSS [training: 0.15771121888831366 | validation: 0.1430490901368588]
	TIME [epoch: 6.67 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15464761577217392		[learning rate: 0.000184]
	Learning Rate: 0.000184001
	LOSS [training: 0.15464761577217392 | validation: 0.1683316967239836]
	TIME [epoch: 6.64 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15903038774534592		[learning rate: 0.00018357]
	Learning Rate: 0.000183567
	LOSS [training: 0.15903038774534592 | validation: 0.13931175017025083]
	TIME [epoch: 6.62 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1570931120628118		[learning rate: 0.00018313]
	Learning Rate: 0.000183134
	LOSS [training: 0.1570931120628118 | validation: 0.14316580792901484]
	TIME [epoch: 6.62 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14897492193350953		[learning rate: 0.0001827]
	Learning Rate: 0.000182702
	LOSS [training: 0.14897492193350953 | validation: 0.13096407945593347]
	TIME [epoch: 6.62 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15139071256484668		[learning rate: 0.00018227]
	Learning Rate: 0.000182271
	LOSS [training: 0.15139071256484668 | validation: 0.14432410596311723]
	TIME [epoch: 6.63 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15487553146340646		[learning rate: 0.00018184]
	Learning Rate: 0.000181841
	LOSS [training: 0.15487553146340646 | validation: 0.13940988298702547]
	TIME [epoch: 6.66 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14951914534943844		[learning rate: 0.00018141]
	Learning Rate: 0.000181412
	LOSS [training: 0.14951914534943844 | validation: 0.14120114044833673]
	TIME [epoch: 6.63 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15109821845953528		[learning rate: 0.00018098]
	Learning Rate: 0.000180984
	LOSS [training: 0.15109821845953528 | validation: 0.14318414199851442]
	TIME [epoch: 6.62 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15036689934592476		[learning rate: 0.00018056]
	Learning Rate: 0.000180557
	LOSS [training: 0.15036689934592476 | validation: 0.13062477057630675]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1750.pth
	Model improved!!!
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1520148079682772		[learning rate: 0.00018013]
	Learning Rate: 0.000180132
	LOSS [training: 0.1520148079682772 | validation: 0.14408287971374778]
	TIME [epoch: 6.63 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1542781709609302		[learning rate: 0.00017971]
	Learning Rate: 0.000179707
	LOSS [training: 0.1542781709609302 | validation: 0.1343964748709109]
	TIME [epoch: 6.65 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14995985356644345		[learning rate: 0.00017928]
	Learning Rate: 0.000179283
	LOSS [training: 0.14995985356644345 | validation: 0.14271150473581365]
	TIME [epoch: 6.64 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1496519539880805		[learning rate: 0.00017886]
	Learning Rate: 0.00017886
	LOSS [training: 0.1496519539880805 | validation: 0.1318726752402661]
	TIME [epoch: 6.62 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14865042992759708		[learning rate: 0.00017844]
	Learning Rate: 0.000178438
	LOSS [training: 0.14865042992759708 | validation: 0.1421024251838776]
	TIME [epoch: 6.62 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15033646178666343		[learning rate: 0.00017802]
	Learning Rate: 0.000178017
	LOSS [training: 0.15033646178666343 | validation: 0.13451496362126636]
	TIME [epoch: 6.62 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15274720181898033		[learning rate: 0.0001776]
	Learning Rate: 0.000177597
	LOSS [training: 0.15274720181898033 | validation: 0.14452037061415796]
	TIME [epoch: 6.62 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15188886772867577		[learning rate: 0.00017718]
	Learning Rate: 0.000177178
	LOSS [training: 0.15188886772867577 | validation: 0.1405687194073055]
	TIME [epoch: 6.66 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14764190989650472		[learning rate: 0.00017676]
	Learning Rate: 0.00017676
	LOSS [training: 0.14764190989650472 | validation: 0.13338680862436642]
	TIME [epoch: 6.62 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1494584093731977		[learning rate: 0.00017634]
	Learning Rate: 0.000176343
	LOSS [training: 0.1494584093731977 | validation: 0.14476729532428137]
	TIME [epoch: 6.62 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14915501725689623		[learning rate: 0.00017593]
	Learning Rate: 0.000175927
	LOSS [training: 0.14915501725689623 | validation: 0.13555045566253954]
	TIME [epoch: 6.62 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14724722577554056		[learning rate: 0.00017551]
	Learning Rate: 0.000175512
	LOSS [training: 0.14724722577554056 | validation: 0.1320561496713034]
	TIME [epoch: 6.62 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15132321925753273		[learning rate: 0.0001751]
	Learning Rate: 0.000175098
	LOSS [training: 0.15132321925753273 | validation: 0.13464388078549105]
	TIME [epoch: 6.62 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1527481615502913		[learning rate: 0.00017469]
	Learning Rate: 0.000174685
	LOSS [training: 0.1527481615502913 | validation: 0.13963748173888174]
	TIME [epoch: 6.66 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15177473124245527		[learning rate: 0.00017427]
	Learning Rate: 0.000174273
	LOSS [training: 0.15177473124245527 | validation: 0.13533101995152302]
	TIME [epoch: 6.62 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14915426240330004		[learning rate: 0.00017386]
	Learning Rate: 0.000173862
	LOSS [training: 0.14915426240330004 | validation: 0.1447321506888099]
	TIME [epoch: 6.62 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1485049054924934		[learning rate: 0.00017345]
	Learning Rate: 0.000173452
	LOSS [training: 0.1485049054924934 | validation: 0.13405261472668922]
	TIME [epoch: 6.62 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1470311123553949		[learning rate: 0.00017304]
	Learning Rate: 0.000173043
	LOSS [training: 0.1470311123553949 | validation: 0.13793702834531923]
	TIME [epoch: 6.62 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15312666225876426		[learning rate: 0.00017263]
	Learning Rate: 0.000172635
	LOSS [training: 0.15312666225876426 | validation: 0.14958142010832007]
	TIME [epoch: 6.63 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15316180580790723		[learning rate: 0.00017223]
	Learning Rate: 0.000172228
	LOSS [training: 0.15316180580790723 | validation: 0.1360805058261132]
	TIME [epoch: 6.66 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15136555328206852		[learning rate: 0.00017182]
	Learning Rate: 0.000171821
	LOSS [training: 0.15136555328206852 | validation: 0.1318087259805055]
	TIME [epoch: 6.62 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15069941709366425		[learning rate: 0.00017142]
	Learning Rate: 0.000171416
	LOSS [training: 0.15069941709366425 | validation: 0.13445708220563993]
	TIME [epoch: 6.62 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15139899320449207		[learning rate: 0.00017101]
	Learning Rate: 0.000171012
	LOSS [training: 0.15139899320449207 | validation: 0.1463943506924551]
	TIME [epoch: 6.62 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1463022986476054		[learning rate: 0.00017061]
	Learning Rate: 0.000170608
	LOSS [training: 0.1463022986476054 | validation: 0.13878064042184848]
	TIME [epoch: 6.62 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1592362038796244		[learning rate: 0.00017021]
	Learning Rate: 0.000170206
	LOSS [training: 0.1592362038796244 | validation: 0.14190226212949325]
	TIME [epoch: 6.66 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15204871692778607		[learning rate: 0.0001698]
	Learning Rate: 0.000169804
	LOSS [training: 0.15204871692778607 | validation: 0.13637596231149937]
	TIME [epoch: 6.63 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15123441328461		[learning rate: 0.0001694]
	Learning Rate: 0.000169404
	LOSS [training: 0.15123441328461 | validation: 0.14497078277909403]
	TIME [epoch: 6.62 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16496477527160386		[learning rate: 0.000169]
	Learning Rate: 0.000169004
	LOSS [training: 0.16496477527160386 | validation: 0.15139415056310074]
	TIME [epoch: 6.62 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.17296200158792913		[learning rate: 0.00016861]
	Learning Rate: 0.000168606
	LOSS [training: 0.17296200158792913 | validation: 0.14681138150224493]
	TIME [epoch: 6.62 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16163274844984737		[learning rate: 0.00016821]
	Learning Rate: 0.000168208
	LOSS [training: 0.16163274844984737 | validation: 0.13390954927664334]
	TIME [epoch: 6.63 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15848465621715932		[learning rate: 0.00016781]
	Learning Rate: 0.000167811
	LOSS [training: 0.15848465621715932 | validation: 0.1441820822503822]
	TIME [epoch: 6.66 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1574939977606375		[learning rate: 0.00016742]
	Learning Rate: 0.000167415
	LOSS [training: 0.1574939977606375 | validation: 0.1585271780282242]
	TIME [epoch: 6.63 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16143496064617957		[learning rate: 0.00016702]
	Learning Rate: 0.00016702
	LOSS [training: 0.16143496064617957 | validation: 0.13429758487231536]
	TIME [epoch: 6.6 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16110508220322028		[learning rate: 0.00016663]
	Learning Rate: 0.000166626
	LOSS [training: 0.16110508220322028 | validation: 0.13595899326327632]
	TIME [epoch: 6.62 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1503226514829757		[learning rate: 0.00016623]
	Learning Rate: 0.000166233
	LOSS [training: 0.1503226514829757 | validation: 0.13790011741901195]
	TIME [epoch: 6.6 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15782922045584552		[learning rate: 0.00016584]
	Learning Rate: 0.000165841
	LOSS [training: 0.15782922045584552 | validation: 0.14461600823743287]
	TIME [epoch: 6.63 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.157953389324309		[learning rate: 0.00016545]
	Learning Rate: 0.00016545
	LOSS [training: 0.157953389324309 | validation: 0.14745971202888714]
	TIME [epoch: 6.64 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15146072651279813		[learning rate: 0.00016506]
	Learning Rate: 0.00016506
	LOSS [training: 0.15146072651279813 | validation: 0.1282560281769491]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1788.pth
	Model improved!!!
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15141543501434138		[learning rate: 0.00016467]
	Learning Rate: 0.00016467
	LOSS [training: 0.15141543501434138 | validation: 0.14350593650968363]
	TIME [epoch: 6.58 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1526201844220049		[learning rate: 0.00016428]
	Learning Rate: 0.000164282
	LOSS [training: 0.1526201844220049 | validation: 0.13239007438985387]
	TIME [epoch: 6.59 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1525398949382978		[learning rate: 0.00016389]
	Learning Rate: 0.000163894
	LOSS [training: 0.1525398949382978 | validation: 0.12715097514360704]
	TIME [epoch: 6.62 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1791.pth
	Model improved!!!
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1489248026310719		[learning rate: 0.00016351]
	Learning Rate: 0.000163508
	LOSS [training: 0.1489248026310719 | validation: 0.13014245104490926]
	TIME [epoch: 6.62 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15090498382915082		[learning rate: 0.00016312]
	Learning Rate: 0.000163122
	LOSS [training: 0.15090498382915082 | validation: 0.13519562302999236]
	TIME [epoch: 6.58 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15061423193641693		[learning rate: 0.00016274]
	Learning Rate: 0.000162737
	LOSS [training: 0.15061423193641693 | validation: 0.13776596974287547]
	TIME [epoch: 6.58 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15438740558414793		[learning rate: 0.00016235]
	Learning Rate: 0.000162353
	LOSS [training: 0.15438740558414793 | validation: 0.13624633072599973]
	TIME [epoch: 6.58 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14993584962494028		[learning rate: 0.00016197]
	Learning Rate: 0.00016197
	LOSS [training: 0.14993584962494028 | validation: 0.13700519581870557]
	TIME [epoch: 6.59 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15014511419961576		[learning rate: 0.00016159]
	Learning Rate: 0.000161588
	LOSS [training: 0.15014511419961576 | validation: 0.13932919545124559]
	TIME [epoch: 6.61 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1427463424402407		[learning rate: 0.00016121]
	Learning Rate: 0.000161207
	LOSS [training: 0.1427463424402407 | validation: 0.1386276670246389]
	TIME [epoch: 6.6 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14758177803192638		[learning rate: 0.00016083]
	Learning Rate: 0.000160827
	LOSS [training: 0.14758177803192638 | validation: 0.1436161596188682]
	TIME [epoch: 6.58 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14828488257128541		[learning rate: 0.00016045]
	Learning Rate: 0.000160448
	LOSS [training: 0.14828488257128541 | validation: 0.14236149303741602]
	TIME [epoch: 6.58 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14365391618774828		[learning rate: 0.00016007]
	Learning Rate: 0.000160069
	LOSS [training: 0.14365391618774828 | validation: 0.14141654746035862]
	TIME [epoch: 6.58 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1473302359602235		[learning rate: 0.00015969]
	Learning Rate: 0.000159692
	LOSS [training: 0.1473302359602235 | validation: 0.13887122399517177]
	TIME [epoch: 6.58 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1447550028101707		[learning rate: 0.00015931]
	Learning Rate: 0.000159315
	LOSS [training: 0.1447550028101707 | validation: 0.14196024686551661]
	TIME [epoch: 6.59 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1468249365465862		[learning rate: 0.00015894]
	Learning Rate: 0.000158939
	LOSS [training: 0.1468249365465862 | validation: 0.14078905136875008]
	TIME [epoch: 6.61 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1467480787182681		[learning rate: 0.00015856]
	Learning Rate: 0.000158564
	LOSS [training: 0.1467480787182681 | validation: 0.14358500775039212]
	TIME [epoch: 6.58 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14770068851624124		[learning rate: 0.00015819]
	Learning Rate: 0.00015819
	LOSS [training: 0.14770068851624124 | validation: 0.13728937464226743]
	TIME [epoch: 6.58 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1456670692638595		[learning rate: 0.00015782]
	Learning Rate: 0.000157817
	LOSS [training: 0.1456670692638595 | validation: 0.13779954865563432]
	TIME [epoch: 6.58 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14698889102677695		[learning rate: 0.00015744]
	Learning Rate: 0.000157445
	LOSS [training: 0.14698889102677695 | validation: 0.1346448164732274]
	TIME [epoch: 6.58 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14975391985908265		[learning rate: 0.00015707]
	Learning Rate: 0.000157073
	LOSS [training: 0.14975391985908265 | validation: 0.13212358821318934]
	TIME [epoch: 6.61 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14418502887234033		[learning rate: 0.0001567]
	Learning Rate: 0.000156703
	LOSS [training: 0.14418502887234033 | validation: 0.13752174614024815]
	TIME [epoch: 6.59 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14370478682207471		[learning rate: 0.00015633]
	Learning Rate: 0.000156333
	LOSS [training: 0.14370478682207471 | validation: 0.12926707123581396]
	TIME [epoch: 6.58 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1492818832320467		[learning rate: 0.00015596]
	Learning Rate: 0.000155964
	LOSS [training: 0.1492818832320467 | validation: 0.127860844991846]
	TIME [epoch: 6.58 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14621396031283		[learning rate: 0.0001556]
	Learning Rate: 0.000155597
	LOSS [training: 0.14621396031283 | validation: 0.13476233338887444]
	TIME [epoch: 6.6 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15009602988850573		[learning rate: 0.00015523]
	Learning Rate: 0.00015523
	LOSS [training: 0.15009602988850573 | validation: 0.12991820193606443]
	TIME [epoch: 6.58 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14531923172477101		[learning rate: 0.00015486]
	Learning Rate: 0.000154863
	LOSS [training: 0.14531923172477101 | validation: 0.13627178054359426]
	TIME [epoch: 6.62 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14372458275964156		[learning rate: 0.0001545]
	Learning Rate: 0.000154498
	LOSS [training: 0.14372458275964156 | validation: 0.13673982811261953]
	TIME [epoch: 6.58 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14791203987531398		[learning rate: 0.00015413]
	Learning Rate: 0.000154134
	LOSS [training: 0.14791203987531398 | validation: 0.1398830431632519]
	TIME [epoch: 6.58 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14983120712579692		[learning rate: 0.00015377]
	Learning Rate: 0.00015377
	LOSS [training: 0.14983120712579692 | validation: 0.132920580730836]
	TIME [epoch: 6.58 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14794484339491917		[learning rate: 0.00015341]
	Learning Rate: 0.000153407
	LOSS [training: 0.14794484339491917 | validation: 0.1422261773550905]
	TIME [epoch: 6.58 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14270135432488565		[learning rate: 0.00015305]
	Learning Rate: 0.000153045
	LOSS [training: 0.14270135432488565 | validation: 0.1259168493073803]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1820.pth
	Model improved!!!
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14646976478232815		[learning rate: 0.00015268]
	Learning Rate: 0.000152684
	LOSS [training: 0.14646976478232815 | validation: 0.1498641710055545]
	TIME [epoch: 6.6 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1539281915983667		[learning rate: 0.00015232]
	Learning Rate: 0.000152324
	LOSS [training: 0.1539281915983667 | validation: 0.16100262733720505]
	TIME [epoch: 6.58 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15171118425607058		[learning rate: 0.00015196]
	Learning Rate: 0.000151965
	LOSS [training: 0.15171118425607058 | validation: 0.1328164548314591]
	TIME [epoch: 6.58 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14646364901272046		[learning rate: 0.00015161]
	Learning Rate: 0.000151607
	LOSS [training: 0.14646364901272046 | validation: 0.13869684628805756]
	TIME [epoch: 6.57 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14530887970268141		[learning rate: 0.00015125]
	Learning Rate: 0.000151249
	LOSS [training: 0.14530887970268141 | validation: 0.13267788075503087]
	TIME [epoch: 6.58 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14612269174058867		[learning rate: 0.00015089]
	Learning Rate: 0.000150892
	LOSS [training: 0.14612269174058867 | validation: 0.1330364906557795]
	TIME [epoch: 6.61 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14827537652196562		[learning rate: 0.00015054]
	Learning Rate: 0.000150536
	LOSS [training: 0.14827537652196562 | validation: 0.1356688829163456]
	TIME [epoch: 6.58 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1431672678716235		[learning rate: 0.00015018]
	Learning Rate: 0.000150181
	LOSS [training: 0.1431672678716235 | validation: 0.14046846006743358]
	TIME [epoch: 6.57 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14753765742880548		[learning rate: 0.00014983]
	Learning Rate: 0.000149827
	LOSS [training: 0.14753765742880548 | validation: 0.1387321663028217]
	TIME [epoch: 6.58 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14440883274940144		[learning rate: 0.00014947]
	Learning Rate: 0.000149473
	LOSS [training: 0.14440883274940144 | validation: 0.12916458202514727]
	TIME [epoch: 6.57 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14682933607758858		[learning rate: 0.00014912]
	Learning Rate: 0.000149121
	LOSS [training: 0.14682933607758858 | validation: 0.13232580971901914]
	TIME [epoch: 6.57 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14948494220432568		[learning rate: 0.00014877]
	Learning Rate: 0.000148769
	LOSS [training: 0.14948494220432568 | validation: 0.1289326666389206]
	TIME [epoch: 6.62 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1500891398872344		[learning rate: 0.00014842]
	Learning Rate: 0.000148418
	LOSS [training: 0.1500891398872344 | validation: 0.13027066981634158]
	TIME [epoch: 6.59 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14811477094311082		[learning rate: 0.00014807]
	Learning Rate: 0.000148068
	LOSS [training: 0.14811477094311082 | validation: 0.13783168228751302]
	TIME [epoch: 6.6 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14642705958527805		[learning rate: 0.00014772]
	Learning Rate: 0.000147719
	LOSS [training: 0.14642705958527805 | validation: 0.13476420962817923]
	TIME [epoch: 6.57 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14794214752942708		[learning rate: 0.00014737]
	Learning Rate: 0.00014737
	LOSS [training: 0.14794214752942708 | validation: 0.1350546380352372]
	TIME [epoch: 6.57 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1511031781419359		[learning rate: 0.00014702]
	Learning Rate: 0.000147023
	LOSS [training: 0.1511031781419359 | validation: 0.1333500981830023]
	TIME [epoch: 6.59 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14655653139179045		[learning rate: 0.00014668]
	Learning Rate: 0.000146676
	LOSS [training: 0.14655653139179045 | validation: 0.1391888260073332]
	TIME [epoch: 6.62 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.147396529939736		[learning rate: 0.00014633]
	Learning Rate: 0.00014633
	LOSS [training: 0.147396529939736 | validation: 0.1339243323978299]
	TIME [epoch: 6.58 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1441823749023307		[learning rate: 0.00014598]
	Learning Rate: 0.000145985
	LOSS [training: 0.1441823749023307 | validation: 0.13352750705154706]
	TIME [epoch: 6.58 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15084719048179196		[learning rate: 0.00014564]
	Learning Rate: 0.00014564
	LOSS [training: 0.15084719048179196 | validation: 0.149897670332142]
	TIME [epoch: 6.61 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.16155946696216356		[learning rate: 0.0001453]
	Learning Rate: 0.000145297
	LOSS [training: 0.16155946696216356 | validation: 0.14056725167620737]
	TIME [epoch: 6.59 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1524581402122035		[learning rate: 0.00014495]
	Learning Rate: 0.000144954
	LOSS [training: 0.1524581402122035 | validation: 0.13918489744528625]
	TIME [epoch: 6.62 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1482050925769253		[learning rate: 0.00014461]
	Learning Rate: 0.000144612
	LOSS [training: 0.1482050925769253 | validation: 0.13300972145765821]
	TIME [epoch: 6.6 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14637149884567813		[learning rate: 0.00014427]
	Learning Rate: 0.000144271
	LOSS [training: 0.14637149884567813 | validation: 0.13940159011611603]
	TIME [epoch: 6.58 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14354991697521866		[learning rate: 0.00014393]
	Learning Rate: 0.000143931
	LOSS [training: 0.14354991697521866 | validation: 0.14720602741673555]
	TIME [epoch: 6.59 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14391684888527712		[learning rate: 0.00014359]
	Learning Rate: 0.000143591
	LOSS [training: 0.14391684888527712 | validation: 0.14291617837638926]
	TIME [epoch: 6.59 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15663498766582729		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 0.15663498766582729 | validation: 0.13890094181659357]
	TIME [epoch: 6.59 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14504414127880416		[learning rate: 0.00014291]
	Learning Rate: 0.000142915
	LOSS [training: 0.14504414127880416 | validation: 0.1441107451615244]
	TIME [epoch: 6.64 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14760946276899928		[learning rate: 0.00014258]
	Learning Rate: 0.000142578
	LOSS [training: 0.14760946276899928 | validation: 0.1346339119577106]
	TIME [epoch: 6.59 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15195779256814054		[learning rate: 0.00014224]
	Learning Rate: 0.000142241
	LOSS [training: 0.15195779256814054 | validation: 0.13683108673810324]
	TIME [epoch: 6.6 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14385437644657956		[learning rate: 0.00014191]
	Learning Rate: 0.000141906
	LOSS [training: 0.14385437644657956 | validation: 0.13672284756179215]
	TIME [epoch: 6.62 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14862336208136046		[learning rate: 0.00014157]
	Learning Rate: 0.000141571
	LOSS [training: 0.14862336208136046 | validation: 0.14498016845425715]
	TIME [epoch: 6.59 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14843563188759382		[learning rate: 0.00014124]
	Learning Rate: 0.000141237
	LOSS [training: 0.14843563188759382 | validation: 0.13855209182032502]
	TIME [epoch: 6.59 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1471159034666952		[learning rate: 0.0001409]
	Learning Rate: 0.000140904
	LOSS [training: 0.1471159034666952 | validation: 0.14134919853580186]
	TIME [epoch: 6.62 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15005020430383104		[learning rate: 0.00014057]
	Learning Rate: 0.000140572
	LOSS [training: 0.15005020430383104 | validation: 0.12298344172401918]
	TIME [epoch: 6.58 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1856.pth
	Model improved!!!
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.155609560575243		[learning rate: 0.00014024]
	Learning Rate: 0.00014024
	LOSS [training: 0.155609560575243 | validation: 0.13628027753860728]
	TIME [epoch: 6.58 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1492549011663788		[learning rate: 0.00013991]
	Learning Rate: 0.000139909
	LOSS [training: 0.1492549011663788 | validation: 0.13873754997911925]
	TIME [epoch: 6.58 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14424926078028416		[learning rate: 0.00013958]
	Learning Rate: 0.000139579
	LOSS [training: 0.14424926078028416 | validation: 0.13617032294258946]
	TIME [epoch: 6.58 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1516593293504254		[learning rate: 0.00013925]
	Learning Rate: 0.00013925
	LOSS [training: 0.1516593293504254 | validation: 0.1346093395502166]
	TIME [epoch: 6.62 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1446581586837595		[learning rate: 0.00013892]
	Learning Rate: 0.000138921
	LOSS [training: 0.1446581586837595 | validation: 0.13067963692551157]
	TIME [epoch: 6.58 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14124120279583866		[learning rate: 0.00013859]
	Learning Rate: 0.000138594
	LOSS [training: 0.14124120279583866 | validation: 0.14265008948737967]
	TIME [epoch: 6.58 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1555235087451465		[learning rate: 0.00013827]
	Learning Rate: 0.000138267
	LOSS [training: 0.1555235087451465 | validation: 0.14785820307152586]
	TIME [epoch: 6.58 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1523560044362739		[learning rate: 0.00013794]
	Learning Rate: 0.000137941
	LOSS [training: 0.1523560044362739 | validation: 0.12707834833114887]
	TIME [epoch: 6.58 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14647895352801135		[learning rate: 0.00013762]
	Learning Rate: 0.000137615
	LOSS [training: 0.14647895352801135 | validation: 0.13707872284908407]
	TIME [epoch: 6.59 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14976781529914313		[learning rate: 0.00013729]
	Learning Rate: 0.000137291
	LOSS [training: 0.14976781529914313 | validation: 0.13556424789470223]
	TIME [epoch: 6.61 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14536110004459174		[learning rate: 0.00013697]
	Learning Rate: 0.000136967
	LOSS [training: 0.14536110004459174 | validation: 0.12923262897351365]
	TIME [epoch: 6.58 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14806969798143474		[learning rate: 0.00013664]
	Learning Rate: 0.000136644
	LOSS [training: 0.14806969798143474 | validation: 0.1361694875733612]
	TIME [epoch: 6.58 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14627979419531112		[learning rate: 0.00013632]
	Learning Rate: 0.000136321
	LOSS [training: 0.14627979419531112 | validation: 0.1308086384939575]
	TIME [epoch: 6.58 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14112206889454953		[learning rate: 0.000136]
	Learning Rate: 0.000136
	LOSS [training: 0.14112206889454953 | validation: 0.1439112533920126]
	TIME [epoch: 6.58 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14377361958883692		[learning rate: 0.00013568]
	Learning Rate: 0.000135679
	LOSS [training: 0.14377361958883692 | validation: 0.14020407953595543]
	TIME [epoch: 6.58 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14202136975249519		[learning rate: 0.00013536]
	Learning Rate: 0.000135359
	LOSS [training: 0.14202136975249519 | validation: 0.13184106751755842]
	TIME [epoch: 6.61 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14806089108743828		[learning rate: 0.00013504]
	Learning Rate: 0.00013504
	LOSS [training: 0.14806089108743828 | validation: 0.13599282925256695]
	TIME [epoch: 6.58 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14788722465177045		[learning rate: 0.00013472]
	Learning Rate: 0.000134721
	LOSS [training: 0.14788722465177045 | validation: 0.1377114963138223]
	TIME [epoch: 6.58 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1436877625954996		[learning rate: 0.0001344]
	Learning Rate: 0.000134403
	LOSS [training: 0.1436877625954996 | validation: 0.13680608734720284]
	TIME [epoch: 6.57 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1483612235228966		[learning rate: 0.00013409]
	Learning Rate: 0.000134086
	LOSS [training: 0.1483612235228966 | validation: 0.12644912650507956]
	TIME [epoch: 6.58 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14708257884633857		[learning rate: 0.00013377]
	Learning Rate: 0.00013377
	LOSS [training: 0.14708257884633857 | validation: 0.13987115335405403]
	TIME [epoch: 6.6 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14057120936164846		[learning rate: 0.00013345]
	Learning Rate: 0.000133455
	LOSS [training: 0.14057120936164846 | validation: 0.13445166531631672]
	TIME [epoch: 6.6 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14549328226822458		[learning rate: 0.00013314]
	Learning Rate: 0.00013314
	LOSS [training: 0.14549328226822458 | validation: 0.13423935260386688]
	TIME [epoch: 6.58 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14483251073184378		[learning rate: 0.00013283]
	Learning Rate: 0.000132826
	LOSS [training: 0.14483251073184378 | validation: 0.13765265596746215]
	TIME [epoch: 6.57 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15567425484115605		[learning rate: 0.00013251]
	Learning Rate: 0.000132512
	LOSS [training: 0.15567425484115605 | validation: 0.13345990598661822]
	TIME [epoch: 6.58 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15337317056337738		[learning rate: 0.0001322]
	Learning Rate: 0.0001322
	LOSS [training: 0.15337317056337738 | validation: 0.13217936951725456]
	TIME [epoch: 6.58 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14785478254042025		[learning rate: 0.00013189]
	Learning Rate: 0.000131888
	LOSS [training: 0.14785478254042025 | validation: 0.13535279011472676]
	TIME [epoch: 6.62 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15152014805376002		[learning rate: 0.00013158]
	Learning Rate: 0.000131577
	LOSS [training: 0.15152014805376002 | validation: 0.14199183160460926]
	TIME [epoch: 6.58 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15281720737791957		[learning rate: 0.00013127]
	Learning Rate: 0.000131266
	LOSS [training: 0.15281720737791957 | validation: 0.1333120144535359]
	TIME [epoch: 6.58 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14114710484825085		[learning rate: 0.00013096]
	Learning Rate: 0.000130957
	LOSS [training: 0.14114710484825085 | validation: 0.12677342770102135]
	TIME [epoch: 6.58 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14192355509525734		[learning rate: 0.00013065]
	Learning Rate: 0.000130648
	LOSS [training: 0.14192355509525734 | validation: 0.13829665333890295]
	TIME [epoch: 6.58 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14968667044814463		[learning rate: 0.00013034]
	Learning Rate: 0.00013034
	LOSS [training: 0.14968667044814463 | validation: 0.1203633220087647]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1888.pth
	Model improved!!!
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13879574528970545		[learning rate: 0.00013003]
	Learning Rate: 0.000130032
	LOSS [training: 0.13879574528970545 | validation: 0.11963838372474081]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240226_114313/states/model_phi1_1a_v2_1889.pth
	Model improved!!!
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14512513476573013		[learning rate: 0.00012973]
	Learning Rate: 0.000129726
	LOSS [training: 0.14512513476573013 | validation: 0.13517549522611771]
	TIME [epoch: 6.58 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14408322775518598		[learning rate: 0.00012942]
	Learning Rate: 0.00012942
	LOSS [training: 0.14408322775518598 | validation: 0.13336122733722813]
	TIME [epoch: 6.58 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14022209141447525		[learning rate: 0.00012911]
	Learning Rate: 0.000129114
	LOSS [training: 0.14022209141447525 | validation: 0.13964302449641353]
	TIME [epoch: 6.58 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14708602999890508		[learning rate: 0.00012881]
	Learning Rate: 0.00012881
	LOSS [training: 0.14708602999890508 | validation: 0.13390489122639387]
	TIME [epoch: 6.57 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1485674156009094		[learning rate: 0.00012851]
	Learning Rate: 0.000128506
	LOSS [training: 0.1485674156009094 | validation: 0.13966149179914028]
	TIME [epoch: 6.61 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14889462731297773		[learning rate: 0.0001282]
	Learning Rate: 0.000128203
	LOSS [training: 0.14889462731297773 | validation: 0.13334172506751016]
	TIME [epoch: 6.58 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14488255242696346		[learning rate: 0.0001279]
	Learning Rate: 0.0001279
	LOSS [training: 0.14488255242696346 | validation: 0.1320887205815564]
	TIME [epoch: 6.58 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14695291805629243		[learning rate: 0.0001276]
	Learning Rate: 0.000127599
	LOSS [training: 0.14695291805629243 | validation: 0.12881483903547986]
	TIME [epoch: 6.57 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1465104639526157		[learning rate: 0.0001273]
	Learning Rate: 0.000127298
	LOSS [training: 0.1465104639526157 | validation: 0.1318768001889288]
	TIME [epoch: 6.58 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14800940749019215		[learning rate: 0.000127]
	Learning Rate: 0.000126997
	LOSS [training: 0.14800940749019215 | validation: 0.13054349058207887]
	TIME [epoch: 6.58 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15613433885410546		[learning rate: 0.0001267]
	Learning Rate: 0.000126698
	LOSS [training: 0.15613433885410546 | validation: 0.13737870340545]
	TIME [epoch: 6.61 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14879498502496397		[learning rate: 0.0001264]
	Learning Rate: 0.000126399
	LOSS [training: 0.14879498502496397 | validation: 0.13422221629835648]
	TIME [epoch: 6.59 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15301637550943126		[learning rate: 0.0001261]
	Learning Rate: 0.000126101
	LOSS [training: 0.15301637550943126 | validation: 0.1436550921642987]
	TIME [epoch: 6.57 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15724574463766505		[learning rate: 0.0001258]
	Learning Rate: 0.000125803
	LOSS [training: 0.15724574463766505 | validation: 0.13574323462923782]
	TIME [epoch: 6.58 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15483186890516304		[learning rate: 0.00012551]
	Learning Rate: 0.000125507
	LOSS [training: 0.15483186890516304 | validation: 0.12876493603090453]
	TIME [epoch: 6.58 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1520066883213165		[learning rate: 0.00012521]
	Learning Rate: 0.000125211
	LOSS [training: 0.1520066883213165 | validation: 0.14067491477949529]
	TIME [epoch: 6.57 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14956865318208157		[learning rate: 0.00012492]
	Learning Rate: 0.000124915
	LOSS [training: 0.14956865318208157 | validation: 0.12926252329835064]
	TIME [epoch: 6.61 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1494038532728736		[learning rate: 0.00012462]
	Learning Rate: 0.000124621
	LOSS [training: 0.1494038532728736 | validation: 0.14782635525861948]
	TIME [epoch: 6.58 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15062265612650153		[learning rate: 0.00012433]
	Learning Rate: 0.000124327
	LOSS [training: 0.15062265612650153 | validation: 0.14022721675145228]
	TIME [epoch: 6.57 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1509665582319078		[learning rate: 0.00012403]
	Learning Rate: 0.000124033
	LOSS [training: 0.1509665582319078 | validation: 0.13389685964255527]
	TIME [epoch: 6.57 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1538675807449379		[learning rate: 0.00012374]
	Learning Rate: 0.000123741
	LOSS [training: 0.1538675807449379 | validation: 0.1475351984615381]
	TIME [epoch: 6.58 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15472333487199852		[learning rate: 0.00012345]
	Learning Rate: 0.000123449
	LOSS [training: 0.15472333487199852 | validation: 0.1409301499387552]
	TIME [epoch: 6.58 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15561622311800904		[learning rate: 0.00012316]
	Learning Rate: 0.000123158
	LOSS [training: 0.15561622311800904 | validation: 0.1309052358424657]
	TIME [epoch: 6.6 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1487820380230778		[learning rate: 0.00012287]
	Learning Rate: 0.000122867
	LOSS [training: 0.1487820380230778 | validation: 0.12902440209319285]
	TIME [epoch: 6.58 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14899947305862501		[learning rate: 0.00012258]
	Learning Rate: 0.000122577
	LOSS [training: 0.14899947305862501 | validation: 0.13533333786637763]
	TIME [epoch: 6.57 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1491551143252142		[learning rate: 0.00012229]
	Learning Rate: 0.000122288
	LOSS [training: 0.1491551143252142 | validation: 0.13623778261384076]
	TIME [epoch: 6.58 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1477020604261651		[learning rate: 0.000122]
	Learning Rate: 0.000122
	LOSS [training: 0.1477020604261651 | validation: 0.13779650996921503]
	TIME [epoch: 6.57 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14741784535260632		[learning rate: 0.00012171]
	Learning Rate: 0.000121712
	LOSS [training: 0.14741784535260632 | validation: 0.13217146467446933]
	TIME [epoch: 6.59 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15048567208228475		[learning rate: 0.00012142]
	Learning Rate: 0.000121425
	LOSS [training: 0.15048567208228475 | validation: 0.13968241895639458]
	TIME [epoch: 6.6 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1456390286758242		[learning rate: 0.00012114]
	Learning Rate: 0.000121138
	LOSS [training: 0.1456390286758242 | validation: 0.12824388674370876]
	TIME [epoch: 6.58 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14733686387559503		[learning rate: 0.00012085]
	Learning Rate: 0.000120853
	LOSS [training: 0.14733686387559503 | validation: 0.13266958684513758]
	TIME [epoch: 6.57 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.147328468070965		[learning rate: 0.00012057]
	Learning Rate: 0.000120568
	LOSS [training: 0.147328468070965 | validation: 0.12892167503412982]
	TIME [epoch: 6.57 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14824785759299547		[learning rate: 0.00012028]
	Learning Rate: 0.000120283
	LOSS [training: 0.14824785759299547 | validation: 0.1436527855776762]
	TIME [epoch: 6.57 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15916084335239536		[learning rate: 0.00012]
	Learning Rate: 0.00012
	LOSS [training: 0.15916084335239536 | validation: 0.13738160813810407]
	TIME [epoch: 6.61 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14804160341430928		[learning rate: 0.00011972]
	Learning Rate: 0.000119716
	LOSS [training: 0.14804160341430928 | validation: 0.12426495761755652]
	TIME [epoch: 6.58 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14548216769840844		[learning rate: 0.00011943]
	Learning Rate: 0.000119434
	LOSS [training: 0.14548216769840844 | validation: 0.1340292740222851]
	TIME [epoch: 6.57 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14830183703673144		[learning rate: 0.00011915]
	Learning Rate: 0.000119152
	LOSS [training: 0.14830183703673144 | validation: 0.13009390428024986]
	TIME [epoch: 6.57 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1505221756644533		[learning rate: 0.00011887]
	Learning Rate: 0.000118871
	LOSS [training: 0.1505221756644533 | validation: 0.13737697412040567]
	TIME [epoch: 6.57 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14979102272969996		[learning rate: 0.00011859]
	Learning Rate: 0.000118591
	LOSS [training: 0.14979102272969996 | validation: 0.14321212885233253]
	TIME [epoch: 6.58 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1549312979032386		[learning rate: 0.00011831]
	Learning Rate: 0.000118311
	LOSS [training: 0.1549312979032386 | validation: 0.1466618130482652]
	TIME [epoch: 6.61 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14813524417178195		[learning rate: 0.00011803]
	Learning Rate: 0.000118032
	LOSS [training: 0.14813524417178195 | validation: 0.1363941309375946]
	TIME [epoch: 6.58 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14541741894231636		[learning rate: 0.00011775]
	Learning Rate: 0.000117754
	LOSS [training: 0.14541741894231636 | validation: 0.13516576856432777]
	TIME [epoch: 6.58 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14685944760473024		[learning rate: 0.00011748]
	Learning Rate: 0.000117476
	LOSS [training: 0.14685944760473024 | validation: 0.1470279207081741]
	TIME [epoch: 6.57 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1497426424018288		[learning rate: 0.0001172]
	Learning Rate: 0.000117199
	LOSS [training: 0.1497426424018288 | validation: 0.13811549505539328]
	TIME [epoch: 6.58 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15925600775770868		[learning rate: 0.00011692]
	Learning Rate: 0.000116922
	LOSS [training: 0.15925600775770868 | validation: 0.13579076632327094]
	TIME [epoch: 6.6 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15375926094708423		[learning rate: 0.00011665]
	Learning Rate: 0.000116647
	LOSS [training: 0.15375926094708423 | validation: 0.14380306740018056]
	TIME [epoch: 6.59 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1479661812510539		[learning rate: 0.00011637]
	Learning Rate: 0.000116371
	LOSS [training: 0.1479661812510539 | validation: 0.135760926973019]
	TIME [epoch: 6.57 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14622825747070728		[learning rate: 0.0001161]
	Learning Rate: 0.000116097
	LOSS [training: 0.14622825747070728 | validation: 0.12767291105803502]
	TIME [epoch: 6.57 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15221293722931		[learning rate: 0.00011582]
	Learning Rate: 0.000115823
	LOSS [training: 0.15221293722931 | validation: 0.13221993537210253]
	TIME [epoch: 6.57 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15049195048218053		[learning rate: 0.00011555]
	Learning Rate: 0.00011555
	LOSS [training: 0.15049195048218053 | validation: 0.1363160548513046]
	TIME [epoch: 6.58 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15117566872529994		[learning rate: 0.00011528]
	Learning Rate: 0.000115277
	LOSS [training: 0.15117566872529994 | validation: 0.12811475448298068]
	TIME [epoch: 6.61 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14915874778666258		[learning rate: 0.00011501]
	Learning Rate: 0.000115005
	LOSS [training: 0.14915874778666258 | validation: 0.13350420658385848]
	TIME [epoch: 6.58 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15319832995683158		[learning rate: 0.00011473]
	Learning Rate: 0.000114734
	LOSS [training: 0.15319832995683158 | validation: 0.14350855733925105]
	TIME [epoch: 6.58 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15261601676229852		[learning rate: 0.00011446]
	Learning Rate: 0.000114463
	LOSS [training: 0.15261601676229852 | validation: 0.1384256696804449]
	TIME [epoch: 6.57 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14791423081543137		[learning rate: 0.00011419]
	Learning Rate: 0.000114193
	LOSS [training: 0.14791423081543137 | validation: 0.13049275879220154]
	TIME [epoch: 6.58 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15158901129516064		[learning rate: 0.00011392]
	Learning Rate: 0.000113924
	LOSS [training: 0.15158901129516064 | validation: 0.13737357499833613]
	TIME [epoch: 6.59 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15561045937066603		[learning rate: 0.00011366]
	Learning Rate: 0.000113655
	LOSS [training: 0.15561045937066603 | validation: 0.13857945449606118]
	TIME [epoch: 6.61 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1483317014585248		[learning rate: 0.00011339]
	Learning Rate: 0.000113387
	LOSS [training: 0.1483317014585248 | validation: 0.13979065559461618]
	TIME [epoch: 6.58 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14675882024410108		[learning rate: 0.00011312]
	Learning Rate: 0.00011312
	LOSS [training: 0.14675882024410108 | validation: 0.13945011987326988]
	TIME [epoch: 6.57 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14966111982418426		[learning rate: 0.00011285]
	Learning Rate: 0.000112853
	LOSS [training: 0.14966111982418426 | validation: 0.1351267660522232]
	TIME [epoch: 6.58 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1502648860489763		[learning rate: 0.00011259]
	Learning Rate: 0.000112587
	LOSS [training: 0.1502648860489763 | validation: 0.12632592312879773]
	TIME [epoch: 6.57 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14496589549005567		[learning rate: 0.00011232]
	Learning Rate: 0.000112321
	LOSS [training: 0.14496589549005567 | validation: 0.1254839621076082]
	TIME [epoch: 6.6 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14562374798711045		[learning rate: 0.00011206]
	Learning Rate: 0.000112056
	LOSS [training: 0.14562374798711045 | validation: 0.1277127078655122]
	TIME [epoch: 6.6 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14760785081245995		[learning rate: 0.00011179]
	Learning Rate: 0.000111792
	LOSS [training: 0.14760785081245995 | validation: 0.14532884364496723]
	TIME [epoch: 6.58 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14824223177323964		[learning rate: 0.00011153]
	Learning Rate: 0.000111528
	LOSS [training: 0.14824223177323964 | validation: 0.1346499884677066]
	TIME [epoch: 6.58 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1486970447181402		[learning rate: 0.00011127]
	Learning Rate: 0.000111265
	LOSS [training: 0.1486970447181402 | validation: 0.1413316631711043]
	TIME [epoch: 6.58 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14668522518703342		[learning rate: 0.000111]
	Learning Rate: 0.000111003
	LOSS [training: 0.14668522518703342 | validation: 0.12958058897474334]
	TIME [epoch: 6.58 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15160403649335102		[learning rate: 0.00011074]
	Learning Rate: 0.000110741
	LOSS [training: 0.15160403649335102 | validation: 0.13486661768048233]
	TIME [epoch: 6.61 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1514907729052771		[learning rate: 0.00011048]
	Learning Rate: 0.00011048
	LOSS [training: 0.1514907729052771 | validation: 0.13195080251809363]
	TIME [epoch: 6.58 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15170163972787326		[learning rate: 0.00011022]
	Learning Rate: 0.000110219
	LOSS [training: 0.15170163972787326 | validation: 0.12641783223985442]
	TIME [epoch: 6.58 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15059409350268987		[learning rate: 0.00010996]
	Learning Rate: 0.000109959
	LOSS [training: 0.15059409350268987 | validation: 0.14896959227673562]
	TIME [epoch: 6.58 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15227164582990688		[learning rate: 0.0001097]
	Learning Rate: 0.0001097
	LOSS [training: 0.15227164582990688 | validation: 0.13500394882874311]
	TIME [epoch: 6.58 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14493683313758576		[learning rate: 0.00010944]
	Learning Rate: 0.000109441
	LOSS [training: 0.14493683313758576 | validation: 0.12916046273797746]
	TIME [epoch: 6.59 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15156454323792062		[learning rate: 0.00010918]
	Learning Rate: 0.000109183
	LOSS [training: 0.15156454323792062 | validation: 0.1217084548675201]
	TIME [epoch: 6.61 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1511215092970271		[learning rate: 0.00010893]
	Learning Rate: 0.000108925
	LOSS [training: 0.1511215092970271 | validation: 0.14064916473418124]
	TIME [epoch: 6.58 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1510205190153547		[learning rate: 0.00010867]
	Learning Rate: 0.000108668
	LOSS [training: 0.1510205190153547 | validation: 0.12936704948587557]
	TIME [epoch: 6.58 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14960745278577559		[learning rate: 0.00010841]
	Learning Rate: 0.000108412
	LOSS [training: 0.14960745278577559 | validation: 0.13565199372703204]
	TIME [epoch: 6.58 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15022507344980918		[learning rate: 0.00010816]
	Learning Rate: 0.000108156
	LOSS [training: 0.15022507344980918 | validation: 0.1277874719749622]
	TIME [epoch: 6.57 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1514453801290816		[learning rate: 0.0001079]
	Learning Rate: 0.000107901
	LOSS [training: 0.1514453801290816 | validation: 0.1387713811559786]
	TIME [epoch: 6.61 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14953335912530819		[learning rate: 0.00010765]
	Learning Rate: 0.000107647
	LOSS [training: 0.14953335912530819 | validation: 0.13123034324556881]
	TIME [epoch: 6.58 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1482183728521472		[learning rate: 0.00010739]
	Learning Rate: 0.000107393
	LOSS [training: 0.1482183728521472 | validation: 0.12302534789008585]
	TIME [epoch: 6.58 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14462258744248624		[learning rate: 0.00010714]
	Learning Rate: 0.000107139
	LOSS [training: 0.14462258744248624 | validation: 0.1336609226065754]
	TIME [epoch: 6.58 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14886408811105886		[learning rate: 0.00010689]
	Learning Rate: 0.000106887
	LOSS [training: 0.14886408811105886 | validation: 0.1373449976117313]
	TIME [epoch: 6.58 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1519315533072385		[learning rate: 0.00010663]
	Learning Rate: 0.000106634
	LOSS [training: 0.1519315533072385 | validation: 0.1337372538258825]
	TIME [epoch: 6.59 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15566481049349398		[learning rate: 0.00010638]
	Learning Rate: 0.000106383
	LOSS [training: 0.15566481049349398 | validation: 0.1314237529437669]
	TIME [epoch: 6.61 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14937895663129988		[learning rate: 0.00010613]
	Learning Rate: 0.000106132
	LOSS [training: 0.14937895663129988 | validation: 0.14022475628946277]
	TIME [epoch: 6.58 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1516373068459017		[learning rate: 0.00010588]
	Learning Rate: 0.000105882
	LOSS [training: 0.1516373068459017 | validation: 0.14285345910059055]
	TIME [epoch: 6.58 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1538414394492793		[learning rate: 0.00010563]
	Learning Rate: 0.000105632
	LOSS [training: 0.1538414394492793 | validation: 0.13721711696879596]
	TIME [epoch: 6.58 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15241622244656405		[learning rate: 0.00010538]
	Learning Rate: 0.000105383
	LOSS [training: 0.15241622244656405 | validation: 0.1347984259534652]
	TIME [epoch: 6.57 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14867858037017226		[learning rate: 0.00010513]
	Learning Rate: 0.000105134
	LOSS [training: 0.14867858037017226 | validation: 0.1338641542115691]
	TIME [epoch: 6.59 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15023882366057612		[learning rate: 0.00010489]
	Learning Rate: 0.000104886
	LOSS [training: 0.15023882366057612 | validation: 0.14135224352226208]
	TIME [epoch: 6.6 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15164026670096561		[learning rate: 0.00010464]
	Learning Rate: 0.000104639
	LOSS [training: 0.15164026670096561 | validation: 0.13436145949401643]
	TIME [epoch: 6.58 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1515340085991949		[learning rate: 0.00010439]
	Learning Rate: 0.000104392
	LOSS [training: 0.1515340085991949 | validation: 0.14253655753239147]
	TIME [epoch: 6.57 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15224159789016795		[learning rate: 0.00010415]
	Learning Rate: 0.000104146
	LOSS [training: 0.15224159789016795 | validation: 0.141322349688204]
	TIME [epoch: 6.58 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15136932534069816		[learning rate: 0.0001039]
	Learning Rate: 0.0001039
	LOSS [training: 0.15136932534069816 | validation: 0.14484957193182418]
	TIME [epoch: 6.58 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15936988760707888		[learning rate: 0.00010365]
	Learning Rate: 0.000103655
	LOSS [training: 0.15936988760707888 | validation: 0.14884009749185653]
	TIME [epoch: 6.61 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15986685271460455		[learning rate: 0.00010341]
	Learning Rate: 0.00010341
	LOSS [training: 0.15986685271460455 | validation: 0.13618352795506783]
	TIME [epoch: 6.59 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14613802809218632		[learning rate: 0.00010317]
	Learning Rate: 0.000103166
	LOSS [training: 0.14613802809218632 | validation: 0.12873212384523347]
	TIME [epoch: 6.58 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1499692974013262		[learning rate: 0.00010292]
	Learning Rate: 0.000102923
	LOSS [training: 0.1499692974013262 | validation: 0.147372520040309]
	TIME [epoch: 6.58 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1549568651983916		[learning rate: 0.00010268]
	Learning Rate: 0.00010268
	LOSS [training: 0.1549568651983916 | validation: 0.13753826651719991]
	TIME [epoch: 6.58 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1534694307643545		[learning rate: 0.00010244]
	Learning Rate: 0.000102438
	LOSS [training: 0.1534694307643545 | validation: 0.13510713302984245]
	TIME [epoch: 6.59 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1543145692230503		[learning rate: 0.0001022]
	Learning Rate: 0.000102196
	LOSS [training: 0.1543145692230503 | validation: 0.13585548466433028]
	TIME [epoch: 6.61 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.15052343504653998		[learning rate: 0.00010196]
	Learning Rate: 0.000101955
	LOSS [training: 0.15052343504653998 | validation: 0.13532340378709357]
	TIME [epoch: 6.58 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14878547032890793		[learning rate: 0.00010171]
	Learning Rate: 0.000101715
	LOSS [training: 0.14878547032890793 | validation: 0.13365404770732175]
	TIME [epoch: 6.58 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14501580312720264		[learning rate: 0.00010147]
	Learning Rate: 0.000101475
	LOSS [training: 0.14501580312720264 | validation: 0.13059864953156028]
	TIME [epoch: 6.57 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.13975657720041912		[learning rate: 0.00010124]
	Learning Rate: 0.000101236
	LOSS [training: 0.13975657720041912 | validation: 0.12887176489731547]
	TIME [epoch: 6.58 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1430053676937219		[learning rate: 0.000101]
	Learning Rate: 0.000100997
	LOSS [training: 0.1430053676937219 | validation: 0.13302485463944283]
	TIME [epoch: 6.6 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1466625310299297		[learning rate: 0.00010076]
	Learning Rate: 0.000100759
	LOSS [training: 0.1466625310299297 | validation: 0.12959466457062047]
	TIME [epoch: 6.59 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1434951198318652		[learning rate: 0.00010052]
	Learning Rate: 0.000100521
	LOSS [training: 0.1434951198318652 | validation: 0.13547664649729363]
	TIME [epoch: 6.58 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.14103161610513928		[learning rate: 0.00010028]
	Learning Rate: 0.000100284
	LOSS [training: 0.14103161610513928 | validation: 0.12807937382372245]
	TIME [epoch: 6.58 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.1404263702175817		[learning rate: 0.00010005]
	Learning Rate: 0.000100047
	LOSS [training: 0.1404263702175817 | validation: 0.13905869559141076]
	TIME [epoch: 6.58 sec]
Finished training in 13503.891 seconds.
