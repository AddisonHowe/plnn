Args:
Namespace(name='model_algphiq_1a_v_kl1', outdir='out/model_training/model_algphiq_1a_v_kl1', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='quadratic', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[50, 100, 250], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', kernel='multiscale', bw_range=None, optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3813237407

Training model...

Saving initial model state to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 9.020355320873952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.020355320873952 | validation: 9.132409184842633]
	TIME [epoch: 110 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 8.959967387640411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.959967387640411 | validation: 9.085114514025655]
	TIME [epoch: 4.34 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_2.pth
	Model improved!!!
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 8.920620292749465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.920620292749465 | validation: 9.05310034669478]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_3.pth
	Model improved!!!
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 8.890294335967212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.890294335967212 | validation: 9.02367427750192]
	TIME [epoch: 4.15 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_4.pth
	Model improved!!!
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 8.863722528061242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.863722528061242 | validation: 8.997666712037278]
	TIME [epoch: 4.17 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_5.pth
	Model improved!!!
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 8.836502625908302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.836502625908302 | validation: 8.96920212004461]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_6.pth
	Model improved!!!
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 8.807477651672308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.807477651672308 | validation: 8.936778068946122]
	TIME [epoch: 4.12 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_7.pth
	Model improved!!!
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 8.771904449934707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.771904449934707 | validation: 8.897269568715542]
	TIME [epoch: 4.12 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_8.pth
	Model improved!!!
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 8.726944590870213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.726944590870213 | validation: 8.844172292670123]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_9.pth
	Model improved!!!
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 8.66527718532047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.66527718532047 | validation: 8.766866255386544]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_10.pth
	Model improved!!!
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 8.572424514364824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.572424514364824 | validation: 8.644715340405162]
	TIME [epoch: 4.12 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_11.pth
	Model improved!!!
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 8.40782204928873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.40782204928873 | validation: 8.391539205618587]
	TIME [epoch: 4.16 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_12.pth
	Model improved!!!
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 8.07760517434838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.07760517434838 | validation: 8.169264365830252]
	TIME [epoch: 4.15 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_13.pth
	Model improved!!!
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 7.918213255287916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.918213255287916 | validation: 8.083072138250929]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_14.pth
	Model improved!!!
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 7.824846967419914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.824846967419914 | validation: 8.000763449376691]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_15.pth
	Model improved!!!
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 7.722775123894278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.722775123894278 | validation: 7.860482410684845]
	TIME [epoch: 4.12 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_16.pth
	Model improved!!!
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 7.561440024625617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.561440024625617 | validation: 7.6822297485398305]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_17.pth
	Model improved!!!
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 7.331684956630259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.331684956630259 | validation: 7.392877321199071]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_18.pth
	Model improved!!!
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 6.966414021039181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.966414021039181 | validation: 6.904642734595892]
	TIME [epoch: 4.12 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_19.pth
	Model improved!!!
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 6.365091784919224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.365091784919224 | validation: 5.943120221050412]
	TIME [epoch: 4.16 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_20.pth
	Model improved!!!
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 5.01680235324822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.01680235324822 | validation: 3.151585483763859]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_21.pth
	Model improved!!!
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4279549573622923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4279549573622923 | validation: 1.1372059718225391]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_22.pth
	Model improved!!!
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 0.411040714988746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.411040714988746 | validation: 0.18175570114105377]
	TIME [epoch: 4.12 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_23.pth
	Model improved!!!
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17424557874746865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17424557874746865 | validation: 0.3330135693639058]
	TIME [epoch: 4.13 sec]
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17452889704320215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17452889704320215 | validation: 0.2995566547310998]
	TIME [epoch: 4.12 sec]
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 0.21771655896826703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21771655896826703 | validation: 0.11932196106431359]
	TIME [epoch: 4.12 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_26.pth
	Model improved!!!
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15570189990888117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15570189990888117 | validation: 0.15276416022770373]
	TIME [epoch: 4.13 sec]
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14470821208850382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14470821208850382 | validation: 0.15967244423970434]
	TIME [epoch: 4.17 sec]
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12819807876182987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12819807876182987 | validation: 0.07430382029608348]
	TIME [epoch: 4.15 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_29.pth
	Model improved!!!
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07841249640038056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07841249640038056 | validation: 0.05825877250762927]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_30.pth
	Model improved!!!
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07436806709491875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07436806709491875 | validation: 0.04382046481171936]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_31.pth
	Model improved!!!
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2551635023555291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2551635023555291 | validation: 0.49261975041875816]
	TIME [epoch: 4.12 sec]
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3907758617568176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3907758617568176 | validation: 0.05957606800379468]
	TIME [epoch: 4.12 sec]
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08093837007149277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08093837007149277 | validation: 0.10584345746200993]
	TIME [epoch: 4.12 sec]
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08507538677239918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08507538677239918 | validation: 0.051094936164253216]
	TIME [epoch: 4.12 sec]
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06251017516008844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06251017516008844 | validation: 0.09872832157822425]
	TIME [epoch: 4.12 sec]
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2798604253820021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2798604253820021 | validation: 0.3324657820286149]
	TIME [epoch: 4.16 sec]
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 0.21800508131064605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21800508131064605 | validation: 0.1842823602077625]
	TIME [epoch: 4.13 sec]
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2072665535668747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2072665535668747 | validation: 0.2634321886144477]
	TIME [epoch: 4.12 sec]
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1658921421972074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1658921421972074 | validation: 0.16231649473372584]
	TIME [epoch: 4.12 sec]
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14617477589593875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14617477589593875 | validation: 0.16032095724165157]
	TIME [epoch: 4.12 sec]
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1593643461067345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1593643461067345 | validation: 0.20030637136199253]
	TIME [epoch: 4.12 sec]
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14094752371536337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14094752371536337 | validation: 0.20312184191953747]
	TIME [epoch: 4.12 sec]
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14332578130260054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14332578130260054 | validation: 0.25521049641295523]
	TIME [epoch: 4.12 sec]
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18686141123344813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18686141123344813 | validation: 0.28136946551251174]
	TIME [epoch: 4.12 sec]
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16180965925983504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16180965925983504 | validation: 0.22199384488183932]
	TIME [epoch: 4.16 sec]
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15820510283607292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15820510283607292 | validation: 0.2034037441783424]
	TIME [epoch: 4.13 sec]
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14752848772971713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14752848772971713 | validation: 0.2079510512646978]
	TIME [epoch: 4.12 sec]
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12897224163084767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12897224163084767 | validation: 0.2120498918318488]
	TIME [epoch: 4.13 sec]
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14362852821881897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14362852821881897 | validation: 0.23394026822110475]
	TIME [epoch: 4.12 sec]
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14072795088145418		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 0.14072795088145418 | validation: 0.22400469935603573]
	TIME [epoch: 112 sec]
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15743233822327127		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 0.15743233822327127 | validation: 0.20188389245703195]
	TIME [epoch: 8.14 sec]
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14174224897295637		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 0.14174224897295637 | validation: 0.2771317205459894]
	TIME [epoch: 8.12 sec]
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13717611254016795		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 0.13717611254016795 | validation: 0.2302705030320514]
	TIME [epoch: 8.04 sec]
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13622957230120397		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 0.13622957230120397 | validation: 0.20543186417008905]
	TIME [epoch: 8.04 sec]
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 0.11970613643529801		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 0.11970613643529801 | validation: 0.2315605201753907]
	TIME [epoch: 8.04 sec]
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13110025930809377		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 0.13110025930809377 | validation: 0.22219274736377587]
	TIME [epoch: 8.07 sec]
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 0.11756516802225372		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 0.11756516802225372 | validation: 0.1604227550815579]
	TIME [epoch: 8.08 sec]
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 0.11863020284324291		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 0.11863020284324291 | validation: 0.1488147284392487]
	TIME [epoch: 8.05 sec]
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13945276783315738		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.13945276783315738 | validation: 0.21683461957668984]
	TIME [epoch: 8.05 sec]
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13858566285538093		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 0.13858566285538093 | validation: 0.15451945286917418]
	TIME [epoch: 8.05 sec]
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 0.11371557357920048		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 0.11371557357920048 | validation: 0.1448885798758318]
	TIME [epoch: 8.1 sec]
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 0.10406650309434529		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 0.10406650309434529 | validation: 0.15933405822251748]
	TIME [epoch: 8.05 sec]
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 0.11344017672315336		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 0.11344017672315336 | validation: 0.11300918117355098]
	TIME [epoch: 8.05 sec]
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 0.10432685802229652		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 0.10432685802229652 | validation: 0.2335444604109417]
	TIME [epoch: 8.04 sec]
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1379319529615555		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 0.1379319529615555 | validation: 0.16084533401083345]
	TIME [epoch: 8.09 sec]
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 0.10454428520178434		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 0.10454428520178434 | validation: 0.11751439792046744]
	TIME [epoch: 8.07 sec]
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 0.09677505056535289		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 0.09677505056535289 | validation: 0.11801117473730624]
	TIME [epoch: 8.05 sec]
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08688149115166777		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 0.08688149115166777 | validation: 0.11035569182623095]
	TIME [epoch: 8.05 sec]
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 0.09515110179670208		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 0.09515110179670208 | validation: 0.12031518972443717]
	TIME [epoch: 8.07 sec]
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08209341753772154		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 0.08209341753772154 | validation: 0.12312912607715147]
	TIME [epoch: 8.09 sec]
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08506759630281727		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 0.08506759630281727 | validation: 0.11279354997199229]
	TIME [epoch: 8.05 sec]
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08987973091133672		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 0.08987973091133672 | validation: 0.12450702249432344]
	TIME [epoch: 8.04 sec]
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 0.09574922029595237		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 0.09574922029595237 | validation: 0.12720087132741412]
	TIME [epoch: 8.05 sec]
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 0.25761089863483294		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.25761089863483294 | validation: 0.6593223747480066]
	TIME [epoch: 8.1 sec]
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 0.23082481935090526		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 0.23082481935090526 | validation: 0.13819778318867587]
	TIME [epoch: 8.07 sec]
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12017504820657532		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 0.12017504820657532 | validation: 0.07383818902723062]
	TIME [epoch: 8.05 sec]
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08218218005688172		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 0.08218218005688172 | validation: 0.1095855582488871]
	TIME [epoch: 8.05 sec]
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07437958172147176		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 0.07437958172147176 | validation: 0.061114391223992044]
	TIME [epoch: 8.07 sec]
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06708043482180363		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 0.06708043482180363 | validation: 0.06886349111957918]
	TIME [epoch: 8.09 sec]
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0643688664103376		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 0.0643688664103376 | validation: 0.08594695657386967]
	TIME [epoch: 8.05 sec]
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08082303462510576		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 0.08082303462510576 | validation: 0.058908371188104554]
	TIME [epoch: 8.05 sec]
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06902329808300103		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 0.06902329808300103 | validation: 0.06933116257086173]
	TIME [epoch: 8.05 sec]
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06746939690635674		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 0.06746939690635674 | validation: 0.05697115640512465]
	TIME [epoch: 8.11 sec]
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06595197027613511		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 0.06595197027613511 | validation: 0.060978652419725235]
	TIME [epoch: 8.05 sec]
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05954141489019351		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 0.05954141489019351 | validation: 0.06482731870129711]
	TIME [epoch: 8.04 sec]
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05969561623098461		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 0.05969561623098461 | validation: 0.08409736829362278]
	TIME [epoch: 8.05 sec]
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06726493996808224		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 0.06726493996808224 | validation: 0.07125605800365925]
	TIME [epoch: 8.08 sec]
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05702162471800752		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 0.05702162471800752 | validation: 0.07075606242985182]
	TIME [epoch: 8.07 sec]
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05293726091582874		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.05293726091582874 | validation: 0.09273541381136277]
	TIME [epoch: 8.04 sec]
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07633103419600863		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 0.07633103419600863 | validation: 0.07451433420199352]
	TIME [epoch: 8.04 sec]
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 0.045386683909038585		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 0.045386683909038585 | validation: 0.09702153300431549]
	TIME [epoch: 8.05 sec]
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05208605475505649		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 0.05208605475505649 | validation: 0.10405271556578434]
	TIME [epoch: 8.11 sec]
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05488433516475755		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 0.05488433516475755 | validation: 0.04103327663864765]
	TIME [epoch: 8.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_94.pth
	Model improved!!!
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1012988617183574		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 0.1012988617183574 | validation: 0.03983834459423928]
	TIME [epoch: 8.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_95.pth
	Model improved!!!
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0656835772858124		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 0.0656835772858124 | validation: 0.06905133326967101]
	TIME [epoch: 8.05 sec]
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06405289984688502		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 0.06405289984688502 | validation: 0.07998086946656466]
	TIME [epoch: 8.09 sec]
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04789959203362443		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 0.04789959203362443 | validation: 0.020108910689251976]
	TIME [epoch: 8.03 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_98.pth
	Model improved!!!
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04944231216751017		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 0.04944231216751017 | validation: 0.06858960470871203]
	TIME [epoch: 8.05 sec]
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06023775062959468		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 0.06023775062959468 | validation: 0.058556914314370184]
	TIME [epoch: 8.05 sec]
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05850775195494645		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 0.05850775195494645 | validation: 0.046531185946928844]
	TIME [epoch: 122 sec]
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 0.043888343956230494		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 0.043888343956230494 | validation: 0.0294819924759053]
	TIME [epoch: 18.3 sec]
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03822765203394054		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 0.03822765203394054 | validation: 0.03758959815813471]
	TIME [epoch: 18.3 sec]
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05931830732930138		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 0.05931830732930138 | validation: 0.028158623877388624]
	TIME [epoch: 18.3 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04244073844999213		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.04244073844999213 | validation: 0.025920471788753563]
	TIME [epoch: 18.3 sec]
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04555865433725759		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 0.04555865433725759 | validation: 0.035750877053742414]
	TIME [epoch: 18.3 sec]
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05615385032477122		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 0.05615385032477122 | validation: 0.019127891643462524]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_107.pth
	Model improved!!!
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 0.041835682643524566		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 0.041835682643524566 | validation: 0.022095474773533894]
	TIME [epoch: 18.3 sec]
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03925488507163048		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: 0.03925488507163048 | validation: 0.05271287820285131]
	TIME [epoch: 18.3 sec]
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06298397829704222		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: 0.06298397829704222 | validation: 0.14304462949922964]
	TIME [epoch: 18.3 sec]
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06556942336067323		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: 0.06556942336067323 | validation: 0.05393878179769879]
	TIME [epoch: 18.3 sec]
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: 0.09855609171421707		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: 0.09855609171421707 | validation: 0.21652697791024572]
	TIME [epoch: 18.4 sec]
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08675434120179597		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: 0.08675434120179597 | validation: 0.025024985661347282]
	TIME [epoch: 18.3 sec]
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07389273023292975		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: 0.07389273023292975 | validation: 0.6174847180104442]
	TIME [epoch: 18.4 sec]
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3652871084544144		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: 0.3652871084544144 | validation: 0.06048192720608638]
	TIME [epoch: 18.3 sec]
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: 0.048012035692339985		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: 0.048012035692339985 | validation: 0.14161175635587547]
	TIME [epoch: 18.3 sec]
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08169638935960763		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: 0.08169638935960763 | validation: 0.07911401116317035]
	TIME [epoch: 18.3 sec]
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05969725176569664		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: 0.05969725176569664 | validation: 0.018087499894063316]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_118.pth
	Model improved!!!
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05391883845398579		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: 0.05391883845398579 | validation: 0.025681925276946276]
	TIME [epoch: 18.3 sec]
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05494977130097771		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.05494977130097771 | validation: 0.07016842606734573]
	TIME [epoch: 18.3 sec]
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02496515415345852		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: 0.02496515415345852 | validation: 0.014027608667654979]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_121.pth
	Model improved!!!
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: 0.010697582392124746		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: 0.010697582392124746 | validation: 0.011990665457599633]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_122.pth
	Model improved!!!
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: 0.011569558892847432		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: 0.011569558892847432 | validation: 0.011819542547274096]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_123.pth
	Model improved!!!
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: 0.026569970878283007		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: 0.026569970878283007 | validation: 0.014836283421735883]
	TIME [epoch: 18.3 sec]
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: 0.015597802238645818		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: 0.015597802238645818 | validation: 0.07719890323028356]
	TIME [epoch: 18.3 sec]
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05796716155596225		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: 0.05796716155596225 | validation: 0.11847175126451226]
	TIME [epoch: 18.3 sec]
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04511511045580804		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: 0.04511511045580804 | validation: 0.09267509049713257]
	TIME [epoch: 18.3 sec]
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04402303714471466		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: 0.04402303714471466 | validation: 0.021142672761648287]
	TIME [epoch: 18.3 sec]
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: 0.023615154362511978		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: 0.023615154362511978 | validation: 0.023994830179493564]
	TIME [epoch: 18.3 sec]
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: 0.021793541187039685		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: 0.021793541187039685 | validation: 0.010290969514717316]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_130.pth
	Model improved!!!
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02302715575286703		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: 0.02302715575286703 | validation: 0.014599022863756736]
	TIME [epoch: 18.4 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: 0.015904024363409007		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: 0.015904024363409007 | validation: 0.03714231918042778]
	TIME [epoch: 18.3 sec]
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: 0.014244273325999732		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: 0.014244273325999732 | validation: 0.030681615965745958]
	TIME [epoch: 18.4 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: 0.028848899519509138		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: 0.028848899519509138 | validation: 0.030868742348457014]
	TIME [epoch: 18.3 sec]
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01783606535108657		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 0.01783606535108657 | validation: 0.017340589703816838]
	TIME [epoch: 18.4 sec]
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04608068933552503		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: 0.04608068933552503 | validation: 0.009111072047321058]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_136.pth
	Model improved!!!
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: 0.009459784900318787		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: 0.009459784900318787 | validation: 0.0031132051380179503]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_137.pth
	Model improved!!!
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: 0.019848171105406102		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: 0.019848171105406102 | validation: 0.01682465456561059]
	TIME [epoch: 18.3 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006216403690719188		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: 0.006216403690719188 | validation: 0.06473423370446574]
	TIME [epoch: 18.3 sec]
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: 0.034386904055128356		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: 0.034386904055128356 | validation: 0.04107633499436543]
	TIME [epoch: 18.3 sec]
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: 0.018145575391297575		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: 0.018145575391297575 | validation: 0.005947641823568538]
	TIME [epoch: 18.3 sec]
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: 0.010647947886453898		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: 0.010647947886453898 | validation: 0.03915858192447463]
	TIME [epoch: 18.3 sec]
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: 0.040545840767886976		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: 0.040545840767886976 | validation: 0.009289664029688005]
	TIME [epoch: 18.3 sec]
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03837123546749596		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: 0.03837123546749596 | validation: 0.031674149366831425]
	TIME [epoch: 18.3 sec]
EPOCH 145/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01809757433721508		[learning rate: 0.0023352]
	Learning Rate: 0.00233525
	LOSS [training: 0.01809757433721508 | validation: 0.051814934417092895]
	TIME [epoch: 18.3 sec]
EPOCH 146/500:
	Training over batches...
		[batch 4/4] avg loss: 0.018351214865377816		[learning rate: 0.0022997]
	Learning Rate: 0.00229968
	LOSS [training: 0.018351214865377816 | validation: 0.027451390812971613]
	TIME [epoch: 18.3 sec]
EPOCH 147/500:
	Training over batches...
		[batch 4/4] avg loss: 0.008745580762124833		[learning rate: 0.0022646]
	Learning Rate: 0.00226464
	LOSS [training: 0.008745580762124833 | validation: 0.017675095419083277]
	TIME [epoch: 18.3 sec]
EPOCH 148/500:
	Training over batches...
		[batch 4/4] avg loss: 0.014161244820152831		[learning rate: 0.0022301]
	Learning Rate: 0.00223015
	LOSS [training: 0.014161244820152831 | validation: 0.028289903084089484]
	TIME [epoch: 18.3 sec]
EPOCH 149/500:
	Training over batches...
		[batch 4/4] avg loss: 0.018973897384520193		[learning rate: 0.0021962]
	Learning Rate: 0.00219617
	LOSS [training: 0.018973897384520193 | validation: 0.032341486137307585]
	TIME [epoch: 18.3 sec]
EPOCH 150/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0051042216413423784		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 0.0051042216413423784 | validation: 0.02456572272496194]
	TIME [epoch: 18.3 sec]
EPOCH 151/500:
	Training over batches...
		[batch 4/4] avg loss: 0.024944730558908345		[learning rate: 0.0021298]
	Learning Rate: 0.00212977
	LOSS [training: 0.024944730558908345 | validation: 0.07632116500549224]
	TIME [epoch: 18.3 sec]
EPOCH 152/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0383961277048249		[learning rate: 0.0020973]
	Learning Rate: 0.00209733
	LOSS [training: 0.0383961277048249 | validation: 0.012655898258871812]
	TIME [epoch: 18.4 sec]
EPOCH 153/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007957209645134303		[learning rate: 0.0020654]
	Learning Rate: 0.00206538
	LOSS [training: 0.007957209645134303 | validation: 0.02171186233553205]
	TIME [epoch: 18.3 sec]
EPOCH 154/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01165932837295388		[learning rate: 0.0020339]
	Learning Rate: 0.00203392
	LOSS [training: 0.01165932837295388 | validation: 0.010254986344753969]
	TIME [epoch: 18.4 sec]
EPOCH 155/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0308029462410083		[learning rate: 0.0020029]
	Learning Rate: 0.00200293
	LOSS [training: 0.0308029462410083 | validation: 0.013074440310080062]
	TIME [epoch: 18.3 sec]
EPOCH 156/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04571874666273698		[learning rate: 0.0019724]
	Learning Rate: 0.00197242
	LOSS [training: 0.04571874666273698 | validation: 0.021065009707848262]
	TIME [epoch: 18.4 sec]
EPOCH 157/500:
	Training over batches...
		[batch 4/4] avg loss: 0.026081010525968683		[learning rate: 0.0019424]
	Learning Rate: 0.00194238
	LOSS [training: 0.026081010525968683 | validation: 0.012459875479323201]
	TIME [epoch: 18.3 sec]
EPOCH 158/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0067176842061893085		[learning rate: 0.0019128]
	Learning Rate: 0.00191279
	LOSS [training: 0.0067176842061893085 | validation: 0.001991975720661638]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_158.pth
	Model improved!!!
EPOCH 159/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0046308226060354715		[learning rate: 0.0018836]
	Learning Rate: 0.00188365
	LOSS [training: 0.0046308226060354715 | validation: 0.005614892150962609]
	TIME [epoch: 18.3 sec]
EPOCH 160/500:
	Training over batches...
		[batch 4/4] avg loss: 0.021459350963492753		[learning rate: 0.001855]
	Learning Rate: 0.00185495
	LOSS [training: 0.021459350963492753 | validation: 0.009073567175456564]
	TIME [epoch: 18.3 sec]
EPOCH 161/500:
	Training over batches...
		[batch 4/4] avg loss: 0.015846254487873343		[learning rate: 0.0018267]
	Learning Rate: 0.0018267
	LOSS [training: 0.015846254487873343 | validation: 0.007955323689314956]
	TIME [epoch: 18.3 sec]
EPOCH 162/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006931117133667827		[learning rate: 0.0017989]
	Learning Rate: 0.00179887
	LOSS [training: 0.006931117133667827 | validation: 0.013927798505458711]
	TIME [epoch: 18.4 sec]
EPOCH 163/500:
	Training over batches...
		[batch 4/4] avg loss: 0.014737105631612476		[learning rate: 0.0017715]
	Learning Rate: 0.00177147
	LOSS [training: 0.014737105631612476 | validation: 0.005288770450465858]
	TIME [epoch: 18.3 sec]
EPOCH 164/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006768101949261033		[learning rate: 0.0017445]
	Learning Rate: 0.00174448
	LOSS [training: 0.006768101949261033 | validation: 0.0080501403573218]
	TIME [epoch: 18.3 sec]
EPOCH 165/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002556316910813123		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: 0.002556316910813123 | validation: 0.013671748964862979]
	TIME [epoch: 18.3 sec]
EPOCH 166/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011545699614519097		[learning rate: 0.0016917]
	Learning Rate: 0.00169174
	LOSS [training: 0.0011545699614519097 | validation: 0.021211844467992857]
	TIME [epoch: 18.3 sec]
EPOCH 167/500:
	Training over batches...
		[batch 4/4] avg loss: 0.017195263541867317		[learning rate: 0.001666]
	Learning Rate: 0.00166597
	LOSS [training: 0.017195263541867317 | validation: 0.1006695021637997]
	TIME [epoch: 18.3 sec]
EPOCH 168/500:
	Training over batches...
		[batch 4/4] avg loss: 0.046807602719720946		[learning rate: 0.0016406]
	Learning Rate: 0.00164059
	LOSS [training: 0.046807602719720946 | validation: 0.01169800400792654]
	TIME [epoch: 18.4 sec]
EPOCH 169/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006191287254934355		[learning rate: 0.0016156]
	Learning Rate: 0.0016156
	LOSS [training: 0.006191287254934355 | validation: 0.01803662304529585]
	TIME [epoch: 18.3 sec]
EPOCH 170/500:
	Training over batches...
		[batch 4/4] avg loss: 0.014410335460549654		[learning rate: 0.001591]
	Learning Rate: 0.00159099
	LOSS [training: 0.014410335460549654 | validation: 0.063295613903886]
	TIME [epoch: 18.4 sec]
EPOCH 171/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01846743182827291		[learning rate: 0.0015668]
	Learning Rate: 0.00156675
	LOSS [training: 0.01846743182827291 | validation: 0.003369002539402511]
	TIME [epoch: 18.3 sec]
EPOCH 172/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004026888166940416		[learning rate: 0.0015429]
	Learning Rate: 0.00154288
	LOSS [training: 0.004026888166940416 | validation: 0.011160242067517395]
	TIME [epoch: 18.3 sec]
EPOCH 173/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006279229295846726		[learning rate: 0.0015194]
	Learning Rate: 0.00151938
	LOSS [training: 0.006279229295846726 | validation: 0.01366371660504507]
	TIME [epoch: 18.3 sec]
EPOCH 174/500:
	Training over batches...
		[batch 4/4] avg loss: 0.038695007792582184		[learning rate: 0.0014962]
	Learning Rate: 0.00149624
	LOSS [training: 0.038695007792582184 | validation: 0.013471026643704503]
	TIME [epoch: 18.3 sec]
EPOCH 175/500:
	Training over batches...
		[batch 4/4] avg loss: 0.015261580992313645		[learning rate: 0.0014734]
	Learning Rate: 0.00147344
	LOSS [training: 0.015261580992313645 | validation: 0.011152863507659632]
	TIME [epoch: 18.3 sec]
EPOCH 176/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002235795283484273		[learning rate: 0.001451]
	Learning Rate: 0.001451
	LOSS [training: 0.0002235795283484273 | validation: -0.0065344090468404566]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_176.pth
	Model improved!!!
EPOCH 177/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005320109560297018		[learning rate: 0.0014289]
	Learning Rate: 0.00142889
	LOSS [training: 0.005320109560297018 | validation: 0.006141980700802137]
	TIME [epoch: 18.3 sec]
EPOCH 178/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0033581529431533745		[learning rate: 0.0014071]
	Learning Rate: 0.00140713
	LOSS [training: 0.0033581529431533745 | validation: 0.006032690997129464]
	TIME [epoch: 18.3 sec]
EPOCH 179/500:
	Training over batches...
		[batch 4/4] avg loss: 0.012362574502679295		[learning rate: 0.0013857]
	Learning Rate: 0.00138569
	LOSS [training: 0.012362574502679295 | validation: 0.011785036211028234]
	TIME [epoch: 18.3 sec]
EPOCH 180/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01857323653880038		[learning rate: 0.0013646]
	Learning Rate: 0.00136458
	LOSS [training: 0.01857323653880038 | validation: 0.01629573275740549]
	TIME [epoch: 18.3 sec]
EPOCH 181/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01923381029957787		[learning rate: 0.0013438]
	Learning Rate: 0.0013438
	LOSS [training: 0.01923381029957787 | validation: 0.009001564719000398]
	TIME [epoch: 18.3 sec]
EPOCH 182/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0024787007403986776		[learning rate: 0.0013233]
	Learning Rate: 0.00132333
	LOSS [training: -0.0024787007403986776 | validation: -0.0025314077864557302]
	TIME [epoch: 18.3 sec]
EPOCH 183/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009618769533910531		[learning rate: 0.0013032]
	Learning Rate: 0.00130317
	LOSS [training: 0.0009618769533910531 | validation: 0.013494065627415132]
	TIME [epoch: 18.3 sec]
EPOCH 184/500:
	Training over batches...
		[batch 4/4] avg loss: 0.017472951110883556		[learning rate: 0.0012833]
	Learning Rate: 0.00128332
	LOSS [training: 0.017472951110883556 | validation: -0.000162088919098465]
	TIME [epoch: 18.3 sec]
EPOCH 185/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01063687722784635		[learning rate: 0.0012638]
	Learning Rate: 0.00126377
	LOSS [training: 0.01063687722784635 | validation: 0.004412504265520949]
	TIME [epoch: 18.3 sec]
EPOCH 186/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0029106605634114297		[learning rate: 0.0012445]
	Learning Rate: 0.00124451
	LOSS [training: 0.0029106605634114297 | validation: -0.001102474697203073]
	TIME [epoch: 18.3 sec]
EPOCH 187/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005871036149077539		[learning rate: 0.0012256]
	Learning Rate: 0.00122556
	LOSS [training: 0.0005871036149077539 | validation: 0.041513470744427156]
	TIME [epoch: 18.3 sec]
EPOCH 188/500:
	Training over batches...
		[batch 4/4] avg loss: 0.023987435909765098		[learning rate: 0.0012069]
	Learning Rate: 0.00120689
	LOSS [training: 0.023987435909765098 | validation: 0.006989553613161069]
	TIME [epoch: 18.3 sec]
EPOCH 189/500:
	Training over batches...
		[batch 4/4] avg loss: 0.009293721446987098		[learning rate: 0.0011885]
	Learning Rate: 0.0011885
	LOSS [training: 0.009293721446987098 | validation: -0.000945257995854807]
	TIME [epoch: 18.3 sec]
EPOCH 190/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007725519545545637		[learning rate: 0.0011704]
	Learning Rate: 0.0011704
	LOSS [training: 0.007725519545545637 | validation: 0.02591867273976685]
	TIME [epoch: 18.3 sec]
EPOCH 191/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005361330213873687		[learning rate: 0.0011526]
	Learning Rate: 0.00115257
	LOSS [training: 0.005361330213873687 | validation: 0.00672345189937215]
	TIME [epoch: 18.3 sec]
EPOCH 192/500:
	Training over batches...
		[batch 4/4] avg loss: -0.001839525835154748		[learning rate: 0.001135]
	Learning Rate: 0.00113501
	LOSS [training: -0.001839525835154748 | validation: 0.007420926662567187]
	TIME [epoch: 18.3 sec]
EPOCH 193/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008666543485777747		[learning rate: 0.0011177]
	Learning Rate: 0.00111772
	LOSS [training: -0.008666543485777747 | validation: 0.008290420618713351]
	TIME [epoch: 18.3 sec]
EPOCH 194/500:
	Training over batches...
		[batch 4/4] avg loss: 0.016829176422962014		[learning rate: 0.0011007]
	Learning Rate: 0.00110069
	LOSS [training: 0.016829176422962014 | validation: 0.01354074798220559]
	TIME [epoch: 18.3 sec]
EPOCH 195/500:
	Training over batches...
		[batch 4/4] avg loss: -0.003009191194665058		[learning rate: 0.0010839]
	Learning Rate: 0.00108393
	LOSS [training: -0.003009191194665058 | validation: 0.0006059369364269864]
	TIME [epoch: 18.3 sec]
EPOCH 196/500:
	Training over batches...
		[batch 4/4] avg loss: 0.021309148276162437		[learning rate: 0.0010674]
	Learning Rate: 0.00106741
	LOSS [training: 0.021309148276162437 | validation: 0.07174030552686744]
	TIME [epoch: 18.3 sec]
EPOCH 197/500:
	Training over batches...
		[batch 4/4] avg loss: 0.027333604302788986		[learning rate: 0.0010512]
	Learning Rate: 0.00105115
	LOSS [training: 0.027333604302788986 | validation: 0.005382786089862876]
	TIME [epoch: 18.3 sec]
EPOCH 198/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008285605279040686		[learning rate: 0.0010351]
	Learning Rate: 0.00103514
	LOSS [training: -0.008285605279040686 | validation: -0.004808708865501944]
	TIME [epoch: 18.3 sec]
EPOCH 199/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0030355463790512323		[learning rate: 0.0010194]
	Learning Rate: 0.00101937
	LOSS [training: -0.0030355463790512323 | validation: 0.005007012514515826]
	TIME [epoch: 18.3 sec]
EPOCH 200/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0041036095863391225		[learning rate: 0.0010038]
	Learning Rate: 0.00100384
	LOSS [training: -0.0041036095863391225 | validation: -0.011688855677513627]
	TIME [epoch: 18.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_200.pth
	Model improved!!!
EPOCH 201/500:
	Training over batches...
		[batch 4/4] avg loss: 0.019736220344941213		[learning rate: 0.00098855]
	Learning Rate: 0.000988553
	LOSS [training: 0.019736220344941213 | validation: 0.02271249744283565]
	TIME [epoch: 18.4 sec]
EPOCH 202/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003753516121177542		[learning rate: 0.00097349]
	Learning Rate: 0.000973494
	LOSS [training: 0.003753516121177542 | validation: -0.00481026762283527]
	TIME [epoch: 18.3 sec]
EPOCH 203/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004485208185097451		[learning rate: 0.00095866]
	Learning Rate: 0.000958664
	LOSS [training: 0.004485208185097451 | validation: 0.0029319863211344833]
	TIME [epoch: 18.3 sec]
EPOCH 204/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007005915465434784		[learning rate: 0.00094406]
	Learning Rate: 0.000944061
	LOSS [training: 0.007005915465434784 | validation: 0.01366324617842157]
	TIME [epoch: 18.3 sec]
EPOCH 205/500:
	Training over batches...
		[batch 4/4] avg loss: -0.001147567496258227		[learning rate: 0.00092968]
	Learning Rate: 0.00092968
	LOSS [training: -0.001147567496258227 | validation: -0.0030699117206047233]
	TIME [epoch: 18.3 sec]
EPOCH 206/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006646640255540588		[learning rate: 0.00091552]
	Learning Rate: 0.000915518
	LOSS [training: -0.006646640255540588 | validation: 0.002372820725617942]
	TIME [epoch: 18.3 sec]
EPOCH 207/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0034152280675686423		[learning rate: 0.00090157]
	Learning Rate: 0.000901571
	LOSS [training: -0.0034152280675686423 | validation: 0.0012383144742681936]
	TIME [epoch: 18.3 sec]
EPOCH 208/500:
	Training over batches...
		[batch 4/4] avg loss: -0.002262962122103313		[learning rate: 0.00088784]
	Learning Rate: 0.000887837
	LOSS [training: -0.002262962122103313 | validation: 0.014675641408949565]
	TIME [epoch: 18.3 sec]
EPOCH 209/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0009547317800753536		[learning rate: 0.00087431]
	Learning Rate: 0.000874312
	LOSS [training: -0.0009547317800753536 | validation: -0.0010567187321127158]
	TIME [epoch: 18.3 sec]
EPOCH 210/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006855132685139801		[learning rate: 0.00086099]
	Learning Rate: 0.000860994
	LOSS [training: -0.006855132685139801 | validation: -0.004571739621650945]
	TIME [epoch: 18.3 sec]
EPOCH 211/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0014621732657727029		[learning rate: 0.00084788]
	Learning Rate: 0.000847878
	LOSS [training: -0.0014621732657727029 | validation: -0.009775939125851327]
	TIME [epoch: 18.3 sec]
EPOCH 212/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0078561491770257		[learning rate: 0.00083496]
	Learning Rate: 0.000834962
	LOSS [training: -0.0078561491770257 | validation: 0.0005300983778551288]
	TIME [epoch: 18.3 sec]
EPOCH 213/500:
	Training over batches...
		[batch 4/4] avg loss: -0.004026544573502076		[learning rate: 0.00082224]
	Learning Rate: 0.000822243
	LOSS [training: -0.004026544573502076 | validation: 0.006740636548842301]
	TIME [epoch: 18.3 sec]
EPOCH 214/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00396629868810976		[learning rate: 0.00080972]
	Learning Rate: 0.000809717
	LOSS [training: -0.00396629868810976 | validation: 0.0032277438840137805]
	TIME [epoch: 18.3 sec]
EPOCH 215/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00488236815197537		[learning rate: 0.00079738]
	Learning Rate: 0.000797382
	LOSS [training: -0.00488236815197537 | validation: 0.002253013829756558]
	TIME [epoch: 18.3 sec]
EPOCH 216/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006204329283871412		[learning rate: 0.00078524]
	Learning Rate: 0.000785236
	LOSS [training: -0.006204329283871412 | validation: 0.005857190703280126]
	TIME [epoch: 18.4 sec]
EPOCH 217/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007287201061969466		[learning rate: 0.00077327]
	Learning Rate: 0.000773274
	LOSS [training: 0.007287201061969466 | validation: 0.011962142381861635]
	TIME [epoch: 18.3 sec]
EPOCH 218/500:
	Training over batches...
		[batch 4/4] avg loss: 8.498827193247484e-05		[learning rate: 0.00076149]
	Learning Rate: 0.000761494
	LOSS [training: 8.498827193247484e-05 | validation: 0.004661830092548976]
	TIME [epoch: 18.4 sec]
EPOCH 219/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0038509003769189344		[learning rate: 0.00074989]
	Learning Rate: 0.000749894
	LOSS [training: -0.0038509003769189344 | validation: 0.008380683705768097]
	TIME [epoch: 18.3 sec]
EPOCH 220/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019375269876521935		[learning rate: 0.00073847]
	Learning Rate: 0.000738471
	LOSS [training: 0.0019375269876521935 | validation: 0.004786488393142295]
	TIME [epoch: 18.4 sec]
EPOCH 221/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019682387288231466		[learning rate: 0.00072722]
	Learning Rate: 0.000727221
	LOSS [training: 0.0019682387288231466 | validation: 0.00014036185301008193]
	TIME [epoch: 18.3 sec]
EPOCH 222/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0028658712162983953		[learning rate: 0.00071614]
	Learning Rate: 0.000716143
	LOSS [training: 0.0028658712162983953 | validation: 0.01291246883348598]
	TIME [epoch: 18.4 sec]
EPOCH 223/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006491270843209216		[learning rate: 0.00070523]
	Learning Rate: 0.000705234
	LOSS [training: -0.006491270843209216 | validation: 0.0011207575754397265]
	TIME [epoch: 18.3 sec]
EPOCH 224/500:
	Training over batches...
		[batch 4/4] avg loss: -0.007347920992118874		[learning rate: 0.00069449]
	Learning Rate: 0.000694491
	LOSS [training: -0.007347920992118874 | validation: 0.0010596921156161142]
	TIME [epoch: 18.4 sec]
EPOCH 225/500:
	Training over batches...
		[batch 4/4] avg loss: -0.007472856666379102		[learning rate: 0.00068391]
	Learning Rate: 0.000683912
	LOSS [training: -0.007472856666379102 | validation: 0.0027217842726400257]
	TIME [epoch: 18.3 sec]
EPOCH 226/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004677355112098679		[learning rate: 0.00067349]
	Learning Rate: 0.000673493
	LOSS [training: 0.004677355112098679 | validation: 0.0056967353278662355]
	TIME [epoch: 18.4 sec]
EPOCH 227/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00040126214926134736		[learning rate: 0.00066323]
	Learning Rate: 0.000663234
	LOSS [training: 0.00040126214926134736 | validation: -0.0019644017716779283]
	TIME [epoch: 18.3 sec]
EPOCH 228/500:
	Training over batches...
		[batch 4/4] avg loss: -0.003560115499314223		[learning rate: 0.00065313]
	Learning Rate: 0.00065313
	LOSS [training: -0.003560115499314223 | validation: 0.007645185389875931]
	TIME [epoch: 18.4 sec]
EPOCH 229/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0013734074634322159		[learning rate: 0.00064318]
	Learning Rate: 0.000643181
	LOSS [training: -0.0013734074634322159 | validation: 0.020566605863815472]
	TIME [epoch: 18.3 sec]
EPOCH 230/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005422455461880206		[learning rate: 0.00063338]
	Learning Rate: 0.000633383
	LOSS [training: 0.005422455461880206 | validation: 0.002752011342537612]
	TIME [epoch: 18.4 sec]
EPOCH 231/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004153680127182932		[learning rate: 0.00062373]
	Learning Rate: 0.000623735
	LOSS [training: 0.004153680127182932 | validation: 0.001413384852646693]
	TIME [epoch: 18.3 sec]
EPOCH 232/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006349892036260874		[learning rate: 0.00061423]
	Learning Rate: 0.000614233
	LOSS [training: 0.006349892036260874 | validation: 0.019020839629569103]
	TIME [epoch: 18.4 sec]
EPOCH 233/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003358638831067708		[learning rate: 0.00060488]
	Learning Rate: 0.000604876
	LOSS [training: 0.003358638831067708 | validation: 0.009357468873146823]
	TIME [epoch: 18.3 sec]
EPOCH 234/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0009739834547262843		[learning rate: 0.00059566]
	Learning Rate: 0.000595662
	LOSS [training: -0.0009739834547262843 | validation: 0.0026021919752827537]
	TIME [epoch: 18.3 sec]
EPOCH 235/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006974467951158562		[learning rate: 0.00058659]
	Learning Rate: 0.000586588
	LOSS [training: -0.006974467951158562 | validation: -0.010429949000520536]
	TIME [epoch: 18.3 sec]
EPOCH 236/500:
	Training over batches...
		[batch 4/4] avg loss: -0.002710261556265691		[learning rate: 0.00057765]
	Learning Rate: 0.000577652
	LOSS [training: -0.002710261556265691 | validation: 0.01347848970161355]
	TIME [epoch: 18.3 sec]
EPOCH 237/500:
	Training over batches...
		[batch 4/4] avg loss: -1.0286731074047738e-05		[learning rate: 0.00056885]
	Learning Rate: 0.000568853
	LOSS [training: -1.0286731074047738e-05 | validation: -0.004536272137139128]
	TIME [epoch: 18.3 sec]
EPOCH 238/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0017188451094825175		[learning rate: 0.00056019]
	Learning Rate: 0.000560187
	LOSS [training: -0.0017188451094825175 | validation: -0.0006749395393479858]
	TIME [epoch: 18.3 sec]
EPOCH 239/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008735496776537939		[learning rate: 0.00055165]
	Learning Rate: 0.000551654
	LOSS [training: -0.008735496776537939 | validation: 0.000776696460979555]
	TIME [epoch: 18.3 sec]
EPOCH 240/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005812319045435927		[learning rate: 0.00054325]
	Learning Rate: 0.00054325
	LOSS [training: -0.005812319045435927 | validation: -0.0033521253279750057]
	TIME [epoch: 18.3 sec]
EPOCH 241/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005429294363633826		[learning rate: 0.00053497]
	Learning Rate: 0.000534975
	LOSS [training: -0.005429294363633826 | validation: 0.009413308814978654]
	TIME [epoch: 18.3 sec]
EPOCH 242/500:
	Training over batches...
		[batch 4/4] avg loss: -0.007994466345649664		[learning rate: 0.00052683]
	Learning Rate: 0.000526825
	LOSS [training: -0.007994466345649664 | validation: -0.003948557296727012]
	TIME [epoch: 18.3 sec]
EPOCH 243/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006635631485818035		[learning rate: 0.0005188]
	Learning Rate: 0.0005188
	LOSS [training: -0.006635631485818035 | validation: -0.001776666679880949]
	TIME [epoch: 18.3 sec]
EPOCH 244/500:
	Training over batches...
		[batch 4/4] avg loss: -0.007889593847283223		[learning rate: 0.0005109]
	Learning Rate: 0.000510897
	LOSS [training: -0.007889593847283223 | validation: 0.012827351846648526]
	TIME [epoch: 18.3 sec]
EPOCH 245/500:
	Training over batches...
		[batch 4/4] avg loss: -0.001796416455005898		[learning rate: 0.00050311]
	Learning Rate: 0.000503114
	LOSS [training: -0.001796416455005898 | validation: -0.007401988078936333]
	TIME [epoch: 18.3 sec]
EPOCH 246/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00754881541998758		[learning rate: 0.00049545]
	Learning Rate: 0.00049545
	LOSS [training: -0.00754881541998758 | validation: 6.873992903909691e-05]
	TIME [epoch: 18.3 sec]
EPOCH 247/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008721906092428646		[learning rate: 0.0004879]
	Learning Rate: 0.000487903
	LOSS [training: -0.008721906092428646 | validation: 0.004351800058759708]
	TIME [epoch: 18.4 sec]
EPOCH 248/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005989918876873414		[learning rate: 0.00048047]
	Learning Rate: 0.00048047
	LOSS [training: -0.005989918876873414 | validation: 0.003575493434799959]
	TIME [epoch: 18.3 sec]
EPOCH 249/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006768764339669347		[learning rate: 0.00047315]
	Learning Rate: 0.000473151
	LOSS [training: -0.006768764339669347 | validation: -0.001492215102912041]
	TIME [epoch: 18.4 sec]
EPOCH 250/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00500115740984961		[learning rate: 0.00046594]
	Learning Rate: 0.000465944
	LOSS [training: -0.00500115740984961 | validation: 0.0009178891415400621]
	TIME [epoch: 18.3 sec]
EPOCH 251/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005541402358513666		[learning rate: 0.00045885]
	Learning Rate: 0.000458846
	LOSS [training: -0.005541402358513666 | validation: -0.00312422919331791]
	TIME [epoch: 144 sec]
EPOCH 252/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0050708941273029055		[learning rate: 0.00045186]
	Learning Rate: 0.000451856
	LOSS [training: -0.0050708941273029055 | validation: 0.0011136501845119438]
	TIME [epoch: 40.6 sec]
EPOCH 253/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005003135736191687		[learning rate: 0.00044497]
	Learning Rate: 0.000444973
	LOSS [training: -0.005003135736191687 | validation: -0.010693733980648067]
	TIME [epoch: 40.5 sec]
EPOCH 254/500:
	Training over batches...
		[batch 4/4] avg loss: -0.007747857663367759		[learning rate: 0.00043819]
	Learning Rate: 0.000438194
	LOSS [training: -0.007747857663367759 | validation: 0.01676415234720543]
	TIME [epoch: 40.6 sec]
EPOCH 255/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0032456534830625364		[learning rate: 0.00043152]
	Learning Rate: 0.000431519
	LOSS [training: -0.0032456534830625364 | validation: -0.007197602702280676]
	TIME [epoch: 40.5 sec]
EPOCH 256/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008801654085741618		[learning rate: 0.00042495]
	Learning Rate: 0.000424946
	LOSS [training: -0.008801654085741618 | validation: -0.0028055667284728764]
	TIME [epoch: 40.5 sec]
EPOCH 257/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0030110167085060356		[learning rate: 0.00041847]
	Learning Rate: 0.000418472
	LOSS [training: -0.0030110167085060356 | validation: 0.018800768758154576]
	TIME [epoch: 40.5 sec]
EPOCH 258/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005324450964269889		[learning rate: 0.0004121]
	Learning Rate: 0.000412098
	LOSS [training: 0.005324450964269889 | validation: -0.002043186764725782]
	TIME [epoch: 40.5 sec]
EPOCH 259/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006157173064419599		[learning rate: 0.00040582]
	Learning Rate: 0.00040582
	LOSS [training: -0.006157173064419599 | validation: -0.00551165161676401]
	TIME [epoch: 40.5 sec]
EPOCH 260/500:
	Training over batches...
		[batch 4/4] avg loss: -0.004632964569771506		[learning rate: 0.00039964]
	Learning Rate: 0.000399638
	LOSS [training: -0.004632964569771506 | validation: 0.0006056172938822107]
	TIME [epoch: 40.5 sec]
EPOCH 261/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0015727247955364065		[learning rate: 0.00039355]
	Learning Rate: 0.00039355
	LOSS [training: -0.0015727247955364065 | validation: 0.009159183614371343]
	TIME [epoch: 40.5 sec]
EPOCH 262/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0011616946857617829		[learning rate: 0.00038755]
	Learning Rate: 0.000387555
	LOSS [training: -0.0011616946857617829 | validation: 0.00291397998263708]
	TIME [epoch: 40.5 sec]
EPOCH 263/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00424732962720803		[learning rate: 0.00038165]
	Learning Rate: 0.000381651
	LOSS [training: -0.00424732962720803 | validation: -0.0069023168510753945]
	TIME [epoch: 40.6 sec]
EPOCH 264/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005486088808203711		[learning rate: 0.00037584]
	Learning Rate: 0.000375837
	LOSS [training: -0.005486088808203711 | validation: 0.006158748264100163]
	TIME [epoch: 40.5 sec]
EPOCH 265/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008141036254008859		[learning rate: 0.00037011]
	Learning Rate: 0.000370112
	LOSS [training: 0.0008141036254008859 | validation: 0.00995858938018503]
	TIME [epoch: 40.5 sec]
EPOCH 266/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00274520616724179		[learning rate: 0.00036447]
	Learning Rate: 0.000364474
	LOSS [training: -0.00274520616724179 | validation: 0.002571866345262751]
	TIME [epoch: 40.5 sec]
EPOCH 267/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008057592440722223		[learning rate: 0.00035892]
	Learning Rate: 0.000358922
	LOSS [training: -0.008057592440722223 | validation: 0.007701370716618091]
	TIME [epoch: 40.5 sec]
EPOCH 268/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0008272606827403882		[learning rate: 0.00035345]
	Learning Rate: 0.000353454
	LOSS [training: -0.0008272606827403882 | validation: 0.019218781207089343]
	TIME [epoch: 40.5 sec]
EPOCH 269/500:
	Training over batches...
		[batch 4/4] avg loss: 0.009454584609237088		[learning rate: 0.00034807]
	Learning Rate: 0.00034807
	LOSS [training: 0.009454584609237088 | validation: 0.009964931131679166]
	TIME [epoch: 40.5 sec]
EPOCH 270/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005587425953634772		[learning rate: 0.00034277]
	Learning Rate: 0.000342768
	LOSS [training: -0.005587425953634772 | validation: 0.0009629945656097205]
	TIME [epoch: 40.5 sec]
EPOCH 271/500:
	Training over batches...
		[batch 4/4] avg loss: -0.010622458566905999		[learning rate: 0.00033755]
	Learning Rate: 0.000337546
	LOSS [training: -0.010622458566905999 | validation: 0.005494801853031285]
	TIME [epoch: 40.6 sec]
EPOCH 272/500:
	Training over batches...
		[batch 4/4] avg loss: -0.011764968888217969		[learning rate: 0.0003324]
	Learning Rate: 0.000332404
	LOSS [training: -0.011764968888217969 | validation: -0.016359455787772554]
	TIME [epoch: 40.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_272.pth
	Model improved!!!
EPOCH 273/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006129823799252784		[learning rate: 0.00032734]
	Learning Rate: 0.000327341
	LOSS [training: -0.006129823799252784 | validation: 0.00101890561161076]
	TIME [epoch: 40.5 sec]
EPOCH 274/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0022592349293140694		[learning rate: 0.00032235]
	Learning Rate: 0.000322354
	LOSS [training: -0.0022592349293140694 | validation: -0.0020855506194980577]
	TIME [epoch: 40.5 sec]
EPOCH 275/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006885589352384063		[learning rate: 0.00031744]
	Learning Rate: 0.000317444
	LOSS [training: -0.006885589352384063 | validation: -0.006983851842282001]
	TIME [epoch: 40.5 sec]
EPOCH 276/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00889065091771665		[learning rate: 0.00031261]
	Learning Rate: 0.000312608
	LOSS [training: -0.00889065091771665 | validation: 0.0012838474118026565]
	TIME [epoch: 40.5 sec]
EPOCH 277/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005220744221241363		[learning rate: 0.00030785]
	Learning Rate: 0.000307846
	LOSS [training: -0.005220744221241363 | validation: 0.009969960537123122]
	TIME [epoch: 40.5 sec]
EPOCH 278/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00857944505971252		[learning rate: 0.00030316]
	Learning Rate: 0.000303156
	LOSS [training: -0.00857944505971252 | validation: -0.005733128306589651]
	TIME [epoch: 40.5 sec]
EPOCH 279/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005030273108575606		[learning rate: 0.00029854]
	Learning Rate: 0.000298538
	LOSS [training: -0.005030273108575606 | validation: -0.002616407246956233]
	TIME [epoch: 40.5 sec]
EPOCH 280/500:
	Training over batches...
		[batch 4/4] avg loss: -0.009239296382534703		[learning rate: 0.00029399]
	Learning Rate: 0.000293991
	LOSS [training: -0.009239296382534703 | validation: 0.007587280174297583]
	TIME [epoch: 40.5 sec]
EPOCH 281/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0033503694789175035		[learning rate: 0.00028951]
	Learning Rate: 0.000289512
	LOSS [training: -0.0033503694789175035 | validation: 0.001067894002369494]
	TIME [epoch: 40.5 sec]
EPOCH 282/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008239283627785615		[learning rate: 0.0002851]
	Learning Rate: 0.000285102
	LOSS [training: -0.008239283627785615 | validation: -0.0027209267425993174]
	TIME [epoch: 40.5 sec]
EPOCH 283/500:
	Training over batches...
		[batch 4/4] avg loss: -0.009154538215830994		[learning rate: 0.00028076]
	Learning Rate: 0.000280759
	LOSS [training: -0.009154538215830994 | validation: -0.0007179859450807896]
	TIME [epoch: 40.5 sec]
EPOCH 284/500:
	Training over batches...
		[batch 4/4] avg loss: -0.007455611943622828		[learning rate: 0.00027648]
	Learning Rate: 0.000276482
	LOSS [training: -0.007455611943622828 | validation: 0.009237988336993132]
	TIME [epoch: 40.5 sec]
EPOCH 285/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004397622395221632		[learning rate: 0.00027227]
	Learning Rate: 0.00027227
	LOSS [training: 0.0004397622395221632 | validation: 0.002564300989433602]
	TIME [epoch: 40.5 sec]
EPOCH 286/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0071976030568966076		[learning rate: 0.00026812]
	Learning Rate: 0.000268123
	LOSS [training: -0.0071976030568966076 | validation: 0.0023421775684578016]
	TIME [epoch: 40.5 sec]
EPOCH 287/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020437353466423525		[learning rate: 0.00026404]
	Learning Rate: 0.000264038
	LOSS [training: 0.0020437353466423525 | validation: 0.0021935950548733563]
	TIME [epoch: 40.6 sec]
EPOCH 288/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005648179466883716		[learning rate: 0.00026002]
	Learning Rate: 0.000260016
	LOSS [training: -0.005648179466883716 | validation: 0.0037548162018541014]
	TIME [epoch: 40.5 sec]
EPOCH 289/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00900200798405316		[learning rate: 0.00025606]
	Learning Rate: 0.000256055
	LOSS [training: -0.00900200798405316 | validation: 0.001657173896274002]
	TIME [epoch: 40.5 sec]
EPOCH 290/500:
	Training over batches...
		[batch 4/4] avg loss: -0.007065263710384615		[learning rate: 0.00025215]
	Learning Rate: 0.000252154
	LOSS [training: -0.007065263710384615 | validation: 0.004142745570376702]
	TIME [epoch: 40.5 sec]
EPOCH 291/500:
	Training over batches...
		[batch 4/4] avg loss: -0.009243248991562562		[learning rate: 0.00024831]
	Learning Rate: 0.000248313
	LOSS [training: -0.009243248991562562 | validation: -0.005798596998798341]
	TIME [epoch: 40.5 sec]
EPOCH 292/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0072104902110526345		[learning rate: 0.00024453]
	Learning Rate: 0.000244531
	LOSS [training: -0.0072104902110526345 | validation: 0.00028485635355455716]
	TIME [epoch: 40.5 sec]
EPOCH 293/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006502744115009015		[learning rate: 0.00024081]
	Learning Rate: 0.000240806
	LOSS [training: -0.006502744115009015 | validation: -0.0037256053062303617]
	TIME [epoch: 40.5 sec]
EPOCH 294/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0029442578603656156		[learning rate: 0.00023714]
	Learning Rate: 0.000237137
	LOSS [training: -0.0029442578603656156 | validation: -0.008092858396871797]
	TIME [epoch: 40.5 sec]
EPOCH 295/500:
	Training over batches...
		[batch 4/4] avg loss: -0.004531310648761418		[learning rate: 0.00023352]
	Learning Rate: 0.000233525
	LOSS [training: -0.004531310648761418 | validation: 0.0051864138907349085]
	TIME [epoch: 40.5 sec]
EPOCH 296/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013006352384152655		[learning rate: 0.00022997]
	Learning Rate: 0.000229968
	LOSS [training: 0.0013006352384152655 | validation: -0.0008804021288393651]
	TIME [epoch: 40.5 sec]
EPOCH 297/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0063544777914923915		[learning rate: 0.00022646]
	Learning Rate: 0.000226464
	LOSS [training: -0.0063544777914923915 | validation: 0.009712954919388162]
	TIME [epoch: 40.6 sec]
EPOCH 298/500:
	Training over batches...
		[batch 4/4] avg loss: -0.007073440684394256		[learning rate: 0.00022301]
	Learning Rate: 0.000223015
	LOSS [training: -0.007073440684394256 | validation: 0.0003106242324238161]
	TIME [epoch: 40.5 sec]
EPOCH 299/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006972025207947076		[learning rate: 0.00021962]
	Learning Rate: 0.000219617
	LOSS [training: -0.006972025207947076 | validation: -0.004558163051669869]
	TIME [epoch: 40.5 sec]
EPOCH 300/500:
	Training over batches...
		[batch 4/4] avg loss: -0.009672108485228969		[learning rate: 0.00021627]
	Learning Rate: 0.000216272
	LOSS [training: -0.009672108485228969 | validation: 0.003428905845496034]
	TIME [epoch: 40.5 sec]
EPOCH 301/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0011499908321105255		[learning rate: 0.00021298]
	Learning Rate: 0.000212977
	LOSS [training: -0.0011499908321105255 | validation: -0.0036327066639986394]
	TIME [epoch: 40.5 sec]
EPOCH 302/500:
	Training over batches...
		[batch 4/4] avg loss: -0.009979843299509586		[learning rate: 0.00020973]
	Learning Rate: 0.000209733
	LOSS [training: -0.009979843299509586 | validation: 0.0013007047808493335]
	TIME [epoch: 40.5 sec]
EPOCH 303/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00433612333548096		[learning rate: 0.00020654]
	Learning Rate: 0.000206538
	LOSS [training: -0.00433612333548096 | validation: 0.003221017084731435]
	TIME [epoch: 40.5 sec]
EPOCH 304/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008154447673051608		[learning rate: 0.00020339]
	Learning Rate: 0.000203392
	LOSS [training: -0.008154447673051608 | validation: 0.002353183567258009]
	TIME [epoch: 40.5 sec]
EPOCH 305/500:
	Training over batches...
		[batch 4/4] avg loss: -0.011722299217866407		[learning rate: 0.00020029]
	Learning Rate: 0.000200293
	LOSS [training: -0.011722299217866407 | validation: -0.004532152102436597]
	TIME [epoch: 40.6 sec]
EPOCH 306/500:
	Training over batches...
		[batch 4/4] avg loss: -0.011201259538116366		[learning rate: 0.00019724]
	Learning Rate: 0.000197242
	LOSS [training: -0.011201259538116366 | validation: 0.002304029135224375]
	TIME [epoch: 40.5 sec]
EPOCH 307/500:
	Training over batches...
		[batch 4/4] avg loss: -0.004609208925858536		[learning rate: 0.00019424]
	Learning Rate: 0.000194238
	LOSS [training: -0.004609208925858536 | validation: 0.0039153969450075815]
	TIME [epoch: 40.5 sec]
EPOCH 308/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008946106342062884		[learning rate: 0.00019128]
	Learning Rate: 0.000191279
	LOSS [training: -0.008946106342062884 | validation: -0.0034179515981923748]
	TIME [epoch: 40.5 sec]
EPOCH 309/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008909891267236784		[learning rate: 0.00018836]
	Learning Rate: 0.000188365
	LOSS [training: -0.008909891267236784 | validation: 0.005835995563383559]
	TIME [epoch: 40.5 sec]
EPOCH 310/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006132827580710156		[learning rate: 0.0001855]
	Learning Rate: 0.000185495
	LOSS [training: -0.006132827580710156 | validation: -0.008006686921955743]
	TIME [epoch: 40.5 sec]
EPOCH 311/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00748582131009269		[learning rate: 0.00018267]
	Learning Rate: 0.00018267
	LOSS [training: -0.00748582131009269 | validation: -0.0015372169184007707]
	TIME [epoch: 40.5 sec]
EPOCH 312/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0068716118083204086		[learning rate: 0.00017989]
	Learning Rate: 0.000179887
	LOSS [training: -0.0068716118083204086 | validation: -0.0051782324967993565]
	TIME [epoch: 40.5 sec]
EPOCH 313/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005621104540719193		[learning rate: 0.00017715]
	Learning Rate: 0.000177147
	LOSS [training: -0.005621104540719193 | validation: 0.0051826130840863915]
	TIME [epoch: 40.5 sec]
EPOCH 314/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006762993751259187		[learning rate: 0.00017445]
	Learning Rate: 0.000174448
	LOSS [training: -0.006762993751259187 | validation: -0.0028351084740559375]
	TIME [epoch: 40.5 sec]
EPOCH 315/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006127780614838494		[learning rate: 0.00017179]
	Learning Rate: 0.000171791
	LOSS [training: -0.006127780614838494 | validation: -0.005710472292551285]
	TIME [epoch: 40.5 sec]
EPOCH 316/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005236106932176406		[learning rate: 0.00016917]
	Learning Rate: 0.000169174
	LOSS [training: -0.005236106932176406 | validation: 0.0018691689409188912]
	TIME [epoch: 40.5 sec]
EPOCH 317/500:
	Training over batches...
		[batch 4/4] avg loss: -0.003693692025895216		[learning rate: 0.0001666]
	Learning Rate: 0.000166597
	LOSS [training: -0.003693692025895216 | validation: 0.009213013107102378]
	TIME [epoch: 40.5 sec]
EPOCH 318/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0030704856678385505		[learning rate: 0.00016406]
	Learning Rate: 0.000164059
	LOSS [training: -0.0030704856678385505 | validation: 0.0039003308760763747]
	TIME [epoch: 40.5 sec]
EPOCH 319/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00781569385450662		[learning rate: 0.00016156]
	Learning Rate: 0.00016156
	LOSS [training: -0.00781569385450662 | validation: 0.010701418675803553]
	TIME [epoch: 40.5 sec]
EPOCH 320/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0081062284206209		[learning rate: 0.0001591]
	Learning Rate: 0.000159099
	LOSS [training: -0.0081062284206209 | validation: 0.004873541623991982]
	TIME [epoch: 40.5 sec]
EPOCH 321/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008565824336794134		[learning rate: 0.00015668]
	Learning Rate: 0.000156675
	LOSS [training: -0.008565824336794134 | validation: 0.0015521222826587774]
	TIME [epoch: 40.5 sec]
EPOCH 322/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00568532188993166		[learning rate: 0.00015429]
	Learning Rate: 0.000154288
	LOSS [training: -0.00568532188993166 | validation: -0.005063060064462912]
	TIME [epoch: 40.5 sec]
EPOCH 323/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006543961428848668		[learning rate: 0.00015194]
	Learning Rate: 0.000151938
	LOSS [training: -0.006543961428848668 | validation: -0.005314810245158695]
	TIME [epoch: 40.5 sec]
EPOCH 324/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00596260729807752		[learning rate: 0.00014962]
	Learning Rate: 0.000149624
	LOSS [training: -0.00596260729807752 | validation: -0.0005220549497969591]
	TIME [epoch: 40.6 sec]
EPOCH 325/500:
	Training over batches...
		[batch 4/4] avg loss: -0.004755932151708912		[learning rate: 0.00014734]
	Learning Rate: 0.000147344
	LOSS [training: -0.004755932151708912 | validation: 0.002601199028884778]
	TIME [epoch: 40.5 sec]
EPOCH 326/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006628417955553469		[learning rate: 0.0001451]
	Learning Rate: 0.0001451
	LOSS [training: -0.006628417955553469 | validation: 0.003308579038569302]
	TIME [epoch: 40.5 sec]
EPOCH 327/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008953486181949386		[learning rate: 0.00014289]
	Learning Rate: 0.000142889
	LOSS [training: -0.008953486181949386 | validation: -0.003560396606684363]
	TIME [epoch: 40.5 sec]
EPOCH 328/500:
	Training over batches...
		[batch 4/4] avg loss: -0.010186003348898938		[learning rate: 0.00014071]
	Learning Rate: 0.000140713
	LOSS [training: -0.010186003348898938 | validation: 0.0003792873593107069]
	TIME [epoch: 40.6 sec]
EPOCH 329/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008413634440155034		[learning rate: 0.00013857]
	Learning Rate: 0.000138569
	LOSS [training: -0.008413634440155034 | validation: -0.004196510303362009]
	TIME [epoch: 40.5 sec]
EPOCH 330/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0057903006675430226		[learning rate: 0.00013646]
	Learning Rate: 0.000136458
	LOSS [training: -0.0057903006675430226 | validation: -0.0041735088215377105]
	TIME [epoch: 40.5 sec]
EPOCH 331/500:
	Training over batches...
		[batch 4/4] avg loss: -0.010368697333713131		[learning rate: 0.00013438]
	Learning Rate: 0.00013438
	LOSS [training: -0.010368697333713131 | validation: 0.0048888321724402]
	TIME [epoch: 40.5 sec]
EPOCH 332/500:
	Training over batches...
		[batch 4/4] avg loss: -0.011469569110320536		[learning rate: 0.00013233]
	Learning Rate: 0.000132333
	LOSS [training: -0.011469569110320536 | validation: -0.010226490328401335]
	TIME [epoch: 40.5 sec]
EPOCH 333/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00489413523588921		[learning rate: 0.00013032]
	Learning Rate: 0.000130317
	LOSS [training: -0.00489413523588921 | validation: -6.0538938445312517e-05]
	TIME [epoch: 40.5 sec]
EPOCH 334/500:
	Training over batches...
		[batch 4/4] avg loss: -0.010726919300311942		[learning rate: 0.00012833]
	Learning Rate: 0.000128332
	LOSS [training: -0.010726919300311942 | validation: 0.004784891488390077]
	TIME [epoch: 40.5 sec]
EPOCH 335/500:
	Training over batches...
		[batch 4/4] avg loss: -0.004312247762091477		[learning rate: 0.00012638]
	Learning Rate: 0.000126377
	LOSS [training: -0.004312247762091477 | validation: -0.0009391136203824847]
	TIME [epoch: 40.5 sec]
EPOCH 336/500:
	Training over batches...
		[batch 4/4] avg loss: -0.007492774299953831		[learning rate: 0.00012445]
	Learning Rate: 0.000124451
	LOSS [training: -0.007492774299953831 | validation: 0.00573659947859791]
	TIME [epoch: 40.5 sec]
EPOCH 337/500:
	Training over batches...
		[batch 4/4] avg loss: -0.006479884906194702		[learning rate: 0.00012256]
	Learning Rate: 0.000122556
	LOSS [training: -0.006479884906194702 | validation: 0.0044541413484855755]
	TIME [epoch: 40.5 sec]
EPOCH 338/500:
	Training over batches...
		[batch 4/4] avg loss: -0.007936706070881691		[learning rate: 0.00012069]
	Learning Rate: 0.000120689
	LOSS [training: -0.007936706070881691 | validation: -0.004937201964615407]
	TIME [epoch: 40.5 sec]
EPOCH 339/500:
	Training over batches...
		[batch 4/4] avg loss: -0.010373949702361277		[learning rate: 0.00011885]
	Learning Rate: 0.00011885
	LOSS [training: -0.010373949702361277 | validation: -0.007903327207938323]
	TIME [epoch: 40.5 sec]
EPOCH 340/500:
	Training over batches...
		[batch 4/4] avg loss: -0.009620045104773947		[learning rate: 0.00011704]
	Learning Rate: 0.00011704
	LOSS [training: -0.009620045104773947 | validation: -0.0021997746474667966]
	TIME [epoch: 40.5 sec]
EPOCH 341/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005564776247941102		[learning rate: 0.00011526]
	Learning Rate: 0.000115257
	LOSS [training: -0.005564776247941102 | validation: -0.0015596443526165195]
	TIME [epoch: 40.5 sec]
EPOCH 342/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008676621422245526		[learning rate: 0.0001135]
	Learning Rate: 0.000113501
	LOSS [training: -0.008676621422245526 | validation: -0.00954253743374479]
	TIME [epoch: 40.5 sec]
EPOCH 343/500:
	Training over batches...
		[batch 4/4] avg loss: -0.009111562024617052		[learning rate: 0.00011177]
	Learning Rate: 0.000111772
	LOSS [training: -0.009111562024617052 | validation: 0.0020759369233415534]
	TIME [epoch: 40.5 sec]
EPOCH 344/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008865586175480384		[learning rate: 0.00011007]
	Learning Rate: 0.000110069
	LOSS [training: -0.008865586175480384 | validation: -0.0051987509274683315]
	TIME [epoch: 40.5 sec]
EPOCH 345/500:
	Training over batches...
		[batch 4/4] avg loss: -0.008429736932685302		[learning rate: 0.00010839]
	Learning Rate: 0.000108393
	LOSS [training: -0.008429736932685302 | validation: -0.001791398826974661]
	TIME [epoch: 40.4 sec]
EPOCH 346/500:
	Training over batches...
		[batch 4/4] avg loss: -0.011124610359494174		[learning rate: 0.00010674]
	Learning Rate: 0.000106742
	LOSS [training: -0.011124610359494174 | validation: 0.005939672832460791]
	TIME [epoch: 40.4 sec]
EPOCH 347/500:
	Training over batches...
		[batch 4/4] avg loss: -0.009446796344285854		[learning rate: 0.00010512]
	Learning Rate: 0.000105115
	LOSS [training: -0.009446796344285854 | validation: 0.0005379420269685685]
	TIME [epoch: 40.4 sec]
EPOCH 348/500:
	Training over batches...
		[batch 4/4] avg loss: -0.007165096301932211		[learning rate: 0.00010351]
	Learning Rate: 0.000103514
	LOSS [training: -0.007165096301932211 | validation: -0.0004748016894796092]
	TIME [epoch: 40.5 sec]
EPOCH 349/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005829870864368334		[learning rate: 0.00010194]
	Learning Rate: 0.000101937
	LOSS [training: -0.005829870864368334 | validation: 0.004352759368715524]
	TIME [epoch: 40.5 sec]
EPOCH 350/500:
	Training over batches...
		[batch 4/4] avg loss: -0.007191400949970721		[learning rate: 0.00010038]
	Learning Rate: 0.000100385
	LOSS [training: -0.007191400949970721 | validation: 0.0015324208023307106]
	TIME [epoch: 40.5 sec]
EPOCH 351/500:
	Training over batches...
		[batch 4/4] avg loss: -0.004845593234711596		[learning rate: 9.8855e-05]
	Learning Rate: 9.88553e-05
	LOSS [training: -0.004845593234711596 | validation: 0.002441697357691867]
	TIME [epoch: 40.4 sec]
EPOCH 352/500:
	Training over batches...
		[batch 4/4] avg loss: -0.009421987138467178		[learning rate: 9.7349e-05]
	Learning Rate: 9.73494e-05
	LOSS [training: -0.009421987138467178 | validation: 0.002520782375592093]
	TIME [epoch: 40.4 sec]
EPOCH 353/500:
	Training over batches...
		[batch 4/4] avg loss: -0.010792193920158982		[learning rate: 9.5866e-05]
	Learning Rate: 9.58665e-05
	LOSS [training: -0.010792193920158982 | validation: 0.0005030569446992038]
	TIME [epoch: 40.5 sec]
EPOCH 354/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00675366512439905		[learning rate: 9.4406e-05]
	Learning Rate: 9.44061e-05
	LOSS [training: -0.00675366512439905 | validation: -0.00035899330661925635]
	TIME [epoch: 40.5 sec]
EPOCH 355/500:
	Training over batches...
		[batch 4/4] avg loss: -0.009101496046154577		[learning rate: 9.2968e-05]
	Learning Rate: 9.2968e-05
	LOSS [training: -0.009101496046154577 | validation: 0.002612656822505899]
	TIME [epoch: 40.5 sec]
EPOCH 356/500:
	Training over batches...
		[batch 4/4] avg loss: -0.012826655453705161		[learning rate: 9.1552e-05]
	Learning Rate: 9.15518e-05
	LOSS [training: -0.012826655453705161 | validation: -0.0067863806584989705]
	TIME [epoch: 40.5 sec]
EPOCH 357/500:
	Training over batches...
		[batch 4/4] avg loss: -0.010673694113474935		[learning rate: 9.0157e-05]
	Learning Rate: 9.01571e-05
	LOSS [training: -0.010673694113474935 | validation: -0.011149681031579527]
	TIME [epoch: 40.5 sec]
EPOCH 358/500:
	Training over batches...
		[batch 4/4] avg loss: -0.011199595834905228		[learning rate: 8.8784e-05]
	Learning Rate: 8.87837e-05
	LOSS [training: -0.011199595834905228 | validation: -0.008535881980280106]
	TIME [epoch: 40.5 sec]
EPOCH 359/500:
	Training over batches...
		[batch 4/4] avg loss: -0.012938322595415882		[learning rate: 8.7431e-05]
	Learning Rate: 8.74312e-05
	LOSS [training: -0.012938322595415882 | validation: -0.010704853479056833]
	TIME [epoch: 40.5 sec]
EPOCH 360/500:
	Training over batches...
		[batch 4/4] avg loss: -0.009464219601859348		[learning rate: 8.6099e-05]
	Learning Rate: 8.60994e-05
	LOSS [training: -0.009464219601859348 | validation: -0.0010057006700360243]
	TIME [epoch: 40.5 sec]
EPOCH 361/500:
	Training over batches...
		[batch 4/4] avg loss: -0.009211339126372299		[learning rate: 8.4788e-05]
	Learning Rate: 8.47878e-05
	LOSS [training: -0.009211339126372299 | validation: -0.00999801047021173]
	TIME [epoch: 40.5 sec]
EPOCH 362/500:
	Training over batches...
		[batch 4/4] avg loss: -0.009313981886211456		[learning rate: 8.3496e-05]
	Learning Rate: 8.34962e-05
	LOSS [training: -0.009313981886211456 | validation: 0.002474997484002469]
	TIME [epoch: 40.5 sec]
EPOCH 363/500:
	Training over batches...
		[batch 4/4] avg loss: -0.011538938482390718		[learning rate: 8.2224e-05]
	Learning Rate: 8.22243e-05
	LOSS [training: -0.011538938482390718 | validation: -0.0012673127931387695]
	TIME [epoch: 40.5 sec]
EPOCH 364/500:
	Training over batches...
		[batch 4/4] avg loss: -0.010249373652605566		[learning rate: 8.0972e-05]
	Learning Rate: 8.09717e-05
	LOSS [training: -0.010249373652605566 | validation: 0.0019699310091929054]
	TIME [epoch: 40.5 sec]
EPOCH 365/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00727564271305655		[learning rate: 7.9738e-05]
	Learning Rate: 7.97382e-05
	LOSS [training: -0.00727564271305655 | validation: -0.00031946552102668415]
	TIME [epoch: 40.5 sec]
EPOCH 366/500:
	Training over batches...
		[batch 4/4] avg loss: -0.012600341157715413		[learning rate: 7.8524e-05]
	Learning Rate: 7.85235e-05
	LOSS [training: -0.012600341157715413 | validation: -0.004915075016531482]
	TIME [epoch: 40.5 sec]
EPOCH 367/500:
	Training over batches...
		[batch 4/4] avg loss: -0.012325147888458931		[learning rate: 7.7327e-05]
	Learning Rate: 7.73274e-05
	LOSS [training: -0.012325147888458931 | validation: 0.006919333333721133]
	TIME [epoch: 40.5 sec]
EPOCH 368/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00877163519485556		[learning rate: 7.6149e-05]
	Learning Rate: 7.61494e-05
	LOSS [training: -0.00877163519485556 | validation: -0.002198141321236252]
	TIME [epoch: 40.5 sec]
EPOCH 369/500:
	Training over batches...
		[batch 4/4] avg loss: -0.010569717100765387		[learning rate: 7.4989e-05]
	Learning Rate: 7.49894e-05
	LOSS [training: -0.010569717100765387 | validation: -0.0017364511609213986]
	TIME [epoch: 40.5 sec]
EPOCH 370/500:
	Training over batches...
		[batch 4/4] avg loss: -0.007550045201564274		[learning rate: 7.3847e-05]
	Learning Rate: 7.38471e-05
	LOSS [training: -0.007550045201564274 | validation: 0.0003669420951159944]
	TIME [epoch: 40.6 sec]
EPOCH 371/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00734854155535365		[learning rate: 7.2722e-05]
	Learning Rate: 7.27221e-05
	LOSS [training: -0.00734854155535365 | validation: -0.004934004154307546]
	TIME [epoch: 40.6 sec]
EPOCH 372/500:
	Training over batches...
		[batch 4/4] avg loss: -0.01300189208926673		[learning rate: 7.1614e-05]
	Learning Rate: 7.16143e-05
	LOSS [training: -0.01300189208926673 | validation: 0.0020518088856497424]
	TIME [epoch: 40.5 sec]
EPOCH 373/500:
	Training over batches...
		[batch 4/4] avg loss: -0.005921604238886215		[learning rate: 7.0523e-05]
	Learning Rate: 7.05234e-05
	LOSS [training: -0.005921604238886215 | validation: -0.003789486800467038]
	TIME [epoch: 40.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_kl1_20240711_014001/states/model_algphiq_1a_v_kl1_373.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 8798.140 seconds.
