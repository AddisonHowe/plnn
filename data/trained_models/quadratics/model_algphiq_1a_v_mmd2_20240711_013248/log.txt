Args:
Namespace(name='model_algphiq_1a_v_mmd2', outdir='out/model_training/model_algphiq_1a_v_mmd2', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='quadratic', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=50, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[50, 100, 250], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4278320213

Training model...

Saving initial model state to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 5.01617005136606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.01617005136606 | validation: 5.047550526639105]
	TIME [epoch: 92 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 4.947463610787405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.947463610787405 | validation: 4.946799524857411]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 4.8372409356363875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8372409356363875 | validation: 4.781672438060979]
	TIME [epoch: 3.38 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 4.643249938131595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.643249938131595 | validation: 4.574286125444856]
	TIME [epoch: 3.38 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_4.pth
	Model improved!!!
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 4.4568622717655675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4568622717655675 | validation: 4.450915681113361]
	TIME [epoch: 3.41 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 4.282416558808832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.282416558808832 | validation: 4.091663786507697]
	TIME [epoch: 3.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 3.056215185566888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.056215185566888 | validation: 2.0958071817259065]
	TIME [epoch: 3.38 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7247929073283272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7247929073283272 | validation: 0.8764858037252907]
	TIME [epoch: 3.37 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1506552150653362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1506552150653362 | validation: 1.117357343793942]
	TIME [epoch: 3.38 sec]
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0527168807137635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0527168807137635 | validation: 1.0897603794574318]
	TIME [epoch: 3.37 sec]
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9767984249792885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9767984249792885 | validation: 0.9316692072996483]
	TIME [epoch: 3.37 sec]
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9593862280831054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9593862280831054 | validation: 0.8677600643051432]
	TIME [epoch: 3.38 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9215138575901594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9215138575901594 | validation: 0.8840113793524611]
	TIME [epoch: 3.38 sec]
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9013512295456325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9013512295456325 | validation: 0.8469874504460706]
	TIME [epoch: 3.37 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8907701591699697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8907701591699697 | validation: 0.8280092470954703]
	TIME [epoch: 3.37 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8591093742043572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8591093742043572 | validation: 0.8191219422615885]
	TIME [epoch: 3.36 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8597741284689528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8597741284689528 | validation: 0.7510709860014468]
	TIME [epoch: 3.36 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_17.pth
	Model improved!!!
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8000784528152252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8000784528152252 | validation: 0.7810072886985142]
	TIME [epoch: 3.38 sec]
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7921052569136925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7921052569136925 | validation: 0.7516936292755307]
	TIME [epoch: 3.4 sec]
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7668676414415398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7668676414415398 | validation: 0.7080755035028676]
	TIME [epoch: 3.37 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7374390347790154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7374390347790154 | validation: 0.6906832441305728]
	TIME [epoch: 3.37 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_21.pth
	Model improved!!!
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7406224723768187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7406224723768187 | validation: 0.6738911288731122]
	TIME [epoch: 3.36 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_22.pth
	Model improved!!!
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7001431227184257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001431227184257 | validation: 0.6761683265019482]
	TIME [epoch: 3.36 sec]
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7062418465282269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7062418465282269 | validation: 0.6642144265826593]
	TIME [epoch: 3.37 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6824981679554558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6824981679554558 | validation: 0.6419262071334765]
	TIME [epoch: 3.37 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_25.pth
	Model improved!!!
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6740358730767033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6740358730767033 | validation: 0.6493556886330173]
	TIME [epoch: 3.36 sec]
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 0.674696215550151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.674696215550151 | validation: 0.6456606016825974]
	TIME [epoch: 3.36 sec]
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6668320573005762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6668320573005762 | validation: 0.6350795219157306]
	TIME [epoch: 3.36 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6612183152829312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6612183152829312 | validation: 0.6311102930945363]
	TIME [epoch: 3.36 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6593894842633843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6593894842633843 | validation: 0.6423231566022238]
	TIME [epoch: 3.38 sec]
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6655911033302111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6655911033302111 | validation: 0.6601144723151984]
	TIME [epoch: 3.39 sec]
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6655325733194053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6655325733194053 | validation: 0.6406494343098262]
	TIME [epoch: 3.39 sec]
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6456162958361585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6456162958361585 | validation: 0.6307557878544101]
	TIME [epoch: 3.38 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6572767468940994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6572767468940994 | validation: 0.6612327738690087]
	TIME [epoch: 3.37 sec]
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6572000425514314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6572000425514314 | validation: 0.6579660972943773]
	TIME [epoch: 3.36 sec]
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 0.67063303354071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.67063303354071 | validation: 0.6348363232058495]
	TIME [epoch: 3.36 sec]
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6668056743957745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6668056743957745 | validation: 0.638007559033569]
	TIME [epoch: 3.36 sec]
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6560957830761915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6560957830761915 | validation: 0.6419635204410525]
	TIME [epoch: 3.36 sec]
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6671642896194391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6671642896194391 | validation: 0.6328918499272653]
	TIME [epoch: 3.36 sec]
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6625216265064244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6625216265064244 | validation: 0.647392440275185]
	TIME [epoch: 3.36 sec]
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6701163474702837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6701163474702837 | validation: 0.6495164208101603]
	TIME [epoch: 3.36 sec]
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6722332204089012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6722332204089012 | validation: 0.6341098196650171]
	TIME [epoch: 3.36 sec]
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6528015263538487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6528015263538487 | validation: 0.6505566578218258]
	TIME [epoch: 3.36 sec]
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 0.671114533845584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.671114533845584 | validation: 0.6364816151168811]
	TIME [epoch: 3.37 sec]
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6542230963957366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6542230963957366 | validation: 0.6151332529084719]
	TIME [epoch: 3.38 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_45.pth
	Model improved!!!
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6659010341763626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6659010341763626 | validation: 0.6457454443947557]
	TIME [epoch: 3.38 sec]
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6677288588463193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6677288588463193 | validation: 0.6590944428874675]
	TIME [epoch: 3.37 sec]
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6555730006418412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6555730006418412 | validation: 0.645179588789577]
	TIME [epoch: 3.37 sec]
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6652347978893693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6652347978893693 | validation: 0.6365233425999612]
	TIME [epoch: 3.36 sec]
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6612232150551556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6612232150551556 | validation: 0.650982831592269]
	TIME [epoch: 3.36 sec]
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6585408090366096		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 0.6585408090366096 | validation: 0.6291813529540324]
	TIME [epoch: 96.5 sec]
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6640196522327675		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 0.6640196522327675 | validation: 0.645602959104536]
	TIME [epoch: 6.64 sec]
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6504278848966527		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 0.6504278848966527 | validation: 0.6578129155578787]
	TIME [epoch: 6.6 sec]
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6603431982074337		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 0.6603431982074337 | validation: 0.6395424492865331]
	TIME [epoch: 6.59 sec]
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6441741248931548		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 0.6441741248931548 | validation: 0.6148107361565383]
	TIME [epoch: 6.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_55.pth
	Model improved!!!
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6540155340059753		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 0.6540155340059753 | validation: 0.6231168523591932]
	TIME [epoch: 6.6 sec]
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6380107081921418		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 0.6380107081921418 | validation: 0.6265577919274217]
	TIME [epoch: 6.58 sec]
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6258398599612163		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 0.6258398599612163 | validation: 0.6157415373888881]
	TIME [epoch: 6.58 sec]
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6266386157217947		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 0.6266386157217947 | validation: 0.6213553317802587]
	TIME [epoch: 6.58 sec]
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6193129088875707		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.6193129088875707 | validation: 0.6027564918214405]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_60.pth
	Model improved!!!
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6158349796162758		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 0.6158349796162758 | validation: 0.6032342415187131]
	TIME [epoch: 6.6 sec]
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6177856896028169		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 0.6177856896028169 | validation: 0.5938384216629171]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_62.pth
	Model improved!!!
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6194500371572822		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 0.6194500371572822 | validation: 0.6270981847489472]
	TIME [epoch: 6.58 sec]
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6057438922597772		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 0.6057438922597772 | validation: 0.6086408575504352]
	TIME [epoch: 6.57 sec]
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6254090683242026		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 0.6254090683242026 | validation: 0.5984374842085947]
	TIME [epoch: 6.57 sec]
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6093303855330108		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 0.6093303855330108 | validation: 0.6052283328418122]
	TIME [epoch: 6.56 sec]
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6052206042593582		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 0.6052206042593582 | validation: 0.5981142933352332]
	TIME [epoch: 6.6 sec]
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6038582592178006		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 0.6038582592178006 | validation: 0.5807380718137869]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_68.pth
	Model improved!!!
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6035641758660938		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 0.6035641758660938 | validation: 0.5690853852527267]
	TIME [epoch: 6.57 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_69.pth
	Model improved!!!
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 0.58761698013856		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 0.58761698013856 | validation: 0.5924438443455442]
	TIME [epoch: 6.57 sec]
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5430056151495153		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 0.5430056151495153 | validation: 0.45064005969872256]
	TIME [epoch: 6.56 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_71.pth
	Model improved!!!
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5977060929312991		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 0.5977060929312991 | validation: 0.4415619950638422]
	TIME [epoch: 6.55 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_72.pth
	Model improved!!!
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4873337245181592		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 0.4873337245181592 | validation: 0.4337445394955324]
	TIME [epoch: 6.56 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_73.pth
	Model improved!!!
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4662507811303357		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 0.4662507811303357 | validation: 0.4343871796300831]
	TIME [epoch: 6.57 sec]
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4564272292242922		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.4564272292242922 | validation: 0.42258791115237004]
	TIME [epoch: 6.59 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_75.pth
	Model improved!!!
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4575089027590997		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 0.4575089027590997 | validation: 0.42278025467297836]
	TIME [epoch: 6.56 sec]
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45907329378474826		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 0.45907329378474826 | validation: 0.42867549122968884]
	TIME [epoch: 6.56 sec]
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45742211465302274		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 0.45742211465302274 | validation: 0.4281522192111693]
	TIME [epoch: 6.56 sec]
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45435543413557505		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 0.45435543413557505 | validation: 0.41445512137893736]
	TIME [epoch: 6.55 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_79.pth
	Model improved!!!
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45596739818395404		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 0.45596739818395404 | validation: 0.41894489943936414]
	TIME [epoch: 6.55 sec]
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 0.46225306400978616		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 0.46225306400978616 | validation: 0.4214216475792393]
	TIME [epoch: 6.54 sec]
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45139664070098207		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 0.45139664070098207 | validation: 0.42189258691746995]
	TIME [epoch: 6.58 sec]
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4532123117191873		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 0.4532123117191873 | validation: 0.4183553048133035]
	TIME [epoch: 6.56 sec]
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4507713152683339		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 0.4507713152683339 | validation: 0.4114513947572055]
	TIME [epoch: 6.54 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_84.pth
	Model improved!!!
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4514078333457706		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 0.4514078333457706 | validation: 0.4110050125663648]
	TIME [epoch: 6.54 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_85.pth
	Model improved!!!
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 0.44567092385988794		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 0.44567092385988794 | validation: 0.41227974324724564]
	TIME [epoch: 6.55 sec]
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45059869671982783		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 0.45059869671982783 | validation: 0.41891593540451333]
	TIME [epoch: 6.53 sec]
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45219725586745413		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 0.45219725586745413 | validation: 0.4193833994690392]
	TIME [epoch: 6.53 sec]
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4495611054743659		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 0.4495611054743659 | validation: 0.4264438075360422]
	TIME [epoch: 6.56 sec]
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4429645652127103		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.4429645652127103 | validation: 0.40959146434947014]
	TIME [epoch: 6.57 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_90.pth
	Model improved!!!
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4472251057359545		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 0.4472251057359545 | validation: 0.42116569059768727]
	TIME [epoch: 6.55 sec]
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4435618944243089		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 0.4435618944243089 | validation: 0.40610223329584083]
	TIME [epoch: 6.54 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_92.pth
	Model improved!!!
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4467194879516251		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 0.4467194879516251 | validation: 0.4258854315644603]
	TIME [epoch: 6.55 sec]
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 0.44278024071156646		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 0.44278024071156646 | validation: 0.40978557807014493]
	TIME [epoch: 6.55 sec]
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43785209343169074		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 0.43785209343169074 | validation: 0.41905644649054286]
	TIME [epoch: 6.55 sec]
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4444828390487243		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 0.4444828390487243 | validation: 0.40668604597613667]
	TIME [epoch: 6.58 sec]
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 0.44153470067322104		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 0.44153470067322104 | validation: 0.4278080281053682]
	TIME [epoch: 6.59 sec]
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43549578751942		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 0.43549578751942 | validation: 0.4115532792055454]
	TIME [epoch: 6.56 sec]
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43806007570565997		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 0.43806007570565997 | validation: 0.41715369737011154]
	TIME [epoch: 6.55 sec]
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4412144934473618		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 0.4412144934473618 | validation: 0.4143841135292988]
	TIME [epoch: 6.55 sec]
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4337069342680133		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 0.4337069342680133 | validation: 0.4216828203837575]
	TIME [epoch: 105 sec]
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43807456928558974		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 0.43807456928558974 | validation: 0.40211214344346746]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_102.pth
	Model improved!!!
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43540191613938356		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 0.43540191613938356 | validation: 0.40513834534500703]
	TIME [epoch: 14.8 sec]
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4312625936468405		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 0.4312625936468405 | validation: 0.4057541973946581]
	TIME [epoch: 14.9 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43690407118890084		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.43690407118890084 | validation: 0.4001044826105782]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_105.pth
	Model improved!!!
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 0.434137504429509		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 0.434137504429509 | validation: 0.4187309670034802]
	TIME [epoch: 14.8 sec]
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43622551511840824		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 0.43622551511840824 | validation: 0.4027329310549528]
	TIME [epoch: 14.9 sec]
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42584146705212406		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 0.42584146705212406 | validation: 0.4205773157094933]
	TIME [epoch: 15 sec]
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43029947340264796		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: 0.43029947340264796 | validation: 0.4079977607848374]
	TIME [epoch: 14.9 sec]
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43783573388869007		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: 0.43783573388869007 | validation: 0.40312967401712346]
	TIME [epoch: 14.9 sec]
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43613368024976296		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: 0.43613368024976296 | validation: 0.4080677707982274]
	TIME [epoch: 14.9 sec]
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4333985196587742		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: 0.4333985196587742 | validation: 0.4029323220411817]
	TIME [epoch: 14.9 sec]
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42534931395413184		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: 0.42534931395413184 | validation: 0.4166626530288363]
	TIME [epoch: 14.9 sec]
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4316392998924282		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: 0.4316392998924282 | validation: 0.41791089158927885]
	TIME [epoch: 14.9 sec]
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4319048371318517		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: 0.4319048371318517 | validation: 0.3948641348969826]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_115.pth
	Model improved!!!
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4347207184814277		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: 0.4347207184814277 | validation: 0.4207586142516685]
	TIME [epoch: 14.8 sec]
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4265403122394428		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: 0.4265403122394428 | validation: 0.4069509510699081]
	TIME [epoch: 14.9 sec]
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42805173888944276		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: 0.42805173888944276 | validation: 0.40757875894596285]
	TIME [epoch: 14.8 sec]
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42760263859488845		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: 0.42760263859488845 | validation: 0.40581269646682655]
	TIME [epoch: 14.8 sec]
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4281625718666625		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.4281625718666625 | validation: 0.4040411738109884]
	TIME [epoch: 14.8 sec]
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42836132113253794		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: 0.42836132113253794 | validation: 0.41396604365339784]
	TIME [epoch: 14.8 sec]
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4261526404637347		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: 0.4261526404637347 | validation: 0.4074191928229597]
	TIME [epoch: 14.8 sec]
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4253822767129511		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: 0.4253822767129511 | validation: 0.41260850560867973]
	TIME [epoch: 14.8 sec]
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4224903834199607		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: 0.4224903834199607 | validation: 0.40695679453737515]
	TIME [epoch: 14.8 sec]
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42334986982345274		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: 0.42334986982345274 | validation: 0.4127837091215085]
	TIME [epoch: 14.8 sec]
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42922139866918907		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: 0.42922139866918907 | validation: 0.4041865931232801]
	TIME [epoch: 14.8 sec]
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42514876372955757		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: 0.42514876372955757 | validation: 0.4050729723122273]
	TIME [epoch: 14.9 sec]
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4326353053271911		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: 0.4326353053271911 | validation: 0.41091474273583295]
	TIME [epoch: 14.8 sec]
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42874757783272077		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: 0.42874757783272077 | validation: 0.4091137810129496]
	TIME [epoch: 14.8 sec]
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4317921809414501		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: 0.4317921809414501 | validation: 0.4126928286163981]
	TIME [epoch: 14.8 sec]
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43382731709949796		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: 0.43382731709949796 | validation: 0.4098986272439532]
	TIME [epoch: 14.8 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4239228662237913		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: 0.4239228662237913 | validation: 0.3979476502966625]
	TIME [epoch: 14.8 sec]
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4218559702038408		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: 0.4218559702038408 | validation: 0.4043163227844384]
	TIME [epoch: 14.8 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4223812659618528		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: 0.4223812659618528 | validation: 0.39772703345277827]
	TIME [epoch: 14.8 sec]
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4223997779342924		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 0.4223997779342924 | validation: 0.3857803711238288]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_135.pth
	Model improved!!!
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4182283481522029		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: 0.4182283481522029 | validation: 0.3920535742501088]
	TIME [epoch: 14.8 sec]
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: 0.41503386776326323		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: 0.41503386776326323 | validation: 0.3949527380804607]
	TIME [epoch: 14.9 sec]
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4109178401758717		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: 0.4109178401758717 | validation: 0.4416960559377302]
	TIME [epoch: 14.8 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4318107135264043		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: 0.4318107135264043 | validation: 0.4290570250739595]
	TIME [epoch: 14.8 sec]
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42518798114788225		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: 0.42518798114788225 | validation: 0.416764287636786]
	TIME [epoch: 14.9 sec]
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42119160064402356		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: 0.42119160064402356 | validation: 0.41314868199801413]
	TIME [epoch: 14.8 sec]
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: 0.41370694016912324		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: 0.41370694016912324 | validation: 0.3980964302671626]
	TIME [epoch: 14.8 sec]
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: 0.41322735019008044		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: 0.41322735019008044 | validation: 0.38926785453679036]
	TIME [epoch: 14.9 sec]
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: 0.41785594941536436		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: 0.41785594941536436 | validation: 0.40015508836834734]
	TIME [epoch: 14.8 sec]
EPOCH 145/500:
	Training over batches...
		[batch 4/4] avg loss: 0.415050919967748		[learning rate: 0.0023352]
	Learning Rate: 0.00233525
	LOSS [training: 0.415050919967748 | validation: 0.39713169332165643]
	TIME [epoch: 14.8 sec]
EPOCH 146/500:
	Training over batches...
		[batch 4/4] avg loss: 0.416844499688404		[learning rate: 0.0022997]
	Learning Rate: 0.00229968
	LOSS [training: 0.416844499688404 | validation: 0.39957891529645906]
	TIME [epoch: 14.8 sec]
EPOCH 147/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4109406713433421		[learning rate: 0.0022646]
	Learning Rate: 0.00226464
	LOSS [training: 0.4109406713433421 | validation: 0.39681332821199133]
	TIME [epoch: 14.8 sec]
EPOCH 148/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42256664009673695		[learning rate: 0.0022301]
	Learning Rate: 0.00223015
	LOSS [training: 0.42256664009673695 | validation: 0.39710121994602265]
	TIME [epoch: 14.8 sec]
EPOCH 149/500:
	Training over batches...
		[batch 4/4] avg loss: 0.41168550338694615		[learning rate: 0.0021962]
	Learning Rate: 0.00219617
	LOSS [training: 0.41168550338694615 | validation: 0.4059508191512215]
	TIME [epoch: 14.8 sec]
EPOCH 150/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4118750324063903		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 0.4118750324063903 | validation: 0.3975475342061689]
	TIME [epoch: 14.9 sec]
EPOCH 151/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4126414418114104		[learning rate: 0.0021298]
	Learning Rate: 0.00212977
	LOSS [training: 0.4126414418114104 | validation: 0.3897886280816813]
	TIME [epoch: 14.8 sec]
EPOCH 152/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4153360260529916		[learning rate: 0.0020973]
	Learning Rate: 0.00209733
	LOSS [training: 0.4153360260529916 | validation: 0.4059873224828676]
	TIME [epoch: 14.8 sec]
EPOCH 153/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4146732486704817		[learning rate: 0.0020654]
	Learning Rate: 0.00206538
	LOSS [training: 0.4146732486704817 | validation: 0.38981659670620206]
	TIME [epoch: 14.9 sec]
EPOCH 154/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4084490483420845		[learning rate: 0.0020339]
	Learning Rate: 0.00203392
	LOSS [training: 0.4084490483420845 | validation: 0.3888753707077037]
	TIME [epoch: 14.8 sec]
EPOCH 155/500:
	Training over batches...
		[batch 4/4] avg loss: 0.401630237150183		[learning rate: 0.0020029]
	Learning Rate: 0.00200293
	LOSS [training: 0.401630237150183 | validation: 0.3750139322682464]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_155.pth
	Model improved!!!
EPOCH 156/500:
	Training over batches...
		[batch 4/4] avg loss: 0.40587182253382426		[learning rate: 0.0019724]
	Learning Rate: 0.00197242
	LOSS [training: 0.40587182253382426 | validation: 0.377026549877171]
	TIME [epoch: 14.9 sec]
EPOCH 157/500:
	Training over batches...
		[batch 4/4] avg loss: 0.40199488374200987		[learning rate: 0.0019424]
	Learning Rate: 0.00194238
	LOSS [training: 0.40199488374200987 | validation: 0.3769424908833221]
	TIME [epoch: 14.8 sec]
EPOCH 158/500:
	Training over batches...
		[batch 4/4] avg loss: 0.40245422854916035		[learning rate: 0.0019128]
	Learning Rate: 0.00191279
	LOSS [training: 0.40245422854916035 | validation: 0.3765487045046475]
	TIME [epoch: 14.8 sec]
EPOCH 159/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3986736676966909		[learning rate: 0.0018836]
	Learning Rate: 0.00188365
	LOSS [training: 0.3986736676966909 | validation: 0.368004377214753]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_159.pth
	Model improved!!!
EPOCH 160/500:
	Training over batches...
		[batch 4/4] avg loss: 0.40283730159763464		[learning rate: 0.001855]
	Learning Rate: 0.00185495
	LOSS [training: 0.40283730159763464 | validation: 0.36540032855438764]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_160.pth
	Model improved!!!
EPOCH 161/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4034030029152569		[learning rate: 0.0018267]
	Learning Rate: 0.0018267
	LOSS [training: 0.4034030029152569 | validation: 0.3652257374944806]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_161.pth
	Model improved!!!
EPOCH 162/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4012872824872647		[learning rate: 0.0017989]
	Learning Rate: 0.00179887
	LOSS [training: 0.4012872824872647 | validation: 0.37332083499297625]
	TIME [epoch: 14.8 sec]
EPOCH 163/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4004336539958903		[learning rate: 0.0017715]
	Learning Rate: 0.00177147
	LOSS [training: 0.4004336539958903 | validation: 0.37371291161271514]
	TIME [epoch: 14.8 sec]
EPOCH 164/500:
	Training over batches...
		[batch 4/4] avg loss: 0.39505329679160944		[learning rate: 0.0017445]
	Learning Rate: 0.00174448
	LOSS [training: 0.39505329679160944 | validation: 0.3746016372841464]
	TIME [epoch: 14.8 sec]
EPOCH 165/500:
	Training over batches...
		[batch 4/4] avg loss: 0.39202665848142204		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: 0.39202665848142204 | validation: 0.38271933348020093]
	TIME [epoch: 14.8 sec]
EPOCH 166/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4020934648857652		[learning rate: 0.0016917]
	Learning Rate: 0.00169174
	LOSS [training: 0.4020934648857652 | validation: 0.3602108775403623]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_166.pth
	Model improved!!!
EPOCH 167/500:
	Training over batches...
		[batch 4/4] avg loss: 0.39527653434890053		[learning rate: 0.001666]
	Learning Rate: 0.00166597
	LOSS [training: 0.39527653434890053 | validation: 0.3629090453415229]
	TIME [epoch: 14.8 sec]
EPOCH 168/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3837114896949175		[learning rate: 0.0016406]
	Learning Rate: 0.00164059
	LOSS [training: 0.3837114896949175 | validation: 0.35580215776713475]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_168.pth
	Model improved!!!
EPOCH 169/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3781244068219549		[learning rate: 0.0016156]
	Learning Rate: 0.0016156
	LOSS [training: 0.3781244068219549 | validation: 0.353854752541504]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_169.pth
	Model improved!!!
EPOCH 170/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3802900426968663		[learning rate: 0.001591]
	Learning Rate: 0.00159099
	LOSS [training: 0.3802900426968663 | validation: 0.361996782874489]
	TIME [epoch: 14.8 sec]
EPOCH 171/500:
	Training over batches...
		[batch 4/4] avg loss: 0.37926721839536864		[learning rate: 0.0015668]
	Learning Rate: 0.00156675
	LOSS [training: 0.37926721839536864 | validation: 0.34313400450964865]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_171.pth
	Model improved!!!
EPOCH 172/500:
	Training over batches...
		[batch 4/4] avg loss: 0.377699780255891		[learning rate: 0.0015429]
	Learning Rate: 0.00154288
	LOSS [training: 0.377699780255891 | validation: 0.35516627017074354]
	TIME [epoch: 14.9 sec]
EPOCH 173/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3732999541481013		[learning rate: 0.0015194]
	Learning Rate: 0.00151938
	LOSS [training: 0.3732999541481013 | validation: 0.3512638277485891]
	TIME [epoch: 14.8 sec]
EPOCH 174/500:
	Training over batches...
		[batch 4/4] avg loss: 0.37164453798795294		[learning rate: 0.0014962]
	Learning Rate: 0.00149624
	LOSS [training: 0.37164453798795294 | validation: 0.3574127982348795]
	TIME [epoch: 14.8 sec]
EPOCH 175/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3723009268210119		[learning rate: 0.0014734]
	Learning Rate: 0.00147344
	LOSS [training: 0.3723009268210119 | validation: 0.3533473634702142]
	TIME [epoch: 14.8 sec]
EPOCH 176/500:
	Training over batches...
		[batch 4/4] avg loss: 0.368667549282811		[learning rate: 0.001451]
	Learning Rate: 0.001451
	LOSS [training: 0.368667549282811 | validation: 0.3361048719078944]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_176.pth
	Model improved!!!
EPOCH 177/500:
	Training over batches...
		[batch 4/4] avg loss: 0.36790429250404383		[learning rate: 0.0014289]
	Learning Rate: 0.00142889
	LOSS [training: 0.36790429250404383 | validation: 0.33640341198716506]
	TIME [epoch: 14.8 sec]
EPOCH 178/500:
	Training over batches...
		[batch 4/4] avg loss: 0.36139023404723447		[learning rate: 0.0014071]
	Learning Rate: 0.00140713
	LOSS [training: 0.36139023404723447 | validation: 0.3423153818202769]
	TIME [epoch: 14.8 sec]
EPOCH 179/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3601400349465502		[learning rate: 0.0013857]
	Learning Rate: 0.00138569
	LOSS [training: 0.3601400349465502 | validation: 0.34182929812821017]
	TIME [epoch: 14.9 sec]
EPOCH 180/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3576453691246305		[learning rate: 0.0013646]
	Learning Rate: 0.00136458
	LOSS [training: 0.3576453691246305 | validation: 0.3368858071290407]
	TIME [epoch: 14.8 sec]
EPOCH 181/500:
	Training over batches...
		[batch 4/4] avg loss: 0.35470989114015666		[learning rate: 0.0013438]
	Learning Rate: 0.0013438
	LOSS [training: 0.35470989114015666 | validation: 0.3446269699897371]
	TIME [epoch: 14.8 sec]
EPOCH 182/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3486641262768716		[learning rate: 0.0013233]
	Learning Rate: 0.00132333
	LOSS [training: 0.3486641262768716 | validation: 0.32036746205406724]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_182.pth
	Model improved!!!
EPOCH 183/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3462806331204582		[learning rate: 0.0013032]
	Learning Rate: 0.00130317
	LOSS [training: 0.3462806331204582 | validation: 0.315652505989722]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_183.pth
	Model improved!!!
EPOCH 184/500:
	Training over batches...
		[batch 4/4] avg loss: 0.34492322124736496		[learning rate: 0.0012833]
	Learning Rate: 0.00128332
	LOSS [training: 0.34492322124736496 | validation: 0.32370523433447684]
	TIME [epoch: 14.8 sec]
EPOCH 185/500:
	Training over batches...
		[batch 4/4] avg loss: 0.34670834855098775		[learning rate: 0.0012638]
	Learning Rate: 0.00126377
	LOSS [training: 0.34670834855098775 | validation: 0.3259790557812544]
	TIME [epoch: 14.9 sec]
EPOCH 186/500:
	Training over batches...
		[batch 4/4] avg loss: 0.34105691063914656		[learning rate: 0.0012445]
	Learning Rate: 0.00124451
	LOSS [training: 0.34105691063914656 | validation: 0.32316557299606047]
	TIME [epoch: 14.8 sec]
EPOCH 187/500:
	Training over batches...
		[batch 4/4] avg loss: 0.34458994358973405		[learning rate: 0.0012256]
	Learning Rate: 0.00122556
	LOSS [training: 0.34458994358973405 | validation: 0.3160307776630845]
	TIME [epoch: 14.8 sec]
EPOCH 188/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3406477830416701		[learning rate: 0.0012069]
	Learning Rate: 0.00120689
	LOSS [training: 0.3406477830416701 | validation: 0.3204735934148979]
	TIME [epoch: 14.8 sec]
EPOCH 189/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3389942366718673		[learning rate: 0.0011885]
	Learning Rate: 0.0011885
	LOSS [training: 0.3389942366718673 | validation: 0.31478432268612233]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_189.pth
	Model improved!!!
EPOCH 190/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3349897234464879		[learning rate: 0.0011704]
	Learning Rate: 0.0011704
	LOSS [training: 0.3349897234464879 | validation: 0.311042445203601]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_190.pth
	Model improved!!!
EPOCH 191/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3257187360755026		[learning rate: 0.0011526]
	Learning Rate: 0.00115257
	LOSS [training: 0.3257187360755026 | validation: 0.29577075453742085]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_191.pth
	Model improved!!!
EPOCH 192/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3317507296983665		[learning rate: 0.001135]
	Learning Rate: 0.00113501
	LOSS [training: 0.3317507296983665 | validation: 0.3124252388328256]
	TIME [epoch: 14.8 sec]
EPOCH 193/500:
	Training over batches...
		[batch 4/4] avg loss: 0.32549469194749875		[learning rate: 0.0011177]
	Learning Rate: 0.00111772
	LOSS [training: 0.32549469194749875 | validation: 0.2978442783791463]
	TIME [epoch: 14.8 sec]
EPOCH 194/500:
	Training over batches...
		[batch 4/4] avg loss: 0.33074828783080346		[learning rate: 0.0011007]
	Learning Rate: 0.00110069
	LOSS [training: 0.33074828783080346 | validation: 0.29423642037436015]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_194.pth
	Model improved!!!
EPOCH 195/500:
	Training over batches...
		[batch 4/4] avg loss: 0.33307903747246514		[learning rate: 0.0010839]
	Learning Rate: 0.00108393
	LOSS [training: 0.33307903747246514 | validation: 0.2967883111474307]
	TIME [epoch: 14.9 sec]
EPOCH 196/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3215584438897101		[learning rate: 0.0010674]
	Learning Rate: 0.00106741
	LOSS [training: 0.3215584438897101 | validation: 0.2882001579221448]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_196.pth
	Model improved!!!
EPOCH 197/500:
	Training over batches...
		[batch 4/4] avg loss: 0.32511209875515223		[learning rate: 0.0010512]
	Learning Rate: 0.00105115
	LOSS [training: 0.32511209875515223 | validation: 0.2991772828646697]
	TIME [epoch: 14.8 sec]
EPOCH 198/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3215004318376526		[learning rate: 0.0010351]
	Learning Rate: 0.00103514
	LOSS [training: 0.3215004318376526 | validation: 0.2977653581836766]
	TIME [epoch: 14.9 sec]
EPOCH 199/500:
	Training over batches...
		[batch 4/4] avg loss: 0.32071569826272306		[learning rate: 0.0010194]
	Learning Rate: 0.00101937
	LOSS [training: 0.32071569826272306 | validation: 0.29127532204421175]
	TIME [epoch: 14.9 sec]
EPOCH 200/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3227399647076611		[learning rate: 0.0010038]
	Learning Rate: 0.00100384
	LOSS [training: 0.3227399647076611 | validation: 0.2963105891851916]
	TIME [epoch: 14.8 sec]
EPOCH 201/500:
	Training over batches...
		[batch 4/4] avg loss: 0.32131193501358063		[learning rate: 0.00098855]
	Learning Rate: 0.000988553
	LOSS [training: 0.32131193501358063 | validation: 0.2974385337319865]
	TIME [epoch: 14.8 sec]
EPOCH 202/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3199970269020016		[learning rate: 0.00097349]
	Learning Rate: 0.000973494
	LOSS [training: 0.3199970269020016 | validation: 0.2967126439471592]
	TIME [epoch: 14.8 sec]
EPOCH 203/500:
	Training over batches...
		[batch 4/4] avg loss: 0.31638822950022316		[learning rate: 0.00095866]
	Learning Rate: 0.000958664
	LOSS [training: 0.31638822950022316 | validation: 0.28987927911975814]
	TIME [epoch: 14.6 sec]
EPOCH 204/500:
	Training over batches...
		[batch 4/4] avg loss: 0.31493998983422283		[learning rate: 0.00094406]
	Learning Rate: 0.000944061
	LOSS [training: 0.31493998983422283 | validation: 0.2889974575694858]
	TIME [epoch: 14.6 sec]
EPOCH 205/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3120706659818919		[learning rate: 0.00092968]
	Learning Rate: 0.00092968
	LOSS [training: 0.3120706659818919 | validation: 0.2824690049172188]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_205.pth
	Model improved!!!
EPOCH 206/500:
	Training over batches...
		[batch 4/4] avg loss: 0.310421790016955		[learning rate: 0.00091552]
	Learning Rate: 0.000915518
	LOSS [training: 0.310421790016955 | validation: 0.28651053565665185]
	TIME [epoch: 14.6 sec]
EPOCH 207/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3127117401030951		[learning rate: 0.00090157]
	Learning Rate: 0.000901571
	LOSS [training: 0.3127117401030951 | validation: 0.2787658984709856]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_207.pth
	Model improved!!!
EPOCH 208/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3078774774567983		[learning rate: 0.00088784]
	Learning Rate: 0.000887837
	LOSS [training: 0.3078774774567983 | validation: 0.28316075674315966]
	TIME [epoch: 14.6 sec]
EPOCH 209/500:
	Training over batches...
		[batch 4/4] avg loss: 0.30595335121602923		[learning rate: 0.00087431]
	Learning Rate: 0.000874312
	LOSS [training: 0.30595335121602923 | validation: 0.2835090553293501]
	TIME [epoch: 14.7 sec]
EPOCH 210/500:
	Training over batches...
		[batch 4/4] avg loss: 0.30377848037169913		[learning rate: 0.00086099]
	Learning Rate: 0.000860994
	LOSS [training: 0.30377848037169913 | validation: 0.2841954980609539]
	TIME [epoch: 14.6 sec]
EPOCH 211/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3030099264546276		[learning rate: 0.00084788]
	Learning Rate: 0.000847878
	LOSS [training: 0.3030099264546276 | validation: 0.27782316702068194]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_211.pth
	Model improved!!!
EPOCH 212/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2998089388139366		[learning rate: 0.00083496]
	Learning Rate: 0.000834962
	LOSS [training: 0.2998089388139366 | validation: 0.2694356533167518]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_212.pth
	Model improved!!!
EPOCH 213/500:
	Training over batches...
		[batch 4/4] avg loss: 0.302858163192459		[learning rate: 0.00082224]
	Learning Rate: 0.000822243
	LOSS [training: 0.302858163192459 | validation: 0.25827115512016297]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_213.pth
	Model improved!!!
EPOCH 214/500:
	Training over batches...
		[batch 4/4] avg loss: 0.29639164259403955		[learning rate: 0.00080972]
	Learning Rate: 0.000809717
	LOSS [training: 0.29639164259403955 | validation: 0.25956024348999407]
	TIME [epoch: 14.6 sec]
EPOCH 215/500:
	Training over batches...
		[batch 4/4] avg loss: 0.28966915388235914		[learning rate: 0.00079738]
	Learning Rate: 0.000797382
	LOSS [training: 0.28966915388235914 | validation: 0.26302301482651114]
	TIME [epoch: 14.6 sec]
EPOCH 216/500:
	Training over batches...
		[batch 4/4] avg loss: 0.29208940412078865		[learning rate: 0.00078524]
	Learning Rate: 0.000785236
	LOSS [training: 0.29208940412078865 | validation: 0.25673904813802156]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_216.pth
	Model improved!!!
EPOCH 217/500:
	Training over batches...
		[batch 4/4] avg loss: 0.28512309430053856		[learning rate: 0.00077327]
	Learning Rate: 0.000773274
	LOSS [training: 0.28512309430053856 | validation: 0.26198847183559293]
	TIME [epoch: 14.6 sec]
EPOCH 218/500:
	Training over batches...
		[batch 4/4] avg loss: 0.28403754019791727		[learning rate: 0.00076149]
	Learning Rate: 0.000761494
	LOSS [training: 0.28403754019791727 | validation: 0.2597299803315237]
	TIME [epoch: 14.6 sec]
EPOCH 219/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2853901730964975		[learning rate: 0.00074989]
	Learning Rate: 0.000749894
	LOSS [training: 0.2853901730964975 | validation: 0.2546010900504585]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_219.pth
	Model improved!!!
EPOCH 220/500:
	Training over batches...
		[batch 4/4] avg loss: 0.28308516729237687		[learning rate: 0.00073847]
	Learning Rate: 0.000738471
	LOSS [training: 0.28308516729237687 | validation: 0.2656676786280123]
	TIME [epoch: 14.6 sec]
EPOCH 221/500:
	Training over batches...
		[batch 4/4] avg loss: 0.28028523845569003		[learning rate: 0.00072722]
	Learning Rate: 0.000727221
	LOSS [training: 0.28028523845569003 | validation: 0.2511179199005708]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_221.pth
	Model improved!!!
EPOCH 222/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2751961310615493		[learning rate: 0.00071614]
	Learning Rate: 0.000716143
	LOSS [training: 0.2751961310615493 | validation: 0.26181413304003753]
	TIME [epoch: 14.6 sec]
EPOCH 223/500:
	Training over batches...
		[batch 4/4] avg loss: 0.27696048261841005		[learning rate: 0.00070523]
	Learning Rate: 0.000705234
	LOSS [training: 0.27696048261841005 | validation: 0.24696036450064032]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_223.pth
	Model improved!!!
EPOCH 224/500:
	Training over batches...
		[batch 4/4] avg loss: 0.27965456669667443		[learning rate: 0.00069449]
	Learning Rate: 0.000694491
	LOSS [training: 0.27965456669667443 | validation: 0.2588835754746746]
	TIME [epoch: 14.6 sec]
EPOCH 225/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2742299845768722		[learning rate: 0.00068391]
	Learning Rate: 0.000683912
	LOSS [training: 0.2742299845768722 | validation: 0.24961886888107415]
	TIME [epoch: 14.6 sec]
EPOCH 226/500:
	Training over batches...
		[batch 4/4] avg loss: 0.27283674559635906		[learning rate: 0.00067349]
	Learning Rate: 0.000673493
	LOSS [training: 0.27283674559635906 | validation: 0.24747611795614366]
	TIME [epoch: 14.6 sec]
EPOCH 227/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2658433453173902		[learning rate: 0.00066323]
	Learning Rate: 0.000663234
	LOSS [training: 0.2658433453173902 | validation: 0.2365027620638248]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_227.pth
	Model improved!!!
EPOCH 228/500:
	Training over batches...
		[batch 4/4] avg loss: 0.26310530127686493		[learning rate: 0.00065313]
	Learning Rate: 0.00065313
	LOSS [training: 0.26310530127686493 | validation: 0.23491205098625728]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_228.pth
	Model improved!!!
EPOCH 229/500:
	Training over batches...
		[batch 4/4] avg loss: 0.26344707771969683		[learning rate: 0.00064318]
	Learning Rate: 0.000643181
	LOSS [training: 0.26344707771969683 | validation: 0.24495841035636634]
	TIME [epoch: 14.7 sec]
EPOCH 230/500:
	Training over batches...
		[batch 4/4] avg loss: 0.25904605913430073		[learning rate: 0.00063338]
	Learning Rate: 0.000633383
	LOSS [training: 0.25904605913430073 | validation: 0.239751378345335]
	TIME [epoch: 14.6 sec]
EPOCH 231/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2606805637444845		[learning rate: 0.00062373]
	Learning Rate: 0.000623735
	LOSS [training: 0.2606805637444845 | validation: 0.23258563197846394]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_231.pth
	Model improved!!!
EPOCH 232/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2605697911091865		[learning rate: 0.00061423]
	Learning Rate: 0.000614233
	LOSS [training: 0.2605697911091865 | validation: 0.23422801541433186]
	TIME [epoch: 14.6 sec]
EPOCH 233/500:
	Training over batches...
		[batch 4/4] avg loss: 0.25973165763615846		[learning rate: 0.00060488]
	Learning Rate: 0.000604876
	LOSS [training: 0.25973165763615846 | validation: 0.22836840686927626]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_233.pth
	Model improved!!!
EPOCH 234/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2580799649426883		[learning rate: 0.00059566]
	Learning Rate: 0.000595662
	LOSS [training: 0.2580799649426883 | validation: 0.22575720104191754]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_234.pth
	Model improved!!!
EPOCH 235/500:
	Training over batches...
		[batch 4/4] avg loss: 0.25652157010906745		[learning rate: 0.00058659]
	Learning Rate: 0.000586588
	LOSS [training: 0.25652157010906745 | validation: 0.22841233383654386]
	TIME [epoch: 14.6 sec]
EPOCH 236/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2504602709303329		[learning rate: 0.00057765]
	Learning Rate: 0.000577652
	LOSS [training: 0.2504602709303329 | validation: 0.21980362852414315]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_236.pth
	Model improved!!!
EPOCH 237/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2518945545125772		[learning rate: 0.00056885]
	Learning Rate: 0.000568853
	LOSS [training: 0.2518945545125772 | validation: 0.21873304006558725]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_237.pth
	Model improved!!!
EPOCH 238/500:
	Training over batches...
		[batch 4/4] avg loss: 0.24506563164037956		[learning rate: 0.00056019]
	Learning Rate: 0.000560187
	LOSS [training: 0.24506563164037956 | validation: 0.21979226730736784]
	TIME [epoch: 14.6 sec]
EPOCH 239/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2477303019295083		[learning rate: 0.00055165]
	Learning Rate: 0.000551654
	LOSS [training: 0.2477303019295083 | validation: 0.21800219386619274]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_239.pth
	Model improved!!!
EPOCH 240/500:
	Training over batches...
		[batch 4/4] avg loss: 0.24465812291162858		[learning rate: 0.00054325]
	Learning Rate: 0.00054325
	LOSS [training: 0.24465812291162858 | validation: 0.23404435727637446]
	TIME [epoch: 14.6 sec]
EPOCH 241/500:
	Training over batches...
		[batch 4/4] avg loss: 0.24784618136678352		[learning rate: 0.00053497]
	Learning Rate: 0.000534975
	LOSS [training: 0.24784618136678352 | validation: 0.2202226566259147]
	TIME [epoch: 14.6 sec]
EPOCH 242/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2441113594689429		[learning rate: 0.00052683]
	Learning Rate: 0.000526825
	LOSS [training: 0.2441113594689429 | validation: 0.2235509142846394]
	TIME [epoch: 14.7 sec]
EPOCH 243/500:
	Training over batches...
		[batch 4/4] avg loss: 0.24751918691991168		[learning rate: 0.0005188]
	Learning Rate: 0.0005188
	LOSS [training: 0.24751918691991168 | validation: 0.23091826031246732]
	TIME [epoch: 14.6 sec]
EPOCH 244/500:
	Training over batches...
		[batch 4/4] avg loss: 0.24199644724937105		[learning rate: 0.0005109]
	Learning Rate: 0.000510897
	LOSS [training: 0.24199644724937105 | validation: 0.2252483611337796]
	TIME [epoch: 14.6 sec]
EPOCH 245/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2410443346952214		[learning rate: 0.00050311]
	Learning Rate: 0.000503114
	LOSS [training: 0.2410443346952214 | validation: 0.21147879470164027]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_245.pth
	Model improved!!!
EPOCH 246/500:
	Training over batches...
		[batch 4/4] avg loss: 0.23856528631128449		[learning rate: 0.00049545]
	Learning Rate: 0.00049545
	LOSS [training: 0.23856528631128449 | validation: 0.203904954585913]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_246.pth
	Model improved!!!
EPOCH 247/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2348909179459086		[learning rate: 0.0004879]
	Learning Rate: 0.000487903
	LOSS [training: 0.2348909179459086 | validation: 0.1996188103992818]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_247.pth
	Model improved!!!
EPOCH 248/500:
	Training over batches...
		[batch 4/4] avg loss: 0.23464788987589383		[learning rate: 0.00048047]
	Learning Rate: 0.00048047
	LOSS [training: 0.23464788987589383 | validation: 0.20393606472446846]
	TIME [epoch: 14.6 sec]
EPOCH 249/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2375804526247195		[learning rate: 0.00047315]
	Learning Rate: 0.000473151
	LOSS [training: 0.2375804526247195 | validation: 0.2035275020436717]
	TIME [epoch: 14.6 sec]
EPOCH 250/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2361687417743875		[learning rate: 0.00046594]
	Learning Rate: 0.000465944
	LOSS [training: 0.2361687417743875 | validation: 0.2122594574541996]
	TIME [epoch: 14.6 sec]
EPOCH 251/500:
	Training over batches...
		[batch 4/4] avg loss: 0.23084723938673407		[learning rate: 0.00045885]
	Learning Rate: 0.000458846
	LOSS [training: 0.23084723938673407 | validation: 0.2004960775215897]
	TIME [epoch: 120 sec]
EPOCH 252/500:
	Training over batches...
		[batch 4/4] avg loss: 0.23100019436883248		[learning rate: 0.00045186]
	Learning Rate: 0.000451856
	LOSS [training: 0.23100019436883248 | validation: 0.19841522795080324]
	TIME [epoch: 32.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_252.pth
	Model improved!!!
EPOCH 253/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22670766574506976		[learning rate: 0.00044497]
	Learning Rate: 0.000444973
	LOSS [training: 0.22670766574506976 | validation: 0.20397272015855378]
	TIME [epoch: 32.4 sec]
EPOCH 254/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22886220823699754		[learning rate: 0.00043819]
	Learning Rate: 0.000438194
	LOSS [training: 0.22886220823699754 | validation: 0.20496030248960984]
	TIME [epoch: 32.3 sec]
EPOCH 255/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22627565773147637		[learning rate: 0.00043152]
	Learning Rate: 0.000431519
	LOSS [training: 0.22627565773147637 | validation: 0.2011766460179165]
	TIME [epoch: 32.4 sec]
EPOCH 256/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22494115814040744		[learning rate: 0.00042495]
	Learning Rate: 0.000424946
	LOSS [training: 0.22494115814040744 | validation: 0.20312785261920946]
	TIME [epoch: 32.4 sec]
EPOCH 257/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22581530580997275		[learning rate: 0.00041847]
	Learning Rate: 0.000418472
	LOSS [training: 0.22581530580997275 | validation: 0.20884346503488205]
	TIME [epoch: 32.3 sec]
EPOCH 258/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22226948261365267		[learning rate: 0.0004121]
	Learning Rate: 0.000412098
	LOSS [training: 0.22226948261365267 | validation: 0.19141685536511566]
	TIME [epoch: 32.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_258.pth
	Model improved!!!
EPOCH 259/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22298125117464396		[learning rate: 0.00040582]
	Learning Rate: 0.00040582
	LOSS [training: 0.22298125117464396 | validation: 0.19970530811940562]
	TIME [epoch: 32.3 sec]
EPOCH 260/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2219897599335539		[learning rate: 0.00039964]
	Learning Rate: 0.000399638
	LOSS [training: 0.2219897599335539 | validation: 0.19047186098171853]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_260.pth
	Model improved!!!
EPOCH 261/500:
	Training over batches...
		[batch 4/4] avg loss: 0.21953875952285068		[learning rate: 0.00039355]
	Learning Rate: 0.00039355
	LOSS [training: 0.21953875952285068 | validation: 0.19653162128861967]
	TIME [epoch: 32.3 sec]
EPOCH 262/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22013453063094532		[learning rate: 0.00038755]
	Learning Rate: 0.000387555
	LOSS [training: 0.22013453063094532 | validation: 0.1985916272100756]
	TIME [epoch: 32.3 sec]
EPOCH 263/500:
	Training over batches...
		[batch 4/4] avg loss: 0.21768668824002713		[learning rate: 0.00038165]
	Learning Rate: 0.000381651
	LOSS [training: 0.21768668824002713 | validation: 0.18269905129600872]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_263.pth
	Model improved!!!
EPOCH 264/500:
	Training over batches...
		[batch 4/4] avg loss: 0.21634012431635236		[learning rate: 0.00037584]
	Learning Rate: 0.000375837
	LOSS [training: 0.21634012431635236 | validation: 0.19131428308874016]
	TIME [epoch: 32.3 sec]
EPOCH 265/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2154151186992494		[learning rate: 0.00037011]
	Learning Rate: 0.000370112
	LOSS [training: 0.2154151186992494 | validation: 0.18518308464089694]
	TIME [epoch: 32.3 sec]
EPOCH 266/500:
	Training over batches...
		[batch 4/4] avg loss: 0.21589316829915253		[learning rate: 0.00036447]
	Learning Rate: 0.000364474
	LOSS [training: 0.21589316829915253 | validation: 0.19066837363650338]
	TIME [epoch: 32.3 sec]
EPOCH 267/500:
	Training over batches...
		[batch 4/4] avg loss: 0.21283914236187057		[learning rate: 0.00035892]
	Learning Rate: 0.000358922
	LOSS [training: 0.21283914236187057 | validation: 0.18340676264829942]
	TIME [epoch: 32.3 sec]
EPOCH 268/500:
	Training over batches...
		[batch 4/4] avg loss: 0.21399245060595504		[learning rate: 0.00035345]
	Learning Rate: 0.000353454
	LOSS [training: 0.21399245060595504 | validation: 0.18751629098958394]
	TIME [epoch: 32.3 sec]
EPOCH 269/500:
	Training over batches...
		[batch 4/4] avg loss: 0.211212714334256		[learning rate: 0.00034807]
	Learning Rate: 0.00034807
	LOSS [training: 0.211212714334256 | validation: 0.18507763692198725]
	TIME [epoch: 32.3 sec]
EPOCH 270/500:
	Training over batches...
		[batch 4/4] avg loss: 0.20845637559104954		[learning rate: 0.00034277]
	Learning Rate: 0.000342768
	LOSS [training: 0.20845637559104954 | validation: 0.17771402717774643]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_270.pth
	Model improved!!!
EPOCH 271/500:
	Training over batches...
		[batch 4/4] avg loss: 0.20621017364208633		[learning rate: 0.00033755]
	Learning Rate: 0.000337546
	LOSS [training: 0.20621017364208633 | validation: 0.1849605134517679]
	TIME [epoch: 32.3 sec]
EPOCH 272/500:
	Training over batches...
		[batch 4/4] avg loss: 0.20780839173482152		[learning rate: 0.0003324]
	Learning Rate: 0.000332404
	LOSS [training: 0.20780839173482152 | validation: 0.17403432044780953]
	TIME [epoch: 32.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_272.pth
	Model improved!!!
EPOCH 273/500:
	Training over batches...
		[batch 4/4] avg loss: 0.20588658963757933		[learning rate: 0.00032734]
	Learning Rate: 0.000327341
	LOSS [training: 0.20588658963757933 | validation: 0.18194716990938264]
	TIME [epoch: 32.3 sec]
EPOCH 274/500:
	Training over batches...
		[batch 4/4] avg loss: 0.20489349361924628		[learning rate: 0.00032235]
	Learning Rate: 0.000322354
	LOSS [training: 0.20489349361924628 | validation: 0.18177130706373912]
	TIME [epoch: 32.3 sec]
EPOCH 275/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2073511759266689		[learning rate: 0.00031744]
	Learning Rate: 0.000317444
	LOSS [training: 0.2073511759266689 | validation: 0.1827004885498456]
	TIME [epoch: 32.3 sec]
EPOCH 276/500:
	Training over batches...
		[batch 4/4] avg loss: 0.20136663914835312		[learning rate: 0.00031261]
	Learning Rate: 0.000312608
	LOSS [training: 0.20136663914835312 | validation: 0.17653861471338667]
	TIME [epoch: 32.3 sec]
EPOCH 277/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19951826403167844		[learning rate: 0.00030785]
	Learning Rate: 0.000307846
	LOSS [training: 0.19951826403167844 | validation: 0.18604391858604363]
	TIME [epoch: 32.3 sec]
EPOCH 278/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19984164947007227		[learning rate: 0.00030316]
	Learning Rate: 0.000303156
	LOSS [training: 0.19984164947007227 | validation: 0.17761795651640475]
	TIME [epoch: 32.3 sec]
EPOCH 279/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1973681369608828		[learning rate: 0.00029854]
	Learning Rate: 0.000298538
	LOSS [training: 0.1973681369608828 | validation: 0.17713298098676933]
	TIME [epoch: 32.3 sec]
EPOCH 280/500:
	Training over batches...
		[batch 4/4] avg loss: 0.20256927349263845		[learning rate: 0.00029399]
	Learning Rate: 0.000293991
	LOSS [training: 0.20256927349263845 | validation: 0.18132168829482198]
	TIME [epoch: 32.3 sec]
EPOCH 281/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19731733165334728		[learning rate: 0.00028951]
	Learning Rate: 0.000289512
	LOSS [training: 0.19731733165334728 | validation: 0.16669177679907407]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_281.pth
	Model improved!!!
EPOCH 282/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19818449088993303		[learning rate: 0.0002851]
	Learning Rate: 0.000285102
	LOSS [training: 0.19818449088993303 | validation: 0.16242611605283108]
	TIME [epoch: 32.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_282.pth
	Model improved!!!
EPOCH 283/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19654843328715138		[learning rate: 0.00028076]
	Learning Rate: 0.000280759
	LOSS [training: 0.19654843328715138 | validation: 0.17181679126231375]
	TIME [epoch: 32.3 sec]
EPOCH 284/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1977533692447162		[learning rate: 0.00027648]
	Learning Rate: 0.000276482
	LOSS [training: 0.1977533692447162 | validation: 0.17181624631325493]
	TIME [epoch: 32.4 sec]
EPOCH 285/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19686441573600358		[learning rate: 0.00027227]
	Learning Rate: 0.00027227
	LOSS [training: 0.19686441573600358 | validation: 0.1746422736344516]
	TIME [epoch: 32.4 sec]
EPOCH 286/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1932807512790195		[learning rate: 0.00026812]
	Learning Rate: 0.000268123
	LOSS [training: 0.1932807512790195 | validation: 0.16911014951439468]
	TIME [epoch: 32.3 sec]
EPOCH 287/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19361938863504802		[learning rate: 0.00026404]
	Learning Rate: 0.000264038
	LOSS [training: 0.19361938863504802 | validation: 0.16272942303978466]
	TIME [epoch: 32.4 sec]
EPOCH 288/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19004113654140953		[learning rate: 0.00026002]
	Learning Rate: 0.000260016
	LOSS [training: 0.19004113654140953 | validation: 0.1561450907738544]
	TIME [epoch: 32.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_288.pth
	Model improved!!!
EPOCH 289/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19160614069849885		[learning rate: 0.00025606]
	Learning Rate: 0.000256055
	LOSS [training: 0.19160614069849885 | validation: 0.1662943618472698]
	TIME [epoch: 32.3 sec]
EPOCH 290/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1901193122341382		[learning rate: 0.00025215]
	Learning Rate: 0.000252154
	LOSS [training: 0.1901193122341382 | validation: 0.1613640921134373]
	TIME [epoch: 32.3 sec]
EPOCH 291/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18808827503129974		[learning rate: 0.00024831]
	Learning Rate: 0.000248313
	LOSS [training: 0.18808827503129974 | validation: 0.16093030193253238]
	TIME [epoch: 32.3 sec]
EPOCH 292/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1930681923618181		[learning rate: 0.00024453]
	Learning Rate: 0.000244531
	LOSS [training: 0.1930681923618181 | validation: 0.15319452844728948]
	TIME [epoch: 32.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_292.pth
	Model improved!!!
EPOCH 293/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18743709458163527		[learning rate: 0.00024081]
	Learning Rate: 0.000240806
	LOSS [training: 0.18743709458163527 | validation: 0.15345266947673347]
	TIME [epoch: 32.4 sec]
EPOCH 294/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19108685770900352		[learning rate: 0.00023714]
	Learning Rate: 0.000237137
	LOSS [training: 0.19108685770900352 | validation: 0.16645874252204557]
	TIME [epoch: 32.3 sec]
EPOCH 295/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18492599180450556		[learning rate: 0.00023352]
	Learning Rate: 0.000233525
	LOSS [training: 0.18492599180450556 | validation: 0.15741162913200973]
	TIME [epoch: 32.3 sec]
EPOCH 296/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18593084113598068		[learning rate: 0.00022997]
	Learning Rate: 0.000229968
	LOSS [training: 0.18593084113598068 | validation: 0.15732607369769624]
	TIME [epoch: 32.3 sec]
EPOCH 297/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18554781124619338		[learning rate: 0.00022646]
	Learning Rate: 0.000226464
	LOSS [training: 0.18554781124619338 | validation: 0.15923322322201086]
	TIME [epoch: 32.3 sec]
EPOCH 298/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18643035984928147		[learning rate: 0.00022301]
	Learning Rate: 0.000223015
	LOSS [training: 0.18643035984928147 | validation: 0.15421590523736828]
	TIME [epoch: 32.3 sec]
EPOCH 299/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18717931697549467		[learning rate: 0.00021962]
	Learning Rate: 0.000219617
	LOSS [training: 0.18717931697549467 | validation: 0.16450299308810806]
	TIME [epoch: 32.3 sec]
EPOCH 300/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18048163976358544		[learning rate: 0.00021627]
	Learning Rate: 0.000216272
	LOSS [training: 0.18048163976358544 | validation: 0.1661506416372454]
	TIME [epoch: 32.3 sec]
EPOCH 301/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18398804173041491		[learning rate: 0.00021298]
	Learning Rate: 0.000212977
	LOSS [training: 0.18398804173041491 | validation: 0.15649798451157632]
	TIME [epoch: 32.3 sec]
EPOCH 302/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18240658479396088		[learning rate: 0.00020973]
	Learning Rate: 0.000209733
	LOSS [training: 0.18240658479396088 | validation: 0.1664098509737837]
	TIME [epoch: 32.3 sec]
EPOCH 303/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18106470679444198		[learning rate: 0.00020654]
	Learning Rate: 0.000206538
	LOSS [training: 0.18106470679444198 | validation: 0.15663561132918502]
	TIME [epoch: 32.3 sec]
EPOCH 304/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18059023877709576		[learning rate: 0.00020339]
	Learning Rate: 0.000203392
	LOSS [training: 0.18059023877709576 | validation: 0.15648373897456647]
	TIME [epoch: 32.3 sec]
EPOCH 305/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18075204431405495		[learning rate: 0.00020029]
	Learning Rate: 0.000200293
	LOSS [training: 0.18075204431405495 | validation: 0.15923940482514706]
	TIME [epoch: 32.3 sec]
EPOCH 306/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17787435479221742		[learning rate: 0.00019724]
	Learning Rate: 0.000197242
	LOSS [training: 0.17787435479221742 | validation: 0.1570780563151036]
	TIME [epoch: 32.3 sec]
EPOCH 307/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17533411873631832		[learning rate: 0.00019424]
	Learning Rate: 0.000194238
	LOSS [training: 0.17533411873631832 | validation: 0.14842464461020408]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_307.pth
	Model improved!!!
EPOCH 308/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17604862949297004		[learning rate: 0.00019128]
	Learning Rate: 0.000191279
	LOSS [training: 0.17604862949297004 | validation: 0.15388012887224245]
	TIME [epoch: 32.3 sec]
EPOCH 309/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18192130591960878		[learning rate: 0.00018836]
	Learning Rate: 0.000188365
	LOSS [training: 0.18192130591960878 | validation: 0.16207312212802505]
	TIME [epoch: 32.3 sec]
EPOCH 310/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17748067190095115		[learning rate: 0.0001855]
	Learning Rate: 0.000185495
	LOSS [training: 0.17748067190095115 | validation: 0.1493830778474312]
	TIME [epoch: 32.3 sec]
EPOCH 311/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17503689100330017		[learning rate: 0.00018267]
	Learning Rate: 0.00018267
	LOSS [training: 0.17503689100330017 | validation: 0.14988561201081713]
	TIME [epoch: 32.3 sec]
EPOCH 312/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17459614903235682		[learning rate: 0.00017989]
	Learning Rate: 0.000179887
	LOSS [training: 0.17459614903235682 | validation: 0.1502570187261162]
	TIME [epoch: 32.3 sec]
EPOCH 313/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17352730496082144		[learning rate: 0.00017715]
	Learning Rate: 0.000177147
	LOSS [training: 0.17352730496082144 | validation: 0.14945063032624717]
	TIME [epoch: 32.3 sec]
EPOCH 314/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17544608942422368		[learning rate: 0.00017445]
	Learning Rate: 0.000174448
	LOSS [training: 0.17544608942422368 | validation: 0.14634501101986136]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_314.pth
	Model improved!!!
EPOCH 315/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17278176711064286		[learning rate: 0.00017179]
	Learning Rate: 0.000171791
	LOSS [training: 0.17278176711064286 | validation: 0.1506972485573233]
	TIME [epoch: 32.3 sec]
EPOCH 316/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17268069707187922		[learning rate: 0.00016917]
	Learning Rate: 0.000169174
	LOSS [training: 0.17268069707187922 | validation: 0.14280525206249334]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_316.pth
	Model improved!!!
EPOCH 317/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17277167185500303		[learning rate: 0.0001666]
	Learning Rate: 0.000166597
	LOSS [training: 0.17277167185500303 | validation: 0.13734096085779804]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_317.pth
	Model improved!!!
EPOCH 318/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16954088702942155		[learning rate: 0.00016406]
	Learning Rate: 0.000164059
	LOSS [training: 0.16954088702942155 | validation: 0.1461567535301906]
	TIME [epoch: 32.3 sec]
EPOCH 319/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17234908725053139		[learning rate: 0.00016156]
	Learning Rate: 0.00016156
	LOSS [training: 0.17234908725053139 | validation: 0.14020189702325328]
	TIME [epoch: 32.3 sec]
EPOCH 320/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17319938230765503		[learning rate: 0.0001591]
	Learning Rate: 0.000159099
	LOSS [training: 0.17319938230765503 | validation: 0.14290551108309105]
	TIME [epoch: 32.3 sec]
EPOCH 321/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17119469856554942		[learning rate: 0.00015668]
	Learning Rate: 0.000156675
	LOSS [training: 0.17119469856554942 | validation: 0.15213118824927857]
	TIME [epoch: 32.3 sec]
EPOCH 322/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17080384580617775		[learning rate: 0.00015429]
	Learning Rate: 0.000154288
	LOSS [training: 0.17080384580617775 | validation: 0.14737431704002546]
	TIME [epoch: 32.3 sec]
EPOCH 323/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16611150317393075		[learning rate: 0.00015194]
	Learning Rate: 0.000151938
	LOSS [training: 0.16611150317393075 | validation: 0.14532026717106566]
	TIME [epoch: 32.3 sec]
EPOCH 324/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16833398262842575		[learning rate: 0.00014962]
	Learning Rate: 0.000149624
	LOSS [training: 0.16833398262842575 | validation: 0.13938749200574524]
	TIME [epoch: 32.2 sec]
EPOCH 325/500:
	Training over batches...
		[batch 4/4] avg loss: 0.168252696814934		[learning rate: 0.00014734]
	Learning Rate: 0.000147344
	LOSS [training: 0.168252696814934 | validation: 0.14238521759296155]
	TIME [epoch: 32.3 sec]
EPOCH 326/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16589201506317203		[learning rate: 0.0001451]
	Learning Rate: 0.0001451
	LOSS [training: 0.16589201506317203 | validation: 0.14242337049285317]
	TIME [epoch: 32.3 sec]
EPOCH 327/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1673595630170448		[learning rate: 0.00014289]
	Learning Rate: 0.000142889
	LOSS [training: 0.1673595630170448 | validation: 0.13975158790586967]
	TIME [epoch: 32.2 sec]
EPOCH 328/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16630795017537955		[learning rate: 0.00014071]
	Learning Rate: 0.000140713
	LOSS [training: 0.16630795017537955 | validation: 0.13956063581912664]
	TIME [epoch: 32.3 sec]
EPOCH 329/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16492963207356007		[learning rate: 0.00013857]
	Learning Rate: 0.000138569
	LOSS [training: 0.16492963207356007 | validation: 0.13958547356519468]
	TIME [epoch: 32.3 sec]
EPOCH 330/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16411140106206643		[learning rate: 0.00013646]
	Learning Rate: 0.000136458
	LOSS [training: 0.16411140106206643 | validation: 0.14068577601754162]
	TIME [epoch: 32.2 sec]
EPOCH 331/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1648894139418612		[learning rate: 0.00013438]
	Learning Rate: 0.00013438
	LOSS [training: 0.1648894139418612 | validation: 0.1330060409824197]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_331.pth
	Model improved!!!
EPOCH 332/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16482950570421426		[learning rate: 0.00013233]
	Learning Rate: 0.000132333
	LOSS [training: 0.16482950570421426 | validation: 0.137461122430694]
	TIME [epoch: 32.3 sec]
EPOCH 333/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16592614812013354		[learning rate: 0.00013032]
	Learning Rate: 0.000130317
	LOSS [training: 0.16592614812013354 | validation: 0.14034091477506144]
	TIME [epoch: 32.3 sec]
EPOCH 334/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16556372141906478		[learning rate: 0.00012833]
	Learning Rate: 0.000128332
	LOSS [training: 0.16556372141906478 | validation: 0.1370330177225385]
	TIME [epoch: 32.3 sec]
EPOCH 335/500:
	Training over batches...
		[batch 4/4] avg loss: 0.162905665673423		[learning rate: 0.00012638]
	Learning Rate: 0.000126377
	LOSS [training: 0.162905665673423 | validation: 0.13294823418632085]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_335.pth
	Model improved!!!
EPOCH 336/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16459345377046905		[learning rate: 0.00012445]
	Learning Rate: 0.000124451
	LOSS [training: 0.16459345377046905 | validation: 0.14188580851955274]
	TIME [epoch: 32.3 sec]
EPOCH 337/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16174279171163147		[learning rate: 0.00012256]
	Learning Rate: 0.000122556
	LOSS [training: 0.16174279171163147 | validation: 0.14235685674342913]
	TIME [epoch: 32.3 sec]
EPOCH 338/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16215475059259066		[learning rate: 0.00012069]
	Learning Rate: 0.000120689
	LOSS [training: 0.16215475059259066 | validation: 0.13262124266089242]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_338.pth
	Model improved!!!
EPOCH 339/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16111046608054658		[learning rate: 0.00011885]
	Learning Rate: 0.00011885
	LOSS [training: 0.16111046608054658 | validation: 0.14144894646066325]
	TIME [epoch: 32.3 sec]
EPOCH 340/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16185814624470696		[learning rate: 0.00011704]
	Learning Rate: 0.00011704
	LOSS [training: 0.16185814624470696 | validation: 0.1324738717379666]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_340.pth
	Model improved!!!
EPOCH 341/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16117723100507503		[learning rate: 0.00011526]
	Learning Rate: 0.000115257
	LOSS [training: 0.16117723100507503 | validation: 0.13548294115458592]
	TIME [epoch: 32.4 sec]
EPOCH 342/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1627413735019779		[learning rate: 0.0001135]
	Learning Rate: 0.000113501
	LOSS [training: 0.1627413735019779 | validation: 0.13804249807309657]
	TIME [epoch: 32.3 sec]
EPOCH 343/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1599356552185752		[learning rate: 0.00011177]
	Learning Rate: 0.000111772
	LOSS [training: 0.1599356552185752 | validation: 0.13200823100005815]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_343.pth
	Model improved!!!
EPOCH 344/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16175753560467757		[learning rate: 0.00011007]
	Learning Rate: 0.000110069
	LOSS [training: 0.16175753560467757 | validation: 0.13758668078873748]
	TIME [epoch: 32.3 sec]
EPOCH 345/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1615924228930173		[learning rate: 0.00010839]
	Learning Rate: 0.000108393
	LOSS [training: 0.1615924228930173 | validation: 0.13817902720989683]
	TIME [epoch: 32.3 sec]
EPOCH 346/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16051666011230278		[learning rate: 0.00010674]
	Learning Rate: 0.000106742
	LOSS [training: 0.16051666011230278 | validation: 0.13048135358801494]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_346.pth
	Model improved!!!
EPOCH 347/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16191727259048766		[learning rate: 0.00010512]
	Learning Rate: 0.000105115
	LOSS [training: 0.16191727259048766 | validation: 0.14146564659434646]
	TIME [epoch: 32.3 sec]
EPOCH 348/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15852916237967393		[learning rate: 0.00010351]
	Learning Rate: 0.000103514
	LOSS [training: 0.15852916237967393 | validation: 0.13870494704167072]
	TIME [epoch: 32.3 sec]
EPOCH 349/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1539748429803987		[learning rate: 0.00010194]
	Learning Rate: 0.000101937
	LOSS [training: 0.1539748429803987 | validation: 0.13059783794534724]
	TIME [epoch: 32.3 sec]
EPOCH 350/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15650261511784758		[learning rate: 0.00010038]
	Learning Rate: 0.000100385
	LOSS [training: 0.15650261511784758 | validation: 0.13752478442755295]
	TIME [epoch: 32.2 sec]
EPOCH 351/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15523593645494224		[learning rate: 9.8855e-05]
	Learning Rate: 9.88553e-05
	LOSS [training: 0.15523593645494224 | validation: 0.13993626170924112]
	TIME [epoch: 32.3 sec]
EPOCH 352/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16038441800876152		[learning rate: 9.7349e-05]
	Learning Rate: 9.73494e-05
	LOSS [training: 0.16038441800876152 | validation: 0.1347473897136396]
	TIME [epoch: 32.3 sec]
EPOCH 353/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1577622463242088		[learning rate: 9.5866e-05]
	Learning Rate: 9.58665e-05
	LOSS [training: 0.1577622463242088 | validation: 0.13440423834569573]
	TIME [epoch: 32.3 sec]
EPOCH 354/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15805894857193448		[learning rate: 9.4406e-05]
	Learning Rate: 9.44061e-05
	LOSS [training: 0.15805894857193448 | validation: 0.14424950066024816]
	TIME [epoch: 32.3 sec]
EPOCH 355/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15323447606206494		[learning rate: 9.2968e-05]
	Learning Rate: 9.2968e-05
	LOSS [training: 0.15323447606206494 | validation: 0.12506623639551576]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_355.pth
	Model improved!!!
EPOCH 356/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1543407087244248		[learning rate: 9.1552e-05]
	Learning Rate: 9.15518e-05
	LOSS [training: 0.1543407087244248 | validation: 0.1325701147797107]
	TIME [epoch: 32.2 sec]
EPOCH 357/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1589496461615102		[learning rate: 9.0157e-05]
	Learning Rate: 9.01571e-05
	LOSS [training: 0.1589496461615102 | validation: 0.1320189714515047]
	TIME [epoch: 32.3 sec]
EPOCH 358/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15560450784863486		[learning rate: 8.8784e-05]
	Learning Rate: 8.87837e-05
	LOSS [training: 0.15560450784863486 | validation: 0.1315766221493923]
	TIME [epoch: 32.3 sec]
EPOCH 359/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15574411497161228		[learning rate: 8.7431e-05]
	Learning Rate: 8.74312e-05
	LOSS [training: 0.15574411497161228 | validation: 0.13338079748452858]
	TIME [epoch: 32.2 sec]
EPOCH 360/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15316020873117997		[learning rate: 8.6099e-05]
	Learning Rate: 8.60994e-05
	LOSS [training: 0.15316020873117997 | validation: 0.1330846210612482]
	TIME [epoch: 32.3 sec]
EPOCH 361/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15127462059362465		[learning rate: 8.4788e-05]
	Learning Rate: 8.47878e-05
	LOSS [training: 0.15127462059362465 | validation: 0.12650133184594317]
	TIME [epoch: 32.3 sec]
EPOCH 362/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15305641742616738		[learning rate: 8.3496e-05]
	Learning Rate: 8.34962e-05
	LOSS [training: 0.15305641742616738 | validation: 0.12724866719608308]
	TIME [epoch: 32.2 sec]
EPOCH 363/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15380460821042444		[learning rate: 8.2224e-05]
	Learning Rate: 8.22243e-05
	LOSS [training: 0.15380460821042444 | validation: 0.13384522676055785]
	TIME [epoch: 32.3 sec]
EPOCH 364/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15165775569147774		[learning rate: 8.0972e-05]
	Learning Rate: 8.09717e-05
	LOSS [training: 0.15165775569147774 | validation: 0.13438359168570366]
	TIME [epoch: 32.3 sec]
EPOCH 365/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15380323789729206		[learning rate: 7.9738e-05]
	Learning Rate: 7.97382e-05
	LOSS [training: 0.15380323789729206 | validation: 0.13017978355336152]
	TIME [epoch: 32.2 sec]
EPOCH 366/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15572589197319064		[learning rate: 7.8524e-05]
	Learning Rate: 7.85235e-05
	LOSS [training: 0.15572589197319064 | validation: 0.12706699841986221]
	TIME [epoch: 32.3 sec]
EPOCH 367/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1548602336980307		[learning rate: 7.7327e-05]
	Learning Rate: 7.73274e-05
	LOSS [training: 0.1548602336980307 | validation: 0.12472441500699219]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_367.pth
	Model improved!!!
EPOCH 368/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15402938149830425		[learning rate: 7.6149e-05]
	Learning Rate: 7.61494e-05
	LOSS [training: 0.15402938149830425 | validation: 0.12912023116438842]
	TIME [epoch: 32.3 sec]
EPOCH 369/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15373044470457647		[learning rate: 7.4989e-05]
	Learning Rate: 7.49894e-05
	LOSS [training: 0.15373044470457647 | validation: 0.1329617993395149]
	TIME [epoch: 32.4 sec]
EPOCH 370/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14783978846596277		[learning rate: 7.3847e-05]
	Learning Rate: 7.38471e-05
	LOSS [training: 0.14783978846596277 | validation: 0.1274944547648258]
	TIME [epoch: 32.4 sec]
EPOCH 371/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14934035556441103		[learning rate: 7.2722e-05]
	Learning Rate: 7.27221e-05
	LOSS [training: 0.14934035556441103 | validation: 0.12859201117796917]
	TIME [epoch: 32.3 sec]
EPOCH 372/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15279494669710803		[learning rate: 7.1614e-05]
	Learning Rate: 7.16143e-05
	LOSS [training: 0.15279494669710803 | validation: 0.12652055818177832]
	TIME [epoch: 32.4 sec]
EPOCH 373/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15301105391594638		[learning rate: 7.0523e-05]
	Learning Rate: 7.05234e-05
	LOSS [training: 0.15301105391594638 | validation: 0.1269185270063219]
	TIME [epoch: 32.4 sec]
EPOCH 374/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15526412173730308		[learning rate: 6.9449e-05]
	Learning Rate: 6.94491e-05
	LOSS [training: 0.15526412173730308 | validation: 0.12728076873051106]
	TIME [epoch: 32.3 sec]
EPOCH 375/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15293396851442226		[learning rate: 6.8391e-05]
	Learning Rate: 6.83912e-05
	LOSS [training: 0.15293396851442226 | validation: 0.12807735151209385]
	TIME [epoch: 32.4 sec]
EPOCH 376/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15150776905982039		[learning rate: 6.7349e-05]
	Learning Rate: 6.73493e-05
	LOSS [training: 0.15150776905982039 | validation: 0.12963084314183942]
	TIME [epoch: 32.4 sec]
EPOCH 377/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15289710738257992		[learning rate: 6.6323e-05]
	Learning Rate: 6.63234e-05
	LOSS [training: 0.15289710738257992 | validation: 0.13646353715035653]
	TIME [epoch: 32.4 sec]
EPOCH 378/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1500189274970795		[learning rate: 6.5313e-05]
	Learning Rate: 6.5313e-05
	LOSS [training: 0.1500189274970795 | validation: 0.12367067085293731]
	TIME [epoch: 32.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_378.pth
	Model improved!!!
EPOCH 379/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15210115536310342		[learning rate: 6.4318e-05]
	Learning Rate: 6.43181e-05
	LOSS [training: 0.15210115536310342 | validation: 0.11795113721685199]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_379.pth
	Model improved!!!
EPOCH 380/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15013630352777876		[learning rate: 6.3338e-05]
	Learning Rate: 6.33383e-05
	LOSS [training: 0.15013630352777876 | validation: 0.1279658768849178]
	TIME [epoch: 32.3 sec]
EPOCH 381/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1470613338375989		[learning rate: 6.2373e-05]
	Learning Rate: 6.23735e-05
	LOSS [training: 0.1470613338375989 | validation: 0.1307113425339913]
	TIME [epoch: 32.4 sec]
EPOCH 382/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14855327078255934		[learning rate: 6.1423e-05]
	Learning Rate: 6.14233e-05
	LOSS [training: 0.14855327078255934 | validation: 0.12136651356339606]
	TIME [epoch: 32.3 sec]
EPOCH 383/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14957703302408365		[learning rate: 6.0488e-05]
	Learning Rate: 6.04876e-05
	LOSS [training: 0.14957703302408365 | validation: 0.12382013580005456]
	TIME [epoch: 32.3 sec]
EPOCH 384/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14698443325201294		[learning rate: 5.9566e-05]
	Learning Rate: 5.95662e-05
	LOSS [training: 0.14698443325201294 | validation: 0.11757334122610433]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_384.pth
	Model improved!!!
EPOCH 385/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15079820307875175		[learning rate: 5.8659e-05]
	Learning Rate: 5.86588e-05
	LOSS [training: 0.15079820307875175 | validation: 0.12859348007923743]
	TIME [epoch: 32.3 sec]
EPOCH 386/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15165085664025268		[learning rate: 5.7765e-05]
	Learning Rate: 5.77652e-05
	LOSS [training: 0.15165085664025268 | validation: 0.12502728749255274]
	TIME [epoch: 32.3 sec]
EPOCH 387/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14912693248867323		[learning rate: 5.6885e-05]
	Learning Rate: 5.68853e-05
	LOSS [training: 0.14912693248867323 | validation: 0.11980183017158806]
	TIME [epoch: 32.3 sec]
EPOCH 388/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14553171586291297		[learning rate: 5.6019e-05]
	Learning Rate: 5.60187e-05
	LOSS [training: 0.14553171586291297 | validation: 0.12934812747065033]
	TIME [epoch: 32.3 sec]
EPOCH 389/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14908882080006963		[learning rate: 5.5165e-05]
	Learning Rate: 5.51654e-05
	LOSS [training: 0.14908882080006963 | validation: 0.12578151084035982]
	TIME [epoch: 32.3 sec]
EPOCH 390/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14836908434666873		[learning rate: 5.4325e-05]
	Learning Rate: 5.4325e-05
	LOSS [training: 0.14836908434666873 | validation: 0.12274545957056923]
	TIME [epoch: 32.3 sec]
EPOCH 391/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14759155717020825		[learning rate: 5.3497e-05]
	Learning Rate: 5.34975e-05
	LOSS [training: 0.14759155717020825 | validation: 0.11731644733075992]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_391.pth
	Model improved!!!
EPOCH 392/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1474882420362283		[learning rate: 5.2683e-05]
	Learning Rate: 5.26825e-05
	LOSS [training: 0.1474882420362283 | validation: 0.11932547078476707]
	TIME [epoch: 32.3 sec]
EPOCH 393/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14591786317078537		[learning rate: 5.188e-05]
	Learning Rate: 5.188e-05
	LOSS [training: 0.14591786317078537 | validation: 0.1262904232085863]
	TIME [epoch: 32.3 sec]
EPOCH 394/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1488409512018749		[learning rate: 5.109e-05]
	Learning Rate: 5.10897e-05
	LOSS [training: 0.1488409512018749 | validation: 0.11685591380925162]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_394.pth
	Model improved!!!
EPOCH 395/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14309641081052704		[learning rate: 5.0311e-05]
	Learning Rate: 5.03114e-05
	LOSS [training: 0.14309641081052704 | validation: 0.13216325505549997]
	TIME [epoch: 32.3 sec]
EPOCH 396/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1436151419796124		[learning rate: 4.9545e-05]
	Learning Rate: 4.9545e-05
	LOSS [training: 0.1436151419796124 | validation: 0.1154989144934547]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_396.pth
	Model improved!!!
EPOCH 397/500:
	Training over batches...
		[batch 4/4] avg loss: 0.145309428990832		[learning rate: 4.879e-05]
	Learning Rate: 4.87903e-05
	LOSS [training: 0.145309428990832 | validation: 0.12768180341219937]
	TIME [epoch: 32.2 sec]
EPOCH 398/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14422042478405545		[learning rate: 4.8047e-05]
	Learning Rate: 4.8047e-05
	LOSS [training: 0.14422042478405545 | validation: 0.12513993499270346]
	TIME [epoch: 32.3 sec]
EPOCH 399/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1488351158281851		[learning rate: 4.7315e-05]
	Learning Rate: 4.73151e-05
	LOSS [training: 0.1488351158281851 | validation: 0.1261449092256355]
	TIME [epoch: 32.3 sec]
EPOCH 400/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14684257638990494		[learning rate: 4.6594e-05]
	Learning Rate: 4.65944e-05
	LOSS [training: 0.14684257638990494 | validation: 0.12165232623569625]
	TIME [epoch: 32.2 sec]
EPOCH 401/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14605053699924728		[learning rate: 4.5885e-05]
	Learning Rate: 4.58846e-05
	LOSS [training: 0.14605053699924728 | validation: 0.1193302540390958]
	TIME [epoch: 32.3 sec]
EPOCH 402/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14516382143052126		[learning rate: 4.5186e-05]
	Learning Rate: 4.51856e-05
	LOSS [training: 0.14516382143052126 | validation: 0.1252125525399085]
	TIME [epoch: 32.3 sec]
EPOCH 403/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14624300938559526		[learning rate: 4.4497e-05]
	Learning Rate: 4.44973e-05
	LOSS [training: 0.14624300938559526 | validation: 0.11928854186335394]
	TIME [epoch: 32.3 sec]
EPOCH 404/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14637140995032105		[learning rate: 4.3819e-05]
	Learning Rate: 4.38194e-05
	LOSS [training: 0.14637140995032105 | validation: 0.12605724181450492]
	TIME [epoch: 32.3 sec]
EPOCH 405/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1456173918807918		[learning rate: 4.3152e-05]
	Learning Rate: 4.31519e-05
	LOSS [training: 0.1456173918807918 | validation: 0.12183225085987712]
	TIME [epoch: 32.3 sec]
EPOCH 406/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14629462757635053		[learning rate: 4.2495e-05]
	Learning Rate: 4.24946e-05
	LOSS [training: 0.14629462757635053 | validation: 0.12449715361001332]
	TIME [epoch: 32.3 sec]
EPOCH 407/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14390660029197613		[learning rate: 4.1847e-05]
	Learning Rate: 4.18472e-05
	LOSS [training: 0.14390660029197613 | validation: 0.11579120590929709]
	TIME [epoch: 32.3 sec]
EPOCH 408/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14147107732300576		[learning rate: 4.121e-05]
	Learning Rate: 4.12098e-05
	LOSS [training: 0.14147107732300576 | validation: 0.12771096282158934]
	TIME [epoch: 32.3 sec]
EPOCH 409/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14360868126331938		[learning rate: 4.0582e-05]
	Learning Rate: 4.0582e-05
	LOSS [training: 0.14360868126331938 | validation: 0.1255895707851078]
	TIME [epoch: 32.3 sec]
EPOCH 410/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14511435558835253		[learning rate: 3.9964e-05]
	Learning Rate: 3.99638e-05
	LOSS [training: 0.14511435558835253 | validation: 0.126183393220064]
	TIME [epoch: 32.3 sec]
EPOCH 411/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14758256235142095		[learning rate: 3.9355e-05]
	Learning Rate: 3.9355e-05
	LOSS [training: 0.14758256235142095 | validation: 0.11593020174619505]
	TIME [epoch: 32.3 sec]
EPOCH 412/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14414558755233392		[learning rate: 3.8755e-05]
	Learning Rate: 3.87555e-05
	LOSS [training: 0.14414558755233392 | validation: 0.12467824705756445]
	TIME [epoch: 32.3 sec]
EPOCH 413/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1458493832086562		[learning rate: 3.8165e-05]
	Learning Rate: 3.81651e-05
	LOSS [training: 0.1458493832086562 | validation: 0.11581332668537273]
	TIME [epoch: 32.3 sec]
EPOCH 414/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14483646733003466		[learning rate: 3.7584e-05]
	Learning Rate: 3.75837e-05
	LOSS [training: 0.14483646733003466 | validation: 0.12123320944607888]
	TIME [epoch: 32.4 sec]
EPOCH 415/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1451007113288338		[learning rate: 3.7011e-05]
	Learning Rate: 3.70112e-05
	LOSS [training: 0.1451007113288338 | validation: 0.11688783153456161]
	TIME [epoch: 32.3 sec]
EPOCH 416/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1462080949549455		[learning rate: 3.6447e-05]
	Learning Rate: 3.64474e-05
	LOSS [training: 0.1462080949549455 | validation: 0.12625692989894532]
	TIME [epoch: 32.4 sec]
EPOCH 417/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13992134195004746		[learning rate: 3.5892e-05]
	Learning Rate: 3.58922e-05
	LOSS [training: 0.13992134195004746 | validation: 0.11603634470278654]
	TIME [epoch: 32.3 sec]
EPOCH 418/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13986762696837604		[learning rate: 3.5345e-05]
	Learning Rate: 3.53454e-05
	LOSS [training: 0.13986762696837604 | validation: 0.12666009014786533]
	TIME [epoch: 32.3 sec]
EPOCH 419/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14477095596840472		[learning rate: 3.4807e-05]
	Learning Rate: 3.4807e-05
	LOSS [training: 0.14477095596840472 | validation: 0.11751826115710749]
	TIME [epoch: 32.4 sec]
EPOCH 420/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14414847304031858		[learning rate: 3.4277e-05]
	Learning Rate: 3.42768e-05
	LOSS [training: 0.14414847304031858 | validation: 0.11577316502912224]
	TIME [epoch: 32.3 sec]
EPOCH 421/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14316037175407073		[learning rate: 3.3755e-05]
	Learning Rate: 3.37546e-05
	LOSS [training: 0.14316037175407073 | validation: 0.12207373313260247]
	TIME [epoch: 32.3 sec]
EPOCH 422/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14626642673651047		[learning rate: 3.324e-05]
	Learning Rate: 3.32404e-05
	LOSS [training: 0.14626642673651047 | validation: 0.12399381532402763]
	TIME [epoch: 32.3 sec]
EPOCH 423/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14479965958749658		[learning rate: 3.2734e-05]
	Learning Rate: 3.27341e-05
	LOSS [training: 0.14479965958749658 | validation: 0.12049646770858749]
	TIME [epoch: 32.3 sec]
EPOCH 424/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1469037062800812		[learning rate: 3.2235e-05]
	Learning Rate: 3.22354e-05
	LOSS [training: 0.1469037062800812 | validation: 0.12121930993029509]
	TIME [epoch: 32.3 sec]
EPOCH 425/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1431996774705065		[learning rate: 3.1744e-05]
	Learning Rate: 3.17444e-05
	LOSS [training: 0.1431996774705065 | validation: 0.1120870029340021]
	TIME [epoch: 32.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_425.pth
	Model improved!!!
EPOCH 426/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1431238495729168		[learning rate: 3.1261e-05]
	Learning Rate: 3.12608e-05
	LOSS [training: 0.1431238495729168 | validation: 0.12114410001279163]
	TIME [epoch: 32.3 sec]
EPOCH 427/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14274951369996483		[learning rate: 3.0785e-05]
	Learning Rate: 3.07846e-05
	LOSS [training: 0.14274951369996483 | validation: 0.12082967327064545]
	TIME [epoch: 32.3 sec]
EPOCH 428/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14311839850810382		[learning rate: 3.0316e-05]
	Learning Rate: 3.03156e-05
	LOSS [training: 0.14311839850810382 | validation: 0.11637378933644107]
	TIME [epoch: 32.3 sec]
EPOCH 429/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14390239359856824		[learning rate: 2.9854e-05]
	Learning Rate: 2.98538e-05
	LOSS [training: 0.14390239359856824 | validation: 0.11920031194203246]
	TIME [epoch: 32.2 sec]
EPOCH 430/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14212992086332152		[learning rate: 2.9399e-05]
	Learning Rate: 2.9399e-05
	LOSS [training: 0.14212992086332152 | validation: 0.12010414575776238]
	TIME [epoch: 32.3 sec]
EPOCH 431/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14141047622197786		[learning rate: 2.8951e-05]
	Learning Rate: 2.89512e-05
	LOSS [training: 0.14141047622197786 | validation: 0.11803456799193261]
	TIME [epoch: 32.3 sec]
EPOCH 432/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14335509601258123		[learning rate: 2.851e-05]
	Learning Rate: 2.85102e-05
	LOSS [training: 0.14335509601258123 | validation: 0.11340728511305008]
	TIME [epoch: 32.3 sec]
EPOCH 433/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14177868888725542		[learning rate: 2.8076e-05]
	Learning Rate: 2.80759e-05
	LOSS [training: 0.14177868888725542 | validation: 0.12359499632710676]
	TIME [epoch: 32.3 sec]
EPOCH 434/500:
	Training over batches...
		[batch 4/4] avg loss: 0.145248854183443		[learning rate: 2.7648e-05]
	Learning Rate: 2.76482e-05
	LOSS [training: 0.145248854183443 | validation: 0.11589753033879047]
	TIME [epoch: 32.3 sec]
EPOCH 435/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14070044923671438		[learning rate: 2.7227e-05]
	Learning Rate: 2.7227e-05
	LOSS [training: 0.14070044923671438 | validation: 0.12235496207083967]
	TIME [epoch: 32.2 sec]
EPOCH 436/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14112974988183874		[learning rate: 2.6812e-05]
	Learning Rate: 2.68122e-05
	LOSS [training: 0.14112974988183874 | validation: 0.11962839559365208]
	TIME [epoch: 32.3 sec]
EPOCH 437/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13940560992063827		[learning rate: 2.6404e-05]
	Learning Rate: 2.64038e-05
	LOSS [training: 0.13940560992063827 | validation: 0.11218057203451472]
	TIME [epoch: 32.3 sec]
EPOCH 438/500:
	Training over batches...
		[batch 4/4] avg loss: 0.142104655812494		[learning rate: 2.6002e-05]
	Learning Rate: 2.60016e-05
	LOSS [training: 0.142104655812494 | validation: 0.11842013845136677]
	TIME [epoch: 32.2 sec]
EPOCH 439/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14228809379649016		[learning rate: 2.5605e-05]
	Learning Rate: 2.56055e-05
	LOSS [training: 0.14228809379649016 | validation: 0.12303388578463395]
	TIME [epoch: 32.3 sec]
EPOCH 440/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14430423085260702		[learning rate: 2.5215e-05]
	Learning Rate: 2.52154e-05
	LOSS [training: 0.14430423085260702 | validation: 0.12789454344573314]
	TIME [epoch: 32.3 sec]
EPOCH 441/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14301657793277306		[learning rate: 2.4831e-05]
	Learning Rate: 2.48313e-05
	LOSS [training: 0.14301657793277306 | validation: 0.115127957246042]
	TIME [epoch: 32.3 sec]
EPOCH 442/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14115652330966694		[learning rate: 2.4453e-05]
	Learning Rate: 2.44531e-05
	LOSS [training: 0.14115652330966694 | validation: 0.11890467629000709]
	TIME [epoch: 32.3 sec]
EPOCH 443/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1408816422452118		[learning rate: 2.4081e-05]
	Learning Rate: 2.40806e-05
	LOSS [training: 0.1408816422452118 | validation: 0.1177297705310822]
	TIME [epoch: 32.3 sec]
EPOCH 444/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14238091164139016		[learning rate: 2.3714e-05]
	Learning Rate: 2.37137e-05
	LOSS [training: 0.14238091164139016 | validation: 0.11771841886283543]
	TIME [epoch: 32.3 sec]
EPOCH 445/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14147156308916323		[learning rate: 2.3352e-05]
	Learning Rate: 2.33525e-05
	LOSS [training: 0.14147156308916323 | validation: 0.11899226739153117]
	TIME [epoch: 32.3 sec]
EPOCH 446/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1441277270264551		[learning rate: 2.2997e-05]
	Learning Rate: 2.29968e-05
	LOSS [training: 0.1441277270264551 | validation: 0.11220051195988165]
	TIME [epoch: 32.3 sec]
EPOCH 447/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14097257042818823		[learning rate: 2.2646e-05]
	Learning Rate: 2.26464e-05
	LOSS [training: 0.14097257042818823 | validation: 0.11936683712368257]
	TIME [epoch: 32.3 sec]
EPOCH 448/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14053037978364546		[learning rate: 2.2301e-05]
	Learning Rate: 2.23015e-05
	LOSS [training: 0.14053037978364546 | validation: 0.12427909813330626]
	TIME [epoch: 32.3 sec]
EPOCH 449/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13944166681072429		[learning rate: 2.1962e-05]
	Learning Rate: 2.19617e-05
	LOSS [training: 0.13944166681072429 | validation: 0.11380544226234794]
	TIME [epoch: 32.2 sec]
EPOCH 450/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14180982545475726		[learning rate: 2.1627e-05]
	Learning Rate: 2.16272e-05
	LOSS [training: 0.14180982545475726 | validation: 0.12106126557487565]
	TIME [epoch: 32.3 sec]
EPOCH 451/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13775101539702805		[learning rate: 2.1298e-05]
	Learning Rate: 2.12977e-05
	LOSS [training: 0.13775101539702805 | validation: 0.11828433510226416]
	TIME [epoch: 32.3 sec]
EPOCH 452/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14036166357508928		[learning rate: 2.0973e-05]
	Learning Rate: 2.09733e-05
	LOSS [training: 0.14036166357508928 | validation: 0.1191952692624259]
	TIME [epoch: 32.3 sec]
EPOCH 453/500:
	Training over batches...
		[batch 4/4] avg loss: 0.142191194936591		[learning rate: 2.0654e-05]
	Learning Rate: 2.06538e-05
	LOSS [training: 0.142191194936591 | validation: 0.11213031335310382]
	TIME [epoch: 32.3 sec]
EPOCH 454/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14162014352415775		[learning rate: 2.0339e-05]
	Learning Rate: 2.03392e-05
	LOSS [training: 0.14162014352415775 | validation: 0.11243428265816677]
	TIME [epoch: 32.3 sec]
EPOCH 455/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14088907641643236		[learning rate: 2.0029e-05]
	Learning Rate: 2.00293e-05
	LOSS [training: 0.14088907641643236 | validation: 0.11920181802359321]
	TIME [epoch: 32.3 sec]
EPOCH 456/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13912853438194528		[learning rate: 1.9724e-05]
	Learning Rate: 1.97242e-05
	LOSS [training: 0.13912853438194528 | validation: 0.11831242591682448]
	TIME [epoch: 32.3 sec]
EPOCH 457/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14075910263522953		[learning rate: 1.9424e-05]
	Learning Rate: 1.94238e-05
	LOSS [training: 0.14075910263522953 | validation: 0.11419671386051305]
	TIME [epoch: 32.3 sec]
EPOCH 458/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13783890168719393		[learning rate: 1.9128e-05]
	Learning Rate: 1.91279e-05
	LOSS [training: 0.13783890168719393 | validation: 0.11475565880560412]
	TIME [epoch: 32.3 sec]
EPOCH 459/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1424113414250889		[learning rate: 1.8836e-05]
	Learning Rate: 1.88365e-05
	LOSS [training: 0.1424113414250889 | validation: 0.11888738792147403]
	TIME [epoch: 32.3 sec]
EPOCH 460/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13692169635497697		[learning rate: 1.855e-05]
	Learning Rate: 1.85495e-05
	LOSS [training: 0.13692169635497697 | validation: 0.11247481828233463]
	TIME [epoch: 32.3 sec]
EPOCH 461/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1383521940651008		[learning rate: 1.8267e-05]
	Learning Rate: 1.8267e-05
	LOSS [training: 0.1383521940651008 | validation: 0.1161000590891936]
	TIME [epoch: 32.3 sec]
EPOCH 462/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14045988728426015		[learning rate: 1.7989e-05]
	Learning Rate: 1.79887e-05
	LOSS [training: 0.14045988728426015 | validation: 0.1221105015740889]
	TIME [epoch: 32.3 sec]
EPOCH 463/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14100558444577618		[learning rate: 1.7715e-05]
	Learning Rate: 1.77147e-05
	LOSS [training: 0.14100558444577618 | validation: 0.11771013526057494]
	TIME [epoch: 32.3 sec]
EPOCH 464/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14240513295151772		[learning rate: 1.7445e-05]
	Learning Rate: 1.74448e-05
	LOSS [training: 0.14240513295151772 | validation: 0.11990077843037282]
	TIME [epoch: 32.3 sec]
EPOCH 465/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1418585871280551		[learning rate: 1.7179e-05]
	Learning Rate: 1.71791e-05
	LOSS [training: 0.1418585871280551 | validation: 0.11217255119826461]
	TIME [epoch: 32.3 sec]
EPOCH 466/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13893976744479486		[learning rate: 1.6917e-05]
	Learning Rate: 1.69174e-05
	LOSS [training: 0.13893976744479486 | validation: 0.11271417719678689]
	TIME [epoch: 32.3 sec]
EPOCH 467/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14235069677761797		[learning rate: 1.666e-05]
	Learning Rate: 1.66597e-05
	LOSS [training: 0.14235069677761797 | validation: 0.118714438264613]
	TIME [epoch: 32.3 sec]
EPOCH 468/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1409898027394837		[learning rate: 1.6406e-05]
	Learning Rate: 1.64059e-05
	LOSS [training: 0.1409898027394837 | validation: 0.12035452883809059]
	TIME [epoch: 32.3 sec]
EPOCH 469/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13806686388296677		[learning rate: 1.6156e-05]
	Learning Rate: 1.6156e-05
	LOSS [training: 0.13806686388296677 | validation: 0.11645317275750751]
	TIME [epoch: 32.3 sec]
EPOCH 470/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13859076237623358		[learning rate: 1.591e-05]
	Learning Rate: 1.59099e-05
	LOSS [training: 0.13859076237623358 | validation: 0.12241730701799212]
	TIME [epoch: 32.3 sec]
EPOCH 471/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14133719443015474		[learning rate: 1.5668e-05]
	Learning Rate: 1.56675e-05
	LOSS [training: 0.14133719443015474 | validation: 0.11478663172023082]
	TIME [epoch: 32.3 sec]
EPOCH 472/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13774277269678797		[learning rate: 1.5429e-05]
	Learning Rate: 1.54288e-05
	LOSS [training: 0.13774277269678797 | validation: 0.11913712991725184]
	TIME [epoch: 32.3 sec]
EPOCH 473/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14088067553833977		[learning rate: 1.5194e-05]
	Learning Rate: 1.51938e-05
	LOSS [training: 0.14088067553833977 | validation: 0.11288052084494267]
	TIME [epoch: 32.3 sec]
EPOCH 474/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14050579196859145		[learning rate: 1.4962e-05]
	Learning Rate: 1.49624e-05
	LOSS [training: 0.14050579196859145 | validation: 0.1098171707452037]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd2_20240711_013248/states/model_algphiq_1a_v_mmd2_474.pth
	Model improved!!!
EPOCH 475/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13668891049224735		[learning rate: 1.4734e-05]
	Learning Rate: 1.47344e-05
	LOSS [training: 0.13668891049224735 | validation: 0.1244209802358303]
	TIME [epoch: 32.3 sec]
EPOCH 476/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13734090091891596		[learning rate: 1.451e-05]
	Learning Rate: 1.451e-05
	LOSS [training: 0.13734090091891596 | validation: 0.11400602211379332]
	TIME [epoch: 32.3 sec]
EPOCH 477/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1397901477835156		[learning rate: 1.4289e-05]
	Learning Rate: 1.42889e-05
	LOSS [training: 0.1397901477835156 | validation: 0.1142307553154331]
	TIME [epoch: 32.3 sec]
EPOCH 478/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14047468240154448		[learning rate: 1.4071e-05]
	Learning Rate: 1.40713e-05
	LOSS [training: 0.14047468240154448 | validation: 0.11523845762875648]
	TIME [epoch: 32.3 sec]
EPOCH 479/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14148982143743072		[learning rate: 1.3857e-05]
	Learning Rate: 1.38569e-05
	LOSS [training: 0.14148982143743072 | validation: 0.11617939736566538]
	TIME [epoch: 32.4 sec]
EPOCH 480/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1370106606507775		[learning rate: 1.3646e-05]
	Learning Rate: 1.36458e-05
	LOSS [training: 0.1370106606507775 | validation: 0.11116571065861729]
	TIME [epoch: 32.3 sec]
EPOCH 481/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13974416819117247		[learning rate: 1.3438e-05]
	Learning Rate: 1.3438e-05
	LOSS [training: 0.13974416819117247 | validation: 0.11919326612064692]
	TIME [epoch: 32.3 sec]
EPOCH 482/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1356608819608914		[learning rate: 1.3233e-05]
	Learning Rate: 1.32333e-05
	LOSS [training: 0.1356608819608914 | validation: 0.11665029605185957]
	TIME [epoch: 32.3 sec]
EPOCH 483/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1406994499986859		[learning rate: 1.3032e-05]
	Learning Rate: 1.30317e-05
	LOSS [training: 0.1406994499986859 | validation: 0.10983162852148681]
	TIME [epoch: 32.3 sec]
EPOCH 484/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13730753334417842		[learning rate: 1.2833e-05]
	Learning Rate: 1.28332e-05
	LOSS [training: 0.13730753334417842 | validation: 0.11466443026691453]
	TIME [epoch: 32.3 sec]
EPOCH 485/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13638856891751694		[learning rate: 1.2638e-05]
	Learning Rate: 1.26377e-05
	LOSS [training: 0.13638856891751694 | validation: 0.11389428655382666]
	TIME [epoch: 32.3 sec]
EPOCH 486/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13665850856099618		[learning rate: 1.2445e-05]
	Learning Rate: 1.24451e-05
	LOSS [training: 0.13665850856099618 | validation: 0.11018982551159587]
	TIME [epoch: 32.3 sec]
EPOCH 487/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13549577624401637		[learning rate: 1.2256e-05]
	Learning Rate: 1.22556e-05
	LOSS [training: 0.13549577624401637 | validation: 0.12142317204174966]
	TIME [epoch: 32.2 sec]
EPOCH 488/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13737017252882444		[learning rate: 1.2069e-05]
	Learning Rate: 1.20689e-05
	LOSS [training: 0.13737017252882444 | validation: 0.11201001191988882]
	TIME [epoch: 32.3 sec]
EPOCH 489/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13761297792160077		[learning rate: 1.1885e-05]
	Learning Rate: 1.1885e-05
	LOSS [training: 0.13761297792160077 | validation: 0.11657116741761733]
	TIME [epoch: 32.3 sec]
EPOCH 490/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13911868574122663		[learning rate: 1.1704e-05]
	Learning Rate: 1.1704e-05
	LOSS [training: 0.13911868574122663 | validation: 0.11372824319046257]
	TIME [epoch: 32.3 sec]
EPOCH 491/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1395367745343701		[learning rate: 1.1526e-05]
	Learning Rate: 1.15257e-05
	LOSS [training: 0.1395367745343701 | validation: 0.11564085578394855]
	TIME [epoch: 32.3 sec]
EPOCH 492/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13820057670842972		[learning rate: 1.135e-05]
	Learning Rate: 1.13501e-05
	LOSS [training: 0.13820057670842972 | validation: 0.11350477145468715]
	TIME [epoch: 32.3 sec]
EPOCH 493/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13849778419993938		[learning rate: 1.1177e-05]
	Learning Rate: 1.11772e-05
	LOSS [training: 0.13849778419993938 | validation: 0.11173675530517635]
	TIME [epoch: 32.3 sec]
EPOCH 494/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13767869545148692		[learning rate: 1.1007e-05]
	Learning Rate: 1.10069e-05
	LOSS [training: 0.13767869545148692 | validation: 0.11623369076760755]
	TIME [epoch: 32.3 sec]
EPOCH 495/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1396462132529054		[learning rate: 1.0839e-05]
	Learning Rate: 1.08393e-05
	LOSS [training: 0.1396462132529054 | validation: 0.11749676197954485]
	TIME [epoch: 32.3 sec]
EPOCH 496/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13740074948418537		[learning rate: 1.0674e-05]
	Learning Rate: 1.06741e-05
	LOSS [training: 0.13740074948418537 | validation: 0.11763666793532063]
	TIME [epoch: 32.3 sec]
EPOCH 497/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13592574348362102		[learning rate: 1.0512e-05]
	Learning Rate: 1.05115e-05
	LOSS [training: 0.13592574348362102 | validation: 0.11347889741367781]
	TIME [epoch: 32.3 sec]
EPOCH 498/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13900363139505664		[learning rate: 1.0351e-05]
	Learning Rate: 1.03514e-05
	LOSS [training: 0.13900363139505664 | validation: 0.11655848630762108]
	TIME [epoch: 32.3 sec]
EPOCH 499/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13883140273403377		[learning rate: 1.0194e-05]
	Learning Rate: 1.01937e-05
	LOSS [training: 0.13883140273403377 | validation: 0.11768676191686654]
	TIME [epoch: 32.3 sec]
EPOCH 500/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1373077417714368		[learning rate: 1.0038e-05]
	Learning Rate: 1.00384e-05
	LOSS [training: 0.1373077417714368 | validation: 0.1200405115884775]
	TIME [epoch: 32.3 sec]
Finished training in 11231.473 seconds.
