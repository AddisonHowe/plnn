Args:
Namespace(name='model_algphiq_1a_v_mmd5', outdir='out/model_training/model_algphiq_1a_v_mmd5', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='quadratic', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[50, 100, 250], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[10.0], optimizer='rms', momentum=0.0, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 301512291

Training model...

Saving initial model state to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6173475778109188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6173475778109188 | validation: 0.6207700552047626]
	TIME [epoch: 109 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6027106174656542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6027106174656542 | validation: 0.6080424098238912]
	TIME [epoch: 4.28 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_2.pth
	Model improved!!!
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5893260996896987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5893260996896987 | validation: 0.5948098556304631]
	TIME [epoch: 4.12 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_3.pth
	Model improved!!!
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5750051816343645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5750051816343645 | validation: 0.58004572717353]
	TIME [epoch: 4.17 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_4.pth
	Model improved!!!
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5589003153123633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5589003153123633 | validation: 0.5631277116399902]
	TIME [epoch: 4.15 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_5.pth
	Model improved!!!
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5400922872406619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5400922872406619 | validation: 0.5430920084784971]
	TIME [epoch: 4.14 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_6.pth
	Model improved!!!
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5178351497791126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5178351497791126 | validation: 0.5189229343181225]
	TIME [epoch: 4.14 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_7.pth
	Model improved!!!
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4904555462077952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4904555462077952 | validation: 0.48893074885466603]
	TIME [epoch: 4.14 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_8.pth
	Model improved!!!
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4560884490454325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4560884490454325 | validation: 0.45051245056550576]
	TIME [epoch: 4.14 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_9.pth
	Model improved!!!
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 0.41151891203242563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41151891203242563 | validation: 0.3992409008458011]
	TIME [epoch: 4.15 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_10.pth
	Model improved!!!
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3511400842963489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3511400842963489 | validation: 0.3266057228609398]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_11.pth
	Model improved!!!
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 0.266644867777614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.266644867777614 | validation: 0.2249559051779188]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_12.pth
	Model improved!!!
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1596947635433713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1596947635433713 | validation: 0.10371897582244356]
	TIME [epoch: 4.16 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_13.pth
	Model improved!!!
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 0.051265508358113214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051265508358113214 | validation: 0.0066389772320243345]
	TIME [epoch: 4.15 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_14.pth
	Model improved!!!
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003245267784790743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.003245267784790743 | validation: 0.0012805075343572932]
	TIME [epoch: 4.11 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_15.pth
	Model improved!!!
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009979666700235478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0009979666700235478 | validation: 0.0006378721745898592]
	TIME [epoch: 4.14 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_16.pth
	Model improved!!!
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007115771001793049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0007115771001793049 | validation: 0.0005739774145334]
	TIME [epoch: 4.13 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_17.pth
	Model improved!!!
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00071309042254796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00071309042254796 | validation: 0.0009230706527978219]
	TIME [epoch: 4.11 sec]
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003930850514730982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.003930850514730982 | validation: 0.003435684433487362]
	TIME [epoch: 4.16 sec]
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003654268310076946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.003654268310076946 | validation: 0.0012676556281607456]
	TIME [epoch: 4.14 sec]
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001279392751398819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.001279392751398819 | validation: 0.0016039536431442709]
	TIME [epoch: 4.11 sec]
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002634433254101921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002634433254101921 | validation: 0.002351938986346081]
	TIME [epoch: 4.12 sec]
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0029521224089577827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0029521224089577827 | validation: 0.002300119679269393]
	TIME [epoch: 4.14 sec]
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021262286053372764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0021262286053372764 | validation: 0.00208037603361991]
	TIME [epoch: 4.11 sec]
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002492394524073875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002492394524073875 | validation: 0.0025679708263479906]
	TIME [epoch: 4.1 sec]
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002977318983575226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002977318983575226 | validation: 0.0021415026312386837]
	TIME [epoch: 4.12 sec]
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021732691756287444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0021732691756287444 | validation: 0.0014586830000272244]
	TIME [epoch: 4.12 sec]
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021080922377257815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0021080922377257815 | validation: 0.002502094279601604]
	TIME [epoch: 4.15 sec]
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0030093320928020638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0030093320928020638 | validation: 0.001889561890319367]
	TIME [epoch: 4.12 sec]
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002225995864251847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002225995864251847 | validation: 0.0016498159851273005]
	TIME [epoch: 4.12 sec]
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0022388210494545652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0022388210494545652 | validation: 0.0019436410890440996]
	TIME [epoch: 4.11 sec]
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003069604189089843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.003069604189089843 | validation: 0.002063996483298255]
	TIME [epoch: 4.1 sec]
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00212636068689487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00212636068689487 | validation: 0.0017218310664185823]
	TIME [epoch: 4.12 sec]
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019629619202237054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0019629619202237054 | validation: 0.0027516914256533634]
	TIME [epoch: 4.1 sec]
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0034376287359123726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0034376287359123726 | validation: 0.0017383835124705482]
	TIME [epoch: 4.11 sec]
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001972424408034321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.001972424408034321 | validation: 0.0016665998711204392]
	TIME [epoch: 4.16 sec]
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021055190485953132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0021055190485953132 | validation: 0.002701200463482192]
	TIME [epoch: 4.13 sec]
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0030117041711753955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0030117041711753955 | validation: 0.0023766127151156526]
	TIME [epoch: 4.11 sec]
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0023356367206636137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0023356367206636137 | validation: 0.0021478154066247833]
	TIME [epoch: 4.13 sec]
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002477973908099712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002477973908099712 | validation: 0.0016935270818225958]
	TIME [epoch: 4.11 sec]
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002123900425523958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002123900425523958 | validation: 0.001824628521297046]
	TIME [epoch: 4.13 sec]
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002435093126700838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002435093126700838 | validation: 0.0031900323369709518]
	TIME [epoch: 4.12 sec]
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0037450456329874322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0037450456329874322 | validation: 0.0012540130016430971]
	TIME [epoch: 4.13 sec]
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013281731979276484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0013281731979276484 | validation: 0.001363350037726174]
	TIME [epoch: 4.16 sec]
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018809165940275306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0018809165940275306 | validation: 0.0023671298095603605]
	TIME [epoch: 4.16 sec]
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0029912540619061586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0029912540619061586 | validation: 0.002781734083511753]
	TIME [epoch: 4.1 sec]
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0029044942507501104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0029044942507501104 | validation: 0.002023359997586011]
	TIME [epoch: 4.15 sec]
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002033334495745884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002033334495745884 | validation: 0.00186879880071313]
	TIME [epoch: 4.11 sec]
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0023861309004461614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0023861309004461614 | validation: 0.0021640309586151785]
	TIME [epoch: 4.14 sec]
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0024038092914173657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0024038092914173657 | validation: 0.0019541408011381064]
	TIME [epoch: 4.13 sec]
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002363741643435926		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 0.002363741643435926 | validation: 0.002489145000596541]
	TIME [epoch: 110 sec]
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002724405993447813		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 0.002724405993447813 | validation: 0.0015408325790593914]
	TIME [epoch: 7.94 sec]
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001627372987475209		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 0.001627372987475209 | validation: 0.0014342075888840598]
	TIME [epoch: 7.92 sec]
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021024610051652414		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 0.0021024610051652414 | validation: 0.0025720865025173865]
	TIME [epoch: 7.92 sec]
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002454166773531311		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 0.002454166773531311 | validation: 0.001855563625713538]
	TIME [epoch: 7.99 sec]
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002023846143186818		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 0.002023846143186818 | validation: 0.0014801391569961506]
	TIME [epoch: 8.07 sec]
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016559452563056898		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 0.0016559452563056898 | validation: 0.0019537654852142747]
	TIME [epoch: 8 sec]
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002219033324329893		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 0.002219033324329893 | validation: 0.0021042439629074365]
	TIME [epoch: 8.01 sec]
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002367279171557336		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 0.002367279171557336 | validation: 0.0010629714642751034]
	TIME [epoch: 7.99 sec]
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010540109043212845		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.0010540109043212845 | validation: 0.0010712229272578679]
	TIME [epoch: 7.98 sec]
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018840259378391224		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 0.0018840259378391224 | validation: 0.0019053470924073274]
	TIME [epoch: 8.08 sec]
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020694885545610646		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 0.0020694885545610646 | validation: 0.001843606067771718]
	TIME [epoch: 7.98 sec]
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016464948018339034		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 0.0016464948018339034 | validation: 0.0010928490929839828]
	TIME [epoch: 7.99 sec]
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013764828378660986		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 0.0013764828378660986 | validation: 0.0014758217847876147]
	TIME [epoch: 8 sec]
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002129005267652509		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 0.002129005267652509 | validation: 0.001219302626633124]
	TIME [epoch: 8.02 sec]
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012144996225905793		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 0.0012144996225905793 | validation: 0.0009168118277152449]
	TIME [epoch: 8.03 sec]
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014140604967367624		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 0.0014140604967367624 | validation: 0.0015608128025190308]
	TIME [epoch: 8.02 sec]
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018988017720491658		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 0.0018988017720491658 | validation: 0.0011321870715415888]
	TIME [epoch: 8 sec]
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010126196796487875		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 0.0010126196796487875 | validation: 0.001141976393810831]
	TIME [epoch: 8.01 sec]
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013927372858854143		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 0.0013927372858854143 | validation: 0.0018397855368817331]
	TIME [epoch: 8.05 sec]
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019728826327772785		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 0.0019728826327772785 | validation: 0.0010855578450210544]
	TIME [epoch: 8.11 sec]
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011067035065789846		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 0.0011067035065789846 | validation: 0.0009833727926478579]
	TIME [epoch: 8.05 sec]
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001231289388038415		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 0.001231289388038415 | validation: 0.0010767504186153065]
	TIME [epoch: 8.04 sec]
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014375128467792098		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 0.0014375128467792098 | validation: 0.000846445123023943]
	TIME [epoch: 8.07 sec]
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011286040138918428		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.0011286040138918428 | validation: 0.0009702339889983744]
	TIME [epoch: 8.09 sec]
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013300602293299182		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 0.0013300602293299182 | validation: 0.0011202578091980293]
	TIME [epoch: 8.04 sec]
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013053477252040523		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 0.0013053477252040523 | validation: 0.0008847575538501152]
	TIME [epoch: 8 sec]
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000977588315792414		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 0.000977588315792414 | validation: 0.0011338185573919982]
	TIME [epoch: 8.03 sec]
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014380561601211308		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 0.0014380561601211308 | validation: 0.001027255801435113]
	TIME [epoch: 7.99 sec]
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010227642724037881		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 0.0010227642724037881 | validation: 0.0009276598888710709]
	TIME [epoch: 8.08 sec]
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011003212603950523		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 0.0011003212603950523 | validation: 0.0009111799867523762]
	TIME [epoch: 8.03 sec]
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010637746090373815		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 0.0010637746090373815 | validation: 0.0009590165491858399]
	TIME [epoch: 8.03 sec]
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010403860164441689		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 0.0010403860164441689 | validation: 0.0010325349214538471]
	TIME [epoch: 8.03 sec]
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010370708186419333		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 0.0010370708186419333 | validation: 0.0006796349104362711]
	TIME [epoch: 8.06 sec]
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009298995875066858		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 0.0009298995875066858 | validation: 0.0007864136529608637]
	TIME [epoch: 8.08 sec]
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008459895270557384		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 0.0008459895270557384 | validation: 0.0009629679239710904]
	TIME [epoch: 8.05 sec]
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011549664121643494		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 0.0011549664121643494 | validation: 0.0010132737101970057]
	TIME [epoch: 8.03 sec]
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010159662344114604		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 0.0010159662344114604 | validation: 0.0005755597244466118]
	TIME [epoch: 8.01 sec]
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006240899504546435		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 0.0006240899504546435 | validation: 0.0005964729633558545]
	TIME [epoch: 8.06 sec]
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008780003876972717		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.0008780003876972717 | validation: 0.0009535252647768036]
	TIME [epoch: 8.09 sec]
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010077686604300054		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 0.0010077686604300054 | validation: 0.0008212777349630867]
	TIME [epoch: 8.07 sec]
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009367907489707701		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 0.0009367907489707701 | validation: 0.0005061303384986249]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_92.pth
	Model improved!!!
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005571409934256484		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 0.0005571409934256484 | validation: 0.0004616557783121109]
	TIME [epoch: 8.05 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_93.pth
	Model improved!!!
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006488650704146796		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 0.0006488650704146796 | validation: 0.0007200345610398465]
	TIME [epoch: 8.09 sec]
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010094045733737402		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 0.0010094045733737402 | validation: 0.00110688320717742]
	TIME [epoch: 8.07 sec]
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008220155513269161		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 0.0008220155513269161 | validation: 0.0005316407089588362]
	TIME [epoch: 8.02 sec]
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006527911744678767		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 0.0006527911744678767 | validation: 0.0005251143887261348]
	TIME [epoch: 8.04 sec]
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006356011986101042		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 0.0006356011986101042 | validation: 0.0005662229452131548]
	TIME [epoch: 8.06 sec]
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007386870573408043		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 0.0007386870573408043 | validation: 0.0006450436352843687]
	TIME [epoch: 8.12 sec]
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007035734674504567		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 0.0007035734674504567 | validation: 0.0003833007802230879]
	TIME [epoch: 8.04 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_100.pth
	Model improved!!!
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004843360596454751		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 0.0004843360596454751 | validation: 0.0005824026560629163]
	TIME [epoch: 121 sec]
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007499977770390016		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 0.0007499977770390016 | validation: 0.000527522084427877]
	TIME [epoch: 18.1 sec]
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005749556016436642		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 0.0005749556016436642 | validation: 0.0005071329095985388]
	TIME [epoch: 18.3 sec]
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005456031032223751		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 0.0005456031032223751 | validation: 0.0006702536234923187]
	TIME [epoch: 18.4 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007488735203648319		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.0007488735203648319 | validation: 0.000521522732156841]
	TIME [epoch: 18.5 sec]
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005309397235860663		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 0.0005309397235860663 | validation: 0.00034522316151266465]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_106.pth
	Model improved!!!
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004067992063191063		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 0.0004067992063191063 | validation: 0.0005093326554204274]
	TIME [epoch: 18.4 sec]
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006176325948243272		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 0.0006176325948243272 | validation: 0.0006578505048153358]
	TIME [epoch: 18.4 sec]
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006344281226880632		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: 0.0006344281226880632 | validation: 0.000336159313109269]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_109.pth
	Model improved!!!
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000385841067473223		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: 0.000385841067473223 | validation: 0.0003359885593735681]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_110.pth
	Model improved!!!
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00040404355016015824		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: 0.00040404355016015824 | validation: 0.0005360191177759941]
	TIME [epoch: 18.4 sec]
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006810912890121046		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: 0.0006810912890121046 | validation: 0.00033126532595610496]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_112.pth
	Model improved!!!
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00029210087021697107		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: 0.00029210087021697107 | validation: 0.00020460018401031023]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_113.pth
	Model improved!!!
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00030395185310565497		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: 0.00030395185310565497 | validation: 0.00046367047729880585]
	TIME [epoch: 18.5 sec]
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006045569479425714		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: 0.0006045569479425714 | validation: 0.00041662541665026323]
	TIME [epoch: 18.8 sec]
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003823622784030374		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: 0.0003823622784030374 | validation: 0.0002340699320203248]
	TIME [epoch: 18.5 sec]
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00029363458288941846		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: 0.00029363458288941846 | validation: 0.000332853961964642]
	TIME [epoch: 18.5 sec]
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00046590928142887454		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: 0.00046590928142887454 | validation: 0.00025025245616491223]
	TIME [epoch: 18.5 sec]
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00028560384935176963		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: 0.00028560384935176963 | validation: 0.0002937797953282633]
	TIME [epoch: 18.4 sec]
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003572839914954852		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.0003572839914954852 | validation: 0.00037314119665104117]
	TIME [epoch: 18.5 sec]
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00038791708749863826		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: 0.00038791708749863826 | validation: 0.00029904147957211477]
	TIME [epoch: 18.3 sec]
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003326593500184367		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: 0.0003326593500184367 | validation: 0.0002791444062407065]
	TIME [epoch: 18.4 sec]
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00034559152118730254		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: 0.00034559152118730254 | validation: 0.0002220304452969295]
	TIME [epoch: 18.5 sec]
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003140499280922633		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: 0.0003140499280922633 | validation: 0.0002098523983401914]
	TIME [epoch: 18.4 sec]
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00025537335548985656		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: 0.00025537335548985656 | validation: 0.00028502002172783045]
	TIME [epoch: 18.4 sec]
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00028162068587512146		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: 0.00028162068587512146 | validation: 0.00028341844719192695]
	TIME [epoch: 18.5 sec]
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00031367307161160986		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: 0.00031367307161160986 | validation: 0.00026590637187524544]
	TIME [epoch: 18.5 sec]
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003211591279847982		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: 0.0003211591279847982 | validation: 0.00017221921024639976]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_128.pth
	Model improved!!!
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00017545449164380124		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: 0.00017545449164380124 | validation: 0.00018436912283292762]
	TIME [epoch: 18.5 sec]
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00026265208751445813		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: 0.00026265208751445813 | validation: 0.00023526772375718629]
	TIME [epoch: 18.3 sec]
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00023445844172284282		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: 0.00023445844172284282 | validation: 0.00017226107836722715]
	TIME [epoch: 18.4 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00021318199387073487		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: 0.00021318199387073487 | validation: 0.00017687246987532236]
	TIME [epoch: 18.4 sec]
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002663581167819211		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: 0.0002663581167819211 | validation: 0.0002605493456145307]
	TIME [epoch: 18.5 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00021549918151776904		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: 0.00021549918151776904 | validation: 0.00010500372376749479]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_134.pth
	Model improved!!!
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00018352836107463845		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 0.00018352836107463845 | validation: 0.00014474409404478973]
	TIME [epoch: 18.5 sec]
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00020968628678615212		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: 0.00020968628678615212 | validation: 0.00017730840074853038]
	TIME [epoch: 18.4 sec]
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001616916554063227		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: 0.0001616916554063227 | validation: 0.00012990566975195651]
	TIME [epoch: 18.5 sec]
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00017788821179146622		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: 0.00017788821179146622 | validation: 0.0001360385420253969]
	TIME [epoch: 18.4 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002153278233220952		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: 0.0002153278233220952 | validation: 0.0001928302833650393]
	TIME [epoch: 18.4 sec]
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00018586004315451331		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: 0.00018586004315451331 | validation: 9.312320871316437e-05]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_140.pth
	Model improved!!!
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00010157793142648041		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: 0.00010157793142648041 | validation: 8.772764974025504e-05]
	TIME [epoch: 18.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_141.pth
	Model improved!!!
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00018063677672530755		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: 0.00018063677672530755 | validation: 0.0001846743037279566]
	TIME [epoch: 18.4 sec]
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00019824377463801066		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: 0.00019824377463801066 | validation: 7.92044277395485e-05]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_143.pth
	Model improved!!!
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00010149913907807605		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: 0.00010149913907807605 | validation: 6.733441818872231e-05]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_144.pth
	Model improved!!!
EPOCH 145/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00013627485399051476		[learning rate: 0.0023352]
	Learning Rate: 0.00233525
	LOSS [training: 0.00013627485399051476 | validation: 0.00021952923842950844]
	TIME [epoch: 18.6 sec]
EPOCH 146/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00020663378469741554		[learning rate: 0.0022997]
	Learning Rate: 0.00229968
	LOSS [training: 0.00020663378469741554 | validation: 0.00010770682171621848]
	TIME [epoch: 18.6 sec]
EPOCH 147/500:
	Training over batches...
		[batch 4/4] avg loss: 9.491292612084524e-05		[learning rate: 0.0022646]
	Learning Rate: 0.00226464
	LOSS [training: 9.491292612084524e-05 | validation: 6.26980509526538e-05]
	TIME [epoch: 18.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_147.pth
	Model improved!!!
EPOCH 148/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00010434411259706977		[learning rate: 0.0022301]
	Learning Rate: 0.00223015
	LOSS [training: 0.00010434411259706977 | validation: 0.00017514272156409394]
	TIME [epoch: 18.7 sec]
EPOCH 149/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001482322001155606		[learning rate: 0.0021962]
	Learning Rate: 0.00219617
	LOSS [training: 0.0001482322001155606 | validation: 0.00018231464385639785]
	TIME [epoch: 18.5 sec]
EPOCH 150/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001719647855276496		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 0.0001719647855276496 | validation: 0.00011903546961855516]
	TIME [epoch: 18.6 sec]
EPOCH 151/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011188964869070662		[learning rate: 0.0021298]
	Learning Rate: 0.00212977
	LOSS [training: 0.00011188964869070662 | validation: 6.211984253290548e-05]
	TIME [epoch: 18.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_151.pth
	Model improved!!!
EPOCH 152/500:
	Training over batches...
		[batch 4/4] avg loss: 9.019861301320165e-05		[learning rate: 0.0020973]
	Learning Rate: 0.00209733
	LOSS [training: 9.019861301320165e-05 | validation: 7.27494686071184e-05]
	TIME [epoch: 18.5 sec]
EPOCH 153/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001395170718079043		[learning rate: 0.0020654]
	Learning Rate: 0.00206538
	LOSS [training: 0.0001395170718079043 | validation: 0.00017326528967217048]
	TIME [epoch: 18.5 sec]
EPOCH 154/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011560708838668354		[learning rate: 0.0020339]
	Learning Rate: 0.00203392
	LOSS [training: 0.00011560708838668354 | validation: 4.9560081318502645e-05]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_154.pth
	Model improved!!!
EPOCH 155/500:
	Training over batches...
		[batch 4/4] avg loss: 7.769079244297305e-05		[learning rate: 0.0020029]
	Learning Rate: 0.00200293
	LOSS [training: 7.769079244297305e-05 | validation: 2.8257532065413928e-05]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_155.pth
	Model improved!!!
EPOCH 156/500:
	Training over batches...
		[batch 4/4] avg loss: 9.252089779381779e-05		[learning rate: 0.0019724]
	Learning Rate: 0.00197242
	LOSS [training: 9.252089779381779e-05 | validation: 6.632218031344329e-05]
	TIME [epoch: 18.6 sec]
EPOCH 157/500:
	Training over batches...
		[batch 4/4] avg loss: 9.563420210134467e-05		[learning rate: 0.0019424]
	Learning Rate: 0.00194238
	LOSS [training: 9.563420210134467e-05 | validation: 9.859007300139021e-05]
	TIME [epoch: 18.6 sec]
EPOCH 158/500:
	Training over batches...
		[batch 4/4] avg loss: 9.744270765015528e-05		[learning rate: 0.0019128]
	Learning Rate: 0.00191279
	LOSS [training: 9.744270765015528e-05 | validation: 6.794391412668222e-05]
	TIME [epoch: 18.6 sec]
EPOCH 159/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00014044764932439425		[learning rate: 0.0018836]
	Learning Rate: 0.00188365
	LOSS [training: 0.00014044764932439425 | validation: 9.018263806298999e-05]
	TIME [epoch: 18.6 sec]
EPOCH 160/500:
	Training over batches...
		[batch 4/4] avg loss: 4.615808256850939e-05		[learning rate: 0.001855]
	Learning Rate: 0.00185495
	LOSS [training: 4.615808256850939e-05 | validation: 1.5311214492595983e-05]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_160.pth
	Model improved!!!
EPOCH 161/500:
	Training over batches...
		[batch 4/4] avg loss: 6.349790125949185e-05		[learning rate: 0.0018267]
	Learning Rate: 0.0018267
	LOSS [training: 6.349790125949185e-05 | validation: 0.00010204109190064736]
	TIME [epoch: 18.5 sec]
EPOCH 162/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00012098093562869295		[learning rate: 0.0017989]
	Learning Rate: 0.00179887
	LOSS [training: 0.00012098093562869295 | validation: 9.912507381477864e-05]
	TIME [epoch: 18.6 sec]
EPOCH 163/500:
	Training over batches...
		[batch 4/4] avg loss: 7.987890037288237e-05		[learning rate: 0.0017715]
	Learning Rate: 0.00177147
	LOSS [training: 7.987890037288237e-05 | validation: 2.3442818258781807e-05]
	TIME [epoch: 18.7 sec]
EPOCH 164/500:
	Training over batches...
		[batch 4/4] avg loss: 2.4893379180833234e-05		[learning rate: 0.0017445]
	Learning Rate: 0.00174448
	LOSS [training: 2.4893379180833234e-05 | validation: 1.6013660079786664e-05]
	TIME [epoch: 18.5 sec]
EPOCH 165/500:
	Training over batches...
		[batch 4/4] avg loss: 2.2400852444033116e-05		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: 2.2400852444033116e-05 | validation: 5.1629946348551894e-05]
	TIME [epoch: 18.6 sec]
EPOCH 166/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001530598994820991		[learning rate: 0.0016917]
	Learning Rate: 0.00169174
	LOSS [training: 0.0001530598994820991 | validation: 0.00010626477214387698]
	TIME [epoch: 18.6 sec]
EPOCH 167/500:
	Training over batches...
		[batch 4/4] avg loss: 6.925390972866287e-05		[learning rate: 0.001666]
	Learning Rate: 0.00166597
	LOSS [training: 6.925390972866287e-05 | validation: 3.109778704610355e-05]
	TIME [epoch: 18.6 sec]
EPOCH 168/500:
	Training over batches...
		[batch 4/4] avg loss: 2.679885930054893e-05		[learning rate: 0.0016406]
	Learning Rate: 0.00164059
	LOSS [training: 2.679885930054893e-05 | validation: 3.879306680714878e-05]
	TIME [epoch: 18.6 sec]
EPOCH 169/500:
	Training over batches...
		[batch 4/4] avg loss: 7.288031072551793e-05		[learning rate: 0.0016156]
	Learning Rate: 0.0016156
	LOSS [training: 7.288031072551793e-05 | validation: 6.30899186083369e-05]
	TIME [epoch: 18.6 sec]
EPOCH 170/500:
	Training over batches...
		[batch 4/4] avg loss: 7.065854869752696e-05		[learning rate: 0.001591]
	Learning Rate: 0.00159099
	LOSS [training: 7.065854869752696e-05 | validation: 4.446932873229437e-05]
	TIME [epoch: 18.6 sec]
EPOCH 171/500:
	Training over batches...
		[batch 4/4] avg loss: 5.4628816487019e-05		[learning rate: 0.0015668]
	Learning Rate: 0.00156675
	LOSS [training: 5.4628816487019e-05 | validation: 2.573068076815854e-05]
	TIME [epoch: 18.6 sec]
EPOCH 172/500:
	Training over batches...
		[batch 4/4] avg loss: 6.967416478763644e-05		[learning rate: 0.0015429]
	Learning Rate: 0.00154288
	LOSS [training: 6.967416478763644e-05 | validation: 6.360820972309701e-05]
	TIME [epoch: 18.7 sec]
EPOCH 173/500:
	Training over batches...
		[batch 4/4] avg loss: 7.309219509249732e-05		[learning rate: 0.0015194]
	Learning Rate: 0.00151938
	LOSS [training: 7.309219509249732e-05 | validation: 3.943080107282415e-05]
	TIME [epoch: 18.6 sec]
EPOCH 174/500:
	Training over batches...
		[batch 4/4] avg loss: 4.1846095002883394e-05		[learning rate: 0.0014962]
	Learning Rate: 0.00149624
	LOSS [training: 4.1846095002883394e-05 | validation: 1.7637443070886994e-05]
	TIME [epoch: 18.6 sec]
EPOCH 175/500:
	Training over batches...
		[batch 4/4] avg loss: 4.781240756852967e-05		[learning rate: 0.0014734]
	Learning Rate: 0.00147344
	LOSS [training: 4.781240756852967e-05 | validation: 3.8057278526646196e-05]
	TIME [epoch: 18.5 sec]
EPOCH 176/500:
	Training over batches...
		[batch 4/4] avg loss: 6.319539305892974e-05		[learning rate: 0.001451]
	Learning Rate: 0.001451
	LOSS [training: 6.319539305892974e-05 | validation: 4.477107610773357e-05]
	TIME [epoch: 18.6 sec]
EPOCH 177/500:
	Training over batches...
		[batch 4/4] avg loss: 4.110080665289884e-05		[learning rate: 0.0014289]
	Learning Rate: 0.00142889
	LOSS [training: 4.110080665289884e-05 | validation: 1.9786804202287957e-05]
	TIME [epoch: 18.6 sec]
EPOCH 178/500:
	Training over batches...
		[batch 4/4] avg loss: 3.734755423911396e-05		[learning rate: 0.0014071]
	Learning Rate: 0.00140713
	LOSS [training: 3.734755423911396e-05 | validation: 6.696223153001269e-05]
	TIME [epoch: 18.6 sec]
EPOCH 179/500:
	Training over batches...
		[batch 4/4] avg loss: 7.979788723966365e-05		[learning rate: 0.0013857]
	Learning Rate: 0.00138569
	LOSS [training: 7.979788723966365e-05 | validation: 5.734500492810879e-05]
	TIME [epoch: 18.5 sec]
EPOCH 180/500:
	Training over batches...
		[batch 4/4] avg loss: 4.3052270830162164e-05		[learning rate: 0.0013646]
	Learning Rate: 0.00136458
	LOSS [training: 4.3052270830162164e-05 | validation: 2.2404970424967808e-05]
	TIME [epoch: 18.6 sec]
EPOCH 181/500:
	Training over batches...
		[batch 4/4] avg loss: 3.937659438254959e-05		[learning rate: 0.0013438]
	Learning Rate: 0.0013438
	LOSS [training: 3.937659438254959e-05 | validation: 1.4118323110561714e-05]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_181.pth
	Model improved!!!
EPOCH 182/500:
	Training over batches...
		[batch 4/4] avg loss: 4.886089948964523e-05		[learning rate: 0.0013233]
	Learning Rate: 0.00132333
	LOSS [training: 4.886089948964523e-05 | validation: 1.060105607600148e-05]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_182.pth
	Model improved!!!
EPOCH 183/500:
	Training over batches...
		[batch 4/4] avg loss: 4.1958518569436e-05		[learning rate: 0.0013032]
	Learning Rate: 0.00130317
	LOSS [training: 4.1958518569436e-05 | validation: 2.5991341431016004e-05]
	TIME [epoch: 18.5 sec]
EPOCH 184/500:
	Training over batches...
		[batch 4/4] avg loss: 5.774827702212226e-05		[learning rate: 0.0012833]
	Learning Rate: 0.00128332
	LOSS [training: 5.774827702212226e-05 | validation: 5.777310922626656e-05]
	TIME [epoch: 18.4 sec]
EPOCH 185/500:
	Training over batches...
		[batch 4/4] avg loss: 4.499654308719181e-05		[learning rate: 0.0012638]
	Learning Rate: 0.00126377
	LOSS [training: 4.499654308719181e-05 | validation: 1.6368791453937215e-05]
	TIME [epoch: 18.4 sec]
EPOCH 186/500:
	Training over batches...
		[batch 4/4] avg loss: 2.62290665938697e-05		[learning rate: 0.0012445]
	Learning Rate: 0.00124451
	LOSS [training: 2.62290665938697e-05 | validation: 2.967859219322167e-05]
	TIME [epoch: 18.5 sec]
EPOCH 187/500:
	Training over batches...
		[batch 4/4] avg loss: 1.302619971062924e-05		[learning rate: 0.0012256]
	Learning Rate: 0.00122556
	LOSS [training: 1.302619971062924e-05 | validation: 1.0737181043915634e-05]
	TIME [epoch: 18.4 sec]
EPOCH 188/500:
	Training over batches...
		[batch 4/4] avg loss: 7.415152557286498e-05		[learning rate: 0.0012069]
	Learning Rate: 0.00120689
	LOSS [training: 7.415152557286498e-05 | validation: 2.5226060018805896e-05]
	TIME [epoch: 18.3 sec]
EPOCH 189/500:
	Training over batches...
		[batch 4/4] avg loss: 2.6803841375153238e-05		[learning rate: 0.0011885]
	Learning Rate: 0.0011885
	LOSS [training: 2.6803841375153238e-05 | validation: 2.2662090633818854e-05]
	TIME [epoch: 18.5 sec]
EPOCH 190/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9870571666855128e-05		[learning rate: 0.0011704]
	Learning Rate: 0.0011704
	LOSS [training: 1.9870571666855128e-05 | validation: 1.2019594296751369e-05]
	TIME [epoch: 18.4 sec]
EPOCH 191/500:
	Training over batches...
		[batch 4/4] avg loss: 5.233485802693427e-05		[learning rate: 0.0011526]
	Learning Rate: 0.00115257
	LOSS [training: 5.233485802693427e-05 | validation: 4.385525578691718e-05]
	TIME [epoch: 18.6 sec]
EPOCH 192/500:
	Training over batches...
		[batch 4/4] avg loss: 4.302593867989024e-05		[learning rate: 0.001135]
	Learning Rate: 0.00113501
	LOSS [training: 4.302593867989024e-05 | validation: 2.118031506289042e-05]
	TIME [epoch: 18.5 sec]
EPOCH 193/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2866662244388483e-05		[learning rate: 0.0011177]
	Learning Rate: 0.00111772
	LOSS [training: 1.2866662244388483e-05 | validation: 7.607701381656895e-06]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_193.pth
	Model improved!!!
EPOCH 194/500:
	Training over batches...
		[batch 4/4] avg loss: 3.812008440758141e-05		[learning rate: 0.0011007]
	Learning Rate: 0.00110069
	LOSS [training: 3.812008440758141e-05 | validation: 5.375390330247143e-05]
	TIME [epoch: 18.3 sec]
EPOCH 195/500:
	Training over batches...
		[batch 4/4] avg loss: 2.7517659799792704e-05		[learning rate: 0.0010839]
	Learning Rate: 0.00108393
	LOSS [training: 2.7517659799792704e-05 | validation: 1.4073951873659008e-05]
	TIME [epoch: 18.4 sec]
EPOCH 196/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1781145986788923e-05		[learning rate: 0.0010674]
	Learning Rate: 0.00106741
	LOSS [training: 1.1781145986788923e-05 | validation: 1.6297810551438197e-05]
	TIME [epoch: 18.5 sec]
EPOCH 197/500:
	Training over batches...
		[batch 4/4] avg loss: 3.613564180966211e-05		[learning rate: 0.0010512]
	Learning Rate: 0.00105115
	LOSS [training: 3.613564180966211e-05 | validation: 7.335130688102964e-05]
	TIME [epoch: 18.3 sec]
EPOCH 198/500:
	Training over batches...
		[batch 4/4] avg loss: 5.161971652262598e-05		[learning rate: 0.0010351]
	Learning Rate: 0.00103514
	LOSS [training: 5.161971652262598e-05 | validation: 5.3167972848287e-05]
	TIME [epoch: 18.5 sec]
EPOCH 199/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7830298705474588e-05		[learning rate: 0.0010194]
	Learning Rate: 0.00101937
	LOSS [training: 1.7830298705474588e-05 | validation: 2.2186685275645823e-05]
	TIME [epoch: 18.4 sec]
EPOCH 200/500:
	Training over batches...
		[batch 4/4] avg loss: 3.157800007176137e-05		[learning rate: 0.0010038]
	Learning Rate: 0.00100384
	LOSS [training: 3.157800007176137e-05 | validation: 3.836354617563953e-05]
	TIME [epoch: 18.4 sec]
EPOCH 201/500:
	Training over batches...
		[batch 4/4] avg loss: 3.435035596135805e-05		[learning rate: 0.00098855]
	Learning Rate: 0.000988553
	LOSS [training: 3.435035596135805e-05 | validation: 4.341397680261983e-06]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_201.pth
	Model improved!!!
EPOCH 202/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4849606983821207e-05		[learning rate: 0.00097349]
	Learning Rate: 0.000973494
	LOSS [training: 1.4849606983821207e-05 | validation: 7.780380098459272e-06]
	TIME [epoch: 18.6 sec]
EPOCH 203/500:
	Training over batches...
		[batch 4/4] avg loss: 1.686743895103493e-05		[learning rate: 0.00095866]
	Learning Rate: 0.000958664
	LOSS [training: 1.686743895103493e-05 | validation: 2.3508339153258008e-05]
	TIME [epoch: 18.5 sec]
EPOCH 204/500:
	Training over batches...
		[batch 4/4] avg loss: 8.490425559607217e-06		[learning rate: 0.00094406]
	Learning Rate: 0.000944061
	LOSS [training: 8.490425559607217e-06 | validation: 1.7744006424587734e-05]
	TIME [epoch: 18.6 sec]
EPOCH 205/500:
	Training over batches...
		[batch 4/4] avg loss: 1.8042016024733655e-05		[learning rate: 0.00092968]
	Learning Rate: 0.00092968
	LOSS [training: 1.8042016024733655e-05 | validation: 6.780832675541038e-05]
	TIME [epoch: 18.6 sec]
EPOCH 206/500:
	Training over batches...
		[batch 4/4] avg loss: 4.042303736509978e-05		[learning rate: 0.00091552]
	Learning Rate: 0.000915518
	LOSS [training: 4.042303736509978e-05 | validation: 2.8473838423446197e-06]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_206.pth
	Model improved!!!
EPOCH 207/500:
	Training over batches...
		[batch 4/4] avg loss: 8.887656567127311e-06		[learning rate: 0.00090157]
	Learning Rate: 0.000901571
	LOSS [training: 8.887656567127311e-06 | validation: 2.8105200577903224e-05]
	TIME [epoch: 18.6 sec]
EPOCH 208/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5532051036470487e-05		[learning rate: 0.00088784]
	Learning Rate: 0.000887837
	LOSS [training: 1.5532051036470487e-05 | validation: 1.6948012286128743e-05]
	TIME [epoch: 18.6 sec]
EPOCH 209/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4014209604888528e-05		[learning rate: 0.00087431]
	Learning Rate: 0.000874312
	LOSS [training: 1.4014209604888528e-05 | validation: 5.449759142496192e-05]
	TIME [epoch: 18.5 sec]
EPOCH 210/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7723899211688506e-05		[learning rate: 0.00086099]
	Learning Rate: 0.000860994
	LOSS [training: 3.7723899211688506e-05 | validation: 2.035447557377701e-05]
	TIME [epoch: 18.6 sec]
EPOCH 211/500:
	Training over batches...
		[batch 4/4] avg loss: 7.689460971082473e-06		[learning rate: 0.00084788]
	Learning Rate: 0.000847878
	LOSS [training: 7.689460971082473e-06 | validation: 8.463021758825739e-06]
	TIME [epoch: 18.6 sec]
EPOCH 212/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5393298839798674e-05		[learning rate: 0.00083496]
	Learning Rate: 0.000834962
	LOSS [training: 1.5393298839798674e-05 | validation: 3.15275196514182e-05]
	TIME [epoch: 18.6 sec]
EPOCH 213/500:
	Training over batches...
		[batch 4/4] avg loss: 1.866439707197709e-05		[learning rate: 0.00082224]
	Learning Rate: 0.000822243
	LOSS [training: 1.866439707197709e-05 | validation: 2.343677398727162e-05]
	TIME [epoch: 18.7 sec]
EPOCH 214/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3966989888143376e-05		[learning rate: 0.00080972]
	Learning Rate: 0.000809717
	LOSS [training: 1.3966989888143376e-05 | validation: -9.844601089047611e-06]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_214.pth
	Model improved!!!
EPOCH 215/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4654389428456315e-05		[learning rate: 0.00079738]
	Learning Rate: 0.000797382
	LOSS [training: 1.4654389428456315e-05 | validation: 3.415016193481391e-05]
	TIME [epoch: 18.6 sec]
EPOCH 216/500:
	Training over batches...
		[batch 4/4] avg loss: 1.909858808310483e-05		[learning rate: 0.00078524]
	Learning Rate: 0.000785236
	LOSS [training: 1.909858808310483e-05 | validation: -1.9758540513483028e-05]
	TIME [epoch: 18.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_216.pth
	Model improved!!!
EPOCH 217/500:
	Training over batches...
		[batch 4/4] avg loss: 6.288521490175336e-06		[learning rate: 0.00077327]
	Learning Rate: 0.000773274
	LOSS [training: 6.288521490175336e-06 | validation: -1.1064119760619828e-05]
	TIME [epoch: 18.6 sec]
EPOCH 218/500:
	Training over batches...
		[batch 4/4] avg loss: 3.0289900080464616e-05		[learning rate: 0.00076149]
	Learning Rate: 0.000761494
	LOSS [training: 3.0289900080464616e-05 | validation: 1.203874432147245e-05]
	TIME [epoch: 18.5 sec]
EPOCH 219/500:
	Training over batches...
		[batch 4/4] avg loss: 1.429446373991816e-05		[learning rate: 0.00074989]
	Learning Rate: 0.000749894
	LOSS [training: 1.429446373991816e-05 | validation: -1.0543935049449616e-05]
	TIME [epoch: 18.6 sec]
EPOCH 220/500:
	Training over batches...
		[batch 4/4] avg loss: 2.3012343352268874e-07		[learning rate: 0.00073847]
	Learning Rate: 0.000738471
	LOSS [training: 2.3012343352268874e-07 | validation: 1.3594303887998426e-05]
	TIME [epoch: 18.5 sec]
EPOCH 221/500:
	Training over batches...
		[batch 4/4] avg loss: 2.861317649487938e-05		[learning rate: 0.00072722]
	Learning Rate: 0.000727221
	LOSS [training: 2.861317649487938e-05 | validation: 8.208008211583184e-06]
	TIME [epoch: 18.6 sec]
EPOCH 222/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0901191956155465e-05		[learning rate: 0.00071614]
	Learning Rate: 0.000716143
	LOSS [training: 1.0901191956155465e-05 | validation: 1.137505114366033e-05]
	TIME [epoch: 18.5 sec]
EPOCH 223/500:
	Training over batches...
		[batch 4/4] avg loss: 1.8370685693857914e-05		[learning rate: 0.00070523]
	Learning Rate: 0.000705234
	LOSS [training: 1.8370685693857914e-05 | validation: 2.1289736289636708e-05]
	TIME [epoch: 18.5 sec]
EPOCH 224/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0633180422467859e-05		[learning rate: 0.00069449]
	Learning Rate: 0.000694491
	LOSS [training: 1.0633180422467859e-05 | validation: 8.592045722279893e-07]
	TIME [epoch: 18.5 sec]
EPOCH 225/500:
	Training over batches...
		[batch 4/4] avg loss: 5.538122737690498e-06		[learning rate: 0.00068391]
	Learning Rate: 0.000683912
	LOSS [training: 5.538122737690498e-06 | validation: 3.0081679768029225e-05]
	TIME [epoch: 18.5 sec]
EPOCH 226/500:
	Training over batches...
		[batch 4/4] avg loss: 2.047699923407365e-05		[learning rate: 0.00067349]
	Learning Rate: 0.000673493
	LOSS [training: 2.047699923407365e-05 | validation: 1.8997862672823775e-05]
	TIME [epoch: 18.5 sec]
EPOCH 227/500:
	Training over batches...
		[batch 4/4] avg loss: 1.392409461172417e-05		[learning rate: 0.00066323]
	Learning Rate: 0.000663234
	LOSS [training: 1.392409461172417e-05 | validation: 1.7729463774751376e-05]
	TIME [epoch: 18.5 sec]
EPOCH 228/500:
	Training over batches...
		[batch 4/4] avg loss: 7.964132099242405e-06		[learning rate: 0.00065313]
	Learning Rate: 0.00065313
	LOSS [training: 7.964132099242405e-06 | validation: 8.961980902007839e-06]
	TIME [epoch: 18.6 sec]
EPOCH 229/500:
	Training over batches...
		[batch 4/4] avg loss: 3.851698960731676e-06		[learning rate: 0.00064318]
	Learning Rate: 0.000643181
	LOSS [training: 3.851698960731676e-06 | validation: 1.4445280673221818e-05]
	TIME [epoch: 18.6 sec]
EPOCH 230/500:
	Training over batches...
		[batch 4/4] avg loss: 2.687487422969881e-05		[learning rate: 0.00063338]
	Learning Rate: 0.000633383
	LOSS [training: 2.687487422969881e-05 | validation: 1.0847235574276142e-06]
	TIME [epoch: 18.6 sec]
EPOCH 231/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0186679074511873e-05		[learning rate: 0.00062373]
	Learning Rate: 0.000623735
	LOSS [training: 1.0186679074511873e-05 | validation: 9.661132904453584e-06]
	TIME [epoch: 18.5 sec]
EPOCH 232/500:
	Training over batches...
		[batch 4/4] avg loss: 4.627684372697783e-06		[learning rate: 0.00061423]
	Learning Rate: 0.000614233
	LOSS [training: 4.627684372697783e-06 | validation: -9.152582473241911e-07]
	TIME [epoch: 18.6 sec]
EPOCH 233/500:
	Training over batches...
		[batch 4/4] avg loss: 1.562173899144148e-06		[learning rate: 0.00060488]
	Learning Rate: 0.000604876
	LOSS [training: 1.562173899144148e-06 | validation: -7.472337353028769e-06]
	TIME [epoch: 18.5 sec]
EPOCH 234/500:
	Training over batches...
		[batch 4/4] avg loss: 1.894649425906547e-05		[learning rate: 0.00059566]
	Learning Rate: 0.000595662
	LOSS [training: 1.894649425906547e-05 | validation: 3.18675450888335e-05]
	TIME [epoch: 18.6 sec]
EPOCH 235/500:
	Training over batches...
		[batch 4/4] avg loss: 8.141312131739187e-06		[learning rate: 0.00058659]
	Learning Rate: 0.000586588
	LOSS [training: 8.141312131739187e-06 | validation: 8.673121700642827e-06]
	TIME [epoch: 18.6 sec]
EPOCH 236/500:
	Training over batches...
		[batch 4/4] avg loss: 2.2096657408328117e-06		[learning rate: 0.00057765]
	Learning Rate: 0.000577652
	LOSS [training: 2.2096657408328117e-06 | validation: 4.153048079718502e-06]
	TIME [epoch: 18.6 sec]
EPOCH 237/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4427672938060577e-05		[learning rate: 0.00056885]
	Learning Rate: 0.000568853
	LOSS [training: 1.4427672938060577e-05 | validation: 1.7919657158798508e-05]
	TIME [epoch: 18.5 sec]
EPOCH 238/500:
	Training over batches...
		[batch 4/4] avg loss: 9.794258801340328e-06		[learning rate: 0.00056019]
	Learning Rate: 0.000560187
	LOSS [training: 9.794258801340328e-06 | validation: 5.832644746309735e-06]
	TIME [epoch: 18.6 sec]
EPOCH 239/500:
	Training over batches...
		[batch 4/4] avg loss: 2.8882692423041204e-06		[learning rate: 0.00055165]
	Learning Rate: 0.000551654
	LOSS [training: 2.8882692423041204e-06 | validation: 4.23314355608051e-07]
	TIME [epoch: 18.6 sec]
EPOCH 240/500:
	Training over batches...
		[batch 4/4] avg loss: 4.620377264432762e-06		[learning rate: 0.00054325]
	Learning Rate: 0.00054325
	LOSS [training: 4.620377264432762e-06 | validation: 3.270577242019846e-06]
	TIME [epoch: 18.5 sec]
EPOCH 241/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4682082011361673e-05		[learning rate: 0.00053497]
	Learning Rate: 0.000534975
	LOSS [training: 1.4682082011361673e-05 | validation: 5.5164283019675245e-06]
	TIME [epoch: 18.6 sec]
EPOCH 242/500:
	Training over batches...
		[batch 4/4] avg loss: 5.24353413450418e-06		[learning rate: 0.00052683]
	Learning Rate: 0.000526825
	LOSS [training: 5.24353413450418e-06 | validation: 1.2160530643390955e-05]
	TIME [epoch: 18.7 sec]
EPOCH 243/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9328217252974063e-06		[learning rate: 0.0005188]
	Learning Rate: 0.0005188
	LOSS [training: 1.9328217252974063e-06 | validation: 8.192067572052598e-07]
	TIME [epoch: 18.6 sec]
EPOCH 244/500:
	Training over batches...
		[batch 4/4] avg loss: 8.02243637831923e-06		[learning rate: 0.0005109]
	Learning Rate: 0.000510897
	LOSS [training: 8.02243637831923e-06 | validation: 1.7434785053666026e-05]
	TIME [epoch: 18.5 sec]
EPOCH 245/500:
	Training over batches...
		[batch 4/4] avg loss: -4.985623190216782e-07		[learning rate: 0.00050311]
	Learning Rate: 0.000503114
	LOSS [training: -4.985623190216782e-07 | validation: -1.535188933401832e-05]
	TIME [epoch: 18.6 sec]
EPOCH 246/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2399415974403195e-05		[learning rate: 0.00049545]
	Learning Rate: 0.00049545
	LOSS [training: 1.2399415974403195e-05 | validation: 7.2621457315975575e-06]
	TIME [epoch: 18.5 sec]
EPOCH 247/500:
	Training over batches...
		[batch 4/4] avg loss: 7.557829441343201e-06		[learning rate: 0.0004879]
	Learning Rate: 0.000487903
	LOSS [training: 7.557829441343201e-06 | validation: -6.403038048495624e-06]
	TIME [epoch: 18.5 sec]
EPOCH 248/500:
	Training over batches...
		[batch 4/4] avg loss: -1.2130218359084346e-06		[learning rate: 0.00048047]
	Learning Rate: 0.00048047
	LOSS [training: -1.2130218359084346e-06 | validation: 1.030759909733381e-05]
	TIME [epoch: 18.5 sec]
EPOCH 249/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3342417851024857e-05		[learning rate: 0.00047315]
	Learning Rate: 0.000473151
	LOSS [training: 1.3342417851024857e-05 | validation: -1.6204618379552116e-05]
	TIME [epoch: 18.5 sec]
EPOCH 250/500:
	Training over batches...
		[batch 4/4] avg loss: 2.407194556429793e-06		[learning rate: 0.00046594]
	Learning Rate: 0.000465944
	LOSS [training: 2.407194556429793e-06 | validation: -6.792441435121033e-06]
	TIME [epoch: 18.3 sec]
EPOCH 251/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2947211473959275e-05		[learning rate: 0.00045885]
	Learning Rate: 0.000458846
	LOSS [training: 1.2947211473959275e-05 | validation: 1.1736119908232868e-05]
	TIME [epoch: 141 sec]
EPOCH 252/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6051742115306354e-05		[learning rate: 0.00045186]
	Learning Rate: 0.000451856
	LOSS [training: 1.6051742115306354e-05 | validation: -1.2425515647481068e-05]
	TIME [epoch: 40.6 sec]
EPOCH 253/500:
	Training over batches...
		[batch 4/4] avg loss: 5.913266714067667e-06		[learning rate: 0.00044497]
	Learning Rate: 0.000444973
	LOSS [training: 5.913266714067667e-06 | validation: -1.0734840760229235e-05]
	TIME [epoch: 41.1 sec]
EPOCH 254/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2964132647049769e-06		[learning rate: 0.00043819]
	Learning Rate: 0.000438194
	LOSS [training: 1.2964132647049769e-06 | validation: 1.4019252713979967e-05]
	TIME [epoch: 41.3 sec]
EPOCH 255/500:
	Training over batches...
		[batch 4/4] avg loss: -1.894819645702489e-07		[learning rate: 0.00043152]
	Learning Rate: 0.000431519
	LOSS [training: -1.894819645702489e-07 | validation: 5.241018788313404e-07]
	TIME [epoch: 41.2 sec]
EPOCH 256/500:
	Training over batches...
		[batch 4/4] avg loss: 7.485897262730412e-06		[learning rate: 0.00042495]
	Learning Rate: 0.000424946
	LOSS [training: 7.485897262730412e-06 | validation: -2.076022075786721e-06]
	TIME [epoch: 41.1 sec]
EPOCH 257/500:
	Training over batches...
		[batch 4/4] avg loss: 7.218046732520022e-07		[learning rate: 0.00041847]
	Learning Rate: 0.000418472
	LOSS [training: 7.218046732520022e-07 | validation: -8.06121407035998e-06]
	TIME [epoch: 41 sec]
EPOCH 258/500:
	Training over batches...
		[batch 4/4] avg loss: -1.1336815045943637e-06		[learning rate: 0.0004121]
	Learning Rate: 0.000412098
	LOSS [training: -1.1336815045943637e-06 | validation: 6.282887862173237e-06]
	TIME [epoch: 41.1 sec]
EPOCH 259/500:
	Training over batches...
		[batch 4/4] avg loss: 4.130855404507528e-06		[learning rate: 0.00040582]
	Learning Rate: 0.00040582
	LOSS [training: 4.130855404507528e-06 | validation: -2.6462402428171215e-06]
	TIME [epoch: 41.1 sec]
EPOCH 260/500:
	Training over batches...
		[batch 4/4] avg loss: -2.3809557398450918e-06		[learning rate: 0.00039964]
	Learning Rate: 0.000399638
	LOSS [training: -2.3809557398450918e-06 | validation: 2.1966434830103408e-05]
	TIME [epoch: 41.3 sec]
EPOCH 261/500:
	Training over batches...
		[batch 4/4] avg loss: 8.269898882324988e-06		[learning rate: 0.00039355]
	Learning Rate: 0.00039355
	LOSS [training: 8.269898882324988e-06 | validation: 7.504818375203336e-06]
	TIME [epoch: 40.9 sec]
EPOCH 262/500:
	Training over batches...
		[batch 4/4] avg loss: -8.516434011665021e-07		[learning rate: 0.00038755]
	Learning Rate: 0.000387555
	LOSS [training: -8.516434011665021e-07 | validation: 4.888376154267738e-06]
	TIME [epoch: 40.8 sec]
EPOCH 263/500:
	Training over batches...
		[batch 4/4] avg loss: 4.5636717639065965e-06		[learning rate: 0.00038165]
	Learning Rate: 0.000381651
	LOSS [training: 4.5636717639065965e-06 | validation: -1.4278508744911542e-06]
	TIME [epoch: 40.9 sec]
EPOCH 264/500:
	Training over batches...
		[batch 4/4] avg loss: 7.153178915235814e-06		[learning rate: 0.00037584]
	Learning Rate: 0.000375837
	LOSS [training: 7.153178915235814e-06 | validation: 2.0240385363671898e-05]
	TIME [epoch: 40.8 sec]
EPOCH 265/500:
	Training over batches...
		[batch 4/4] avg loss: 9.57902980624714e-06		[learning rate: 0.00037011]
	Learning Rate: 0.000370112
	LOSS [training: 9.57902980624714e-06 | validation: -1.3055844933493878e-06]
	TIME [epoch: 40.9 sec]
EPOCH 266/500:
	Training over batches...
		[batch 4/4] avg loss: 9.891827271374388e-07		[learning rate: 0.00036447]
	Learning Rate: 0.000364474
	LOSS [training: 9.891827271374388e-07 | validation: 4.506279984417105e-06]
	TIME [epoch: 40.9 sec]
EPOCH 267/500:
	Training over batches...
		[batch 4/4] avg loss: 5.312536300589365e-06		[learning rate: 0.00035892]
	Learning Rate: 0.000358922
	LOSS [training: 5.312536300589365e-06 | validation: 1.7684216648241158e-05]
	TIME [epoch: 41.1 sec]
EPOCH 268/500:
	Training over batches...
		[batch 4/4] avg loss: 3.3736381764083446e-06		[learning rate: 0.00035345]
	Learning Rate: 0.000353454
	LOSS [training: 3.3736381764083446e-06 | validation: 5.151487256894072e-06]
	TIME [epoch: 41.2 sec]
EPOCH 269/500:
	Training over batches...
		[batch 4/4] avg loss: 2.1442856725036878e-06		[learning rate: 0.00034807]
	Learning Rate: 0.00034807
	LOSS [training: 2.1442856725036878e-06 | validation: -1.0691304477443709e-05]
	TIME [epoch: 41.2 sec]
EPOCH 270/500:
	Training over batches...
		[batch 4/4] avg loss: 1.053583522950008e-05		[learning rate: 0.00034277]
	Learning Rate: 0.000342768
	LOSS [training: 1.053583522950008e-05 | validation: 7.303472010051326e-06]
	TIME [epoch: 41 sec]
EPOCH 271/500:
	Training over batches...
		[batch 4/4] avg loss: 8.552513197034762e-06		[learning rate: 0.00033755]
	Learning Rate: 0.000337546
	LOSS [training: 8.552513197034762e-06 | validation: 4.2431092546890647e-07]
	TIME [epoch: 41.1 sec]
EPOCH 272/500:
	Training over batches...
		[batch 4/4] avg loss: 5.933903894897452e-06		[learning rate: 0.0003324]
	Learning Rate: 0.000332404
	LOSS [training: 5.933903894897452e-06 | validation: -3.663931376812313e-06]
	TIME [epoch: 41.2 sec]
EPOCH 273/500:
	Training over batches...
		[batch 4/4] avg loss: 3.474072356892898e-06		[learning rate: 0.00032734]
	Learning Rate: 0.000327341
	LOSS [training: 3.474072356892898e-06 | validation: -5.36121748561369e-06]
	TIME [epoch: 41.1 sec]
EPOCH 274/500:
	Training over batches...
		[batch 4/4] avg loss: 4.733333734369128e-06		[learning rate: 0.00032235]
	Learning Rate: 0.000322354
	LOSS [training: 4.733333734369128e-06 | validation: 6.398214002298896e-07]
	TIME [epoch: 41.1 sec]
EPOCH 275/500:
	Training over batches...
		[batch 4/4] avg loss: -3.5278092494295295e-07		[learning rate: 0.00031744]
	Learning Rate: 0.000317444
	LOSS [training: -3.5278092494295295e-07 | validation: -1.350085309538507e-06]
	TIME [epoch: 41.1 sec]
EPOCH 276/500:
	Training over batches...
		[batch 4/4] avg loss: -1.121104006152951e-06		[learning rate: 0.00031261]
	Learning Rate: 0.000312608
	LOSS [training: -1.121104006152951e-06 | validation: -1.2053118856760081e-05]
	TIME [epoch: 41.2 sec]
EPOCH 277/500:
	Training over batches...
		[batch 4/4] avg loss: 5.6622916388955336e-06		[learning rate: 0.00030785]
	Learning Rate: 0.000307846
	LOSS [training: 5.6622916388955336e-06 | validation: 2.1564065021396184e-05]
	TIME [epoch: 41.1 sec]
EPOCH 278/500:
	Training over batches...
		[batch 4/4] avg loss: -7.68787756303202e-06		[learning rate: 0.00030316]
	Learning Rate: 0.000303156
	LOSS [training: -7.68787756303202e-06 | validation: 7.844254705777765e-07]
	TIME [epoch: 41.1 sec]
EPOCH 279/500:
	Training over batches...
		[batch 4/4] avg loss: -3.446887295925128e-06		[learning rate: 0.00029854]
	Learning Rate: 0.000298538
	LOSS [training: -3.446887295925128e-06 | validation: -1.999678576197539e-06]
	TIME [epoch: 41.1 sec]
EPOCH 280/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4736779560268199e-06		[learning rate: 0.00029399]
	Learning Rate: 0.000293991
	LOSS [training: 1.4736779560268199e-06 | validation: 3.3018293408180187e-06]
	TIME [epoch: 41.3 sec]
EPOCH 281/500:
	Training over batches...
		[batch 4/4] avg loss: -3.246442854702769e-06		[learning rate: 0.00028951]
	Learning Rate: 0.000289512
	LOSS [training: -3.246442854702769e-06 | validation: -4.725392822992891e-06]
	TIME [epoch: 41.2 sec]
EPOCH 282/500:
	Training over batches...
		[batch 4/4] avg loss: 1.9487414829311158e-06		[learning rate: 0.0002851]
	Learning Rate: 0.000285102
	LOSS [training: 1.9487414829311158e-06 | validation: -4.769803097664837e-06]
	TIME [epoch: 41 sec]
EPOCH 283/500:
	Training over batches...
		[batch 4/4] avg loss: 2.610058308121443e-06		[learning rate: 0.00028076]
	Learning Rate: 0.000280759
	LOSS [training: 2.610058308121443e-06 | validation: 9.132448725573061e-06]
	TIME [epoch: 41 sec]
EPOCH 284/500:
	Training over batches...
		[batch 4/4] avg loss: 3.1967413494713435e-06		[learning rate: 0.00027648]
	Learning Rate: 0.000276482
	LOSS [training: 3.1967413494713435e-06 | validation: 2.033353332273835e-06]
	TIME [epoch: 40.8 sec]
EPOCH 285/500:
	Training over batches...
		[batch 4/4] avg loss: 9.907530341957129e-07		[learning rate: 0.00027227]
	Learning Rate: 0.00027227
	LOSS [training: 9.907530341957129e-07 | validation: -7.278573454376772e-07]
	TIME [epoch: 41.1 sec]
EPOCH 286/500:
	Training over batches...
		[batch 4/4] avg loss: 1.128111106489582e-06		[learning rate: 0.00026812]
	Learning Rate: 0.000268123
	LOSS [training: 1.128111106489582e-06 | validation: 8.451422438066513e-06]
	TIME [epoch: 40.9 sec]
EPOCH 287/500:
	Training over batches...
		[batch 4/4] avg loss: 3.2569258376401457e-06		[learning rate: 0.00026404]
	Learning Rate: 0.000264038
	LOSS [training: 3.2569258376401457e-06 | validation: 1.35209313753335e-05]
	TIME [epoch: 40.4 sec]
EPOCH 288/500:
	Training over batches...
		[batch 4/4] avg loss: -6.162732412693386e-07		[learning rate: 0.00026002]
	Learning Rate: 0.000260016
	LOSS [training: -6.162732412693386e-07 | validation: -8.160102016069759e-06]
	TIME [epoch: 40.6 sec]
EPOCH 289/500:
	Training over batches...
		[batch 4/4] avg loss: 2.577276232640369e-07		[learning rate: 0.00025606]
	Learning Rate: 0.000256055
	LOSS [training: 2.577276232640369e-07 | validation: -9.164467422306811e-07]
	TIME [epoch: 40.7 sec]
EPOCH 290/500:
	Training over batches...
		[batch 4/4] avg loss: -5.73369844811511e-06		[learning rate: 0.00025215]
	Learning Rate: 0.000252154
	LOSS [training: -5.73369844811511e-06 | validation: 7.545190591846219e-06]
	TIME [epoch: 40.4 sec]
EPOCH 291/500:
	Training over batches...
		[batch 4/4] avg loss: -9.300325061872838e-07		[learning rate: 0.00024831]
	Learning Rate: 0.000248313
	LOSS [training: -9.300325061872838e-07 | validation: 1.4899819471883901e-05]
	TIME [epoch: 40.7 sec]
EPOCH 292/500:
	Training over batches...
		[batch 4/4] avg loss: 2.43693917867216e-06		[learning rate: 0.00024453]
	Learning Rate: 0.000244531
	LOSS [training: 2.43693917867216e-06 | validation: 2.0136403928613158e-05]
	TIME [epoch: 40.5 sec]
EPOCH 293/500:
	Training over batches...
		[batch 4/4] avg loss: -6.264039059571847e-06		[learning rate: 0.00024081]
	Learning Rate: 0.000240806
	LOSS [training: -6.264039059571847e-06 | validation: 8.229111771798748e-06]
	TIME [epoch: 40.6 sec]
EPOCH 294/500:
	Training over batches...
		[batch 4/4] avg loss: 5.231912956675799e-06		[learning rate: 0.00023714]
	Learning Rate: 0.000237137
	LOSS [training: 5.231912956675799e-06 | validation: -1.369488903736782e-06]
	TIME [epoch: 40.9 sec]
EPOCH 295/500:
	Training over batches...
		[batch 4/4] avg loss: -1.365041109997689e-06		[learning rate: 0.00023352]
	Learning Rate: 0.000233525
	LOSS [training: -1.365041109997689e-06 | validation: -4.50788864005891e-06]
	TIME [epoch: 40.7 sec]
EPOCH 296/500:
	Training over batches...
		[batch 4/4] avg loss: -1.1401368228839416e-06		[learning rate: 0.00022997]
	Learning Rate: 0.000229968
	LOSS [training: -1.1401368228839416e-06 | validation: 1.914328429375889e-06]
	TIME [epoch: 40.6 sec]
EPOCH 297/500:
	Training over batches...
		[batch 4/4] avg loss: 4.381218448005208e-07		[learning rate: 0.00022646]
	Learning Rate: 0.000226464
	LOSS [training: 4.381218448005208e-07 | validation: 2.3505757676121065e-06]
	TIME [epoch: 40.6 sec]
EPOCH 298/500:
	Training over batches...
		[batch 4/4] avg loss: 4.216016601705008e-06		[learning rate: 0.00022301]
	Learning Rate: 0.000223015
	LOSS [training: 4.216016601705008e-06 | validation: 6.651592770915871e-06]
	TIME [epoch: 40.6 sec]
EPOCH 299/500:
	Training over batches...
		[batch 4/4] avg loss: -4.988823398430986e-06		[learning rate: 0.00021962]
	Learning Rate: 0.000219617
	LOSS [training: -4.988823398430986e-06 | validation: -6.666633654393461e-06]
	TIME [epoch: 40.4 sec]
EPOCH 300/500:
	Training over batches...
		[batch 4/4] avg loss: 8.048325885094966e-07		[learning rate: 0.00021627]
	Learning Rate: 0.000216272
	LOSS [training: 8.048325885094966e-07 | validation: -1.2108934452352841e-06]
	TIME [epoch: 40.5 sec]
EPOCH 301/500:
	Training over batches...
		[batch 4/4] avg loss: 5.133748941834715e-06		[learning rate: 0.00021298]
	Learning Rate: 0.000212977
	LOSS [training: 5.133748941834715e-06 | validation: 3.6909716695059382e-06]
	TIME [epoch: 40.8 sec]
EPOCH 302/500:
	Training over batches...
		[batch 4/4] avg loss: -3.1280774367125064e-06		[learning rate: 0.00020973]
	Learning Rate: 0.000209733
	LOSS [training: -3.1280774367125064e-06 | validation: 2.171420722859807e-06]
	TIME [epoch: 40.8 sec]
EPOCH 303/500:
	Training over batches...
		[batch 4/4] avg loss: -2.8061611598239057e-06		[learning rate: 0.00020654]
	Learning Rate: 0.000206538
	LOSS [training: -2.8061611598239057e-06 | validation: -3.0640576716427505e-06]
	TIME [epoch: 41 sec]
EPOCH 304/500:
	Training over batches...
		[batch 4/4] avg loss: -1.509832687810686e-06		[learning rate: 0.00020339]
	Learning Rate: 0.000203392
	LOSS [training: -1.509832687810686e-06 | validation: 1.598834053076814e-06]
	TIME [epoch: 40.6 sec]
EPOCH 305/500:
	Training over batches...
		[batch 4/4] avg loss: -3.981274710718164e-08		[learning rate: 0.00020029]
	Learning Rate: 0.000200293
	LOSS [training: -3.981274710718164e-08 | validation: 1.7447527733882582e-06]
	TIME [epoch: 40.7 sec]
EPOCH 306/500:
	Training over batches...
		[batch 4/4] avg loss: -1.6087674578120214e-06		[learning rate: 0.00019724]
	Learning Rate: 0.000197242
	LOSS [training: -1.6087674578120214e-06 | validation: 2.9586447020817362e-06]
	TIME [epoch: 40.7 sec]
EPOCH 307/500:
	Training over batches...
		[batch 4/4] avg loss: 2.1558251880507975e-06		[learning rate: 0.00019424]
	Learning Rate: 0.000194238
	LOSS [training: 2.1558251880507975e-06 | validation: -6.097523228485446e-06]
	TIME [epoch: 40.6 sec]
EPOCH 308/500:
	Training over batches...
		[batch 4/4] avg loss: -4.690126136851602e-06		[learning rate: 0.00019128]
	Learning Rate: 0.000191279
	LOSS [training: -4.690126136851602e-06 | validation: -4.699393087056691e-06]
	TIME [epoch: 40.9 sec]
EPOCH 309/500:
	Training over batches...
		[batch 4/4] avg loss: 6.354991531510312e-06		[learning rate: 0.00018836]
	Learning Rate: 0.000188365
	LOSS [training: 6.354991531510312e-06 | validation: 3.981229131050368e-06]
	TIME [epoch: 40.7 sec]
EPOCH 310/500:
	Training over batches...
		[batch 4/4] avg loss: 1.220551865231734e-06		[learning rate: 0.0001855]
	Learning Rate: 0.000185495
	LOSS [training: 1.220551865231734e-06 | validation: -7.99426220864552e-06]
	TIME [epoch: 40.9 sec]
EPOCH 311/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1163036378835867e-05		[learning rate: 0.00018267]
	Learning Rate: 0.00018267
	LOSS [training: 1.1163036378835867e-05 | validation: 9.251403259328718e-06]
	TIME [epoch: 40.6 sec]
EPOCH 312/500:
	Training over batches...
		[batch 4/4] avg loss: -8.045905036888623e-07		[learning rate: 0.00017989]
	Learning Rate: 0.000179887
	LOSS [training: -8.045905036888623e-07 | validation: 3.188438967232088e-07]
	TIME [epoch: 40.7 sec]
EPOCH 313/500:
	Training over batches...
		[batch 4/4] avg loss: 3.721508900415116e-06		[learning rate: 0.00017715]
	Learning Rate: 0.000177147
	LOSS [training: 3.721508900415116e-06 | validation: -2.8952003833224714e-06]
	TIME [epoch: 40.8 sec]
EPOCH 314/500:
	Training over batches...
		[batch 4/4] avg loss: 5.300550683388528e-06		[learning rate: 0.00017445]
	Learning Rate: 0.000174448
	LOSS [training: 5.300550683388528e-06 | validation: 3.266925210482663e-06]
	TIME [epoch: 40.6 sec]
EPOCH 315/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6616491772706299e-06		[learning rate: 0.00017179]
	Learning Rate: 0.000171791
	LOSS [training: 1.6616491772706299e-06 | validation: -1.275853676064398e-05]
	TIME [epoch: 40.8 sec]
EPOCH 316/500:
	Training over batches...
		[batch 4/4] avg loss: -8.155173198762467e-06		[learning rate: 0.00016917]
	Learning Rate: 0.000169174
	LOSS [training: -8.155173198762467e-06 | validation: -5.071667646568523e-06]
	TIME [epoch: 40.6 sec]
EPOCH 317/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4311296782161251e-06		[learning rate: 0.0001666]
	Learning Rate: 0.000166597
	LOSS [training: 1.4311296782161251e-06 | validation: 4.102864735117784e-06]
	TIME [epoch: 40.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd5_20240711_014001/states/model_algphiq_1a_v_mmd5_317.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6573.356 seconds.
