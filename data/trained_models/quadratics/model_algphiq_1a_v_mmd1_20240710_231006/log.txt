Args:
Namespace(name='model_algphiq_1a_v_mmd1', outdir='out/model_training/model_algphiq_1a_v_mmd1', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='quadratic', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[50, 100, 250], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 292210934

Training model...

Saving initial model state to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 5.046842576511193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.046842576511193 | validation: 5.091667493235357]
	TIME [epoch: 91.2 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 4.987395345289112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.987395345289112 | validation: 5.011612193984715]
	TIME [epoch: 4.34 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 4.896406095653794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.896406095653794 | validation: 4.883189489361425]
	TIME [epoch: 4.27 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 4.727459013685333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.727459013685333 | validation: 4.60579257657149]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 4.318613126094979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.318613126094979 | validation: 4.38464578935926]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 4.087966020646844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.087966020646844 | validation: 4.231818069713833]
	TIME [epoch: 4.26 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9392073983304465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9392073983304465 | validation: 4.117188102251468]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 3.7631996707054025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7631996707054025 | validation: 3.9972834529274803]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 3.6202164386335514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6202164386335514 | validation: 3.862775228694028]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4890645026592435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4890645026592435 | validation: 3.6976795548169035]
	TIME [epoch: 4.24 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 3.313722437996355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.313722437996355 | validation: 3.5120471916579783]
	TIME [epoch: 4.24 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 3.1159129039404556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1159129039404556 | validation: 3.306004873031466]
	TIME [epoch: 4.26 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 2.881719525450111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.881719525450111 | validation: 3.046809418497734]
	TIME [epoch: 4.28 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 2.59355784530264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.59355784530264 | validation: 2.7008172714113554]
	TIME [epoch: 4.26 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 2.2496775235819477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2496775235819477 | validation: 2.237505323040405]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7758682116948448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7758682116948448 | validation: 1.5873867607543812]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 1.2333943873028121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2333943873028121 | validation: 0.7627390934510869]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6019850647825733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6019850647825733 | validation: 0.3768847769298142]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 0.33222919402765305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33222919402765305 | validation: 0.23613032063239353]
	TIME [epoch: 4.24 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2524838718842279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2524838718842279 | validation: 0.20245141691230345]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 0.21834718378919582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21834718378919582 | validation: 0.1711948046207245]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1854122695817454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1854122695817454 | validation: 0.14352136166767418]
	TIME [epoch: 4.26 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15698384542012028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15698384542012028 | validation: 0.1186911859986567]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13242566347920665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13242566347920665 | validation: 0.09998872189040275]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1154431498574437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1154431498574437 | validation: 0.08512914006023056]
	TIME [epoch: 4.24 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 0.10197555430460836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10197555430460836 | validation: 0.0734876391179865]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 0.09462527327935145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09462527327935145 | validation: 0.06758375004756684]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08759681902914054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08759681902914054 | validation: 0.06323152283267415]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08332006518881975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08332006518881975 | validation: 0.059172076523454895]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07839548678606165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07839548678606165 | validation: 0.059778787975089724]
	TIME [epoch: 4.29 sec]
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0805682231088473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0805682231088473 | validation: 0.0583724394416859]
	TIME [epoch: 4.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07970667985584645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07970667985584645 | validation: 0.061620047183644375]
	TIME [epoch: 4.25 sec]
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08071730967558254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08071730967558254 | validation: 0.055737672290062615]
	TIME [epoch: 4.24 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08012492400751507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08012492400751507 | validation: 0.0609002893907521]
	TIME [epoch: 4.24 sec]
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07901122876141387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07901122876141387 | validation: 0.060042131794631415]
	TIME [epoch: 4.24 sec]
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07947496361424863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07947496361424863 | validation: 0.06088455968247418]
	TIME [epoch: 4.24 sec]
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0806290627542828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0806290627542828 | validation: 0.05643919387388803]
	TIME [epoch: 4.24 sec]
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07667251019853964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07667251019853964 | validation: 0.058210732945231713]
	TIME [epoch: 4.24 sec]
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07987410216101595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07987410216101595 | validation: 0.05781986292361968]
	TIME [epoch: 4.25 sec]
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07885668554898062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07885668554898062 | validation: 0.060306054412891406]
	TIME [epoch: 4.26 sec]
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07934498826134617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07934498826134617 | validation: 0.05826435428789925]
	TIME [epoch: 4.24 sec]
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07823285258502512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07823285258502512 | validation: 0.05908149529932824]
	TIME [epoch: 4.24 sec]
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07831664366288482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07831664366288482 | validation: 0.06229956696479072]
	TIME [epoch: 4.24 sec]
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07857394761559706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07857394761559706 | validation: 0.0581426954102454]
	TIME [epoch: 4.24 sec]
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07885632493578708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07885632493578708 | validation: 0.05623462249315751]
	TIME [epoch: 4.24 sec]
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08057740717075142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08057740717075142 | validation: 0.05796050279197095]
	TIME [epoch: 4.24 sec]
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07922746416154841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07922746416154841 | validation: 0.06024595773793846]
	TIME [epoch: 4.25 sec]
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07977040406481048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07977040406481048 | validation: 0.058697733681313154]
	TIME [epoch: 4.24 sec]
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07713841711270544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07713841711270544 | validation: 0.056183405543269636]
	TIME [epoch: 4.25 sec]
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07770294724140558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07770294724140558 | validation: 0.056726288854735615]
	TIME [epoch: 4.28 sec]
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07184107015622639		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 0.07184107015622639 | validation: 0.060667593581688065]
	TIME [epoch: 95.2 sec]
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0724089453985591		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 0.0724089453985591 | validation: 0.05776740376637249]
	TIME [epoch: 8.39 sec]
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07149263016989314		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 0.07149263016989314 | validation: 0.057958775364574264]
	TIME [epoch: 8.38 sec]
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06988914204565869		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 0.06988914204565869 | validation: 0.05525086535920698]
	TIME [epoch: 8.37 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06840579333033446		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 0.06840579333033446 | validation: 0.05647455044073872]
	TIME [epoch: 8.35 sec]
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06920339370106221		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 0.06920339370106221 | validation: 0.05648055218776779]
	TIME [epoch: 8.33 sec]
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0680712027762176		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 0.0680712027762176 | validation: 0.056966899409187734]
	TIME [epoch: 8.34 sec]
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06883056517320804		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 0.06883056517320804 | validation: 0.05310520263684927]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06760349150568287		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 0.06760349150568287 | validation: 0.05334258838292083]
	TIME [epoch: 8.36 sec]
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06792773370345698		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.06792773370345698 | validation: 0.05533451236736651]
	TIME [epoch: 8.34 sec]
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06769684722342505		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 0.06769684722342505 | validation: 0.05236447577420925]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06816225119966593		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 0.06816225119966593 | validation: 0.054527955568628994]
	TIME [epoch: 8.33 sec]
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06601976184544193		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 0.06601976184544193 | validation: 0.051039320718897316]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06392539923502302		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 0.06392539923502302 | validation: 0.05011426263857029]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06374761531199594		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 0.06374761531199594 | validation: 0.050108466766120685]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06377023843181301		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 0.06377023843181301 | validation: 0.05218118550151099]
	TIME [epoch: 8.34 sec]
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0645769457368639		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 0.0645769457368639 | validation: 0.05250180289255306]
	TIME [epoch: 8.31 sec]
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06603888244004134		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 0.06603888244004134 | validation: 0.05097510553198225]
	TIME [epoch: 8.3 sec]
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06505833119209498		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 0.06505833119209498 | validation: 0.0488836994388627]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06327069002360897		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 0.06327069002360897 | validation: 0.05095419898412769]
	TIME [epoch: 8.32 sec]
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 0.062437891156288255		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 0.062437891156288255 | validation: 0.04996035711610283]
	TIME [epoch: 8.34 sec]
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06145559406430756		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 0.06145559406430756 | validation: 0.04798266492633671]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06232176615040615		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 0.06232176615040615 | validation: 0.05025553802150794]
	TIME [epoch: 8.32 sec]
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06242035734550706		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 0.06242035734550706 | validation: 0.052080659718195275]
	TIME [epoch: 8.31 sec]
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05946516633177967		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.05946516633177967 | validation: 0.0548773570456985]
	TIME [epoch: 8.31 sec]
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0631550363405538		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 0.0631550363405538 | validation: 0.047985133885653286]
	TIME [epoch: 8.32 sec]
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0591972351613065		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 0.0591972351613065 | validation: 0.047658526152965924]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 0.059079091154924476		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 0.059079091154924476 | validation: 0.04906297193660547]
	TIME [epoch: 8.32 sec]
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06090900257380755		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 0.06090900257380755 | validation: 0.04594361129009078]
	TIME [epoch: 8.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05925017167153604		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 0.05925017167153604 | validation: 0.04581294749221157]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 0.058498491908377526		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 0.058498491908377526 | validation: 0.04698623447142884]
	TIME [epoch: 8.34 sec]
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 0.059691654898746886		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 0.059691654898746886 | validation: 0.04577786046910393]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05693279971914965		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 0.05693279971914965 | validation: 0.046287541415466225]
	TIME [epoch: 8.36 sec]
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05654434291505673		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 0.05654434291505673 | validation: 0.04775949035183338]
	TIME [epoch: 8.32 sec]
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05896507815266279		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 0.05896507815266279 | validation: 0.04412017692068239]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05727658325923568		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 0.05727658325923568 | validation: 0.04554316294812501]
	TIME [epoch: 8.32 sec]
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05894726515722608		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 0.05894726515722608 | validation: 0.04570335435518283]
	TIME [epoch: 8.32 sec]
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05806507002980575		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 0.05806507002980575 | validation: 0.04404241088970571]
	TIME [epoch: 8.36 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05840950136421387		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 0.05840950136421387 | validation: 0.04538868898156745]
	TIME [epoch: 8.33 sec]
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05664635067269342		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.05664635067269342 | validation: 0.04289303084961657]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05774220988036808		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 0.05774220988036808 | validation: 0.04573255445469068]
	TIME [epoch: 8.34 sec]
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0568626070223663		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 0.0568626070223663 | validation: 0.04440628903658785]
	TIME [epoch: 8.33 sec]
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05704051058515956		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 0.05704051058515956 | validation: 0.04376204078419843]
	TIME [epoch: 8.34 sec]
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05575849107622304		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 0.05575849107622304 | validation: 0.04424888628558993]
	TIME [epoch: 8.37 sec]
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05618234627560548		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 0.05618234627560548 | validation: 0.04231840965150027]
	TIME [epoch: 8.33 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05841222368658141		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 0.05841222368658141 | validation: 0.04461833230744644]
	TIME [epoch: 8.35 sec]
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05659264016863556		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 0.05659264016863556 | validation: 0.04522394866236846]
	TIME [epoch: 8.34 sec]
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05727205586283999		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 0.05727205586283999 | validation: 0.04145121072458036]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05385643985096988		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 0.05385643985096988 | validation: 0.04383532997662533]
	TIME [epoch: 8.35 sec]
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0550041747385579		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 0.0550041747385579 | validation: 0.04403769911359973]
	TIME [epoch: 8.33 sec]
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05635600589347826		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 0.05635600589347826 | validation: 0.04201866622136734]
	TIME [epoch: 107 sec]
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05568791459426568		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 0.05568791459426568 | validation: 0.04482685916953792]
	TIME [epoch: 19 sec]
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 0.055335248987524635		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 0.055335248987524635 | validation: 0.04414018857335752]
	TIME [epoch: 18.9 sec]
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 0.055969276266474466		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 0.055969276266474466 | validation: 0.04413634011091011]
	TIME [epoch: 19 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05475761656612831		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.05475761656612831 | validation: 0.04411674444317528]
	TIME [epoch: 19 sec]
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05440018021311779		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 0.05440018021311779 | validation: 0.04385659791445455]
	TIME [epoch: 18.9 sec]
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 0.055270739384436676		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 0.055270739384436676 | validation: 0.04104540298157959]
	TIME [epoch: 19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05455081569327147		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 0.05455081569327147 | validation: 0.04332709133058805]
	TIME [epoch: 18.9 sec]
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05580544118613372		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: 0.05580544118613372 | validation: 0.04399670696649102]
	TIME [epoch: 18.9 sec]
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: 0.054474175520649794		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: 0.054474175520649794 | validation: 0.043476566788337404]
	TIME [epoch: 19 sec]
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05472236692854596		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: 0.05472236692854596 | validation: 0.04084023068896084]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05363847514261044		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: 0.05363847514261044 | validation: 0.04265630012220495]
	TIME [epoch: 19 sec]
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: 0.055084412586439944		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: 0.055084412586439944 | validation: 0.042441277311289764]
	TIME [epoch: 18.9 sec]
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05336761688041979		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: 0.05336761688041979 | validation: 0.0423699770144251]
	TIME [epoch: 18.9 sec]
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: 0.052254906165450796		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: 0.052254906165450796 | validation: 0.0433993099667081]
	TIME [epoch: 19 sec]
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05316294697657452		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: 0.05316294697657452 | validation: 0.042231034307066914]
	TIME [epoch: 18.9 sec]
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: 0.052344101103579094		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: 0.052344101103579094 | validation: 0.03997036598013761]
	TIME [epoch: 19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05075832902518695		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: 0.05075832902518695 | validation: 0.0539298871105882]
	TIME [epoch: 18.9 sec]
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06037674514441833		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: 0.06037674514441833 | validation: 0.05279317956332418]
	TIME [epoch: 18.9 sec]
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05639413396323401		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.05639413396323401 | validation: 0.052483033447812015]
	TIME [epoch: 18.9 sec]
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05461331615412211		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: 0.05461331615412211 | validation: 0.05589002967470641]
	TIME [epoch: 18.9 sec]
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05525165691648984		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: 0.05525165691648984 | validation: 0.052422515344398915]
	TIME [epoch: 19 sec]
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05292959568246447		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: 0.05292959568246447 | validation: 0.04068768946633491]
	TIME [epoch: 18.9 sec]
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04905384827956563		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: 0.04905384827956563 | validation: 0.03982964498230093]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04761969709011238		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: 0.04761969709011238 | validation: 0.03989521848092267]
	TIME [epoch: 19 sec]
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: 0.046226614189794575		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: 0.046226614189794575 | validation: 0.03729268835382718]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: 0.045696581842540954		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: 0.045696581842540954 | validation: 0.035420594276836824]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: 0.043889235470040935		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: 0.043889235470040935 | validation: 0.03470489819625358]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04334889900965686		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: 0.04334889900965686 | validation: 0.031091580992109596]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_129.pth
	Model improved!!!
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04246142611896066		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: 0.04246142611896066 | validation: 0.03679030522749663]
	TIME [epoch: 18.9 sec]
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0420253388512605		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: 0.0420253388512605 | validation: 0.031108632826371135]
	TIME [epoch: 18.9 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04180557668833496		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: 0.04180557668833496 | validation: 0.03128062862398563]
	TIME [epoch: 18.9 sec]
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04005276069087009		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: 0.04005276069087009 | validation: 0.032345350351690054]
	TIME [epoch: 18.9 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: 0.039987697264420766		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: 0.039987697264420766 | validation: 0.030556254729930468]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03888388388314626		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 0.03888388388314626 | validation: 0.029605924040779233]
	TIME [epoch: 19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: 0.037650765964581334		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: 0.037650765964581334 | validation: 0.027969604825077425]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03717399117623108		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: 0.03717399117623108 | validation: 0.03156399163523672]
	TIME [epoch: 19 sec]
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04077758800816315		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: 0.04077758800816315 | validation: 0.03479887204274786]
	TIME [epoch: 18.9 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0374223464353442		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: 0.0374223464353442 | validation: 0.03642329019336967]
	TIME [epoch: 18.9 sec]
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03797501758200447		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: 0.03797501758200447 | validation: 0.03855560222889951]
	TIME [epoch: 19 sec]
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: 0.036920520895074777		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: 0.036920520895074777 | validation: 0.03549647435030982]
	TIME [epoch: 18.9 sec]
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03762622944298815		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: 0.03762622944298815 | validation: 0.03665077243060236]
	TIME [epoch: 18.9 sec]
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03736000176877875		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: 0.03736000176877875 | validation: 0.03637917422608815]
	TIME [epoch: 18.9 sec]
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03748492880350131		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: 0.03748492880350131 | validation: 0.03515882572106051]
	TIME [epoch: 18.9 sec]
EPOCH 145/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03643189067292073		[learning rate: 0.0023352]
	Learning Rate: 0.00233525
	LOSS [training: 0.03643189067292073 | validation: 0.034626258114252906]
	TIME [epoch: 19 sec]
EPOCH 146/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03546715113039963		[learning rate: 0.0022997]
	Learning Rate: 0.00229968
	LOSS [training: 0.03546715113039963 | validation: 0.027946864364963277]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/500:
	Training over batches...
		[batch 4/4] avg loss: 0.034103940088746725		[learning rate: 0.0022646]
	Learning Rate: 0.00226464
	LOSS [training: 0.034103940088746725 | validation: 0.02721792737650062]
	TIME [epoch: 19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03202860929589925		[learning rate: 0.0022301]
	Learning Rate: 0.00223015
	LOSS [training: 0.03202860929589925 | validation: 0.024372441045633596]
	TIME [epoch: 19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_148.pth
	Model improved!!!
EPOCH 149/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03248565435758023		[learning rate: 0.0021962]
	Learning Rate: 0.00219617
	LOSS [training: 0.03248565435758023 | validation: 0.02718505454036777]
	TIME [epoch: 18.9 sec]
EPOCH 150/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03231637048529132		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 0.03231637048529132 | validation: 0.027041595148360076]
	TIME [epoch: 19 sec]
EPOCH 151/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03161633200099487		[learning rate: 0.0021298]
	Learning Rate: 0.00212977
	LOSS [training: 0.03161633200099487 | validation: 0.027168874460175622]
	TIME [epoch: 18.9 sec]
EPOCH 152/500:
	Training over batches...
		[batch 4/4] avg loss: 0.031234030296232226		[learning rate: 0.0020973]
	Learning Rate: 0.00209733
	LOSS [training: 0.031234030296232226 | validation: 0.023424038985734356]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_152.pth
	Model improved!!!
EPOCH 153/500:
	Training over batches...
		[batch 4/4] avg loss: 0.029960298676550666		[learning rate: 0.0020654]
	Learning Rate: 0.00206538
	LOSS [training: 0.029960298676550666 | validation: 0.026194163687587507]
	TIME [epoch: 19 sec]
EPOCH 154/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03202786513172149		[learning rate: 0.0020339]
	Learning Rate: 0.00203392
	LOSS [training: 0.03202786513172149 | validation: 0.025600161127270392]
	TIME [epoch: 18.9 sec]
EPOCH 155/500:
	Training over batches...
		[batch 4/4] avg loss: 0.028640302878877255		[learning rate: 0.0020029]
	Learning Rate: 0.00200293
	LOSS [training: 0.028640302878877255 | validation: 0.029997583009543645]
	TIME [epoch: 19 sec]
EPOCH 156/500:
	Training over batches...
		[batch 4/4] avg loss: 0.028045006462872082		[learning rate: 0.0019724]
	Learning Rate: 0.00197242
	LOSS [training: 0.028045006462872082 | validation: 0.02496271320674212]
	TIME [epoch: 18.9 sec]
EPOCH 157/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0296650258875377		[learning rate: 0.0019424]
	Learning Rate: 0.00194238
	LOSS [training: 0.0296650258875377 | validation: 0.02208728559611873]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02593254115298347		[learning rate: 0.0019128]
	Learning Rate: 0.00191279
	LOSS [training: 0.02593254115298347 | validation: 0.026166235127905974]
	TIME [epoch: 18.9 sec]
EPOCH 159/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02483464892819526		[learning rate: 0.0018836]
	Learning Rate: 0.00188365
	LOSS [training: 0.02483464892819526 | validation: 0.018139750925699444]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_159.pth
	Model improved!!!
EPOCH 160/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02384821760318149		[learning rate: 0.001855]
	Learning Rate: 0.00185495
	LOSS [training: 0.02384821760318149 | validation: 0.01834034343936658]
	TIME [epoch: 18.9 sec]
EPOCH 161/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02308195047426501		[learning rate: 0.0018267]
	Learning Rate: 0.0018267
	LOSS [training: 0.02308195047426501 | validation: 0.016515495547075872]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02150688718925194		[learning rate: 0.0017989]
	Learning Rate: 0.00179887
	LOSS [training: 0.02150688718925194 | validation: 0.017958996544638065]
	TIME [epoch: 18.9 sec]
EPOCH 163/500:
	Training over batches...
		[batch 4/4] avg loss: 0.022051853357877932		[learning rate: 0.0017715]
	Learning Rate: 0.00177147
	LOSS [training: 0.022051853357877932 | validation: 0.01822113834838803]
	TIME [epoch: 18.9 sec]
EPOCH 164/500:
	Training over batches...
		[batch 4/4] avg loss: 0.020912812272926604		[learning rate: 0.0017445]
	Learning Rate: 0.00174448
	LOSS [training: 0.020912812272926604 | validation: 0.016292163404276343]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/500:
	Training over batches...
		[batch 4/4] avg loss: 0.020461879995413355		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: 0.020461879995413355 | validation: 0.01700792749273055]
	TIME [epoch: 19 sec]
EPOCH 166/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02090469172741287		[learning rate: 0.0016917]
	Learning Rate: 0.00169174
	LOSS [training: 0.02090469172741287 | validation: 0.014995764660531371]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01821846727233223		[learning rate: 0.001666]
	Learning Rate: 0.00166597
	LOSS [training: 0.01821846727233223 | validation: 0.018442002620063284]
	TIME [epoch: 18.9 sec]
EPOCH 168/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0224082474977428		[learning rate: 0.0016406]
	Learning Rate: 0.00164059
	LOSS [training: 0.0224082474977428 | validation: 0.016098347492560755]
	TIME [epoch: 18.9 sec]
EPOCH 169/500:
	Training over batches...
		[batch 4/4] avg loss: 0.017378075695373217		[learning rate: 0.0016156]
	Learning Rate: 0.0016156
	LOSS [training: 0.017378075695373217 | validation: 0.014572034491544504]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01740379235680513		[learning rate: 0.001591]
	Learning Rate: 0.00159099
	LOSS [training: 0.01740379235680513 | validation: 0.012981738894728437]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/500:
	Training over batches...
		[batch 4/4] avg loss: 0.015909759906220223		[learning rate: 0.0015668]
	Learning Rate: 0.00156675
	LOSS [training: 0.015909759906220223 | validation: 0.012165289664281587]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/500:
	Training over batches...
		[batch 4/4] avg loss: 0.015022961430774074		[learning rate: 0.0015429]
	Learning Rate: 0.00154288
	LOSS [training: 0.015022961430774074 | validation: 0.010768481653371747]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/500:
	Training over batches...
		[batch 4/4] avg loss: 0.014215082030389854		[learning rate: 0.0015194]
	Learning Rate: 0.00151938
	LOSS [training: 0.014215082030389854 | validation: 0.013592797212628065]
	TIME [epoch: 19 sec]
EPOCH 174/500:
	Training over batches...
		[batch 4/4] avg loss: 0.013832043247411462		[learning rate: 0.0014962]
	Learning Rate: 0.00149624
	LOSS [training: 0.013832043247411462 | validation: 0.010342295174546275]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01384525992821354		[learning rate: 0.0014734]
	Learning Rate: 0.00147344
	LOSS [training: 0.01384525992821354 | validation: 0.008998094795425191]
	TIME [epoch: 19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_175.pth
	Model improved!!!
EPOCH 176/500:
	Training over batches...
		[batch 4/4] avg loss: 0.014625141720198437		[learning rate: 0.001451]
	Learning Rate: 0.001451
	LOSS [training: 0.014625141720198437 | validation: 0.011465109441150857]
	TIME [epoch: 18.9 sec]
EPOCH 177/500:
	Training over batches...
		[batch 4/4] avg loss: 0.014403743079287865		[learning rate: 0.0014289]
	Learning Rate: 0.00142889
	LOSS [training: 0.014403743079287865 | validation: 0.008051547574582462]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/500:
	Training over batches...
		[batch 4/4] avg loss: 0.012685434370971128		[learning rate: 0.0014071]
	Learning Rate: 0.00140713
	LOSS [training: 0.012685434370971128 | validation: 0.007491975382627237]
	TIME [epoch: 19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/500:
	Training over batches...
		[batch 4/4] avg loss: 0.012503864544794664		[learning rate: 0.0013857]
	Learning Rate: 0.00138569
	LOSS [training: 0.012503864544794664 | validation: 0.008737160098479559]
	TIME [epoch: 18.9 sec]
EPOCH 180/500:
	Training over batches...
		[batch 4/4] avg loss: 0.012147659644149033		[learning rate: 0.0013646]
	Learning Rate: 0.00136458
	LOSS [training: 0.012147659644149033 | validation: 0.007444530792578971]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/500:
	Training over batches...
		[batch 4/4] avg loss: 0.011091819301581226		[learning rate: 0.0013438]
	Learning Rate: 0.0013438
	LOSS [training: 0.011091819301581226 | validation: 0.008043060667157148]
	TIME [epoch: 18.9 sec]
EPOCH 182/500:
	Training over batches...
		[batch 4/4] avg loss: 0.010536954465726328		[learning rate: 0.0013233]
	Learning Rate: 0.00132333
	LOSS [training: 0.010536954465726328 | validation: 0.006802479887444058]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/500:
	Training over batches...
		[batch 4/4] avg loss: 0.010350058388653865		[learning rate: 0.0013032]
	Learning Rate: 0.00130317
	LOSS [training: 0.010350058388653865 | validation: 0.0073838336442330985]
	TIME [epoch: 19 sec]
EPOCH 184/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00998696350729114		[learning rate: 0.0012833]
	Learning Rate: 0.00128332
	LOSS [training: 0.00998696350729114 | validation: 0.011690284157265907]
	TIME [epoch: 18.9 sec]
EPOCH 185/500:
	Training over batches...
		[batch 4/4] avg loss: 0.010700995290885577		[learning rate: 0.0012638]
	Learning Rate: 0.00126377
	LOSS [training: 0.010700995290885577 | validation: 0.008608096307610363]
	TIME [epoch: 18.9 sec]
EPOCH 186/500:
	Training over batches...
		[batch 4/4] avg loss: 0.010940076030334823		[learning rate: 0.0012445]
	Learning Rate: 0.00124451
	LOSS [training: 0.010940076030334823 | validation: 0.00661702834643672]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00963814859108501		[learning rate: 0.0012256]
	Learning Rate: 0.00122556
	LOSS [training: 0.00963814859108501 | validation: 0.008882936107466231]
	TIME [epoch: 18.9 sec]
EPOCH 188/500:
	Training over batches...
		[batch 4/4] avg loss: 0.010051649212987242		[learning rate: 0.0012069]
	Learning Rate: 0.00120689
	LOSS [training: 0.010051649212987242 | validation: 0.006919500673451971]
	TIME [epoch: 18.9 sec]
EPOCH 189/500:
	Training over batches...
		[batch 4/4] avg loss: 0.009179333626275196		[learning rate: 0.0011885]
	Learning Rate: 0.0011885
	LOSS [training: 0.009179333626275196 | validation: 0.006770191134588103]
	TIME [epoch: 18.9 sec]
EPOCH 190/500:
	Training over batches...
		[batch 4/4] avg loss: 0.008578527364708282		[learning rate: 0.0011704]
	Learning Rate: 0.0011704
	LOSS [training: 0.008578527364708282 | validation: 0.005691707802251367]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/500:
	Training over batches...
		[batch 4/4] avg loss: 0.008197861823936633		[learning rate: 0.0011526]
	Learning Rate: 0.00115257
	LOSS [training: 0.008197861823936633 | validation: 0.006496165088792557]
	TIME [epoch: 18.9 sec]
EPOCH 192/500:
	Training over batches...
		[batch 4/4] avg loss: 0.008457048277436218		[learning rate: 0.001135]
	Learning Rate: 0.00113501
	LOSS [training: 0.008457048277436218 | validation: 0.006273424853126467]
	TIME [epoch: 18.9 sec]
EPOCH 193/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007300450684778337		[learning rate: 0.0011177]
	Learning Rate: 0.00111772
	LOSS [training: 0.007300450684778337 | validation: 0.006221725695118436]
	TIME [epoch: 18.9 sec]
EPOCH 194/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00663888640324205		[learning rate: 0.0011007]
	Learning Rate: 0.00110069
	LOSS [training: 0.00663888640324205 | validation: 0.005677104602563215]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007289613039937955		[learning rate: 0.0010839]
	Learning Rate: 0.00108393
	LOSS [training: 0.007289613039937955 | validation: 0.004800525717220963]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006000434911858001		[learning rate: 0.0010674]
	Learning Rate: 0.00106741
	LOSS [training: 0.006000434911858001 | validation: 0.004207046068654162]
	TIME [epoch: 19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005833070685070873		[learning rate: 0.0010512]
	Learning Rate: 0.00105115
	LOSS [training: 0.005833070685070873 | validation: 0.003659992674064843]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006240487722407016		[learning rate: 0.0010351]
	Learning Rate: 0.00103514
	LOSS [training: 0.006240487722407016 | validation: 0.004784515098298168]
	TIME [epoch: 19 sec]
EPOCH 199/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006316286508850761		[learning rate: 0.0010194]
	Learning Rate: 0.00101937
	LOSS [training: 0.006316286508850761 | validation: 0.0035268545031675344]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005040313924209373		[learning rate: 0.0010038]
	Learning Rate: 0.00100384
	LOSS [training: 0.005040313924209373 | validation: 0.003598058379547565]
	TIME [epoch: 18.9 sec]
EPOCH 201/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005567963463120626		[learning rate: 0.00098855]
	Learning Rate: 0.000988553
	LOSS [training: 0.005567963463120626 | validation: 0.0037816599545352266]
	TIME [epoch: 19 sec]
EPOCH 202/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005464019820882666		[learning rate: 0.00097349]
	Learning Rate: 0.000973494
	LOSS [training: 0.005464019820882666 | validation: 0.004931865436309585]
	TIME [epoch: 18.9 sec]
EPOCH 203/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004824345772265114		[learning rate: 0.00095866]
	Learning Rate: 0.000958664
	LOSS [training: 0.004824345772265114 | validation: 0.004996334304695151]
	TIME [epoch: 19 sec]
EPOCH 204/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005146836036582785		[learning rate: 0.00094406]
	Learning Rate: 0.000944061
	LOSS [training: 0.005146836036582785 | validation: 0.005862579218185975]
	TIME [epoch: 18.9 sec]
EPOCH 205/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0049284872714596955		[learning rate: 0.00092968]
	Learning Rate: 0.00092968
	LOSS [training: 0.0049284872714596955 | validation: 0.0025514664331510476]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003953038670080208		[learning rate: 0.00091552]
	Learning Rate: 0.000915518
	LOSS [training: 0.003953038670080208 | validation: 0.0032359790430049945]
	TIME [epoch: 19 sec]
EPOCH 207/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003949841670843225		[learning rate: 0.00090157]
	Learning Rate: 0.000901571
	LOSS [training: 0.003949841670843225 | validation: 0.002417391431397625]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003997919752313116		[learning rate: 0.00088784]
	Learning Rate: 0.000887837
	LOSS [training: 0.003997919752313116 | validation: 0.0023482599703762595]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0035072698424745423		[learning rate: 0.00087431]
	Learning Rate: 0.000874312
	LOSS [training: 0.0035072698424745423 | validation: 0.0017001452823486116]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0035327556894967672		[learning rate: 0.00086099]
	Learning Rate: 0.000860994
	LOSS [training: 0.0035327556894967672 | validation: 0.0027876157095753466]
	TIME [epoch: 18.9 sec]
EPOCH 211/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002935020663201399		[learning rate: 0.00084788]
	Learning Rate: 0.000847878
	LOSS [training: 0.002935020663201399 | validation: 0.000881524327388838]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0032351922602194333		[learning rate: 0.00083496]
	Learning Rate: 0.000834962
	LOSS [training: 0.0032351922602194333 | validation: 0.0029368314881175177]
	TIME [epoch: 18.9 sec]
EPOCH 213/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0029113298766783747		[learning rate: 0.00082224]
	Learning Rate: 0.000822243
	LOSS [training: 0.0029113298766783747 | validation: 0.0025852675320571614]
	TIME [epoch: 18.9 sec]
EPOCH 214/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0028833024645296577		[learning rate: 0.00080972]
	Learning Rate: 0.000809717
	LOSS [training: 0.0028833024645296577 | validation: 0.0010604749635552623]
	TIME [epoch: 18.9 sec]
EPOCH 215/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003155557919409278		[learning rate: 0.00079738]
	Learning Rate: 0.000797382
	LOSS [training: 0.003155557919409278 | validation: 0.002463487106440487]
	TIME [epoch: 18.9 sec]
EPOCH 216/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0024290459639379446		[learning rate: 0.00078524]
	Learning Rate: 0.000785236
	LOSS [training: 0.0024290459639379446 | validation: 0.002322311289865547]
	TIME [epoch: 18.9 sec]
EPOCH 217/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0023696884053731396		[learning rate: 0.00077327]
	Learning Rate: 0.000773274
	LOSS [training: 0.0023696884053731396 | validation: 0.0017965207176114115]
	TIME [epoch: 18.9 sec]
EPOCH 218/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0022465397186576938		[learning rate: 0.00076149]
	Learning Rate: 0.000761494
	LOSS [training: 0.0022465397186576938 | validation: 0.002405594390256233]
	TIME [epoch: 18.9 sec]
EPOCH 219/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019198640451392863		[learning rate: 0.00074989]
	Learning Rate: 0.000749894
	LOSS [training: 0.0019198640451392863 | validation: 0.0012228891584674165]
	TIME [epoch: 18.9 sec]
EPOCH 220/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019701232640248955		[learning rate: 0.00073847]
	Learning Rate: 0.000738471
	LOSS [training: 0.0019701232640248955 | validation: 0.0020960919470942563]
	TIME [epoch: 18.9 sec]
EPOCH 221/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016987128165923168		[learning rate: 0.00072722]
	Learning Rate: 0.000727221
	LOSS [training: 0.0016987128165923168 | validation: 0.0013709055848183027]
	TIME [epoch: 18.9 sec]
EPOCH 222/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010937402547027799		[learning rate: 0.00071614]
	Learning Rate: 0.000716143
	LOSS [training: 0.0010937402547027799 | validation: 0.0005106674883190094]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001513123385118349		[learning rate: 0.00070523]
	Learning Rate: 0.000705234
	LOSS [training: 0.001513123385118349 | validation: 0.0006100848292676875]
	TIME [epoch: 18.9 sec]
EPOCH 224/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012659433774886502		[learning rate: 0.00069449]
	Learning Rate: 0.000694491
	LOSS [training: 0.0012659433774886502 | validation: 0.0009097225730295735]
	TIME [epoch: 18.9 sec]
EPOCH 225/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001694473849496171		[learning rate: 0.00068391]
	Learning Rate: 0.000683912
	LOSS [training: 0.001694473849496171 | validation: 0.0012608430167189112]
	TIME [epoch: 18.9 sec]
EPOCH 226/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019635041681899256		[learning rate: 0.00067349]
	Learning Rate: 0.000673493
	LOSS [training: 0.0019635041681899256 | validation: 0.0023074678082584134]
	TIME [epoch: 18.9 sec]
EPOCH 227/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0015603815139758489		[learning rate: 0.00066323]
	Learning Rate: 0.000663234
	LOSS [training: 0.0015603815139758489 | validation: 0.0015691795477120927]
	TIME [epoch: 18.9 sec]
EPOCH 228/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016682043853422672		[learning rate: 0.00065313]
	Learning Rate: 0.00065313
	LOSS [training: 0.0016682043853422672 | validation: 0.0016154315184155445]
	TIME [epoch: 18.9 sec]
EPOCH 229/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001691803337741827		[learning rate: 0.00064318]
	Learning Rate: 0.000643181
	LOSS [training: 0.001691803337741827 | validation: 0.0009859956569209997]
	TIME [epoch: 18.9 sec]
EPOCH 230/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009317814581180484		[learning rate: 0.00063338]
	Learning Rate: 0.000633383
	LOSS [training: 0.0009317814581180484 | validation: 0.00031034707085383543]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011726294787215324		[learning rate: 0.00062373]
	Learning Rate: 0.000623735
	LOSS [training: 0.0011726294787215324 | validation: 0.0011481412707382002]
	TIME [epoch: 18.9 sec]
EPOCH 232/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010181143554901968		[learning rate: 0.00061423]
	Learning Rate: 0.000614233
	LOSS [training: 0.0010181143554901968 | validation: 0.0004200943345853135]
	TIME [epoch: 18.9 sec]
EPOCH 233/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008324992811620043		[learning rate: 0.00060488]
	Learning Rate: 0.000604876
	LOSS [training: 0.0008324992811620043 | validation: 0.0004603888517850172]
	TIME [epoch: 18.9 sec]
EPOCH 234/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001093933917149775		[learning rate: 0.00059566]
	Learning Rate: 0.000595662
	LOSS [training: 0.001093933917149775 | validation: 0.00043919734381138827]
	TIME [epoch: 18.9 sec]
EPOCH 235/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001005991246837556		[learning rate: 0.00058659]
	Learning Rate: 0.000586588
	LOSS [training: 0.001005991246837556 | validation: 0.0008773934729764022]
	TIME [epoch: 18.9 sec]
EPOCH 236/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005003604264996415		[learning rate: 0.00057765]
	Learning Rate: 0.000577652
	LOSS [training: 0.0005003604264996415 | validation: 0.0006841379214683903]
	TIME [epoch: 18.9 sec]
EPOCH 237/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001064512412696293		[learning rate: 0.00056885]
	Learning Rate: 0.000568853
	LOSS [training: 0.001064512412696293 | validation: 0.0002710716780824769]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005973437799503161		[learning rate: 0.00056019]
	Learning Rate: 0.000560187
	LOSS [training: 0.0005973437799503161 | validation: 0.00056326165803205]
	TIME [epoch: 18.9 sec]
EPOCH 239/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008807241998006112		[learning rate: 0.00055165]
	Learning Rate: 0.000551654
	LOSS [training: 0.0008807241998006112 | validation: 0.00030970459517636154]
	TIME [epoch: 18.9 sec]
EPOCH 240/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006386630950696083		[learning rate: 0.00054325]
	Learning Rate: 0.00054325
	LOSS [training: 0.0006386630950696083 | validation: 0.0003830367142633575]
	TIME [epoch: 18.9 sec]
EPOCH 241/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008231007975461307		[learning rate: 0.00053497]
	Learning Rate: 0.000534975
	LOSS [training: 0.0008231007975461307 | validation: 0.0009864672761390162]
	TIME [epoch: 18.9 sec]
EPOCH 242/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004752702036915513		[learning rate: 0.00052683]
	Learning Rate: 0.000526825
	LOSS [training: 0.0004752702036915513 | validation: 0.00022202826590057213]
	TIME [epoch: 18.9 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005342286227587747		[learning rate: 0.0005188]
	Learning Rate: 0.0005188
	LOSS [training: 0.0005342286227587747 | validation: 7.448892347611303e-05]
	TIME [epoch: 19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_243.pth
	Model improved!!!
EPOCH 244/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00037959334898349814		[learning rate: 0.0005109]
	Learning Rate: 0.000510897
	LOSS [training: 0.00037959334898349814 | validation: 0.0013793378214662553]
	TIME [epoch: 18.9 sec]
EPOCH 245/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006804342126960142		[learning rate: 0.00050311]
	Learning Rate: 0.000503114
	LOSS [training: 0.0006804342126960142 | validation: 2.027112305898272e-05]
	TIME [epoch: 19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_245.pth
	Model improved!!!
EPOCH 246/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004905190240673043		[learning rate: 0.00049545]
	Learning Rate: 0.00049545
	LOSS [training: 0.0004905190240673043 | validation: -0.00025985218707470943]
	TIME [epoch: 19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00021675439150287557		[learning rate: 0.0004879]
	Learning Rate: 0.000487903
	LOSS [training: 0.00021675439150287557 | validation: 0.0006979119150832248]
	TIME [epoch: 18.9 sec]
EPOCH 248/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006745991058849782		[learning rate: 0.00048047]
	Learning Rate: 0.00048047
	LOSS [training: 0.0006745991058849782 | validation: 0.0001600055449512552]
	TIME [epoch: 19 sec]
EPOCH 249/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008652474768492159		[learning rate: 0.00047315]
	Learning Rate: 0.000473151
	LOSS [training: 0.0008652474768492159 | validation: 0.0018946873447998054]
	TIME [epoch: 18.9 sec]
EPOCH 250/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00040167528671418396		[learning rate: 0.00046594]
	Learning Rate: 0.000465944
	LOSS [training: 0.00040167528671418396 | validation: 0.0009880623009907033]
	TIME [epoch: 19 sec]
EPOCH 251/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00038537394267819456		[learning rate: 0.00045885]
	Learning Rate: 0.000458846
	LOSS [training: 0.00038537394267819456 | validation: -0.0009413179926439876]
	TIME [epoch: 130 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00042565420141167507		[learning rate: 0.00045186]
	Learning Rate: 0.000451856
	LOSS [training: 0.00042565420141167507 | validation: -0.00018119669501983054]
	TIME [epoch: 42.2 sec]
EPOCH 253/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005418874956143757		[learning rate: 0.00044497]
	Learning Rate: 0.000444973
	LOSS [training: 0.0005418874956143757 | validation: -6.525741800747384e-06]
	TIME [epoch: 42.1 sec]
EPOCH 254/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004869493665321159		[learning rate: 0.00043819]
	Learning Rate: 0.000438194
	LOSS [training: 0.0004869493665321159 | validation: 0.0006495551321261286]
	TIME [epoch: 42.1 sec]
EPOCH 255/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005852640960612519		[learning rate: 0.00043152]
	Learning Rate: 0.000431519
	LOSS [training: 0.0005852640960612519 | validation: 3.2763438307930476e-05]
	TIME [epoch: 42 sec]
EPOCH 256/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002569996027664108		[learning rate: 0.00042495]
	Learning Rate: 0.000424946
	LOSS [training: 0.0002569996027664108 | validation: 0.00025215218272723174]
	TIME [epoch: 42 sec]
EPOCH 257/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00045863467137787346		[learning rate: 0.00041847]
	Learning Rate: 0.000418472
	LOSS [training: 0.00045863467137787346 | validation: 0.0012704667841763123]
	TIME [epoch: 42 sec]
EPOCH 258/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006419200851001491		[learning rate: 0.0004121]
	Learning Rate: 0.000412098
	LOSS [training: 0.0006419200851001491 | validation: -8.906687972568765e-06]
	TIME [epoch: 42 sec]
EPOCH 259/500:
	Training over batches...
		[batch 4/4] avg loss: -8.555366974211375e-05		[learning rate: 0.00040582]
	Learning Rate: 0.00040582
	LOSS [training: -8.555366974211375e-05 | validation: 0.00037274600794678746]
	TIME [epoch: 42 sec]
EPOCH 260/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000639873802006259		[learning rate: 0.00039964]
	Learning Rate: 0.000399638
	LOSS [training: 0.000639873802006259 | validation: 0.00026332162973815226]
	TIME [epoch: 42 sec]
EPOCH 261/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00036596244907917733		[learning rate: 0.00039355]
	Learning Rate: 0.00039355
	LOSS [training: 0.00036596244907917733 | validation: 0.0005932704370326673]
	TIME [epoch: 42 sec]
EPOCH 262/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003310555489999358		[learning rate: 0.00038755]
	Learning Rate: 0.000387555
	LOSS [training: 0.0003310555489999358 | validation: -0.0010427994776029802]
	TIME [epoch: 42 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/500:
	Training over batches...
		[batch 4/4] avg loss: 3.847500471514943e-05		[learning rate: 0.00038165]
	Learning Rate: 0.000381651
	LOSS [training: 3.847500471514943e-05 | validation: 0.00040104062717803934]
	TIME [epoch: 42.1 sec]
EPOCH 264/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00037608862633871045		[learning rate: 0.00037584]
	Learning Rate: 0.000375837
	LOSS [training: 0.00037608862633871045 | validation: 0.00015602225772186882]
	TIME [epoch: 42.1 sec]
EPOCH 265/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005565706424387888		[learning rate: 0.00037011]
	Learning Rate: 0.000370112
	LOSS [training: 0.0005565706424387888 | validation: 0.0006835990987904692]
	TIME [epoch: 42 sec]
EPOCH 266/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003954828468155511		[learning rate: 0.00036447]
	Learning Rate: 0.000364474
	LOSS [training: 0.0003954828468155511 | validation: 0.0008817442350989918]
	TIME [epoch: 42.1 sec]
EPOCH 267/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00021771747128969412		[learning rate: 0.00035892]
	Learning Rate: 0.000358922
	LOSS [training: 0.00021771747128969412 | validation: 0.0001726758524596903]
	TIME [epoch: 42.1 sec]
EPOCH 268/500:
	Training over batches...
		[batch 4/4] avg loss: 8.301022537397353e-05		[learning rate: 0.00035345]
	Learning Rate: 0.000353454
	LOSS [training: 8.301022537397353e-05 | validation: 0.00024035989272614701]
	TIME [epoch: 42 sec]
EPOCH 269/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004118594912873477		[learning rate: 0.00034807]
	Learning Rate: 0.00034807
	LOSS [training: 0.0004118594912873477 | validation: -0.00032038730370861977]
	TIME [epoch: 42.1 sec]
EPOCH 270/500:
	Training over batches...
		[batch 4/4] avg loss: -3.76901177408322e-05		[learning rate: 0.00034277]
	Learning Rate: 0.000342768
	LOSS [training: -3.76901177408322e-05 | validation: 0.00046932439546583594]
	TIME [epoch: 42 sec]
EPOCH 271/500:
	Training over batches...
		[batch 4/4] avg loss: -2.193214976976465e-05		[learning rate: 0.00033755]
	Learning Rate: 0.000337546
	LOSS [training: -2.193214976976465e-05 | validation: 0.0006568167972709866]
	TIME [epoch: 42.1 sec]
EPOCH 272/500:
	Training over batches...
		[batch 4/4] avg loss: 9.10948968544769e-05		[learning rate: 0.0003324]
	Learning Rate: 0.000332404
	LOSS [training: 9.10948968544769e-05 | validation: 0.0004927863538949771]
	TIME [epoch: 42 sec]
EPOCH 273/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006784433179015502		[learning rate: 0.00032734]
	Learning Rate: 0.000327341
	LOSS [training: 0.0006784433179015502 | validation: -0.0002242856067934196]
	TIME [epoch: 42.1 sec]
EPOCH 274/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00015319677772331563		[learning rate: 0.00032235]
	Learning Rate: 0.000322354
	LOSS [training: -0.00015319677772331563 | validation: 0.00025919213001766205]
	TIME [epoch: 42.1 sec]
EPOCH 275/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003085988854149813		[learning rate: 0.00031744]
	Learning Rate: 0.000317444
	LOSS [training: 0.0003085988854149813 | validation: 0.0006001798764737996]
	TIME [epoch: 42.1 sec]
EPOCH 276/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002333077844027016		[learning rate: 0.00031261]
	Learning Rate: 0.000312608
	LOSS [training: 0.0002333077844027016 | validation: 0.0003581071653898409]
	TIME [epoch: 42.1 sec]
EPOCH 277/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00020189828668568398		[learning rate: 0.00030785]
	Learning Rate: 0.000307846
	LOSS [training: 0.00020189828668568398 | validation: 0.00015040078285853475]
	TIME [epoch: 42.1 sec]
EPOCH 278/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003333843549631339		[learning rate: 0.00030316]
	Learning Rate: 0.000303156
	LOSS [training: 0.0003333843549631339 | validation: -0.00013685673881912267]
	TIME [epoch: 42.1 sec]
EPOCH 279/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00029095289092242175		[learning rate: 0.00029854]
	Learning Rate: 0.000298538
	LOSS [training: 0.00029095289092242175 | validation: 0.00028442074635497193]
	TIME [epoch: 42.1 sec]
EPOCH 280/500:
	Training over batches...
		[batch 4/4] avg loss: -2.8821561597945706e-05		[learning rate: 0.00029399]
	Learning Rate: 0.000293991
	LOSS [training: -2.8821561597945706e-05 | validation: -0.00031226606623992973]
	TIME [epoch: 42.1 sec]
EPOCH 281/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00032207528720544		[learning rate: 0.00028951]
	Learning Rate: 0.000289512
	LOSS [training: 0.00032207528720544 | validation: -0.000114265402010302]
	TIME [epoch: 42.1 sec]
EPOCH 282/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00022123681652108697		[learning rate: 0.0002851]
	Learning Rate: 0.000285102
	LOSS [training: 0.00022123681652108697 | validation: -0.0003982717348961935]
	TIME [epoch: 42.1 sec]
EPOCH 283/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00014377301384632535		[learning rate: 0.00028076]
	Learning Rate: 0.000280759
	LOSS [training: 0.00014377301384632535 | validation: -0.00017116954095321813]
	TIME [epoch: 42 sec]
EPOCH 284/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005154064263826972		[learning rate: 0.00027648]
	Learning Rate: 0.000276482
	LOSS [training: 0.0005154064263826972 | validation: -0.0007902293191342822]
	TIME [epoch: 42 sec]
EPOCH 285/500:
	Training over batches...
		[batch 4/4] avg loss: 4.0478824603948466e-05		[learning rate: 0.00027227]
	Learning Rate: 0.00027227
	LOSS [training: 4.0478824603948466e-05 | validation: -1.5944916474381731e-06]
	TIME [epoch: 42 sec]
EPOCH 286/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000575134491404708		[learning rate: 0.00026812]
	Learning Rate: 0.000268123
	LOSS [training: 0.000575134491404708 | validation: -0.0001258682598079983]
	TIME [epoch: 42 sec]
EPOCH 287/500:
	Training over batches...
		[batch 4/4] avg loss: 8.35252793088115e-05		[learning rate: 0.00026404]
	Learning Rate: 0.000264038
	LOSS [training: 8.35252793088115e-05 | validation: -0.0002401899355027086]
	TIME [epoch: 42 sec]
EPOCH 288/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00015027180300821553		[learning rate: 0.00026002]
	Learning Rate: 0.000260016
	LOSS [training: 0.00015027180300821553 | validation: 8.669670551094246e-05]
	TIME [epoch: 44.8 sec]
EPOCH 289/500:
	Training over batches...
		[batch 4/4] avg loss: -4.4653830476340455e-05		[learning rate: 0.00025606]
	Learning Rate: 0.000256055
	LOSS [training: -4.4653830476340455e-05 | validation: -0.0009520999130887207]
	TIME [epoch: 42.1 sec]
EPOCH 290/500:
	Training over batches...
		[batch 4/4] avg loss: -7.728868149543655e-05		[learning rate: 0.00025215]
	Learning Rate: 0.000252154
	LOSS [training: -7.728868149543655e-05 | validation: -0.0007140355462593462]
	TIME [epoch: 42 sec]
EPOCH 291/500:
	Training over batches...
		[batch 4/4] avg loss: 2.088824175110181e-05		[learning rate: 0.00024831]
	Learning Rate: 0.000248313
	LOSS [training: 2.088824175110181e-05 | validation: 0.0005559803316363219]
	TIME [epoch: 42 sec]
EPOCH 292/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001290373082543135		[learning rate: 0.00024453]
	Learning Rate: 0.000244531
	LOSS [training: 0.0001290373082543135 | validation: 0.0006624756696218595]
	TIME [epoch: 42 sec]
EPOCH 293/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00033490001393275735		[learning rate: 0.00024081]
	Learning Rate: 0.000240806
	LOSS [training: 0.00033490001393275735 | validation: -6.996983489709587e-05]
	TIME [epoch: 42 sec]
EPOCH 294/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00042414233825732397		[learning rate: 0.00023714]
	Learning Rate: 0.000237137
	LOSS [training: 0.00042414233825732397 | validation: 4.918931654136218e-05]
	TIME [epoch: 42 sec]
EPOCH 295/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001836149036184902		[learning rate: 0.00023352]
	Learning Rate: 0.000233525
	LOSS [training: 0.0001836149036184902 | validation: -0.00025770980230656586]
	TIME [epoch: 42 sec]
EPOCH 296/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7096431175587466e-05		[learning rate: 0.00022997]
	Learning Rate: 0.000229968
	LOSS [training: 1.7096431175587466e-05 | validation: 0.0007855307764592276]
	TIME [epoch: 42 sec]
EPOCH 297/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004030449552429611		[learning rate: 0.00022646]
	Learning Rate: 0.000226464
	LOSS [training: 0.0004030449552429611 | validation: 0.0001503871007351516]
	TIME [epoch: 42 sec]
EPOCH 298/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00014404231385069255		[learning rate: 0.00022301]
	Learning Rate: 0.000223015
	LOSS [training: 0.00014404231385069255 | validation: -0.000424519338618961]
	TIME [epoch: 42 sec]
EPOCH 299/500:
	Training over batches...
		[batch 4/4] avg loss: -5.5778744634589416e-05		[learning rate: 0.00021962]
	Learning Rate: 0.000219617
	LOSS [training: -5.5778744634589416e-05 | validation: -5.382715134718019e-05]
	TIME [epoch: 42 sec]
EPOCH 300/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00047567876386114507		[learning rate: 0.00021627]
	Learning Rate: 0.000216272
	LOSS [training: 0.00047567876386114507 | validation: -0.0004336735773137139]
	TIME [epoch: 42 sec]
EPOCH 301/500:
	Training over batches...
		[batch 4/4] avg loss: 9.001913623494583e-05		[learning rate: 0.00021298]
	Learning Rate: 0.000212977
	LOSS [training: 9.001913623494583e-05 | validation: -0.00034078342423197317]
	TIME [epoch: 42 sec]
EPOCH 302/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1998358390168155e-05		[learning rate: 0.00020973]
	Learning Rate: 0.000209733
	LOSS [training: 1.1998358390168155e-05 | validation: 0.00036142712278856636]
	TIME [epoch: 42 sec]
EPOCH 303/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001118540645519581		[learning rate: 0.00020654]
	Learning Rate: 0.000206538
	LOSS [training: 0.0001118540645519581 | validation: 0.00022852808160047112]
	TIME [epoch: 42 sec]
EPOCH 304/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0001258567602271501		[learning rate: 0.00020339]
	Learning Rate: 0.000203392
	LOSS [training: -0.0001258567602271501 | validation: 0.0001395059137484642]
	TIME [epoch: 42.1 sec]
EPOCH 305/500:
	Training over batches...
		[batch 4/4] avg loss: 3.9330576181129587e-05		[learning rate: 0.00020029]
	Learning Rate: 0.000200293
	LOSS [training: 3.9330576181129587e-05 | validation: -0.00048232092249852565]
	TIME [epoch: 42 sec]
EPOCH 306/500:
	Training over batches...
		[batch 4/4] avg loss: 2.925823392745834e-06		[learning rate: 0.00019724]
	Learning Rate: 0.000197242
	LOSS [training: 2.925823392745834e-06 | validation: 0.00017624118967164026]
	TIME [epoch: 42.1 sec]
EPOCH 307/500:
	Training over batches...
		[batch 4/4] avg loss: -3.095150257482549e-05		[learning rate: 0.00019424]
	Learning Rate: 0.000194238
	LOSS [training: -3.095150257482549e-05 | validation: 0.000522731804648902]
	TIME [epoch: 42 sec]
EPOCH 308/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004332881513918772		[learning rate: 0.00019128]
	Learning Rate: 0.000191279
	LOSS [training: 0.0004332881513918772 | validation: -0.000674934150203546]
	TIME [epoch: 42 sec]
EPOCH 309/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011584676772900937		[learning rate: 0.00018836]
	Learning Rate: 0.000188365
	LOSS [training: 0.00011584676772900937 | validation: -0.00010040986541729733]
	TIME [epoch: 42 sec]
EPOCH 310/500:
	Training over batches...
		[batch 4/4] avg loss: -8.13685733823153e-05		[learning rate: 0.0001855]
	Learning Rate: 0.000185495
	LOSS [training: -8.13685733823153e-05 | validation: -0.00040441114307772217]
	TIME [epoch: 42 sec]
EPOCH 311/500:
	Training over batches...
		[batch 4/4] avg loss: -2.6923829360719898e-05		[learning rate: 0.00018267]
	Learning Rate: 0.00018267
	LOSS [training: -2.6923829360719898e-05 | validation: 0.0005836386923800702]
	TIME [epoch: 42 sec]
EPOCH 312/500:
	Training over batches...
		[batch 4/4] avg loss: 6.525131427253814e-05		[learning rate: 0.00017989]
	Learning Rate: 0.000179887
	LOSS [training: 6.525131427253814e-05 | validation: 0.00010747468839009721]
	TIME [epoch: 42 sec]
EPOCH 313/500:
	Training over batches...
		[batch 4/4] avg loss: -7.464545512711983e-05		[learning rate: 0.00017715]
	Learning Rate: 0.000177147
	LOSS [training: -7.464545512711983e-05 | validation: -0.0007093545054942538]
	TIME [epoch: 42 sec]
EPOCH 314/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003284497175999999		[learning rate: 0.00017445]
	Learning Rate: 0.000174448
	LOSS [training: -0.0003284497175999999 | validation: -0.0012860428324214315]
	TIME [epoch: 42 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00043776684585843193		[learning rate: 0.00017179]
	Learning Rate: 0.000171791
	LOSS [training: 0.00043776684585843193 | validation: -0.0004870716793620282]
	TIME [epoch: 42 sec]
EPOCH 316/500:
	Training over batches...
		[batch 4/4] avg loss: -2.4950221719649557e-05		[learning rate: 0.00016917]
	Learning Rate: 0.000169174
	LOSS [training: -2.4950221719649557e-05 | validation: 0.00043716722420556223]
	TIME [epoch: 42 sec]
EPOCH 317/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4443144258704505e-05		[learning rate: 0.0001666]
	Learning Rate: 0.000166597
	LOSS [training: 3.4443144258704505e-05 | validation: 0.00014061876771612124]
	TIME [epoch: 42 sec]
EPOCH 318/500:
	Training over batches...
		[batch 4/4] avg loss: 2.65908829556903e-06		[learning rate: 0.00016406]
	Learning Rate: 0.000164059
	LOSS [training: 2.65908829556903e-06 | validation: 0.0010748902490493944]
	TIME [epoch: 42 sec]
EPOCH 319/500:
	Training over batches...
		[batch 4/4] avg loss: 2.214462924707572e-05		[learning rate: 0.00016156]
	Learning Rate: 0.00016156
	LOSS [training: 2.214462924707572e-05 | validation: 0.0005760744078942146]
	TIME [epoch: 42 sec]
EPOCH 320/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00030902021549464754		[learning rate: 0.0001591]
	Learning Rate: 0.000159099
	LOSS [training: -0.00030902021549464754 | validation: 0.00025945682109220234]
	TIME [epoch: 42 sec]
EPOCH 321/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002390067178569002		[learning rate: 0.00015668]
	Learning Rate: 0.000156675
	LOSS [training: 0.0002390067178569002 | validation: 0.00018702486071861115]
	TIME [epoch: 42 sec]
EPOCH 322/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003413834099148969		[learning rate: 0.00015429]
	Learning Rate: 0.000154288
	LOSS [training: 0.0003413834099148969 | validation: -3.346820005081951e-05]
	TIME [epoch: 42 sec]
EPOCH 323/500:
	Training over batches...
		[batch 4/4] avg loss: 6.2339753901081795e-06		[learning rate: 0.00015194]
	Learning Rate: 0.000151938
	LOSS [training: 6.2339753901081795e-06 | validation: -4.982049577688485e-05]
	TIME [epoch: 42 sec]
EPOCH 324/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00019891275013236734		[learning rate: 0.00014962]
	Learning Rate: 0.000149624
	LOSS [training: 0.00019891275013236734 | validation: 0.00016128400468246173]
	TIME [epoch: 42.1 sec]
EPOCH 325/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00014484752008346558		[learning rate: 0.00014734]
	Learning Rate: 0.000147344
	LOSS [training: 0.00014484752008346558 | validation: -0.0005063450328122463]
	TIME [epoch: 42.1 sec]
EPOCH 326/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004120273639376164		[learning rate: 0.0001451]
	Learning Rate: 0.0001451
	LOSS [training: 0.0004120273639376164 | validation: -0.00022823508680934747]
	TIME [epoch: 42.1 sec]
EPOCH 327/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00010383583472349223		[learning rate: 0.00014289]
	Learning Rate: 0.000142889
	LOSS [training: 0.00010383583472349223 | validation: -0.0004217891005755638]
	TIME [epoch: 42 sec]
EPOCH 328/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00048416098156108455		[learning rate: 0.00014071]
	Learning Rate: 0.000140713
	LOSS [training: -0.00048416098156108455 | validation: -3.994859929328507e-05]
	TIME [epoch: 42.1 sec]
EPOCH 329/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001709407675292125		[learning rate: 0.00013857]
	Learning Rate: 0.000138569
	LOSS [training: 0.0001709407675292125 | validation: -2.3978475630191726e-05]
	TIME [epoch: 42 sec]
EPOCH 330/500:
	Training over batches...
		[batch 4/4] avg loss: 4.412585633997517e-05		[learning rate: 0.00013646]
	Learning Rate: 0.000136458
	LOSS [training: 4.412585633997517e-05 | validation: 6.166598124113465e-05]
	TIME [epoch: 42 sec]
EPOCH 331/500:
	Training over batches...
		[batch 4/4] avg loss: 2.3106964969690673e-05		[learning rate: 0.00013438]
	Learning Rate: 0.00013438
	LOSS [training: 2.3106964969690673e-05 | validation: 0.0001023956837293034]
	TIME [epoch: 42 sec]
EPOCH 332/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003299824716533835		[learning rate: 0.00013233]
	Learning Rate: 0.000132333
	LOSS [training: 0.0003299824716533835 | validation: -0.00024079089374101013]
	TIME [epoch: 42 sec]
EPOCH 333/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003077710170712875		[learning rate: 0.00013032]
	Learning Rate: 0.000130317
	LOSS [training: 0.0003077710170712875 | validation: -0.00040284447600782205]
	TIME [epoch: 42 sec]
EPOCH 334/500:
	Training over batches...
		[batch 4/4] avg loss: 8.935433155480977e-05		[learning rate: 0.00012833]
	Learning Rate: 0.000128332
	LOSS [training: 8.935433155480977e-05 | validation: 9.160472451142533e-05]
	TIME [epoch: 42.1 sec]
EPOCH 335/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00021361679789764132		[learning rate: 0.00012638]
	Learning Rate: 0.000126377
	LOSS [training: 0.00021361679789764132 | validation: -0.00012399748220901023]
	TIME [epoch: 42.1 sec]
EPOCH 336/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001132543107782258		[learning rate: 0.00012445]
	Learning Rate: 0.000124451
	LOSS [training: 0.0001132543107782258 | validation: 0.00032194601910421873]
	TIME [epoch: 42.1 sec]
EPOCH 337/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00014749923302285445		[learning rate: 0.00012256]
	Learning Rate: 0.000122556
	LOSS [training: 0.00014749923302285445 | validation: 1.6915557094690416e-05]
	TIME [epoch: 42.1 sec]
EPOCH 338/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00013837870016380395		[learning rate: 0.00012069]
	Learning Rate: 0.000120689
	LOSS [training: 0.00013837870016380395 | validation: -0.000356743168073356]
	TIME [epoch: 42.1 sec]
EPOCH 339/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00016933849829580994		[learning rate: 0.00011885]
	Learning Rate: 0.00011885
	LOSS [training: -0.00016933849829580994 | validation: -0.0009612980425252063]
	TIME [epoch: 42.1 sec]
EPOCH 340/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00011710513692990143		[learning rate: 0.00011704]
	Learning Rate: 0.00011704
	LOSS [training: -0.00011710513692990143 | validation: 0.00011019892092524851]
	TIME [epoch: 42.1 sec]
EPOCH 341/500:
	Training over batches...
		[batch 4/4] avg loss: -2.597493246688431e-05		[learning rate: 0.00011526]
	Learning Rate: 0.000115257
	LOSS [training: -2.597493246688431e-05 | validation: -2.4707054474267144e-05]
	TIME [epoch: 42.1 sec]
EPOCH 342/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003143537316357057		[learning rate: 0.0001135]
	Learning Rate: 0.000113501
	LOSS [training: 0.0003143537316357057 | validation: 0.000560138584121907]
	TIME [epoch: 42 sec]
EPOCH 343/500:
	Training over batches...
		[batch 4/4] avg loss: -5.818105137577768e-05		[learning rate: 0.00011177]
	Learning Rate: 0.000111772
	LOSS [training: -5.818105137577768e-05 | validation: 0.0005894766525465572]
	TIME [epoch: 42 sec]
EPOCH 344/500:
	Training over batches...
		[batch 4/4] avg loss: 4.6906583490699227e-05		[learning rate: 0.00011007]
	Learning Rate: 0.000110069
	LOSS [training: 4.6906583490699227e-05 | validation: -0.00039230471620327914]
	TIME [epoch: 42.1 sec]
EPOCH 345/500:
	Training over batches...
		[batch 4/4] avg loss: -4.980805793975286e-05		[learning rate: 0.00010839]
	Learning Rate: 0.000108393
	LOSS [training: -4.980805793975286e-05 | validation: -0.0005174860904149012]
	TIME [epoch: 42.1 sec]
EPOCH 346/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00015036009224230894		[learning rate: 0.00010674]
	Learning Rate: 0.000106742
	LOSS [training: 0.00015036009224230894 | validation: 0.0009780675684264426]
	TIME [epoch: 42 sec]
EPOCH 347/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00044505493067629684		[learning rate: 0.00010512]
	Learning Rate: 0.000105115
	LOSS [training: 0.00044505493067629684 | validation: -0.0008209336057563887]
	TIME [epoch: 42.1 sec]
EPOCH 348/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004435861145078284		[learning rate: 0.00010351]
	Learning Rate: 0.000103514
	LOSS [training: 0.0004435861145078284 | validation: -0.00022145117984506826]
	TIME [epoch: 42.1 sec]
EPOCH 349/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00016575397649408298		[learning rate: 0.00010194]
	Learning Rate: 0.000101937
	LOSS [training: 0.00016575397649408298 | validation: -0.00030091913452010347]
	TIME [epoch: 42 sec]
EPOCH 350/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7552232604403075e-05		[learning rate: 0.00010038]
	Learning Rate: 0.000100385
	LOSS [training: 1.7552232604403075e-05 | validation: 0.0006067840027889995]
	TIME [epoch: 42.1 sec]
EPOCH 351/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00043179290274436035		[learning rate: 9.8855e-05]
	Learning Rate: 9.88553e-05
	LOSS [training: 0.00043179290274436035 | validation: -0.00034867282549524644]
	TIME [epoch: 42 sec]
EPOCH 352/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000205594223289993		[learning rate: 9.7349e-05]
	Learning Rate: 9.73494e-05
	LOSS [training: 0.000205594223289993 | validation: -0.00028214687690059884]
	TIME [epoch: 42.1 sec]
EPOCH 353/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000222781237478378		[learning rate: 9.5866e-05]
	Learning Rate: 9.58665e-05
	LOSS [training: 0.000222781237478378 | validation: -0.0003586683972824414]
	TIME [epoch: 42.1 sec]
EPOCH 354/500:
	Training over batches...
		[batch 4/4] avg loss: -9.683968181120472e-05		[learning rate: 9.4406e-05]
	Learning Rate: 9.44061e-05
	LOSS [training: -9.683968181120472e-05 | validation: -0.00011991581468618541]
	TIME [epoch: 42 sec]
EPOCH 355/500:
	Training over batches...
		[batch 4/4] avg loss: -1.059926711028214e-05		[learning rate: 9.2968e-05]
	Learning Rate: 9.2968e-05
	LOSS [training: -1.059926711028214e-05 | validation: 0.0009147213076299616]
	TIME [epoch: 42.1 sec]
EPOCH 356/500:
	Training over batches...
		[batch 4/4] avg loss: -1.0788510686217346e-05		[learning rate: 9.1552e-05]
	Learning Rate: 9.15518e-05
	LOSS [training: -1.0788510686217346e-05 | validation: 0.0009124768775833521]
	TIME [epoch: 42.1 sec]
EPOCH 357/500:
	Training over batches...
		[batch 4/4] avg loss: -2.5614442948651115e-05		[learning rate: 9.0157e-05]
	Learning Rate: 9.01571e-05
	LOSS [training: -2.5614442948651115e-05 | validation: 0.00036512095212251736]
	TIME [epoch: 42.1 sec]
EPOCH 358/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00020284645298336004		[learning rate: 8.8784e-05]
	Learning Rate: 8.87837e-05
	LOSS [training: 0.00020284645298336004 | validation: -0.0005538325884559501]
	TIME [epoch: 42.1 sec]
EPOCH 359/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002823529862820233		[learning rate: 8.7431e-05]
	Learning Rate: 8.74312e-05
	LOSS [training: 0.0002823529862820233 | validation: -1.5880572597902514e-05]
	TIME [epoch: 42.1 sec]
EPOCH 360/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001064738056155492		[learning rate: 8.6099e-05]
	Learning Rate: 8.60994e-05
	LOSS [training: 0.0001064738056155492 | validation: -0.0005830971308224155]
	TIME [epoch: 42.1 sec]
EPOCH 361/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00012929373870684092		[learning rate: 8.4788e-05]
	Learning Rate: 8.47878e-05
	LOSS [training: -0.00012929373870684092 | validation: -0.00015928713183588082]
	TIME [epoch: 42.2 sec]
EPOCH 362/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003951370578881115		[learning rate: 8.3496e-05]
	Learning Rate: 8.34962e-05
	LOSS [training: 0.0003951370578881115 | validation: -6.389072903792892e-05]
	TIME [epoch: 42.3 sec]
EPOCH 363/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004123520171842619		[learning rate: 8.2224e-05]
	Learning Rate: 8.22243e-05
	LOSS [training: 0.0004123520171842619 | validation: 0.0007423228788075469]
	TIME [epoch: 42.7 sec]
EPOCH 364/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002606939674328126		[learning rate: 8.0972e-05]
	Learning Rate: 8.09717e-05
	LOSS [training: 0.0002606939674328126 | validation: 0.0006938723380447352]
	TIME [epoch: 42.7 sec]
EPOCH 365/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00011251622272866291		[learning rate: 7.9738e-05]
	Learning Rate: 7.97382e-05
	LOSS [training: -0.00011251622272866291 | validation: -0.0008410380918145327]
	TIME [epoch: 42.6 sec]
EPOCH 366/500:
	Training over batches...
		[batch 4/4] avg loss: -6.739068883966981e-05		[learning rate: 7.8524e-05]
	Learning Rate: 7.85235e-05
	LOSS [training: -6.739068883966981e-05 | validation: 0.0004206088294523536]
	TIME [epoch: 42.6 sec]
EPOCH 367/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00023991899744245024		[learning rate: 7.7327e-05]
	Learning Rate: 7.73274e-05
	LOSS [training: 0.00023991899744245024 | validation: 0.00018178833007719366]
	TIME [epoch: 42.6 sec]
EPOCH 368/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00014718609146016062		[learning rate: 7.6149e-05]
	Learning Rate: 7.61494e-05
	LOSS [training: 0.00014718609146016062 | validation: -0.0003412055749498251]
	TIME [epoch: 42.7 sec]
EPOCH 369/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00020981211861799865		[learning rate: 7.4989e-05]
	Learning Rate: 7.49894e-05
	LOSS [training: 0.00020981211861799865 | validation: 0.0006449611206885942]
	TIME [epoch: 42.6 sec]
EPOCH 370/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005357265912830202		[learning rate: 7.3847e-05]
	Learning Rate: 7.38471e-05
	LOSS [training: 0.0005357265912830202 | validation: -0.00040159381245924664]
	TIME [epoch: 42.7 sec]
EPOCH 371/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00018617635963067092		[learning rate: 7.2722e-05]
	Learning Rate: 7.27221e-05
	LOSS [training: 0.00018617635963067092 | validation: 0.0005021035502090604]
	TIME [epoch: 42.7 sec]
EPOCH 372/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003709362390837336		[learning rate: 7.1614e-05]
	Learning Rate: 7.16143e-05
	LOSS [training: 0.0003709362390837336 | validation: -8.865253033541532e-05]
	TIME [epoch: 42.7 sec]
EPOCH 373/500:
	Training over batches...
		[batch 4/4] avg loss: 8.793638534043336e-05		[learning rate: 7.0523e-05]
	Learning Rate: 7.05234e-05
	LOSS [training: 8.793638534043336e-05 | validation: -0.00037594792268726395]
	TIME [epoch: 42.5 sec]
EPOCH 374/500:
	Training over batches...
		[batch 4/4] avg loss: 5.891674502026766e-05		[learning rate: 6.9449e-05]
	Learning Rate: 6.94491e-05
	LOSS [training: 5.891674502026766e-05 | validation: -0.000994379703779034]
	TIME [epoch: 42.5 sec]
EPOCH 375/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004783572567016534		[learning rate: 6.8391e-05]
	Learning Rate: 6.83912e-05
	LOSS [training: 0.0004783572567016534 | validation: -0.0005334592484818902]
	TIME [epoch: 42.5 sec]
EPOCH 376/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00041913329637800767		[learning rate: 6.7349e-05]
	Learning Rate: 6.73493e-05
	LOSS [training: 0.00041913329637800767 | validation: -6.756700746687999e-06]
	TIME [epoch: 42.5 sec]
EPOCH 377/500:
	Training over batches...
		[batch 4/4] avg loss: 4.846133036612877e-05		[learning rate: 6.6323e-05]
	Learning Rate: 6.63234e-05
	LOSS [training: 4.846133036612877e-05 | validation: -1.6029237988965806e-06]
	TIME [epoch: 42.5 sec]
EPOCH 378/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003444540430812562		[learning rate: 6.5313e-05]
	Learning Rate: 6.5313e-05
	LOSS [training: -0.0003444540430812562 | validation: 3.3928414240237616e-05]
	TIME [epoch: 42.7 sec]
EPOCH 379/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00022327299240183126		[learning rate: 6.4318e-05]
	Learning Rate: 6.43181e-05
	LOSS [training: 0.00022327299240183126 | validation: 0.00023861660588068645]
	TIME [epoch: 42.6 sec]
EPOCH 380/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00011996765980234203		[learning rate: 6.3338e-05]
	Learning Rate: 6.33383e-05
	LOSS [training: 0.00011996765980234203 | validation: 0.00044003885195394334]
	TIME [epoch: 42.5 sec]
EPOCH 381/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00016683813997398888		[learning rate: 6.2373e-05]
	Learning Rate: 6.23735e-05
	LOSS [training: -0.00016683813997398888 | validation: 0.00012080689491469921]
	TIME [epoch: 42.5 sec]
EPOCH 382/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5128698521242877e-05		[learning rate: 6.1423e-05]
	Learning Rate: 6.14233e-05
	LOSS [training: 1.5128698521242877e-05 | validation: 0.0005732281489437989]
	TIME [epoch: 42.5 sec]
EPOCH 383/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00019165700063988278		[learning rate: 6.0488e-05]
	Learning Rate: 6.04876e-05
	LOSS [training: 0.00019165700063988278 | validation: -7.216602842618336e-05]
	TIME [epoch: 42.6 sec]
EPOCH 384/500:
	Training over batches...
		[batch 4/4] avg loss: -7.117435162125196e-05		[learning rate: 5.9566e-05]
	Learning Rate: 5.95662e-05
	LOSS [training: -7.117435162125196e-05 | validation: 6.590676958445041e-05]
	TIME [epoch: 42.6 sec]
EPOCH 385/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004119107338260328		[learning rate: 5.8659e-05]
	Learning Rate: 5.86588e-05
	LOSS [training: 0.0004119107338260328 | validation: 1.0050540100671869e-05]
	TIME [epoch: 42.5 sec]
EPOCH 386/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00026324647445541465		[learning rate: 5.7765e-05]
	Learning Rate: 5.77652e-05
	LOSS [training: -0.00026324647445541465 | validation: -0.0005459143393907513]
	TIME [epoch: 42.5 sec]
EPOCH 387/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005696521844504749		[learning rate: 5.6885e-05]
	Learning Rate: 5.68853e-05
	LOSS [training: 0.0005696521844504749 | validation: -0.0005331692605686201]
	TIME [epoch: 42.5 sec]
EPOCH 388/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003310575259034863		[learning rate: 5.6019e-05]
	Learning Rate: 5.60187e-05
	LOSS [training: 0.0003310575259034863 | validation: 0.0003348174822537811]
	TIME [epoch: 42.5 sec]
EPOCH 389/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00012197018378428526		[learning rate: 5.5165e-05]
	Learning Rate: 5.51654e-05
	LOSS [training: 0.00012197018378428526 | validation: -0.0003309029337892486]
	TIME [epoch: 42.5 sec]
EPOCH 390/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006111143550177456		[learning rate: 5.4325e-05]
	Learning Rate: 5.4325e-05
	LOSS [training: 0.0006111143550177456 | validation: 1.5071829824691023e-05]
	TIME [epoch: 42.6 sec]
EPOCH 391/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0003747141281091959		[learning rate: 5.3497e-05]
	Learning Rate: 5.34975e-05
	LOSS [training: -0.0003747141281091959 | validation: 6.599473416966627e-05]
	TIME [epoch: 42.5 sec]
EPOCH 392/500:
	Training over batches...
		[batch 4/4] avg loss: -9.523259904876903e-05		[learning rate: 5.2683e-05]
	Learning Rate: 5.26825e-05
	LOSS [training: -9.523259904876903e-05 | validation: 0.00014742566359086685]
	TIME [epoch: 42.5 sec]
EPOCH 393/500:
	Training over batches...
		[batch 4/4] avg loss: -7.510778477183955e-05		[learning rate: 5.188e-05]
	Learning Rate: 5.188e-05
	LOSS [training: -7.510778477183955e-05 | validation: 0.0008324048886646103]
	TIME [epoch: 42.5 sec]
EPOCH 394/500:
	Training over batches...
		[batch 4/4] avg loss: -5.742578356471963e-05		[learning rate: 5.109e-05]
	Learning Rate: 5.10897e-05
	LOSS [training: -5.742578356471963e-05 | validation: 0.0005760090122293513]
	TIME [epoch: 42.5 sec]
EPOCH 395/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001798669593380704		[learning rate: 5.0311e-05]
	Learning Rate: 5.03114e-05
	LOSS [training: 0.0001798669593380704 | validation: -2.4104255614949995e-05]
	TIME [epoch: 42.5 sec]
EPOCH 396/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00017825924093606925		[learning rate: 4.9545e-05]
	Learning Rate: 4.9545e-05
	LOSS [training: -0.00017825924093606925 | validation: -3.8665798259957637e-05]
	TIME [epoch: 42.5 sec]
EPOCH 397/500:
	Training over batches...
		[batch 4/4] avg loss: -6.636525156699834e-05		[learning rate: 4.879e-05]
	Learning Rate: 4.87903e-05
	LOSS [training: -6.636525156699834e-05 | validation: -0.0003659521094409644]
	TIME [epoch: 42.5 sec]
EPOCH 398/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001790608392140012		[learning rate: 4.8047e-05]
	Learning Rate: 4.8047e-05
	LOSS [training: 0.0001790608392140012 | validation: 3.8344197312018664e-05]
	TIME [epoch: 42.5 sec]
EPOCH 399/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0002751865483886591		[learning rate: 4.7315e-05]
	Learning Rate: 4.73151e-05
	LOSS [training: 0.0002751865483886591 | validation: 0.0004990003450200824]
	TIME [epoch: 42.5 sec]
EPOCH 400/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00018419170689269396		[learning rate: 4.6594e-05]
	Learning Rate: 4.65944e-05
	LOSS [training: 0.00018419170689269396 | validation: -0.00018733707318006364]
	TIME [epoch: 42.5 sec]
EPOCH 401/500:
	Training over batches...
		[batch 4/4] avg loss: 8.812335891316892e-05		[learning rate: 4.5885e-05]
	Learning Rate: 4.58846e-05
	LOSS [training: 8.812335891316892e-05 | validation: -0.00013998137563741865]
	TIME [epoch: 42.5 sec]
EPOCH 402/500:
	Training over batches...
		[batch 4/4] avg loss: -0.0002678230751795951		[learning rate: 4.5186e-05]
	Learning Rate: 4.51856e-05
	LOSS [training: -0.0002678230751795951 | validation: 0.0009265707002766646]
	TIME [epoch: 42.5 sec]
EPOCH 403/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00037174815509989796		[learning rate: 4.4497e-05]
	Learning Rate: 4.44973e-05
	LOSS [training: 0.00037174815509989796 | validation: -4.0354429401234364e-05]
	TIME [epoch: 42.5 sec]
EPOCH 404/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00027581987341215993		[learning rate: 4.3819e-05]
	Learning Rate: 4.38194e-05
	LOSS [training: 0.00027581987341215993 | validation: 0.0007944836843953511]
	TIME [epoch: 42.5 sec]
EPOCH 405/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00023799730319656455		[learning rate: 4.3152e-05]
	Learning Rate: 4.31519e-05
	LOSS [training: 0.00023799730319656455 | validation: -0.0005209112109880803]
	TIME [epoch: 42.5 sec]
EPOCH 406/500:
	Training over batches...
		[batch 4/4] avg loss: -0.00016027738119233324		[learning rate: 4.2495e-05]
	Learning Rate: 4.24946e-05
	LOSS [training: -0.00016027738119233324 | validation: -0.0004083275178059882]
	TIME [epoch: 42.5 sec]
EPOCH 407/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003282398330707883		[learning rate: 4.1847e-05]
	Learning Rate: 4.18472e-05
	LOSS [training: 0.0003282398330707883 | validation: -0.0004614000576397337]
	TIME [epoch: 42.5 sec]
EPOCH 408/500:
	Training over batches...
		[batch 4/4] avg loss: -7.862968351271538e-05		[learning rate: 4.121e-05]
	Learning Rate: 4.12098e-05
	LOSS [training: -7.862968351271538e-05 | validation: -0.00012460593751189908]
	TIME [epoch: 42.5 sec]
EPOCH 409/500:
	Training over batches...
		[batch 4/4] avg loss: 2.333040602302506e-05		[learning rate: 4.0582e-05]
	Learning Rate: 4.0582e-05
	LOSS [training: 2.333040602302506e-05 | validation: -0.000852997590152548]
	TIME [epoch: 42.4 sec]
EPOCH 410/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00012143895710823461		[learning rate: 3.9964e-05]
	Learning Rate: 3.99638e-05
	LOSS [training: 0.00012143895710823461 | validation: 4.140188164386061e-05]
	TIME [epoch: 42.5 sec]
EPOCH 411/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0003380340929767298		[learning rate: 3.9355e-05]
	Learning Rate: 3.9355e-05
	LOSS [training: 0.0003380340929767298 | validation: -0.0005243880468020498]
	TIME [epoch: 42.4 sec]
EPOCH 412/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000267001664313429		[learning rate: 3.8755e-05]
	Learning Rate: 3.87555e-05
	LOSS [training: 0.000267001664313429 | validation: -0.0006377356350980642]
	TIME [epoch: 42.5 sec]
EPOCH 413/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0004087368800586739		[learning rate: 3.8165e-05]
	Learning Rate: 3.81651e-05
	LOSS [training: 0.0004087368800586739 | validation: 0.0001848526345454484]
	TIME [epoch: 42.5 sec]
EPOCH 414/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00022383199616692173		[learning rate: 3.7584e-05]
	Learning Rate: 3.75837e-05
	LOSS [training: 0.00022383199616692173 | validation: -0.00045251650539535597]
	TIME [epoch: 42.4 sec]
EPOCH 415/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0001871296898706496		[learning rate: 3.7011e-05]
	Learning Rate: 3.70112e-05
	LOSS [training: 0.0001871296898706496 | validation: -3.2326226885821086e-05]
	TIME [epoch: 42.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20240710_231006/states/model_algphiq_1a_v_mmd1_415.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 10862.152 seconds.
