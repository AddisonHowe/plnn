Args:
Namespace(name='model_algphi2_1a_v_klv2', outdir='out/model_training/model_algphi2_1a_v_klv2', training_data='data/training_data/data_phi2_1a/training', validation_data='data/training_data/data_phi2_1a/validation', model_type='binary_flip', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='klv2', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3859842266

Training model...

Saving initial model state to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 4.714979687026978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.714979687026978 | validation: 4.607735406743618]
	TIME [epoch: 94.9 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 4.234388597647199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.234388597647199 | validation: 4.122110464226546]
	TIME [epoch: 4.35 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_2.pth
	Model improved!!!
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 3.829942436697678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.829942436697678 | validation: 3.6494706611517556]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_3.pth
	Model improved!!!
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 3.4540270464906015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4540270464906015 | validation: 3.263428839064991]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_4.pth
	Model improved!!!
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 3.0949365461148575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0949365461148575 | validation: 2.9313022129125414]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_5.pth
	Model improved!!!
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 2.7766018991830808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7766018991830808 | validation: 2.6753113213974284]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_6.pth
	Model improved!!!
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 2.474485768798729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.474485768798729 | validation: 2.414872535261427]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_7.pth
	Model improved!!!
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 2.2882384152301065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2882384152301065 | validation: 2.0946915276063716]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_8.pth
	Model improved!!!
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 2.0575514754862616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0575514754862616 | validation: 1.910190256277748]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_9.pth
	Model improved!!!
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 1.8849387305546472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8849387305546472 | validation: 1.715947679820259]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_10.pth
	Model improved!!!
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6795851103777633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6795851103777633 | validation: 1.5280790282771148]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_11.pth
	Model improved!!!
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4880089647071124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4880089647071124 | validation: 1.4133264204928553]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_12.pth
	Model improved!!!
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3724799919239428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3724799919239428 | validation: 1.262764121963543]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_13.pth
	Model improved!!!
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1585576861240607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1585576861240607 | validation: 1.1378889471101754]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_14.pth
	Model improved!!!
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0884049304791519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0884049304791519 | validation: 0.993639134631795]
	TIME [epoch: 4.33 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_15.pth
	Model improved!!!
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8762832209307143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8762832209307143 | validation: 0.7414985723903282]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_16.pth
	Model improved!!!
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7479523345616179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7479523345616179 | validation: 0.6558895408481993]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_17.pth
	Model improved!!!
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6009907178021534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6009907178021534 | validation: 0.49301302681192716]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_18.pth
	Model improved!!!
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 0.48993283150016964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48993283150016964 | validation: 0.4034701365323164]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_19.pth
	Model improved!!!
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4074542566018736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4074542566018736 | validation: 0.33726673808290564]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_20.pth
	Model improved!!!
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3286577454188916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3286577454188916 | validation: 0.2632959030508159]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_21.pth
	Model improved!!!
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 0.28395511752364017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28395511752364017 | validation: 0.25828886532202255]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_22.pth
	Model improved!!!
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3006787798642267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3006787798642267 | validation: 0.3206683414624442]
	TIME [epoch: 4.3 sec]
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3046165210378776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3046165210378776 | validation: 0.26900372325756783]
	TIME [epoch: 4.33 sec]
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 0.243465133346272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.243465133346272 | validation: 0.1926832361471803]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_25.pth
	Model improved!!!
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2145837509860532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2145837509860532 | validation: 0.20332432923523708]
	TIME [epoch: 4.3 sec]
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1834304766070766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1834304766070766 | validation: 0.13022230300257104]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_27.pth
	Model improved!!!
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13715743182297785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13715743182297785 | validation: 0.1394130682766463]
	TIME [epoch: 4.29 sec]
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16132765518025938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16132765518025938 | validation: 0.17485626378693497]
	TIME [epoch: 4.29 sec]
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1639018847727671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1639018847727671 | validation: 0.1726311789884912]
	TIME [epoch: 4.29 sec]
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15828964706534568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15828964706534568 | validation: 0.15068692208382767]
	TIME [epoch: 4.29 sec]
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16966243597384228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16966243597384228 | validation: 0.16189538499452655]
	TIME [epoch: 4.29 sec]
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15546481483665175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15546481483665175 | validation: 0.14372522711753652]
	TIME [epoch: 4.29 sec]
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1488194190923571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1488194190923571 | validation: 0.13955033223211]
	TIME [epoch: 4.31 sec]
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16418618421707798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16418618421707798 | validation: 0.16178702925350813]
	TIME [epoch: 4.31 sec]
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17817438484719267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17817438484719267 | validation: 0.14297467183720924]
	TIME [epoch: 4.3 sec]
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16678370629440825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16678370629440825 | validation: 0.1370833690253892]
	TIME [epoch: 4.29 sec]
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15184027720001858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15184027720001858 | validation: 0.15262398716775621]
	TIME [epoch: 4.29 sec]
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1840295713430305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1840295713430305 | validation: 0.1502360775637665]
	TIME [epoch: 4.29 sec]
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18403190872531172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18403190872531172 | validation: 0.14537999167931287]
	TIME [epoch: 4.29 sec]
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1685448677901039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1685448677901039 | validation: 0.13474334643998598]
	TIME [epoch: 4.39 sec]
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1761093414899541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1761093414899541 | validation: 0.13450362025079995]
	TIME [epoch: 4.29 sec]
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15178495679645587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15178495679645587 | validation: 0.10710090348213266]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_43.pth
	Model improved!!!
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12638260373424642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12638260373424642 | validation: 0.12054314197044089]
	TIME [epoch: 4.29 sec]
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13911403738978406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13911403738978406 | validation: 0.12707123036213733]
	TIME [epoch: 4.32 sec]
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14107645692252885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14107645692252885 | validation: 0.13777844883263257]
	TIME [epoch: 4.31 sec]
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1331569890692778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1331569890692778 | validation: 0.14019946004967732]
	TIME [epoch: 4.3 sec]
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13540227621589573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13540227621589573 | validation: 0.1258735583985948]
	TIME [epoch: 4.29 sec]
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12817089365481138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12817089365481138 | validation: 0.12180012271303259]
	TIME [epoch: 4.29 sec]
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12798412579363713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12798412579363713 | validation: 0.11229114332575436]
	TIME [epoch: 4.3 sec]
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13375241329306223		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 0.13375241329306223 | validation: 0.13036736270068613]
	TIME [epoch: 4.29 sec]
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13012569400543786		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 0.13012569400543786 | validation: 0.12023449489780838]
	TIME [epoch: 4.29 sec]
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1371133127842097		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 0.1371133127842097 | validation: 0.1476982654294149]
	TIME [epoch: 4.29 sec]
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17613559155194633		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 0.17613559155194633 | validation: 0.18580921663913993]
	TIME [epoch: 4.29 sec]
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 0.20072850835548622		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 0.20072850835548622 | validation: 0.18871601143015249]
	TIME [epoch: 4.29 sec]
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17950116408301592		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 0.17950116408301592 | validation: 0.15398486888393256]
	TIME [epoch: 4.32 sec]
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18143499519579961		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 0.18143499519579961 | validation: 0.17778628356910786]
	TIME [epoch: 4.3 sec]
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17195694250600455		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 0.17195694250600455 | validation: 0.1474663741494913]
	TIME [epoch: 4.29 sec]
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15027190831609957		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 0.15027190831609957 | validation: 0.13112795285930962]
	TIME [epoch: 4.29 sec]
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15709684115977485		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.15709684115977485 | validation: 0.15932785625865675]
	TIME [epoch: 4.29 sec]
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16019050255435283		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 0.16019050255435283 | validation: 0.14920984433251938]
	TIME [epoch: 4.29 sec]
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14930525822130236		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 0.14930525822130236 | validation: 0.15627012460385947]
	TIME [epoch: 4.29 sec]
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15602083216047033		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 0.15602083216047033 | validation: 0.1362022731942804]
	TIME [epoch: 4.29 sec]
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14632535518610895		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 0.14632535518610895 | validation: 0.12542239793175247]
	TIME [epoch: 4.29 sec]
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14164310743402608		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 0.14164310743402608 | validation: 0.1287481292732161]
	TIME [epoch: 4.29 sec]
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1386832796190413		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 0.1386832796190413 | validation: 0.13363922707711412]
	TIME [epoch: 4.3 sec]
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14459032395838528		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 0.14459032395838528 | validation: 0.13166031754856639]
	TIME [epoch: 4.32 sec]
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1475542130929893		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 0.1475542130929893 | validation: 0.14068894923307496]
	TIME [epoch: 4.29 sec]
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15161321303110453		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 0.15161321303110453 | validation: 0.1413479578586842]
	TIME [epoch: 4.29 sec]
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14451507048419152		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 0.14451507048419152 | validation: 0.12674670737280475]
	TIME [epoch: 4.29 sec]
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14171046896901537		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 0.14171046896901537 | validation: 0.13146475086733017]
	TIME [epoch: 4.29 sec]
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14327151925883228		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 0.14327151925883228 | validation: 0.13288230577754853]
	TIME [epoch: 4.29 sec]
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16351554303800866		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 0.16351554303800866 | validation: 0.1307043676010724]
	TIME [epoch: 4.29 sec]
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15043589393115367		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 0.15043589393115367 | validation: 0.120666807632277]
	TIME [epoch: 4.28 sec]
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13870695260261348		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.13870695260261348 | validation: 0.13706936547233134]
	TIME [epoch: 4.29 sec]
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15558985941167194		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 0.15558985941167194 | validation: 0.1379149265531173]
	TIME [epoch: 4.29 sec]
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1522527583247294		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 0.1522527583247294 | validation: 0.14110889955737044]
	TIME [epoch: 4.3 sec]
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14546252573250534		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 0.14546252573250534 | validation: 0.14100009004391495]
	TIME [epoch: 4.33 sec]
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14299043360545668		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 0.14299043360545668 | validation: 0.13760652407557883]
	TIME [epoch: 4.29 sec]
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16623494545332748		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 0.16623494545332748 | validation: 0.15918113982079868]
	TIME [epoch: 4.29 sec]
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16881517626969206		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 0.16881517626969206 | validation: 0.1737927787096207]
	TIME [epoch: 4.3 sec]
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18277643364845164		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 0.18277643364845164 | validation: 0.17664518175986332]
	TIME [epoch: 4.29 sec]
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18607040762981103		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 0.18607040762981103 | validation: 0.18264846887675396]
	TIME [epoch: 4.29 sec]
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1828606693485032		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 0.1828606693485032 | validation: 0.1666346220982164]
	TIME [epoch: 4.29 sec]
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17611602762941633		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 0.17611602762941633 | validation: 0.15897022318641002]
	TIME [epoch: 4.29 sec]
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16871230974682347		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 0.16871230974682347 | validation: 0.13786629348123397]
	TIME [epoch: 4.29 sec]
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14648536484611185		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 0.14648536484611185 | validation: 0.13197877952277073]
	TIME [epoch: 4.29 sec]
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14260432372598617		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 0.14260432372598617 | validation: 0.13004089090961682]
	TIME [epoch: 4.31 sec]
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14698493720912215		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 0.14698493720912215 | validation: 0.12618584512446482]
	TIME [epoch: 4.31 sec]
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14640510500464296		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.14640510500464296 | validation: 0.14774521153725922]
	TIME [epoch: 4.29 sec]
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1485034719356701		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 0.1485034719356701 | validation: 0.14654358594180866]
	TIME [epoch: 4.29 sec]
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1427184709075218		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 0.1427184709075218 | validation: 0.14592585246464718]
	TIME [epoch: 4.29 sec]
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14310362494917678		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 0.14310362494917678 | validation: 0.14778107071220384]
	TIME [epoch: 4.29 sec]
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15296345999898087		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 0.15296345999898087 | validation: 0.15060885034522964]
	TIME [epoch: 4.29 sec]
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16372636650624278		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 0.16372636650624278 | validation: 0.1542940659468391]
	TIME [epoch: 4.29 sec]
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1569562522639809		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 0.1569562522639809 | validation: 0.1446119326540078]
	TIME [epoch: 4.29 sec]
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1617629861243366		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 0.1617629861243366 | validation: 0.1644372829264457]
	TIME [epoch: 4.29 sec]
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16970734012529137		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 0.16970734012529137 | validation: 0.15828245962415816]
	TIME [epoch: 4.29 sec]
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15727218958799297		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 0.15727218958799297 | validation: 0.15914142321562846]
	TIME [epoch: 4.31 sec]
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1667636070373419		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 0.1667636070373419 | validation: 0.16418514236103143]
	TIME [epoch: 4.31 sec]
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16685484736646516		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 0.16685484736646516 | validation: 0.1708448605684624]
	TIME [epoch: 4.3 sec]
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17018739601971278		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 0.17018739601971278 | validation: 0.17787803179842715]
	TIME [epoch: 4.29 sec]
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18487475925636726		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 0.18487475925636726 | validation: 0.17191851229454216]
	TIME [epoch: 4.29 sec]
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17305252754568018		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 0.17305252754568018 | validation: 0.15350322530078164]
	TIME [epoch: 4.29 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16748230976844952		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.16748230976844952 | validation: 0.14237664422829416]
	TIME [epoch: 4.29 sec]
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15649412099530097		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 0.15649412099530097 | validation: 0.15074424966142416]
	TIME [epoch: 4.29 sec]
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15061660398120433		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 0.15061660398120433 | validation: 0.14250559901614002]
	TIME [epoch: 4.31 sec]
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15466317166995966		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 0.15466317166995966 | validation: 0.1567114008610953]
	TIME [epoch: 4.29 sec]
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15301288059388873		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: 0.15301288059388873 | validation: 0.14530416591593126]
	TIME [epoch: 4.29 sec]
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14578787768150456		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: 0.14578787768150456 | validation: 0.14599727734957185]
	TIME [epoch: 4.31 sec]
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14674348291271166		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: 0.14674348291271166 | validation: 0.13391266228024856]
	TIME [epoch: 4.31 sec]
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14769071596854955		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: 0.14769071596854955 | validation: 0.1363452747187178]
	TIME [epoch: 4.29 sec]
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14451407429163818		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: 0.14451407429163818 | validation: 0.1311418171418982]
	TIME [epoch: 4.28 sec]
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14516671223837163		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: 0.14516671223837163 | validation: 0.14137629404147942]
	TIME [epoch: 4.29 sec]
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15075373420055527		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: 0.15075373420055527 | validation: 0.13760317334920902]
	TIME [epoch: 4.29 sec]
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14995300644716908		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: 0.14995300644716908 | validation: 0.14428435992185906]
	TIME [epoch: 4.29 sec]
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15336534401093868		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: 0.15336534401093868 | validation: 0.13760444686380652]
	TIME [epoch: 4.29 sec]
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14709353665637048		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: 0.14709353665637048 | validation: 0.12607660035860918]
	TIME [epoch: 4.28 sec]
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14339919528747194		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: 0.14339919528747194 | validation: 0.11275814592820385]
	TIME [epoch: 4.29 sec]
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13589464446481586		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.13589464446481586 | validation: 0.12780469009942813]
	TIME [epoch: 4.29 sec]
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13282985394866095		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: 0.13282985394866095 | validation: 0.1176260492408035]
	TIME [epoch: 4.32 sec]
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13044373092247336		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: 0.13044373092247336 | validation: 0.11306089413940348]
	TIME [epoch: 4.3 sec]
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: 0.127025762165555		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: 0.127025762165555 | validation: 0.11731376683483415]
	TIME [epoch: 4.29 sec]
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12463630823028193		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: 0.12463630823028193 | validation: 0.1294489913227629]
	TIME [epoch: 4.29 sec]
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13095274840238158		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: 0.13095274840238158 | validation: 0.11727928228419016]
	TIME [epoch: 4.29 sec]
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13107545838868329		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: 0.13107545838868329 | validation: 0.10997466184138432]
	TIME [epoch: 4.29 sec]
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1329101687569894		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: 0.1329101687569894 | validation: 0.11684572569678381]
	TIME [epoch: 4.29 sec]
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13992533913748303		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: 0.13992533913748303 | validation: 0.11953970143642323]
	TIME [epoch: 4.29 sec]
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14810730676494183		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: 0.14810730676494183 | validation: 0.1263044998433975]
	TIME [epoch: 4.29 sec]
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16924088258758735		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: 0.16924088258758735 | validation: 0.1394340747930595]
	TIME [epoch: 4.29 sec]
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16745243382942185		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: 0.16745243382942185 | validation: 0.1367719623183898]
	TIME [epoch: 4.3 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16925106396274608		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: 0.16925106396274608 | validation: 0.13348139805605663]
	TIME [epoch: 4.32 sec]
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1604548258044894		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: 0.1604548258044894 | validation: 0.1295620132365901]
	TIME [epoch: 4.29 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15469967143172292		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: 0.15469967143172292 | validation: 0.12676574091754905]
	TIME [epoch: 4.29 sec]
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16300980762150097		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 0.16300980762150097 | validation: 0.12862848905313362]
	TIME [epoch: 4.29 sec]
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1511593007733721		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: 0.1511593007733721 | validation: 0.115190089570363]
	TIME [epoch: 4.29 sec]
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14215957961345643		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: 0.14215957961345643 | validation: 0.10907887947606665]
	TIME [epoch: 4.29 sec]
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1339110872388765		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: 0.1339110872388765 | validation: 0.11735912440165899]
	TIME [epoch: 4.29 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13127210253874022		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: 0.13127210253874022 | validation: 0.11870026468609438]
	TIME [epoch: 4.29 sec]
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13089013908871142		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: 0.13089013908871142 | validation: 0.12133524449596525]
	TIME [epoch: 4.29 sec]
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13017970898890538		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: 0.13017970898890538 | validation: 0.12888671925990347]
	TIME [epoch: 4.29 sec]
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12906019565945662		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: 0.12906019565945662 | validation: 0.11677176997074883]
	TIME [epoch: 4.31 sec]
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12261945318761353		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: 0.12261945318761353 | validation: 0.10820577696322406]
	TIME [epoch: 4.31 sec]
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12483479771180815		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: 0.12483479771180815 | validation: 0.10699381302132085]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_klv2_20240703_152313/states/model_algphi2_1a_v_klv2_144.pth
	Model improved!!!
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 729.022 seconds.
