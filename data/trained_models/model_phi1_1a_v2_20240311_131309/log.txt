Args:
Namespace(name='model_phi1_1a_v2', outdir='out/model_training/model_phi1_1a_v2', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3049070894

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 11.259213935920167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.259213935920167 | validation: 11.450782283457004]
	TIME [epoch: 164 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 11.013565496971912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.013565496971912 | validation: 11.359856408916798]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.395402667556695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.395402667556695 | validation: 10.87219679744514]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.061414530692563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.061414530692563 | validation: 10.616706306368716]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.721274104439491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.721274104439491 | validation: 10.22426439025299]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.367736854783834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.367736854783834 | validation: 9.777442605967757]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.000385747192789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.000385747192789 | validation: 9.957945504720112]
	TIME [epoch: 6.3 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.732869000446794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.732869000446794 | validation: 9.674318749167972]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.860864910651182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.860864910651182 | validation: 9.68511147018697]
	TIME [epoch: 6.31 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.9043445592939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.9043445592939 | validation: 9.034370687862886]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.653141012287907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.653141012287907 | validation: 8.849865403614636]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.40847672950757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.40847672950757 | validation: 9.313127694262565]
	TIME [epoch: 6.31 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.014132828730949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.014132828730949 | validation: 8.9102363086932]
	TIME [epoch: 6.31 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.33744615760133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.33744615760133 | validation: 11.083564943005964]
	TIME [epoch: 6.31 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.352492923800666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.352492923800666 | validation: 10.76557847872525]
	TIME [epoch: 6.3 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.029621554339528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.029621554339528 | validation: 8.681920726467748]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.976872208317358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.976872208317358 | validation: 8.217134891100681]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.712708308151973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.712708308151973 | validation: 9.95262073863272]
	TIME [epoch: 6.3 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.780891119362712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.780891119362712 | validation: 8.770025254413282]
	TIME [epoch: 6.3 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.934521809993347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.934521809993347 | validation: 9.537915165136502]
	TIME [epoch: 6.3 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.103648876278388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.103648876278388 | validation: 9.129395619740233]
	TIME [epoch: 6.3 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.722075852843497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.722075852843497 | validation: 8.829849746739699]
	TIME [epoch: 6.34 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.007169159887612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.007169159887612 | validation: 8.477213655500613]
	TIME [epoch: 6.32 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.89711143551705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.89711143551705 | validation: 8.922358475724513]
	TIME [epoch: 6.3 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.289969997314897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.289969997314897 | validation: 8.633259834697814]
	TIME [epoch: 6.3 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.211228611333368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.211228611333368 | validation: 8.575792854735273]
	TIME [epoch: 6.3 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.807776737327568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.807776737327568 | validation: 8.76408942076728]
	TIME [epoch: 6.31 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.820642011765102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.820642011765102 | validation: 7.9178009169040475]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.015638365786854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.015638365786854 | validation: 10.498466067063667]
	TIME [epoch: 6.32 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.832225644675454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.832225644675454 | validation: 8.305234733000281]
	TIME [epoch: 6.31 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.643615247567391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.643615247567391 | validation: 7.802175695778995]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.292784476139241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.292784476139241 | validation: 7.815440149073951]
	TIME [epoch: 6.31 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.998696540873248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.998696540873248 | validation: 7.614465425407258]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.909251622381263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.909251622381263 | validation: 7.359646391247031]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.063062348548504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.063062348548504 | validation: 7.501397744269594]
	TIME [epoch: 6.31 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.818513829210163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.818513829210163 | validation: 7.941216977262022]
	TIME [epoch: 6.31 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.019994549595995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.019994549595995 | validation: 7.649110302969049]
	TIME [epoch: 6.31 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.8212958987735455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.8212958987735455 | validation: 6.868610941336005]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.481148768827834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.481148768827834 | validation: 7.310721628482866]
	TIME [epoch: 6.31 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.4713163115805505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.4713163115805505 | validation: 8.020032306153276]
	TIME [epoch: 6.33 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.965660841549221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.965660841549221 | validation: 7.162576819446054]
	TIME [epoch: 6.3 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.57186532609879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.57186532609879 | validation: 6.8045418552768435]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.467772728730419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.467772728730419 | validation: 6.907066765567565]
	TIME [epoch: 6.3 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.061835498621337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.061835498621337 | validation: 6.3346856954549775]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.835577704735218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.835577704735218 | validation: 6.333705823678586]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.306868403961148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.306868403961148 | validation: 6.929679113791057]
	TIME [epoch: 6.34 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.406722626019358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.406722626019358 | validation: 5.7113736791094]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.54188956317827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.54188956317827 | validation: 6.690656562804287]
	TIME [epoch: 6.31 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.706516156199617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.706516156199617 | validation: 5.509793253488289]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.538375438139371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.538375438139371 | validation: 6.966573534784576]
	TIME [epoch: 6.31 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.225473748462489		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 6.225473748462489 | validation: 5.167097696261543]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.5345453624861385		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 5.5345453624861385 | validation: 5.407403232497381]
	TIME [epoch: 6.35 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.117897896615224		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 5.117897896615224 | validation: 5.928006108816669]
	TIME [epoch: 6.31 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.288209628143116		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 5.288209628143116 | validation: 5.5205330408000926]
	TIME [epoch: 6.32 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.38414191196852		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 5.38414191196852 | validation: 5.080113561375319]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5237008130504535		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 4.5237008130504535 | validation: 3.9418267932873774]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.384011914132506		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 4.384011914132506 | validation: 3.7451037666964373]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.068297784458375		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 4.068297784458375 | validation: 3.7533565372887248]
	TIME [epoch: 6.37 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.13997373158217		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 4.13997373158217 | validation: 4.555670950392236]
	TIME [epoch: 6.32 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.740164456324495		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 3.740164456324495 | validation: 3.050154617655421]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3168746583863418		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 3.3168746583863418 | validation: 3.55712216970635]
	TIME [epoch: 6.31 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3843914049054273		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 3.3843914049054273 | validation: 3.0347600076908248]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0518711849112625		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 3.0518711849112625 | validation: 3.1405661765497257]
	TIME [epoch: 6.34 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8399772024620664		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 2.8399772024620664 | validation: 4.177847220438204]
	TIME [epoch: 6.33 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3781465129445327		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 3.3781465129445327 | validation: 3.7256107885156746]
	TIME [epoch: 6.31 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0986949757406768		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 3.0986949757406768 | validation: 2.8045747224799182]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0084396884518605		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 3.0084396884518605 | validation: 2.5060407930904334]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0654260689017607		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 3.0654260689017607 | validation: 2.602859175254479]
	TIME [epoch: 6.31 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.861661740826143		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 2.861661740826143 | validation: 2.588710414973353]
	TIME [epoch: 6.33 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7322147095564677		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 2.7322147095564677 | validation: 2.646771561311768]
	TIME [epoch: 6.32 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.677757196665733		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 2.677757196665733 | validation: 3.0027675297732177]
	TIME [epoch: 6.3 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.16056211226725		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 3.16056211226725 | validation: 3.3333094609899145]
	TIME [epoch: 6.3 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1485086074542976		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 3.1485086074542976 | validation: 2.2430217935551156]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.078011751253795		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 3.078011751253795 | validation: 3.565513385360598]
	TIME [epoch: 6.31 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7112895633816034		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 2.7112895633816034 | validation: 2.794794766491038]
	TIME [epoch: 6.31 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6071842943841785		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 2.6071842943841785 | validation: 2.2162500472417634]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7463742068228365		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 2.7463742068228365 | validation: 2.081523018471684]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2739239255710633		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 2.2739239255710633 | validation: 2.3609412104533765]
	TIME [epoch: 6.31 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.291017245322102		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 2.291017245322102 | validation: 1.8812403593282196]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.054784778226055		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 2.054784778226055 | validation: 2.201829799665291]
	TIME [epoch: 6.31 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.148829608785931		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 2.148829608785931 | validation: 3.2069678923814795]
	TIME [epoch: 6.33 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2596458732478752		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 2.2596458732478752 | validation: 1.8950043627161266]
	TIME [epoch: 6.32 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3321088443525038		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 2.3321088443525038 | validation: 2.5732858324061256]
	TIME [epoch: 6.31 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.847629992887057		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 2.847629992887057 | validation: 2.940233312092539]
	TIME [epoch: 6.3 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4379066917200305		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 2.4379066917200305 | validation: 1.9268333852507766]
	TIME [epoch: 6.3 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.01128725631674		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 2.01128725631674 | validation: 2.2383362965834923]
	TIME [epoch: 6.29 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0758763157575935		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 2.0758763157575935 | validation: 1.8271983729588128]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.074154786001281		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 2.074154786001281 | validation: 1.6419076943564557]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4017446077766267		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 2.4017446077766267 | validation: 3.0287039084895087]
	TIME [epoch: 6.31 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.452601823892822		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 2.452601823892822 | validation: 2.2304204123299405]
	TIME [epoch: 6.3 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5342628994215803		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 2.5342628994215803 | validation: 2.511486864237103]
	TIME [epoch: 6.29 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.059587015642308		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 2.059587015642308 | validation: 2.0380336915816613]
	TIME [epoch: 6.31 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1453363445830234		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 2.1453363445830234 | validation: 1.6614799657866353]
	TIME [epoch: 6.33 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.26057983127255		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 2.26057983127255 | validation: 2.033867066830992]
	TIME [epoch: 6.32 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.398676837219109		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 2.398676837219109 | validation: 2.246120604208514]
	TIME [epoch: 6.29 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0276111786557975		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 2.0276111786557975 | validation: 1.9523475617650847]
	TIME [epoch: 6.29 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.505111507931236		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 2.505111507931236 | validation: 2.3668920515772744]
	TIME [epoch: 6.3 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0665364145110456		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 2.0665364145110456 | validation: 2.324098468598848]
	TIME [epoch: 6.29 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.823500759046273		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 1.823500759046273 | validation: 1.948686525415969]
	TIME [epoch: 6.32 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2548152912165933		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 2.2548152912165933 | validation: 1.9514118839131243]
	TIME [epoch: 6.32 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9457553686420157		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 1.9457553686420157 | validation: 2.1052595354757013]
	TIME [epoch: 6.31 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.00371232360083		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 2.00371232360083 | validation: 2.3674805276196413]
	TIME [epoch: 6.3 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2050846336616017		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 2.2050846336616017 | validation: 1.698144155349555]
	TIME [epoch: 6.31 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6829229083684663		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 1.6829229083684663 | validation: 1.735895270142919]
	TIME [epoch: 6.3 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.52211112384074		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 2.52211112384074 | validation: 1.8889075730571148]
	TIME [epoch: 6.32 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9575118292274798		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 1.9575118292274798 | validation: 3.0748890269735387]
	TIME [epoch: 6.33 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5311308454074366		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 2.5311308454074366 | validation: 1.594533352181847]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.055014069803166		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 2.055014069803166 | validation: 2.4137235596509936]
	TIME [epoch: 6.32 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2389125649452204		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 2.2389125649452204 | validation: 1.9383849913523203]
	TIME [epoch: 6.3 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8700035559869495		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 1.8700035559869495 | validation: 1.5932479924871663]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.815729436019661		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 1.815729436019661 | validation: 2.2196991717674788]
	TIME [epoch: 6.34 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2338497092650607		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 2.2338497092650607 | validation: 1.795268034839251]
	TIME [epoch: 6.34 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9414751782666764		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 1.9414751782666764 | validation: 1.6905699597333737]
	TIME [epoch: 6.31 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6631035127524378		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 1.6631035127524378 | validation: 1.5060888647153532]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0927794028218836		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 2.0927794028218836 | validation: 1.885311980030228]
	TIME [epoch: 6.32 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.083917556953753		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 2.083917556953753 | validation: 2.2310423481837645]
	TIME [epoch: 6.31 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9761544645893054		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 1.9761544645893054 | validation: 2.0610145052747133]
	TIME [epoch: 6.33 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1355087735875515		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 2.1355087735875515 | validation: 1.5887673838057812]
	TIME [epoch: 6.34 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1991641827768125		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 2.1991641827768125 | validation: 1.6271526499491755]
	TIME [epoch: 6.31 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.302210173601533		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 2.302210173601533 | validation: 1.7412019795008407]
	TIME [epoch: 6.31 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.907353508937466		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 1.907353508937466 | validation: 2.1353805459858157]
	TIME [epoch: 6.31 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9669917938620523		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 1.9669917938620523 | validation: 1.7867845058156624]
	TIME [epoch: 6.31 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.829048818203076		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 1.829048818203076 | validation: 2.517268043664531]
	TIME [epoch: 6.33 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.826681995650128		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 1.826681995650128 | validation: 1.5401935707958927]
	TIME [epoch: 6.33 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.104113080274545		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 2.104113080274545 | validation: 2.2395591022538817]
	TIME [epoch: 6.31 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1881936827108834		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 2.1881936827108834 | validation: 1.6221387894452568]
	TIME [epoch: 6.31 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8268618951068032		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 1.8268618951068032 | validation: 1.7791947164659938]
	TIME [epoch: 6.31 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8866785610405823		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 1.8866785610405823 | validation: 1.5231495866643041]
	TIME [epoch: 6.31 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6632889457589515		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 1.6632889457589515 | validation: 1.7705588941909287]
	TIME [epoch: 6.31 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9524028171833843		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 1.9524028171833843 | validation: 1.9433568505694176]
	TIME [epoch: 6.35 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.277718311595409		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 2.277718311595409 | validation: 1.933167499218119]
	TIME [epoch: 6.32 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0280033676372327		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 2.0280033676372327 | validation: 1.6758613401547873]
	TIME [epoch: 6.31 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7822493811182036		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 1.7822493811182036 | validation: 1.486969125329931]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6702912744731298		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 1.6702912744731298 | validation: 1.9234390899032145]
	TIME [epoch: 6.31 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.912675157846645		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 1.912675157846645 | validation: 1.7664085038409418]
	TIME [epoch: 6.32 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6202748500777027		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 1.6202748500777027 | validation: 2.661106462422622]
	TIME [epoch: 6.35 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9449182092636954		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 1.9449182092636954 | validation: 1.5783590158215979]
	TIME [epoch: 6.32 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9143229099860617		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 1.9143229099860617 | validation: 3.0845145780305234]
	TIME [epoch: 6.32 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5789641523400513		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 2.5789641523400513 | validation: 1.8129995488480763]
	TIME [epoch: 6.31 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8866576589819983		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 1.8866576589819983 | validation: 1.591595482448087]
	TIME [epoch: 6.31 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8422484479684875		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 1.8422484479684875 | validation: 1.5382719706540273]
	TIME [epoch: 6.31 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.594412062803808		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 1.594412062803808 | validation: 1.6502374817603993]
	TIME [epoch: 6.35 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9420843462592812		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 1.9420843462592812 | validation: 2.2540250853955053]
	TIME [epoch: 6.32 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7746446730838503		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 1.7746446730838503 | validation: 1.7740458337558018]
	TIME [epoch: 6.31 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6481770440303565		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 1.6481770440303565 | validation: 2.147580383762846]
	TIME [epoch: 6.31 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8064499290579317		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 1.8064499290579317 | validation: 2.676239194852004]
	TIME [epoch: 6.31 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.064003945716205		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 2.064003945716205 | validation: 2.0412043667439224]
	TIME [epoch: 6.32 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9754203542904736		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 1.9754203542904736 | validation: 1.718176384199235]
	TIME [epoch: 6.36 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7780840665141606		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 1.7780840665141606 | validation: 1.8121500578841225]
	TIME [epoch: 6.32 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.856586544869348		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 1.856586544869348 | validation: 1.5349348325143568]
	TIME [epoch: 6.31 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5530762187860394		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 1.5530762187860394 | validation: 1.504673292857017]
	TIME [epoch: 6.31 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7848726469394867		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 1.7848726469394867 | validation: 2.4088771280758654]
	TIME [epoch: 6.31 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0695206455056367		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 2.0695206455056367 | validation: 2.057681012208028]
	TIME [epoch: 6.32 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7594827311883303		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 1.7594827311883303 | validation: 1.9291903685232814]
	TIME [epoch: 6.36 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8645142285902663		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 1.8645142285902663 | validation: 1.7926853013148114]
	TIME [epoch: 6.31 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.622742254490318		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 1.622742254490318 | validation: 1.8799145718154588]
	TIME [epoch: 6.32 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9355891181307776		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 1.9355891181307776 | validation: 1.4295582281189425]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.964342049592969		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 1.964342049592969 | validation: 1.7194802377279799]
	TIME [epoch: 6.31 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7153439607599539		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 1.7153439607599539 | validation: 1.8741423177677072]
	TIME [epoch: 6.31 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6078075436907473		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 1.6078075436907473 | validation: 1.5771803635367605]
	TIME [epoch: 6.35 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8721029614161684		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 1.8721029614161684 | validation: 1.3793033794492309]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6301958341760554		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 1.6301958341760554 | validation: 1.4817781420897602]
	TIME [epoch: 6.31 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8496093515629717		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 1.8496093515629717 | validation: 1.4648704312591103]
	TIME [epoch: 6.31 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7489347780546256		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 1.7489347780546256 | validation: 1.5370890986345227]
	TIME [epoch: 6.31 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.76434058289043		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 1.76434058289043 | validation: 1.5205224973177032]
	TIME [epoch: 6.31 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1275376368382193		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 2.1275376368382193 | validation: 2.1834006876335055]
	TIME [epoch: 6.36 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8592485128106262		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 1.8592485128106262 | validation: 2.2133925473912575]
	TIME [epoch: 6.31 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.667578136917475		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 1.667578136917475 | validation: 1.6241229596162539]
	TIME [epoch: 6.31 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.560907309393342		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 1.560907309393342 | validation: 2.2669459926345104]
	TIME [epoch: 6.31 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.139014139895979		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 2.139014139895979 | validation: 1.9242690366842763]
	TIME [epoch: 6.31 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6908664315183024		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 1.6908664315183024 | validation: 1.9461396323474855]
	TIME [epoch: 6.31 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2165640216256692		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 2.2165640216256692 | validation: 1.439828014214119]
	TIME [epoch: 6.37 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.618631296536072		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 2.618631296536072 | validation: 2.5706759945609754]
	TIME [epoch: 6.31 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.371688190793141		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 2.371688190793141 | validation: 1.8532175546100966]
	TIME [epoch: 6.31 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9574715677550487		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 1.9574715677550487 | validation: 2.0315887864267563]
	TIME [epoch: 6.31 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0242612242599156		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 2.0242612242599156 | validation: 1.6399774689178779]
	TIME [epoch: 6.31 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9095558980029834		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 1.9095558980029834 | validation: 1.6203490578277895]
	TIME [epoch: 6.31 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8669100303211423		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 1.8669100303211423 | validation: 1.6316898396050619]
	TIME [epoch: 6.34 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.075764718281947		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 2.075764718281947 | validation: 1.4635167906337596]
	TIME [epoch: 6.32 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5177293409018702		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 1.5177293409018702 | validation: 1.6464566039243227]
	TIME [epoch: 6.31 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.791366040399455		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 1.791366040399455 | validation: 2.432643777963608]
	TIME [epoch: 6.32 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7131845198994684		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 1.7131845198994684 | validation: 1.5539406249930663]
	TIME [epoch: 6.31 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6037059589817377		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 1.6037059589817377 | validation: 1.4574621924349946]
	TIME [epoch: 6.31 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7621620745043396		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 1.7621620745043396 | validation: 1.757241386041926]
	TIME [epoch: 6.33 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7613229680432032		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 1.7613229680432032 | validation: 1.913053814876183]
	TIME [epoch: 6.35 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7848268964206215		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 1.7848268964206215 | validation: 1.5178802857770388]
	TIME [epoch: 6.31 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.616141893845902		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 1.616141893845902 | validation: 1.524114818428507]
	TIME [epoch: 6.31 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8545160809006203		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 1.8545160809006203 | validation: 1.4641266662494488]
	TIME [epoch: 6.31 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6519822292308595		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 1.6519822292308595 | validation: 1.5157626701587974]
	TIME [epoch: 6.31 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4938449320610427		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 1.4938449320610427 | validation: 1.488408997605286]
	TIME [epoch: 6.34 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7740472862728764		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 1.7740472862728764 | validation: 1.5136905186556335]
	TIME [epoch: 6.34 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5595638707186712		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 1.5595638707186712 | validation: 2.1973474748390442]
	TIME [epoch: 6.31 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0702259914989827		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 2.0702259914989827 | validation: 1.799791470292844]
	TIME [epoch: 6.31 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6815142734658488		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 1.6815142734658488 | validation: 1.4960732876786795]
	TIME [epoch: 6.31 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6462263211352832		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 1.6462263211352832 | validation: 1.4159013152096942]
	TIME [epoch: 6.31 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5197885701725338		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 1.5197885701725338 | validation: 1.4215729898188263]
	TIME [epoch: 6.34 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6669635328658872		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 1.6669635328658872 | validation: 1.3285981985815307]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6698807377034688		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 1.6698807377034688 | validation: 1.9449809186033136]
	TIME [epoch: 6.31 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.733760896037571		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 1.733760896037571 | validation: 1.5591203916765501]
	TIME [epoch: 6.31 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4329551880180968		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 1.4329551880180968 | validation: 1.7471516035125438]
	TIME [epoch: 6.31 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7767728972057069		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 1.7767728972057069 | validation: 1.7064100654700936]
	TIME [epoch: 6.3 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.573917976936586		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 1.573917976936586 | validation: 3.746437234168895]
	TIME [epoch: 6.72 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.581213592742504		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 2.581213592742504 | validation: 1.6323643546376718]
	TIME [epoch: 6.33 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6186700781663017		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 1.6186700781663017 | validation: 1.5304739106707113]
	TIME [epoch: 6.31 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4643360116698352		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 1.4643360116698352 | validation: 1.5404982654206996]
	TIME [epoch: 6.31 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5574884450897586		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 1.5574884450897586 | validation: 1.993919045078823]
	TIME [epoch: 6.31 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.775531341776209		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 1.775531341776209 | validation: 1.4395193403954945]
	TIME [epoch: 6.31 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4707908454011567		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 1.4707908454011567 | validation: 1.5990596520918547]
	TIME [epoch: 6.32 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5494350451321397		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 1.5494350451321397 | validation: 1.3488631907008748]
	TIME [epoch: 6.34 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.486104856056749		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 1.486104856056749 | validation: 1.3410840105652628]
	TIME [epoch: 6.3 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6573309681395432		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 1.6573309681395432 | validation: 1.548838064035448]
	TIME [epoch: 6.31 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5628010921995088		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 1.5628010921995088 | validation: 2.1023882926500717]
	TIME [epoch: 6.3 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.729666012788891		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 1.729666012788891 | validation: 1.3582019068128095]
	TIME [epoch: 6.3 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6179062596208549		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 1.6179062596208549 | validation: 1.3710466327562525]
	TIME [epoch: 6.31 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5783059251179816		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 1.5783059251179816 | validation: 1.516166810674305]
	TIME [epoch: 6.34 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4979149492679193		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 1.4979149492679193 | validation: 1.273406979468443]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5384670221929737		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 1.5384670221929737 | validation: 1.3493757418890588]
	TIME [epoch: 6.31 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.568686647717514		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 1.568686647717514 | validation: 1.3289268624141228]
	TIME [epoch: 6.31 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4150156269387029		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 1.4150156269387029 | validation: 1.621611575734712]
	TIME [epoch: 6.31 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4499195817131931		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 1.4499195817131931 | validation: 1.2921767845149579]
	TIME [epoch: 6.32 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4849894188871087		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 1.4849894188871087 | validation: 1.4155648314396376]
	TIME [epoch: 6.34 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.432788904385959		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 1.432788904385959 | validation: 1.5183692270619455]
	TIME [epoch: 6.31 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9131254191813278		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 1.9131254191813278 | validation: 1.3412133403818496]
	TIME [epoch: 6.31 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6357619293571914		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 1.6357619293571914 | validation: 1.7887425697047854]
	TIME [epoch: 6.31 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5207400972022507		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 1.5207400972022507 | validation: 1.5283265130399868]
	TIME [epoch: 6.31 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8292079495319657		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 1.8292079495319657 | validation: 1.3857851905920295]
	TIME [epoch: 6.31 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3453909013616525		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 2.3453909013616525 | validation: 2.572189931821634]
	TIME [epoch: 6.34 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.797127198671001		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 1.797127198671001 | validation: 1.4135413471227327]
	TIME [epoch: 6.32 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.415370443123293		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 1.415370443123293 | validation: 1.649485157856153]
	TIME [epoch: 6.31 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4175668876353547		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 1.4175668876353547 | validation: 1.7203662198640015]
	TIME [epoch: 6.31 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3782165690986816		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 1.3782165690986816 | validation: 1.3476927913144587]
	TIME [epoch: 6.31 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3546249199217384		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 1.3546249199217384 | validation: 1.5718103470964122]
	TIME [epoch: 6.32 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5726398074723873		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 1.5726398074723873 | validation: 1.4252518416559359]
	TIME [epoch: 6.34 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5422975267944807		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 1.5422975267944807 | validation: 1.387509746921642]
	TIME [epoch: 6.32 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5381514996639665		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 1.5381514996639665 | validation: 1.6548815584264258]
	TIME [epoch: 6.31 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5863866580280521		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 1.5863866580280521 | validation: 1.3979386276244519]
	TIME [epoch: 6.31 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6821625273337797		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 1.6821625273337797 | validation: 1.7269009357015532]
	TIME [epoch: 6.31 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4485658154988035		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 1.4485658154988035 | validation: 1.4974480275103557]
	TIME [epoch: 6.31 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5017200467325802		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 1.5017200467325802 | validation: 1.598174766897913]
	TIME [epoch: 6.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5387268270821584		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 1.5387268270821584 | validation: 1.4125222634279009]
	TIME [epoch: 6.31 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6599180436986074		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 1.6599180436986074 | validation: 4.184036867708503]
	TIME [epoch: 6.3 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.181935463136256		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 2.181935463136256 | validation: 2.0091420643203097]
	TIME [epoch: 6.31 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8485924349293488		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 1.8485924349293488 | validation: 1.5338942205107102]
	TIME [epoch: 6.31 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.642065269068665		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 1.642065269068665 | validation: 1.689655076733783]
	TIME [epoch: 6.31 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4841952984796838		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 1.4841952984796838 | validation: 1.5617987743922237]
	TIME [epoch: 6.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8449772789357755		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 1.8449772789357755 | validation: 1.4817569995461919]
	TIME [epoch: 6.32 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6357876693190014		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 1.6357876693190014 | validation: 1.5086792215686355]
	TIME [epoch: 6.31 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.00043326592364		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 2.00043326592364 | validation: 4.58168116991915]
	TIME [epoch: 6.3 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7475735776998276		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 3.7475735776998276 | validation: 1.6286040109497208]
	TIME [epoch: 6.31 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0431192527712345		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 2.0431192527712345 | validation: 2.159048049208333]
	TIME [epoch: 6.3 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7401242787730324		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 1.7401242787730324 | validation: 1.4445821250780613]
	TIME [epoch: 6.34 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.617754240893047		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 1.617754240893047 | validation: 1.736276891322905]
	TIME [epoch: 6.32 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.535729413210141		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 1.535729413210141 | validation: 1.510756689431687]
	TIME [epoch: 6.3 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4980329869573654		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 1.4980329869573654 | validation: 1.389617932871758]
	TIME [epoch: 6.3 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3859707113013275		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 1.3859707113013275 | validation: 1.3862931892474994]
	TIME [epoch: 6.31 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6664963130703798		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 1.6664963130703798 | validation: 1.3210593212624477]
	TIME [epoch: 6.31 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4796159399426856		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 1.4796159399426856 | validation: 1.7468966422685241]
	TIME [epoch: 6.32 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.547584584321288		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 1.547584584321288 | validation: 1.9432758820131684]
	TIME [epoch: 6.33 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.819871961943467		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 1.819871961943467 | validation: 2.094673659435565]
	TIME [epoch: 6.31 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6485978036880162		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 1.6485978036880162 | validation: 1.5684764128881337]
	TIME [epoch: 6.3 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4481071872418994		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 1.4481071872418994 | validation: 1.3251207334054316]
	TIME [epoch: 6.31 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4099300378340482		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 1.4099300378340482 | validation: 1.3416100821396464]
	TIME [epoch: 6.31 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4131466056628887		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 1.4131466056628887 | validation: 1.6856528597354132]
	TIME [epoch: 6.32 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.896759491423668		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 1.896759491423668 | validation: 1.4487085101846664]
	TIME [epoch: 6.33 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6390415152547673		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 1.6390415152547673 | validation: 1.4774773798153258]
	TIME [epoch: 6.31 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4425121112106272		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 1.4425121112106272 | validation: 1.5517323452062464]
	TIME [epoch: 6.3 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5069994381396041		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 1.5069994381396041 | validation: 1.3198450157504065]
	TIME [epoch: 6.3 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6354089102619165		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 1.6354089102619165 | validation: 1.6380649865542485]
	TIME [epoch: 6.3 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3948692520267225		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 1.3948692520267225 | validation: 3.4638895745341873]
	TIME [epoch: 6.32 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9824408193056253		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 1.9824408193056253 | validation: 1.7160171461879092]
	TIME [epoch: 6.34 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7666585189187578		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 1.7666585189187578 | validation: 1.5424998119952238]
	TIME [epoch: 6.31 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5391033261694285		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 1.5391033261694285 | validation: 1.4056555283953673]
	TIME [epoch: 6.3 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.68955237289898		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 1.68955237289898 | validation: 1.4169804841542093]
	TIME [epoch: 6.3 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6434700240216864		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 1.6434700240216864 | validation: 1.3590074527273357]
	TIME [epoch: 6.3 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.001160648508387		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 2.001160648508387 | validation: 1.5334003759003578]
	TIME [epoch: 6.31 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4731333068952392		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 1.4731333068952392 | validation: 1.3826446835544586]
	TIME [epoch: 6.34 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3777624250817087		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 1.3777624250817087 | validation: 1.5996364996817314]
	TIME [epoch: 6.31 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4506031301676008		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 1.4506031301676008 | validation: 1.5589383936815446]
	TIME [epoch: 6.3 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5672044852841027		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 1.5672044852841027 | validation: 2.5265422048816406]
	TIME [epoch: 6.3 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8633925355900398		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 1.8633925355900398 | validation: 1.8780374453703186]
	TIME [epoch: 6.31 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5929115885765301		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 1.5929115885765301 | validation: 2.2635559160068075]
	TIME [epoch: 6.31 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7201051773556844		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 1.7201051773556844 | validation: 1.5559646214989162]
	TIME [epoch: 6.34 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7116941473041354		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 1.7116941473041354 | validation: 1.4439248418614017]
	TIME [epoch: 6.3 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5102729506354597		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 1.5102729506354597 | validation: 1.4369254180769975]
	TIME [epoch: 6.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3559858866606447		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 1.3559858866606447 | validation: 2.1170396235515394]
	TIME [epoch: 6.29 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9755951992028695		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 1.9755951992028695 | validation: 1.7529720391171417]
	TIME [epoch: 6.3 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7349406283263686		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 1.7349406283263686 | validation: 1.588837782934677]
	TIME [epoch: 6.31 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5816749970155448		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 1.5816749970155448 | validation: 1.5806650208943926]
	TIME [epoch: 6.33 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1491678580250007		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 2.1491678580250007 | validation: 1.58597428440305]
	TIME [epoch: 6.31 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6724793664993685		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 1.6724793664993685 | validation: 1.9843688751547972]
	TIME [epoch: 6.3 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7179435459563153		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 1.7179435459563153 | validation: 1.6157430100818726]
	TIME [epoch: 6.3 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.516104342368354		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 1.516104342368354 | validation: 1.7893197270043375]
	TIME [epoch: 6.3 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6414898421946185		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 1.6414898421946185 | validation: 1.8621607147926773]
	TIME [epoch: 6.3 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6084251371023548		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 1.6084251371023548 | validation: 1.5936020350023017]
	TIME [epoch: 6.34 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4505807493181906		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 1.4505807493181906 | validation: 1.6254559739267367]
	TIME [epoch: 6.31 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.556498622458655		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 1.556498622458655 | validation: 3.176235169993209]
	TIME [epoch: 6.3 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0280179620918157		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 2.0280179620918157 | validation: 1.4986415549539882]
	TIME [epoch: 6.31 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.473945048972897		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 1.473945048972897 | validation: 1.527664632353761]
	TIME [epoch: 6.3 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7293146936603274		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 1.7293146936603274 | validation: 1.4686404056358817]
	TIME [epoch: 6.3 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4142207069711556		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 1.4142207069711556 | validation: 1.4193389679169184]
	TIME [epoch: 6.33 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.382677901683461		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 1.382677901683461 | validation: 1.5674199517785632]
	TIME [epoch: 6.31 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9054590130007005		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 1.9054590130007005 | validation: 2.2341311405822397]
	TIME [epoch: 6.31 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.611887859612965		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 1.611887859612965 | validation: 2.0562623387731964]
	TIME [epoch: 6.31 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5883864381205055		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 1.5883864381205055 | validation: 1.6645817126277405]
	TIME [epoch: 6.31 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4502395444130638		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 1.4502395444130638 | validation: 1.308075883140529]
	TIME [epoch: 6.3 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5964299393795793		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 1.5964299393795793 | validation: 1.4817210660341464]
	TIME [epoch: 6.33 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3476713636584434		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 1.3476713636584434 | validation: 2.863216157969176]
	TIME [epoch: 6.32 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2949510867250953		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 2.2949510867250953 | validation: 1.6218534645476124]
	TIME [epoch: 6.31 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5378082348389095		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 1.5378082348389095 | validation: 1.3773583083782208]
	TIME [epoch: 6.31 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5074858029423446		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 1.5074858029423446 | validation: 1.3063858047399393]
	TIME [epoch: 6.31 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2951438035244855		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 1.2951438035244855 | validation: 1.5685224468400223]
	TIME [epoch: 6.31 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4461870877169802		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 1.4461870877169802 | validation: 1.2914243824344989]
	TIME [epoch: 6.32 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6880892222248416		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 1.6880892222248416 | validation: 1.4670590207378806]
	TIME [epoch: 6.33 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4055358236100481		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 1.4055358236100481 | validation: 1.3385755003091564]
	TIME [epoch: 6.31 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0809968286600804		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 2.0809968286600804 | validation: 3.098756539866515]
	TIME [epoch: 6.31 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4342995065379682		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 2.4342995065379682 | validation: 1.8618896285663011]
	TIME [epoch: 6.31 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8211587595565464		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 1.8211587595565464 | validation: 1.8395417521529396]
	TIME [epoch: 6.3 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7880536937837688		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 1.7880536937837688 | validation: 1.6103825953856121]
	TIME [epoch: 6.32 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4473147047229604		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 1.4473147047229604 | validation: 1.3942344851542563]
	TIME [epoch: 6.33 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3754144626584583		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 1.3754144626584583 | validation: 1.4203832938775691]
	TIME [epoch: 6.31 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3565712941228296		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 1.3565712941228296 | validation: 1.4920476461995278]
	TIME [epoch: 6.31 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3302046300881805		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 1.3302046300881805 | validation: 1.7386863232913519]
	TIME [epoch: 6.31 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.460032805807207		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 1.460032805807207 | validation: 1.3428867255868113]
	TIME [epoch: 6.3 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3791169551284168		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 1.3791169551284168 | validation: 1.2358195436672343]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3781415515225344		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 1.3781415515225344 | validation: 1.8171543189500325]
	TIME [epoch: 6.34 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6141247695749414		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 1.6141247695749414 | validation: 1.3914779612435653]
	TIME [epoch: 6.31 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3436884486416387		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 1.3436884486416387 | validation: 1.782152140056127]
	TIME [epoch: 6.31 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6935064733606198		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 1.6935064733606198 | validation: 1.4196027047004445]
	TIME [epoch: 6.31 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4493314691638357		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 1.4493314691638357 | validation: 1.282070874735425]
	TIME [epoch: 6.31 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2969236492700746		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 2.2969236492700746 | validation: 2.0981383745561817]
	TIME [epoch: 6.32 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0365089465852524		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 2.0365089465852524 | validation: 1.7248541384914136]
	TIME [epoch: 6.34 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5677612536850614		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 1.5677612536850614 | validation: 1.7292295932230417]
	TIME [epoch: 6.31 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5470274294489825		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 1.5470274294489825 | validation: 1.876281980265809]
	TIME [epoch: 6.3 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4654902270005763		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 1.4654902270005763 | validation: 2.0831689342471327]
	TIME [epoch: 6.31 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4596227802886237		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 1.4596227802886237 | validation: 1.433406957720805]
	TIME [epoch: 6.31 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4096509213465218		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 1.4096509213465218 | validation: 1.2966635709401342]
	TIME [epoch: 6.31 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2903567894467158		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 1.2903567894467158 | validation: 1.3947955168323904]
	TIME [epoch: 6.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3245337164201691		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 1.3245337164201691 | validation: 2.7522053272063354]
	TIME [epoch: 6.31 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9227855752020204		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 1.9227855752020204 | validation: 1.5350504055047427]
	TIME [epoch: 6.3 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5640989358237014		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 1.5640989358237014 | validation: 1.3404362533227812]
	TIME [epoch: 6.31 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.469227500197447		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 1.469227500197447 | validation: 1.9704572840896892]
	TIME [epoch: 6.31 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6335430294244642		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 1.6335430294244642 | validation: 1.4629729745102842]
	TIME [epoch: 6.31 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3747523788690226		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 1.3747523788690226 | validation: 1.432478788140417]
	TIME [epoch: 6.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5034935869423338		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 1.5034935869423338 | validation: 1.693186798159044]
	TIME [epoch: 6.31 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5007987833693797		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 1.5007987833693797 | validation: 1.379854608561234]
	TIME [epoch: 6.31 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2835527617953013		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 1.2835527617953013 | validation: 1.3729430382817869]
	TIME [epoch: 6.3 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3030636715654316		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 1.3030636715654316 | validation: 1.5851707264442707]
	TIME [epoch: 6.31 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2460340327245203		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 1.2460340327245203 | validation: 1.50015657940743]
	TIME [epoch: 6.31 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2415207546938531		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 1.2415207546938531 | validation: 1.9141230028017973]
	TIME [epoch: 6.34 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8209818223763483		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 1.8209818223763483 | validation: 1.739122631785873]
	TIME [epoch: 6.31 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4885234780585377		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 1.4885234780585377 | validation: 1.6299138396734194]
	TIME [epoch: 6.3 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2608475813987412		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 1.2608475813987412 | validation: 1.4212417584598223]
	TIME [epoch: 6.3 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5722266171388266		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 1.5722266171388266 | validation: 1.222635497755268]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.423800121373054		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 1.423800121373054 | validation: 1.7973658198304956]
	TIME [epoch: 6.3 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.439554264321698		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 1.439554264321698 | validation: 1.30683110928732]
	TIME [epoch: 6.32 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.325595076115336		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 1.325595076115336 | validation: 1.9575339649287233]
	TIME [epoch: 6.29 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5208631885364756		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 1.5208631885364756 | validation: 1.4370564650602204]
	TIME [epoch: 6.3 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3637256194845016		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 1.3637256194845016 | validation: 1.4945463967441226]
	TIME [epoch: 6.29 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2195559010263133		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 1.2195559010263133 | validation: 1.428530077991129]
	TIME [epoch: 6.29 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2571309746575952		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 1.2571309746575952 | validation: 1.3178692530438854]
	TIME [epoch: 6.3 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3102394435676608		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 1.3102394435676608 | validation: 1.2036727782573267]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1850176199637488		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 1.1850176199637488 | validation: 1.5356704826040275]
	TIME [epoch: 6.29 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2413881144995915		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 1.2413881144995915 | validation: 1.8557689391621732]
	TIME [epoch: 6.3 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5666239220376048		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 1.5666239220376048 | validation: 1.444672433988437]
	TIME [epoch: 6.29 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6382223752913272		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 1.6382223752913272 | validation: 1.4167487595948298]
	TIME [epoch: 6.29 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2333698028889237		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 1.2333698028889237 | validation: 1.3720386760400514]
	TIME [epoch: 6.3 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3827814082553125		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 1.3827814082553125 | validation: 1.3195233283033736]
	TIME [epoch: 6.33 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3962126806431827		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 1.3962126806431827 | validation: 2.351194876480517]
	TIME [epoch: 6.3 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5230301053876787		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 1.5230301053876787 | validation: 2.2140580266605383]
	TIME [epoch: 6.3 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5559622113789229		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 1.5559622113789229 | validation: 1.3335393557644162]
	TIME [epoch: 6.28 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0960691996618865		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 1.0960691996618865 | validation: 1.2306074183183]
	TIME [epoch: 6.3 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9176268383499895		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 1.9176268383499895 | validation: 1.427051843425717]
	TIME [epoch: 6.3 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1354071080767514		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 1.1354071080767514 | validation: 1.280717512428848]
	TIME [epoch: 6.32 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.114189943172598		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 1.114189943172598 | validation: 1.3306216116944376]
	TIME [epoch: 6.3 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9677659129728633		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 1.9677659129728633 | validation: 1.8288759780624093]
	TIME [epoch: 6.29 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.469549917780199		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 1.469549917780199 | validation: 1.832954844537919]
	TIME [epoch: 6.29 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4601487205161632		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 1.4601487205161632 | validation: 1.2732969876069324]
	TIME [epoch: 6.29 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.202325835132776		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 1.202325835132776 | validation: 1.3072009783227205]
	TIME [epoch: 6.29 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3220453952806794		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 1.3220453952806794 | validation: 1.5537265006768834]
	TIME [epoch: 6.32 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2776551825639317		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 1.2776551825639317 | validation: 1.2585330814550488]
	TIME [epoch: 6.3 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.197542958850782		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 1.197542958850782 | validation: 1.4998744308327192]
	TIME [epoch: 6.29 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2338213843943966		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 1.2338213843943966 | validation: 1.22664569336245]
	TIME [epoch: 6.29 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3645056445599415		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 1.3645056445599415 | validation: 1.431514781622526]
	TIME [epoch: 6.29 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4977263528896816		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 1.4977263528896816 | validation: 1.291878034379618]
	TIME [epoch: 6.29 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1700490657629643		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 1.1700490657629643 | validation: 1.6189396275542225]
	TIME [epoch: 6.31 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2852067794374427		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 1.2852067794374427 | validation: 1.41559291311717]
	TIME [epoch: 6.31 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2042296829075883		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 1.2042296829075883 | validation: 1.352316523654887]
	TIME [epoch: 6.29 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1593127408658033		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 1.1593127408658033 | validation: 1.2565425191558517]
	TIME [epoch: 6.29 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.139919223325839		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 1.139919223325839 | validation: 1.195193868009521]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4557888434973165		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 1.4557888434973165 | validation: 1.2815679570404237]
	TIME [epoch: 6.3 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2455594494399396		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 1.2455594494399396 | validation: 1.118480972973584]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0831275083830534		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 1.0831275083830534 | validation: 1.1151073252847277]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0657562459562402		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 1.0657562459562402 | validation: 2.703524570849783]
	TIME [epoch: 6.31 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9314879469654638		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 1.9314879469654638 | validation: 1.3285291943008806]
	TIME [epoch: 6.31 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2617621295086856		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 1.2617621295086856 | validation: 1.4397828155326893]
	TIME [epoch: 6.3 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3370988048055497		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 1.3370988048055497 | validation: 1.2111281973069874]
	TIME [epoch: 6.3 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0601973482731912		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 1.0601973482731912 | validation: 1.1462839766756554]
	TIME [epoch: 6.32 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1103733717290178		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 1.1103733717290178 | validation: 1.59663712381269]
	TIME [epoch: 6.33 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1876048125158627		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 1.1876048125158627 | validation: 1.5792555640463037]
	TIME [epoch: 6.31 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4467229482824238		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 1.4467229482824238 | validation: 1.2169212430913237]
	TIME [epoch: 6.31 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0933730227215197		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 1.0933730227215197 | validation: 1.6233788489561627]
	TIME [epoch: 6.31 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1401347279886522		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 1.1401347279886522 | validation: 1.2050070916391675]
	TIME [epoch: 6.3 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.085206536603323		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 1.085206536603323 | validation: 1.5533777311277626]
	TIME [epoch: 6.32 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3527718120190968		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 1.3527718120190968 | validation: 1.4373107779799716]
	TIME [epoch: 6.33 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4769552937602561		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 1.4769552937602561 | validation: 1.463738251813076]
	TIME [epoch: 6.3 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1843219863959937		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 1.1843219863959937 | validation: 1.2805521960396038]
	TIME [epoch: 6.3 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1541154998585577		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 1.1541154998585577 | validation: 1.1975222429500687]
	TIME [epoch: 6.3 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1788187059352984		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 1.1788187059352984 | validation: 1.2043666229775205]
	TIME [epoch: 6.3 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0427649409447244		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 1.0427649409447244 | validation: 1.2855776971900434]
	TIME [epoch: 6.31 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1025065114894848		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 1.1025065114894848 | validation: 1.646953389643091]
	TIME [epoch: 6.34 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.489830481200237		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 1.489830481200237 | validation: 2.6193067045255494]
	TIME [epoch: 6.31 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6364558745206002		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 1.6364558745206002 | validation: 1.4426744274468506]
	TIME [epoch: 6.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2956481395335024		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 1.2956481395335024 | validation: 1.2143747880718023]
	TIME [epoch: 6.3 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0116372413635484		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 1.0116372413635484 | validation: 1.0605292850678496]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.324613439559253		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 1.324613439559253 | validation: 1.0320426067156474]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1327478448180213		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 1.1327478448180213 | validation: 1.176994410254205]
	TIME [epoch: 6.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.095165941493518		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 1.095165941493518 | validation: 1.2300851974461358]
	TIME [epoch: 6.32 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0876722404934107		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 1.0876722404934107 | validation: 1.0407206078934146]
	TIME [epoch: 6.32 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3221074736054086		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 1.3221074736054086 | validation: 1.2613951510257455]
	TIME [epoch: 6.31 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0668689368828215		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 1.0668689368828215 | validation: 1.1393320737420138]
	TIME [epoch: 6.31 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0851113311193672		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 1.0851113311193672 | validation: 1.269923504357914]
	TIME [epoch: 6.32 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.067744075683226		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 1.067744075683226 | validation: 1.5888870795368417]
	TIME [epoch: 6.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1607357628504364		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 1.1607357628504364 | validation: 1.456819977326314]
	TIME [epoch: 6.31 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0786982176475328		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 1.0786982176475328 | validation: 1.874630129598523]
	TIME [epoch: 6.31 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1773397414311646		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 1.1773397414311646 | validation: 1.3007834639683815]
	TIME [epoch: 6.31 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0744536263925308		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 1.0744536263925308 | validation: 1.1058100252120722]
	TIME [epoch: 6.31 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2065484381414415		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 1.2065484381414415 | validation: 0.9840047129102413]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2153006946669116		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 1.2153006946669116 | validation: 1.278423635836715]
	TIME [epoch: 6.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.148959338818515		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 1.148959338818515 | validation: 1.208874434671911]
	TIME [epoch: 6.32 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0953397010697368		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 1.0953397010697368 | validation: 1.1642410432221162]
	TIME [epoch: 6.31 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.978518762675887		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.978518762675887 | validation: 0.968412799289207]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2486617330789698		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 1.2486617330789698 | validation: 1.0017845908111043]
	TIME [epoch: 6.32 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1937409228174696		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 1.1937409228174696 | validation: 1.5882000157922596]
	TIME [epoch: 6.32 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.123300660555632		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 1.123300660555632 | validation: 1.3023498608705455]
	TIME [epoch: 6.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0514544032107853		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 1.0514544032107853 | validation: 1.0894683914291197]
	TIME [epoch: 6.31 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9769573632164887		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.9769573632164887 | validation: 1.3301459171229295]
	TIME [epoch: 6.32 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2508539712124045		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 1.2508539712124045 | validation: 1.3500665322455365]
	TIME [epoch: 6.31 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0392715500353122		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 1.0392715500353122 | validation: 1.121839323615831]
	TIME [epoch: 6.32 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9806893987336326		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.9806893987336326 | validation: 1.2064269805629708]
	TIME [epoch: 6.32 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.066497359504369		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 1.066497359504369 | validation: 1.6267629472753753]
	TIME [epoch: 6.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1910844801267744		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 1.1910844801267744 | validation: 1.179943182500054]
	TIME [epoch: 6.32 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1645541767353547		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 1.1645541767353547 | validation: 1.1780480264770188]
	TIME [epoch: 6.31 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9338112750753904		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.9338112750753904 | validation: 1.0754746961584176]
	TIME [epoch: 6.32 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9864341529445291		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.9864341529445291 | validation: 1.0734922540274559]
	TIME [epoch: 6.31 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.069915673892972		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 1.069915673892972 | validation: 1.226741368129528]
	TIME [epoch: 6.32 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0745901009840988		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 1.0745901009840988 | validation: 1.1489311534512392]
	TIME [epoch: 6.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0732328956965098		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 1.0732328956965098 | validation: 1.3290554042224794]
	TIME [epoch: 6.31 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.108664699206492		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 1.108664699206492 | validation: 1.1496847701337476]
	TIME [epoch: 6.32 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1003405522328893		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 1.1003405522328893 | validation: 1.2322856694038329]
	TIME [epoch: 6.31 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9823125620643347		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.9823125620643347 | validation: 1.00532508190053]
	TIME [epoch: 6.32 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.976677923279744		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.976677923279744 | validation: 1.0560720778530197]
	TIME [epoch: 6.32 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0573869985233773		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 1.0573869985233773 | validation: 1.0004088905039392]
	TIME [epoch: 6.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9008594905755485		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.9008594905755485 | validation: 0.9137590823690617]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9563524418885562		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.9563524418885562 | validation: 1.0279284061807301]
	TIME [epoch: 6.32 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3680838434295062		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 1.3680838434295062 | validation: 1.980965910748938]
	TIME [epoch: 6.31 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.338574281823586		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 1.338574281823586 | validation: 1.0596451018564976]
	TIME [epoch: 6.31 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9696049445357539		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.9696049445357539 | validation: 0.9982892950504658]
	TIME [epoch: 6.32 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9681085347758323		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.9681085347758323 | validation: 1.1196332657246826]
	TIME [epoch: 6.36 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.952044192816386		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.952044192816386 | validation: 0.9500409387860989]
	TIME [epoch: 6.31 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9250542021276036		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.9250542021276036 | validation: 1.2402932434994813]
	TIME [epoch: 6.31 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0059791369994482		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 1.0059791369994482 | validation: 1.2733710301299501]
	TIME [epoch: 6.31 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9512947869528903		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.9512947869528903 | validation: 1.5761540213577208]
	TIME [epoch: 6.31 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2971717884850091		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 1.2971717884850091 | validation: 1.1291172760201902]
	TIME [epoch: 6.31 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9861341680541867		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.9861341680541867 | validation: 1.1717305602441392]
	TIME [epoch: 6.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.924599799015227		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.924599799015227 | validation: 1.211872619685928]
	TIME [epoch: 6.31 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9273228663135114		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.9273228663135114 | validation: 1.634480431043304]
	TIME [epoch: 6.31 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2773445116707132		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 1.2773445116707132 | validation: 1.0132161400549036]
	TIME [epoch: 6.31 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9257678394811782		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.9257678394811782 | validation: 1.0470622058167274]
	TIME [epoch: 6.31 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8485439518045303		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.8485439518045303 | validation: 0.9521590540237214]
	TIME [epoch: 6.31 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8641942043200596		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.8641942043200596 | validation: 0.8617685624480569]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8992039129505971		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.8992039129505971 | validation: 1.3283996764797812]
	TIME [epoch: 6.32 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0147363456856922		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 1.0147363456856922 | validation: 1.378771486210559]
	TIME [epoch: 6.31 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3123638465869953		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 1.3123638465869953 | validation: 1.1785250306820054]
	TIME [epoch: 6.31 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.139242384149528		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 1.139242384149528 | validation: 1.3404496043677958]
	TIME [epoch: 6.31 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0791987371301617		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 1.0791987371301617 | validation: 1.0202347522444812]
	TIME [epoch: 6.31 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8918267355443358		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.8918267355443358 | validation: 2.2641230043223235]
	TIME [epoch: 6.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6582739891182205		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 1.6582739891182205 | validation: 2.0831847583469085]
	TIME [epoch: 6.33 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3079202166401327		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 1.3079202166401327 | validation: 1.0330193378397414]
	TIME [epoch: 6.31 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8932842071027876		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.8932842071027876 | validation: 1.1071659499558715]
	TIME [epoch: 6.31 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8362926488494502		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.8362926488494502 | validation: 1.1403648230179075]
	TIME [epoch: 6.31 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8404983476626018		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.8404983476626018 | validation: 0.9294150616418169]
	TIME [epoch: 6.31 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.192759832935823		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 1.192759832935823 | validation: 1.292058244464073]
	TIME [epoch: 6.34 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0481581677980882		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 1.0481581677980882 | validation: 0.9172280073982066]
	TIME [epoch: 6.33 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8986531465166938		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.8986531465166938 | validation: 1.0758740408300733]
	TIME [epoch: 6.31 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.21103555297501		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 1.21103555297501 | validation: 1.4901064836209619]
	TIME [epoch: 6.31 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0519017166818068		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 1.0519017166818068 | validation: 1.27441557184603]
	TIME [epoch: 6.31 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0671481685929864		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 1.0671481685929864 | validation: 0.9267780603028966]
	TIME [epoch: 6.31 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7318471837482684		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.7318471837482684 | validation: 1.1497896887289196]
	TIME [epoch: 6.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9461915436677428		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.9461915436677428 | validation: 0.8362110277036221]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8766586967117409		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.8766586967117409 | validation: 0.8137477609418009]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7830790352686627		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.7830790352686627 | validation: 0.9296960969476897]
	TIME [epoch: 6.32 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7471257867106775		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.7471257867106775 | validation: 0.9770841365572827]
	TIME [epoch: 6.32 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8782305447360244		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.8782305447360244 | validation: 1.7642541404673127]
	TIME [epoch: 6.31 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1240036011758607		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 1.1240036011758607 | validation: 0.9747745871434219]
	TIME [epoch: 6.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8737367159653986		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.8737367159653986 | validation: 0.8519762697192883]
	TIME [epoch: 6.33 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.756541131102974		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.756541131102974 | validation: 0.933344469475715]
	TIME [epoch: 6.32 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9179276798290605		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.9179276798290605 | validation: 0.9886066648450231]
	TIME [epoch: 6.32 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8951621674218675		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.8951621674218675 | validation: 1.02005680531836]
	TIME [epoch: 6.32 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8085902183682063		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.8085902183682063 | validation: 0.8469119047232778]
	TIME [epoch: 6.32 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8119523077052653		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.8119523077052653 | validation: 0.8936720252903265]
	TIME [epoch: 6.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7508062374619685		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.7508062374619685 | validation: 1.2964906052844938]
	TIME [epoch: 6.33 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8768721490920948		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.8768721490920948 | validation: 0.9514096946763497]
	TIME [epoch: 6.32 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7456440962563745		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.7456440962563745 | validation: 0.8510103630455672]
	TIME [epoch: 6.32 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8131651171565476		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.8131651171565476 | validation: 1.34937763084478]
	TIME [epoch: 6.32 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.164218939596969		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 1.164218939596969 | validation: 0.9229368703383581]
	TIME [epoch: 6.32 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7661281149445296		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.7661281149445296 | validation: 1.2091191561676091]
	TIME [epoch: 6.34 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7998666092497478		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.7998666092497478 | validation: 0.8788632766799525]
	TIME [epoch: 6.35 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7135367790562941		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.7135367790562941 | validation: 0.8855942122661485]
	TIME [epoch: 6.31 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8352494002961582		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.8352494002961582 | validation: 0.9760118574040099]
	TIME [epoch: 6.32 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7872688825005973		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.7872688825005973 | validation: 0.7954082941144335]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7021426521791376		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.7021426521791376 | validation: 0.8595237386360641]
	TIME [epoch: 6.31 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7504502491611393		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.7504502491611393 | validation: 0.8038812703979137]
	TIME [epoch: 6.33 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8666659752628245		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.8666659752628245 | validation: 0.8578239333419657]
	TIME [epoch: 6.34 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8579477436347268		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.8579477436347268 | validation: 0.7144359842071908]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8413404994681317		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.8413404994681317 | validation: 1.0752092600065946]
	TIME [epoch: 6.32 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7649844813369446		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.7649844813369446 | validation: 0.8827863035267662]
	TIME [epoch: 6.31 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8116713890212863		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.8116713890212863 | validation: 0.7097864424934628]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6493553853096108		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.6493553853096108 | validation: 1.053078575259513]
	TIME [epoch: 6.36 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7632233915167465		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.7632233915167465 | validation: 0.7420090609109766]
	TIME [epoch: 6.34 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7039426204357078		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.7039426204357078 | validation: 0.7674307769373147]
	TIME [epoch: 6.32 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7255443727333742		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.7255443727333742 | validation: 0.9108015319825342]
	TIME [epoch: 6.32 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8058471384833583		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.8058471384833583 | validation: 0.7156377376874661]
	TIME [epoch: 6.32 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6932541475695772		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.6932541475695772 | validation: 0.690592788528289]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6384207684991434		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.6384207684991434 | validation: 0.8678288159664316]
	TIME [epoch: 6.35 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7541578620962424		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.7541578620962424 | validation: 0.7916705356554297]
	TIME [epoch: 6.34 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7074896447495863		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.7074896447495863 | validation: 0.8428397056467354]
	TIME [epoch: 6.31 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7936019065992526		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.7936019065992526 | validation: 0.785867986091247]
	TIME [epoch: 6.31 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8658698305905341		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.8658698305905341 | validation: 0.8207645883383166]
	TIME [epoch: 6.31 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7773121471468813		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.7773121471468813 | validation: 1.0741225715969516]
	TIME [epoch: 6.31 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8061639070063836		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.8061639070063836 | validation: 1.5431473331781183]
	TIME [epoch: 6.33 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1119742396579941		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 1.1119742396579941 | validation: 0.8767562251441754]
	TIME [epoch: 6.34 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7373949568839491		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.7373949568839491 | validation: 0.7578909624225149]
	TIME [epoch: 6.31 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7312629095183752		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.7312629095183752 | validation: 1.6448330324053484]
	TIME [epoch: 6.3 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1975763979789846		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 1.1975763979789846 | validation: 0.8952821340443524]
	TIME [epoch: 6.31 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.874448881723942		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.874448881723942 | validation: 1.7663473073684983]
	TIME [epoch: 6.31 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9989399163717956		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.9989399163717956 | validation: 0.9727753983223166]
	TIME [epoch: 6.31 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8123019024826129		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.8123019024826129 | validation: 0.860981414023185]
	TIME [epoch: 6.32 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.723575317235505		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.723575317235505 | validation: 0.7421333863684365]
	TIME [epoch: 6.31 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7291375110397537		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.7291375110397537 | validation: 0.9314326191773528]
	TIME [epoch: 6.31 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6720138947399239		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.6720138947399239 | validation: 0.6794069984851165]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6122315767373383		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.6122315767373383 | validation: 0.7824014390279119]
	TIME [epoch: 6.31 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.771537693765376		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.771537693765376 | validation: 1.005066089090809]
	TIME [epoch: 6.33 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8938842074971753		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.8938842074971753 | validation: 0.8938625640452631]
	TIME [epoch: 6.34 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6766144593389372		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.6766144593389372 | validation: 0.7859785663915125]
	TIME [epoch: 6.31 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7644972903936624		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.7644972903936624 | validation: 1.1498906800680215]
	TIME [epoch: 6.32 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3405738351029126		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 1.3405738351029126 | validation: 1.079476332250187]
	TIME [epoch: 6.31 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.774854652358316		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.774854652358316 | validation: 0.7411607394665105]
	TIME [epoch: 6.31 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8169000174828822		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.8169000174828822 | validation: 0.7556233738242644]
	TIME [epoch: 6.33 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7337783859030755		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.7337783859030755 | validation: 0.6725011984286516]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_549.pth
	Model improved!!!
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.696826671931376		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.696826671931376 | validation: 0.9215525405715792]
	TIME [epoch: 6.31 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7294143829642998		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.7294143829642998 | validation: 0.8593015470149747]
	TIME [epoch: 6.31 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8516953694659193		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.8516953694659193 | validation: 0.8404484540777083]
	TIME [epoch: 6.31 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8707302487383233		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.8707302487383233 | validation: 0.9827277917323822]
	TIME [epoch: 6.31 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8217384800045676		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.8217384800045676 | validation: 0.6614058092981816]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7170365465331597		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.7170365465331597 | validation: 0.8118801689144182]
	TIME [epoch: 6.35 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6709654365549259		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.6709654365549259 | validation: 1.5112266185975527]
	TIME [epoch: 6.31 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9577002646973941		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.9577002646973941 | validation: 0.8558269983740832]
	TIME [epoch: 6.31 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8069314430706869		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.8069314430706869 | validation: 0.690946335210843]
	TIME [epoch: 6.31 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7590140486703671		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.7590140486703671 | validation: 0.9711589005561665]
	TIME [epoch: 6.32 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8123226563787753		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.8123226563787753 | validation: 0.9425771945300481]
	TIME [epoch: 6.32 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6485902897107104		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.6485902897107104 | validation: 0.7702226107436458]
	TIME [epoch: 6.35 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8727867564846846		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.8727867564846846 | validation: 0.6995130572056854]
	TIME [epoch: 6.31 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6712538837626096		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.6712538837626096 | validation: 0.7650963908395543]
	TIME [epoch: 6.31 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7707283216559885		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.7707283216559885 | validation: 1.0036600942153286]
	TIME [epoch: 6.31 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8545072305717568		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.8545072305717568 | validation: 0.868161037367996]
	TIME [epoch: 6.31 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6640250471532645		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.6640250471532645 | validation: 0.6611591221837543]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7006888774206166		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.7006888774206166 | validation: 1.1709519400264323]
	TIME [epoch: 6.36 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7973804673814533		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.7973804673814533 | validation: 0.7743605231397372]
	TIME [epoch: 6.31 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6314407499654867		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.6314407499654867 | validation: 0.8916048986083522]
	TIME [epoch: 6.31 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6349118281372861		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.6349118281372861 | validation: 0.6581413921475274]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.708931677066327		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.708931677066327 | validation: 0.7234867842200889]
	TIME [epoch: 6.31 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.607890093169382		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.607890093169382 | validation: 0.7277300276023579]
	TIME [epoch: 6.32 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6580361686867396		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.6580361686867396 | validation: 0.9065987514524734]
	TIME [epoch: 6.36 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6738042968838359		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.6738042968838359 | validation: 0.821487033267283]
	TIME [epoch: 6.31 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7226352902376505		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.7226352902376505 | validation: 0.6851028432535735]
	TIME [epoch: 6.31 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6181770191966737		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.6181770191966737 | validation: 0.7731635699836819]
	TIME [epoch: 6.31 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7586406413313516		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.7586406413313516 | validation: 0.9026224021785543]
	TIME [epoch: 6.31 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6894281532526134		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.6894281532526134 | validation: 1.0168264299238545]
	TIME [epoch: 6.32 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6734780228085226		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.6734780228085226 | validation: 0.7014352530599983]
	TIME [epoch: 6.35 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6137612014453139		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.6137612014453139 | validation: 0.7019547333718561]
	TIME [epoch: 6.31 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.754362150465278		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.754362150465278 | validation: 1.0661410830645794]
	TIME [epoch: 6.31 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7071372304630041		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.7071372304630041 | validation: 0.7072288477575832]
	TIME [epoch: 6.31 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6771886170184813		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.6771886170184813 | validation: 0.712155863308897]
	TIME [epoch: 6.31 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7031981704401858		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.7031981704401858 | validation: 0.8429486006830242]
	TIME [epoch: 6.32 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2486187368045614		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 1.2486187368045614 | validation: 1.479311947028753]
	TIME [epoch: 6.35 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.065971305532862		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 1.065971305532862 | validation: 0.8910770324895394]
	TIME [epoch: 6.31 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0169627860676869		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 1.0169627860676869 | validation: 0.7816976757846348]
	TIME [epoch: 6.31 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6858001820061104		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.6858001820061104 | validation: 0.6993102358868108]
	TIME [epoch: 6.31 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6849094284464463		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.6849094284464463 | validation: 0.7121607597454263]
	TIME [epoch: 6.31 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7250230308975831		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.7250230308975831 | validation: 0.8346784744377262]
	TIME [epoch: 6.31 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7036351050249321		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.7036351050249321 | validation: 0.7131494401497334]
	TIME [epoch: 6.36 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6087649862613338		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.6087649862613338 | validation: 0.7021790760539889]
	TIME [epoch: 6.32 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6757931662654211		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.6757931662654211 | validation: 0.981321872741366]
	TIME [epoch: 6.31 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7928522720326725		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.7928522720326725 | validation: 0.7001308942548082]
	TIME [epoch: 6.31 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9643973981953979		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.9643973981953979 | validation: 0.9430051808221471]
	TIME [epoch: 6.31 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6680009592407121		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.6680009592407121 | validation: 0.804997203917345]
	TIME [epoch: 6.32 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7125524302226651		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.7125524302226651 | validation: 1.019128091436929]
	TIME [epoch: 6.35 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7274142972292247		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.7274142972292247 | validation: 0.6551551367022526]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7676058855377101		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.7676058855377101 | validation: 0.8243885768199142]
	TIME [epoch: 6.31 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6860419062547793		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.6860419062547793 | validation: 0.7038071423976484]
	TIME [epoch: 6.31 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5800271845640457		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.5800271845640457 | validation: 0.6711827203154204]
	TIME [epoch: 6.31 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6724826323099116		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.6724826323099116 | validation: 0.7250026563311791]
	TIME [epoch: 6.31 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7933897604222951		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.7933897604222951 | validation: 0.7437834470715607]
	TIME [epoch: 6.36 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6029115783484646		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.6029115783484646 | validation: 0.5762459830987144]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6074770666838021		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.6074770666838021 | validation: 0.7433058428478851]
	TIME [epoch: 6.32 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5847990581021959		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.5847990581021959 | validation: 0.7351361571186397]
	TIME [epoch: 6.31 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7290924189428422		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.7290924189428422 | validation: 0.6538150723726376]
	TIME [epoch: 6.31 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9845030871625202		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.9845030871625202 | validation: 1.065331026887506]
	TIME [epoch: 6.31 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7876751485856373		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.7876751485856373 | validation: 0.6268180175954217]
	TIME [epoch: 6.35 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5882477879319936		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.5882477879319936 | validation: 0.6138770329535302]
	TIME [epoch: 6.31 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5348956423300273		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.5348956423300273 | validation: 0.7237055473748298]
	TIME [epoch: 6.31 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6152192204881022		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.6152192204881022 | validation: 0.5899990082890904]
	TIME [epoch: 6.31 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.675348988118094		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.675348988118094 | validation: 1.182977183549655]
	TIME [epoch: 6.31 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7835986601395865		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.7835986601395865 | validation: 0.7498074055568926]
	TIME [epoch: 6.31 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5967914318514838		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.5967914318514838 | validation: 0.6278692540219614]
	TIME [epoch: 6.37 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7286772023443292		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.7286772023443292 | validation: 0.6416631062936486]
	TIME [epoch: 6.31 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5796347724895102		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.5796347724895102 | validation: 0.7562655777162792]
	TIME [epoch: 6.31 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6576176532738469		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.6576176532738469 | validation: 0.654854071944178]
	TIME [epoch: 6.32 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9975280993291669		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.9975280993291669 | validation: 0.906823484066182]
	TIME [epoch: 6.32 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7063333786003454		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.7063333786003454 | validation: 0.7781781417513709]
	TIME [epoch: 6.31 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9038696086032908		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.9038696086032908 | validation: 1.2062722357207658]
	TIME [epoch: 6.35 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8790778037568739		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.8790778037568739 | validation: 0.9705873103423073]
	TIME [epoch: 6.33 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.128856661441611		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 1.128856661441611 | validation: 0.9157030308057621]
	TIME [epoch: 6.31 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7982734776923122		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.7982734776923122 | validation: 0.6498791019681948]
	TIME [epoch: 6.31 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5693650909378937		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.5693650909378937 | validation: 0.6517797503445135]
	TIME [epoch: 6.31 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5822210775980898		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.5822210775980898 | validation: 0.6410086303149751]
	TIME [epoch: 6.32 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5789559934245797		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.5789559934245797 | validation: 0.8438439501903476]
	TIME [epoch: 6.35 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7407511596564593		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.7407511596564593 | validation: 0.9313333316461785]
	TIME [epoch: 6.33 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1919494997570173		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 1.1919494997570173 | validation: 0.997594085128116]
	TIME [epoch: 6.31 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7764449674746408		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.7764449674746408 | validation: 0.7285586051413915]
	TIME [epoch: 6.31 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6397698848024624		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.6397698848024624 | validation: 0.6653543125476578]
	TIME [epoch: 6.31 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.607543685720773		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.607543685720773 | validation: 0.6048842227425283]
	TIME [epoch: 6.31 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6143034438012084		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.6143034438012084 | validation: 1.116737844095492]
	TIME [epoch: 6.34 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8643344837194642		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.8643344837194642 | validation: 0.7793289443103948]
	TIME [epoch: 6.34 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.800468163045237		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.800468163045237 | validation: 0.6369299545320484]
	TIME [epoch: 6.32 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.578431017493637		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.578431017493637 | validation: 0.6421957350292024]
	TIME [epoch: 6.31 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6270650629719448		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.6270650629719448 | validation: 0.9337109278365608]
	TIME [epoch: 6.31 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7417517379723391		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.7417517379723391 | validation: 0.7657943358308159]
	TIME [epoch: 6.31 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.748888094802084		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.748888094802084 | validation: 0.6763000650024542]
	TIME [epoch: 6.34 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6056762774568477		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.6056762774568477 | validation: 0.7661131718052974]
	TIME [epoch: 6.35 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6366955559890477		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.6366955559890477 | validation: 0.842401843192353]
	TIME [epoch: 6.32 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6358475508763303		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.6358475508763303 | validation: 0.6781945145283457]
	TIME [epoch: 6.31 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.57385820897859		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.57385820897859 | validation: 0.8214392705117728]
	TIME [epoch: 6.32 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5978945709293038		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.5978945709293038 | validation: 0.618866942630498]
	TIME [epoch: 6.31 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7314974841744628		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.7314974841744628 | validation: 0.8038859006360481]
	TIME [epoch: 6.34 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6829777191797757		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.6829777191797757 | validation: 0.7309701553552597]
	TIME [epoch: 6.34 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6479446013934244		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.6479446013934244 | validation: 0.5492588043193682]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_647.pth
	Model improved!!!
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5746449467900752		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.5746449467900752 | validation: 0.7279521873219743]
	TIME [epoch: 6.32 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6382323976979494		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.6382323976979494 | validation: 0.6170477694269705]
	TIME [epoch: 6.31 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6186146283679976		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.6186146283679976 | validation: 0.8040638470066903]
	TIME [epoch: 6.31 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7105433185717248		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.7105433185717248 | validation: 0.6234540647720688]
	TIME [epoch: 6.34 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5268958479512665		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.5268958479512665 | validation: 0.8225306695637983]
	TIME [epoch: 6.35 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6611848542409271		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.6611848542409271 | validation: 0.6576063325545494]
	TIME [epoch: 6.32 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5743524265231474		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.5743524265231474 | validation: 0.6873218207767864]
	TIME [epoch: 6.32 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6644182619314418		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.6644182619314418 | validation: 0.6023658206318481]
	TIME [epoch: 6.31 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6546592854984762		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.6546592854984762 | validation: 0.6396579752841318]
	TIME [epoch: 6.32 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5654212341041286		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.5654212341041286 | validation: 0.6061471869624102]
	TIME [epoch: 6.33 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5869127552303286		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.5869127552303286 | validation: 0.8636745582349077]
	TIME [epoch: 6.36 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6235029799011772		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.6235029799011772 | validation: 0.7810384391771431]
	TIME [epoch: 6.32 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7943591790927431		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.7943591790927431 | validation: 0.6012444047956349]
	TIME [epoch: 6.32 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5727001044928156		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.5727001044928156 | validation: 0.5901337018010316]
	TIME [epoch: 6.31 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5865338071865975		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.5865338071865975 | validation: 0.5441731413499555]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.50902769951415		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.50902769951415 | validation: 0.8467070258683149]
	TIME [epoch: 6.32 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.575906895031664		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.575906895031664 | validation: 0.5732334300354942]
	TIME [epoch: 6.35 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.462517204960279		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.462517204960279 | validation: 0.6283768854949854]
	TIME [epoch: 6.31 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5084177036448058		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.5084177036448058 | validation: 0.5482107157542075]
	TIME [epoch: 6.32 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5069093584497381		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.5069093584497381 | validation: 0.5492771724231877]
	TIME [epoch: 6.31 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4867928427466305		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.4867928427466305 | validation: 0.642097167942509]
	TIME [epoch: 6.31 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49889594079884103		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.49889594079884103 | validation: 0.5731273595635074]
	TIME [epoch: 6.32 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5651352952462583		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.5651352952462583 | validation: 0.6601031355772486]
	TIME [epoch: 6.35 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6798107389644992		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.6798107389644992 | validation: 1.0017127286600234]
	TIME [epoch: 6.31 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6497871978458989		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.6497871978458989 | validation: 0.5448171673183142]
	TIME [epoch: 6.31 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5764106581749441		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.5764106581749441 | validation: 0.6943341785390909]
	TIME [epoch: 6.31 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5965246356519841		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.5965246356519841 | validation: 0.7139648632074014]
	TIME [epoch: 6.31 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5659915655021076		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.5659915655021076 | validation: 0.5261234860092495]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6077341078444035		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.6077341078444035 | validation: 0.7926795959783985]
	TIME [epoch: 6.37 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6935298068640614		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.6935298068640614 | validation: 0.7725049102116603]
	TIME [epoch: 6.32 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5769092909752822		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.5769092909752822 | validation: 0.7921143294740816]
	TIME [epoch: 6.31 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6481711106542258		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.6481711106542258 | validation: 0.7073769109764325]
	TIME [epoch: 6.31 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5882248405845634		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.5882248405845634 | validation: 0.9554677655791854]
	TIME [epoch: 6.31 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.649943352804138		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.649943352804138 | validation: 0.5961338590927483]
	TIME [epoch: 6.33 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5190717602397773		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.5190717602397773 | validation: 0.5273969652310436]
	TIME [epoch: 6.35 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.510402015277171		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.510402015277171 | validation: 0.8865710574983042]
	TIME [epoch: 6.32 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5613528992057155		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.5613528992057155 | validation: 1.1457729757138342]
	TIME [epoch: 6.31 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9089759480263553		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.9089759480263553 | validation: 1.3679861746869089]
	TIME [epoch: 6.31 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4232013587145842		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 1.4232013587145842 | validation: 0.9839084183828581]
	TIME [epoch: 6.31 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7061493269404076		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.7061493269404076 | validation: 0.7473594056872797]
	TIME [epoch: 6.32 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6350265070210869		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.6350265070210869 | validation: 0.84886650967099]
	TIME [epoch: 6.35 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.666858057838393		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.666858057838393 | validation: 0.8527506181850261]
	TIME [epoch: 6.32 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6257101554146036		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.6257101554146036 | validation: 0.4752201962142854]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_690.pth
	Model improved!!!
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5108811391706788		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.5108811391706788 | validation: 0.4935033521978549]
	TIME [epoch: 6.32 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4537588337437095		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.4537588337437095 | validation: 0.5496287463605352]
	TIME [epoch: 6.32 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4792206224623591		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.4792206224623591 | validation: 0.4931441504695184]
	TIME [epoch: 6.32 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48370457183571114		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.48370457183571114 | validation: 0.5450474355989949]
	TIME [epoch: 6.36 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47729065326756526		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.47729065326756526 | validation: 0.5323216638053172]
	TIME [epoch: 6.32 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5488489003953936		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.5488489003953936 | validation: 0.5891429443000028]
	TIME [epoch: 6.32 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5755048588381657		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.5755048588381657 | validation: 0.6685405084608167]
	TIME [epoch: 6.32 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5563843248819527		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.5563843248819527 | validation: 0.5854145130200219]
	TIME [epoch: 6.32 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5436836529310562		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.5436836529310562 | validation: 0.7788179495020509]
	TIME [epoch: 6.32 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6027772900711775		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.6027772900711775 | validation: 0.6038340581910591]
	TIME [epoch: 6.36 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5344140818795052		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.5344140818795052 | validation: 0.5287057063755782]
	TIME [epoch: 6.32 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5027567985938506		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.5027567985938506 | validation: 0.554230008377455]
	TIME [epoch: 6.32 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5380784153842304		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.5380784153842304 | validation: 0.6007643539152947]
	TIME [epoch: 6.32 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6318710722487625		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.6318710722487625 | validation: 0.643704532360047]
	TIME [epoch: 6.32 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5696019391281112		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.5696019391281112 | validation: 0.4890329085956314]
	TIME [epoch: 6.31 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6113215543499944		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.6113215543499944 | validation: 0.498769150225793]
	TIME [epoch: 6.36 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5427434611667474		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.5427434611667474 | validation: 0.5528297583245843]
	TIME [epoch: 6.33 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4994804553827323		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.4994804553827323 | validation: 0.5425190635684707]
	TIME [epoch: 6.32 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45419375336753304		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.45419375336753304 | validation: 0.5258097772142517]
	TIME [epoch: 6.32 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4914855731923018		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.4914855731923018 | validation: 0.4747022243626092]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_710.pth
	Model improved!!!
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48762054099667707		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.48762054099667707 | validation: 0.6092862331690034]
	TIME [epoch: 6.32 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5564168869351148		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.5564168869351148 | validation: 0.6345611097488777]
	TIME [epoch: 6.34 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5381130872601757		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.5381130872601757 | validation: 0.9704173588141465]
	TIME [epoch: 6.32 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7029011174842359		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.7029011174842359 | validation: 0.792611068477553]
	TIME [epoch: 6.31 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6096449533251509		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.6096449533251509 | validation: 0.5281066327276003]
	TIME [epoch: 6.31 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4844947269957771		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.4844947269957771 | validation: 0.614832914938227]
	TIME [epoch: 6.31 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5500367608264455		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.5500367608264455 | validation: 0.5880020637232605]
	TIME [epoch: 6.31 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5109339252467845		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.5109339252467845 | validation: 0.49213091663497965]
	TIME [epoch: 6.35 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5159584004918284		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.5159584004918284 | validation: 0.6528544000145265]
	TIME [epoch: 6.32 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5282300586805637		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.5282300586805637 | validation: 0.5190260246825388]
	TIME [epoch: 6.31 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5716309480752587		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.5716309480752587 | validation: 0.6481207240277118]
	TIME [epoch: 6.31 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4949241275485936		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.4949241275485936 | validation: 0.6897863025968585]
	TIME [epoch: 6.31 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5480093432229283		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.5480093432229283 | validation: 0.5497148607446352]
	TIME [epoch: 6.31 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4593390382118778		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.4593390382118778 | validation: 0.45822777206795906]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_724.pth
	Model improved!!!
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5594104434409983		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.5594104434409983 | validation: 0.5883896814158556]
	TIME [epoch: 6.33 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5744603694653598		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.5744603694653598 | validation: 0.8701686267309665]
	TIME [epoch: 6.31 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6689839956058371		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.6689839956058371 | validation: 0.6790259244566792]
	TIME [epoch: 6.31 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8007938917350905		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.8007938917350905 | validation: 1.463656424588439]
	TIME [epoch: 6.31 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.014098518687235		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 1.014098518687235 | validation: 0.7823407428474138]
	TIME [epoch: 6.31 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6120543190080608		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.6120543190080608 | validation: 0.7613975310239909]
	TIME [epoch: 6.34 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.527437312487953		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.527437312487953 | validation: 0.5813542953483934]
	TIME [epoch: 6.33 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6614050510059963		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.6614050510059963 | validation: 1.1753897926751258]
	TIME [epoch: 6.31 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6969677112925765		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.6969677112925765 | validation: 0.5624653672266903]
	TIME [epoch: 6.31 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5292359190495971		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.5292359190495971 | validation: 0.6120191548804592]
	TIME [epoch: 6.31 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5469193074207521		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.5469193074207521 | validation: 0.594973234878426]
	TIME [epoch: 6.31 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48600419865510425		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.48600419865510425 | validation: 0.561264752139117]
	TIME [epoch: 6.35 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4966529191693349		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.4966529191693349 | validation: 0.46184969352918]
	TIME [epoch: 6.32 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43056918955659007		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.43056918955659007 | validation: 0.4976365306458266]
	TIME [epoch: 6.31 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5258251716285731		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.5258251716285731 | validation: 0.5535788399657167]
	TIME [epoch: 6.31 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5056612028414503		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.5056612028414503 | validation: 0.5613979385788969]
	TIME [epoch: 6.31 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5006856866397904		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.5006856866397904 | validation: 0.7544568079080516]
	TIME [epoch: 6.31 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6395331366279691		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.6395331366279691 | validation: 0.8343175568909631]
	TIME [epoch: 6.33 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5268110825434411		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.5268110825434411 | validation: 0.6957586434593709]
	TIME [epoch: 6.34 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5775008090128284		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.5775008090128284 | validation: 0.4923729326796249]
	TIME [epoch: 6.32 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5257104892076915		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.5257104892076915 | validation: 0.5357426011675047]
	TIME [epoch: 6.31 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.495875906982633		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.495875906982633 | validation: 0.5847966419542189]
	TIME [epoch: 6.31 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.518237068892323		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.518237068892323 | validation: 0.7238274042758601]
	TIME [epoch: 6.3 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5304022016819318		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.5304022016819318 | validation: 0.6035031569221316]
	TIME [epoch: 6.32 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5413438591472944		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.5413438591472944 | validation: 0.5611896129456354]
	TIME [epoch: 6.33 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4953834832106407		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.4953834832106407 | validation: 0.6898516848964658]
	TIME [epoch: 6.31 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5600561243272267		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.5600561243272267 | validation: 0.56520215067063]
	TIME [epoch: 6.31 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49768978795098373		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.49768978795098373 | validation: 0.5841518246143764]
	TIME [epoch: 6.31 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5192371571296098		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.5192371571296098 | validation: 0.6995258119116654]
	TIME [epoch: 6.31 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5609304408779809		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.5609304408779809 | validation: 0.6036214237029806]
	TIME [epoch: 6.33 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5348552794827669		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.5348552794827669 | validation: 0.5190317762295307]
	TIME [epoch: 6.34 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4692169545653678		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.4692169545653678 | validation: 0.48932938594337805]
	TIME [epoch: 6.31 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5432788558923538		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.5432788558923538 | validation: 0.6415665996436336]
	TIME [epoch: 6.31 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5786937122941231		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.5786937122941231 | validation: 1.0362901965814002]
	TIME [epoch: 6.31 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7990600435936567		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.7990600435936567 | validation: 0.7386656698864551]
	TIME [epoch: 6.31 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5486071968508418		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.5486071968508418 | validation: 0.5021394270245855]
	TIME [epoch: 6.32 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4606822697956539		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.4606822697956539 | validation: 0.48294892108693166]
	TIME [epoch: 6.35 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41349630743564403		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.41349630743564403 | validation: 0.5398563239747407]
	TIME [epoch: 6.31 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48185325636354204		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.48185325636354204 | validation: 0.5009756816712371]
	TIME [epoch: 6.31 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4896727769059153		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.4896727769059153 | validation: 0.5656758616210475]
	TIME [epoch: 6.31 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5765094988839123		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.5765094988839123 | validation: 0.5469917209203939]
	TIME [epoch: 6.31 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48897023786414806		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.48897023786414806 | validation: 0.5079697506353571]
	TIME [epoch: 6.32 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.486722295788835		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.486722295788835 | validation: 0.4874601958033101]
	TIME [epoch: 6.35 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4393285236463124		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.4393285236463124 | validation: 0.5932338601154938]
	TIME [epoch: 6.32 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46692040879981866		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.46692040879981866 | validation: 0.5352160426791706]
	TIME [epoch: 6.31 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4781519861222806		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.4781519861222806 | validation: 0.47296977239324384]
	TIME [epoch: 6.31 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44908268963593184		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.44908268963593184 | validation: 0.6629767781953587]
	TIME [epoch: 6.31 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5583880015033842		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.5583880015033842 | validation: 0.5001154210367023]
	TIME [epoch: 6.32 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46731930797067356		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.46731930797067356 | validation: 0.5395118377501873]
	TIME [epoch: 6.35 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4543226853713891		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.4543226853713891 | validation: 0.5354849199630265]
	TIME [epoch: 6.31 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45419965027007514		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.45419965027007514 | validation: 0.6793624112924663]
	TIME [epoch: 6.31 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4902195528441317		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.4902195528441317 | validation: 0.4540321785019984]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_776.pth
	Model improved!!!
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4491197745780799		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.4491197745780799 | validation: 0.6415310530385729]
	TIME [epoch: 6.31 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5603786870845114		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.5603786870845114 | validation: 0.5014633137179664]
	TIME [epoch: 6.32 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4969322426005261		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.4969322426005261 | validation: 0.4891567271302599]
	TIME [epoch: 6.35 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44418261918996926		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.44418261918996926 | validation: 0.5936227787111112]
	TIME [epoch: 6.32 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6123749430390455		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.6123749430390455 | validation: 0.7576795827794782]
	TIME [epoch: 6.31 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5740130672184187		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.5740130672184187 | validation: 0.5830950293737033]
	TIME [epoch: 6.31 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4740379923065733		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.4740379923065733 | validation: 0.5416178982158387]
	TIME [epoch: 6.32 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5764899270742654		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.5764899270742654 | validation: 0.5581872267376727]
	TIME [epoch: 6.32 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4555763383288244		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.4555763383288244 | validation: 0.443378793836925]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5139694862041333		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.5139694862041333 | validation: 0.5334011658632468]
	TIME [epoch: 6.32 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5080484184572434		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.5080484184572434 | validation: 0.5185772550577149]
	TIME [epoch: 6.31 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44822546209496633		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.44822546209496633 | validation: 0.48269480343685084]
	TIME [epoch: 6.32 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6872840280983191		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.6872840280983191 | validation: 0.5197347553711014]
	TIME [epoch: 6.31 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7020473720022743		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.7020473720022743 | validation: 1.0128041536361148]
	TIME [epoch: 6.32 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6768726671448047		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.6768726671448047 | validation: 0.6866924876835813]
	TIME [epoch: 6.35 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8179888152343748		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.8179888152343748 | validation: 0.7588611400291031]
	TIME [epoch: 6.32 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7823419225965331		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.7823419225965331 | validation: 0.5077871317066865]
	TIME [epoch: 6.31 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5567904891883192		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.5567904891883192 | validation: 0.5501615063589471]
	TIME [epoch: 6.31 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.549112178856105		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.549112178856105 | validation: 0.5130344074301614]
	TIME [epoch: 6.31 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4962503744878317		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.4962503744878317 | validation: 0.6466015367350314]
	TIME [epoch: 6.31 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5246875931102551		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.5246875931102551 | validation: 0.4643161802368384]
	TIME [epoch: 6.35 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4209323396955339		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.4209323396955339 | validation: 0.4880434273284875]
	TIME [epoch: 6.32 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4492588378218173		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.4492588378218173 | validation: 0.4516132006306681]
	TIME [epoch: 6.32 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44511332912387913		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.44511332912387913 | validation: 0.5348731708828205]
	TIME [epoch: 6.31 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5285486432425567		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.5285486432425567 | validation: 0.4795863557966621]
	TIME [epoch: 6.31 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.509170730588088		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.509170730588088 | validation: 0.5290448585437055]
	TIME [epoch: 6.31 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48421231903517853		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.48421231903517853 | validation: 0.5674911128267055]
	TIME [epoch: 6.36 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46520547057201084		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.46520547057201084 | validation: 0.4803560377272379]
	TIME [epoch: 6.32 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4838464069573286		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.4838464069573286 | validation: 0.429631043246368]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_805.pth
	Model improved!!!
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4021294841794686		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.4021294841794686 | validation: 0.4349787080407209]
	TIME [epoch: 6.31 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4342586455602018		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.4342586455602018 | validation: 0.46590954559126496]
	TIME [epoch: 6.31 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5365983057146035		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.5365983057146035 | validation: 0.6983294973164947]
	TIME [epoch: 6.31 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5470217157964135		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.5470217157964135 | validation: 0.6099133529363281]
	TIME [epoch: 6.36 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5917717625867357		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.5917717625867357 | validation: 0.5365607020739349]
	TIME [epoch: 6.32 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4610069582685049		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.4610069582685049 | validation: 0.5190197530692335]
	TIME [epoch: 6.31 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.507669985559265		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.507669985559265 | validation: 0.5873134896152784]
	TIME [epoch: 6.31 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48239315380257347		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.48239315380257347 | validation: 0.46605854786388945]
	TIME [epoch: 6.31 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4349985405635977		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.4349985405635977 | validation: 0.5378617284374569]
	TIME [epoch: 6.31 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46337362452604736		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.46337362452604736 | validation: 0.5693399626272722]
	TIME [epoch: 6.35 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5934720812717695		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.5934720812717695 | validation: 0.5627190783247882]
	TIME [epoch: 6.33 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5075831687187232		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.5075831687187232 | validation: 0.48450491773679133]
	TIME [epoch: 6.31 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43234289880468474		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.43234289880468474 | validation: 0.4505369972841117]
	TIME [epoch: 6.31 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4183204152031852		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.4183204152031852 | validation: 0.5257807615551144]
	TIME [epoch: 6.31 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44753680665088935		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.44753680665088935 | validation: 0.534486236761606]
	TIME [epoch: 6.31 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44571149905339835		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.44571149905339835 | validation: 0.511728672170763]
	TIME [epoch: 6.34 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41759542073674094		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.41759542073674094 | validation: 0.4586743222183668]
	TIME [epoch: 6.33 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43166390369264646		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.43166390369264646 | validation: 0.4520524741839611]
	TIME [epoch: 6.31 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45260374638326545		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.45260374638326545 | validation: 0.4656900225725154]
	TIME [epoch: 6.31 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5549458886147208		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.5549458886147208 | validation: 0.6814372410135257]
	TIME [epoch: 6.31 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5618754266659306		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.5618754266659306 | validation: 0.49010739915582]
	TIME [epoch: 6.31 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4437759007828676		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.4437759007828676 | validation: 0.4121052349320721]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_827.pth
	Model improved!!!
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4763212610483216		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.4763212610483216 | validation: 0.6956116617392784]
	TIME [epoch: 6.35 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4825102467694816		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.4825102467694816 | validation: 0.7148782217178142]
	TIME [epoch: 6.31 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8059992618220131		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.8059992618220131 | validation: 0.5409450595533452]
	TIME [epoch: 6.31 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5546675216812504		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.5546675216812504 | validation: 0.5771236211385355]
	TIME [epoch: 6.31 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5403904521870004		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.5403904521870004 | validation: 0.6204722285509696]
	TIME [epoch: 6.31 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4869886789043344		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.4869886789043344 | validation: 0.5297998448133059]
	TIME [epoch: 6.33 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4582234406092065		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.4582234406092065 | validation: 0.464454109419855]
	TIME [epoch: 6.34 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4257280794850164		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.4257280794850164 | validation: 0.4267610440812131]
	TIME [epoch: 6.31 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4113524278091778		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.4113524278091778 | validation: 0.5993076444169232]
	TIME [epoch: 6.32 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46987241796583573		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.46987241796583573 | validation: 0.4773518822826814]
	TIME [epoch: 6.31 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4382205936045483		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.4382205936045483 | validation: 0.4493747819386693]
	TIME [epoch: 6.31 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4196452401338359		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.4196452401338359 | validation: 0.4087697629355932]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_839.pth
	Model improved!!!
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37381856613720554		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.37381856613720554 | validation: 0.39759650531229473]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_840.pth
	Model improved!!!
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43462257992344433		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.43462257992344433 | validation: 0.5823871336183883]
	TIME [epoch: 6.32 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5073995719201229		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.5073995719201229 | validation: 0.6830088740562388]
	TIME [epoch: 6.31 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6888094616993818		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.6888094616993818 | validation: 0.6488481548148874]
	TIME [epoch: 6.32 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6905143193006047		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.6905143193006047 | validation: 0.600076078414064]
	TIME [epoch: 6.31 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6229402339870899		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.6229402339870899 | validation: 0.5588037888574133]
	TIME [epoch: 6.34 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5688622400268946		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.5688622400268946 | validation: 0.45389572515496945]
	TIME [epoch: 6.34 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5506259441148016		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.5506259441148016 | validation: 0.4698764226222932]
	TIME [epoch: 6.32 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5128138850426065		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.5128138850426065 | validation: 0.44227532304787853]
	TIME [epoch: 6.31 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43649426960598336		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.43649426960598336 | validation: 0.45705131008591793]
	TIME [epoch: 6.31 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43961942226075		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.43961942226075 | validation: 0.4818263147390556]
	TIME [epoch: 6.31 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4823376174528035		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.4823376174528035 | validation: 0.5486141476226896]
	TIME [epoch: 6.33 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5412807439345841		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.5412807439345841 | validation: 0.546764928650775]
	TIME [epoch: 6.34 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46632877167526277		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.46632877167526277 | validation: 0.5731113208373586]
	TIME [epoch: 6.31 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4512637010510585		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.4512637010510585 | validation: 0.4528972512832548]
	TIME [epoch: 6.31 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4209882429232181		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.4209882429232181 | validation: 0.39891797087738046]
	TIME [epoch: 6.31 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3805611466220913		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.3805611466220913 | validation: 0.42690441020003295]
	TIME [epoch: 6.31 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36946488868427363		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.36946488868427363 | validation: 0.4326028317557855]
	TIME [epoch: 6.34 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42932525937082233		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.42932525937082233 | validation: 0.40730640476190777]
	TIME [epoch: 6.33 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4196059307800622		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.4196059307800622 | validation: 0.39669943670864405]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_859.pth
	Model improved!!!
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39139010894480686		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.39139010894480686 | validation: 0.4528243445000695]
	TIME [epoch: 6.31 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42825603676447455		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.42825603676447455 | validation: 0.4306026867947309]
	TIME [epoch: 6.31 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4675805984354493		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.4675805984354493 | validation: 1.116871223098896]
	TIME [epoch: 6.31 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2305323919716964		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 1.2305323919716964 | validation: 0.9432986170932629]
	TIME [epoch: 6.33 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6195971691282572		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.6195971691282572 | validation: 0.49584365348122456]
	TIME [epoch: 6.34 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4163455574403224		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.4163455574403224 | validation: 0.5074478845751189]
	TIME [epoch: 6.32 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5519465682068309		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.5519465682068309 | validation: 0.4722998910158257]
	TIME [epoch: 6.32 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47620580454821226		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.47620580454821226 | validation: 0.4675671900890264]
	TIME [epoch: 6.31 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3819871770522015		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.3819871770522015 | validation: 0.4269016889281035]
	TIME [epoch: 6.31 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5008328779006728		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.5008328779006728 | validation: 0.38909200804602206]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_869.pth
	Model improved!!!
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43217944999221247		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.43217944999221247 | validation: 0.4128054065581017]
	TIME [epoch: 6.35 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4138900971556159		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.4138900971556159 | validation: 0.3632894318958726]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_871.pth
	Model improved!!!
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38474830860065534		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.38474830860065534 | validation: 0.46806699421494036]
	TIME [epoch: 6.31 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4152463887635706		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.4152463887635706 | validation: 0.4353792616936991]
	TIME [epoch: 6.32 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40567319406288294		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.40567319406288294 | validation: 0.3677603108631929]
	TIME [epoch: 6.32 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37955233715692394		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.37955233715692394 | validation: 0.5613495452011575]
	TIME [epoch: 6.34 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4469868776406272		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.4469868776406272 | validation: 0.42985381907806025]
	TIME [epoch: 6.35 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4288028894675164		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.4288028894675164 | validation: 0.351802699109502]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_877.pth
	Model improved!!!
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36471792314822754		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.36471792314822754 | validation: 0.36677769432422913]
	TIME [epoch: 6.31 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36010150460431967		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.36010150460431967 | validation: 0.36607688055622223]
	TIME [epoch: 6.32 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4234886267027802		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.4234886267027802 | validation: 0.492104926697118]
	TIME [epoch: 6.31 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44595184158275925		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.44595184158275925 | validation: 0.4039194266593885]
	TIME [epoch: 6.33 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37947303201970545		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.37947303201970545 | validation: 0.3722817837235155]
	TIME [epoch: 6.35 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3516353323904472		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.3516353323904472 | validation: 0.3778235966646651]
	TIME [epoch: 6.31 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3521711144852906		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.3521711144852906 | validation: 0.3983821883701081]
	TIME [epoch: 6.32 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44047671832829494		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.44047671832829494 | validation: 0.3954638285229044]
	TIME [epoch: 6.31 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3939852890581076		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.3939852890581076 | validation: 0.4036800911738814]
	TIME [epoch: 6.31 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3678149277458542		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.3678149277458542 | validation: 0.37887906765912827]
	TIME [epoch: 6.33 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3992161737648894		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.3992161737648894 | validation: 0.7194318944477605]
	TIME [epoch: 6.35 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46499141624547513		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.46499141624547513 | validation: 0.4388973905602288]
	TIME [epoch: 6.32 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39827044341665824		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.39827044341665824 | validation: 0.5691979998273442]
	TIME [epoch: 6.31 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46014055718693747		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.46014055718693747 | validation: 0.49858463528099894]
	TIME [epoch: 6.31 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5674012149603724		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.5674012149603724 | validation: 0.4685001782750523]
	TIME [epoch: 6.31 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39698626453454444		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.39698626453454444 | validation: 0.41712068004546315]
	TIME [epoch: 6.32 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39654998427262855		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.39654998427262855 | validation: 0.4270988920944907]
	TIME [epoch: 6.36 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39410376265120517		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.39410376265120517 | validation: 0.5235664808140563]
	TIME [epoch: 6.31 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44942687587850527		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.44942687587850527 | validation: 0.420157990753078]
	TIME [epoch: 6.31 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4218070858102557		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.4218070858102557 | validation: 0.4274227616013651]
	TIME [epoch: 6.31 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42709496152881377		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.42709496152881377 | validation: 0.536075206114861]
	TIME [epoch: 6.31 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.403545455404648		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.403545455404648 | validation: 0.4006890759202206]
	TIME [epoch: 6.32 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41945830699498715		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.41945830699498715 | validation: 0.41788089939871276]
	TIME [epoch: 6.36 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43525205854422566		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.43525205854422566 | validation: 0.4144889390090062]
	TIME [epoch: 6.31 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38507970119516544		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.38507970119516544 | validation: 0.4045948069564679]
	TIME [epoch: 6.31 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43107197445444123		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.43107197445444123 | validation: 0.42270926497042904]
	TIME [epoch: 6.31 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4412814861684719		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.4412814861684719 | validation: 0.4699520799844866]
	TIME [epoch: 6.31 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42975408187522723		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.42975408187522723 | validation: 0.43339136563055003]
	TIME [epoch: 6.31 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37775006589467586		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.37775006589467586 | validation: 0.38625555755562]
	TIME [epoch: 6.35 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45147033489542315		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.45147033489542315 | validation: 0.478176066261911]
	TIME [epoch: 6.32 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40190502700459385		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.40190502700459385 | validation: 0.4038783562383659]
	TIME [epoch: 6.31 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5046496544296075		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.5046496544296075 | validation: 0.6004674576594664]
	TIME [epoch: 6.31 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4997938214607598		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.4997938214607598 | validation: 0.42996812334879875]
	TIME [epoch: 6.31 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.436805633100281		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.436805633100281 | validation: 0.5105719248529588]
	TIME [epoch: 6.32 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46834322959230046		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.46834322959230046 | validation: 0.4168183886718149]
	TIME [epoch: 6.35 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41246298644438384		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.41246298644438384 | validation: 0.4967712989200188]
	TIME [epoch: 6.32 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4414150775312604		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.4414150775312604 | validation: 0.38999219160984566]
	TIME [epoch: 6.31 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36110981941559606		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.36110981941559606 | validation: 0.3991153206837781]
	TIME [epoch: 6.31 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35383683544099565		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.35383683544099565 | validation: 0.37835257220772534]
	TIME [epoch: 6.31 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38833947564678994		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.38833947564678994 | validation: 0.37186937908363776]
	TIME [epoch: 6.31 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36295895621613894		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.36295895621613894 | validation: 0.4602222350346384]
	TIME [epoch: 6.36 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3653114950941072		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.3653114950941072 | validation: 0.3980684225842072]
	TIME [epoch: 6.32 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3955115540972463		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.3955115540972463 | validation: 0.39608178584666454]
	TIME [epoch: 6.31 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49916636309875		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.49916636309875 | validation: 0.598472284495102]
	TIME [epoch: 6.31 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46945272053791076		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.46945272053791076 | validation: 0.42026519086541547]
	TIME [epoch: 6.31 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3938774913720712		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.3938774913720712 | validation: 0.5742392306701252]
	TIME [epoch: 6.32 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48703195619907774		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.48703195619907774 | validation: 0.449088274385966]
	TIME [epoch: 6.36 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4361330176408152		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.4361330176408152 | validation: 0.5738347378676439]
	TIME [epoch: 6.32 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.445373068191106		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.445373068191106 | validation: 0.4026979025151254]
	TIME [epoch: 6.31 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36405682018147395		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.36405682018147395 | validation: 0.38701075865893353]
	TIME [epoch: 6.31 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35250050870689387		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.35250050870689387 | validation: 0.37203917949926735]
	TIME [epoch: 6.31 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35938196677571405		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.35938196677571405 | validation: 0.3925616640677862]
	TIME [epoch: 6.31 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3648167193638483		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.3648167193638483 | validation: 0.3810527639068285]
	TIME [epoch: 6.35 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.393053073160543		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.393053073160543 | validation: 0.41416081172572927]
	TIME [epoch: 6.33 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36063019711259725		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.36063019711259725 | validation: 0.43023604532206616]
	TIME [epoch: 6.31 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36689428034645377		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.36689428034645377 | validation: 0.38269017760444146]
	TIME [epoch: 6.31 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3690730051649397		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.3690730051649397 | validation: 0.4245485459468139]
	TIME [epoch: 6.31 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3910735382528815		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.3910735382528815 | validation: 0.4371366141608869]
	TIME [epoch: 6.31 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3748622838794288		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.3748622838794288 | validation: 0.37997048165265757]
	TIME [epoch: 6.34 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3757795361118693		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.3757795361118693 | validation: 0.35118916612450024]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_937.pth
	Model improved!!!
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34860553881360135		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.34860553881360135 | validation: 0.4240660027328026]
	TIME [epoch: 6.32 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3885288148138489		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.3885288148138489 | validation: 0.4357628087803837]
	TIME [epoch: 6.31 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44725846364251426		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.44725846364251426 | validation: 0.4463703495491973]
	TIME [epoch: 6.32 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40207053212067045		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.40207053212067045 | validation: 0.4502405973688788]
	TIME [epoch: 6.31 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4864821925972906		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.4864821925972906 | validation: 0.5935709997326226]
	TIME [epoch: 6.35 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49720229900976676		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.49720229900976676 | validation: 0.5321523236027097]
	TIME [epoch: 6.33 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48017332483549485		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.48017332483549485 | validation: 0.46833661997688325]
	TIME [epoch: 6.31 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4048153150073973		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.4048153150073973 | validation: 0.3868166226053661]
	TIME [epoch: 6.31 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37984051208353575		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.37984051208353575 | validation: 0.4325270408875175]
	TIME [epoch: 6.31 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.439267195461666		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.439267195461666 | validation: 0.45913189857832]
	TIME [epoch: 6.31 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3683667980936044		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.3683667980936044 | validation: 0.3718069187544525]
	TIME [epoch: 6.35 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33448321486032395		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.33448321486032395 | validation: 0.34927165574733476]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_949.pth
	Model improved!!!
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3361331054038975		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.3361331054038975 | validation: 0.41507862162344156]
	TIME [epoch: 6.32 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4492562918066383		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.4492562918066383 | validation: 0.4294388999878554]
	TIME [epoch: 6.32 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3844606779712292		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.3844606779712292 | validation: 0.3981131681601197]
	TIME [epoch: 6.32 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43304703600658995		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.43304703600658995 | validation: 0.4884142590628804]
	TIME [epoch: 6.32 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.385235495998535		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.385235495998535 | validation: 0.41329973286707833]
	TIME [epoch: 6.35 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3623444506605799		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.3623444506605799 | validation: 0.37625264908754064]
	TIME [epoch: 6.33 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3895815176052031		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.3895815176052031 | validation: 0.4528231902806286]
	TIME [epoch: 6.32 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4054581498072496		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.4054581498072496 | validation: 0.3871062625684983]
	TIME [epoch: 6.32 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39306988508722596		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.39306988508722596 | validation: 0.4666293667546011]
	TIME [epoch: 6.32 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40295900667829243		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.40295900667829243 | validation: 0.4250691185938924]
	TIME [epoch: 6.32 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38319982229122707		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.38319982229122707 | validation: 0.3580116850095801]
	TIME [epoch: 6.34 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35806075467731546		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.35806075467731546 | validation: 0.3994816439968055]
	TIME [epoch: 6.35 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35248425738733047		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.35248425738733047 | validation: 0.34857331997255864]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_962.pth
	Model improved!!!
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34476971632086717		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.34476971632086717 | validation: 0.34733285072916753]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_963.pth
	Model improved!!!
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33346888531108393		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.33346888531108393 | validation: 0.34605395842572373]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_964.pth
	Model improved!!!
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32907640028814433		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.32907640028814433 | validation: 0.3276085040262945]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_965.pth
	Model improved!!!
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33345479715362863		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.33345479715362863 | validation: 0.34785575002287655]
	TIME [epoch: 6.36 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3426295256036636		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.3426295256036636 | validation: 0.39001012134159807]
	TIME [epoch: 6.32 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37860268748791615		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.37860268748791615 | validation: 0.37140438180577817]
	TIME [epoch: 6.31 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40135880860197565		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.40135880860197565 | validation: 0.4231915015099914]
	TIME [epoch: 6.31 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39303354892107534		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.39303354892107534 | validation: 0.39852509254153623]
	TIME [epoch: 6.31 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39246027678054435		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.39246027678054435 | validation: 0.3668005694789492]
	TIME [epoch: 6.31 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33763528451334873		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.33763528451334873 | validation: 0.3918168561854617]
	TIME [epoch: 6.35 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3564439230787655		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.3564439230787655 | validation: 0.33427767869322494]
	TIME [epoch: 6.32 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34450219302740825		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.34450219302740825 | validation: 0.3394429087344367]
	TIME [epoch: 6.31 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3844032853403141		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.3844032853403141 | validation: 0.3989341137272155]
	TIME [epoch: 6.31 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3678633233844521		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.3678633233844521 | validation: 0.3735589966508346]
	TIME [epoch: 6.31 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37608248037250297		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.37608248037250297 | validation: 0.39626296074224127]
	TIME [epoch: 6.31 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36024398939827024		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.36024398939827024 | validation: 0.37115809233592084]
	TIME [epoch: 6.34 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3387188696100791		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.3387188696100791 | validation: 0.3270685080935707]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_979.pth
	Model improved!!!
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32616800470381324		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.32616800470381324 | validation: 0.41105947413165544]
	TIME [epoch: 6.32 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39193231795007244		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.39193231795007244 | validation: 0.44594016760841804]
	TIME [epoch: 6.32 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40888590680098286		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.40888590680098286 | validation: 0.44072059303211125]
	TIME [epoch: 6.32 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40619256648296903		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.40619256648296903 | validation: 0.366255221895051]
	TIME [epoch: 6.31 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37060510161900306		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.37060510161900306 | validation: 0.39646443235523887]
	TIME [epoch: 6.35 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39734412262622476		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.39734412262622476 | validation: 0.38610912105029416]
	TIME [epoch: 6.34 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33937291209939957		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.33937291209939957 | validation: 0.386530645897173]
	TIME [epoch: 6.32 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3736884884474811		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.3736884884474811 | validation: 0.42710745439574277]
	TIME [epoch: 6.31 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3499750989204937		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.3499750989204937 | validation: 0.4254207060084404]
	TIME [epoch: 6.31 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4820924717778383		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.4820924717778383 | validation: 0.5310430145102938]
	TIME [epoch: 6.31 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43652213420409514		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.43652213420409514 | validation: 0.3653322247777907]
	TIME [epoch: 6.33 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3405984631008174		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.3405984631008174 | validation: 0.3375474458206885]
	TIME [epoch: 6.34 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3143431818660381		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.3143431818660381 | validation: 0.3196002946743799]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_992.pth
	Model improved!!!
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31730485618547843		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.31730485618547843 | validation: 0.40383136602145486]
	TIME [epoch: 6.32 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40352603497600004		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.40352603497600004 | validation: 0.5283987890388067]
	TIME [epoch: 6.31 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4173515291953454		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.4173515291953454 | validation: 0.40334889352408815]
	TIME [epoch: 6.32 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3682871430426531		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.3682871430426531 | validation: 0.4140402470424939]
	TIME [epoch: 6.33 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3842858482163662		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.3842858482163662 | validation: 0.4172381934340027]
	TIME [epoch: 6.35 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4131993865704042		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.4131993865704042 | validation: 0.38524792229462557]
	TIME [epoch: 6.32 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3495003891211015		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.3495003891211015 | validation: 0.3439275078223821]
	TIME [epoch: 6.32 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3255721649828917		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.3255721649828917 | validation: 0.3520476248676933]
	TIME [epoch: 6.31 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35457261416618785		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.35457261416618785 | validation: 0.4139027341778868]
	TIME [epoch: 6.32 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36960809083655527		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.36960809083655527 | validation: 0.4229531514757412]
	TIME [epoch: 6.33 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3635821100650263		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.3635821100650263 | validation: 0.3397038081399431]
	TIME [epoch: 6.34 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32710432958234925		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.32710432958234925 | validation: 0.3344788998812894]
	TIME [epoch: 6.32 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3426735875506265		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.3426735875506265 | validation: 0.39938644645246685]
	TIME [epoch: 6.32 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37134316805609796		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.37134316805609796 | validation: 0.40520445292938667]
	TIME [epoch: 6.32 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40947639520116075		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.40947639520116075 | validation: 0.38822573477162653]
	TIME [epoch: 6.32 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3677710698308073		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.3677710698308073 | validation: 0.34748444427793523]
	TIME [epoch: 6.33 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36466421514897057		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.36466421514897057 | validation: 0.43754037334433266]
	TIME [epoch: 6.34 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3720160189867712		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.3720160189867712 | validation: 0.39888888745074835]
	TIME [epoch: 6.31 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3939562101431159		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.3939562101431159 | validation: 0.375411494392847]
	TIME [epoch: 6.31 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3673162117216414		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.3673162117216414 | validation: 0.32945678090434893]
	TIME [epoch: 6.32 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34930035677931326		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.34930035677931326 | validation: 0.4003163596341211]
	TIME [epoch: 6.31 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37123140515251246		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.37123140515251246 | validation: 0.3661672585579214]
	TIME [epoch: 6.33 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3790153682272832		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.3790153682272832 | validation: 0.4021405194571681]
	TIME [epoch: 6.35 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41265143589045605		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.41265143589045605 | validation: 0.36712797178149775]
	TIME [epoch: 6.31 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35773448998881385		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.35773448998881385 | validation: 0.3394276314343522]
	TIME [epoch: 6.31 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35869069275560783		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.35869069275560783 | validation: 0.46849607080672406]
	TIME [epoch: 6.31 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4382349036806936		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.4382349036806936 | validation: 0.3352210418169137]
	TIME [epoch: 6.31 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3387256699548782		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.3387256699548782 | validation: 0.3599009929481327]
	TIME [epoch: 6.32 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3657310477339838		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.3657310477339838 | validation: 0.4194710190368436]
	TIME [epoch: 6.35 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4762951085633665		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.4762951085633665 | validation: 0.39602008071191735]
	TIME [epoch: 6.31 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3713159720526788		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.3713159720526788 | validation: 0.38258882591078147]
	TIME [epoch: 6.31 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3623835626208458		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.3623835626208458 | validation: 0.44980304278651406]
	TIME [epoch: 6.31 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4341073887672299		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.4341073887672299 | validation: 0.352675937420182]
	TIME [epoch: 6.31 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36932720194200386		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.36932720194200386 | validation: 0.3676056830197376]
	TIME [epoch: 6.32 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36524775463443554		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.36524775463443554 | validation: 0.3741801847896902]
	TIME [epoch: 6.35 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36643907513039403		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.36643907513039403 | validation: 0.32265914413578056]
	TIME [epoch: 6.32 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34369866205453736		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.34369866205453736 | validation: 0.4027829040429128]
	TIME [epoch: 6.31 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3733286236259018		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.3733286236259018 | validation: 0.352510254893246]
	TIME [epoch: 6.31 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31667157528720674		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.31667157528720674 | validation: 0.355363796655324]
	TIME [epoch: 6.31 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31655877717415853		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.31655877717415853 | validation: 0.31608000748262244]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1032.pth
	Model improved!!!
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3370919395317997		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.3370919395317997 | validation: 0.32845053033759875]
	TIME [epoch: 6.36 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3284923643847326		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.3284923643847326 | validation: 0.35081557574312794]
	TIME [epoch: 6.31 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3441551029903338		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.3441551029903338 | validation: 0.3052675614739838]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1035.pth
	Model improved!!!
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3046900032435773		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.3046900032435773 | validation: 0.30882455267255526]
	TIME [epoch: 6.31 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3189691635103393		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.3189691635103393 | validation: 0.3133493377671863]
	TIME [epoch: 6.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3200886806252079		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.3200886806252079 | validation: 0.31613882635796997]
	TIME [epoch: 6.32 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3291237504576803		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.3291237504576803 | validation: 0.41995797066804685]
	TIME [epoch: 6.35 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38756709782005205		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.38756709782005205 | validation: 0.32608030076293804]
	TIME [epoch: 6.31 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34278635923108863		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.34278635923108863 | validation: 0.4208066806634345]
	TIME [epoch: 6.31 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3754550138774755		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.3754550138774755 | validation: 0.3584255618243594]
	TIME [epoch: 6.31 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.338603810658957		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.338603810658957 | validation: 0.32117410314155365]
	TIME [epoch: 6.31 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3315224457528246		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.3315224457528246 | validation: 0.36341822909713706]
	TIME [epoch: 6.32 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3205080038796473		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.3205080038796473 | validation: 0.32491462838673685]
	TIME [epoch: 6.35 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32431905457591287		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.32431905457591287 | validation: 0.3213548985364483]
	TIME [epoch: 6.31 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3194894484195699		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.3194894484195699 | validation: 0.2979224882149645]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1047.pth
	Model improved!!!
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3044317388313381		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.3044317388313381 | validation: 0.33561125571145634]
	TIME [epoch: 6.31 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3217068794797795		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.3217068794797795 | validation: 0.3001926437904343]
	TIME [epoch: 6.31 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30899166201028094		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.30899166201028094 | validation: 0.32119700835729603]
	TIME [epoch: 6.32 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3191678212052272		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.3191678212052272 | validation: 0.3533036334035923]
	TIME [epoch: 6.36 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3951722392527331		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.3951722392527331 | validation: 0.35215641949376897]
	TIME [epoch: 6.31 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33243955125941965		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.33243955125941965 | validation: 0.3233103329226159]
	TIME [epoch: 6.31 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34227539631870507		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.34227539631870507 | validation: 0.35981977196868986]
	TIME [epoch: 6.31 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40063180658920583		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.40063180658920583 | validation: 0.36245730727831826]
	TIME [epoch: 6.31 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.357867903803158		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.357867903803158 | validation: 0.32627111352784166]
	TIME [epoch: 6.35 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35646716583702637		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.35646716583702637 | validation: 0.4572402478775708]
	TIME [epoch: 6.36 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37869585509975223		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.37869585509975223 | validation: 0.34435073596118876]
	TIME [epoch: 6.32 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3205245250635006		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.3205245250635006 | validation: 0.3471246197567215]
	TIME [epoch: 6.31 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34774419614809193		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.34774419614809193 | validation: 0.30892823290726484]
	TIME [epoch: 6.31 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3229525822803902		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.3229525822803902 | validation: 0.34068874832649004]
	TIME [epoch: 6.31 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3447777326066827		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.3447777326066827 | validation: 0.3629584189061861]
	TIME [epoch: 6.32 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35346702104508937		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.35346702104508937 | validation: 0.3329670039064776]
	TIME [epoch: 6.36 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3410897394141145		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.3410897394141145 | validation: 0.33819965718938855]
	TIME [epoch: 6.31 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31020724309115383		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.31020724309115383 | validation: 0.295469045785285]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1065.pth
	Model improved!!!
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3108472862093625		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.3108472862093625 | validation: 0.34287859185095587]
	TIME [epoch: 6.32 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4295560221242065		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.4295560221242065 | validation: 0.4975786943188302]
	TIME [epoch: 6.31 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3929688896894963		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.3929688896894963 | validation: 0.33219591794224745]
	TIME [epoch: 6.32 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33129236026968667		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.33129236026968667 | validation: 0.31556482770295613]
	TIME [epoch: 6.36 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33067841187033997		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.33067841187033997 | validation: 0.3291900992327388]
	TIME [epoch: 6.31 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3492535834334386		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.3492535834334386 | validation: 0.3590814408588865]
	TIME [epoch: 6.31 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35094594993129363		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.35094594993129363 | validation: 0.31613963716790555]
	TIME [epoch: 6.31 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3152344021075318		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.3152344021075318 | validation: 0.32611514148013554]
	TIME [epoch: 6.31 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33278964934754357		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.33278964934754357 | validation: 0.3703324952688468]
	TIME [epoch: 6.31 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3699116178598501		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.3699116178598501 | validation: 0.37628537271313645]
	TIME [epoch: 6.36 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3682105379863957		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.3682105379863957 | validation: 0.33681755419504844]
	TIME [epoch: 6.32 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3674336500339302		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.3674336500339302 | validation: 0.34153387894140236]
	TIME [epoch: 6.3 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31622230517408273		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.31622230517408273 | validation: 0.3420563672359459]
	TIME [epoch: 6.31 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3189675308976432		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.3189675308976432 | validation: 0.31656612170950094]
	TIME [epoch: 6.31 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3102792302118964		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.3102792302118964 | validation: 0.31492912022975333]
	TIME [epoch: 6.31 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3233958204959816		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.3233958204959816 | validation: 0.36530300977039887]
	TIME [epoch: 6.36 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3289857591327594		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.3289857591327594 | validation: 0.35327935523177717]
	TIME [epoch: 6.33 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3448238857693293		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.3448238857693293 | validation: 0.31532542528892693]
	TIME [epoch: 6.31 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3320137432019344		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.3320137432019344 | validation: 0.352049417379376]
	TIME [epoch: 6.31 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32418555501713564		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.32418555501713564 | validation: 0.35301048981821365]
	TIME [epoch: 6.31 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3268847089393374		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.3268847089393374 | validation: 0.3348971305718811]
	TIME [epoch: 6.32 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32438554034019984		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.32438554034019984 | validation: 0.2950051524056474]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1087.pth
	Model improved!!!
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2999267540168371		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.2999267540168371 | validation: 0.3203565463503673]
	TIME [epoch: 6.33 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31058908309179856		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.31058908309179856 | validation: 0.357889680754506]
	TIME [epoch: 6.31 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32292137123542447		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.32292137123542447 | validation: 0.3226392248371596]
	TIME [epoch: 6.31 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3210051287284822		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.3210051287284822 | validation: 0.30895253859930993]
	TIME [epoch: 6.31 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31185380112450334		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.31185380112450334 | validation: 0.3044106343400864]
	TIME [epoch: 6.31 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.313327787544452		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.313327787544452 | validation: 0.3040935478144942]
	TIME [epoch: 6.35 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30928124645254074		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.30928124645254074 | validation: 0.3759051423641129]
	TIME [epoch: 6.33 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34017507245050294		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.34017507245050294 | validation: 0.39102373144305724]
	TIME [epoch: 6.31 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3263593543347037		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.3263593543347037 | validation: 0.33157464838314676]
	TIME [epoch: 6.31 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30565390034425205		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.30565390034425205 | validation: 0.3003217374658543]
	TIME [epoch: 6.31 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29904819106534		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.29904819106534 | validation: 0.2910154618433938]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1098.pth
	Model improved!!!
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29488863805322374		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.29488863805322374 | validation: 0.30854168241997837]
	TIME [epoch: 6.35 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30380789402486547		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.30380789402486547 | validation: 0.3152912210094964]
	TIME [epoch: 6.33 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3101376010080695		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.3101376010080695 | validation: 0.3243949585464898]
	TIME [epoch: 6.3 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3638808227070877		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.3638808227070877 | validation: 0.33092388554390934]
	TIME [epoch: 6.31 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31164679011493857		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.31164679011493857 | validation: 0.32039210501157145]
	TIME [epoch: 6.31 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3337405304860982		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.3337405304860982 | validation: 0.3590205242417306]
	TIME [epoch: 6.31 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32701390097927807		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.32701390097927807 | validation: 0.36009335731114206]
	TIME [epoch: 6.34 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3238094454860963		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.3238094454860963 | validation: 0.31653900036408034]
	TIME [epoch: 6.33 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29880769707062166		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.29880769707062166 | validation: 0.28269336368159254]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1107.pth
	Model improved!!!
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3030087904452555		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.3030087904452555 | validation: 0.31408554138399614]
	TIME [epoch: 6.31 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29585011045019705		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.29585011045019705 | validation: 0.30801343492224265]
	TIME [epoch: 6.31 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29720534569620155		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.29720534569620155 | validation: 0.29101455105042473]
	TIME [epoch: 6.31 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31100343163488614		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.31100343163488614 | validation: 0.32518919573867255]
	TIME [epoch: 6.34 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3179680167044495		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.3179680167044495 | validation: 0.3357315828073545]
	TIME [epoch: 6.33 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32127566691947596		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.32127566691947596 | validation: 0.3339369872762591]
	TIME [epoch: 6.31 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32436404221476006		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.32436404221476006 | validation: 0.30965214754835874]
	TIME [epoch: 6.31 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2984499663479959		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.2984499663479959 | validation: 0.2961072526463281]
	TIME [epoch: 6.31 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30515619583952613		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.30515619583952613 | validation: 0.35879138712799474]
	TIME [epoch: 6.31 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4011858740178261		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.4011858740178261 | validation: 0.41132603081738184]
	TIME [epoch: 6.33 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34448258782899244		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.34448258782899244 | validation: 0.31619485269161]
	TIME [epoch: 6.34 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3172645698087776		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.3172645698087776 | validation: 0.323574232405738]
	TIME [epoch: 6.31 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3077949752388359		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.3077949752388359 | validation: 0.29656566413492386]
	TIME [epoch: 6.31 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29691052518591554		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.29691052518591554 | validation: 0.2873546468717595]
	TIME [epoch: 6.31 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30046328653668364		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.30046328653668364 | validation: 0.29807415080297384]
	TIME [epoch: 6.31 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32124945399105986		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.32124945399105986 | validation: 0.31319944523755505]
	TIME [epoch: 6.33 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31621086262134274		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.31621086262134274 | validation: 0.2969139759720901]
	TIME [epoch: 6.35 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3128570205931209		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.3128570205931209 | validation: 0.299956395826537]
	TIME [epoch: 6.31 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29705605404900215		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.29705605404900215 | validation: 0.3009493671767359]
	TIME [epoch: 6.31 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3079652100751752		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.3079652100751752 | validation: 0.36163997607037707]
	TIME [epoch: 6.31 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3469931495012211		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.3469931495012211 | validation: 0.34845501124069506]
	TIME [epoch: 6.31 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34143267525142074		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.34143267525142074 | validation: 0.3247277595182726]
	TIME [epoch: 6.33 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31136540029776405		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.31136540029776405 | validation: 0.29085004350414767]
	TIME [epoch: 6.34 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3037168122766199		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.3037168122766199 | validation: 0.2976159858307481]
	TIME [epoch: 6.32 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30277902502676757		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.30277902502676757 | validation: 0.3251305977981291]
	TIME [epoch: 6.31 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3704101307768987		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.3704101307768987 | validation: 0.3309556376961411]
	TIME [epoch: 6.31 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3237981217004083		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.3237981217004083 | validation: 0.31879829946454763]
	TIME [epoch: 6.31 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33945702580727055		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.33945702580727055 | validation: 0.3121757427646725]
	TIME [epoch: 6.32 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3074716306331739		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.3074716306331739 | validation: 0.34804740146172003]
	TIME [epoch: 6.36 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3289750778856517		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.3289750778856517 | validation: 0.3245222667297267]
	TIME [epoch: 6.31 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3165468507187934		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.3165468507187934 | validation: 0.3743112429886235]
	TIME [epoch: 6.32 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35481180049402894		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.35481180049402894 | validation: 0.4109035762582145]
	TIME [epoch: 6.31 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3768214266891985		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.3768214266891985 | validation: 0.3664992762589856]
	TIME [epoch: 6.32 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3431965815164036		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.3431965815164036 | validation: 0.35373242986897646]
	TIME [epoch: 6.33 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3312544477857766		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.3312544477857766 | validation: 0.3036864053430653]
	TIME [epoch: 6.36 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29931263002593644		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.29931263002593644 | validation: 0.31232753555173676]
	TIME [epoch: 6.31 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32285655740636454		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.32285655740636454 | validation: 0.3431710793121501]
	TIME [epoch: 6.31 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3202042972704818		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.3202042972704818 | validation: 0.31663455986244227]
	TIME [epoch: 6.31 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31018529126784267		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.31018529126784267 | validation: 0.2822441504000208]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1146.pth
	Model improved!!!
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29020777140781784		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.29020777140781784 | validation: 0.2923558870535983]
	TIME [epoch: 6.33 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29663223261869714		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.29663223261869714 | validation: 0.294692438459493]
	TIME [epoch: 6.35 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30084880554363647		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.30084880554363647 | validation: 0.37345472959571047]
	TIME [epoch: 6.31 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3283609887222163		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.3283609887222163 | validation: 0.3890570701414817]
	TIME [epoch: 6.31 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3433877773976967		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.3433877773976967 | validation: 0.31883570280116597]
	TIME [epoch: 6.31 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3099113908600635		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.3099113908600635 | validation: 0.30623420033274334]
	TIME [epoch: 6.31 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32317574772553476		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.32317574772553476 | validation: 0.3284229748443449]
	TIME [epoch: 6.32 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3199847746164598		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.3199847746164598 | validation: 0.31190734675764653]
	TIME [epoch: 6.35 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3171190844397801		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.3171190844397801 | validation: 0.32553562124982743]
	TIME [epoch: 6.31 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3413094605488578		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.3413094605488578 | validation: 0.3692595121488952]
	TIME [epoch: 6.31 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3314667548170743		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.3314667548170743 | validation: 0.36706665583976283]
	TIME [epoch: 6.31 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3422277423045328		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.3422277423045328 | validation: 0.3585830661950812]
	TIME [epoch: 6.31 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35534575566138943		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.35534575566138943 | validation: 0.3787654411943065]
	TIME [epoch: 6.32 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33653624970009555		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.33653624970009555 | validation: 0.3242396717250649]
	TIME [epoch: 6.35 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31454866901791273		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.31454866901791273 | validation: 0.30454302238894515]
	TIME [epoch: 6.31 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2928433120097432		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.2928433120097432 | validation: 0.30790882628471206]
	TIME [epoch: 6.31 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3199087451501803		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.3199087451501803 | validation: 0.31286829823468126]
	TIME [epoch: 6.31 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30009144424203393		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.30009144424203393 | validation: 0.28307766041423976]
	TIME [epoch: 6.31 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29613563343649874		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.29613563343649874 | validation: 0.37387314783292885]
	TIME [epoch: 6.31 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3494182510379981		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.3494182510379981 | validation: 0.31485128333161383]
	TIME [epoch: 6.36 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32545278674699474		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.32545278674699474 | validation: 0.3469970119772057]
	TIME [epoch: 6.32 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.335819367535634		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.335819367535634 | validation: 0.32960765797566194]
	TIME [epoch: 6.31 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3271601994587101		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 0.3271601994587101 | validation: 0.3511751588747014]
	TIME [epoch: 6.31 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3295815701249572		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.3295815701249572 | validation: 0.3261210036119325]
	TIME [epoch: 6.31 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3342384678410872		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.3342384678410872 | validation: 0.3588318064211227]
	TIME [epoch: 6.33 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3515873327121526		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.3515873327121526 | validation: 0.38079054734234985]
	TIME [epoch: 6.36 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3346914679497781		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.3346914679497781 | validation: 0.3406848547337911]
	TIME [epoch: 6.33 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3095266839152922		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.3095266839152922 | validation: 0.3506856567921216]
	TIME [epoch: 6.32 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30795074476852696		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.30795074476852696 | validation: 0.3105141563547377]
	TIME [epoch: 6.32 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30333885244480946		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.30333885244480946 | validation: 0.295985166752453]
	TIME [epoch: 6.32 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31191157741216113		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.31191157741216113 | validation: 0.326756463455089]
	TIME [epoch: 6.32 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32109800275156897		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.32109800275156897 | validation: 0.30700569604735584]
	TIME [epoch: 6.36 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3144134861344213		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.3144134861344213 | validation: 0.34315171335859196]
	TIME [epoch: 6.33 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33070677244851227		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.33070677244851227 | validation: 0.3646785148227716]
	TIME [epoch: 6.32 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34557122075411145		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.34557122075411145 | validation: 0.3537340378300407]
	TIME [epoch: 6.32 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3061980193003804		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.3061980193003804 | validation: 0.29079251713969845]
	TIME [epoch: 6.32 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2987643285812737		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.2987643285812737 | validation: 0.30288744734060224]
	TIME [epoch: 6.32 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29909537042488		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.29909537042488 | validation: 0.2958012587522427]
	TIME [epoch: 6.37 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30096951162061225		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.30096951162061225 | validation: 0.303947116608282]
	TIME [epoch: 6.33 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3144610274524897		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.3144610274524897 | validation: 0.3083993570802751]
	TIME [epoch: 6.32 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3199713907370284		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.3199713907370284 | validation: 0.3146517738567315]
	TIME [epoch: 6.32 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30780407893540895		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.30780407893540895 | validation: 0.30848260212239237]
	TIME [epoch: 6.32 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3052569832813024		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.3052569832813024 | validation: 0.3262221259137077]
	TIME [epoch: 6.32 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32691508047095985		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.32691508047095985 | validation: 0.33099031259124645]
	TIME [epoch: 6.36 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3089400333763992		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.3089400333763992 | validation: 0.36230310975237895]
	TIME [epoch: 6.33 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3312840327762202		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.3312840327762202 | validation: 0.3387209887402485]
	TIME [epoch: 6.32 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33683653217294207		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.33683653217294207 | validation: 0.365502645770572]
	TIME [epoch: 6.32 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3633271625222204		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.3633271625222204 | validation: 0.37771174759462456]
	TIME [epoch: 6.32 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3775629102383669		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.3775629102383669 | validation: 0.4070267697519476]
	TIME [epoch: 6.32 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36575418274789606		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.36575418274789606 | validation: 0.3909934301672805]
	TIME [epoch: 6.35 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3659550309060563		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.3659550309060563 | validation: 0.4075136067611736]
	TIME [epoch: 6.34 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36694997117178485		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.36694997117178485 | validation: 0.34261118711951577]
	TIME [epoch: 6.32 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33102436495668447		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.33102436495668447 | validation: 0.34041766774993654]
	TIME [epoch: 6.32 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32584703655875125		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.32584703655875125 | validation: 0.3077959981049745]
	TIME [epoch: 6.32 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31324433367913207		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.31324433367913207 | validation: 0.3026934174168392]
	TIME [epoch: 6.32 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3188177703579481		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.3188177703579481 | validation: 0.3594043622545363]
	TIME [epoch: 6.34 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32482226411239545		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.32482226411239545 | validation: 0.35147101338326736]
	TIME [epoch: 6.35 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3215190449338616		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.3215190449338616 | validation: 0.34690921684461457]
	TIME [epoch: 6.32 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3166327871736814		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.3166327871736814 | validation: 0.3256235526775806]
	TIME [epoch: 6.32 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30168675647062126		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.30168675647062126 | validation: 0.31897387187992976]
	TIME [epoch: 6.32 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29952667909697445		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.29952667909697445 | validation: 0.3115773302944809]
	TIME [epoch: 6.32 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3093326656158461		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.3093326656158461 | validation: 0.3435672058074949]
	TIME [epoch: 6.34 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3107600231615288		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.3107600231615288 | validation: 0.3143201949903885]
	TIME [epoch: 6.35 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30322866842139107		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.30322866842139107 | validation: 0.3167529594734627]
	TIME [epoch: 6.32 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3191057871707846		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.3191057871707846 | validation: 0.32529812454822526]
	TIME [epoch: 6.32 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3122318250279632		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.3122318250279632 | validation: 0.3385668116169763]
	TIME [epoch: 6.32 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3615961176733845		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.3615961176733845 | validation: 0.35521551230348314]
	TIME [epoch: 6.32 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33706569536861003		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.33706569536861003 | validation: 0.2953134978167642]
	TIME [epoch: 6.34 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3030605726038711		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.3030605726038711 | validation: 0.3234622598303927]
	TIME [epoch: 6.35 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3102609907205904		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.3102609907205904 | validation: 0.2943591093355036]
	TIME [epoch: 6.33 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28515808799610926		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.28515808799610926 | validation: 0.2888758991248168]
	TIME [epoch: 6.32 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2990543581612908		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.2990543581612908 | validation: 0.3255188399828207]
	TIME [epoch: 6.32 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3012539513507127		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.3012539513507127 | validation: 0.3129914496827778]
	TIME [epoch: 6.32 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31105310095777866		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.31105310095777866 | validation: 0.3196983521521192]
	TIME [epoch: 6.34 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3143707223952247		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.3143707223952247 | validation: 0.31293515260257343]
	TIME [epoch: 6.35 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30350769677615674		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.30350769677615674 | validation: 0.30965028437863384]
	TIME [epoch: 6.32 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29327869795483846		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.29327869795483846 | validation: 0.2997949940805096]
	TIME [epoch: 6.32 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2975320152614169		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.2975320152614169 | validation: 0.3034778674428539]
	TIME [epoch: 6.32 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2954309108527801		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.2954309108527801 | validation: 0.29554693939618465]
	TIME [epoch: 6.32 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3209902727332363		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.3209902727332363 | validation: 0.3086990115464359]
	TIME [epoch: 6.33 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30869354278791655		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.30869354278791655 | validation: 0.2980518576040746]
	TIME [epoch: 6.36 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3027400781397187		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.3027400781397187 | validation: 0.36371888131118]
	TIME [epoch: 6.32 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31381203118467527		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.31381203118467527 | validation: 0.30311674069830397]
	TIME [epoch: 6.32 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30052203838340336		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.30052203838340336 | validation: 0.3094216000876894]
	TIME [epoch: 6.32 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2904731701543364		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.2904731701543364 | validation: 0.3139587207295682]
	TIME [epoch: 6.31 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3024437988547646		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.3024437988547646 | validation: 0.31431939392202224]
	TIME [epoch: 6.33 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2945643294450353		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.2945643294450353 | validation: 0.31786591513974294]
	TIME [epoch: 6.36 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29792086396729844		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.29792086396729844 | validation: 0.32760860981610884]
	TIME [epoch: 6.32 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3007940915964553		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.3007940915964553 | validation: 0.3129156446605498]
	TIME [epoch: 6.31 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32114650255544763		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.32114650255544763 | validation: 0.33445687534176993]
	TIME [epoch: 6.32 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.316188381997079		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.316188381997079 | validation: 0.31636668223226416]
	TIME [epoch: 6.31 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2982009851808687		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.2982009851808687 | validation: 0.30538302040828635]
	TIME [epoch: 6.32 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2960871985436387		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.2960871985436387 | validation: 0.29206854689557127]
	TIME [epoch: 6.36 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3001618391166133		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.3001618391166133 | validation: 0.33907199094004237]
	TIME [epoch: 6.32 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3526847118946423		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.3526847118946423 | validation: 0.3297289463934437]
	TIME [epoch: 6.32 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31867140585811476		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.31867140585811476 | validation: 0.29816870216370406]
	TIME [epoch: 6.32 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3104109614226866		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.3104109614226866 | validation: 0.2896420975691709]
	TIME [epoch: 6.32 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30344200145738287		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.30344200145738287 | validation: 0.29424341374584706]
	TIME [epoch: 6.32 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29401514894727815		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.29401514894727815 | validation: 0.31279594674500955]
	TIME [epoch: 6.36 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3174306355502895		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.3174306355502895 | validation: 0.35044960997110897]
	TIME [epoch: 6.32 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3097275733716226		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.3097275733716226 | validation: 0.3169028878332891]
	TIME [epoch: 6.32 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2891409348960711		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.2891409348960711 | validation: 0.30929453278719676]
	TIME [epoch: 6.32 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3180599226888545		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.3180599226888545 | validation: 0.3224399681449023]
	TIME [epoch: 6.32 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3060671551424732		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.3060671551424732 | validation: 0.30435944622697053]
	TIME [epoch: 6.33 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2974260819052536		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.2974260819052536 | validation: 0.3112051579852704]
	TIME [epoch: 6.36 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2965961023597879		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.2965961023597879 | validation: 0.28577828764277036]
	TIME [epoch: 6.32 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28332269988191444		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.28332269988191444 | validation: 0.28597795037605833]
	TIME [epoch: 6.32 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2969832208632352		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.2969832208632352 | validation: 0.3212417553140209]
	TIME [epoch: 6.32 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29944238762295305		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.29944238762295305 | validation: 0.281357110385524]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1255.pth
	Model improved!!!
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29156636269574066		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.29156636269574066 | validation: 0.321028747669607]
	TIME [epoch: 6.32 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3081957768948825		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.3081957768948825 | validation: 0.31607030345899195]
	TIME [epoch: 6.36 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2964224622329409		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.2964224622329409 | validation: 0.3031587300659008]
	TIME [epoch: 6.32 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29014654263353923		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.29014654263353923 | validation: 0.2782997005820506]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1259.pth
	Model improved!!!
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29808958023112325		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.29808958023112325 | validation: 0.30029147021024594]
	TIME [epoch: 6.31 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30647663404232134		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.30647663404232134 | validation: 0.34641152460574864]
	TIME [epoch: 6.31 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3474694484872726		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.3474694484872726 | validation: 0.3331716424229688]
	TIME [epoch: 6.31 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31343188874160477		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.31343188874160477 | validation: 0.3067373082352317]
	TIME [epoch: 6.35 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.300340452456868		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.300340452456868 | validation: 0.2909850979796625]
	TIME [epoch: 6.31 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29415052271895026		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.29415052271895026 | validation: 0.30536936367012535]
	TIME [epoch: 6.31 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3053236393629442		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.3053236393629442 | validation: 0.29007877869162985]
	TIME [epoch: 6.31 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2914831586923171		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.2914831586923171 | validation: 0.2772804166997585]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1267.pth
	Model improved!!!
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.288497597699781		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.288497597699781 | validation: 0.27854203430129365]
	TIME [epoch: 6.32 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2846401310316878		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.2846401310316878 | validation: 0.2796592489179255]
	TIME [epoch: 6.35 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2964685724150591		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.2964685724150591 | validation: 0.2920904425921471]
	TIME [epoch: 6.31 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3092583346224319		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.3092583346224319 | validation: 0.31689449613275544]
	TIME [epoch: 6.31 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3491272002391568		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.3491272002391568 | validation: 0.34377217051282977]
	TIME [epoch: 6.31 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3240289057753577		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.3240289057753577 | validation: 0.30377547995968557]
	TIME [epoch: 6.31 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29691230842397004		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.29691230842397004 | validation: 0.27392398075109753]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1274.pth
	Model improved!!!
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2950082441304379		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.2950082441304379 | validation: 0.31616679052069696]
	TIME [epoch: 6.35 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31460989812447493		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.31460989812447493 | validation: 0.32433804158948487]
	TIME [epoch: 6.31 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.305031189162166		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.305031189162166 | validation: 0.31158952857619643]
	TIME [epoch: 6.31 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31279670587851865		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.31279670587851865 | validation: 0.36701540643261976]
	TIME [epoch: 6.31 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34020325248528954		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.34020325248528954 | validation: 0.3697328307290779]
	TIME [epoch: 6.31 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32740020986471646		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.32740020986471646 | validation: 0.3116496104999117]
	TIME [epoch: 6.32 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3034901113284939		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.3034901113284939 | validation: 0.3424171450869944]
	TIME [epoch: 6.35 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31259530736820235		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.31259530736820235 | validation: 0.33276647773985935]
	TIME [epoch: 6.33 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32865658282161175		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.32865658282161175 | validation: 0.3412354750806702]
	TIME [epoch: 6.31 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3232497095082566		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.3232497095082566 | validation: 0.3219283678283067]
	TIME [epoch: 6.31 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3025002967074407		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.3025002967074407 | validation: 0.3233380776626166]
	TIME [epoch: 6.31 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30698896663466524		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.30698896663466524 | validation: 0.33848636372992985]
	TIME [epoch: 6.31 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31336252071240567		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.31336252071240567 | validation: 0.32551661403138993]
	TIME [epoch: 6.35 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3038331658507109		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.3038331658507109 | validation: 0.3002878376324119]
	TIME [epoch: 6.31 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2967930756443744		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.2967930756443744 | validation: 0.31739802367950615]
	TIME [epoch: 6.31 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35725736210277803		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.35725736210277803 | validation: 0.3773797994961269]
	TIME [epoch: 6.31 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34560068329145155		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.34560068329145155 | validation: 0.3072791459220785]
	TIME [epoch: 6.31 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2925933626248607		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.2925933626248607 | validation: 0.2833884696219029]
	TIME [epoch: 6.31 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2877924207331253		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.2877924207331253 | validation: 0.2632328086954777]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1293.pth
	Model improved!!!
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2832523245404732		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.2832523245404732 | validation: 0.26738284863547984]
	TIME [epoch: 6.31 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30291354586019836		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.30291354586019836 | validation: 0.2895238344928134]
	TIME [epoch: 6.31 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2949586630471805		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.2949586630471805 | validation: 0.28710934008376304]
	TIME [epoch: 6.31 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2858110573241496		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.2858110573241496 | validation: 0.2777283142780546]
	TIME [epoch: 6.31 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.284636782679919		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.284636782679919 | validation: 0.2873721938877054]
	TIME [epoch: 6.31 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28976546581320217		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.28976546581320217 | validation: 0.3186986491632524]
	TIME [epoch: 6.35 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3123745196467905		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.3123745196467905 | validation: 0.2974615310196351]
	TIME [epoch: 6.31 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30685250836114786		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.30685250836114786 | validation: 0.2922365634009721]
	TIME [epoch: 6.31 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3008897012975157		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.3008897012975157 | validation: 0.2905750867944323]
	TIME [epoch: 6.31 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2956771595875213		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.2956771595875213 | validation: 0.29433945129363087]
	TIME [epoch: 6.31 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30117613282322664		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.30117613282322664 | validation: 0.3084241163884371]
	TIME [epoch: 6.31 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31005827299275235		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.31005827299275235 | validation: 0.332999635075899]
	TIME [epoch: 6.34 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30166047032891985		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.30166047032891985 | validation: 0.3046775830557168]
	TIME [epoch: 6.32 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2884113359297396		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.2884113359297396 | validation: 0.2974695419456542]
	TIME [epoch: 6.31 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29418792560354085		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.29418792560354085 | validation: 0.3184321181457786]
	TIME [epoch: 6.31 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3017118956662104		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.3017118956662104 | validation: 0.3317775249432778]
	TIME [epoch: 6.31 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31905755033437555		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 0.31905755033437555 | validation: 0.32822446862255417]
	TIME [epoch: 6.31 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3209966641940321		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 0.3209966641940321 | validation: 0.3375862266407734]
	TIME [epoch: 6.33 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31530138339553		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 0.31530138339553 | validation: 0.3112213343849135]
	TIME [epoch: 6.32 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28632671378190894		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 0.28632671378190894 | validation: 0.3009194144420996]
	TIME [epoch: 6.31 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29713414348001105		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 0.29713414348001105 | validation: 0.30845831940665813]
	TIME [epoch: 6.31 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3091213189707127		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 0.3091213189707127 | validation: 0.32230722638323606]
	TIME [epoch: 6.31 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3141032890801977		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 0.3141032890801977 | validation: 0.32809954050280293]
	TIME [epoch: 6.31 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3001089793145412		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 0.3001089793145412 | validation: 0.30283565549096936]
	TIME [epoch: 6.33 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2890816785490259		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 0.2890816785490259 | validation: 0.278492816842808]
	TIME [epoch: 6.33 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28979565267380836		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 0.28979565267380836 | validation: 0.29960602315754026]
	TIME [epoch: 6.31 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2866212571626006		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 0.2866212571626006 | validation: 0.2897591893948138]
	TIME [epoch: 6.31 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2868913031191803		[learning rate: 0.00011092]
	Learning Rate: 0.000110918
	LOSS [training: 0.2868913031191803 | validation: 0.3114405895076442]
	TIME [epoch: 6.31 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2912304682811434		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 0.2912304682811434 | validation: 0.29193065159554177]
	TIME [epoch: 6.31 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2929116560541689		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 0.2929116560541689 | validation: 0.28317925627335266]
	TIME [epoch: 6.33 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2858474009869745		[learning rate: 0.00010975]
	Learning Rate: 0.000109745
	LOSS [training: 0.2858474009869745 | validation: 0.2783506443323318]
	TIME [epoch: 6.34 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2810314324681339		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 0.2810314324681339 | validation: 0.28594345133859556]
	TIME [epoch: 6.31 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2950150876711991		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 0.2950150876711991 | validation: 0.26826403989345265]
	TIME [epoch: 6.31 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28727946346911915		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 0.28727946346911915 | validation: 0.2912520208055166]
	TIME [epoch: 6.31 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2913781201302464		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 0.2913781201302464 | validation: 0.2895005123276877]
	TIME [epoch: 6.31 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2841334241170044		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 0.2841334241170044 | validation: 0.29479431357992847]
	TIME [epoch: 6.33 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2903484666032072		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 0.2903484666032072 | validation: 0.2805883470706281]
	TIME [epoch: 6.33 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2883876132365825		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 0.2883876132365825 | validation: 0.29515733989309667]
	TIME [epoch: 6.31 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28763671046811257		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 0.28763671046811257 | validation: 0.27983989028573064]
	TIME [epoch: 6.31 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28681108779748166		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 0.28681108779748166 | validation: 0.28254853570937266]
	TIME [epoch: 6.31 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2901521030082871		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 0.2901521030082871 | validation: 0.30175958928170954]
	TIME [epoch: 6.31 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3062873267612999		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 0.3062873267612999 | validation: 0.29674108669935756]
	TIME [epoch: 6.32 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29531664958246295		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 0.29531664958246295 | validation: 0.278770697268454]
	TIME [epoch: 6.34 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3101011444496384		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 0.3101011444496384 | validation: 0.29566867809057285]
	TIME [epoch: 6.31 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30464139228622905		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 0.30464139228622905 | validation: 0.28379443454274267]
	TIME [epoch: 6.31 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30058837931748006		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 0.30058837931748006 | validation: 0.3140107841086408]
	TIME [epoch: 6.31 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33527902766711853		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 0.33527902766711853 | validation: 0.31316446634014883]
	TIME [epoch: 6.3 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33309762357196715		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 0.33309762357196715 | validation: 0.31764972989062945]
	TIME [epoch: 6.32 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3607757350779998		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 0.3607757350779998 | validation: 0.3328374006756817]
	TIME [epoch: 6.34 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33626927681882357		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 0.33626927681882357 | validation: 0.2995982054784161]
	TIME [epoch: 6.31 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3100201110963388		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 0.3100201110963388 | validation: 0.27330943180734524]
	TIME [epoch: 6.3 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28789877218800897		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 0.28789877218800897 | validation: 0.2766381288809705]
	TIME [epoch: 6.31 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2784829839065255		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 0.2784829839065255 | validation: 0.2856070743148515]
	TIME [epoch: 6.3 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28262832933077164		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.28262832933077164 | validation: 0.29217716096919466]
	TIME [epoch: 6.32 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29558115160889664		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 0.29558115160889664 | validation: 0.30604425398761026]
	TIME [epoch: 6.35 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29323970303263713		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 0.29323970303263713 | validation: 0.31727092447470145]
	TIME [epoch: 6.31 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29783495113727665		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 0.29783495113727665 | validation: 0.3192309145957517]
	TIME [epoch: 6.31 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3056881691289152		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 0.3056881691289152 | validation: 0.3065771825111165]
	TIME [epoch: 6.3 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31290194442603536		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 0.31290194442603536 | validation: 0.2951728417313142]
	TIME [epoch: 6.31 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2940744515508896		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 0.2940744515508896 | validation: 0.2825527315576708]
	TIME [epoch: 6.31 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29688126974487805		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 0.29688126974487805 | validation: 0.29639360220783295]
	TIME [epoch: 6.35 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2873851809609782		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 0.2873851809609782 | validation: 0.2801182990777097]
	TIME [epoch: 6.31 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2936305953343835		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 0.2936305953343835 | validation: 0.2950928534855166]
	TIME [epoch: 6.31 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.298627742378022		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 0.298627742378022 | validation: 0.3040295111947744]
	TIME [epoch: 6.31 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3043875972033607		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 0.3043875972033607 | validation: 0.2823190574772162]
	TIME [epoch: 6.31 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2904278779539586		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 0.2904278779539586 | validation: 0.2785124195779477]
	TIME [epoch: 6.31 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2863124115044966		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 0.2863124115044966 | validation: 0.30161509803336023]
	TIME [epoch: 6.35 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3159809872383424		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 0.3159809872383424 | validation: 0.34598578944062675]
	TIME [epoch: 6.32 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32419410570810897		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 0.32419410570810897 | validation: 0.323765594926002]
	TIME [epoch: 6.3 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30886997674434336		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 0.30886997674434336 | validation: 0.289149669072475]
	TIME [epoch: 6.31 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28797316517758353		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 0.28797316517758353 | validation: 0.29320893000824994]
	TIME [epoch: 6.32 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2909886584389856		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 0.2909886584389856 | validation: 0.2757954965900007]
	TIME [epoch: 6.31 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28187740611214235		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 0.28187740611214235 | validation: 0.2747758835966912]
	TIME [epoch: 6.34 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2797910936351015		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 0.2797910936351015 | validation: 0.2675101681230462]
	TIME [epoch: 6.31 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28455078849734716		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 0.28455078849734716 | validation: 0.26928805319121335]
	TIME [epoch: 6.31 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28116854707971034		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 0.28116854707971034 | validation: 0.2795177322558826]
	TIME [epoch: 6.31 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30035061393813767		[learning rate: 9.3243e-05]
	Learning Rate: 9.32428e-05
	LOSS [training: 0.30035061393813767 | validation: 0.2992549071510323]
	TIME [epoch: 6.31 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30674180625392655		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 0.30674180625392655 | validation: 0.3155483282935599]
	TIME [epoch: 6.31 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3066192503229884		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 0.3066192503229884 | validation: 0.27804312058652486]
	TIME [epoch: 6.35 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.287619661110535		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 0.287619661110535 | validation: 0.2794453389290661]
	TIME [epoch: 6.31 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29732531759652653		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 0.29732531759652653 | validation: 0.29159964001626637]
	TIME [epoch: 6.31 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2989359702490233		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 0.2989359702490233 | validation: 0.28100135786007335]
	TIME [epoch: 6.3 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27871198125001967		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 0.27871198125001967 | validation: 0.27438853002527486]
	TIME [epoch: 6.31 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.274743835177395		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 0.274743835177395 | validation: 0.28335049360450176]
	TIME [epoch: 6.31 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27807553972946525		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 0.27807553972946525 | validation: 0.26768363431449865]
	TIME [epoch: 6.35 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28074490292801335		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 0.28074490292801335 | validation: 0.27531114763681747]
	TIME [epoch: 6.32 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2827043297927637		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 0.2827043297927637 | validation: 0.28406435748322645]
	TIME [epoch: 6.31 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29247971348000246		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 0.29247971348000246 | validation: 0.2767989446236615]
	TIME [epoch: 6.31 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28065352586828424		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 0.28065352586828424 | validation: 0.28622452671694676]
	TIME [epoch: 6.31 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27791053110277036		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 0.27791053110277036 | validation: 0.2705296879514253]
	TIME [epoch: 6.31 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2888875842224301		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 0.2888875842224301 | validation: 0.3027135263805124]
	TIME [epoch: 6.35 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30344360455986974		[learning rate: 8.8418e-05]
	Learning Rate: 8.84175e-05
	LOSS [training: 0.30344360455986974 | validation: 0.29830713334458825]
	TIME [epoch: 6.33 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29256268471429575		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.29256268471429575 | validation: 0.2995456967392296]
	TIME [epoch: 6.31 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30050450197717726		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 0.30050450197717726 | validation: 0.3203555881973884]
	TIME [epoch: 6.31 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3031223781974576		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 0.3031223781974576 | validation: 0.286106176014909]
	TIME [epoch: 6.32 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28974234382616176		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 0.28974234382616176 | validation: 0.2848318659509165]
	TIME [epoch: 6.31 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2863187805476331		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 0.2863187805476331 | validation: 0.27450826187517924]
	TIME [epoch: 6.35 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2800872108480731		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 0.2800872108480731 | validation: 0.2617707196746004]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1391.pth
	Model improved!!!
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27267549171482885		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 0.27267549171482885 | validation: 0.26754373914133034]
	TIME [epoch: 6.31 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2765487404749519		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 0.2765487404749519 | validation: 0.26317528036450333]
	TIME [epoch: 6.31 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2827361148367077		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 0.2827361148367077 | validation: 0.26780361398672775]
	TIME [epoch: 6.31 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28318525795208555		[learning rate: 8.534e-05]
	Learning Rate: 8.53402e-05
	LOSS [training: 0.28318525795208555 | validation: 0.2539914753815581]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1395.pth
	Model improved!!!
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27986797979825606		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 0.27986797979825606 | validation: 0.2853608169912214]
	TIME [epoch: 6.34 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3157412076648095		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 0.3157412076648095 | validation: 0.26577195193405273]
	TIME [epoch: 6.33 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2976331667481876		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 0.2976331667481876 | validation: 0.2672019429957744]
	TIME [epoch: 6.31 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.292661286328933		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 0.292661286328933 | validation: 0.3025577289052803]
	TIME [epoch: 6.31 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33120861507635957		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 0.33120861507635957 | validation: 0.31478113904270444]
	TIME [epoch: 6.31 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3091065755137794		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 0.3091065755137794 | validation: 0.267981959810771]
	TIME [epoch: 6.31 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2869552206915076		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 0.2869552206915076 | validation: 0.28060784735674793]
	TIME [epoch: 6.35 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28279784332987945		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 0.28279784332987945 | validation: 0.2627018902986312]
	TIME [epoch: 6.33 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2769604513637899		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 0.2769604513637899 | validation: 0.2806192109289902]
	TIME [epoch: 6.31 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28489324427421525		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 0.28489324427421525 | validation: 0.28279818302582316]
	TIME [epoch: 6.31 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2923534017907656		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 0.2923534017907656 | validation: 0.28733195653716587]
	TIME [epoch: 6.31 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28350435450130596		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 0.28350435450130596 | validation: 0.30460309180053097]
	TIME [epoch: 6.31 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29385666478460126		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 0.29385666478460126 | validation: 0.3217773348661179]
	TIME [epoch: 6.33 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29737333724396314		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 0.29737333724396314 | validation: 0.3053154758979645]
	TIME [epoch: 6.34 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2936873821871553		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 0.2936873821871553 | validation: 0.29927559086834343]
	TIME [epoch: 6.31 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2935656919013982		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 0.2935656919013982 | validation: 0.2953865525526387]
	TIME [epoch: 6.31 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2838086715661621		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 0.2838086715661621 | validation: 0.2731215508033383]
	TIME [epoch: 6.31 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27237129835762786		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 0.27237129835762786 | validation: 0.2673755138074121]
	TIME [epoch: 6.31 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2798020298318922		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 0.2798020298318922 | validation: 0.2731255782164918]
	TIME [epoch: 6.33 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26997915043952303		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 0.26997915043952303 | validation: 0.27556204454968775]
	TIME [epoch: 6.35 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2708063094644706		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 0.2708063094644706 | validation: 0.27305833448695266]
	TIME [epoch: 6.32 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2754665899932038		[learning rate: 7.8942e-05]
	Learning Rate: 7.8942e-05
	LOSS [training: 0.2754665899932038 | validation: 0.2989051783961516]
	TIME [epoch: 6.31 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2767750795935012		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 0.2767750795935012 | validation: 0.2905089922525408]
	TIME [epoch: 6.31 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2952810925537527		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 0.2952810925537527 | validation: 0.3076554851577455]
	TIME [epoch: 6.31 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3135347263917504		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 0.3135347263917504 | validation: 0.3077832451025079]
	TIME [epoch: 6.33 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2955031028402026		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 0.2955031028402026 | validation: 0.2731071697012354]
	TIME [epoch: 6.34 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28719352451597		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 0.28719352451597 | validation: 0.2758880805452065]
	TIME [epoch: 6.31 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3004929798576905		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 0.3004929798576905 | validation: 0.2650826955239065]
	TIME [epoch: 6.31 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2901672618248853		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 0.2901672618248853 | validation: 0.27283107910126847]
	TIME [epoch: 6.31 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28448848434219753		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.28448848434219753 | validation: 0.26529883777877494]
	TIME [epoch: 6.31 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28925978333768887		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 0.28925978333768887 | validation: 0.2792535228822489]
	TIME [epoch: 6.32 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2921021966845966		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 0.2921021966845966 | validation: 0.2902177821614241]
	TIME [epoch: 6.35 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.296309866266933		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 0.296309866266933 | validation: 0.268493073233583]
	TIME [epoch: 6.31 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28420170720459254		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 0.28420170720459254 | validation: 0.2721208791891919]
	TIME [epoch: 6.31 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27720590670306766		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 0.27720590670306766 | validation: 0.27003695760173096]
	TIME [epoch: 6.31 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2819533690840287		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 0.2819533690840287 | validation: 0.27900897672982544]
	TIME [epoch: 6.31 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2929281818003626		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 0.2929281818003626 | validation: 0.2585318980120499]
	TIME [epoch: 6.32 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27444706833213306		[learning rate: 7.4592e-05]
	Learning Rate: 7.4592e-05
	LOSS [training: 0.27444706833213306 | validation: 0.2813254303496281]
	TIME [epoch: 6.36 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2727698188874151		[learning rate: 7.4328e-05]
	Learning Rate: 7.43283e-05
	LOSS [training: 0.2727698188874151 | validation: 0.27752409921918947]
	TIME [epoch: 6.32 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2729422705855448		[learning rate: 7.4065e-05]
	Learning Rate: 7.40654e-05
	LOSS [training: 0.2729422705855448 | validation: 0.2701453639841404]
	TIME [epoch: 6.31 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27045947067104903		[learning rate: 7.3803e-05]
	Learning Rate: 7.38035e-05
	LOSS [training: 0.27045947067104903 | validation: 0.2630415253698049]
	TIME [epoch: 6.32 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27453362315027446		[learning rate: 7.3543e-05]
	Learning Rate: 7.35425e-05
	LOSS [training: 0.27453362315027446 | validation: 0.25584695391429446]
	TIME [epoch: 6.31 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2703525634917886		[learning rate: 7.3282e-05]
	Learning Rate: 7.32825e-05
	LOSS [training: 0.2703525634917886 | validation: 0.25193257091474724]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1438.pth
	Model improved!!!
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2714417384240445		[learning rate: 7.3023e-05]
	Learning Rate: 7.30233e-05
	LOSS [training: 0.2714417384240445 | validation: 0.26809410474408]
	TIME [epoch: 6.36 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27416290204709176		[learning rate: 7.2765e-05]
	Learning Rate: 7.27651e-05
	LOSS [training: 0.27416290204709176 | validation: 0.2726389945947695]
	TIME [epoch: 6.32 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2807387253492034		[learning rate: 7.2508e-05]
	Learning Rate: 7.25078e-05
	LOSS [training: 0.2807387253492034 | validation: 0.2668961741677093]
	TIME [epoch: 6.31 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2752268594100618		[learning rate: 7.2251e-05]
	Learning Rate: 7.22514e-05
	LOSS [training: 0.2752268594100618 | validation: 0.28256043932701835]
	TIME [epoch: 6.31 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2783996424247256		[learning rate: 7.1996e-05]
	Learning Rate: 7.19959e-05
	LOSS [training: 0.2783996424247256 | validation: 0.26781681388808676]
	TIME [epoch: 6.31 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2757588615447579		[learning rate: 7.1741e-05]
	Learning Rate: 7.17413e-05
	LOSS [training: 0.2757588615447579 | validation: 0.26511337651362926]
	TIME [epoch: 6.33 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2751623156098834		[learning rate: 7.1488e-05]
	Learning Rate: 7.14876e-05
	LOSS [training: 0.2751623156098834 | validation: 0.264526562559637]
	TIME [epoch: 6.36 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28239992895020727		[learning rate: 7.1235e-05]
	Learning Rate: 7.12348e-05
	LOSS [training: 0.28239992895020727 | validation: 0.2655131680908776]
	TIME [epoch: 6.31 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2698609039325131		[learning rate: 7.0983e-05]
	Learning Rate: 7.09829e-05
	LOSS [training: 0.2698609039325131 | validation: 0.2686549245262038]
	TIME [epoch: 6.31 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2683325011187176		[learning rate: 7.0732e-05]
	Learning Rate: 7.07319e-05
	LOSS [training: 0.2683325011187176 | validation: 0.2757620562041814]
	TIME [epoch: 6.31 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2717466685958242		[learning rate: 7.0482e-05]
	Learning Rate: 7.04818e-05
	LOSS [training: 0.2717466685958242 | validation: 0.27110435943942945]
	TIME [epoch: 6.31 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2665083975198622		[learning rate: 7.0233e-05]
	Learning Rate: 7.02326e-05
	LOSS [training: 0.2665083975198622 | validation: 0.2688020039547473]
	TIME [epoch: 6.32 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28154974217010037		[learning rate: 6.9984e-05]
	Learning Rate: 6.99842e-05
	LOSS [training: 0.28154974217010037 | validation: 0.25680397823678497]
	TIME [epoch: 6.36 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27239163586279513		[learning rate: 6.9737e-05]
	Learning Rate: 6.97367e-05
	LOSS [training: 0.27239163586279513 | validation: 0.26969975516886396]
	TIME [epoch: 6.32 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2777134857785607		[learning rate: 6.949e-05]
	Learning Rate: 6.94901e-05
	LOSS [training: 0.2777134857785607 | validation: 0.2669205990615875]
	TIME [epoch: 6.32 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27955801516994644		[learning rate: 6.9244e-05]
	Learning Rate: 6.92444e-05
	LOSS [training: 0.27955801516994644 | validation: 0.2666332577434931]
	TIME [epoch: 6.31 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2772602095361318		[learning rate: 6.9e-05]
	Learning Rate: 6.89995e-05
	LOSS [training: 0.2772602095361318 | validation: 0.261354258499111]
	TIME [epoch: 6.31 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2775636699260646		[learning rate: 6.8756e-05]
	Learning Rate: 6.87555e-05
	LOSS [training: 0.2775636699260646 | validation: 0.28286541369208407]
	TIME [epoch: 6.32 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2803686303321469		[learning rate: 6.8512e-05]
	Learning Rate: 6.85124e-05
	LOSS [training: 0.2803686303321469 | validation: 0.2731854007532502]
	TIME [epoch: 6.36 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2861270804469217		[learning rate: 6.827e-05]
	Learning Rate: 6.82702e-05
	LOSS [training: 0.2861270804469217 | validation: 0.2600541685520773]
	TIME [epoch: 6.31 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28151256498201604		[learning rate: 6.8029e-05]
	Learning Rate: 6.80287e-05
	LOSS [training: 0.28151256498201604 | validation: 0.26488712587107327]
	TIME [epoch: 6.31 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27265404849823904		[learning rate: 6.7788e-05]
	Learning Rate: 6.77882e-05
	LOSS [training: 0.27265404849823904 | validation: 0.26079001543629854]
	TIME [epoch: 6.31 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28292592379505965		[learning rate: 6.7548e-05]
	Learning Rate: 6.75485e-05
	LOSS [training: 0.28292592379505965 | validation: 0.28100904208950545]
	TIME [epoch: 6.31 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2782244433231221		[learning rate: 6.731e-05]
	Learning Rate: 6.73096e-05
	LOSS [training: 0.2782244433231221 | validation: 0.2617976912199421]
	TIME [epoch: 6.32 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27466783095389835		[learning rate: 6.7072e-05]
	Learning Rate: 6.70716e-05
	LOSS [training: 0.27466783095389835 | validation: 0.2739131686163443]
	TIME [epoch: 6.35 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2701542545783723		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 0.2701542545783723 | validation: 0.2707060129486523]
	TIME [epoch: 6.31 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2661007194624887		[learning rate: 6.6598e-05]
	Learning Rate: 6.65981e-05
	LOSS [training: 0.2661007194624887 | validation: 0.2552997927363276]
	TIME [epoch: 6.32 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2746552258618352		[learning rate: 6.6363e-05]
	Learning Rate: 6.63626e-05
	LOSS [training: 0.2746552258618352 | validation: 0.26926649002764746]
	TIME [epoch: 6.34 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2763192584515266		[learning rate: 6.6128e-05]
	Learning Rate: 6.61279e-05
	LOSS [training: 0.2763192584515266 | validation: 0.2730302812233312]
	TIME [epoch: 6.31 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27285160941078185		[learning rate: 6.5894e-05]
	Learning Rate: 6.58941e-05
	LOSS [training: 0.27285160941078185 | validation: 0.2543994945679875]
	TIME [epoch: 6.32 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27912761404220593		[learning rate: 6.5661e-05]
	Learning Rate: 6.5661e-05
	LOSS [training: 0.27912761404220593 | validation: 0.2732729180691835]
	TIME [epoch: 6.36 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28336368248209154		[learning rate: 6.5429e-05]
	Learning Rate: 6.54289e-05
	LOSS [training: 0.28336368248209154 | validation: 0.26545701863689586]
	TIME [epoch: 6.32 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29172325068330435		[learning rate: 6.5197e-05]
	Learning Rate: 6.51975e-05
	LOSS [training: 0.29172325068330435 | validation: 0.2815605813653035]
	TIME [epoch: 6.32 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29293178266726616		[learning rate: 6.4967e-05]
	Learning Rate: 6.49669e-05
	LOSS [training: 0.29293178266726616 | validation: 0.2871590295089801]
	TIME [epoch: 6.31 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28744739088875704		[learning rate: 6.4737e-05]
	Learning Rate: 6.47372e-05
	LOSS [training: 0.28744739088875704 | validation: 0.2626753203421465]
	TIME [epoch: 6.31 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2883756154337545		[learning rate: 6.4508e-05]
	Learning Rate: 6.45083e-05
	LOSS [training: 0.2883756154337545 | validation: 0.26738252670397966]
	TIME [epoch: 6.31 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2751761254996665		[learning rate: 6.428e-05]
	Learning Rate: 6.42802e-05
	LOSS [training: 0.2751761254996665 | validation: 0.26137834687069583]
	TIME [epoch: 6.36 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2738742041543282		[learning rate: 6.4053e-05]
	Learning Rate: 6.40529e-05
	LOSS [training: 0.2738742041543282 | validation: 0.2790143832893017]
	TIME [epoch: 6.31 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2849546526730712		[learning rate: 6.3826e-05]
	Learning Rate: 6.38264e-05
	LOSS [training: 0.2849546526730712 | validation: 0.2772155266455828]
	TIME [epoch: 6.31 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27867061335629056		[learning rate: 6.3601e-05]
	Learning Rate: 6.36007e-05
	LOSS [training: 0.27867061335629056 | validation: 0.28389277005560665]
	TIME [epoch: 6.31 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.278312107829394		[learning rate: 6.3376e-05]
	Learning Rate: 6.33758e-05
	LOSS [training: 0.278312107829394 | validation: 0.28519034744226124]
	TIME [epoch: 6.31 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27575051413236		[learning rate: 6.3152e-05]
	Learning Rate: 6.31517e-05
	LOSS [training: 0.27575051413236 | validation: 0.2780396559383458]
	TIME [epoch: 6.31 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28418567988876076		[learning rate: 6.2928e-05]
	Learning Rate: 6.29283e-05
	LOSS [training: 0.28418567988876076 | validation: 0.2846454727325518]
	TIME [epoch: 6.36 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28083083370229234		[learning rate: 6.2706e-05]
	Learning Rate: 6.27058e-05
	LOSS [training: 0.28083083370229234 | validation: 0.2807712159428375]
	TIME [epoch: 6.31 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28184391047918267		[learning rate: 6.2484e-05]
	Learning Rate: 6.24841e-05
	LOSS [training: 0.28184391047918267 | validation: 0.27401966455476817]
	TIME [epoch: 6.31 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.279438134284297		[learning rate: 6.2263e-05]
	Learning Rate: 6.22631e-05
	LOSS [training: 0.279438134284297 | validation: 0.276463730349561]
	TIME [epoch: 6.31 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2750865072717202		[learning rate: 6.2043e-05]
	Learning Rate: 6.20429e-05
	LOSS [training: 0.2750865072717202 | validation: 0.2689329359042715]
	TIME [epoch: 6.31 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27766593819338214		[learning rate: 6.1824e-05]
	Learning Rate: 6.18235e-05
	LOSS [training: 0.27766593819338214 | validation: 0.27291265301564116]
	TIME [epoch: 6.31 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27288809863864394		[learning rate: 6.1605e-05]
	Learning Rate: 6.16049e-05
	LOSS [training: 0.27288809863864394 | validation: 0.2679906717915837]
	TIME [epoch: 6.35 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27237565801830577		[learning rate: 6.1387e-05]
	Learning Rate: 6.13871e-05
	LOSS [training: 0.27237565801830577 | validation: 0.2868579917904793]
	TIME [epoch: 6.32 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27467626506355297		[learning rate: 6.117e-05]
	Learning Rate: 6.117e-05
	LOSS [training: 0.27467626506355297 | validation: 0.2697015600907581]
	TIME [epoch: 6.31 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2759118621880846		[learning rate: 6.0954e-05]
	Learning Rate: 6.09537e-05
	LOSS [training: 0.2759118621880846 | validation: 0.26868503419595713]
	TIME [epoch: 6.31 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27782986164727663		[learning rate: 6.0738e-05]
	Learning Rate: 6.07382e-05
	LOSS [training: 0.27782986164727663 | validation: 0.26891986653629923]
	TIME [epoch: 6.31 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2762225100941458		[learning rate: 6.0523e-05]
	Learning Rate: 6.05234e-05
	LOSS [training: 0.2762225100941458 | validation: 0.2753173041806444]
	TIME [epoch: 6.31 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2826399120451033		[learning rate: 6.0309e-05]
	Learning Rate: 6.03094e-05
	LOSS [training: 0.2826399120451033 | validation: 0.27161672651628693]
	TIME [epoch: 6.35 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2744455880171867		[learning rate: 6.0096e-05]
	Learning Rate: 6.00961e-05
	LOSS [training: 0.2744455880171867 | validation: 0.27323137015627824]
	TIME [epoch: 6.33 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.270847284707995		[learning rate: 5.9884e-05]
	Learning Rate: 5.98836e-05
	LOSS [training: 0.270847284707995 | validation: 0.2580053316307237]
	TIME [epoch: 6.31 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27290198647479663		[learning rate: 5.9672e-05]
	Learning Rate: 5.96718e-05
	LOSS [training: 0.27290198647479663 | validation: 0.2599150203042656]
	TIME [epoch: 6.31 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.272587466206371		[learning rate: 5.9461e-05]
	Learning Rate: 5.94608e-05
	LOSS [training: 0.272587466206371 | validation: 0.2623085765114448]
	TIME [epoch: 6.31 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2688596032759002		[learning rate: 5.9251e-05]
	Learning Rate: 5.92505e-05
	LOSS [training: 0.2688596032759002 | validation: 0.26036079854627725]
	TIME [epoch: 6.31 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27244283501478705		[learning rate: 5.9041e-05]
	Learning Rate: 5.9041e-05
	LOSS [training: 0.27244283501478705 | validation: 0.2784539285621449]
	TIME [epoch: 6.33 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27433839018901324		[learning rate: 5.8832e-05]
	Learning Rate: 5.88323e-05
	LOSS [training: 0.27433839018901324 | validation: 0.2734017843720986]
	TIME [epoch: 6.34 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27701143503451214		[learning rate: 5.8624e-05]
	Learning Rate: 5.86242e-05
	LOSS [training: 0.27701143503451214 | validation: 0.2623614593458813]
	TIME [epoch: 6.31 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28119148326645704		[learning rate: 5.8417e-05]
	Learning Rate: 5.84169e-05
	LOSS [training: 0.28119148326645704 | validation: 0.2696775847788378]
	TIME [epoch: 6.31 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29234174057814344		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 0.29234174057814344 | validation: 0.2726581317095002]
	TIME [epoch: 6.31 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27848474462475964		[learning rate: 5.8004e-05]
	Learning Rate: 5.80045e-05
	LOSS [training: 0.27848474462475964 | validation: 0.2735948352927442]
	TIME [epoch: 6.31 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2810390999120008		[learning rate: 5.7799e-05]
	Learning Rate: 5.77994e-05
	LOSS [training: 0.2810390999120008 | validation: 0.27185990326998644]
	TIME [epoch: 6.33 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2822163095206394		[learning rate: 5.7595e-05]
	Learning Rate: 5.7595e-05
	LOSS [training: 0.2822163095206394 | validation: 0.2646493773853339]
	TIME [epoch: 6.34 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27701238460967315		[learning rate: 5.7391e-05]
	Learning Rate: 5.73913e-05
	LOSS [training: 0.27701238460967315 | validation: 0.2876070599203837]
	TIME [epoch: 6.31 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2830301035851881		[learning rate: 5.7188e-05]
	Learning Rate: 5.71884e-05
	LOSS [training: 0.2830301035851881 | validation: 0.30642924042389774]
	TIME [epoch: 6.31 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2967211314140257		[learning rate: 5.6986e-05]
	Learning Rate: 5.69861e-05
	LOSS [training: 0.2967211314140257 | validation: 0.3141029360732304]
	TIME [epoch: 6.31 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31937539601874537		[learning rate: 5.6785e-05]
	Learning Rate: 5.67846e-05
	LOSS [training: 0.31937539601874537 | validation: 0.3048730906926078]
	TIME [epoch: 6.32 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30628508480195693		[learning rate: 5.6584e-05]
	Learning Rate: 5.65838e-05
	LOSS [training: 0.30628508480195693 | validation: 0.2909009460386868]
	TIME [epoch: 6.34 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2994980600509354		[learning rate: 5.6384e-05]
	Learning Rate: 5.63837e-05
	LOSS [training: 0.2994980600509354 | validation: 0.27558402886719857]
	TIME [epoch: 6.34 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2825875770727758		[learning rate: 5.6184e-05]
	Learning Rate: 5.61844e-05
	LOSS [training: 0.2825875770727758 | validation: 0.26746565985828236]
	TIME [epoch: 6.31 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2906644761114264		[learning rate: 5.5986e-05]
	Learning Rate: 5.59857e-05
	LOSS [training: 0.2906644761114264 | validation: 0.2666092370485277]
	TIME [epoch: 6.31 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28148279932849174		[learning rate: 5.5788e-05]
	Learning Rate: 5.57877e-05
	LOSS [training: 0.28148279932849174 | validation: 0.2854026254432032]
	TIME [epoch: 6.31 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28235264033157476		[learning rate: 5.559e-05]
	Learning Rate: 5.55904e-05
	LOSS [training: 0.28235264033157476 | validation: 0.2814687804871827]
	TIME [epoch: 6.31 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2753085737696123		[learning rate: 5.5394e-05]
	Learning Rate: 5.53939e-05
	LOSS [training: 0.2753085737696123 | validation: 0.25989739145360746]
	TIME [epoch: 6.33 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28283273199020814		[learning rate: 5.5198e-05]
	Learning Rate: 5.5198e-05
	LOSS [training: 0.28283273199020814 | validation: 0.26286419413336926]
	TIME [epoch: 6.35 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2760006050414257		[learning rate: 5.5003e-05]
	Learning Rate: 5.50028e-05
	LOSS [training: 0.2760006050414257 | validation: 0.2588893990700848]
	TIME [epoch: 6.31 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27440798239681186		[learning rate: 5.4808e-05]
	Learning Rate: 5.48083e-05
	LOSS [training: 0.27440798239681186 | validation: 0.25306287846900133]
	TIME [epoch: 6.31 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2709028648716617		[learning rate: 5.4614e-05]
	Learning Rate: 5.46145e-05
	LOSS [training: 0.2709028648716617 | validation: 0.260692825578669]
	TIME [epoch: 6.31 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27141239981882037		[learning rate: 5.4421e-05]
	Learning Rate: 5.44213e-05
	LOSS [training: 0.27141239981882037 | validation: 0.2765377488762335]
	TIME [epoch: 6.31 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28080862719506955		[learning rate: 5.4229e-05]
	Learning Rate: 5.42289e-05
	LOSS [training: 0.28080862719506955 | validation: 0.2742860766409694]
	TIME [epoch: 6.32 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28092454749279616		[learning rate: 5.4037e-05]
	Learning Rate: 5.40371e-05
	LOSS [training: 0.28092454749279616 | validation: 0.283896031193035]
	TIME [epoch: 6.35 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2786686183047862		[learning rate: 5.3846e-05]
	Learning Rate: 5.38461e-05
	LOSS [training: 0.2786686183047862 | validation: 0.25888401557322516]
	TIME [epoch: 6.32 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28324158829044155		[learning rate: 5.3656e-05]
	Learning Rate: 5.36556e-05
	LOSS [training: 0.28324158829044155 | validation: 0.28047680422257537]
	TIME [epoch: 6.31 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.282587584591264		[learning rate: 5.3466e-05]
	Learning Rate: 5.34659e-05
	LOSS [training: 0.282587584591264 | validation: 0.2648411490839878]
	TIME [epoch: 6.31 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27507237879055374		[learning rate: 5.3277e-05]
	Learning Rate: 5.32769e-05
	LOSS [training: 0.27507237879055374 | validation: 0.2534233632904696]
	TIME [epoch: 6.31 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26495746869878256		[learning rate: 5.3088e-05]
	Learning Rate: 5.30884e-05
	LOSS [training: 0.26495746869878256 | validation: 0.270913000940453]
	TIME [epoch: 6.32 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2782811273449053		[learning rate: 5.2901e-05]
	Learning Rate: 5.29007e-05
	LOSS [training: 0.2782811273449053 | validation: 0.25722222053541766]
	TIME [epoch: 6.35 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2732729843344624		[learning rate: 5.2714e-05]
	Learning Rate: 5.27137e-05
	LOSS [training: 0.2732729843344624 | validation: 0.2720963424494962]
	TIME [epoch: 6.31 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2801587090147467		[learning rate: 5.2527e-05]
	Learning Rate: 5.25272e-05
	LOSS [training: 0.2801587090147467 | validation: 0.256867907199098]
	TIME [epoch: 6.31 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27359179501733677		[learning rate: 5.2342e-05]
	Learning Rate: 5.23415e-05
	LOSS [training: 0.27359179501733677 | validation: 0.2605616336504095]
	TIME [epoch: 6.31 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27307403840110506		[learning rate: 5.2156e-05]
	Learning Rate: 5.21564e-05
	LOSS [training: 0.27307403840110506 | validation: 0.2620275678928033]
	TIME [epoch: 6.31 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2714172125273064		[learning rate: 5.1972e-05]
	Learning Rate: 5.1972e-05
	LOSS [training: 0.2714172125273064 | validation: 0.2464095857953302]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1535.pth
	Model improved!!!
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26505240025330745		[learning rate: 5.1788e-05]
	Learning Rate: 5.17882e-05
	LOSS [training: 0.26505240025330745 | validation: 0.2637532168660494]
	TIME [epoch: 6.36 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2692056736776548		[learning rate: 5.1605e-05]
	Learning Rate: 5.16051e-05
	LOSS [training: 0.2692056736776548 | validation: 0.272480212239405]
	TIME [epoch: 6.31 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27651736528488		[learning rate: 5.1423e-05]
	Learning Rate: 5.14226e-05
	LOSS [training: 0.27651736528488 | validation: 0.26270437085266723]
	TIME [epoch: 6.31 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2715095468280883		[learning rate: 5.1241e-05]
	Learning Rate: 5.12407e-05
	LOSS [training: 0.2715095468280883 | validation: 0.25864527719732733]
	TIME [epoch: 6.31 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27738220627942534		[learning rate: 5.106e-05]
	Learning Rate: 5.10596e-05
	LOSS [training: 0.27738220627942534 | validation: 0.25517674068648794]
	TIME [epoch: 6.31 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28136608505247734		[learning rate: 5.0879e-05]
	Learning Rate: 5.0879e-05
	LOSS [training: 0.28136608505247734 | validation: 0.2728007434182585]
	TIME [epoch: 6.31 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2882898090906022		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 0.2882898090906022 | validation: 0.2840779741024049]
	TIME [epoch: 6.36 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29137303129452985		[learning rate: 5.052e-05]
	Learning Rate: 5.05198e-05
	LOSS [training: 0.29137303129452985 | validation: 0.271606112924657]
	TIME [epoch: 6.32 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2804405497383864		[learning rate: 5.0341e-05]
	Learning Rate: 5.03412e-05
	LOSS [training: 0.2804405497383864 | validation: 0.2552496899237321]
	TIME [epoch: 6.31 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2784515974440723		[learning rate: 5.0163e-05]
	Learning Rate: 5.01631e-05
	LOSS [training: 0.2784515974440723 | validation: 0.2525991101470314]
	TIME [epoch: 6.31 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28336610305542725		[learning rate: 4.9986e-05]
	Learning Rate: 4.99857e-05
	LOSS [training: 0.28336610305542725 | validation: 0.25735664301525357]
	TIME [epoch: 6.31 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2883555879097931		[learning rate: 4.9809e-05]
	Learning Rate: 4.9809e-05
	LOSS [training: 0.2883555879097931 | validation: 0.2669264869913944]
	TIME [epoch: 6.32 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29558903530184377		[learning rate: 4.9633e-05]
	Learning Rate: 4.96329e-05
	LOSS [training: 0.29558903530184377 | validation: 0.2800830145007718]
	TIME [epoch: 6.36 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30660973738677977		[learning rate: 4.9457e-05]
	Learning Rate: 4.94573e-05
	LOSS [training: 0.30660973738677977 | validation: 0.2947432798628637]
	TIME [epoch: 6.32 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3057652088098866		[learning rate: 4.9282e-05]
	Learning Rate: 4.92825e-05
	LOSS [training: 0.3057652088098866 | validation: 0.2689372975815034]
	TIME [epoch: 6.31 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29571570829713023		[learning rate: 4.9108e-05]
	Learning Rate: 4.91082e-05
	LOSS [training: 0.29571570829713023 | validation: 0.2673169807656527]
	TIME [epoch: 6.31 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28290565559271424		[learning rate: 4.8935e-05]
	Learning Rate: 4.89345e-05
	LOSS [training: 0.28290565559271424 | validation: 0.27045577407038746]
	TIME [epoch: 6.31 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.278160236894079		[learning rate: 4.8762e-05]
	Learning Rate: 4.87615e-05
	LOSS [training: 0.278160236894079 | validation: 0.2664982388863433]
	TIME [epoch: 6.32 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2732770933592298		[learning rate: 4.8589e-05]
	Learning Rate: 4.85891e-05
	LOSS [training: 0.2732770933592298 | validation: 0.2568716585788282]
	TIME [epoch: 6.36 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2754194606319659		[learning rate: 4.8417e-05]
	Learning Rate: 4.84172e-05
	LOSS [training: 0.2754194606319659 | validation: 0.2560688592479311]
	TIME [epoch: 6.31 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27444597559054484		[learning rate: 4.8246e-05]
	Learning Rate: 4.8246e-05
	LOSS [training: 0.27444597559054484 | validation: 0.2660553889983166]
	TIME [epoch: 6.31 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2700080646129911		[learning rate: 4.8075e-05]
	Learning Rate: 4.80754e-05
	LOSS [training: 0.2700080646129911 | validation: 0.256387199691978]
	TIME [epoch: 6.31 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26830402028630307		[learning rate: 4.7905e-05]
	Learning Rate: 4.79054e-05
	LOSS [training: 0.26830402028630307 | validation: 0.2611106477122648]
	TIME [epoch: 6.31 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2751212061235354		[learning rate: 4.7736e-05]
	Learning Rate: 4.7736e-05
	LOSS [training: 0.2751212061235354 | validation: 0.27209988817407543]
	TIME [epoch: 6.32 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2704839310509838		[learning rate: 4.7567e-05]
	Learning Rate: 4.75672e-05
	LOSS [training: 0.2704839310509838 | validation: 0.2638077012442305]
	TIME [epoch: 6.35 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26678484836336963		[learning rate: 4.7399e-05]
	Learning Rate: 4.7399e-05
	LOSS [training: 0.26678484836336963 | validation: 0.2535475974170363]
	TIME [epoch: 6.32 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2733778719281485		[learning rate: 4.7231e-05]
	Learning Rate: 4.72314e-05
	LOSS [training: 0.2733778719281485 | validation: 0.25053919012990294]
	TIME [epoch: 6.31 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2833998605483499		[learning rate: 4.7064e-05]
	Learning Rate: 4.70644e-05
	LOSS [training: 0.2833998605483499 | validation: 0.2611336827890967]
	TIME [epoch: 6.31 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28736182141378575		[learning rate: 4.6898e-05]
	Learning Rate: 4.6898e-05
	LOSS [training: 0.28736182141378575 | validation: 0.2622725116689487]
	TIME [epoch: 6.31 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28399370951589686		[learning rate: 4.6732e-05]
	Learning Rate: 4.67321e-05
	LOSS [training: 0.28399370951589686 | validation: 0.25902775574721154]
	TIME [epoch: 6.31 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27467425288424085		[learning rate: 4.6567e-05]
	Learning Rate: 4.65669e-05
	LOSS [training: 0.27467425288424085 | validation: 0.2566397393400355]
	TIME [epoch: 6.36 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28098801874326546		[learning rate: 4.6402e-05]
	Learning Rate: 4.64022e-05
	LOSS [training: 0.28098801874326546 | validation: 0.2731414724705231]
	TIME [epoch: 6.32 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2739560518929054		[learning rate: 4.6238e-05]
	Learning Rate: 4.62381e-05
	LOSS [training: 0.2739560518929054 | validation: 0.2658343154950713]
	TIME [epoch: 6.3 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26735083697493656		[learning rate: 4.6075e-05]
	Learning Rate: 4.60746e-05
	LOSS [training: 0.26735083697493656 | validation: 0.2661868139186735]
	TIME [epoch: 6.31 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2697712246797753		[learning rate: 4.5912e-05]
	Learning Rate: 4.59117e-05
	LOSS [training: 0.2697712246797753 | validation: 0.2675566669163643]
	TIME [epoch: 6.31 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27873346112343833		[learning rate: 4.5749e-05]
	Learning Rate: 4.57493e-05
	LOSS [training: 0.27873346112343833 | validation: 0.26780093531637783]
	TIME [epoch: 6.31 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2756018285092119		[learning rate: 4.5588e-05]
	Learning Rate: 4.55875e-05
	LOSS [training: 0.2756018285092119 | validation: 0.27707190729248576]
	TIME [epoch: 6.35 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2804283335945231		[learning rate: 4.5426e-05]
	Learning Rate: 4.54264e-05
	LOSS [training: 0.2804283335945231 | validation: 0.269731013833507]
	TIME [epoch: 6.33 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2721774798345004		[learning rate: 4.5266e-05]
	Learning Rate: 4.52657e-05
	LOSS [training: 0.2721774798345004 | validation: 0.2634095990354335]
	TIME [epoch: 6.31 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27439136486922333		[learning rate: 4.5106e-05]
	Learning Rate: 4.51056e-05
	LOSS [training: 0.27439136486922333 | validation: 0.2758928514773776]
	TIME [epoch: 6.31 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2774174710013767		[learning rate: 4.4946e-05]
	Learning Rate: 4.49461e-05
	LOSS [training: 0.2774174710013767 | validation: 0.273534049962502]
	TIME [epoch: 6.31 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26774038352688867		[learning rate: 4.4787e-05]
	Learning Rate: 4.47872e-05
	LOSS [training: 0.26774038352688867 | validation: 0.2683866936525218]
	TIME [epoch: 6.31 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2662448562096102		[learning rate: 4.4629e-05]
	Learning Rate: 4.46288e-05
	LOSS [training: 0.2662448562096102 | validation: 0.24807263185757825]
	TIME [epoch: 6.35 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2639709214169871		[learning rate: 4.4471e-05]
	Learning Rate: 4.4471e-05
	LOSS [training: 0.2639709214169871 | validation: 0.2691738517122853]
	TIME [epoch: 6.33 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26440130431646525		[learning rate: 4.4314e-05]
	Learning Rate: 4.43138e-05
	LOSS [training: 0.26440130431646525 | validation: 0.27237279303010875]
	TIME [epoch: 6.32 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27068802958713223		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 0.27068802958713223 | validation: 0.2602723639662792]
	TIME [epoch: 6.31 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2682426726664179		[learning rate: 4.4001e-05]
	Learning Rate: 4.40009e-05
	LOSS [training: 0.2682426726664179 | validation: 0.2674694403762334]
	TIME [epoch: 6.32 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2683237010995905		[learning rate: 4.3845e-05]
	Learning Rate: 4.38453e-05
	LOSS [training: 0.2683237010995905 | validation: 0.2721702867136608]
	TIME [epoch: 6.31 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2727926230759553		[learning rate: 4.369e-05]
	Learning Rate: 4.36903e-05
	LOSS [training: 0.2727926230759553 | validation: 0.25896183713712234]
	TIME [epoch: 6.35 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26410509456149467		[learning rate: 4.3536e-05]
	Learning Rate: 4.35358e-05
	LOSS [training: 0.26410509456149467 | validation: 0.2509391683012445]
	TIME [epoch: 6.33 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26368997869412314		[learning rate: 4.3382e-05]
	Learning Rate: 4.33818e-05
	LOSS [training: 0.26368997869412314 | validation: 0.25817231671427643]
	TIME [epoch: 6.31 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27126683786238404		[learning rate: 4.3228e-05]
	Learning Rate: 4.32284e-05
	LOSS [training: 0.27126683786238404 | validation: 0.24526687452929252]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1587.pth
	Model improved!!!
EPOCH 1588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2645612170748465		[learning rate: 4.3076e-05]
	Learning Rate: 4.30756e-05
	LOSS [training: 0.2645612170748465 | validation: 0.25821321436523703]
	TIME [epoch: 6.31 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26959158888664125		[learning rate: 4.2923e-05]
	Learning Rate: 4.29232e-05
	LOSS [training: 0.26959158888664125 | validation: 0.2573612712833738]
	TIME [epoch: 6.31 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2741851549437045		[learning rate: 4.2771e-05]
	Learning Rate: 4.27714e-05
	LOSS [training: 0.2741851549437045 | validation: 0.2562634620017564]
	TIME [epoch: 6.34 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2702975913553635		[learning rate: 4.262e-05]
	Learning Rate: 4.26202e-05
	LOSS [training: 0.2702975913553635 | validation: 0.25741213646056293]
	TIME [epoch: 6.33 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26593710923801095		[learning rate: 4.2469e-05]
	Learning Rate: 4.24695e-05
	LOSS [training: 0.26593710923801095 | validation: 0.25453600251520236]
	TIME [epoch: 6.31 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2719714449763382		[learning rate: 4.2319e-05]
	Learning Rate: 4.23193e-05
	LOSS [training: 0.2719714449763382 | validation: 0.2553667700711194]
	TIME [epoch: 6.31 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2707543043729983		[learning rate: 4.217e-05]
	Learning Rate: 4.21697e-05
	LOSS [training: 0.2707543043729983 | validation: 0.2562014040249908]
	TIME [epoch: 6.31 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26680819826182434		[learning rate: 4.2021e-05]
	Learning Rate: 4.20205e-05
	LOSS [training: 0.26680819826182434 | validation: 0.25896628360474194]
	TIME [epoch: 6.31 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27066151482185996		[learning rate: 4.1872e-05]
	Learning Rate: 4.18719e-05
	LOSS [training: 0.27066151482185996 | validation: 0.2604848101562808]
	TIME [epoch: 6.34 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26396294612584204		[learning rate: 4.1724e-05]
	Learning Rate: 4.17239e-05
	LOSS [training: 0.26396294612584204 | validation: 0.25285721816261264]
	TIME [epoch: 6.33 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2689212905866102		[learning rate: 4.1576e-05]
	Learning Rate: 4.15763e-05
	LOSS [training: 0.2689212905866102 | validation: 0.2548804777904018]
	TIME [epoch: 6.31 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27023769965604827		[learning rate: 4.1429e-05]
	Learning Rate: 4.14293e-05
	LOSS [training: 0.27023769965604827 | validation: 0.24536293125233233]
	TIME [epoch: 6.31 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27283935025613343		[learning rate: 4.1283e-05]
	Learning Rate: 4.12828e-05
	LOSS [training: 0.27283935025613343 | validation: 0.2762480060544187]
	TIME [epoch: 6.3 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27202750111615853		[learning rate: 4.1137e-05]
	Learning Rate: 4.11368e-05
	LOSS [training: 0.27202750111615853 | validation: 0.28539930305464445]
	TIME [epoch: 6.31 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.288182104043634		[learning rate: 4.0991e-05]
	Learning Rate: 4.09914e-05
	LOSS [training: 0.288182104043634 | validation: 0.29218656446035407]
	TIME [epoch: 6.32 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2912942921431158		[learning rate: 4.0846e-05]
	Learning Rate: 4.08464e-05
	LOSS [training: 0.2912942921431158 | validation: 0.27873843919290164]
	TIME [epoch: 6.35 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2880937406758708		[learning rate: 4.0702e-05]
	Learning Rate: 4.0702e-05
	LOSS [training: 0.2880937406758708 | validation: 0.2710852180505315]
	TIME [epoch: 6.31 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2760069977240517		[learning rate: 4.0558e-05]
	Learning Rate: 4.0558e-05
	LOSS [training: 0.2760069977240517 | validation: 0.2740769635124675]
	TIME [epoch: 6.31 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26588251606811425		[learning rate: 4.0415e-05]
	Learning Rate: 4.04146e-05
	LOSS [training: 0.26588251606811425 | validation: 0.2611877009013029]
	TIME [epoch: 6.31 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26975441907070136		[learning rate: 4.0272e-05]
	Learning Rate: 4.02717e-05
	LOSS [training: 0.26975441907070136 | validation: 0.27594537692629995]
	TIME [epoch: 6.31 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26969395203278035		[learning rate: 4.0129e-05]
	Learning Rate: 4.01293e-05
	LOSS [training: 0.26969395203278035 | validation: 0.2660571238079947]
	TIME [epoch: 6.33 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27251670782953286		[learning rate: 3.9987e-05]
	Learning Rate: 3.99874e-05
	LOSS [training: 0.27251670782953286 | validation: 0.2631153272369541]
	TIME [epoch: 6.33 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2695038588285571		[learning rate: 3.9846e-05]
	Learning Rate: 3.9846e-05
	LOSS [training: 0.2695038588285571 | validation: 0.2669071635893093]
	TIME [epoch: 6.31 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2668099252457451		[learning rate: 3.9705e-05]
	Learning Rate: 3.97051e-05
	LOSS [training: 0.2668099252457451 | validation: 0.2610095680019392]
	TIME [epoch: 6.3 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26726388941050994		[learning rate: 3.9565e-05]
	Learning Rate: 3.95647e-05
	LOSS [training: 0.26726388941050994 | validation: 0.27161921745836065]
	TIME [epoch: 6.31 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2819378403666839		[learning rate: 3.9425e-05]
	Learning Rate: 3.94248e-05
	LOSS [training: 0.2819378403666839 | validation: 0.2638658128018512]
	TIME [epoch: 6.31 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27678210012752935		[learning rate: 3.9285e-05]
	Learning Rate: 3.92854e-05
	LOSS [training: 0.27678210012752935 | validation: 0.26418688994100653]
	TIME [epoch: 6.33 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26414977781426363		[learning rate: 3.9146e-05]
	Learning Rate: 3.91464e-05
	LOSS [training: 0.26414977781426363 | validation: 0.27005225591638715]
	TIME [epoch: 6.34 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2640754828616198		[learning rate: 3.9008e-05]
	Learning Rate: 3.9008e-05
	LOSS [training: 0.2640754828616198 | validation: 0.2492445105956822]
	TIME [epoch: 6.31 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2626890015394431		[learning rate: 3.887e-05]
	Learning Rate: 3.88701e-05
	LOSS [training: 0.2626890015394431 | validation: 0.25774661843543273]
	TIME [epoch: 6.31 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2649630159822534		[learning rate: 3.8733e-05]
	Learning Rate: 3.87326e-05
	LOSS [training: 0.2649630159822534 | validation: 0.2623094170195294]
	TIME [epoch: 6.31 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26376394722156316		[learning rate: 3.8596e-05]
	Learning Rate: 3.85957e-05
	LOSS [training: 0.26376394722156316 | validation: 0.24876600296627832]
	TIME [epoch: 6.31 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2691754380516997		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 0.2691754380516997 | validation: 0.2528779315157745]
	TIME [epoch: 6.33 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26758978439516024		[learning rate: 3.8323e-05]
	Learning Rate: 3.83232e-05
	LOSS [training: 0.26758978439516024 | validation: 0.25900408659389895]
	TIME [epoch: 6.35 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26515121385354734		[learning rate: 3.8188e-05]
	Learning Rate: 3.81877e-05
	LOSS [training: 0.26515121385354734 | validation: 0.24757867860894262]
	TIME [epoch: 6.31 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2676101095651008		[learning rate: 3.8053e-05]
	Learning Rate: 3.80526e-05
	LOSS [training: 0.2676101095651008 | validation: 0.25278945387877244]
	TIME [epoch: 6.31 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2701264862739962		[learning rate: 3.7918e-05]
	Learning Rate: 3.79181e-05
	LOSS [training: 0.2701264862739962 | validation: 0.24449600673313585]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1624.pth
	Model improved!!!
EPOCH 1625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25858640577172193		[learning rate: 3.7784e-05]
	Learning Rate: 3.7784e-05
	LOSS [training: 0.25858640577172193 | validation: 0.2461375928253161]
	TIME [epoch: 6.31 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2643557292700557		[learning rate: 3.765e-05]
	Learning Rate: 3.76504e-05
	LOSS [training: 0.2643557292700557 | validation: 0.24491697451126349]
	TIME [epoch: 6.32 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2764896430053473		[learning rate: 3.7517e-05]
	Learning Rate: 3.75172e-05
	LOSS [training: 0.2764896430053473 | validation: 0.2535135993918157]
	TIME [epoch: 6.35 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2642555394329559		[learning rate: 3.7385e-05]
	Learning Rate: 3.73846e-05
	LOSS [training: 0.2642555394329559 | validation: 0.25888812766153313]
	TIME [epoch: 6.31 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27119608243019394		[learning rate: 3.7252e-05]
	Learning Rate: 3.72524e-05
	LOSS [training: 0.27119608243019394 | validation: 0.2560334749553392]
	TIME [epoch: 6.31 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26449197071746267		[learning rate: 3.7121e-05]
	Learning Rate: 3.71206e-05
	LOSS [training: 0.26449197071746267 | validation: 0.2501631385551798]
	TIME [epoch: 6.3 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26440382103544946		[learning rate: 3.6989e-05]
	Learning Rate: 3.69894e-05
	LOSS [training: 0.26440382103544946 | validation: 0.25930051019856504]
	TIME [epoch: 6.31 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2745418754809209		[learning rate: 3.6859e-05]
	Learning Rate: 3.68586e-05
	LOSS [training: 0.2745418754809209 | validation: 0.26276339627224493]
	TIME [epoch: 6.32 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2749572723541276		[learning rate: 3.6728e-05]
	Learning Rate: 3.67282e-05
	LOSS [training: 0.2749572723541276 | validation: 0.2609095898747871]
	TIME [epoch: 6.36 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2809760460911003		[learning rate: 3.6598e-05]
	Learning Rate: 3.65984e-05
	LOSS [training: 0.2809760460911003 | validation: 0.2582697077128362]
	TIME [epoch: 6.31 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2658112231933015		[learning rate: 3.6469e-05]
	Learning Rate: 3.64689e-05
	LOSS [training: 0.2658112231933015 | validation: 0.25573177408150455]
	TIME [epoch: 6.31 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2672920420008543		[learning rate: 3.634e-05]
	Learning Rate: 3.634e-05
	LOSS [training: 0.2672920420008543 | validation: 0.2751711536830296]
	TIME [epoch: 6.31 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26765761858881587		[learning rate: 3.6211e-05]
	Learning Rate: 3.62115e-05
	LOSS [training: 0.26765761858881587 | validation: 0.2613465523537266]
	TIME [epoch: 6.31 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26428614534173084		[learning rate: 3.6083e-05]
	Learning Rate: 3.60834e-05
	LOSS [training: 0.26428614534173084 | validation: 0.26876844629677343]
	TIME [epoch: 6.32 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2705518209049285		[learning rate: 3.5956e-05]
	Learning Rate: 3.59558e-05
	LOSS [training: 0.2705518209049285 | validation: 0.2536980343645199]
	TIME [epoch: 6.36 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2646675376004364		[learning rate: 3.5829e-05]
	Learning Rate: 3.58287e-05
	LOSS [training: 0.2646675376004364 | validation: 0.25414488526309365]
	TIME [epoch: 6.31 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2699123888477829		[learning rate: 3.5702e-05]
	Learning Rate: 3.5702e-05
	LOSS [training: 0.2699123888477829 | validation: 0.23883006075601537]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1641.pth
	Model improved!!!
EPOCH 1642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26635928822990884		[learning rate: 3.5576e-05]
	Learning Rate: 3.55757e-05
	LOSS [training: 0.26635928822990884 | validation: 0.23811691026755338]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1642.pth
	Model improved!!!
EPOCH 1643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26354309049653846		[learning rate: 3.545e-05]
	Learning Rate: 3.54499e-05
	LOSS [training: 0.26354309049653846 | validation: 0.24697127677906783]
	TIME [epoch: 6.32 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2633243550795942		[learning rate: 3.5325e-05]
	Learning Rate: 3.53246e-05
	LOSS [training: 0.2633243550795942 | validation: 0.24816904202300694]
	TIME [epoch: 6.32 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2629998370855004		[learning rate: 3.52e-05]
	Learning Rate: 3.51997e-05
	LOSS [training: 0.2629998370855004 | validation: 0.25638782012182726]
	TIME [epoch: 6.75 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2610836501014685		[learning rate: 3.5075e-05]
	Learning Rate: 3.50752e-05
	LOSS [training: 0.2610836501014685 | validation: 0.25139395389735725]
	TIME [epoch: 6.32 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26677261883670317		[learning rate: 3.4951e-05]
	Learning Rate: 3.49512e-05
	LOSS [training: 0.26677261883670317 | validation: 0.251065356904572]
	TIME [epoch: 6.31 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26464832971295954		[learning rate: 3.4828e-05]
	Learning Rate: 3.48276e-05
	LOSS [training: 0.26464832971295954 | validation: 0.26208582245604156]
	TIME [epoch: 6.32 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2619923209479734		[learning rate: 3.4704e-05]
	Learning Rate: 3.47044e-05
	LOSS [training: 0.2619923209479734 | validation: 0.25454015976556693]
	TIME [epoch: 6.31 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26266567050652734		[learning rate: 3.4582e-05]
	Learning Rate: 3.45817e-05
	LOSS [training: 0.26266567050652734 | validation: 0.25866150741998584]
	TIME [epoch: 6.33 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2612068572231934		[learning rate: 3.4459e-05]
	Learning Rate: 3.44594e-05
	LOSS [training: 0.2612068572231934 | validation: 0.25896236422719465]
	TIME [epoch: 6.35 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26409628387030437		[learning rate: 3.4338e-05]
	Learning Rate: 3.43375e-05
	LOSS [training: 0.26409628387030437 | validation: 0.2551906618912736]
	TIME [epoch: 6.31 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26166186786769796		[learning rate: 3.4216e-05]
	Learning Rate: 3.42161e-05
	LOSS [training: 0.26166186786769796 | validation: 0.25624525825426586]
	TIME [epoch: 6.31 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26310857213399275		[learning rate: 3.4095e-05]
	Learning Rate: 3.40951e-05
	LOSS [training: 0.26310857213399275 | validation: 0.23777932708850638]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1654.pth
	Model improved!!!
EPOCH 1655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2674666402377033		[learning rate: 3.3975e-05]
	Learning Rate: 3.39746e-05
	LOSS [training: 0.2674666402377033 | validation: 0.2530469225874345]
	TIME [epoch: 6.32 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26455220981131516		[learning rate: 3.3854e-05]
	Learning Rate: 3.38544e-05
	LOSS [training: 0.26455220981131516 | validation: 0.24085967235992045]
	TIME [epoch: 6.34 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26446404055680967		[learning rate: 3.3735e-05]
	Learning Rate: 3.37347e-05
	LOSS [training: 0.26446404055680967 | validation: 0.24193620772866065]
	TIME [epoch: 6.36 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2587359432023825		[learning rate: 3.3615e-05]
	Learning Rate: 3.36154e-05
	LOSS [training: 0.2587359432023825 | validation: 0.2513649983586062]
	TIME [epoch: 6.32 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2609274399888976		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 0.2609274399888976 | validation: 0.2578666198063591]
	TIME [epoch: 6.32 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26454726677544366		[learning rate: 3.3378e-05]
	Learning Rate: 3.33781e-05
	LOSS [training: 0.26454726677544366 | validation: 0.25088454181783]
	TIME [epoch: 6.32 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26090040556112404		[learning rate: 3.326e-05]
	Learning Rate: 3.32601e-05
	LOSS [training: 0.26090040556112404 | validation: 0.24874859925943932]
	TIME [epoch: 6.32 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26407482023991957		[learning rate: 3.3142e-05]
	Learning Rate: 3.31425e-05
	LOSS [training: 0.26407482023991957 | validation: 0.2568974163370444]
	TIME [epoch: 6.33 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26651463498631534		[learning rate: 3.3025e-05]
	Learning Rate: 3.30253e-05
	LOSS [training: 0.26651463498631534 | validation: 0.24885408827836158]
	TIME [epoch: 6.36 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2567983440296086		[learning rate: 3.2908e-05]
	Learning Rate: 3.29085e-05
	LOSS [training: 0.2567983440296086 | validation: 0.25476020599411675]
	TIME [epoch: 6.33 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2633951765477455		[learning rate: 3.2792e-05]
	Learning Rate: 3.27921e-05
	LOSS [training: 0.2633951765477455 | validation: 0.24927413497719692]
	TIME [epoch: 6.32 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26088072700424303		[learning rate: 3.2676e-05]
	Learning Rate: 3.26761e-05
	LOSS [training: 0.26088072700424303 | validation: 0.2555901088998295]
	TIME [epoch: 6.32 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2616800177216746		[learning rate: 3.2561e-05]
	Learning Rate: 3.25606e-05
	LOSS [training: 0.2616800177216746 | validation: 0.25847868702958426]
	TIME [epoch: 6.32 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26274790388967617		[learning rate: 3.2445e-05]
	Learning Rate: 3.24455e-05
	LOSS [training: 0.26274790388967617 | validation: 0.24144102883678192]
	TIME [epoch: 6.33 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26005221702489256		[learning rate: 3.2331e-05]
	Learning Rate: 3.23307e-05
	LOSS [training: 0.26005221702489256 | validation: 0.25343079468394797]
	TIME [epoch: 6.36 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26407185549279594		[learning rate: 3.2216e-05]
	Learning Rate: 3.22164e-05
	LOSS [training: 0.26407185549279594 | validation: 0.25017551414998995]
	TIME [epoch: 6.32 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2676325431228323		[learning rate: 3.2102e-05]
	Learning Rate: 3.21025e-05
	LOSS [training: 0.2676325431228323 | validation: 0.2514454991501182]
	TIME [epoch: 6.32 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2565922707598779		[learning rate: 3.1989e-05]
	Learning Rate: 3.1989e-05
	LOSS [training: 0.2565922707598779 | validation: 0.25503202536863284]
	TIME [epoch: 6.32 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2667399182018628		[learning rate: 3.1876e-05]
	Learning Rate: 3.18758e-05
	LOSS [training: 0.2667399182018628 | validation: 0.25107416069222793]
	TIME [epoch: 6.32 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26397601983618374		[learning rate: 3.1763e-05]
	Learning Rate: 3.17631e-05
	LOSS [training: 0.26397601983618374 | validation: 0.25221987599303053]
	TIME [epoch: 6.33 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2675790742969013		[learning rate: 3.1651e-05]
	Learning Rate: 3.16508e-05
	LOSS [training: 0.2675790742969013 | validation: 0.2561633164014533]
	TIME [epoch: 6.36 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2669132004574634		[learning rate: 3.1539e-05]
	Learning Rate: 3.15389e-05
	LOSS [training: 0.2669132004574634 | validation: 0.24633143482257558]
	TIME [epoch: 6.32 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27039025630404206		[learning rate: 3.1427e-05]
	Learning Rate: 3.14274e-05
	LOSS [training: 0.27039025630404206 | validation: 0.24199730763490307]
	TIME [epoch: 6.33 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26263070818914974		[learning rate: 3.1316e-05]
	Learning Rate: 3.13162e-05
	LOSS [training: 0.26263070818914974 | validation: 0.2541287252979936]
	TIME [epoch: 6.33 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.265877868730313		[learning rate: 3.1205e-05]
	Learning Rate: 3.12055e-05
	LOSS [training: 0.265877868730313 | validation: 0.24578245452866584]
	TIME [epoch: 6.33 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27831607229396105		[learning rate: 3.1095e-05]
	Learning Rate: 3.10951e-05
	LOSS [training: 0.27831607229396105 | validation: 0.24341365488972663]
	TIME [epoch: 6.33 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2712904092861406		[learning rate: 3.0985e-05]
	Learning Rate: 3.09852e-05
	LOSS [training: 0.2712904092861406 | validation: 0.23544269113693506]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1681.pth
	Model improved!!!
EPOCH 1682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27197458537764685		[learning rate: 3.0876e-05]
	Learning Rate: 3.08756e-05
	LOSS [training: 0.27197458537764685 | validation: 0.2524728032654502]
	TIME [epoch: 6.32 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2794487979301943		[learning rate: 3.0766e-05]
	Learning Rate: 3.07664e-05
	LOSS [training: 0.2794487979301943 | validation: 0.24816064433839824]
	TIME [epoch: 6.32 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2789997033487139		[learning rate: 3.0658e-05]
	Learning Rate: 3.06576e-05
	LOSS [training: 0.2789997033487139 | validation: 0.2557507106475444]
	TIME [epoch: 6.32 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.281183098795821		[learning rate: 3.0549e-05]
	Learning Rate: 3.05492e-05
	LOSS [training: 0.281183098795821 | validation: 0.23525839930104506]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1685.pth
	Model improved!!!
EPOCH 1686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28048983369807734		[learning rate: 3.0441e-05]
	Learning Rate: 3.04412e-05
	LOSS [training: 0.28048983369807734 | validation: 0.24361050138956808]
	TIME [epoch: 6.32 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27203346617487634		[learning rate: 3.0334e-05]
	Learning Rate: 3.03336e-05
	LOSS [training: 0.27203346617487634 | validation: 0.2547484166624472]
	TIME [epoch: 6.37 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2757922696636318		[learning rate: 3.0226e-05]
	Learning Rate: 3.02263e-05
	LOSS [training: 0.2757922696636318 | validation: 0.2483538524646997]
	TIME [epoch: 6.32 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2652181096438672		[learning rate: 3.0119e-05]
	Learning Rate: 3.01194e-05
	LOSS [training: 0.2652181096438672 | validation: 0.24034823114868548]
	TIME [epoch: 6.32 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2696783550032337		[learning rate: 3.0013e-05]
	Learning Rate: 3.00129e-05
	LOSS [training: 0.2696783550032337 | validation: 0.24156780505795175]
	TIME [epoch: 6.32 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2692725390108263		[learning rate: 2.9907e-05]
	Learning Rate: 2.99068e-05
	LOSS [training: 0.2692725390108263 | validation: 0.2573881710148907]
	TIME [epoch: 6.32 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2693695780887447		[learning rate: 2.9801e-05]
	Learning Rate: 2.9801e-05
	LOSS [training: 0.2693695780887447 | validation: 0.24330964229577298]
	TIME [epoch: 6.32 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2762051096643044		[learning rate: 2.9696e-05]
	Learning Rate: 2.96956e-05
	LOSS [training: 0.2762051096643044 | validation: 0.2467465121216326]
	TIME [epoch: 6.36 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27391318341623383		[learning rate: 2.9591e-05]
	Learning Rate: 2.95906e-05
	LOSS [training: 0.27391318341623383 | validation: 0.24293969316285047]
	TIME [epoch: 6.32 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26557039954523853		[learning rate: 2.9486e-05]
	Learning Rate: 2.9486e-05
	LOSS [training: 0.26557039954523853 | validation: 0.2475403552079814]
	TIME [epoch: 6.31 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2620274255564796		[learning rate: 2.9382e-05]
	Learning Rate: 2.93817e-05
	LOSS [training: 0.2620274255564796 | validation: 0.24528679123421354]
	TIME [epoch: 6.31 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2619830173094888		[learning rate: 2.9278e-05]
	Learning Rate: 2.92778e-05
	LOSS [training: 0.2619830173094888 | validation: 0.2575462622743263]
	TIME [epoch: 6.31 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2658586663773811		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 0.2658586663773811 | validation: 0.2525139371001518]
	TIME [epoch: 6.32 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2595147719087617		[learning rate: 2.9071e-05]
	Learning Rate: 2.90711e-05
	LOSS [training: 0.2595147719087617 | validation: 0.24532134454640103]
	TIME [epoch: 6.36 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2635136848086332		[learning rate: 2.8968e-05]
	Learning Rate: 2.89683e-05
	LOSS [training: 0.2635136848086332 | validation: 0.24486775066661814]
	TIME [epoch: 6.32 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26289634996550754		[learning rate: 2.8866e-05]
	Learning Rate: 2.88659e-05
	LOSS [training: 0.26289634996550754 | validation: 0.2537856803130525]
	TIME [epoch: 6.31 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2676413597317411		[learning rate: 2.8764e-05]
	Learning Rate: 2.87638e-05
	LOSS [training: 0.2676413597317411 | validation: 0.2617353075641856]
	TIME [epoch: 6.32 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27086235125308766		[learning rate: 2.8662e-05]
	Learning Rate: 2.86621e-05
	LOSS [training: 0.27086235125308766 | validation: 0.2526816288233292]
	TIME [epoch: 6.31 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2686593870138384		[learning rate: 2.8561e-05]
	Learning Rate: 2.85607e-05
	LOSS [training: 0.2686593870138384 | validation: 0.23933134772370412]
	TIME [epoch: 6.32 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2638877263167119		[learning rate: 2.846e-05]
	Learning Rate: 2.84597e-05
	LOSS [training: 0.2638877263167119 | validation: 0.2538524657330637]
	TIME [epoch: 6.36 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26779393863521084		[learning rate: 2.8359e-05]
	Learning Rate: 2.83591e-05
	LOSS [training: 0.26779393863521084 | validation: 0.24941916196299127]
	TIME [epoch: 6.32 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26842934422748144		[learning rate: 2.8259e-05]
	Learning Rate: 2.82588e-05
	LOSS [training: 0.26842934422748144 | validation: 0.24551028060474167]
	TIME [epoch: 6.31 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2649680585135777		[learning rate: 2.8159e-05]
	Learning Rate: 2.81589e-05
	LOSS [training: 0.2649680585135777 | validation: 0.25011413999682963]
	TIME [epoch: 6.31 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25769309084648007		[learning rate: 2.8059e-05]
	Learning Rate: 2.80593e-05
	LOSS [training: 0.25769309084648007 | validation: 0.24729934589886543]
	TIME [epoch: 6.32 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2640706629696325		[learning rate: 2.796e-05]
	Learning Rate: 2.79601e-05
	LOSS [training: 0.2640706629696325 | validation: 0.2579539982573896]
	TIME [epoch: 6.32 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25940715895317074		[learning rate: 2.7861e-05]
	Learning Rate: 2.78612e-05
	LOSS [training: 0.25940715895317074 | validation: 0.24266334774654502]
	TIME [epoch: 6.36 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2623056581440988		[learning rate: 2.7763e-05]
	Learning Rate: 2.77627e-05
	LOSS [training: 0.2623056581440988 | validation: 0.2524687862271351]
	TIME [epoch: 6.32 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2624762589083015		[learning rate: 2.7665e-05]
	Learning Rate: 2.76645e-05
	LOSS [training: 0.2624762589083015 | validation: 0.2464254432430892]
	TIME [epoch: 6.31 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2600187243800319		[learning rate: 2.7567e-05]
	Learning Rate: 2.75667e-05
	LOSS [training: 0.2600187243800319 | validation: 0.263631433335368]
	TIME [epoch: 6.31 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26369240676437483		[learning rate: 2.7469e-05]
	Learning Rate: 2.74692e-05
	LOSS [training: 0.26369240676437483 | validation: 0.253539822662897]
	TIME [epoch: 6.31 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2607199125526631		[learning rate: 2.7372e-05]
	Learning Rate: 2.73721e-05
	LOSS [training: 0.2607199125526631 | validation: 0.24474640214150195]
	TIME [epoch: 6.31 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2638419929396487		[learning rate: 2.7275e-05]
	Learning Rate: 2.72753e-05
	LOSS [training: 0.2638419929396487 | validation: 0.24799034600908398]
	TIME [epoch: 6.35 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26588596528543884		[learning rate: 2.7179e-05]
	Learning Rate: 2.71788e-05
	LOSS [training: 0.26588596528543884 | validation: 0.24678778887253353]
	TIME [epoch: 6.31 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26701601276530484		[learning rate: 2.7083e-05]
	Learning Rate: 2.70827e-05
	LOSS [training: 0.26701601276530484 | validation: 0.2508672463597214]
	TIME [epoch: 6.31 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26031504058178134		[learning rate: 2.6987e-05]
	Learning Rate: 2.6987e-05
	LOSS [training: 0.26031504058178134 | validation: 0.2512014149487526]
	TIME [epoch: 6.31 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2589820441052491		[learning rate: 2.6892e-05]
	Learning Rate: 2.68915e-05
	LOSS [training: 0.2589820441052491 | validation: 0.25600160925324206]
	TIME [epoch: 6.31 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2622736959462142		[learning rate: 2.6796e-05]
	Learning Rate: 2.67964e-05
	LOSS [training: 0.2622736959462142 | validation: 0.24391592741995766]
	TIME [epoch: 6.31 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26100958851676614		[learning rate: 2.6702e-05]
	Learning Rate: 2.67017e-05
	LOSS [training: 0.26100958851676614 | validation: 0.2499142966711705]
	TIME [epoch: 6.36 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.259847419746917		[learning rate: 2.6607e-05]
	Learning Rate: 2.66073e-05
	LOSS [training: 0.259847419746917 | validation: 0.2462748119031648]
	TIME [epoch: 6.31 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25780389582882396		[learning rate: 2.6513e-05]
	Learning Rate: 2.65132e-05
	LOSS [training: 0.25780389582882396 | validation: 0.2516852446045903]
	TIME [epoch: 6.31 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2690146956542379		[learning rate: 2.6419e-05]
	Learning Rate: 2.64194e-05
	LOSS [training: 0.2690146956542379 | validation: 0.24250621265249547]
	TIME [epoch: 6.31 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2564151186113261		[learning rate: 2.6326e-05]
	Learning Rate: 2.6326e-05
	LOSS [training: 0.2564151186113261 | validation: 0.2456865215673974]
	TIME [epoch: 6.31 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25699162962310035		[learning rate: 2.6233e-05]
	Learning Rate: 2.62329e-05
	LOSS [training: 0.25699162962310035 | validation: 0.24969899539309556]
	TIME [epoch: 6.31 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26759010070870465		[learning rate: 2.614e-05]
	Learning Rate: 2.61401e-05
	LOSS [training: 0.26759010070870465 | validation: 0.24443308366455846]
	TIME [epoch: 6.36 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2636990478373919		[learning rate: 2.6048e-05]
	Learning Rate: 2.60477e-05
	LOSS [training: 0.2636990478373919 | validation: 0.24797566551152772]
	TIME [epoch: 6.33 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2608770478680435		[learning rate: 2.5956e-05]
	Learning Rate: 2.59556e-05
	LOSS [training: 0.2608770478680435 | validation: 0.24255337230669016]
	TIME [epoch: 6.31 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26723671537886995		[learning rate: 2.5864e-05]
	Learning Rate: 2.58638e-05
	LOSS [training: 0.26723671537886995 | validation: 0.25297017623033613]
	TIME [epoch: 6.32 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26115600711114956		[learning rate: 2.5772e-05]
	Learning Rate: 2.57723e-05
	LOSS [training: 0.26115600711114956 | validation: 0.24422195726205126]
	TIME [epoch: 6.31 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26378972184697747		[learning rate: 2.5681e-05]
	Learning Rate: 2.56812e-05
	LOSS [training: 0.26378972184697747 | validation: 0.24280354262741488]
	TIME [epoch: 6.32 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26584142238900615		[learning rate: 2.559e-05]
	Learning Rate: 2.55904e-05
	LOSS [training: 0.26584142238900615 | validation: 0.2513337073511111]
	TIME [epoch: 6.35 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26491362395766405		[learning rate: 2.55e-05]
	Learning Rate: 2.54999e-05
	LOSS [training: 0.26491362395766405 | validation: 0.2568402384330233]
	TIME [epoch: 6.33 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2637876746027788		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 0.2637876746027788 | validation: 0.25107580139189567]
	TIME [epoch: 6.31 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2656120792815626		[learning rate: 2.532e-05]
	Learning Rate: 2.53199e-05
	LOSS [training: 0.2656120792815626 | validation: 0.26120573343164677]
	TIME [epoch: 6.32 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2628257269009574		[learning rate: 2.523e-05]
	Learning Rate: 2.52303e-05
	LOSS [training: 0.2628257269009574 | validation: 0.2511488874656137]
	TIME [epoch: 6.32 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2628360625053348		[learning rate: 2.5141e-05]
	Learning Rate: 2.51411e-05
	LOSS [training: 0.2628360625053348 | validation: 0.25908864164475787]
	TIME [epoch: 6.31 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26305722935992093		[learning rate: 2.5052e-05]
	Learning Rate: 2.50522e-05
	LOSS [training: 0.26305722935992093 | validation: 0.25220498707991285]
	TIME [epoch: 6.35 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.256874964830716		[learning rate: 2.4964e-05]
	Learning Rate: 2.49636e-05
	LOSS [training: 0.256874964830716 | validation: 0.25409368584859204]
	TIME [epoch: 6.33 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2604760768445268		[learning rate: 2.4875e-05]
	Learning Rate: 2.48754e-05
	LOSS [training: 0.2604760768445268 | validation: 0.2571116870983623]
	TIME [epoch: 6.31 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2675565901605074		[learning rate: 2.4787e-05]
	Learning Rate: 2.47874e-05
	LOSS [training: 0.2675565901605074 | validation: 0.26579327234011296]
	TIME [epoch: 6.32 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2653204377614891		[learning rate: 2.47e-05]
	Learning Rate: 2.46997e-05
	LOSS [training: 0.2653204377614891 | validation: 0.25115528005199905]
	TIME [epoch: 6.31 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25908350400788205		[learning rate: 2.4612e-05]
	Learning Rate: 2.46124e-05
	LOSS [training: 0.25908350400788205 | validation: 0.24068287478472042]
	TIME [epoch: 6.31 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26521045317406955		[learning rate: 2.4525e-05]
	Learning Rate: 2.45254e-05
	LOSS [training: 0.26521045317406955 | validation: 0.2583413807597573]
	TIME [epoch: 6.34 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26588111347873367		[learning rate: 2.4439e-05]
	Learning Rate: 2.44386e-05
	LOSS [training: 0.26588111347873367 | validation: 0.2587748427007088]
	TIME [epoch: 6.34 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2714838867382227		[learning rate: 2.4352e-05]
	Learning Rate: 2.43522e-05
	LOSS [training: 0.2714838867382227 | validation: 0.2533432097832658]
	TIME [epoch: 6.32 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26360500907043116		[learning rate: 2.4266e-05]
	Learning Rate: 2.42661e-05
	LOSS [training: 0.26360500907043116 | validation: 0.25119208287737327]
	TIME [epoch: 6.31 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26316165608923814		[learning rate: 2.418e-05]
	Learning Rate: 2.41803e-05
	LOSS [training: 0.26316165608923814 | validation: 0.24917014023419579]
	TIME [epoch: 6.31 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2595585409398544		[learning rate: 2.4095e-05]
	Learning Rate: 2.40948e-05
	LOSS [training: 0.2595585409398544 | validation: 0.25364046654166916]
	TIME [epoch: 6.31 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2653494783935824		[learning rate: 2.401e-05]
	Learning Rate: 2.40096e-05
	LOSS [training: 0.2653494783935824 | validation: 0.25034389420729]
	TIME [epoch: 6.34 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26283055741781025		[learning rate: 2.3925e-05]
	Learning Rate: 2.39247e-05
	LOSS [training: 0.26283055741781025 | validation: 0.25155581023167606]
	TIME [epoch: 6.34 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2593987616607505		[learning rate: 2.384e-05]
	Learning Rate: 2.38401e-05
	LOSS [training: 0.2593987616607505 | validation: 0.24370985693753316]
	TIME [epoch: 6.31 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26740511678660117		[learning rate: 2.3756e-05]
	Learning Rate: 2.37558e-05
	LOSS [training: 0.26740511678660117 | validation: 0.24097713231390075]
	TIME [epoch: 6.32 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2644098234400683		[learning rate: 2.3672e-05]
	Learning Rate: 2.36718e-05
	LOSS [training: 0.2644098234400683 | validation: 0.24278459506530878]
	TIME [epoch: 6.32 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2650152569689325		[learning rate: 2.3588e-05]
	Learning Rate: 2.35881e-05
	LOSS [training: 0.2650152569689325 | validation: 0.24949361718896845]
	TIME [epoch: 6.32 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26387841870389284		[learning rate: 2.3505e-05]
	Learning Rate: 2.35047e-05
	LOSS [training: 0.26387841870389284 | validation: 0.24508053065901841]
	TIME [epoch: 6.33 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27161289878508504		[learning rate: 2.3422e-05]
	Learning Rate: 2.34215e-05
	LOSS [training: 0.27161289878508504 | validation: 0.25414023048595236]
	TIME [epoch: 6.35 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2583135638691532		[learning rate: 2.3339e-05]
	Learning Rate: 2.33387e-05
	LOSS [training: 0.2583135638691532 | validation: 0.2458052051234716]
	TIME [epoch: 6.32 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2595934744244967		[learning rate: 2.3256e-05]
	Learning Rate: 2.32562e-05
	LOSS [training: 0.2595934744244967 | validation: 0.23516206965653166]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1762.pth
	Model improved!!!
EPOCH 1763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26261250843300465		[learning rate: 2.3174e-05]
	Learning Rate: 2.31739e-05
	LOSS [training: 0.26261250843300465 | validation: 0.2509377809840867]
	TIME [epoch: 6.32 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2650779966571086		[learning rate: 2.3092e-05]
	Learning Rate: 2.3092e-05
	LOSS [training: 0.2650779966571086 | validation: 0.24278690298098105]
	TIME [epoch: 6.31 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2619455485383898		[learning rate: 2.301e-05]
	Learning Rate: 2.30103e-05
	LOSS [training: 0.2619455485383898 | validation: 0.2475921391243931]
	TIME [epoch: 6.33 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2601613186907026		[learning rate: 2.2929e-05]
	Learning Rate: 2.2929e-05
	LOSS [training: 0.2601613186907026 | validation: 0.2574741440971084]
	TIME [epoch: 6.35 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2552519328877868		[learning rate: 2.2848e-05]
	Learning Rate: 2.28479e-05
	LOSS [training: 0.2552519328877868 | validation: 0.24734669100963846]
	TIME [epoch: 6.32 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26714905490384044		[learning rate: 2.2767e-05]
	Learning Rate: 2.27671e-05
	LOSS [training: 0.26714905490384044 | validation: 0.2536364274962211]
	TIME [epoch: 6.31 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26798411995583193		[learning rate: 2.2687e-05]
	Learning Rate: 2.26866e-05
	LOSS [training: 0.26798411995583193 | validation: 0.25291506954213555]
	TIME [epoch: 6.31 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25583663027485537		[learning rate: 2.2606e-05]
	Learning Rate: 2.26064e-05
	LOSS [training: 0.25583663027485537 | validation: 0.24999476892052472]
	TIME [epoch: 6.31 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26065671515419364		[learning rate: 2.2526e-05]
	Learning Rate: 2.25264e-05
	LOSS [training: 0.26065671515419364 | validation: 0.25609984757734805]
	TIME [epoch: 6.33 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26078400343171904		[learning rate: 2.2447e-05]
	Learning Rate: 2.24468e-05
	LOSS [training: 0.26078400343171904 | validation: 0.2574901751016095]
	TIME [epoch: 6.34 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2598535271151077		[learning rate: 2.2367e-05]
	Learning Rate: 2.23674e-05
	LOSS [training: 0.2598535271151077 | validation: 0.2436014599143489]
	TIME [epoch: 6.31 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2641225890443479		[learning rate: 2.2288e-05]
	Learning Rate: 2.22883e-05
	LOSS [training: 0.2641225890443479 | validation: 0.25017118765017565]
	TIME [epoch: 6.31 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.269175482946865		[learning rate: 2.2209e-05]
	Learning Rate: 2.22095e-05
	LOSS [training: 0.269175482946865 | validation: 0.2576479697564465]
	TIME [epoch: 6.32 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2647504478316811		[learning rate: 2.2131e-05]
	Learning Rate: 2.21309e-05
	LOSS [training: 0.2647504478316811 | validation: 0.24928380422063734]
	TIME [epoch: 6.31 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2598360962061352		[learning rate: 2.2053e-05]
	Learning Rate: 2.20527e-05
	LOSS [training: 0.2598360962061352 | validation: 0.24032439869010916]
	TIME [epoch: 6.33 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26234258290761514		[learning rate: 2.1975e-05]
	Learning Rate: 2.19747e-05
	LOSS [training: 0.26234258290761514 | validation: 0.2508061476066936]
	TIME [epoch: 6.35 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26390637501714603		[learning rate: 2.1897e-05]
	Learning Rate: 2.1897e-05
	LOSS [training: 0.26390637501714603 | validation: 0.24007862917599862]
	TIME [epoch: 6.32 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26226957589413624		[learning rate: 2.182e-05]
	Learning Rate: 2.18196e-05
	LOSS [training: 0.26226957589413624 | validation: 0.24318151105283953]
	TIME [epoch: 6.31 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26165114741322526		[learning rate: 2.1742e-05]
	Learning Rate: 2.17424e-05
	LOSS [training: 0.26165114741322526 | validation: 0.2443899334445414]
	TIME [epoch: 6.32 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2641870280230996		[learning rate: 2.1666e-05]
	Learning Rate: 2.16655e-05
	LOSS [training: 0.2641870280230996 | validation: 0.2416283787845362]
	TIME [epoch: 6.32 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2627649322267259		[learning rate: 2.1589e-05]
	Learning Rate: 2.15889e-05
	LOSS [training: 0.2627649322267259 | validation: 0.2528076931301446]
	TIME [epoch: 6.32 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25700823316820975		[learning rate: 2.1513e-05]
	Learning Rate: 2.15126e-05
	LOSS [training: 0.25700823316820975 | validation: 0.2555438267016424]
	TIME [epoch: 6.35 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2628297046069797		[learning rate: 2.1437e-05]
	Learning Rate: 2.14365e-05
	LOSS [training: 0.2628297046069797 | validation: 0.24744491891760034]
	TIME [epoch: 6.31 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2634251599330882		[learning rate: 2.1361e-05]
	Learning Rate: 2.13607e-05
	LOSS [training: 0.2634251599330882 | validation: 0.24909390277326068]
	TIME [epoch: 6.31 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2576596948914972		[learning rate: 2.1285e-05]
	Learning Rate: 2.12852e-05
	LOSS [training: 0.2576596948914972 | validation: 0.2567084451128313]
	TIME [epoch: 6.31 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26065305864757565		[learning rate: 2.121e-05]
	Learning Rate: 2.12099e-05
	LOSS [training: 0.26065305864757565 | validation: 0.24029554308189954]
	TIME [epoch: 6.31 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26156172502747255		[learning rate: 2.1135e-05]
	Learning Rate: 2.11349e-05
	LOSS [training: 0.26156172502747255 | validation: 0.2583555495475329]
	TIME [epoch: 6.32 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2616814378415635		[learning rate: 2.106e-05]
	Learning Rate: 2.10602e-05
	LOSS [training: 0.2616814378415635 | validation: 0.24152691169981044]
	TIME [epoch: 6.36 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25806349803129003		[learning rate: 2.0986e-05]
	Learning Rate: 2.09857e-05
	LOSS [training: 0.25806349803129003 | validation: 0.24437494322990885]
	TIME [epoch: 6.31 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2629259704788986		[learning rate: 2.0911e-05]
	Learning Rate: 2.09115e-05
	LOSS [training: 0.2629259704788986 | validation: 0.23880516193897983]
	TIME [epoch: 6.31 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25808062057740255		[learning rate: 2.0838e-05]
	Learning Rate: 2.08375e-05
	LOSS [training: 0.25808062057740255 | validation: 0.24878477741499824]
	TIME [epoch: 6.31 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25723344472949483		[learning rate: 2.0764e-05]
	Learning Rate: 2.07638e-05
	LOSS [training: 0.25723344472949483 | validation: 0.2525398326871821]
	TIME [epoch: 6.31 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26672109662546484		[learning rate: 2.069e-05]
	Learning Rate: 2.06904e-05
	LOSS [training: 0.26672109662546484 | validation: 0.2535530736079818]
	TIME [epoch: 6.32 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25958858976442156		[learning rate: 2.0617e-05]
	Learning Rate: 2.06173e-05
	LOSS [training: 0.25958858976442156 | validation: 0.25262282220626536]
	TIME [epoch: 6.36 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2618113084770816		[learning rate: 2.0544e-05]
	Learning Rate: 2.05444e-05
	LOSS [training: 0.2618113084770816 | validation: 0.253062303154934]
	TIME [epoch: 6.31 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2595047120746036		[learning rate: 2.0472e-05]
	Learning Rate: 2.04717e-05
	LOSS [training: 0.2595047120746036 | validation: 0.2537376099772653]
	TIME [epoch: 6.31 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2651516245140627		[learning rate: 2.0399e-05]
	Learning Rate: 2.03993e-05
	LOSS [training: 0.2651516245140627 | validation: 0.2656661470841856]
	TIME [epoch: 6.31 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2595595701986881		[learning rate: 2.0327e-05]
	Learning Rate: 2.03272e-05
	LOSS [training: 0.2595595701986881 | validation: 0.25474659053419796]
	TIME [epoch: 6.31 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.258837085545618		[learning rate: 2.0255e-05]
	Learning Rate: 2.02553e-05
	LOSS [training: 0.258837085545618 | validation: 0.2405122336781586]
	TIME [epoch: 6.32 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2639977678621005		[learning rate: 2.0184e-05]
	Learning Rate: 2.01837e-05
	LOSS [training: 0.2639977678621005 | validation: 0.25767966718189306]
	TIME [epoch: 6.36 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2647205543151049		[learning rate: 2.0112e-05]
	Learning Rate: 2.01123e-05
	LOSS [training: 0.2647205543151049 | validation: 0.2537474072362901]
	TIME [epoch: 6.31 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2636249453770194		[learning rate: 2.0041e-05]
	Learning Rate: 2.00412e-05
	LOSS [training: 0.2636249453770194 | validation: 0.2688954060393849]
	TIME [epoch: 6.32 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26278175293117445		[learning rate: 1.997e-05]
	Learning Rate: 1.99703e-05
	LOSS [training: 0.26278175293117445 | validation: 0.2660386147748802]
	TIME [epoch: 6.31 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2695471474349413		[learning rate: 1.99e-05]
	Learning Rate: 1.98997e-05
	LOSS [training: 0.2695471474349413 | validation: 0.26863856251581564]
	TIME [epoch: 6.32 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2608021451625837		[learning rate: 1.9829e-05]
	Learning Rate: 1.98293e-05
	LOSS [training: 0.2608021451625837 | validation: 0.25478770477367363]
	TIME [epoch: 6.32 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2699881681465214		[learning rate: 1.9759e-05]
	Learning Rate: 1.97592e-05
	LOSS [training: 0.2699881681465214 | validation: 0.26247395714833244]
	TIME [epoch: 6.36 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2649354590592966		[learning rate: 1.9689e-05]
	Learning Rate: 1.96893e-05
	LOSS [training: 0.2649354590592966 | validation: 0.26511812914829946]
	TIME [epoch: 6.31 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2601646386878202		[learning rate: 1.962e-05]
	Learning Rate: 1.96197e-05
	LOSS [training: 0.2601646386878202 | validation: 0.24606655915798759]
	TIME [epoch: 6.31 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2637337109430809		[learning rate: 1.955e-05]
	Learning Rate: 1.95503e-05
	LOSS [training: 0.2637337109430809 | validation: 0.2503955526481111]
	TIME [epoch: 6.31 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2575068128792158		[learning rate: 1.9481e-05]
	Learning Rate: 1.94812e-05
	LOSS [training: 0.2575068128792158 | validation: 0.24525889702750703]
	TIME [epoch: 6.32 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2632973684194524		[learning rate: 1.9412e-05]
	Learning Rate: 1.94123e-05
	LOSS [training: 0.2632973684194524 | validation: 0.24808505658162378]
	TIME [epoch: 6.32 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2615142768519057		[learning rate: 1.9344e-05]
	Learning Rate: 1.93437e-05
	LOSS [training: 0.2615142768519057 | validation: 0.24733941893496914]
	TIME [epoch: 6.36 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25669695250200014		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 0.25669695250200014 | validation: 0.2584237007085505]
	TIME [epoch: 6.31 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2644304021528334		[learning rate: 1.9207e-05]
	Learning Rate: 1.92071e-05
	LOSS [training: 0.2644304021528334 | validation: 0.25569826445545374]
	TIME [epoch: 6.31 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2627741554796746		[learning rate: 1.9139e-05]
	Learning Rate: 1.91392e-05
	LOSS [training: 0.2627741554796746 | validation: 0.2631691567038307]
	TIME [epoch: 6.31 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2574235343026986		[learning rate: 1.9071e-05]
	Learning Rate: 1.90715e-05
	LOSS [training: 0.2574235343026986 | validation: 0.24100911923236917]
	TIME [epoch: 6.31 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25934924934962644		[learning rate: 1.9004e-05]
	Learning Rate: 1.90041e-05
	LOSS [training: 0.25934924934962644 | validation: 0.2479310997444249]
	TIME [epoch: 6.33 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26078822311883726		[learning rate: 1.8937e-05]
	Learning Rate: 1.89369e-05
	LOSS [training: 0.26078822311883726 | validation: 0.2557851356050224]
	TIME [epoch: 6.36 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25605842029764064		[learning rate: 1.887e-05]
	Learning Rate: 1.88699e-05
	LOSS [training: 0.25605842029764064 | validation: 0.23645436883115473]
	TIME [epoch: 6.31 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26349226505882495		[learning rate: 1.8803e-05]
	Learning Rate: 1.88032e-05
	LOSS [training: 0.26349226505882495 | validation: 0.24334617101445855]
	TIME [epoch: 6.32 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2600135757676762		[learning rate: 1.8737e-05]
	Learning Rate: 1.87367e-05
	LOSS [training: 0.2600135757676762 | validation: 0.24579068246239727]
	TIME [epoch: 6.32 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2647287046338438		[learning rate: 1.867e-05]
	Learning Rate: 1.86704e-05
	LOSS [training: 0.2647287046338438 | validation: 0.23931991582009682]
	TIME [epoch: 6.31 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2556225910858738		[learning rate: 1.8604e-05]
	Learning Rate: 1.86044e-05
	LOSS [training: 0.2556225910858738 | validation: 0.24441863457704704]
	TIME [epoch: 6.32 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2614216415469661		[learning rate: 1.8539e-05]
	Learning Rate: 1.85386e-05
	LOSS [training: 0.2614216415469661 | validation: 0.2441259829165902]
	TIME [epoch: 6.36 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2637868286185865		[learning rate: 1.8473e-05]
	Learning Rate: 1.84731e-05
	LOSS [training: 0.2637868286185865 | validation: 0.23332019378261576]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1827.pth
	Model improved!!!
EPOCH 1828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2625263436673993		[learning rate: 1.8408e-05]
	Learning Rate: 1.84077e-05
	LOSS [training: 0.2625263436673993 | validation: 0.26083872664640506]
	TIME [epoch: 6.31 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2641681538572416		[learning rate: 1.8343e-05]
	Learning Rate: 1.83426e-05
	LOSS [training: 0.2641681538572416 | validation: 0.24338123527407923]
	TIME [epoch: 6.31 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2679448297100091		[learning rate: 1.8278e-05]
	Learning Rate: 1.82778e-05
	LOSS [training: 0.2679448297100091 | validation: 0.2588494501157408]
	TIME [epoch: 6.32 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2682024479594285		[learning rate: 1.8213e-05]
	Learning Rate: 1.82131e-05
	LOSS [training: 0.2682024479594285 | validation: 0.25384883968368566]
	TIME [epoch: 6.31 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2605681564889987		[learning rate: 1.8149e-05]
	Learning Rate: 1.81487e-05
	LOSS [training: 0.2605681564889987 | validation: 0.2547357801721894]
	TIME [epoch: 6.36 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25905693562433796		[learning rate: 1.8085e-05]
	Learning Rate: 1.80846e-05
	LOSS [training: 0.25905693562433796 | validation: 0.2472289998577452]
	TIME [epoch: 6.32 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25968561931152645		[learning rate: 1.8021e-05]
	Learning Rate: 1.80206e-05
	LOSS [training: 0.25968561931152645 | validation: 0.24113089092769613]
	TIME [epoch: 6.31 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25426687902628586		[learning rate: 1.7957e-05]
	Learning Rate: 1.79569e-05
	LOSS [training: 0.25426687902628586 | validation: 0.2465319580297473]
	TIME [epoch: 6.31 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2585058608031843		[learning rate: 1.7893e-05]
	Learning Rate: 1.78934e-05
	LOSS [training: 0.2585058608031843 | validation: 0.24967747364415643]
	TIME [epoch: 6.31 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26367420466565195		[learning rate: 1.783e-05]
	Learning Rate: 1.78301e-05
	LOSS [training: 0.26367420466565195 | validation: 0.24071406101235898]
	TIME [epoch: 6.31 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2623354424983352		[learning rate: 1.7767e-05]
	Learning Rate: 1.77671e-05
	LOSS [training: 0.2623354424983352 | validation: 0.25080819220284994]
	TIME [epoch: 6.36 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2628355673944538		[learning rate: 1.7704e-05]
	Learning Rate: 1.77042e-05
	LOSS [training: 0.2628355673944538 | validation: 0.24078267781514623]
	TIME [epoch: 6.32 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2630015146016399		[learning rate: 1.7642e-05]
	Learning Rate: 1.76416e-05
	LOSS [training: 0.2630015146016399 | validation: 0.24245357273038948]
	TIME [epoch: 6.31 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26582250148534603		[learning rate: 1.7579e-05]
	Learning Rate: 1.75792e-05
	LOSS [training: 0.26582250148534603 | validation: 0.2522615809701657]
	TIME [epoch: 6.32 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2571318899983109		[learning rate: 1.7517e-05]
	Learning Rate: 1.75171e-05
	LOSS [training: 0.2571318899983109 | validation: 0.24605984648763068]
	TIME [epoch: 6.31 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2605043856304135		[learning rate: 1.7455e-05]
	Learning Rate: 1.74551e-05
	LOSS [training: 0.2605043856304135 | validation: 0.24794850980601085]
	TIME [epoch: 6.31 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2601634726842071		[learning rate: 1.7393e-05]
	Learning Rate: 1.73934e-05
	LOSS [training: 0.2601634726842071 | validation: 0.23415800272953602]
	TIME [epoch: 6.35 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25953366020120444		[learning rate: 1.7332e-05]
	Learning Rate: 1.73319e-05
	LOSS [training: 0.25953366020120444 | validation: 0.24973856135213962]
	TIME [epoch: 6.33 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25536117869748287		[learning rate: 1.7271e-05]
	Learning Rate: 1.72706e-05
	LOSS [training: 0.25536117869748287 | validation: 0.23329533298434485]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1846.pth
	Model improved!!!
EPOCH 1847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26161402483887275		[learning rate: 1.721e-05]
	Learning Rate: 1.72095e-05
	LOSS [training: 0.26161402483887275 | validation: 0.23433872342991066]
	TIME [epoch: 6.32 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2656728601840184		[learning rate: 1.7149e-05]
	Learning Rate: 1.71487e-05
	LOSS [training: 0.2656728601840184 | validation: 0.24157746159170784]
	TIME [epoch: 6.32 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25822188988531386		[learning rate: 1.7088e-05]
	Learning Rate: 1.7088e-05
	LOSS [training: 0.25822188988531386 | validation: 0.233886391921795]
	TIME [epoch: 6.32 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25928142942384685		[learning rate: 1.7028e-05]
	Learning Rate: 1.70276e-05
	LOSS [training: 0.25928142942384685 | validation: 0.25714603424290333]
	TIME [epoch: 6.37 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26674570278937837		[learning rate: 1.6967e-05]
	Learning Rate: 1.69674e-05
	LOSS [training: 0.26674570278937837 | validation: 0.24285417946822213]
	TIME [epoch: 6.31 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2649523900984283		[learning rate: 1.6907e-05]
	Learning Rate: 1.69074e-05
	LOSS [training: 0.2649523900984283 | validation: 0.2445936127419631]
	TIME [epoch: 6.31 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2581896967412643		[learning rate: 1.6848e-05]
	Learning Rate: 1.68476e-05
	LOSS [training: 0.2581896967412643 | validation: 0.23693187680199818]
	TIME [epoch: 6.31 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26181877763760936		[learning rate: 1.6788e-05]
	Learning Rate: 1.6788e-05
	LOSS [training: 0.26181877763760936 | validation: 0.2554042234480531]
	TIME [epoch: 6.31 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2644299676521202		[learning rate: 1.6729e-05]
	Learning Rate: 1.67287e-05
	LOSS [training: 0.2644299676521202 | validation: 0.23991541927641102]
	TIME [epoch: 6.31 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2571670261972757		[learning rate: 1.667e-05]
	Learning Rate: 1.66695e-05
	LOSS [training: 0.2571670261972757 | validation: 0.24630098870933254]
	TIME [epoch: 6.35 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2583335198066296		[learning rate: 1.6611e-05]
	Learning Rate: 1.66106e-05
	LOSS [training: 0.2583335198066296 | validation: 0.2404092792810436]
	TIME [epoch: 6.33 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2608152487077336		[learning rate: 1.6552e-05]
	Learning Rate: 1.65518e-05
	LOSS [training: 0.2608152487077336 | validation: 0.23917312599573926]
	TIME [epoch: 6.31 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2582579059238848		[learning rate: 1.6493e-05]
	Learning Rate: 1.64933e-05
	LOSS [training: 0.2582579059238848 | validation: 0.2528603845373246]
	TIME [epoch: 6.31 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25844777657689844		[learning rate: 1.6435e-05]
	Learning Rate: 1.6435e-05
	LOSS [training: 0.25844777657689844 | validation: 0.25231673742583316]
	TIME [epoch: 6.31 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26069654405949044		[learning rate: 1.6377e-05]
	Learning Rate: 1.63769e-05
	LOSS [training: 0.26069654405949044 | validation: 0.23996391769291803]
	TIME [epoch: 6.31 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2567375081408114		[learning rate: 1.6319e-05]
	Learning Rate: 1.6319e-05
	LOSS [training: 0.2567375081408114 | validation: 0.23988675513384233]
	TIME [epoch: 6.35 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25977744272428993		[learning rate: 1.6261e-05]
	Learning Rate: 1.62612e-05
	LOSS [training: 0.25977744272428993 | validation: 0.24763443403528623]
	TIME [epoch: 6.34 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25864944188403705		[learning rate: 1.6204e-05]
	Learning Rate: 1.62038e-05
	LOSS [training: 0.25864944188403705 | validation: 0.24896521606480332]
	TIME [epoch: 6.31 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2575056655612022		[learning rate: 1.6146e-05]
	Learning Rate: 1.61464e-05
	LOSS [training: 0.2575056655612022 | validation: 0.2394157482518856]
	TIME [epoch: 6.31 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.266475486670405		[learning rate: 1.6089e-05]
	Learning Rate: 1.60894e-05
	LOSS [training: 0.266475486670405 | validation: 0.25165866001447856]
	TIME [epoch: 6.32 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26260583911544555		[learning rate: 1.6032e-05]
	Learning Rate: 1.60325e-05
	LOSS [training: 0.26260583911544555 | validation: 0.25187904088208735]
	TIME [epoch: 6.31 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26192527059378096		[learning rate: 1.5976e-05]
	Learning Rate: 1.59758e-05
	LOSS [training: 0.26192527059378096 | validation: 0.24798928453033126]
	TIME [epoch: 6.35 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2605053974084411		[learning rate: 1.5919e-05]
	Learning Rate: 1.59193e-05
	LOSS [training: 0.2605053974084411 | validation: 0.24818826472832628]
	TIME [epoch: 6.33 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2577747142450257		[learning rate: 1.5863e-05]
	Learning Rate: 1.5863e-05
	LOSS [training: 0.2577747142450257 | validation: 0.2503498827094759]
	TIME [epoch: 6.31 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2598390858932477		[learning rate: 1.5807e-05]
	Learning Rate: 1.58069e-05
	LOSS [training: 0.2598390858932477 | validation: 0.24975907948592202]
	TIME [epoch: 6.31 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25576401666880244		[learning rate: 1.5751e-05]
	Learning Rate: 1.5751e-05
	LOSS [training: 0.25576401666880244 | validation: 0.25283410097278664]
	TIME [epoch: 6.31 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2547797715447727		[learning rate: 1.5695e-05]
	Learning Rate: 1.56953e-05
	LOSS [training: 0.2547797715447727 | validation: 0.24852784200443628]
	TIME [epoch: 6.31 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26057514897645506		[learning rate: 1.564e-05]
	Learning Rate: 1.56398e-05
	LOSS [training: 0.26057514897645506 | validation: 0.2541190871635638]
	TIME [epoch: 6.34 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2609906294125218		[learning rate: 1.5584e-05]
	Learning Rate: 1.55845e-05
	LOSS [training: 0.2609906294125218 | validation: 0.25344269287195514]
	TIME [epoch: 6.34 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25116571799897164		[learning rate: 1.5529e-05]
	Learning Rate: 1.55294e-05
	LOSS [training: 0.25116571799897164 | validation: 0.25005895239944204]
	TIME [epoch: 6.32 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26128473434422617		[learning rate: 1.5474e-05]
	Learning Rate: 1.54745e-05
	LOSS [training: 0.26128473434422617 | validation: 0.2421625432890519]
	TIME [epoch: 6.32 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2587404233569417		[learning rate: 1.542e-05]
	Learning Rate: 1.54197e-05
	LOSS [training: 0.2587404233569417 | validation: 0.25420659949318875]
	TIME [epoch: 6.31 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26370287878964016		[learning rate: 1.5365e-05]
	Learning Rate: 1.53652e-05
	LOSS [training: 0.26370287878964016 | validation: 0.24684336207736773]
	TIME [epoch: 6.32 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25636774215446884		[learning rate: 1.5311e-05]
	Learning Rate: 1.53109e-05
	LOSS [training: 0.25636774215446884 | validation: 0.24256345304353555]
	TIME [epoch: 6.34 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26001035186096977		[learning rate: 1.5257e-05]
	Learning Rate: 1.52567e-05
	LOSS [training: 0.26001035186096977 | validation: 0.24919386262903748]
	TIME [epoch: 6.35 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2563301972376647		[learning rate: 1.5203e-05]
	Learning Rate: 1.52028e-05
	LOSS [training: 0.2563301972376647 | validation: 0.2528624482198984]
	TIME [epoch: 6.32 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2587993999026771		[learning rate: 1.5149e-05]
	Learning Rate: 1.5149e-05
	LOSS [training: 0.2587993999026771 | validation: 0.24835563185592036]
	TIME [epoch: 6.32 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26283353505480705		[learning rate: 1.5095e-05]
	Learning Rate: 1.50955e-05
	LOSS [training: 0.26283353505480705 | validation: 0.2414856648680535]
	TIME [epoch: 6.31 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.254633550031744		[learning rate: 1.5042e-05]
	Learning Rate: 1.50421e-05
	LOSS [training: 0.254633550031744 | validation: 0.24952289068571087]
	TIME [epoch: 6.31 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2560905276203185		[learning rate: 1.4989e-05]
	Learning Rate: 1.49889e-05
	LOSS [training: 0.2560905276203185 | validation: 0.25497254824609367]
	TIME [epoch: 6.33 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2606704696198352		[learning rate: 1.4936e-05]
	Learning Rate: 1.49359e-05
	LOSS [training: 0.2606704696198352 | validation: 0.24421879455379303]
	TIME [epoch: 6.34 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2644161311138957		[learning rate: 1.4883e-05]
	Learning Rate: 1.48831e-05
	LOSS [training: 0.2644161311138957 | validation: 0.24642247537610354]
	TIME [epoch: 6.32 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2596584847856955		[learning rate: 1.483e-05]
	Learning Rate: 1.48304e-05
	LOSS [training: 0.2596584847856955 | validation: 0.24311863496296626]
	TIME [epoch: 6.32 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25848047153202275		[learning rate: 1.4778e-05]
	Learning Rate: 1.4778e-05
	LOSS [training: 0.25848047153202275 | validation: 0.25661494857033373]
	TIME [epoch: 6.31 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25912301476450256		[learning rate: 1.4726e-05]
	Learning Rate: 1.47257e-05
	LOSS [training: 0.25912301476450256 | validation: 0.24118976331889586]
	TIME [epoch: 6.31 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2554221525860511		[learning rate: 1.4674e-05]
	Learning Rate: 1.46737e-05
	LOSS [training: 0.2554221525860511 | validation: 0.24880822734063723]
	TIME [epoch: 6.34 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2556408516616289		[learning rate: 1.4622e-05]
	Learning Rate: 1.46218e-05
	LOSS [training: 0.2556408516616289 | validation: 0.240953527430031]
	TIME [epoch: 6.34 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25141239091674494		[learning rate: 1.457e-05]
	Learning Rate: 1.45701e-05
	LOSS [training: 0.25141239091674494 | validation: 0.24315426871143847]
	TIME [epoch: 6.32 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26354152720477636		[learning rate: 1.4519e-05]
	Learning Rate: 1.45185e-05
	LOSS [training: 0.26354152720477636 | validation: 0.23689802646991487]
	TIME [epoch: 6.32 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26452518648793966		[learning rate: 1.4467e-05]
	Learning Rate: 1.44672e-05
	LOSS [training: 0.26452518648793966 | validation: 0.2392134324808649]
	TIME [epoch: 6.32 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2655652661466862		[learning rate: 1.4416e-05]
	Learning Rate: 1.44161e-05
	LOSS [training: 0.2655652661466862 | validation: 0.23906247047857843]
	TIME [epoch: 6.32 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26231026915218986		[learning rate: 1.4365e-05]
	Learning Rate: 1.43651e-05
	LOSS [training: 0.26231026915218986 | validation: 0.25263725023323447]
	TIME [epoch: 6.33 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.268227798696831		[learning rate: 1.4314e-05]
	Learning Rate: 1.43143e-05
	LOSS [training: 0.268227798696831 | validation: 0.2538963183083067]
	TIME [epoch: 6.35 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26834503812936694		[learning rate: 1.4264e-05]
	Learning Rate: 1.42637e-05
	LOSS [training: 0.26834503812936694 | validation: 0.24433181762301764]
	TIME [epoch: 6.31 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27018571843801237		[learning rate: 1.4213e-05]
	Learning Rate: 1.42132e-05
	LOSS [training: 0.27018571843801237 | validation: 0.24104935583787493]
	TIME [epoch: 6.31 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2664973626230341		[learning rate: 1.4163e-05]
	Learning Rate: 1.4163e-05
	LOSS [training: 0.2664973626230341 | validation: 0.24938077975377843]
	TIME [epoch: 6.31 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2615610019383818		[learning rate: 1.4113e-05]
	Learning Rate: 1.41129e-05
	LOSS [training: 0.2615610019383818 | validation: 0.2552372649966738]
	TIME [epoch: 6.31 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2639790986905658		[learning rate: 1.4063e-05]
	Learning Rate: 1.4063e-05
	LOSS [training: 0.2639790986905658 | validation: 0.2568319639311417]
	TIME [epoch: 6.32 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2599047835040089		[learning rate: 1.4013e-05]
	Learning Rate: 1.40132e-05
	LOSS [training: 0.2599047835040089 | validation: 0.24520266998630647]
	TIME [epoch: 6.36 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2612942950479333		[learning rate: 1.3964e-05]
	Learning Rate: 1.39637e-05
	LOSS [training: 0.2612942950479333 | validation: 0.2520858439116964]
	TIME [epoch: 6.31 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26614521348949083		[learning rate: 1.3914e-05]
	Learning Rate: 1.39143e-05
	LOSS [training: 0.26614521348949083 | validation: 0.25363869677613604]
	TIME [epoch: 6.31 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2638377016836456		[learning rate: 1.3865e-05]
	Learning Rate: 1.38651e-05
	LOSS [training: 0.2638377016836456 | validation: 0.24643069102728238]
	TIME [epoch: 6.31 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2613263106685906		[learning rate: 1.3816e-05]
	Learning Rate: 1.38161e-05
	LOSS [training: 0.2613263106685906 | validation: 0.2528300687853139]
	TIME [epoch: 6.31 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.262395129097463		[learning rate: 1.3767e-05]
	Learning Rate: 1.37672e-05
	LOSS [training: 0.262395129097463 | validation: 0.253774278374245]
	TIME [epoch: 6.32 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2597893085819686		[learning rate: 1.3719e-05]
	Learning Rate: 1.37185e-05
	LOSS [training: 0.2597893085819686 | validation: 0.24102143818450855]
	TIME [epoch: 6.36 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2650954281708842		[learning rate: 1.367e-05]
	Learning Rate: 1.367e-05
	LOSS [training: 0.2650954281708842 | validation: 0.23708503919592233]
	TIME [epoch: 6.31 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25104611829334406		[learning rate: 1.3622e-05]
	Learning Rate: 1.36217e-05
	LOSS [training: 0.25104611829334406 | validation: 0.24782844612660931]
	TIME [epoch: 6.31 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25838880772757367		[learning rate: 1.3574e-05]
	Learning Rate: 1.35735e-05
	LOSS [training: 0.25838880772757367 | validation: 0.25649663378777426]
	TIME [epoch: 6.31 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2560503735850763		[learning rate: 1.3526e-05]
	Learning Rate: 1.35255e-05
	LOSS [training: 0.2560503735850763 | validation: 0.23901475395043076]
	TIME [epoch: 6.31 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26326238333438085		[learning rate: 1.3478e-05]
	Learning Rate: 1.34777e-05
	LOSS [training: 0.26326238333438085 | validation: 0.2452475993890991]
	TIME [epoch: 6.32 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2587250988393617		[learning rate: 1.343e-05]
	Learning Rate: 1.343e-05
	LOSS [training: 0.2587250988393617 | validation: 0.24454390125156056]
	TIME [epoch: 6.36 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.257458530297599		[learning rate: 1.3383e-05]
	Learning Rate: 1.33825e-05
	LOSS [training: 0.257458530297599 | validation: 0.2439048948580494]
	TIME [epoch: 6.31 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25768706160260757		[learning rate: 1.3335e-05]
	Learning Rate: 1.33352e-05
	LOSS [training: 0.25768706160260757 | validation: 0.24153949218992882]
	TIME [epoch: 6.31 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25942589102869634		[learning rate: 1.3288e-05]
	Learning Rate: 1.32881e-05
	LOSS [training: 0.25942589102869634 | validation: 0.24572524236689172]
	TIME [epoch: 6.31 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2567470525829991		[learning rate: 1.3241e-05]
	Learning Rate: 1.32411e-05
	LOSS [training: 0.2567470525829991 | validation: 0.24136984001844874]
	TIME [epoch: 6.31 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26475658216535536		[learning rate: 1.3194e-05]
	Learning Rate: 1.31942e-05
	LOSS [training: 0.26475658216535536 | validation: 0.24192780860633065]
	TIME [epoch: 6.32 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2545872876726334		[learning rate: 1.3148e-05]
	Learning Rate: 1.31476e-05
	LOSS [training: 0.2545872876726334 | validation: 0.2472517442299204]
	TIME [epoch: 6.36 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26007255150071934		[learning rate: 1.3101e-05]
	Learning Rate: 1.31011e-05
	LOSS [training: 0.26007255150071934 | validation: 0.24114034648326987]
	TIME [epoch: 6.32 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26035479147669544		[learning rate: 1.3055e-05]
	Learning Rate: 1.30548e-05
	LOSS [training: 0.26035479147669544 | validation: 0.2340501916390672]
	TIME [epoch: 6.31 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25982572914644664		[learning rate: 1.3009e-05]
	Learning Rate: 1.30086e-05
	LOSS [training: 0.25982572914644664 | validation: 0.2566908676931062]
	TIME [epoch: 6.31 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2565062176747121		[learning rate: 1.2963e-05]
	Learning Rate: 1.29626e-05
	LOSS [training: 0.2565062176747121 | validation: 0.23865669139647633]
	TIME [epoch: 6.31 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26179201122373147		[learning rate: 1.2917e-05]
	Learning Rate: 1.29168e-05
	LOSS [training: 0.26179201122373147 | validation: 0.23546523686971949]
	TIME [epoch: 6.32 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25857425121839117		[learning rate: 1.2871e-05]
	Learning Rate: 1.28711e-05
	LOSS [training: 0.25857425121839117 | validation: 0.23797517848073743]
	TIME [epoch: 6.36 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25934717608008906		[learning rate: 1.2826e-05]
	Learning Rate: 1.28256e-05
	LOSS [training: 0.25934717608008906 | validation: 0.24453896369564215]
	TIME [epoch: 6.32 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25406896331508666		[learning rate: 1.278e-05]
	Learning Rate: 1.27802e-05
	LOSS [training: 0.25406896331508666 | validation: 0.24587634480772713]
	TIME [epoch: 6.31 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26333829680910203		[learning rate: 1.2735e-05]
	Learning Rate: 1.2735e-05
	LOSS [training: 0.26333829680910203 | validation: 0.24117558345073617]
	TIME [epoch: 6.32 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25870109812938313		[learning rate: 1.269e-05]
	Learning Rate: 1.269e-05
	LOSS [training: 0.25870109812938313 | validation: 0.24034984823832797]
	TIME [epoch: 6.31 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2577350204000539		[learning rate: 1.2645e-05]
	Learning Rate: 1.26451e-05
	LOSS [training: 0.2577350204000539 | validation: 0.2398584217631199]
	TIME [epoch: 6.33 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2601644203657186		[learning rate: 1.26e-05]
	Learning Rate: 1.26004e-05
	LOSS [training: 0.2601644203657186 | validation: 0.24191299946201733]
	TIME [epoch: 6.36 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25654827002587643		[learning rate: 1.2556e-05]
	Learning Rate: 1.25559e-05
	LOSS [training: 0.25654827002587643 | validation: 0.24962628542882778]
	TIME [epoch: 6.32 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2587111762431041		[learning rate: 1.2511e-05]
	Learning Rate: 1.25115e-05
	LOSS [training: 0.2587111762431041 | validation: 0.2556498704892456]
	TIME [epoch: 6.32 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26231601298811513		[learning rate: 1.2467e-05]
	Learning Rate: 1.24672e-05
	LOSS [training: 0.26231601298811513 | validation: 0.24194899405075873]
	TIME [epoch: 6.32 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2538662882361303		[learning rate: 1.2423e-05]
	Learning Rate: 1.24231e-05
	LOSS [training: 0.2538662882361303 | validation: 0.2424949836209392]
	TIME [epoch: 6.31 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25998627173671784		[learning rate: 1.2379e-05]
	Learning Rate: 1.23792e-05
	LOSS [training: 0.25998627173671784 | validation: 0.24354046558847559]
	TIME [epoch: 6.32 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2591176568183332		[learning rate: 1.2335e-05]
	Learning Rate: 1.23354e-05
	LOSS [training: 0.2591176568183332 | validation: 0.2552799342404247]
	TIME [epoch: 6.36 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2628109558801243		[learning rate: 1.2292e-05]
	Learning Rate: 1.22918e-05
	LOSS [training: 0.2628109558801243 | validation: 0.23532566278453407]
	TIME [epoch: 6.32 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2565149792674801		[learning rate: 1.2248e-05]
	Learning Rate: 1.22483e-05
	LOSS [training: 0.2565149792674801 | validation: 0.2410960658376063]
	TIME [epoch: 6.31 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2618515185289904		[learning rate: 1.2205e-05]
	Learning Rate: 1.2205e-05
	LOSS [training: 0.2618515185289904 | validation: 0.2405764214596282]
	TIME [epoch: 6.31 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25953031457869086		[learning rate: 1.2162e-05]
	Learning Rate: 1.21619e-05
	LOSS [training: 0.25953031457869086 | validation: 0.23778366214544525]
	TIME [epoch: 6.32 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2564023792820136		[learning rate: 1.2119e-05]
	Learning Rate: 1.21189e-05
	LOSS [training: 0.2564023792820136 | validation: 0.24848893940271088]
	TIME [epoch: 6.32 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2600989100806205		[learning rate: 1.2076e-05]
	Learning Rate: 1.2076e-05
	LOSS [training: 0.2600989100806205 | validation: 0.24119532625379647]
	TIME [epoch: 6.37 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2606754081842588		[learning rate: 1.2033e-05]
	Learning Rate: 1.20333e-05
	LOSS [training: 0.2606754081842588 | validation: 0.23711994139777431]
	TIME [epoch: 6.31 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2571835973556722		[learning rate: 1.1991e-05]
	Learning Rate: 1.19907e-05
	LOSS [training: 0.2571835973556722 | validation: 0.24407512662406722]
	TIME [epoch: 6.31 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2625976996772924		[learning rate: 1.1948e-05]
	Learning Rate: 1.19483e-05
	LOSS [training: 0.2625976996772924 | validation: 0.2489532202292052]
	TIME [epoch: 6.31 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25928661710983253		[learning rate: 1.1906e-05]
	Learning Rate: 1.19061e-05
	LOSS [training: 0.25928661710983253 | validation: 0.2422758738617025]
	TIME [epoch: 6.32 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2651055568361073		[learning rate: 1.1864e-05]
	Learning Rate: 1.1864e-05
	LOSS [training: 0.2651055568361073 | validation: 0.24637505557523665]
	TIME [epoch: 6.31 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26135025569424997		[learning rate: 1.1822e-05]
	Learning Rate: 1.1822e-05
	LOSS [training: 0.26135025569424997 | validation: 0.23792586359080015]
	TIME [epoch: 6.37 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25611249318777785		[learning rate: 1.178e-05]
	Learning Rate: 1.17802e-05
	LOSS [training: 0.25611249318777785 | validation: 0.2437146821418262]
	TIME [epoch: 6.32 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2595491362353591		[learning rate: 1.1739e-05]
	Learning Rate: 1.17386e-05
	LOSS [training: 0.2595491362353591 | validation: 0.24597240682714416]
	TIME [epoch: 6.31 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25768059034739077		[learning rate: 1.1697e-05]
	Learning Rate: 1.16971e-05
	LOSS [training: 0.25768059034739077 | validation: 0.24567543918964513]
	TIME [epoch: 6.31 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2617057993394975		[learning rate: 1.1656e-05]
	Learning Rate: 1.16557e-05
	LOSS [training: 0.2617057993394975 | validation: 0.24696316754616238]
	TIME [epoch: 6.31 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25707281549924804		[learning rate: 1.1614e-05]
	Learning Rate: 1.16145e-05
	LOSS [training: 0.25707281549924804 | validation: 0.2488921555651864]
	TIME [epoch: 6.31 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26011821026306037		[learning rate: 1.1573e-05]
	Learning Rate: 1.15734e-05
	LOSS [training: 0.26011821026306037 | validation: 0.24285220876534225]
	TIME [epoch: 6.35 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26135662712639707		[learning rate: 1.1532e-05]
	Learning Rate: 1.15325e-05
	LOSS [training: 0.26135662712639707 | validation: 0.23169957730480084]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1960.pth
	Model improved!!!
EPOCH 1961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2616465441386201		[learning rate: 1.1492e-05]
	Learning Rate: 1.14917e-05
	LOSS [training: 0.2616465441386201 | validation: 0.25442622597706455]
	TIME [epoch: 6.31 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25866375559986143		[learning rate: 1.1451e-05]
	Learning Rate: 1.14511e-05
	LOSS [training: 0.25866375559986143 | validation: 0.23569533205507454]
	TIME [epoch: 6.31 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25795405799232185		[learning rate: 1.1411e-05]
	Learning Rate: 1.14106e-05
	LOSS [training: 0.25795405799232185 | validation: 0.25089586326289914]
	TIME [epoch: 6.31 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2562844647012076		[learning rate: 1.137e-05]
	Learning Rate: 1.13702e-05
	LOSS [training: 0.2562844647012076 | validation: 0.254316551606641]
	TIME [epoch: 6.3 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25834119438215597		[learning rate: 1.133e-05]
	Learning Rate: 1.133e-05
	LOSS [training: 0.25834119438215597 | validation: 0.25013542091716423]
	TIME [epoch: 6.35 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2609576629766926		[learning rate: 1.129e-05]
	Learning Rate: 1.129e-05
	LOSS [training: 0.2609576629766926 | validation: 0.23895001536835755]
	TIME [epoch: 6.33 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26248927396880134		[learning rate: 1.125e-05]
	Learning Rate: 1.125e-05
	LOSS [training: 0.26248927396880134 | validation: 0.23730149287717575]
	TIME [epoch: 6.32 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2608912046460126		[learning rate: 1.121e-05]
	Learning Rate: 1.12103e-05
	LOSS [training: 0.2608912046460126 | validation: 0.22779201095773882]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_1a_v2_20240311_131309/states/model_phi1_1a_v2_1968.pth
	Model improved!!!
EPOCH 1969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25863421736278436		[learning rate: 1.1171e-05]
	Learning Rate: 1.11706e-05
	LOSS [training: 0.25863421736278436 | validation: 0.2520985201136054]
	TIME [epoch: 6.31 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2609982618711225		[learning rate: 1.1131e-05]
	Learning Rate: 1.11311e-05
	LOSS [training: 0.2609982618711225 | validation: 0.23593701366723452]
	TIME [epoch: 6.31 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25163911959583823		[learning rate: 1.1092e-05]
	Learning Rate: 1.10918e-05
	LOSS [training: 0.25163911959583823 | validation: 0.23993342646962013]
	TIME [epoch: 6.36 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2560216528058747		[learning rate: 1.1053e-05]
	Learning Rate: 1.10525e-05
	LOSS [training: 0.2560216528058747 | validation: 0.24076089189783237]
	TIME [epoch: 6.32 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25438245225693173		[learning rate: 1.1013e-05]
	Learning Rate: 1.10134e-05
	LOSS [training: 0.25438245225693173 | validation: 0.23772301432688153]
	TIME [epoch: 6.31 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2616631497973193		[learning rate: 1.0975e-05]
	Learning Rate: 1.09745e-05
	LOSS [training: 0.2616631497973193 | validation: 0.24256035141938848]
	TIME [epoch: 6.31 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26130125544826416		[learning rate: 1.0936e-05]
	Learning Rate: 1.09357e-05
	LOSS [training: 0.26130125544826416 | validation: 0.24577730647215007]
	TIME [epoch: 6.31 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.259860099955649		[learning rate: 1.0897e-05]
	Learning Rate: 1.0897e-05
	LOSS [training: 0.259860099955649 | validation: 0.24137796275842774]
	TIME [epoch: 6.31 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25858788007395545		[learning rate: 1.0858e-05]
	Learning Rate: 1.08585e-05
	LOSS [training: 0.25858788007395545 | validation: 0.24533000786782327]
	TIME [epoch: 6.36 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2613191973395548		[learning rate: 1.082e-05]
	Learning Rate: 1.08201e-05
	LOSS [training: 0.2613191973395548 | validation: 0.2409991270538509]
	TIME [epoch: 6.32 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2656436363390756		[learning rate: 1.0782e-05]
	Learning Rate: 1.07818e-05
	LOSS [training: 0.2656436363390756 | validation: 0.23320412226873988]
	TIME [epoch: 6.32 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2601687299963101		[learning rate: 1.0744e-05]
	Learning Rate: 1.07437e-05
	LOSS [training: 0.2601687299963101 | validation: 0.24491152262705435]
	TIME [epoch: 6.31 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.259799967040768		[learning rate: 1.0706e-05]
	Learning Rate: 1.07057e-05
	LOSS [training: 0.259799967040768 | validation: 0.2402305666361348]
	TIME [epoch: 6.31 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2608751694982979		[learning rate: 1.0668e-05]
	Learning Rate: 1.06679e-05
	LOSS [training: 0.2608751694982979 | validation: 0.2435277190116402]
	TIME [epoch: 6.31 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2598967737354861		[learning rate: 1.063e-05]
	Learning Rate: 1.06301e-05
	LOSS [training: 0.2598967737354861 | validation: 0.23631812221162984]
	TIME [epoch: 6.35 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2528740952084104		[learning rate: 1.0593e-05]
	Learning Rate: 1.05925e-05
	LOSS [training: 0.2528740952084104 | validation: 0.24267131868155245]
	TIME [epoch: 6.33 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25864645289834665		[learning rate: 1.0555e-05]
	Learning Rate: 1.05551e-05
	LOSS [training: 0.25864645289834665 | validation: 0.24042509325472433]
	TIME [epoch: 6.3 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25605129475005955		[learning rate: 1.0518e-05]
	Learning Rate: 1.05178e-05
	LOSS [training: 0.25605129475005955 | validation: 0.2385188588998941]
	TIME [epoch: 6.31 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25938013306492735		[learning rate: 1.0481e-05]
	Learning Rate: 1.04806e-05
	LOSS [training: 0.25938013306492735 | validation: 0.2294858597030479]
	TIME [epoch: 6.31 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2627830541761955		[learning rate: 1.0444e-05]
	Learning Rate: 1.04435e-05
	LOSS [training: 0.2627830541761955 | validation: 0.24476748710688978]
	TIME [epoch: 6.3 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25841356608968635		[learning rate: 1.0407e-05]
	Learning Rate: 1.04066e-05
	LOSS [training: 0.25841356608968635 | validation: 0.24374532186931508]
	TIME [epoch: 6.34 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2615538783753526		[learning rate: 1.037e-05]
	Learning Rate: 1.03698e-05
	LOSS [training: 0.2615538783753526 | validation: 0.23276462554744098]
	TIME [epoch: 6.32 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2620058214712564		[learning rate: 1.0333e-05]
	Learning Rate: 1.03331e-05
	LOSS [training: 0.2620058214712564 | validation: 0.24593324948364076]
	TIME [epoch: 6.31 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25755697341997064		[learning rate: 1.0297e-05]
	Learning Rate: 1.02966e-05
	LOSS [training: 0.25755697341997064 | validation: 0.24606708274289288]
	TIME [epoch: 6.32 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2603591564914848		[learning rate: 1.026e-05]
	Learning Rate: 1.02602e-05
	LOSS [training: 0.2603591564914848 | validation: 0.24101659363635575]
	TIME [epoch: 6.3 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26262848329250404		[learning rate: 1.0224e-05]
	Learning Rate: 1.02239e-05
	LOSS [training: 0.26262848329250404 | validation: 0.24096035850437117]
	TIME [epoch: 6.3 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26025396994284355		[learning rate: 1.0188e-05]
	Learning Rate: 1.01877e-05
	LOSS [training: 0.26025396994284355 | validation: 0.25073044036334813]
	TIME [epoch: 6.32 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25844985242691587		[learning rate: 1.0152e-05]
	Learning Rate: 1.01517e-05
	LOSS [training: 0.25844985242691587 | validation: 0.2471513531864229]
	TIME [epoch: 6.33 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2606312087375385		[learning rate: 1.0116e-05]
	Learning Rate: 1.01158e-05
	LOSS [training: 0.2606312087375385 | validation: 0.23304679703969594]
	TIME [epoch: 6.3 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25779179062161933		[learning rate: 1.008e-05]
	Learning Rate: 1.008e-05
	LOSS [training: 0.25779179062161933 | validation: 0.23464048271301302]
	TIME [epoch: 6.3 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2581358984638944		[learning rate: 1.0044e-05]
	Learning Rate: 1.00444e-05
	LOSS [training: 0.2581358984638944 | validation: 0.23235422648931142]
	TIME [epoch: 6.3 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25550357992670625		[learning rate: 1.0009e-05]
	Learning Rate: 1.00089e-05
	LOSS [training: 0.25550357992670625 | validation: 0.24928715712289023]
	TIME [epoch: 6.3 sec]
Finished training in 12940.333 seconds.
