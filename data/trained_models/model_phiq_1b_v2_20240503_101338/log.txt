Args:
Namespace(name='model_phiq_1b_v2', outdir='out/model_training/model_phiq_1b_v2', training_data='data/training_data/data_phiq_1b/training', validation_data='data/training_data/data_phiq_1b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.01, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.75, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4066892827

Training model...

Saving initial model state to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 4/4] avg loss: 11.058378001020664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.058378001020664 | validation: 11.468996010131741]
	TIME [epoch: 114 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 4/4] avg loss: 10.396116401728854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.396116401728854 | validation: 11.118333148263613]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.761233455497758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.761233455497758 | validation: 10.675851277592429]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.745206786390874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.745206786390874 | validation: 10.471778971743369]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.216319252855065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.216319252855065 | validation: 10.207349106304402]
	TIME [epoch: 6.37 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.863296625934305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.863296625934305 | validation: 9.986645668019328]
	TIME [epoch: 6.39 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_6.pth
	Model improved!!!
EPOCH 7/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.477034981541834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.477034981541834 | validation: 9.434668256335371]
	TIME [epoch: 6.37 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_7.pth
	Model improved!!!
EPOCH 8/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.55753624747685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.55753624747685 | validation: 9.710395269393373]
	TIME [epoch: 6.36 sec]
EPOCH 9/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.5995496635188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.5995496635188 | validation: 9.753410610396386]
	TIME [epoch: 6.36 sec]
EPOCH 10/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.465757677191323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.465757677191323 | validation: 9.560702014739075]
	TIME [epoch: 6.35 sec]
EPOCH 11/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.2714762834071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.2714762834071 | validation: 9.368673301052489]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_11.pth
	Model improved!!!
EPOCH 12/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.612087822647371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.612087822647371 | validation: 9.578931086949106]
	TIME [epoch: 6.35 sec]
EPOCH 13/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.285050014851258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.285050014851258 | validation: 9.110265127129598]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_13.pth
	Model improved!!!
EPOCH 14/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.964485805897472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.964485805897472 | validation: 8.907481506084492]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_14.pth
	Model improved!!!
EPOCH 15/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.132854087210674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.132854087210674 | validation: 8.848974164166538]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_15.pth
	Model improved!!!
EPOCH 16/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.340892762235942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.340892762235942 | validation: 8.835064124561534]
	TIME [epoch: 6.38 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_16.pth
	Model improved!!!
EPOCH 17/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.087225508786261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.087225508786261 | validation: 8.955220719242046]
	TIME [epoch: 6.35 sec]
EPOCH 18/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.349646765214061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.349646765214061 | validation: 9.608380899491696]
	TIME [epoch: 6.34 sec]
EPOCH 19/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.48448781454141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.48448781454141 | validation: 9.134353850517622]
	TIME [epoch: 6.33 sec]
EPOCH 20/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.923253320271035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.923253320271035 | validation: 9.00308115862622]
	TIME [epoch: 6.34 sec]
EPOCH 21/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.853244017202305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.853244017202305 | validation: 8.885982203399294]
	TIME [epoch: 6.38 sec]
EPOCH 22/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.799362529433457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.799362529433457 | validation: 8.893242703218736]
	TIME [epoch: 6.36 sec]
EPOCH 23/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.736882120519317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.736882120519317 | validation: 8.608210656805063]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_23.pth
	Model improved!!!
EPOCH 24/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.624211990473278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.624211990473278 | validation: 8.302285355510215]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_24.pth
	Model improved!!!
EPOCH 25/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.541085835820775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.541085835820775 | validation: 8.427735001149436]
	TIME [epoch: 6.35 sec]
EPOCH 26/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.0166041436003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.0166041436003 | validation: 8.2982776467184]
	TIME [epoch: 6.37 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_26.pth
	Model improved!!!
EPOCH 27/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.787020926867249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.787020926867249 | validation: 8.286398459194555]
	TIME [epoch: 6.37 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_27.pth
	Model improved!!!
EPOCH 28/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.922362838492211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.922362838492211 | validation: 9.15353631717618]
	TIME [epoch: 6.35 sec]
EPOCH 29/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.67575776540842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.67575776540842 | validation: 8.578811213878645]
	TIME [epoch: 6.34 sec]
EPOCH 30/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.953862893105718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.953862893105718 | validation: 8.471654977792507]
	TIME [epoch: 6.34 sec]
EPOCH 31/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.7275195550684534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.7275195550684534 | validation: 8.351187477877707]
	TIME [epoch: 6.34 sec]
EPOCH 32/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.584683769331705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.584683769331705 | validation: 8.43643788057522]
	TIME [epoch: 6.39 sec]
EPOCH 33/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.495112730987252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.495112730987252 | validation: 8.359012500336126]
	TIME [epoch: 6.35 sec]
EPOCH 34/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.395155195270017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.395155195270017 | validation: 8.139255862316073]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_34.pth
	Model improved!!!
EPOCH 35/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.384720828384591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.384720828384591 | validation: 8.034734920954314]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_35.pth
	Model improved!!!
EPOCH 36/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.496209622622004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.496209622622004 | validation: 8.325459485899215]
	TIME [epoch: 6.35 sec]
EPOCH 37/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.501190890233489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.501190890233489 | validation: 8.239442756693318]
	TIME [epoch: 6.4 sec]
EPOCH 38/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.475506806205455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.475506806205455 | validation: 8.279978574433244]
	TIME [epoch: 6.36 sec]
EPOCH 39/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.446586800188284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.446586800188284 | validation: 8.179271136573952]
	TIME [epoch: 6.36 sec]
EPOCH 40/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.341347123703507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.341347123703507 | validation: 8.053555442831794]
	TIME [epoch: 6.35 sec]
EPOCH 41/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.376044247236514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.376044247236514 | validation: 8.328148853133115]
	TIME [epoch: 6.36 sec]
EPOCH 42/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.448246969503951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.448246969503951 | validation: 8.155416715939644]
	TIME [epoch: 6.37 sec]
EPOCH 43/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.341895886445446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.341895886445446 | validation: 8.117445808698369]
	TIME [epoch: 6.39 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.2959718852969235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.2959718852969235 | validation: 8.204228882784353]
	TIME [epoch: 6.36 sec]
EPOCH 45/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.270435984680164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.270435984680164 | validation: 7.969646175667826]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_45.pth
	Model improved!!!
EPOCH 46/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.270058532384083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.270058532384083 | validation: 7.806236706388981]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_46.pth
	Model improved!!!
EPOCH 47/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.3260632822920915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.3260632822920915 | validation: 8.172447810694475]
	TIME [epoch: 6.35 sec]
EPOCH 48/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.344794564938789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.344794564938789 | validation: 8.073458235553053]
	TIME [epoch: 6.39 sec]
EPOCH 49/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.295498278357788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.295498278357788 | validation: 7.990134495092844]
	TIME [epoch: 6.35 sec]
EPOCH 50/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.236211759266704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.236211759266704 | validation: 7.806093559939036]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_50.pth
	Model improved!!!
EPOCH 51/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.171814550516599		[learning rate: 0.0099456]
	Learning Rate: 0.00994561
	LOSS [training: 7.171814550516599 | validation: 7.901062410999087]
	TIME [epoch: 6.35 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.189848930979799		[learning rate: 0.0098736]
	Learning Rate: 0.00987356
	LOSS [training: 7.189848930979799 | validation: 7.7481294043696955]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_52.pth
	Model improved!!!
EPOCH 53/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.199233589458067		[learning rate: 0.009802]
	Learning Rate: 0.00980202
	LOSS [training: 7.199233589458067 | validation: 7.572292155709506]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_53.pth
	Model improved!!!
EPOCH 54/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.210895451527564		[learning rate: 0.009731]
	Learning Rate: 0.00973101
	LOSS [training: 7.210895451527564 | validation: 7.638653876296026]
	TIME [epoch: 6.36 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.067627162597613		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 7.067627162597613 | validation: 7.806513414274319]
	TIME [epoch: 6.35 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.0741917443128965		[learning rate: 0.0095905]
	Learning Rate: 0.00959052
	LOSS [training: 7.0741917443128965 | validation: 7.534657747356858]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_56.pth
	Model improved!!!
EPOCH 57/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.017469543279455		[learning rate: 0.009521]
	Learning Rate: 0.00952104
	LOSS [training: 7.017469543279455 | validation: 7.6237439174372295]
	TIME [epoch: 6.34 sec]
EPOCH 58/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.041959241133381		[learning rate: 0.0094521]
	Learning Rate: 0.00945206
	LOSS [training: 7.041959241133381 | validation: 7.442027976467767]
	TIME [epoch: 6.38 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_58.pth
	Model improved!!!
EPOCH 59/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.973441334052829		[learning rate: 0.0093836]
	Learning Rate: 0.00938358
	LOSS [training: 6.973441334052829 | validation: 7.521253603688267]
	TIME [epoch: 6.37 sec]
EPOCH 60/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.0099360538086355		[learning rate: 0.0093156]
	Learning Rate: 0.00931559
	LOSS [training: 7.0099360538086355 | validation: 7.439740596109685]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_60.pth
	Model improved!!!
EPOCH 61/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.588315089084686		[learning rate: 0.0092481]
	Learning Rate: 0.0092481
	LOSS [training: 7.588315089084686 | validation: 8.609393910227585]
	TIME [epoch: 6.34 sec]
EPOCH 62/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.70505989316292		[learning rate: 0.0091811]
	Learning Rate: 0.0091811
	LOSS [training: 9.70505989316292 | validation: 8.324324276149474]
	TIME [epoch: 6.34 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.730108178752667		[learning rate: 0.0091146]
	Learning Rate: 0.00911458
	LOSS [training: 8.730108178752667 | validation: 7.932781388305154]
	TIME [epoch: 6.35 sec]
EPOCH 64/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.8518560598517215		[learning rate: 0.0090485]
	Learning Rate: 0.00904855
	LOSS [training: 7.8518560598517215 | validation: 7.664456131345167]
	TIME [epoch: 6.38 sec]
EPOCH 65/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.2545443570569486		[learning rate: 0.008983]
	Learning Rate: 0.00898299
	LOSS [training: 7.2545443570569486 | validation: 7.675807629219939]
	TIME [epoch: 6.34 sec]
EPOCH 66/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.104579683065884		[learning rate: 0.0089179]
	Learning Rate: 0.00891791
	LOSS [training: 7.104579683065884 | validation: 7.636151702817566]
	TIME [epoch: 6.35 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.060680758784423		[learning rate: 0.0088533]
	Learning Rate: 0.0088533
	LOSS [training: 7.060680758784423 | validation: 7.58669983534975]
	TIME [epoch: 6.35 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.974356133004878		[learning rate: 0.0087892]
	Learning Rate: 0.00878916
	LOSS [training: 6.974356133004878 | validation: 7.406975171649426]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_68.pth
	Model improved!!!
EPOCH 69/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.991362181045692		[learning rate: 0.0087255]
	Learning Rate: 0.00872548
	LOSS [training: 6.991362181045692 | validation: 7.515938616675156]
	TIME [epoch: 6.39 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.940279523190055		[learning rate: 0.0086623]
	Learning Rate: 0.00866227
	LOSS [training: 6.940279523190055 | validation: 7.536448734426061]
	TIME [epoch: 6.36 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.951913520522968		[learning rate: 0.0085995]
	Learning Rate: 0.00859951
	LOSS [training: 6.951913520522968 | validation: 7.383412358379397]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_71.pth
	Model improved!!!
EPOCH 72/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.018106318184059		[learning rate: 0.0085372]
	Learning Rate: 0.00853721
	LOSS [training: 7.018106318184059 | validation: 7.4021329855423]
	TIME [epoch: 6.35 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.096543249778303		[learning rate: 0.0084754]
	Learning Rate: 0.00847535
	LOSS [training: 7.096543249778303 | validation: 7.45407730157218]
	TIME [epoch: 6.35 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.989245762764102		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 6.989245762764102 | validation: 7.416361839602391]
	TIME [epoch: 6.38 sec]
EPOCH 75/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.977403166508791		[learning rate: 0.008353]
	Learning Rate: 0.00835299
	LOSS [training: 6.977403166508791 | validation: 7.371247681484368]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_75.pth
	Model improved!!!
EPOCH 76/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.891187334545189		[learning rate: 0.0082925]
	Learning Rate: 0.00829248
	LOSS [training: 6.891187334545189 | validation: 7.605947197822871]
	TIME [epoch: 6.35 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.92637299164366		[learning rate: 0.0082324]
	Learning Rate: 0.0082324
	LOSS [training: 6.92637299164366 | validation: 7.375273285547257]
	TIME [epoch: 6.34 sec]
EPOCH 78/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.8835626721387495		[learning rate: 0.0081728]
	Learning Rate: 0.00817275
	LOSS [training: 6.8835626721387495 | validation: 7.185323382941956]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_78.pth
	Model improved!!!
EPOCH 79/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.952157669133511		[learning rate: 0.0081135]
	Learning Rate: 0.00811354
	LOSS [training: 6.952157669133511 | validation: 7.253757738994553]
	TIME [epoch: 6.37 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.93897840308247		[learning rate: 0.0080548]
	Learning Rate: 0.00805476
	LOSS [training: 6.93897840308247 | validation: 7.294269620770974]
	TIME [epoch: 6.38 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.972080434058755		[learning rate: 0.0079964]
	Learning Rate: 0.0079964
	LOSS [training: 6.972080434058755 | validation: 7.166420999010722]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_81.pth
	Model improved!!!
EPOCH 82/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.848454799767385		[learning rate: 0.0079385]
	Learning Rate: 0.00793847
	LOSS [training: 6.848454799767385 | validation: 7.328389043833855]
	TIME [epoch: 6.35 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.868429521599581		[learning rate: 0.007881]
	Learning Rate: 0.00788096
	LOSS [training: 6.868429521599581 | validation: 7.226625766831063]
	TIME [epoch: 6.35 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.8969800704680235		[learning rate: 0.0078239]
	Learning Rate: 0.00782386
	LOSS [training: 6.8969800704680235 | validation: 7.414472885963265]
	TIME [epoch: 6.35 sec]
EPOCH 85/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.851163464695142		[learning rate: 0.0077672]
	Learning Rate: 0.00776718
	LOSS [training: 6.851163464695142 | validation: 7.40353336087664]
	TIME [epoch: 6.4 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.7982121671282885		[learning rate: 0.0077109]
	Learning Rate: 0.0077109
	LOSS [training: 6.7982121671282885 | validation: 7.298262256812272]
	TIME [epoch: 6.35 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.809995383132179		[learning rate: 0.007655]
	Learning Rate: 0.00765504
	LOSS [training: 6.809995383132179 | validation: 7.293733316908519]
	TIME [epoch: 6.35 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.817369228527414		[learning rate: 0.0075996]
	Learning Rate: 0.00759958
	LOSS [training: 6.817369228527414 | validation: 7.470746877259498]
	TIME [epoch: 6.35 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.875542603289623		[learning rate: 0.0075445]
	Learning Rate: 0.00754452
	LOSS [training: 6.875542603289623 | validation: 7.2024923346938206]
	TIME [epoch: 6.35 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.782251037768592		[learning rate: 0.0074899]
	Learning Rate: 0.00748986
	LOSS [training: 6.782251037768592 | validation: 7.18599346948854]
	TIME [epoch: 6.38 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.747680568369481		[learning rate: 0.0074356]
	Learning Rate: 0.0074356
	LOSS [training: 6.747680568369481 | validation: 7.2989747876210505]
	TIME [epoch: 6.35 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.713647048381028		[learning rate: 0.0073817]
	Learning Rate: 0.00738173
	LOSS [training: 6.713647048381028 | validation: 7.158451175812304]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_92.pth
	Model improved!!!
EPOCH 93/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.751590174423758		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 6.751590174423758 | validation: 7.331182608224935]
	TIME [epoch: 6.35 sec]
EPOCH 94/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.882230999591967		[learning rate: 0.0072752]
	Learning Rate: 0.00727515
	LOSS [training: 6.882230999591967 | validation: 7.169439936824432]
	TIME [epoch: 6.35 sec]
EPOCH 95/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.710588430939531		[learning rate: 0.0072224]
	Learning Rate: 0.00722244
	LOSS [training: 6.710588430939531 | validation: 7.0929124322229296]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_95.pth
	Model improved!!!
EPOCH 96/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.6749056362014345		[learning rate: 0.0071701]
	Learning Rate: 0.00717012
	LOSS [training: 6.6749056362014345 | validation: 7.121490990128997]
	TIME [epoch: 6.39 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.673606578369523		[learning rate: 0.0071182]
	Learning Rate: 0.00711817
	LOSS [training: 6.673606578369523 | validation: 6.988676892851673]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_97.pth
	Model improved!!!
EPOCH 98/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.528791847236505		[learning rate: 0.0070666]
	Learning Rate: 0.0070666
	LOSS [training: 6.528791847236505 | validation: 6.848281098015621]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_98.pth
	Model improved!!!
EPOCH 99/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.419954127345928		[learning rate: 0.0070154]
	Learning Rate: 0.0070154
	LOSS [training: 6.419954127345928 | validation: 6.7999672065192165]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_99.pth
	Model improved!!!
EPOCH 100/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.456299493377991		[learning rate: 0.0069646]
	Learning Rate: 0.00696458
	LOSS [training: 6.456299493377991 | validation: 6.5990767169001145]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_100.pth
	Model improved!!!
EPOCH 101/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.334967099628302		[learning rate: 0.0069141]
	Learning Rate: 0.00691412
	LOSS [training: 6.334967099628302 | validation: 6.8666411874111155]
	TIME [epoch: 6.38 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.193941687404499		[learning rate: 0.006864]
	Learning Rate: 0.00686403
	LOSS [training: 7.193941687404499 | validation: 7.747136929870112]
	TIME [epoch: 6.33 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.572502571126094		[learning rate: 0.0068143]
	Learning Rate: 0.0068143
	LOSS [training: 9.572502571126094 | validation: 9.12223592343576]
	TIME [epoch: 6.33 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.806203928550678		[learning rate: 0.0067649]
	Learning Rate: 0.00676493
	LOSS [training: 9.806203928550678 | validation: 8.347349419586928]
	TIME [epoch: 6.33 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.413036812103472		[learning rate: 0.0067159]
	Learning Rate: 0.00671592
	LOSS [training: 9.413036812103472 | validation: 7.949191380191917]
	TIME [epoch: 6.34 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.137080172679253		[learning rate: 0.0066673]
	Learning Rate: 0.00666726
	LOSS [training: 9.137080172679253 | validation: 7.964129243801102]
	TIME [epoch: 6.38 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 4/4] avg loss: 9.084958227719685		[learning rate: 0.006619]
	Learning Rate: 0.00661896
	LOSS [training: 9.084958227719685 | validation: 7.630450526736729]
	TIME [epoch: 6.35 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.81174549476951		[learning rate: 0.006571]
	Learning Rate: 0.006571
	LOSS [training: 8.81174549476951 | validation: 7.567684781690103]
	TIME [epoch: 6.32 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.629122987588499		[learning rate: 0.0065234]
	Learning Rate: 0.00652339
	LOSS [training: 8.629122987588499 | validation: 7.445752250305494]
	TIME [epoch: 6.34 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.879612874240275		[learning rate: 0.0064761]
	Learning Rate: 0.00647613
	LOSS [training: 7.879612874240275 | validation: 6.898366049379889]
	TIME [epoch: 6.34 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.413754635425535		[learning rate: 0.0064292]
	Learning Rate: 0.00642921
	LOSS [training: 7.413754635425535 | validation: 7.277522726492795]
	TIME [epoch: 6.34 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.608916518167591		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 7.608916518167591 | validation: 7.114793148071561]
	TIME [epoch: 6.38 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.988473300348272		[learning rate: 0.0063364]
	Learning Rate: 0.00633639
	LOSS [training: 6.988473300348272 | validation: 6.877262209061978]
	TIME [epoch: 6.35 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.826394754756336		[learning rate: 0.0062905]
	Learning Rate: 0.00629049
	LOSS [training: 6.826394754756336 | validation: 6.890832081893335]
	TIME [epoch: 6.34 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.77222252524524		[learning rate: 0.0062449]
	Learning Rate: 0.00624491
	LOSS [training: 6.77222252524524 | validation: 6.8751586371106015]
	TIME [epoch: 6.34 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.854340750381623		[learning rate: 0.0061997]
	Learning Rate: 0.00619967
	LOSS [training: 6.854340750381623 | validation: 6.921499781707096]
	TIME [epoch: 6.35 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.875671418369711		[learning rate: 0.0061548]
	Learning Rate: 0.00615475
	LOSS [training: 6.875671418369711 | validation: 7.003663736129757]
	TIME [epoch: 6.38 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.839411411881158		[learning rate: 0.0061102]
	Learning Rate: 0.00611016
	LOSS [training: 6.839411411881158 | validation: 6.98300208787022]
	TIME [epoch: 6.36 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.817002847867657		[learning rate: 0.0060659]
	Learning Rate: 0.00606589
	LOSS [training: 6.817002847867657 | validation: 6.963234265800917]
	TIME [epoch: 6.35 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.757188213801264		[learning rate: 0.0060219]
	Learning Rate: 0.00602195
	LOSS [training: 6.757188213801264 | validation: 6.785588200580395]
	TIME [epoch: 6.34 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.696676050201776		[learning rate: 0.0059783]
	Learning Rate: 0.00597832
	LOSS [training: 6.696676050201776 | validation: 6.892227237417065]
	TIME [epoch: 6.34 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.68688735960836		[learning rate: 0.005935]
	Learning Rate: 0.005935
	LOSS [training: 6.68688735960836 | validation: 7.015415649511261]
	TIME [epoch: 6.36 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.724129802140737		[learning rate: 0.005892]
	Learning Rate: 0.00589201
	LOSS [training: 6.724129802140737 | validation: 7.115871214297734]
	TIME [epoch: 6.38 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.638788817023913		[learning rate: 0.0058493]
	Learning Rate: 0.00584932
	LOSS [training: 6.638788817023913 | validation: 6.935911792724227]
	TIME [epoch: 6.36 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.591447713598802		[learning rate: 0.0058069]
	Learning Rate: 0.00580694
	LOSS [training: 6.591447713598802 | validation: 6.759615546765881]
	TIME [epoch: 6.35 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.531443334531682		[learning rate: 0.0057649]
	Learning Rate: 0.00576487
	LOSS [training: 6.531443334531682 | validation: 6.618888971872012]
	TIME [epoch: 6.35 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.44796369140947		[learning rate: 0.0057231]
	Learning Rate: 0.0057231
	LOSS [training: 6.44796369140947 | validation: 6.6380998405896925]
	TIME [epoch: 6.35 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.41301848877305		[learning rate: 0.0056816]
	Learning Rate: 0.00568164
	LOSS [training: 6.41301848877305 | validation: 6.628450112183115]
	TIME [epoch: 6.39 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.436202442972015		[learning rate: 0.0056405]
	Learning Rate: 0.00564048
	LOSS [training: 6.436202442972015 | validation: 6.339478783366145]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_129.pth
	Model improved!!!
EPOCH 130/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.258012720117409		[learning rate: 0.0055996]
	Learning Rate: 0.00559961
	LOSS [training: 6.258012720117409 | validation: 6.761038444009829]
	TIME [epoch: 6.34 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.392792263220208		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 6.392792263220208 | validation: 6.1321494046667135]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_131.pth
	Model improved!!!
EPOCH 132/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.171129880004456		[learning rate: 0.0055188]
	Learning Rate: 0.00551877
	LOSS [training: 6.171129880004456 | validation: 6.5605706305128]
	TIME [epoch: 6.34 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.195022412146686		[learning rate: 0.0054788]
	Learning Rate: 0.00547878
	LOSS [training: 6.195022412146686 | validation: 5.974034309704795]
	TIME [epoch: 6.37 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_133.pth
	Model improved!!!
EPOCH 134/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.061503687675778		[learning rate: 0.0054391]
	Learning Rate: 0.00543909
	LOSS [training: 6.061503687675778 | validation: 6.057580142745879]
	TIME [epoch: 6.35 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.020280163575475		[learning rate: 0.0053997]
	Learning Rate: 0.00539968
	LOSS [training: 6.020280163575475 | validation: 6.128262612574131]
	TIME [epoch: 6.34 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.000257178836727		[learning rate: 0.0053606]
	Learning Rate: 0.00536056
	LOSS [training: 6.000257178836727 | validation: 6.25521564353459]
	TIME [epoch: 6.34 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.061559444080698		[learning rate: 0.0053217]
	Learning Rate: 0.00532173
	LOSS [training: 7.061559444080698 | validation: 7.034898611987908]
	TIME [epoch: 6.35 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.836719507982922		[learning rate: 0.0052832]
	Learning Rate: 0.00528317
	LOSS [training: 6.836719507982922 | validation: 6.718893350978439]
	TIME [epoch: 6.36 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.5060785973334205		[learning rate: 0.0052449]
	Learning Rate: 0.0052449
	LOSS [training: 6.5060785973334205 | validation: 6.783696103231431]
	TIME [epoch: 6.37 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.2380293932954025		[learning rate: 0.0052069]
	Learning Rate: 0.0052069
	LOSS [training: 6.2380293932954025 | validation: 6.072609698072352]
	TIME [epoch: 6.34 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.063809251390476		[learning rate: 0.0051692]
	Learning Rate: 0.00516917
	LOSS [training: 6.063809251390476 | validation: 6.3058313387210525]
	TIME [epoch: 6.34 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.32651047672423		[learning rate: 0.0051317]
	Learning Rate: 0.00513172
	LOSS [training: 6.32651047672423 | validation: 6.18556522149475]
	TIME [epoch: 6.35 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.387634157247469		[learning rate: 0.0050945]
	Learning Rate: 0.00509454
	LOSS [training: 6.387634157247469 | validation: 6.527011852396462]
	TIME [epoch: 6.35 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.824817307953302		[learning rate: 0.0050576]
	Learning Rate: 0.00505763
	LOSS [training: 6.824817307953302 | validation: 5.962672091548667]
	TIME [epoch: 6.39 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_144.pth
	Model improved!!!
EPOCH 145/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.953946189284892		[learning rate: 0.005021]
	Learning Rate: 0.00502099
	LOSS [training: 5.953946189284892 | validation: 5.969779609303123]
	TIME [epoch: 6.35 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.001598424732833		[learning rate: 0.0049846]
	Learning Rate: 0.00498461
	LOSS [training: 6.001598424732833 | validation: 6.328099011952375]
	TIME [epoch: 6.34 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.0652137978198075		[learning rate: 0.0049485]
	Learning Rate: 0.0049485
	LOSS [training: 6.0652137978198075 | validation: 6.020090542205054]
	TIME [epoch: 6.34 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.018343429288917		[learning rate: 0.0049126]
	Learning Rate: 0.00491265
	LOSS [training: 6.018343429288917 | validation: 5.949578224657966]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_148.pth
	Model improved!!!
EPOCH 149/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.974425258620382		[learning rate: 0.0048771]
	Learning Rate: 0.00487706
	LOSS [training: 5.974425258620382 | validation: 6.038399323937551]
	TIME [epoch: 6.37 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.2907020927813155		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 6.2907020927813155 | validation: 6.0605771003164755]
	TIME [epoch: 6.36 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.944818693651605		[learning rate: 0.0048066]
	Learning Rate: 0.00480665
	LOSS [training: 5.944818693651605 | validation: 5.866303217239392]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_151.pth
	Model improved!!!
EPOCH 152/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.752038444639791		[learning rate: 0.0047718]
	Learning Rate: 0.00477182
	LOSS [training: 5.752038444639791 | validation: 5.720541863152924]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_152.pth
	Model improved!!!
EPOCH 153/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.714144231243811		[learning rate: 0.0047373]
	Learning Rate: 0.00473725
	LOSS [training: 5.714144231243811 | validation: 5.602469715819314]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_153.pth
	Model improved!!!
EPOCH 154/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.619990325937243		[learning rate: 0.0047029]
	Learning Rate: 0.00470293
	LOSS [training: 5.619990325937243 | validation: 6.163216747740214]
	TIME [epoch: 6.36 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.379854930633648		[learning rate: 0.0046689]
	Learning Rate: 0.00466886
	LOSS [training: 6.379854930633648 | validation: 6.343434917618006]
	TIME [epoch: 6.38 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.0480150610839605		[learning rate: 0.004635]
	Learning Rate: 0.00463503
	LOSS [training: 6.0480150610839605 | validation: 5.8337625001835]
	TIME [epoch: 6.34 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.964865711558493		[learning rate: 0.0046015]
	Learning Rate: 0.00460145
	LOSS [training: 5.964865711558493 | validation: 6.301645352686938]
	TIME [epoch: 6.33 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.795061856668221		[learning rate: 0.0045681]
	Learning Rate: 0.00456811
	LOSS [training: 5.795061856668221 | validation: 5.518063610116329]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_158.pth
	Model improved!!!
EPOCH 159/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.634266763582157		[learning rate: 0.004535]
	Learning Rate: 0.00453502
	LOSS [training: 5.634266763582157 | validation: 5.928454482159024]
	TIME [epoch: 6.35 sec]
EPOCH 160/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.746012494382189		[learning rate: 0.0045022]
	Learning Rate: 0.00450216
	LOSS [training: 5.746012494382189 | validation: 6.235858763188769]
	TIME [epoch: 6.39 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.789470759053875		[learning rate: 0.0044695]
	Learning Rate: 0.00446954
	LOSS [training: 5.789470759053875 | validation: 5.733210757734833]
	TIME [epoch: 6.36 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.648771867325592		[learning rate: 0.0044372]
	Learning Rate: 0.00443716
	LOSS [training: 5.648771867325592 | validation: 5.689616686471794]
	TIME [epoch: 6.35 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.649533322420181		[learning rate: 0.004405]
	Learning Rate: 0.00440501
	LOSS [training: 5.649533322420181 | validation: 5.712528855681176]
	TIME [epoch: 6.35 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.646233443817575		[learning rate: 0.0043731]
	Learning Rate: 0.0043731
	LOSS [training: 5.646233443817575 | validation: 5.464931380814658]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_164.pth
	Model improved!!!
EPOCH 165/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.408127557420435		[learning rate: 0.0043414]
	Learning Rate: 0.00434142
	LOSS [training: 5.408127557420435 | validation: 5.373710511147651]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_165.pth
	Model improved!!!
EPOCH 166/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.468826435852691		[learning rate: 0.00431]
	Learning Rate: 0.00430996
	LOSS [training: 5.468826435852691 | validation: 5.335129592078964]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_166.pth
	Model improved!!!
EPOCH 167/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.421805725311906		[learning rate: 0.0042787]
	Learning Rate: 0.00427874
	LOSS [training: 5.421805725311906 | validation: 5.374739527421201]
	TIME [epoch: 6.35 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.413959324173696		[learning rate: 0.0042477]
	Learning Rate: 0.00424774
	LOSS [training: 5.413959324173696 | validation: 5.451313343902721]
	TIME [epoch: 6.34 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.381777822201926		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 5.381777822201926 | validation: 5.516148037816043]
	TIME [epoch: 6.34 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.707714976761956		[learning rate: 0.0041864]
	Learning Rate: 0.00418641
	LOSS [training: 5.707714976761956 | validation: 5.221031173518176]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_170.pth
	Model improved!!!
EPOCH 171/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.65993159880723		[learning rate: 0.0041561]
	Learning Rate: 0.00415608
	LOSS [training: 5.65993159880723 | validation: 5.933863575517967]
	TIME [epoch: 6.38 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.482918793119115		[learning rate: 0.004126]
	Learning Rate: 0.00412597
	LOSS [training: 5.482918793119115 | validation: 5.294215677749063]
	TIME [epoch: 6.35 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.875505087365527		[learning rate: 0.0040961]
	Learning Rate: 0.00409608
	LOSS [training: 5.875505087365527 | validation: 5.73143745482157]
	TIME [epoch: 6.35 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.376448528886726		[learning rate: 0.0040664]
	Learning Rate: 0.0040664
	LOSS [training: 5.376448528886726 | validation: 4.952679519597893]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_174.pth
	Model improved!!!
EPOCH 175/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.121744620682474		[learning rate: 0.0040369]
	Learning Rate: 0.00403694
	LOSS [training: 5.121744620682474 | validation: 4.950640420383801]
	TIME [epoch: 6.37 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_175.pth
	Model improved!!!
EPOCH 176/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.314553547358347		[learning rate: 0.0040077]
	Learning Rate: 0.0040077
	LOSS [training: 5.314553547358347 | validation: 6.029209691653507]
	TIME [epoch: 6.38 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.0171455062196735		[learning rate: 0.0039787]
	Learning Rate: 0.00397866
	LOSS [training: 6.0171455062196735 | validation: 6.286582295273943]
	TIME [epoch: 6.34 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.600691708375235		[learning rate: 0.0039498]
	Learning Rate: 0.00394984
	LOSS [training: 5.600691708375235 | validation: 5.862627955957095]
	TIME [epoch: 6.35 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.636583046846182		[learning rate: 0.0039212]
	Learning Rate: 0.00392122
	LOSS [training: 5.636583046846182 | validation: 6.0841917669981616]
	TIME [epoch: 6.35 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.706141445073408		[learning rate: 0.0038928]
	Learning Rate: 0.00389281
	LOSS [training: 5.706141445073408 | validation: 6.373763096927641]
	TIME [epoch: 6.34 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.314488594158475		[learning rate: 0.0038646]
	Learning Rate: 0.00386461
	LOSS [training: 5.314488594158475 | validation: 5.214170007319222]
	TIME [epoch: 6.4 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.081223550768532		[learning rate: 0.0038366]
	Learning Rate: 0.00383661
	LOSS [training: 5.081223550768532 | validation: 5.000108819965673]
	TIME [epoch: 6.35 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.777428102373161		[learning rate: 0.0038088]
	Learning Rate: 0.00380881
	LOSS [training: 5.777428102373161 | validation: 6.06123000836531]
	TIME [epoch: 6.35 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.293735873068921		[learning rate: 0.0037812]
	Learning Rate: 0.00378122
	LOSS [training: 7.293735873068921 | validation: 8.208945560113527]
	TIME [epoch: 6.34 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.716313313686411		[learning rate: 0.0037538]
	Learning Rate: 0.00375382
	LOSS [training: 8.716313313686411 | validation: 6.8974513001205136]
	TIME [epoch: 6.34 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.809738187207883		[learning rate: 0.0037266]
	Learning Rate: 0.00372663
	LOSS [training: 7.809738187207883 | validation: 6.599114843413272]
	TIME [epoch: 6.36 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.770591936959121		[learning rate: 0.0036996]
	Learning Rate: 0.00369963
	LOSS [training: 6.770591936959121 | validation: 5.482573968805786]
	TIME [epoch: 6.38 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.535817600291877		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 5.535817600291877 | validation: 6.339137305219746]
	TIME [epoch: 6.35 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.826124996656309		[learning rate: 0.0036462]
	Learning Rate: 0.00364621
	LOSS [training: 5.826124996656309 | validation: 5.5474751855228615]
	TIME [epoch: 6.34 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.396259283599778		[learning rate: 0.0036198]
	Learning Rate: 0.0036198
	LOSS [training: 5.396259283599778 | validation: 5.457044080376615]
	TIME [epoch: 6.35 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.317639480921892		[learning rate: 0.0035936]
	Learning Rate: 0.00359357
	LOSS [training: 5.317639480921892 | validation: 5.327032564574491]
	TIME [epoch: 6.35 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.2825426937643		[learning rate: 0.0035675]
	Learning Rate: 0.00356754
	LOSS [training: 5.2825426937643 | validation: 4.970238851000731]
	TIME [epoch: 6.39 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.016571792762106		[learning rate: 0.0035417]
	Learning Rate: 0.00354169
	LOSS [training: 5.016571792762106 | validation: 5.03634505999479]
	TIME [epoch: 6.36 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.084550428518579		[learning rate: 0.003516]
	Learning Rate: 0.00351603
	LOSS [training: 5.084550428518579 | validation: 4.971233242712978]
	TIME [epoch: 6.34 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.642782313808742		[learning rate: 0.0034906]
	Learning Rate: 0.00349056
	LOSS [training: 5.642782313808742 | validation: 6.868223017434618]
	TIME [epoch: 6.34 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.284820225852487		[learning rate: 0.0034653]
	Learning Rate: 0.00346527
	LOSS [training: 6.284820225852487 | validation: 7.0312214293478945]
	TIME [epoch: 6.34 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.777144797991624		[learning rate: 0.0034402]
	Learning Rate: 0.00344016
	LOSS [training: 6.777144797991624 | validation: 7.481505856109825]
	TIME [epoch: 6.38 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.675725278069805		[learning rate: 0.0034152]
	Learning Rate: 0.00341524
	LOSS [training: 6.675725278069805 | validation: 6.543003623437064]
	TIME [epoch: 6.37 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.9382681053836395		[learning rate: 0.0033905]
	Learning Rate: 0.0033905
	LOSS [training: 5.9382681053836395 | validation: 6.635673486596909]
	TIME [epoch: 6.35 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.071575163797408		[learning rate: 0.0033659]
	Learning Rate: 0.00336593
	LOSS [training: 6.071575163797408 | validation: 6.479811860034902]
	TIME [epoch: 6.35 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.537459821106078		[learning rate: 0.0033415]
	Learning Rate: 0.00334155
	LOSS [training: 5.537459821106078 | validation: 5.347374622495488]
	TIME [epoch: 6.35 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.379331704895795		[learning rate: 0.0033173]
	Learning Rate: 0.00331734
	LOSS [training: 5.379331704895795 | validation: 5.600086870865164]
	TIME [epoch: 6.35 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.680512477438554		[learning rate: 0.0032933]
	Learning Rate: 0.0032933
	LOSS [training: 5.680512477438554 | validation: 5.735208672341448]
	TIME [epoch: 6.4 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.674315863042669		[learning rate: 0.0032694]
	Learning Rate: 0.00326944
	LOSS [training: 5.674315863042669 | validation: 5.927965481106748]
	TIME [epoch: 6.34 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.226032431504088		[learning rate: 0.0032458]
	Learning Rate: 0.00324576
	LOSS [training: 5.226032431504088 | validation: 5.403585149109625]
	TIME [epoch: 6.35 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.281964220727612		[learning rate: 0.0032222]
	Learning Rate: 0.00322224
	LOSS [training: 5.281964220727612 | validation: 4.922886672539253]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_206.pth
	Model improved!!!
EPOCH 207/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.286782877272087		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 5.286782877272087 | validation: 5.317921338721435]
	TIME [epoch: 6.35 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.550617064401429		[learning rate: 0.0031757]
	Learning Rate: 0.00317572
	LOSS [training: 5.550617064401429 | validation: 4.968044669973361]
	TIME [epoch: 6.39 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.037799970977483		[learning rate: 0.0031527]
	Learning Rate: 0.00315271
	LOSS [training: 5.037799970977483 | validation: 5.2525046674137545]
	TIME [epoch: 6.36 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.280148451909074		[learning rate: 0.0031299]
	Learning Rate: 0.00312987
	LOSS [training: 5.280148451909074 | validation: 5.244337629906795]
	TIME [epoch: 6.34 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.426159254203043		[learning rate: 0.0031072]
	Learning Rate: 0.00310719
	LOSS [training: 5.426159254203043 | validation: 5.390914642912166]
	TIME [epoch: 6.35 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.924312278775933		[learning rate: 0.0030847]
	Learning Rate: 0.00308468
	LOSS [training: 5.924312278775933 | validation: 5.337882448422532]
	TIME [epoch: 6.35 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.624892389429494		[learning rate: 0.0030623]
	Learning Rate: 0.00306233
	LOSS [training: 5.624892389429494 | validation: 6.884117951796819]
	TIME [epoch: 6.36 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.358822450590222		[learning rate: 0.0030401]
	Learning Rate: 0.00304015
	LOSS [training: 7.358822450590222 | validation: 7.907116045530568]
	TIME [epoch: 6.38 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.992676403198645		[learning rate: 0.0030181]
	Learning Rate: 0.00301812
	LOSS [training: 7.992676403198645 | validation: 7.312659703714769]
	TIME [epoch: 6.34 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.128530119916453		[learning rate: 0.0029963]
	Learning Rate: 0.00299626
	LOSS [training: 7.128530119916453 | validation: 6.105416952489907]
	TIME [epoch: 6.34 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.777021681894649		[learning rate: 0.0029745]
	Learning Rate: 0.00297455
	LOSS [training: 5.777021681894649 | validation: 5.009803797026952]
	TIME [epoch: 6.34 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.003108835291653		[learning rate: 0.002953]
	Learning Rate: 0.002953
	LOSS [training: 5.003108835291653 | validation: 4.848894404923049]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_218.pth
	Model improved!!!
EPOCH 219/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.992656958645238		[learning rate: 0.0029316]
	Learning Rate: 0.0029316
	LOSS [training: 4.992656958645238 | validation: 4.915675673061632]
	TIME [epoch: 6.39 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.009017539649754		[learning rate: 0.0029104]
	Learning Rate: 0.00291036
	LOSS [training: 5.009017539649754 | validation: 5.073402540816042]
	TIME [epoch: 6.35 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.0628769161134635		[learning rate: 0.0028893]
	Learning Rate: 0.00288928
	LOSS [training: 6.0628769161134635 | validation: 6.7491616935461485]
	TIME [epoch: 6.34 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.0001570743457595		[learning rate: 0.0028683]
	Learning Rate: 0.00286835
	LOSS [training: 6.0001570743457595 | validation: 5.299512516427867]
	TIME [epoch: 6.34 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.051557147618372		[learning rate: 0.0028476]
	Learning Rate: 0.00284757
	LOSS [training: 5.051557147618372 | validation: 4.843907240905628]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_223.pth
	Model improved!!!
EPOCH 224/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.947632943358644		[learning rate: 0.0028269]
	Learning Rate: 0.00282693
	LOSS [training: 4.947632943358644 | validation: 4.952692070354022]
	TIME [epoch: 6.38 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.1287349202063925		[learning rate: 0.0028065]
	Learning Rate: 0.00280645
	LOSS [training: 5.1287349202063925 | validation: 4.66736165101195]
	TIME [epoch: 6.37 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_225.pth
	Model improved!!!
EPOCH 226/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.8490230372156615		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 4.8490230372156615 | validation: 4.9681728780032905]
	TIME [epoch: 6.35 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.880103544662563		[learning rate: 0.0027659]
	Learning Rate: 0.00276594
	LOSS [training: 4.880103544662563 | validation: 4.768319579059002]
	TIME [epoch: 6.34 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.798161739742497		[learning rate: 0.0027459]
	Learning Rate: 0.0027459
	LOSS [training: 4.798161739742497 | validation: 6.167011229894731]
	TIME [epoch: 6.35 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.648153603313673		[learning rate: 0.002726]
	Learning Rate: 0.002726
	LOSS [training: 6.648153603313673 | validation: 7.386486384063856]
	TIME [epoch: 6.36 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.410611764648877		[learning rate: 0.0027063]
	Learning Rate: 0.00270625
	LOSS [training: 7.410611764648877 | validation: 7.670937859962402]
	TIME [epoch: 6.38 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.351203562951277		[learning rate: 0.0026866]
	Learning Rate: 0.00268665
	LOSS [training: 8.351203562951277 | validation: 8.167486908124506]
	TIME [epoch: 6.35 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.546504159360863		[learning rate: 0.0026672]
	Learning Rate: 0.00266718
	LOSS [training: 8.546504159360863 | validation: 7.578269419179857]
	TIME [epoch: 6.35 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.979336526974573		[learning rate: 0.0026479]
	Learning Rate: 0.00264786
	LOSS [training: 7.979336526974573 | validation: 7.631146476606043]
	TIME [epoch: 6.35 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.667862077488104		[learning rate: 0.0026287]
	Learning Rate: 0.00262867
	LOSS [training: 7.667862077488104 | validation: 7.228092121965894]
	TIME [epoch: 6.35 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.1194026232356515		[learning rate: 0.0026096]
	Learning Rate: 0.00260963
	LOSS [training: 7.1194026232356515 | validation: 5.917406258305581]
	TIME [epoch: 6.39 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.060012894495452		[learning rate: 0.0025907]
	Learning Rate: 0.00259072
	LOSS [training: 6.060012894495452 | validation: 5.795566253165735]
	TIME [epoch: 6.36 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.194958243901048		[learning rate: 0.002572]
	Learning Rate: 0.00257195
	LOSS [training: 7.194958243901048 | validation: 6.251735012235502]
	TIME [epoch: 6.34 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.321518869526639		[learning rate: 0.0025533]
	Learning Rate: 0.00255332
	LOSS [training: 7.321518869526639 | validation: 5.758857332816687]
	TIME [epoch: 6.35 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.966387060482717		[learning rate: 0.0025348]
	Learning Rate: 0.00253482
	LOSS [training: 6.966387060482717 | validation: 5.731399385569812]
	TIME [epoch: 6.34 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.8094976395429345		[learning rate: 0.0025165]
	Learning Rate: 0.00251646
	LOSS [training: 6.8094976395429345 | validation: 5.693460611284616]
	TIME [epoch: 6.36 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.266042480605531		[learning rate: 0.0024982]
	Learning Rate: 0.00249823
	LOSS [training: 7.266042480605531 | validation: 6.7150365989261145]
	TIME [epoch: 6.38 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.30582906894419		[learning rate: 0.0024801]
	Learning Rate: 0.00248013
	LOSS [training: 7.30582906894419 | validation: 5.527469736897217]
	TIME [epoch: 6.35 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.0927412449195515		[learning rate: 0.0024622]
	Learning Rate: 0.00246216
	LOSS [training: 6.0927412449195515 | validation: 5.111764332138382]
	TIME [epoch: 6.35 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.269005020518545		[learning rate: 0.0024443]
	Learning Rate: 0.00244432
	LOSS [training: 5.269005020518545 | validation: 5.151008254141196]
	TIME [epoch: 6.35 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.173970553919715		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 5.173970553919715 | validation: 4.9598129099680115]
	TIME [epoch: 6.35 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.535852123038844		[learning rate: 0.002409]
	Learning Rate: 0.00240903
	LOSS [training: 5.535852123038844 | validation: 6.749768073339004]
	TIME [epoch: 6.39 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.128157487865247		[learning rate: 0.0023916]
	Learning Rate: 0.00239158
	LOSS [training: 7.128157487865247 | validation: 6.3377896201720665]
	TIME [epoch: 6.35 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.933617400263993		[learning rate: 0.0023742]
	Learning Rate: 0.00237425
	LOSS [training: 5.933617400263993 | validation: 6.094896126023194]
	TIME [epoch: 6.35 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.616430729309092		[learning rate: 0.002357]
	Learning Rate: 0.00235705
	LOSS [training: 5.616430729309092 | validation: 6.314475574110618]
	TIME [epoch: 6.34 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.422005375626503		[learning rate: 0.00234]
	Learning Rate: 0.00233997
	LOSS [training: 6.422005375626503 | validation: 6.981437444849941]
	TIME [epoch: 6.35 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.1530341551439705		[learning rate: 0.002323]
	Learning Rate: 0.00232302
	LOSS [training: 7.1530341551439705 | validation: 6.6678694091578645]
	TIME [epoch: 6.38 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.570618766347598		[learning rate: 0.0023062]
	Learning Rate: 0.00230619
	LOSS [training: 6.570618766347598 | validation: 6.646562583354781]
	TIME [epoch: 6.37 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.989505808376934		[learning rate: 0.0022895]
	Learning Rate: 0.00228948
	LOSS [training: 6.989505808376934 | validation: 6.8071420297756475]
	TIME [epoch: 6.35 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.113153693346131		[learning rate: 0.0022729]
	Learning Rate: 0.00227289
	LOSS [training: 7.113153693346131 | validation: 6.733399942512951]
	TIME [epoch: 6.35 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.2261185526929115		[learning rate: 0.0022564]
	Learning Rate: 0.00225643
	LOSS [training: 7.2261185526929115 | validation: 7.175675455899485]
	TIME [epoch: 6.35 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 4/4] avg loss: 8.156269669381748		[learning rate: 0.0022401]
	Learning Rate: 0.00224008
	LOSS [training: 8.156269669381748 | validation: 7.368548432535334]
	TIME [epoch: 6.36 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.818778224199026		[learning rate: 0.0022238]
	Learning Rate: 0.00222385
	LOSS [training: 7.818778224199026 | validation: 7.10816899938327]
	TIME [epoch: 6.4 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 4/4] avg loss: 7.289976921247566		[learning rate: 0.0022077]
	Learning Rate: 0.00220774
	LOSS [training: 7.289976921247566 | validation: 7.017796215628419]
	TIME [epoch: 6.35 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.927088281021543		[learning rate: 0.0021917]
	Learning Rate: 0.00219174
	LOSS [training: 6.927088281021543 | validation: 6.707805137238646]
	TIME [epoch: 6.35 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.748485946915028		[learning rate: 0.0021759]
	Learning Rate: 0.00217586
	LOSS [training: 6.748485946915028 | validation: 6.671046824698681]
	TIME [epoch: 6.35 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.093599864810168		[learning rate: 0.0021601]
	Learning Rate: 0.0021601
	LOSS [training: 6.093599864810168 | validation: 6.197759891999398]
	TIME [epoch: 6.35 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.655903028886195		[learning rate: 0.0021444]
	Learning Rate: 0.00214445
	LOSS [training: 5.655903028886195 | validation: 5.969612807399553]
	TIME [epoch: 6.39 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.525427299818471		[learning rate: 0.0021289]
	Learning Rate: 0.00212891
	LOSS [training: 5.525427299818471 | validation: 5.835682878562254]
	TIME [epoch: 6.37 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.457048341971499		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 5.457048341971499 | validation: 5.409181512288229]
	TIME [epoch: 6.35 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.252904348275207		[learning rate: 0.0020982]
	Learning Rate: 0.00209818
	LOSS [training: 5.252904348275207 | validation: 5.320356064062533]
	TIME [epoch: 6.35 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.180818799750487		[learning rate: 0.002083]
	Learning Rate: 0.00208298
	LOSS [training: 5.180818799750487 | validation: 5.0877535219512655]
	TIME [epoch: 6.35 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.137402165624978		[learning rate: 0.0020679]
	Learning Rate: 0.00206788
	LOSS [training: 5.137402165624978 | validation: 5.172419831341902]
	TIME [epoch: 6.36 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.10569541161332		[learning rate: 0.0020529]
	Learning Rate: 0.0020529
	LOSS [training: 5.10569541161332 | validation: 5.325980867943286]
	TIME [epoch: 6.39 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.5937952147295835		[learning rate: 0.002038]
	Learning Rate: 0.00203803
	LOSS [training: 5.5937952147295835 | validation: 6.472878168071453]
	TIME [epoch: 6.35 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.17074610689574		[learning rate: 0.0020233]
	Learning Rate: 0.00202326
	LOSS [training: 6.17074610689574 | validation: 6.003020346536935]
	TIME [epoch: 6.35 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.786875204234555		[learning rate: 0.0020086]
	Learning Rate: 0.00200861
	LOSS [training: 5.786875204234555 | validation: 5.488826138992964]
	TIME [epoch: 6.35 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.451296200054409		[learning rate: 0.0019941]
	Learning Rate: 0.00199405
	LOSS [training: 5.451296200054409 | validation: 5.362597480089622]
	TIME [epoch: 6.35 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.169387078594144		[learning rate: 0.0019796]
	Learning Rate: 0.00197961
	LOSS [training: 5.169387078594144 | validation: 5.220222544442297]
	TIME [epoch: 6.39 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.146967495609461		[learning rate: 0.0019653]
	Learning Rate: 0.00196527
	LOSS [training: 5.146967495609461 | validation: 5.078358158267611]
	TIME [epoch: 6.35 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.043156045063353		[learning rate: 0.001951]
	Learning Rate: 0.00195103
	LOSS [training: 5.043156045063353 | validation: 5.181145964863472]
	TIME [epoch: 6.35 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.100341039804549		[learning rate: 0.0019369]
	Learning Rate: 0.00193689
	LOSS [training: 5.100341039804549 | validation: 5.074862166723181]
	TIME [epoch: 6.35 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.494047020920473		[learning rate: 0.0019229]
	Learning Rate: 0.00192286
	LOSS [training: 5.494047020920473 | validation: 6.342852117427552]
	TIME [epoch: 6.36 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.942860778496575		[learning rate: 0.0019089]
	Learning Rate: 0.00190893
	LOSS [training: 5.942860778496575 | validation: 5.50405578321055]
	TIME [epoch: 6.36 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.340766196536514		[learning rate: 0.0018951]
	Learning Rate: 0.0018951
	LOSS [training: 5.340766196536514 | validation: 5.009298328632589]
	TIME [epoch: 6.38 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.0401312968103085		[learning rate: 0.0018814]
	Learning Rate: 0.00188137
	LOSS [training: 5.0401312968103085 | validation: 4.964861411820159]
	TIME [epoch: 6.35 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.001431510568293		[learning rate: 0.0018677]
	Learning Rate: 0.00186774
	LOSS [training: 5.001431510568293 | validation: 5.145832595491616]
	TIME [epoch: 6.35 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.086353114084191		[learning rate: 0.0018542]
	Learning Rate: 0.00185421
	LOSS [training: 5.086353114084191 | validation: 5.139090858294117]
	TIME [epoch: 6.34 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.0004152353460025		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 5.0004152353460025 | validation: 5.152086093097018]
	TIME [epoch: 6.35 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.9829708452021855		[learning rate: 0.0018274]
	Learning Rate: 0.00182744
	LOSS [training: 4.9829708452021855 | validation: 5.0526129916675515]
	TIME [epoch: 6.39 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.9725752719237395		[learning rate: 0.0018142]
	Learning Rate: 0.0018142
	LOSS [training: 4.9725752719237395 | validation: 4.793748797245694]
	TIME [epoch: 6.36 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.9089528410289756		[learning rate: 0.0018011]
	Learning Rate: 0.00180105
	LOSS [training: 4.9089528410289756 | validation: 5.494204825639691]
	TIME [epoch: 6.35 sec]
EPOCH 287/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.365710350128768		[learning rate: 0.001788]
	Learning Rate: 0.001788
	LOSS [training: 5.365710350128768 | validation: 6.065190440492534]
	TIME [epoch: 6.35 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.441955886963676		[learning rate: 0.001775]
	Learning Rate: 0.00177505
	LOSS [training: 5.441955886963676 | validation: 4.7103674949170316]
	TIME [epoch: 6.34 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.8206003655739185		[learning rate: 0.0017622]
	Learning Rate: 0.00176219
	LOSS [training: 4.8206003655739185 | validation: 4.938794880880565]
	TIME [epoch: 6.36 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.857713117839214		[learning rate: 0.0017494]
	Learning Rate: 0.00174942
	LOSS [training: 4.857713117839214 | validation: 4.697007455235877]
	TIME [epoch: 6.38 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.865651464322411		[learning rate: 0.0017367]
	Learning Rate: 0.00173675
	LOSS [training: 4.865651464322411 | validation: 4.840280254262331]
	TIME [epoch: 6.35 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.910511653075431		[learning rate: 0.0017242]
	Learning Rate: 0.00172417
	LOSS [training: 4.910511653075431 | validation: 5.89346292677053]
	TIME [epoch: 6.35 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.667277641924795		[learning rate: 0.0017117]
	Learning Rate: 0.00171167
	LOSS [training: 5.667277641924795 | validation: 6.147418058545039]
	TIME [epoch: 6.35 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.504506877804443		[learning rate: 0.0016993]
	Learning Rate: 0.00169927
	LOSS [training: 5.504506877804443 | validation: 6.143900960994834]
	TIME [epoch: 6.35 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.584686199117398		[learning rate: 0.001687]
	Learning Rate: 0.00168696
	LOSS [training: 5.584686199117398 | validation: 5.779258533794491]
	TIME [epoch: 6.39 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.023904075088426		[learning rate: 0.0016747]
	Learning Rate: 0.00167474
	LOSS [training: 5.023904075088426 | validation: 4.730008590071694]
	TIME [epoch: 6.36 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.758486155833769		[learning rate: 0.0016626]
	Learning Rate: 0.00166261
	LOSS [training: 4.758486155833769 | validation: 4.979121009025244]
	TIME [epoch: 6.35 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.03552705242093		[learning rate: 0.0016506]
	Learning Rate: 0.00165056
	LOSS [training: 5.03552705242093 | validation: 5.009176457867548]
	TIME [epoch: 6.35 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.9970632062946425		[learning rate: 0.0016386]
	Learning Rate: 0.0016386
	LOSS [training: 4.9970632062946425 | validation: 4.720162660607853]
	TIME [epoch: 6.35 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.708641143888678		[learning rate: 0.0016267]
	Learning Rate: 0.00162673
	LOSS [training: 4.708641143888678 | validation: 4.5099672037986895]
	TIME [epoch: 6.38 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_300.pth
	Model improved!!!
EPOCH 301/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.730501519402956		[learning rate: 0.0016149]
	Learning Rate: 0.00161495
	LOSS [training: 4.730501519402956 | validation: 5.495901269862493]
	TIME [epoch: 6.37 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.109209831191497		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 5.109209831191497 | validation: 5.770945274987454]
	TIME [epoch: 6.35 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.832636039234659		[learning rate: 0.0015916]
	Learning Rate: 0.00159163
	LOSS [training: 4.832636039234659 | validation: 4.64936229945879]
	TIME [epoch: 6.35 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.216423974336262		[learning rate: 0.0015801]
	Learning Rate: 0.0015801
	LOSS [training: 5.216423974336262 | validation: 6.035965957404654]
	TIME [epoch: 6.35 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.480242232832417		[learning rate: 0.0015687]
	Learning Rate: 0.00156865
	LOSS [training: 5.480242232832417 | validation: 5.151612343072673]
	TIME [epoch: 6.37 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.796900515736182		[learning rate: 0.0015573]
	Learning Rate: 0.00155729
	LOSS [training: 4.796900515736182 | validation: 4.845354553591985]
	TIME [epoch: 6.39 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.780550804585303		[learning rate: 0.001546]
	Learning Rate: 0.001546
	LOSS [training: 4.780550804585303 | validation: 4.937160550445158]
	TIME [epoch: 6.36 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.8025424826475795		[learning rate: 0.0015348]
	Learning Rate: 0.0015348
	LOSS [training: 4.8025424826475795 | validation: 4.387161864616617]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_308.pth
	Model improved!!!
EPOCH 309/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.561026879249189		[learning rate: 0.0015237]
	Learning Rate: 0.00152368
	LOSS [training: 4.561026879249189 | validation: 4.620778452343189]
	TIME [epoch: 6.35 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.56783834116932		[learning rate: 0.0015126]
	Learning Rate: 0.00151264
	LOSS [training: 4.56783834116932 | validation: 4.395350498653265]
	TIME [epoch: 6.35 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.507513773001241		[learning rate: 0.0015017]
	Learning Rate: 0.00150169
	LOSS [training: 4.507513773001241 | validation: 4.43942787444955]
	TIME [epoch: 6.4 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.523128380141914		[learning rate: 0.0014908]
	Learning Rate: 0.00149081
	LOSS [training: 4.523128380141914 | validation: 4.276683981533231]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_312.pth
	Model improved!!!
EPOCH 313/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.431591626987936		[learning rate: 0.00148]
	Learning Rate: 0.00148001
	LOSS [training: 4.431591626987936 | validation: 4.228157399921278]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_313.pth
	Model improved!!!
EPOCH 314/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.424908850821698		[learning rate: 0.0014693]
	Learning Rate: 0.00146928
	LOSS [training: 4.424908850821698 | validation: 4.349851096230989]
	TIME [epoch: 6.36 sec]
EPOCH 315/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.427383329346769		[learning rate: 0.0014586]
	Learning Rate: 0.00145864
	LOSS [training: 4.427383329346769 | validation: 4.281808691203434]
	TIME [epoch: 6.35 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.601495745793281		[learning rate: 0.0014481]
	Learning Rate: 0.00144807
	LOSS [training: 4.601495745793281 | validation: 4.2998296498545585]
	TIME [epoch: 6.39 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.453398321083818		[learning rate: 0.0014376]
	Learning Rate: 0.00143758
	LOSS [training: 4.453398321083818 | validation: 4.905442134661136]
	TIME [epoch: 6.38 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.489110836038758		[learning rate: 0.0014272]
	Learning Rate: 0.00142716
	LOSS [training: 4.489110836038758 | validation: 4.204329767121666]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_318.pth
	Model improved!!!
EPOCH 319/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.356825916379538		[learning rate: 0.0014168]
	Learning Rate: 0.00141682
	LOSS [training: 4.356825916379538 | validation: 4.115561584951921]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_319.pth
	Model improved!!!
EPOCH 320/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.277888038803357		[learning rate: 0.0014066]
	Learning Rate: 0.00140656
	LOSS [training: 4.277888038803357 | validation: 4.13412586352994]
	TIME [epoch: 6.36 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.297218710359999		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 4.297218710359999 | validation: 4.163227416515832]
	TIME [epoch: 6.37 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.330518779337856		[learning rate: 0.0013863]
	Learning Rate: 0.00138625
	LOSS [training: 4.330518779337856 | validation: 4.488098203118637]
	TIME [epoch: 6.38 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.396218612509487		[learning rate: 0.0013762]
	Learning Rate: 0.00137621
	LOSS [training: 4.396218612509487 | validation: 4.1964388383929485]
	TIME [epoch: 6.35 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.269357588150281		[learning rate: 0.0013662]
	Learning Rate: 0.00136624
	LOSS [training: 4.269357588150281 | validation: 4.091928297809715]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_324.pth
	Model improved!!!
EPOCH 325/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.289083003139121		[learning rate: 0.0013563]
	Learning Rate: 0.00135634
	LOSS [training: 4.289083003139121 | validation: 4.648828082712306]
	TIME [epoch: 6.36 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.361135893051442		[learning rate: 0.0013465]
	Learning Rate: 0.00134651
	LOSS [training: 4.361135893051442 | validation: 4.088507347457926]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_326.pth
	Model improved!!!
EPOCH 327/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.403644811304684		[learning rate: 0.0013368]
	Learning Rate: 0.00133676
	LOSS [training: 4.403644811304684 | validation: 4.694419392969061]
	TIME [epoch: 6.4 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.382902205230345		[learning rate: 0.0013271]
	Learning Rate: 0.00132707
	LOSS [training: 4.382902205230345 | validation: 4.53814737440824]
	TIME [epoch: 6.36 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.282416252975951		[learning rate: 0.0013175]
	Learning Rate: 0.00131746
	LOSS [training: 4.282416252975951 | validation: 4.070910735013043]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_329.pth
	Model improved!!!
EPOCH 330/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.1908792353713284		[learning rate: 0.0013079]
	Learning Rate: 0.00130791
	LOSS [training: 4.1908792353713284 | validation: 4.212549206704451]
	TIME [epoch: 6.35 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.186673166633926		[learning rate: 0.0012984]
	Learning Rate: 0.00129844
	LOSS [training: 4.186673166633926 | validation: 4.363271463973584]
	TIME [epoch: 6.36 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.324426937519028		[learning rate: 0.001289]
	Learning Rate: 0.00128903
	LOSS [training: 4.324426937519028 | validation: 3.8809161336370117]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_332.pth
	Model improved!!!
EPOCH 333/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.400721342582724		[learning rate: 0.0012797]
	Learning Rate: 0.00127969
	LOSS [training: 4.400721342582724 | validation: 4.522228614584]
	TIME [epoch: 6.37 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.22925620983072		[learning rate: 0.0012704]
	Learning Rate: 0.00127042
	LOSS [training: 4.22925620983072 | validation: 4.065303076012354]
	TIME [epoch: 6.36 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.1269571498049284		[learning rate: 0.0012612]
	Learning Rate: 0.00126122
	LOSS [training: 4.1269571498049284 | validation: 3.9101558606631053]
	TIME [epoch: 6.36 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.137290260703072		[learning rate: 0.0012521]
	Learning Rate: 0.00125208
	LOSS [training: 4.137290260703072 | validation: 3.8947446582294765]
	TIME [epoch: 6.35 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.073437998415939		[learning rate: 0.001243]
	Learning Rate: 0.00124301
	LOSS [training: 4.073437998415939 | validation: 3.9971819926115093]
	TIME [epoch: 6.38 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.194548171690361		[learning rate: 0.001234]
	Learning Rate: 0.001234
	LOSS [training: 4.194548171690361 | validation: 4.08729125541154]
	TIME [epoch: 6.38 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.483677587418605		[learning rate: 0.0012251]
	Learning Rate: 0.00122506
	LOSS [training: 4.483677587418605 | validation: 4.828546291556444]
	TIME [epoch: 6.36 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.478641463345197		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 4.478641463345197 | validation: 3.799319359037173]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_340.pth
	Model improved!!!
EPOCH 341/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.069842991791119		[learning rate: 0.0012074]
	Learning Rate: 0.00120737
	LOSS [training: 4.069842991791119 | validation: 3.8570278938739144]
	TIME [epoch: 6.36 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.018175863193752		[learning rate: 0.0011986]
	Learning Rate: 0.00119863
	LOSS [training: 4.018175863193752 | validation: 3.8666783354961343]
	TIME [epoch: 6.37 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.09950607207533		[learning rate: 0.0011899]
	Learning Rate: 0.00118994
	LOSS [training: 4.09950607207533 | validation: 4.333836852702293]
	TIME [epoch: 6.41 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.122243871083944		[learning rate: 0.0011813]
	Learning Rate: 0.00118132
	LOSS [training: 4.122243871083944 | validation: 4.438170618240596]
	TIME [epoch: 6.36 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.310944290523297		[learning rate: 0.0011728]
	Learning Rate: 0.00117276
	LOSS [training: 4.310944290523297 | validation: 4.339457508947306]
	TIME [epoch: 6.36 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.265959826382243		[learning rate: 0.0011643]
	Learning Rate: 0.00116427
	LOSS [training: 4.265959826382243 | validation: 4.086506321633793]
	TIME [epoch: 6.36 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.0480792191338075		[learning rate: 0.0011558]
	Learning Rate: 0.00115583
	LOSS [training: 4.0480792191338075 | validation: 3.992469396848506]
	TIME [epoch: 6.36 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.992097528357439		[learning rate: 0.0011475]
	Learning Rate: 0.00114746
	LOSS [training: 3.992097528357439 | validation: 3.6729211825736208]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_348.pth
	Model improved!!!
EPOCH 349/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.222142054596107		[learning rate: 0.0011391]
	Learning Rate: 0.00113914
	LOSS [training: 4.222142054596107 | validation: 4.122698030178011]
	TIME [epoch: 6.36 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.025994232645442		[learning rate: 0.0011309]
	Learning Rate: 0.00113089
	LOSS [training: 4.025994232645442 | validation: 3.70470316984881]
	TIME [epoch: 6.35 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9173450282584747		[learning rate: 0.0011227]
	Learning Rate: 0.0011227
	LOSS [training: 3.9173450282584747 | validation: 3.7913825930161327]
	TIME [epoch: 6.36 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8900232809816813		[learning rate: 0.0011146]
	Learning Rate: 0.00111456
	LOSS [training: 3.8900232809816813 | validation: 3.787243761660694]
	TIME [epoch: 6.36 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9601910511383274		[learning rate: 0.0011065]
	Learning Rate: 0.00110649
	LOSS [training: 3.9601910511383274 | validation: 3.660274544708428]
	TIME [epoch: 6.37 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_353.pth
	Model improved!!!
EPOCH 354/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9246515254615755		[learning rate: 0.0010985]
	Learning Rate: 0.00109847
	LOSS [training: 3.9246515254615755 | validation: 3.8639758874640413]
	TIME [epoch: 6.39 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8508136359296827		[learning rate: 0.0010905]
	Learning Rate: 0.00109051
	LOSS [training: 3.8508136359296827 | validation: 3.8511910536701386]
	TIME [epoch: 6.36 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8050972052065237		[learning rate: 0.0010826]
	Learning Rate: 0.00108261
	LOSS [training: 3.8050972052065237 | validation: 3.9042000470876017]
	TIME [epoch: 6.36 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.79148366054795		[learning rate: 0.0010748]
	Learning Rate: 0.00107477
	LOSS [training: 3.79148366054795 | validation: 3.5136094383714616]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_357.pth
	Model improved!!!
EPOCH 358/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.764193674725077		[learning rate: 0.001067]
	Learning Rate: 0.00106698
	LOSS [training: 3.764193674725077 | validation: 3.6344384570984287]
	TIME [epoch: 6.36 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7339533953210466		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 3.7339533953210466 | validation: 3.4174099000330083]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_359.pth
	Model improved!!!
EPOCH 360/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.744663968688327		[learning rate: 0.0010516]
	Learning Rate: 0.00105158
	LOSS [training: 3.744663968688327 | validation: 3.7818224720908615]
	TIME [epoch: 6.35 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7165158049208284		[learning rate: 0.001044]
	Learning Rate: 0.00104396
	LOSS [training: 3.7165158049208284 | validation: 3.4902264550636204]
	TIME [epoch: 6.35 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.665867602411476		[learning rate: 0.0010364]
	Learning Rate: 0.0010364
	LOSS [training: 3.665867602411476 | validation: 3.7766905893699363]
	TIME [epoch: 6.35 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.871834014075339		[learning rate: 0.0010289]
	Learning Rate: 0.00102889
	LOSS [training: 3.871834014075339 | validation: 3.4609747206131924]
	TIME [epoch: 6.36 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.723938871718764		[learning rate: 0.0010214]
	Learning Rate: 0.00102143
	LOSS [training: 3.723938871718764 | validation: 3.462180840183736]
	TIME [epoch: 6.4 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6867502843296904		[learning rate: 0.001014]
	Learning Rate: 0.00101403
	LOSS [training: 3.6867502843296904 | validation: 3.561714982069903]
	TIME [epoch: 6.36 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.782792954043253		[learning rate: 0.0010067]
	Learning Rate: 0.00100669
	LOSS [training: 3.782792954043253 | validation: 3.5472416672578504]
	TIME [epoch: 6.36 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6484224589801957		[learning rate: 0.00099939]
	Learning Rate: 0.000999394
	LOSS [training: 3.6484224589801957 | validation: 3.438797935419741]
	TIME [epoch: 6.36 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.729605844924544		[learning rate: 0.00099215]
	Learning Rate: 0.000992154
	LOSS [training: 3.729605844924544 | validation: 3.486787429170761]
	TIME [epoch: 6.35 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6563730279578537		[learning rate: 0.00098497]
	Learning Rate: 0.000984966
	LOSS [training: 3.6563730279578537 | validation: 3.8033186274697766]
	TIME [epoch: 6.38 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6866741667298752		[learning rate: 0.00097783]
	Learning Rate: 0.00097783
	LOSS [training: 3.6866741667298752 | validation: 3.4038940369287585]
	TIME [epoch: 6.39 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_370.pth
	Model improved!!!
EPOCH 371/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.571927214273998		[learning rate: 0.00097075]
	Learning Rate: 0.000970745
	LOSS [training: 3.571927214273998 | validation: 3.4448302409881935]
	TIME [epoch: 6.35 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5834698467926818		[learning rate: 0.00096371]
	Learning Rate: 0.000963712
	LOSS [training: 3.5834698467926818 | validation: 3.958972565351091]
	TIME [epoch: 6.34 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.781406517621041		[learning rate: 0.00095673]
	Learning Rate: 0.00095673
	LOSS [training: 3.781406517621041 | validation: 3.8696619411960844]
	TIME [epoch: 6.35 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8219545110910267		[learning rate: 0.0009498]
	Learning Rate: 0.000949799
	LOSS [training: 3.8219545110910267 | validation: 3.7625584054149193]
	TIME [epoch: 6.35 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6234885433213133		[learning rate: 0.00094292]
	Learning Rate: 0.000942918
	LOSS [training: 3.6234885433213133 | validation: 4.0743190085935215]
	TIME [epoch: 6.41 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7130937154728842		[learning rate: 0.00093609]
	Learning Rate: 0.000936086
	LOSS [training: 3.7130937154728842 | validation: 4.47470965886723]
	TIME [epoch: 6.36 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.94549185707495		[learning rate: 0.0009293]
	Learning Rate: 0.000929304
	LOSS [training: 3.94549185707495 | validation: 3.3820796006236042]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_377.pth
	Model improved!!!
EPOCH 378/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.524358770794296		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 3.524358770794296 | validation: 3.5817876362755037]
	TIME [epoch: 6.35 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.52648039185908		[learning rate: 0.00091589]
	Learning Rate: 0.000915888
	LOSS [training: 3.52648039185908 | validation: 3.3552352413427826]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_379.pth
	Model improved!!!
EPOCH 380/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5036061045351863		[learning rate: 0.00090925]
	Learning Rate: 0.000909252
	LOSS [training: 3.5036061045351863 | validation: 3.516058938759599]
	TIME [epoch: 6.4 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5487138614045413		[learning rate: 0.00090266]
	Learning Rate: 0.000902664
	LOSS [training: 3.5487138614045413 | validation: 3.360577056093914]
	TIME [epoch: 6.36 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5722842313967282		[learning rate: 0.00089612]
	Learning Rate: 0.000896125
	LOSS [training: 3.5722842313967282 | validation: 3.3737986057939446]
	TIME [epoch: 6.36 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5349909878954167		[learning rate: 0.00088963]
	Learning Rate: 0.000889632
	LOSS [training: 3.5349909878954167 | validation: 3.3677532970099784]
	TIME [epoch: 6.36 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.639214665334639		[learning rate: 0.00088319]
	Learning Rate: 0.000883187
	LOSS [training: 3.639214665334639 | validation: 3.690979453799781]
	TIME [epoch: 6.36 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6184849976231592		[learning rate: 0.00087679]
	Learning Rate: 0.000876788
	LOSS [training: 3.6184849976231592 | validation: 3.994975793830381]
	TIME [epoch: 6.38 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.561856186713485		[learning rate: 0.00087044]
	Learning Rate: 0.000870436
	LOSS [training: 3.561856186713485 | validation: 3.5658635713490225]
	TIME [epoch: 6.39 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.540094546637823		[learning rate: 0.00086413]
	Learning Rate: 0.00086413
	LOSS [training: 3.540094546637823 | validation: 3.738507676722543]
	TIME [epoch: 6.36 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4972412720221224		[learning rate: 0.00085787]
	Learning Rate: 0.000857869
	LOSS [training: 3.4972412720221224 | validation: 3.558576726038564]
	TIME [epoch: 6.35 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.469315129898021		[learning rate: 0.00085165]
	Learning Rate: 0.000851654
	LOSS [training: 3.469315129898021 | validation: 3.2097748900126115]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_389.pth
	Model improved!!!
EPOCH 390/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.418647929852788		[learning rate: 0.00084548]
	Learning Rate: 0.000845484
	LOSS [training: 3.418647929852788 | validation: 3.82759998283713]
	TIME [epoch: 6.35 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.537066618606813		[learning rate: 0.00083936]
	Learning Rate: 0.000839358
	LOSS [training: 3.537066618606813 | validation: 3.3413541283967865]
	TIME [epoch: 6.4 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.565889993468379		[learning rate: 0.00083328]
	Learning Rate: 0.000833277
	LOSS [training: 3.565889993468379 | validation: 3.5485291201417626]
	TIME [epoch: 6.36 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4781083416018235		[learning rate: 0.00082724]
	Learning Rate: 0.00082724
	LOSS [training: 3.4781083416018235 | validation: 3.771173318704638]
	TIME [epoch: 6.35 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5198041814520953		[learning rate: 0.00082125]
	Learning Rate: 0.000821247
	LOSS [training: 3.5198041814520953 | validation: 3.535354734674239]
	TIME [epoch: 6.35 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4542178316862993		[learning rate: 0.0008153]
	Learning Rate: 0.000815297
	LOSS [training: 3.4542178316862993 | validation: 3.278808491997278]
	TIME [epoch: 6.35 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3901187830434294		[learning rate: 0.00080939]
	Learning Rate: 0.00080939
	LOSS [training: 3.3901187830434294 | validation: 3.60994195270816]
	TIME [epoch: 6.39 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4904440143150843		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 3.4904440143150843 | validation: 3.404000926111016]
	TIME [epoch: 6.37 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4585278920458085		[learning rate: 0.0007977]
	Learning Rate: 0.000797705
	LOSS [training: 3.4585278920458085 | validation: 3.3811655525030515]
	TIME [epoch: 6.35 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.375468945800421		[learning rate: 0.00079193]
	Learning Rate: 0.000791925
	LOSS [training: 3.375468945800421 | validation: 3.4524308487389668]
	TIME [epoch: 6.35 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.052236225378518		[learning rate: 0.00078619]
	Learning Rate: 0.000786188
	LOSS [training: 4.052236225378518 | validation: 4.127267083721198]
	TIME [epoch: 6.35 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.7012795019991636		[learning rate: 0.00078049]
	Learning Rate: 0.000780492
	LOSS [training: 3.7012795019991636 | validation: 3.168306965612876]
	TIME [epoch: 6.37 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_401.pth
	Model improved!!!
EPOCH 402/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.402452326885223		[learning rate: 0.00077484]
	Learning Rate: 0.000774838
	LOSS [training: 3.402452326885223 | validation: 3.5459593749581027]
	TIME [epoch: 6.38 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4554151097160184		[learning rate: 0.00076922]
	Learning Rate: 0.000769224
	LOSS [training: 3.4554151097160184 | validation: 3.5949425187929265]
	TIME [epoch: 6.35 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4361974738487473		[learning rate: 0.00076365]
	Learning Rate: 0.000763651
	LOSS [training: 3.4361974738487473 | validation: 3.4976591940611756]
	TIME [epoch: 6.35 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3512952143704284		[learning rate: 0.00075812]
	Learning Rate: 0.000758118
	LOSS [training: 3.3512952143704284 | validation: 3.832940366722717]
	TIME [epoch: 6.35 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.504260000277495		[learning rate: 0.00075263]
	Learning Rate: 0.000752626
	LOSS [training: 3.504260000277495 | validation: 3.3054282946286637]
	TIME [epoch: 6.35 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3811162855638495		[learning rate: 0.00074717]
	Learning Rate: 0.000747173
	LOSS [training: 3.3811162855638495 | validation: 3.5550106584600507]
	TIME [epoch: 6.4 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.401214726963281		[learning rate: 0.00074176]
	Learning Rate: 0.00074176
	LOSS [training: 3.401214726963281 | validation: 3.5126965869600655]
	TIME [epoch: 6.36 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3428028886694467		[learning rate: 0.00073639]
	Learning Rate: 0.000736386
	LOSS [training: 3.3428028886694467 | validation: 3.331034550165942]
	TIME [epoch: 6.35 sec]
EPOCH 410/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4217449296155924		[learning rate: 0.00073105]
	Learning Rate: 0.000731051
	LOSS [training: 3.4217449296155924 | validation: 3.257082918554933]
	TIME [epoch: 6.35 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.383919542709639		[learning rate: 0.00072575]
	Learning Rate: 0.000725754
	LOSS [training: 3.383919542709639 | validation: 3.4364565192102834]
	TIME [epoch: 6.35 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.474953265387376		[learning rate: 0.0007205]
	Learning Rate: 0.000720496
	LOSS [training: 3.474953265387376 | validation: 3.314913829992715]
	TIME [epoch: 6.38 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.433187666908517		[learning rate: 0.00071528]
	Learning Rate: 0.000715276
	LOSS [training: 3.433187666908517 | validation: 3.1798888651738113]
	TIME [epoch: 6.36 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2851399997916273		[learning rate: 0.00071009]
	Learning Rate: 0.000710094
	LOSS [training: 3.2851399997916273 | validation: 3.276727056435954]
	TIME [epoch: 6.35 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.345485090176521		[learning rate: 0.00070495]
	Learning Rate: 0.000704949
	LOSS [training: 3.345485090176521 | validation: 3.2746261442049995]
	TIME [epoch: 6.35 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.5975424375517253		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 3.5975424375517253 | validation: 3.2506524380532213]
	TIME [epoch: 6.35 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.333238197800431		[learning rate: 0.00069477]
	Learning Rate: 0.000694772
	LOSS [training: 3.333238197800431 | validation: 3.7612670329618156]
	TIME [epoch: 6.35 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3785932904279776		[learning rate: 0.00068974]
	Learning Rate: 0.000689738
	LOSS [training: 3.3785932904279776 | validation: 3.4782751369933953]
	TIME [epoch: 6.39 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3185516751751307		[learning rate: 0.00068474]
	Learning Rate: 0.000684741
	LOSS [training: 3.3185516751751307 | validation: 3.288864200231825]
	TIME [epoch: 6.35 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.278839295964498		[learning rate: 0.00067978]
	Learning Rate: 0.00067978
	LOSS [training: 3.278839295964498 | validation: 3.1275853148093695]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_420.pth
	Model improved!!!
EPOCH 421/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.298362435237384		[learning rate: 0.00067486]
	Learning Rate: 0.000674855
	LOSS [training: 3.298362435237384 | validation: 3.3227546087258095]
	TIME [epoch: 6.35 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2947032988380314		[learning rate: 0.00066997]
	Learning Rate: 0.000669966
	LOSS [training: 3.2947032988380314 | validation: 3.1553955778904665]
	TIME [epoch: 6.35 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2269998805908515		[learning rate: 0.00066511]
	Learning Rate: 0.000665112
	LOSS [training: 3.2269998805908515 | validation: 3.643627958248858]
	TIME [epoch: 6.4 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.282948000468407		[learning rate: 0.00066029]
	Learning Rate: 0.000660293
	LOSS [training: 3.282948000468407 | validation: 3.489840670098326]
	TIME [epoch: 6.36 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4857647951481776		[learning rate: 0.00065551]
	Learning Rate: 0.00065551
	LOSS [training: 3.4857647951481776 | validation: 3.371799729297046]
	TIME [epoch: 6.35 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.233300242672679		[learning rate: 0.00065076]
	Learning Rate: 0.00065076
	LOSS [training: 3.233300242672679 | validation: 3.208018376409415]
	TIME [epoch: 6.35 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3286987737514275		[learning rate: 0.00064605]
	Learning Rate: 0.000646046
	LOSS [training: 3.3286987737514275 | validation: 3.153995335561533]
	TIME [epoch: 6.35 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2716076321156384		[learning rate: 0.00064137]
	Learning Rate: 0.000641365
	LOSS [training: 3.2716076321156384 | validation: 3.1476168844400556]
	TIME [epoch: 6.36 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3523045135628453		[learning rate: 0.00063672]
	Learning Rate: 0.000636718
	LOSS [training: 3.3523045135628453 | validation: 3.0774508542049164]
	TIME [epoch: 6.38 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_429.pth
	Model improved!!!
EPOCH 430/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2897311811702643		[learning rate: 0.00063211]
	Learning Rate: 0.000632105
	LOSS [training: 3.2897311811702643 | validation: 3.4056801864059243]
	TIME [epoch: 6.36 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.270299436614914		[learning rate: 0.00062753]
	Learning Rate: 0.000627526
	LOSS [training: 3.270299436614914 | validation: 3.3799112253850487]
	TIME [epoch: 6.36 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.235732570270314		[learning rate: 0.00062298]
	Learning Rate: 0.000622979
	LOSS [training: 3.235732570270314 | validation: 3.1258128409188206]
	TIME [epoch: 6.35 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1715601261743016		[learning rate: 0.00061847]
	Learning Rate: 0.000618466
	LOSS [training: 3.1715601261743016 | validation: 3.2473911299935954]
	TIME [epoch: 6.35 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.255180719874553		[learning rate: 0.00061399]
	Learning Rate: 0.000613985
	LOSS [training: 3.255180719874553 | validation: 3.2070014419911335]
	TIME [epoch: 6.4 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2295313050776127		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 3.2295313050776127 | validation: 3.2206907218032432]
	TIME [epoch: 6.35 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.319411903099133		[learning rate: 0.00060512]
	Learning Rate: 0.000605121
	LOSS [training: 3.319411903099133 | validation: 3.1884991845866297]
	TIME [epoch: 6.35 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.153451047147702		[learning rate: 0.00060074]
	Learning Rate: 0.000600737
	LOSS [training: 3.153451047147702 | validation: 3.8785525742811604]
	TIME [epoch: 6.35 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2901169136571258		[learning rate: 0.00059638]
	Learning Rate: 0.000596384
	LOSS [training: 3.2901169136571258 | validation: 3.334067751886917]
	TIME [epoch: 6.35 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1775839903020806		[learning rate: 0.00059206]
	Learning Rate: 0.000592064
	LOSS [training: 3.1775839903020806 | validation: 3.1181089748189317]
	TIME [epoch: 6.38 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2058691218120083		[learning rate: 0.00058777]
	Learning Rate: 0.000587774
	LOSS [training: 3.2058691218120083 | validation: 3.239697778171002]
	TIME [epoch: 6.37 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1706554978929185		[learning rate: 0.00058352]
	Learning Rate: 0.000583516
	LOSS [training: 3.1706554978929185 | validation: 3.4526521420931626]
	TIME [epoch: 6.35 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3990780904940903		[learning rate: 0.00057929]
	Learning Rate: 0.000579288
	LOSS [training: 3.3990780904940903 | validation: 3.127685896642591]
	TIME [epoch: 6.35 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.112848568929612		[learning rate: 0.00057509]
	Learning Rate: 0.000575091
	LOSS [training: 3.112848568929612 | validation: 3.072028498796012]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_443.pth
	Model improved!!!
EPOCH 444/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1052613659481842		[learning rate: 0.00057093]
	Learning Rate: 0.000570925
	LOSS [training: 3.1052613659481842 | validation: 3.19456639108932]
	TIME [epoch: 6.35 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.116349689078013		[learning rate: 0.00056679]
	Learning Rate: 0.000566789
	LOSS [training: 3.116349689078013 | validation: 3.055874419560217]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_445.pth
	Model improved!!!
EPOCH 446/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1242370315460706		[learning rate: 0.00056268]
	Learning Rate: 0.000562682
	LOSS [training: 3.1242370315460706 | validation: 3.08613680684168]
	TIME [epoch: 6.35 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1453767859305906		[learning rate: 0.00055861]
	Learning Rate: 0.000558606
	LOSS [training: 3.1453767859305906 | validation: 3.3024367528715297]
	TIME [epoch: 6.35 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.154956447644822		[learning rate: 0.00055456]
	Learning Rate: 0.000554559
	LOSS [training: 3.154956447644822 | validation: 3.0636281724598113]
	TIME [epoch: 6.35 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1116728212146367		[learning rate: 0.00055054]
	Learning Rate: 0.000550541
	LOSS [training: 3.1116728212146367 | validation: 3.233964651586923]
	TIME [epoch: 6.35 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1518942317095613		[learning rate: 0.00054655]
	Learning Rate: 0.000546552
	LOSS [training: 3.1518942317095613 | validation: 3.245595294170143]
	TIME [epoch: 6.39 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1896705357273873		[learning rate: 0.00054259]
	Learning Rate: 0.000542592
	LOSS [training: 3.1896705357273873 | validation: 3.091310694726144]
	TIME [epoch: 6.36 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.124426535132437		[learning rate: 0.00053866]
	Learning Rate: 0.000538661
	LOSS [training: 3.124426535132437 | validation: 3.122753779095578]
	TIME [epoch: 6.35 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2588691330321256		[learning rate: 0.00053476]
	Learning Rate: 0.000534759
	LOSS [training: 3.2588691330321256 | validation: 3.3165584226067164]
	TIME [epoch: 6.35 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.133954660840839		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 3.133954660840839 | validation: 3.3565332681349553]
	TIME [epoch: 6.35 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1018683024960856		[learning rate: 0.00052704]
	Learning Rate: 0.000527038
	LOSS [training: 3.1018683024960856 | validation: 3.539355877735655]
	TIME [epoch: 6.36 sec]
EPOCH 456/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3263836697333247		[learning rate: 0.00052322]
	Learning Rate: 0.00052322
	LOSS [training: 3.3263836697333247 | validation: 3.6282218032573477]
	TIME [epoch: 6.38 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.217603637864726		[learning rate: 0.00051943]
	Learning Rate: 0.000519429
	LOSS [training: 3.217603637864726 | validation: 3.214462386419175]
	TIME [epoch: 6.35 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.142191499925235		[learning rate: 0.00051567]
	Learning Rate: 0.000515666
	LOSS [training: 3.142191499925235 | validation: 3.0577858212453384]
	TIME [epoch: 6.35 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.088565196362116		[learning rate: 0.00051193]
	Learning Rate: 0.00051193
	LOSS [training: 3.088565196362116 | validation: 3.3562101976740957]
	TIME [epoch: 6.34 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0943795455517034		[learning rate: 0.00050822]
	Learning Rate: 0.000508221
	LOSS [training: 3.0943795455517034 | validation: 3.179822716799479]
	TIME [epoch: 6.35 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0439964398187223		[learning rate: 0.00050454]
	Learning Rate: 0.000504539
	LOSS [training: 3.0439964398187223 | validation: 3.3733712466294072]
	TIME [epoch: 6.39 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.12184439605545		[learning rate: 0.00050088]
	Learning Rate: 0.000500884
	LOSS [training: 3.12184439605545 | validation: 3.022950306400513]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_462.pth
	Model improved!!!
EPOCH 463/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1245418318674743		[learning rate: 0.00049725]
	Learning Rate: 0.000497255
	LOSS [training: 3.1245418318674743 | validation: 3.0478229624652027]
	TIME [epoch: 6.35 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0567770267921164		[learning rate: 0.00049365]
	Learning Rate: 0.000493652
	LOSS [training: 3.0567770267921164 | validation: 3.0842347226117908]
	TIME [epoch: 6.35 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0725739071482367		[learning rate: 0.00049008]
	Learning Rate: 0.000490076
	LOSS [training: 3.0725739071482367 | validation: 3.5012709909188167]
	TIME [epoch: 6.35 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1175567147701204		[learning rate: 0.00048653]
	Learning Rate: 0.000486525
	LOSS [training: 3.1175567147701204 | validation: 2.980118990157167]
	TIME [epoch: 6.38 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_466.pth
	Model improved!!!
EPOCH 467/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.05437822130565		[learning rate: 0.000483]
	Learning Rate: 0.000483
	LOSS [training: 3.05437822130565 | validation: 3.0522543479617354]
	TIME [epoch: 6.37 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.074309541439382		[learning rate: 0.0004795]
	Learning Rate: 0.000479501
	LOSS [training: 3.074309541439382 | validation: 3.4251323236977482]
	TIME [epoch: 6.35 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.090639940586926		[learning rate: 0.00047603]
	Learning Rate: 0.000476027
	LOSS [training: 3.090639940586926 | validation: 3.0348728453551503]
	TIME [epoch: 6.35 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.025945637222435		[learning rate: 0.00047258]
	Learning Rate: 0.000472578
	LOSS [training: 3.025945637222435 | validation: 3.238466635233115]
	TIME [epoch: 6.35 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0356881709664005		[learning rate: 0.00046915]
	Learning Rate: 0.000469154
	LOSS [training: 3.0356881709664005 | validation: 3.8531704162226648]
	TIME [epoch: 6.36 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2362401415956636		[learning rate: 0.00046576]
	Learning Rate: 0.000465755
	LOSS [training: 3.2362401415956636 | validation: 3.0259181973342084]
	TIME [epoch: 6.38 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1065342531877636		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 3.1065342531877636 | validation: 2.949992894413171]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_473.pth
	Model improved!!!
EPOCH 474/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.108736400236753		[learning rate: 0.00045903]
	Learning Rate: 0.000459031
	LOSS [training: 3.108736400236753 | validation: 3.2711468921597717]
	TIME [epoch: 6.35 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1173570460591424		[learning rate: 0.00045571]
	Learning Rate: 0.000455706
	LOSS [training: 3.1173570460591424 | validation: 2.9569111608563152]
	TIME [epoch: 6.35 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.988071504519244		[learning rate: 0.0004524]
	Learning Rate: 0.000452404
	LOSS [training: 2.988071504519244 | validation: 2.9096557962859366]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_476.pth
	Model improved!!!
EPOCH 477/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0272706118041723		[learning rate: 0.00044913]
	Learning Rate: 0.000449126
	LOSS [training: 3.0272706118041723 | validation: 3.125736765549142]
	TIME [epoch: 6.4 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.013199012962881		[learning rate: 0.00044587]
	Learning Rate: 0.000445872
	LOSS [training: 3.013199012962881 | validation: 3.227099024171496]
	TIME [epoch: 6.35 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0288230219418857		[learning rate: 0.00044264]
	Learning Rate: 0.000442642
	LOSS [training: 3.0288230219418857 | validation: 3.064399403372825]
	TIME [epoch: 6.34 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0969907478633654		[learning rate: 0.00043944]
	Learning Rate: 0.000439435
	LOSS [training: 3.0969907478633654 | validation: 3.627014647012571]
	TIME [epoch: 6.35 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.146895787370021		[learning rate: 0.00043625]
	Learning Rate: 0.000436251
	LOSS [training: 3.146895787370021 | validation: 3.0717231574909913]
	TIME [epoch: 6.35 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9874553857188224		[learning rate: 0.00043309]
	Learning Rate: 0.000433091
	LOSS [training: 2.9874553857188224 | validation: 3.1952440298040115]
	TIME [epoch: 6.38 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0452761598026203		[learning rate: 0.00042995]
	Learning Rate: 0.000429953
	LOSS [training: 3.0452761598026203 | validation: 3.0251353482010472]
	TIME [epoch: 6.37 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.97763428405556		[learning rate: 0.00042684]
	Learning Rate: 0.000426838
	LOSS [training: 2.97763428405556 | validation: 2.9754857291509813]
	TIME [epoch: 6.34 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.004491333729112		[learning rate: 0.00042375]
	Learning Rate: 0.000423746
	LOSS [training: 3.004491333729112 | validation: 2.9640215965106522]
	TIME [epoch: 6.35 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9833803428255266		[learning rate: 0.00042068]
	Learning Rate: 0.000420676
	LOSS [training: 2.9833803428255266 | validation: 3.2503352126497287]
	TIME [epoch: 6.35 sec]
EPOCH 487/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.021273758058412		[learning rate: 0.00041763]
	Learning Rate: 0.000417628
	LOSS [training: 3.021273758058412 | validation: 2.983630615443772]
	TIME [epoch: 6.35 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0533908482286494		[learning rate: 0.0004146]
	Learning Rate: 0.000414602
	LOSS [training: 3.0533908482286494 | validation: 2.997980713536195]
	TIME [epoch: 6.39 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.931252722244219		[learning rate: 0.0004116]
	Learning Rate: 0.000411598
	LOSS [training: 2.931252722244219 | validation: 3.517925964398806]
	TIME [epoch: 6.35 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0660161374825132		[learning rate: 0.00040862]
	Learning Rate: 0.000408616
	LOSS [training: 3.0660161374825132 | validation: 2.928319720696831]
	TIME [epoch: 6.35 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.970860182213562		[learning rate: 0.00040566]
	Learning Rate: 0.000405656
	LOSS [training: 2.970860182213562 | validation: 2.9666880212013513]
	TIME [epoch: 6.34 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9455705426593064		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 2.9455705426593064 | validation: 2.9778533049726956]
	TIME [epoch: 6.35 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.042980076435127		[learning rate: 0.0003998]
	Learning Rate: 0.000399799
	LOSS [training: 3.042980076435127 | validation: 3.0734091679374527]
	TIME [epoch: 6.39 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.992289684543823		[learning rate: 0.0003969]
	Learning Rate: 0.000396903
	LOSS [training: 2.992289684543823 | validation: 3.124818067168162]
	TIME [epoch: 6.35 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.002573414534665		[learning rate: 0.00039403]
	Learning Rate: 0.000394027
	LOSS [training: 3.002573414534665 | validation: 3.1421411523501304]
	TIME [epoch: 6.35 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9508432521288337		[learning rate: 0.00039117]
	Learning Rate: 0.000391173
	LOSS [training: 2.9508432521288337 | validation: 2.9557167821217867]
	TIME [epoch: 6.35 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.92425108440605		[learning rate: 0.00038834]
	Learning Rate: 0.000388339
	LOSS [training: 2.92425108440605 | validation: 3.0090084183378254]
	TIME [epoch: 6.36 sec]
EPOCH 498/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9943167405398965		[learning rate: 0.00038553]
	Learning Rate: 0.000385525
	LOSS [training: 2.9943167405398965 | validation: 2.860490951365236]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_498.pth
	Model improved!!!
EPOCH 499/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.98346033962295		[learning rate: 0.00038273]
	Learning Rate: 0.000382732
	LOSS [training: 2.98346033962295 | validation: 2.967947624396964]
	TIME [epoch: 6.39 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9011387495074583		[learning rate: 0.00037996]
	Learning Rate: 0.000379959
	LOSS [training: 2.9011387495074583 | validation: 3.090737783525255]
	TIME [epoch: 6.35 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.932729222226307		[learning rate: 0.00037721]
	Learning Rate: 0.000377206
	LOSS [training: 2.932729222226307 | validation: 3.0346884429504275]
	TIME [epoch: 6.35 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9178402792936535		[learning rate: 0.00037447]
	Learning Rate: 0.000374474
	LOSS [training: 2.9178402792936535 | validation: 3.352402427395468]
	TIME [epoch: 6.35 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0440223333820047		[learning rate: 0.00037176]
	Learning Rate: 0.00037176
	LOSS [training: 3.0440223333820047 | validation: 3.1322425396491553]
	TIME [epoch: 6.35 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9522707512085242		[learning rate: 0.00036907]
	Learning Rate: 0.000369067
	LOSS [training: 2.9522707512085242 | validation: 2.9858406410351943]
	TIME [epoch: 6.4 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.972079485010399		[learning rate: 0.00036639]
	Learning Rate: 0.000366393
	LOSS [training: 2.972079485010399 | validation: 2.9509955345750454]
	TIME [epoch: 6.36 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8988558483796285		[learning rate: 0.00036374]
	Learning Rate: 0.000363739
	LOSS [training: 2.8988558483796285 | validation: 2.8887221887914643]
	TIME [epoch: 6.35 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0131448432495804		[learning rate: 0.0003611]
	Learning Rate: 0.000361103
	LOSS [training: 3.0131448432495804 | validation: 3.1711129181466733]
	TIME [epoch: 6.36 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9566120544160466		[learning rate: 0.00035849]
	Learning Rate: 0.000358487
	LOSS [training: 2.9566120544160466 | validation: 2.9847818123718506]
	TIME [epoch: 6.36 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9406493360405705		[learning rate: 0.00035589]
	Learning Rate: 0.00035589
	LOSS [training: 2.9406493360405705 | validation: 2.9204267158046937]
	TIME [epoch: 6.39 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.951659342047761		[learning rate: 0.00035331]
	Learning Rate: 0.000353312
	LOSS [training: 2.951659342047761 | validation: 2.884518433760338]
	TIME [epoch: 6.38 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9602042183870556		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 2.9602042183870556 | validation: 2.9349757776820002]
	TIME [epoch: 6.35 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.021589330760766		[learning rate: 0.00034821]
	Learning Rate: 0.000348211
	LOSS [training: 3.021589330760766 | validation: 2.911099786237295]
	TIME [epoch: 6.36 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0244110341194514		[learning rate: 0.00034569]
	Learning Rate: 0.000345688
	LOSS [training: 3.0244110341194514 | validation: 3.2973932024106025]
	TIME [epoch: 6.35 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.905675383685608		[learning rate: 0.00034318]
	Learning Rate: 0.000343183
	LOSS [training: 2.905675383685608 | validation: 2.879465059643443]
	TIME [epoch: 6.35 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.907661777745194		[learning rate: 0.0003407]
	Learning Rate: 0.000340697
	LOSS [training: 2.907661777745194 | validation: 3.0650154367073896]
	TIME [epoch: 6.4 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.969402743175197		[learning rate: 0.00033823]
	Learning Rate: 0.000338229
	LOSS [training: 2.969402743175197 | validation: 2.931048394740097]
	TIME [epoch: 6.37 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.94407458020092		[learning rate: 0.00033578]
	Learning Rate: 0.000335778
	LOSS [training: 2.94407458020092 | validation: 2.9929528846610047]
	TIME [epoch: 6.36 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9106754221234654		[learning rate: 0.00033335]
	Learning Rate: 0.000333346
	LOSS [training: 2.9106754221234654 | validation: 3.056356724035112]
	TIME [epoch: 6.35 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9196003315077017		[learning rate: 0.00033093]
	Learning Rate: 0.000330931
	LOSS [training: 2.9196003315077017 | validation: 2.928892159167468]
	TIME [epoch: 6.35 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9820505065083394		[learning rate: 0.00032853]
	Learning Rate: 0.000328533
	LOSS [training: 2.9820505065083394 | validation: 3.0265943393523616]
	TIME [epoch: 6.4 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9444564133083864		[learning rate: 0.00032615]
	Learning Rate: 0.000326153
	LOSS [training: 2.9444564133083864 | validation: 2.994119817306062]
	TIME [epoch: 6.36 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8511563806829816		[learning rate: 0.00032379]
	Learning Rate: 0.00032379
	LOSS [training: 2.8511563806829816 | validation: 2.888813273973427]
	TIME [epoch: 6.36 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9238382255253983		[learning rate: 0.00032144]
	Learning Rate: 0.000321444
	LOSS [training: 2.9238382255253983 | validation: 2.977204453364327]
	TIME [epoch: 6.35 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8836170029220285		[learning rate: 0.00031912]
	Learning Rate: 0.000319115
	LOSS [training: 2.8836170029220285 | validation: 2.8481943419688154]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_524.pth
	Model improved!!!
EPOCH 525/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8921285256847975		[learning rate: 0.0003168]
	Learning Rate: 0.000316803
	LOSS [training: 2.8921285256847975 | validation: 3.014773680786911]
	TIME [epoch: 6.37 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.942784665688274		[learning rate: 0.00031451]
	Learning Rate: 0.000314508
	LOSS [training: 2.942784665688274 | validation: 2.8929532356434873]
	TIME [epoch: 6.38 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9275833928643076		[learning rate: 0.00031223]
	Learning Rate: 0.000312229
	LOSS [training: 2.9275833928643076 | validation: 3.035459514985102]
	TIME [epoch: 6.35 sec]
EPOCH 528/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8487880231900693		[learning rate: 0.00030997]
	Learning Rate: 0.000309967
	LOSS [training: 2.8487880231900693 | validation: 2.922292420464696]
	TIME [epoch: 6.35 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.951581646391652		[learning rate: 0.00030772]
	Learning Rate: 0.000307722
	LOSS [training: 2.951581646391652 | validation: 2.9182516823042137]
	TIME [epoch: 6.34 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.920971973795046		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 2.920971973795046 | validation: 2.921367143369653]
	TIME [epoch: 6.35 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8596404403771722		[learning rate: 0.00030328]
	Learning Rate: 0.000303279
	LOSS [training: 2.8596404403771722 | validation: 3.0412454064916403]
	TIME [epoch: 6.4 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8945283902861556		[learning rate: 0.00030108]
	Learning Rate: 0.000301082
	LOSS [training: 2.8945283902861556 | validation: 2.7960834291117775]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_532.pth
	Model improved!!!
EPOCH 533/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9124729219044534		[learning rate: 0.0002989]
	Learning Rate: 0.0002989
	LOSS [training: 2.9124729219044534 | validation: 3.0329525332141047]
	TIME [epoch: 6.35 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.907580303694655		[learning rate: 0.00029673]
	Learning Rate: 0.000296735
	LOSS [training: 2.907580303694655 | validation: 2.8843240525231826]
	TIME [epoch: 6.35 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.868807037586485		[learning rate: 0.00029458]
	Learning Rate: 0.000294585
	LOSS [training: 2.868807037586485 | validation: 2.918649530939355]
	TIME [epoch: 6.35 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.824695416194663		[learning rate: 0.00029245]
	Learning Rate: 0.000292451
	LOSS [training: 2.824695416194663 | validation: 2.907692990899534]
	TIME [epoch: 6.38 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8407777703848627		[learning rate: 0.00029033]
	Learning Rate: 0.000290332
	LOSS [training: 2.8407777703848627 | validation: 2.8929388450652955]
	TIME [epoch: 6.36 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8361104799461554		[learning rate: 0.00028823]
	Learning Rate: 0.000288228
	LOSS [training: 2.8361104799461554 | validation: 2.857983308902509]
	TIME [epoch: 6.35 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.919627723860705		[learning rate: 0.00028614]
	Learning Rate: 0.00028614
	LOSS [training: 2.919627723860705 | validation: 2.884852674497144]
	TIME [epoch: 6.35 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8286765933135407		[learning rate: 0.00028407]
	Learning Rate: 0.000284067
	LOSS [training: 2.8286765933135407 | validation: 2.9303698707031405]
	TIME [epoch: 6.35 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.859779186733183		[learning rate: 0.00028201]
	Learning Rate: 0.000282009
	LOSS [training: 2.859779186733183 | validation: 2.9623993477725765]
	TIME [epoch: 6.35 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.84001174226863		[learning rate: 0.00027997]
	Learning Rate: 0.000279966
	LOSS [training: 2.84001174226863 | validation: 2.900064955588948]
	TIME [epoch: 6.39 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9815589409654204		[learning rate: 0.00027794]
	Learning Rate: 0.000277938
	LOSS [training: 2.9815589409654204 | validation: 3.1316557105617573]
	TIME [epoch: 6.35 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.873787641119225		[learning rate: 0.00027592]
	Learning Rate: 0.000275924
	LOSS [training: 2.873787641119225 | validation: 2.858866530943834]
	TIME [epoch: 6.35 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.810597224338961		[learning rate: 0.00027392]
	Learning Rate: 0.000273925
	LOSS [training: 2.810597224338961 | validation: 2.794958490440167]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_545.pth
	Model improved!!!
EPOCH 546/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.821234570939869		[learning rate: 0.00027194]
	Learning Rate: 0.00027194
	LOSS [training: 2.821234570939869 | validation: 2.870419946708836]
	TIME [epoch: 6.35 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.888649075976148		[learning rate: 0.00026997]
	Learning Rate: 0.00026997
	LOSS [training: 2.888649075976148 | validation: 2.8649602110497243]
	TIME [epoch: 6.4 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8439543064369377		[learning rate: 0.00026801]
	Learning Rate: 0.000268014
	LOSS [training: 2.8439543064369377 | validation: 3.133366508621698]
	TIME [epoch: 6.36 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8482872506328416		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 2.8482872506328416 | validation: 2.9672302961012793]
	TIME [epoch: 6.35 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.844314933595571		[learning rate: 0.00026414]
	Learning Rate: 0.000264145
	LOSS [training: 2.844314933595571 | validation: 2.823967810698311]
	TIME [epoch: 6.35 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8042939537315124		[learning rate: 0.00026223]
	Learning Rate: 0.000262231
	LOSS [training: 2.8042939537315124 | validation: 2.8783720360028147]
	TIME [epoch: 6.35 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.792084737329041		[learning rate: 0.00026033]
	Learning Rate: 0.000260331
	LOSS [training: 2.792084737329041 | validation: 2.7799932219609174]
	TIME [epoch: 6.37 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_552.pth
	Model improved!!!
EPOCH 553/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7987561133126864		[learning rate: 0.00025845]
	Learning Rate: 0.000258445
	LOSS [training: 2.7987561133126864 | validation: 3.222046697363528]
	TIME [epoch: 6.4 sec]
EPOCH 554/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.85302384710847		[learning rate: 0.00025657]
	Learning Rate: 0.000256573
	LOSS [training: 2.85302384710847 | validation: 3.008575177812616]
	TIME [epoch: 6.36 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8648206472010282		[learning rate: 0.00025471]
	Learning Rate: 0.000254714
	LOSS [training: 2.8648206472010282 | validation: 2.8267148865393814]
	TIME [epoch: 6.36 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8547577590537103		[learning rate: 0.00025287]
	Learning Rate: 0.000252868
	LOSS [training: 2.8547577590537103 | validation: 2.8628964712326943]
	TIME [epoch: 6.35 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8163816590184445		[learning rate: 0.00025104]
	Learning Rate: 0.000251037
	LOSS [training: 2.8163816590184445 | validation: 2.87394815333633]
	TIME [epoch: 6.36 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.851342020000817		[learning rate: 0.00024922]
	Learning Rate: 0.000249218
	LOSS [training: 2.851342020000817 | validation: 3.1564084574004014]
	TIME [epoch: 6.4 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.870989192678297		[learning rate: 0.00024741]
	Learning Rate: 0.000247412
	LOSS [training: 2.870989192678297 | validation: 2.815604864580778]
	TIME [epoch: 6.36 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.804142094692769		[learning rate: 0.00024562]
	Learning Rate: 0.00024562
	LOSS [training: 2.804142094692769 | validation: 2.784241665898749]
	TIME [epoch: 6.36 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7731814045335534		[learning rate: 0.00024384]
	Learning Rate: 0.00024384
	LOSS [training: 2.7731814045335534 | validation: 2.77659770587808]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_561.pth
	Model improved!!!
EPOCH 562/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.798652991726912		[learning rate: 0.00024207]
	Learning Rate: 0.000242074
	LOSS [training: 2.798652991726912 | validation: 2.982016451696105]
	TIME [epoch: 6.36 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.831826105401924		[learning rate: 0.00024032]
	Learning Rate: 0.00024032
	LOSS [training: 2.831826105401924 | validation: 2.8588286129894422]
	TIME [epoch: 6.41 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8125795846067767		[learning rate: 0.00023858]
	Learning Rate: 0.000238579
	LOSS [training: 2.8125795846067767 | validation: 2.773773426092995]
	TIME [epoch: 6.36 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_564.pth
	Model improved!!!
EPOCH 565/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8371615372835786		[learning rate: 0.00023685]
	Learning Rate: 0.00023685
	LOSS [training: 2.8371615372835786 | validation: 3.1662533111194406]
	TIME [epoch: 6.36 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.843559848875457		[learning rate: 0.00023513]
	Learning Rate: 0.000235134
	LOSS [training: 2.843559848875457 | validation: 2.774951957927902]
	TIME [epoch: 6.35 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.794923473893287		[learning rate: 0.00023343]
	Learning Rate: 0.000233431
	LOSS [training: 2.794923473893287 | validation: 2.850923197909454]
	TIME [epoch: 6.36 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.75970240046942		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 2.75970240046942 | validation: 2.8491031193327396]
	TIME [epoch: 6.37 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.795811500950353		[learning rate: 0.00023006]
	Learning Rate: 0.000230061
	LOSS [training: 2.795811500950353 | validation: 2.7751725705631856]
	TIME [epoch: 6.38 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7787936371731523		[learning rate: 0.00022839]
	Learning Rate: 0.000228394
	LOSS [training: 2.7787936371731523 | validation: 3.2379489956131806]
	TIME [epoch: 6.36 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8148892356624433		[learning rate: 0.00022674]
	Learning Rate: 0.000226739
	LOSS [training: 2.8148892356624433 | validation: 2.8096537685294365]
	TIME [epoch: 6.36 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.782097604632435		[learning rate: 0.0002251]
	Learning Rate: 0.000225096
	LOSS [training: 2.782097604632435 | validation: 2.9648188588693998]
	TIME [epoch: 6.36 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7826758270613343		[learning rate: 0.00022347]
	Learning Rate: 0.000223466
	LOSS [training: 2.7826758270613343 | validation: 2.8459476641046253]
	TIME [epoch: 6.35 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.778239501878808		[learning rate: 0.00022185]
	Learning Rate: 0.000221847
	LOSS [training: 2.778239501878808 | validation: 2.7706721333975053]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_574.pth
	Model improved!!!
EPOCH 575/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.751279256674284		[learning rate: 0.00022024]
	Learning Rate: 0.000220239
	LOSS [training: 2.751279256674284 | validation: 2.837878361354432]
	TIME [epoch: 6.36 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7784272660652194		[learning rate: 0.00021864]
	Learning Rate: 0.000218644
	LOSS [training: 2.7784272660652194 | validation: 3.2109879884706922]
	TIME [epoch: 6.36 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.776773187684308		[learning rate: 0.00021706]
	Learning Rate: 0.00021706
	LOSS [training: 2.776773187684308 | validation: 2.856663989110934]
	TIME [epoch: 6.35 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.737054697000919		[learning rate: 0.00021549]
	Learning Rate: 0.000215487
	LOSS [training: 2.737054697000919 | validation: 2.819584128270561]
	TIME [epoch: 6.35 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8102901650883534		[learning rate: 0.00021393]
	Learning Rate: 0.000213926
	LOSS [training: 2.8102901650883534 | validation: 3.0934967814781658]
	TIME [epoch: 6.38 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7699391805320563		[learning rate: 0.00021238]
	Learning Rate: 0.000212376
	LOSS [training: 2.7699391805320563 | validation: 2.851691443309423]
	TIME [epoch: 6.38 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.761856097031449		[learning rate: 0.00021084]
	Learning Rate: 0.000210837
	LOSS [training: 2.761856097031449 | validation: 3.0024737807814357]
	TIME [epoch: 6.35 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7506670625364134		[learning rate: 0.00020931]
	Learning Rate: 0.00020931
	LOSS [training: 2.7506670625364134 | validation: 2.821647913086352]
	TIME [epoch: 6.35 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.760747825610294		[learning rate: 0.00020779]
	Learning Rate: 0.000207793
	LOSS [training: 2.760747825610294 | validation: 2.8212942566504706]
	TIME [epoch: 6.36 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7532193290733966		[learning rate: 0.00020629]
	Learning Rate: 0.000206288
	LOSS [training: 2.7532193290733966 | validation: 2.8029831250128714]
	TIME [epoch: 6.36 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7204751062549373		[learning rate: 0.00020479]
	Learning Rate: 0.000204793
	LOSS [training: 2.7204751062549373 | validation: 2.8417861257423427]
	TIME [epoch: 6.4 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.779867874471557		[learning rate: 0.00020331]
	Learning Rate: 0.00020331
	LOSS [training: 2.779867874471557 | validation: 2.9295623401991113]
	TIME [epoch: 6.36 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8102580537461748		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 2.8102580537461748 | validation: 2.8040055207957133]
	TIME [epoch: 6.35 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7425920196689835		[learning rate: 0.00020037]
	Learning Rate: 0.000200374
	LOSS [training: 2.7425920196689835 | validation: 2.772909221034557]
	TIME [epoch: 6.35 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7093728793911485		[learning rate: 0.00019892]
	Learning Rate: 0.000198923
	LOSS [training: 2.7093728793911485 | validation: 2.917510454724175]
	TIME [epoch: 6.35 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7226996594238297		[learning rate: 0.00019748]
	Learning Rate: 0.000197482
	LOSS [training: 2.7226996594238297 | validation: 2.8085911141873687]
	TIME [epoch: 6.4 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.731612923315071		[learning rate: 0.00019605]
	Learning Rate: 0.000196051
	LOSS [training: 2.731612923315071 | validation: 2.8181212568366405]
	TIME [epoch: 6.37 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.743401823420836		[learning rate: 0.00019463]
	Learning Rate: 0.00019463
	LOSS [training: 2.743401823420836 | validation: 2.881858327486091]
	TIME [epoch: 6.35 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7602531960951557		[learning rate: 0.00019322]
	Learning Rate: 0.00019322
	LOSS [training: 2.7602531960951557 | validation: 2.8355204987452036]
	TIME [epoch: 6.35 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.756599988376352		[learning rate: 0.00019182]
	Learning Rate: 0.00019182
	LOSS [training: 2.756599988376352 | validation: 2.7950473477936897]
	TIME [epoch: 6.35 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7259902529674394		[learning rate: 0.00019043]
	Learning Rate: 0.000190431
	LOSS [training: 2.7259902529674394 | validation: 2.794142996059419]
	TIME [epoch: 6.37 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.720904631092644		[learning rate: 0.00018905]
	Learning Rate: 0.000189051
	LOSS [training: 2.720904631092644 | validation: 3.1224526366829144]
	TIME [epoch: 6.39 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.775827979351684		[learning rate: 0.00018768]
	Learning Rate: 0.000187681
	LOSS [training: 2.775827979351684 | validation: 2.8324817099549446]
	TIME [epoch: 6.36 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7223029915258516		[learning rate: 0.00018632]
	Learning Rate: 0.000186322
	LOSS [training: 2.7223029915258516 | validation: 2.9150141755872925]
	TIME [epoch: 6.36 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7771752952757556		[learning rate: 0.00018497]
	Learning Rate: 0.000184972
	LOSS [training: 2.7771752952757556 | validation: 2.772006611939763]
	TIME [epoch: 6.34 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7409159179398896		[learning rate: 0.00018363]
	Learning Rate: 0.000183632
	LOSS [training: 2.7409159179398896 | validation: 2.977035106717117]
	TIME [epoch: 6.35 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7248431945938245		[learning rate: 0.0001823]
	Learning Rate: 0.000182301
	LOSS [training: 2.7248431945938245 | validation: 2.8410241903899807]
	TIME [epoch: 6.41 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6981314577765767		[learning rate: 0.00018098]
	Learning Rate: 0.00018098
	LOSS [training: 2.6981314577765767 | validation: 2.777249580524437]
	TIME [epoch: 6.35 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8129106671201805		[learning rate: 0.00017967]
	Learning Rate: 0.000179669
	LOSS [training: 2.8129106671201805 | validation: 2.8322453477510976]
	TIME [epoch: 6.35 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.707090311453525		[learning rate: 0.00017837]
	Learning Rate: 0.000178368
	LOSS [training: 2.707090311453525 | validation: 2.812879547087455]
	TIME [epoch: 6.35 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.75600219450572		[learning rate: 0.00017708]
	Learning Rate: 0.000177075
	LOSS [training: 2.75600219450572 | validation: 2.7917022321152523]
	TIME [epoch: 6.35 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6892897237406537		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 2.6892897237406537 | validation: 3.489986248696069]
	TIME [epoch: 6.38 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8351213438014424		[learning rate: 0.00017452]
	Learning Rate: 0.000174519
	LOSS [training: 2.8351213438014424 | validation: 2.7969288918321267]
	TIME [epoch: 6.36 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7064558637994227		[learning rate: 0.00017325]
	Learning Rate: 0.000173254
	LOSS [training: 2.7064558637994227 | validation: 2.9379715115150913]
	TIME [epoch: 6.35 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7067058369627945		[learning rate: 0.000172]
	Learning Rate: 0.000171999
	LOSS [training: 2.7067058369627945 | validation: 2.768027694879901]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_609.pth
	Model improved!!!
EPOCH 610/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6867638481527605		[learning rate: 0.00017075]
	Learning Rate: 0.000170753
	LOSS [training: 2.6867638481527605 | validation: 3.075776436717643]
	TIME [epoch: 6.35 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.749026572012403		[learning rate: 0.00016952]
	Learning Rate: 0.000169516
	LOSS [training: 2.749026572012403 | validation: 2.9103511364807533]
	TIME [epoch: 6.36 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.763449341050957		[learning rate: 0.00016829]
	Learning Rate: 0.000168288
	LOSS [training: 2.763449341050957 | validation: 2.772818975454971]
	TIME [epoch: 6.41 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7529200998886294		[learning rate: 0.00016707]
	Learning Rate: 0.000167069
	LOSS [training: 2.7529200998886294 | validation: 2.916546359987399]
	TIME [epoch: 6.35 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.71119897184085		[learning rate: 0.00016586]
	Learning Rate: 0.000165858
	LOSS [training: 2.71119897184085 | validation: 2.9980258917400144]
	TIME [epoch: 6.35 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7228212390086464		[learning rate: 0.00016466]
	Learning Rate: 0.000164657
	LOSS [training: 2.7228212390086464 | validation: 2.773129281361279]
	TIME [epoch: 6.35 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.684003019793853		[learning rate: 0.00016346]
	Learning Rate: 0.000163464
	LOSS [training: 2.684003019793853 | validation: 2.7448257756135392]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_616.pth
	Model improved!!!
EPOCH 617/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6997038756828204		[learning rate: 0.00016228]
	Learning Rate: 0.000162279
	LOSS [training: 2.6997038756828204 | validation: 2.7650101770659132]
	TIME [epoch: 6.4 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7063030417254135		[learning rate: 0.0001611]
	Learning Rate: 0.000161104
	LOSS [training: 2.7063030417254135 | validation: 2.818213141524871]
	TIME [epoch: 6.36 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.694852856181381		[learning rate: 0.00015994]
	Learning Rate: 0.000159936
	LOSS [training: 2.694852856181381 | validation: 2.718447803178157]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_619.pth
	Model improved!!!
EPOCH 620/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.688749249246789		[learning rate: 0.00015878]
	Learning Rate: 0.000158778
	LOSS [training: 2.688749249246789 | validation: 2.769745534658523]
	TIME [epoch: 6.35 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6815824325758344		[learning rate: 0.00015763]
	Learning Rate: 0.000157627
	LOSS [training: 2.6815824325758344 | validation: 2.869887683524213]
	TIME [epoch: 6.35 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.726334082891577		[learning rate: 0.00015649]
	Learning Rate: 0.000156485
	LOSS [training: 2.726334082891577 | validation: 2.876152451228426]
	TIME [epoch: 6.37 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6710036875704284		[learning rate: 0.00015535]
	Learning Rate: 0.000155352
	LOSS [training: 2.6710036875704284 | validation: 2.8089234780369488]
	TIME [epoch: 6.37 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.670308694598142		[learning rate: 0.00015423]
	Learning Rate: 0.000154226
	LOSS [training: 2.670308694598142 | validation: 3.0546347938364375]
	TIME [epoch: 6.34 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6876075617896893		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 2.6876075617896893 | validation: 2.860706678647911]
	TIME [epoch: 6.34 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6929983429156206		[learning rate: 0.000152]
	Learning Rate: 0.000152
	LOSS [training: 2.6929983429156206 | validation: 2.791938281032012]
	TIME [epoch: 6.34 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6709110337248254		[learning rate: 0.0001509]
	Learning Rate: 0.000150898
	LOSS [training: 2.6709110337248254 | validation: 2.8218540412367785]
	TIME [epoch: 6.34 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6972447040444703		[learning rate: 0.00014981]
	Learning Rate: 0.000149805
	LOSS [training: 2.6972447040444703 | validation: 2.8119002554686245]
	TIME [epoch: 6.4 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.683607845606187		[learning rate: 0.00014872]
	Learning Rate: 0.00014872
	LOSS [training: 2.683607845606187 | validation: 2.8795154839654544]
	TIME [epoch: 6.35 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6795475938903204		[learning rate: 0.00014764]
	Learning Rate: 0.000147642
	LOSS [training: 2.6795475938903204 | validation: 2.8750170543806552]
	TIME [epoch: 6.35 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6952166306021788		[learning rate: 0.00014657]
	Learning Rate: 0.000146573
	LOSS [training: 2.6952166306021788 | validation: 2.7863678418565136]
	TIME [epoch: 6.35 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.683214557921888		[learning rate: 0.00014551]
	Learning Rate: 0.000145511
	LOSS [training: 2.683214557921888 | validation: 2.765802292603899]
	TIME [epoch: 6.35 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6575720505756233		[learning rate: 0.00014446]
	Learning Rate: 0.000144456
	LOSS [training: 2.6575720505756233 | validation: 2.7389568044194768]
	TIME [epoch: 6.38 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6693227137847253		[learning rate: 0.00014341]
	Learning Rate: 0.00014341
	LOSS [training: 2.6693227137847253 | validation: 2.9248416970565883]
	TIME [epoch: 6.36 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.716037629190887		[learning rate: 0.00014237]
	Learning Rate: 0.000142371
	LOSS [training: 2.716037629190887 | validation: 2.6993506408747607]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_635.pth
	Model improved!!!
EPOCH 636/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.644829501960005		[learning rate: 0.00014134]
	Learning Rate: 0.000141339
	LOSS [training: 2.644829501960005 | validation: 2.68739366577469]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_636.pth
	Model improved!!!
EPOCH 637/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.667915101924329		[learning rate: 0.00014032]
	Learning Rate: 0.000140315
	LOSS [training: 2.667915101924329 | validation: 2.8073760398705234]
	TIME [epoch: 6.34 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.668900204979143		[learning rate: 0.0001393]
	Learning Rate: 0.000139299
	LOSS [training: 2.668900204979143 | validation: 2.8269704927617028]
	TIME [epoch: 6.36 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7105977570824233		[learning rate: 0.00013829]
	Learning Rate: 0.00013829
	LOSS [training: 2.7105977570824233 | validation: 2.845060059220944]
	TIME [epoch: 6.37 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.66814506853778		[learning rate: 0.00013729]
	Learning Rate: 0.000137288
	LOSS [training: 2.66814506853778 | validation: 2.7088229320065618]
	TIME [epoch: 6.34 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.655337594174841		[learning rate: 0.00013629]
	Learning Rate: 0.000136293
	LOSS [training: 2.655337594174841 | validation: 2.9217858632298093]
	TIME [epoch: 6.34 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6872853480870265		[learning rate: 0.00013531]
	Learning Rate: 0.000135306
	LOSS [training: 2.6872853480870265 | validation: 2.685210834028864]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_642.pth
	Model improved!!!
EPOCH 643/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6935506226883668		[learning rate: 0.00013433]
	Learning Rate: 0.000134325
	LOSS [training: 2.6935506226883668 | validation: 2.7645533676121534]
	TIME [epoch: 6.36 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6777925676845946		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 2.6777925676845946 | validation: 2.7173369630448656]
	TIME [epoch: 6.39 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6456321175925646		[learning rate: 0.00013239]
	Learning Rate: 0.000132386
	LOSS [training: 2.6456321175925646 | validation: 2.747601429870259]
	TIME [epoch: 6.34 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6433932877515973		[learning rate: 0.00013143]
	Learning Rate: 0.000131427
	LOSS [training: 2.6433932877515973 | validation: 2.7544877995620967]
	TIME [epoch: 6.34 sec]
EPOCH 647/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6628853119942724		[learning rate: 0.00013047]
	Learning Rate: 0.000130475
	LOSS [training: 2.6628853119942724 | validation: 2.742474289377271]
	TIME [epoch: 6.34 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.68241615694559		[learning rate: 0.00012953]
	Learning Rate: 0.000129529
	LOSS [training: 2.68241615694559 | validation: 2.718099360753743]
	TIME [epoch: 6.33 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.667156187019729		[learning rate: 0.00012859]
	Learning Rate: 0.000128591
	LOSS [training: 2.667156187019729 | validation: 2.7153488447962832]
	TIME [epoch: 6.37 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6545970911928807		[learning rate: 0.00012766]
	Learning Rate: 0.000127659
	LOSS [training: 2.6545970911928807 | validation: 2.7605314441351876]
	TIME [epoch: 6.36 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.675939853091667		[learning rate: 0.00012673]
	Learning Rate: 0.000126735
	LOSS [training: 2.675939853091667 | validation: 2.7805064609431693]
	TIME [epoch: 6.34 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6594774896886104		[learning rate: 0.00012582]
	Learning Rate: 0.000125816
	LOSS [training: 2.6594774896886104 | validation: 2.7061888645838494]
	TIME [epoch: 6.34 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.647332724760288		[learning rate: 0.0001249]
	Learning Rate: 0.000124905
	LOSS [training: 2.647332724760288 | validation: 3.0255609909267305]
	TIME [epoch: 6.34 sec]
EPOCH 654/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6890135532758395		[learning rate: 0.000124]
	Learning Rate: 0.000124
	LOSS [training: 2.6890135532758395 | validation: 2.732064462168367]
	TIME [epoch: 6.35 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.649609065218957		[learning rate: 0.0001231]
	Learning Rate: 0.000123101
	LOSS [training: 2.649609065218957 | validation: 2.8061938195531297]
	TIME [epoch: 6.38 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.642133864799867		[learning rate: 0.00012221]
	Learning Rate: 0.00012221
	LOSS [training: 2.642133864799867 | validation: 2.728760850837355]
	TIME [epoch: 6.34 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6491906744052938		[learning rate: 0.00012132]
	Learning Rate: 0.000121324
	LOSS [training: 2.6491906744052938 | validation: 2.759308169280457]
	TIME [epoch: 6.34 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6416572525182147		[learning rate: 0.00012045]
	Learning Rate: 0.000120445
	LOSS [training: 2.6416572525182147 | validation: 2.7219539249125306]
	TIME [epoch: 6.34 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6551883069257283		[learning rate: 0.00011957]
	Learning Rate: 0.000119573
	LOSS [training: 2.6551883069257283 | validation: 2.7673257207695148]
	TIME [epoch: 6.34 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6596290951835106		[learning rate: 0.00011871]
	Learning Rate: 0.000118706
	LOSS [training: 2.6596290951835106 | validation: 2.758060651107747]
	TIME [epoch: 6.39 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6301324085414244		[learning rate: 0.00011785]
	Learning Rate: 0.000117846
	LOSS [training: 2.6301324085414244 | validation: 2.7672489718491438]
	TIME [epoch: 6.35 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6442412877384207		[learning rate: 0.00011699]
	Learning Rate: 0.000116992
	LOSS [training: 2.6442412877384207 | validation: 2.7307299253925326]
	TIME [epoch: 6.34 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.63469498068045		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 2.63469498068045 | validation: 2.7374658923209667]
	TIME [epoch: 6.34 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6426192446711445		[learning rate: 0.0001153]
	Learning Rate: 0.000115303
	LOSS [training: 2.6426192446711445 | validation: 2.8407373086419883]
	TIME [epoch: 6.34 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.639331975616113		[learning rate: 0.00011447]
	Learning Rate: 0.000114468
	LOSS [training: 2.639331975616113 | validation: 2.703212940467264]
	TIME [epoch: 6.36 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6363620996697614		[learning rate: 0.00011364]
	Learning Rate: 0.000113639
	LOSS [training: 2.6363620996697614 | validation: 2.719784020486406]
	TIME [epoch: 6.37 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6542684332695075		[learning rate: 0.00011282]
	Learning Rate: 0.000112815
	LOSS [training: 2.6542684332695075 | validation: 2.7174077757291304]
	TIME [epoch: 6.35 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6565968817615193		[learning rate: 0.000112]
	Learning Rate: 0.000111998
	LOSS [training: 2.6565968817615193 | validation: 2.769726045655819]
	TIME [epoch: 6.34 sec]
EPOCH 669/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6223230701895455		[learning rate: 0.00011119]
	Learning Rate: 0.000111187
	LOSS [training: 2.6223230701895455 | validation: 2.679174061560156]
	TIME [epoch: 6.34 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_669.pth
	Model improved!!!
EPOCH 670/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6178225868388263		[learning rate: 0.00011038]
	Learning Rate: 0.000110381
	LOSS [training: 2.6178225868388263 | validation: 2.6745707762921356]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_670.pth
	Model improved!!!
EPOCH 671/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6250235441635423		[learning rate: 0.00010958]
	Learning Rate: 0.000109581
	LOSS [training: 2.6250235441635423 | validation: 2.821777661149887]
	TIME [epoch: 6.4 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6428266152919933		[learning rate: 0.00010879]
	Learning Rate: 0.000108787
	LOSS [training: 2.6428266152919933 | validation: 2.709718858496838]
	TIME [epoch: 6.35 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6294849688525495		[learning rate: 0.000108]
	Learning Rate: 0.000107999
	LOSS [training: 2.6294849688525495 | validation: 2.691878627962236]
	TIME [epoch: 6.35 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6240269803808896		[learning rate: 0.00010722]
	Learning Rate: 0.000107217
	LOSS [training: 2.6240269803808896 | validation: 2.7099740430251567]
	TIME [epoch: 6.35 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6448151992804854		[learning rate: 0.00010644]
	Learning Rate: 0.00010644
	LOSS [training: 2.6448151992804854 | validation: 2.693824584176405]
	TIME [epoch: 6.35 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.643557489401552		[learning rate: 0.00010567]
	Learning Rate: 0.000105669
	LOSS [training: 2.643557489401552 | validation: 2.756819628167631]
	TIME [epoch: 6.39 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6229624242850216		[learning rate: 0.0001049]
	Learning Rate: 0.000104903
	LOSS [training: 2.6229624242850216 | validation: 2.7064891837355756]
	TIME [epoch: 6.38 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6263299509738296		[learning rate: 0.00010414]
	Learning Rate: 0.000104143
	LOSS [training: 2.6263299509738296 | validation: 2.656663671316554]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_678.pth
	Model improved!!!
EPOCH 679/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6179290694250805		[learning rate: 0.00010339]
	Learning Rate: 0.000103389
	LOSS [training: 2.6179290694250805 | validation: 2.709077526849809]
	TIME [epoch: 6.35 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6121728503128576		[learning rate: 0.00010264]
	Learning Rate: 0.00010264
	LOSS [training: 2.6121728503128576 | validation: 2.7545133004226576]
	TIME [epoch: 6.35 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.65961148838656		[learning rate: 0.0001019]
	Learning Rate: 0.000101896
	LOSS [training: 2.65961148838656 | validation: 2.757214944060266]
	TIME [epoch: 6.37 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.629327501064252		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 2.629327501064252 | validation: 2.60125059086423]
	TIME [epoch: 6.39 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_682.pth
	Model improved!!!
EPOCH 683/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6386948376987656		[learning rate: 0.00010043]
	Learning Rate: 0.000100425
	LOSS [training: 2.6386948376987656 | validation: 2.656173288180173]
	TIME [epoch: 6.35 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5929938743166745		[learning rate: 9.9697e-05]
	Learning Rate: 9.96975e-05
	LOSS [training: 2.5929938743166745 | validation: 2.673613984824528]
	TIME [epoch: 6.35 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6310278079073433		[learning rate: 9.8975e-05]
	Learning Rate: 9.89752e-05
	LOSS [training: 2.6310278079073433 | validation: 2.743028224096306]
	TIME [epoch: 6.39 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.62523189646828		[learning rate: 9.8258e-05]
	Learning Rate: 9.82581e-05
	LOSS [training: 2.62523189646828 | validation: 2.743596314774627]
	TIME [epoch: 6.35 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.652026837121614		[learning rate: 9.7546e-05]
	Learning Rate: 9.75463e-05
	LOSS [training: 2.652026837121614 | validation: 2.651747826928247]
	TIME [epoch: 6.41 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.611971485985211		[learning rate: 9.684e-05]
	Learning Rate: 9.68396e-05
	LOSS [training: 2.611971485985211 | validation: 2.7938836771814257]
	TIME [epoch: 6.35 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6223036726407876		[learning rate: 9.6138e-05]
	Learning Rate: 9.61379e-05
	LOSS [training: 2.6223036726407876 | validation: 2.6136273778445833]
	TIME [epoch: 6.35 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6164679760133813		[learning rate: 9.5441e-05]
	Learning Rate: 9.54414e-05
	LOSS [training: 2.6164679760133813 | validation: 2.728065985569727]
	TIME [epoch: 6.35 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6543872420914028		[learning rate: 9.475e-05]
	Learning Rate: 9.475e-05
	LOSS [training: 2.6543872420914028 | validation: 2.7029101781690605]
	TIME [epoch: 6.35 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.641453708123393		[learning rate: 9.4064e-05]
	Learning Rate: 9.40635e-05
	LOSS [training: 2.641453708123393 | validation: 2.6598652998066656]
	TIME [epoch: 6.38 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6137413710410065		[learning rate: 9.3382e-05]
	Learning Rate: 9.3382e-05
	LOSS [training: 2.6137413710410065 | validation: 2.677565319472696]
	TIME [epoch: 6.38 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6215217956471943		[learning rate: 9.2705e-05]
	Learning Rate: 9.27055e-05
	LOSS [training: 2.6215217956471943 | validation: 2.6760461257039365]
	TIME [epoch: 6.35 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6019911574215504		[learning rate: 9.2034e-05]
	Learning Rate: 9.20338e-05
	LOSS [training: 2.6019911574215504 | validation: 2.6811272175023655]
	TIME [epoch: 6.35 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6082260600154648		[learning rate: 9.1367e-05]
	Learning Rate: 9.13671e-05
	LOSS [training: 2.6082260600154648 | validation: 2.647083136159859]
	TIME [epoch: 6.34 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.637441151826118		[learning rate: 9.0705e-05]
	Learning Rate: 9.07051e-05
	LOSS [training: 2.637441151826118 | validation: 2.6846858542215672]
	TIME [epoch: 6.36 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6026488039088576		[learning rate: 9.0048e-05]
	Learning Rate: 9.00479e-05
	LOSS [training: 2.6026488039088576 | validation: 2.708407914029838]
	TIME [epoch: 6.4 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6170822448634024		[learning rate: 8.9396e-05]
	Learning Rate: 8.93955e-05
	LOSS [training: 2.6170822448634024 | validation: 2.760250606672299]
	TIME [epoch: 6.35 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.600113488813779		[learning rate: 8.8748e-05]
	Learning Rate: 8.87479e-05
	LOSS [training: 2.600113488813779 | validation: 2.6889133336097224]
	TIME [epoch: 6.35 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.595686927090692		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 2.595686927090692 | validation: 2.734347673718278]
	TIME [epoch: 6.35 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6381650492756084		[learning rate: 8.7467e-05]
	Learning Rate: 8.74666e-05
	LOSS [training: 2.6381650492756084 | validation: 2.7579471962749738]
	TIME [epoch: 6.35 sec]
EPOCH 703/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.608774939271829		[learning rate: 8.6833e-05]
	Learning Rate: 8.68329e-05
	LOSS [training: 2.608774939271829 | validation: 2.642095504496032]
	TIME [epoch: 6.4 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.608199806137856		[learning rate: 8.6204e-05]
	Learning Rate: 8.62038e-05
	LOSS [training: 2.608199806137856 | validation: 2.6656030919292384]
	TIME [epoch: 6.36 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.583127268186168		[learning rate: 8.5579e-05]
	Learning Rate: 8.55793e-05
	LOSS [training: 2.583127268186168 | validation: 2.735820943130079]
	TIME [epoch: 6.35 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6182250084682965		[learning rate: 8.4959e-05]
	Learning Rate: 8.49592e-05
	LOSS [training: 2.6182250084682965 | validation: 2.6752719624178694]
	TIME [epoch: 6.35 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.588421962368965		[learning rate: 8.4344e-05]
	Learning Rate: 8.43437e-05
	LOSS [training: 2.588421962368965 | validation: 2.7072618356333704]
	TIME [epoch: 6.35 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.590111604190158		[learning rate: 8.3733e-05]
	Learning Rate: 8.37327e-05
	LOSS [training: 2.590111604190158 | validation: 2.7122923594973827]
	TIME [epoch: 6.37 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.597459944172253		[learning rate: 8.3126e-05]
	Learning Rate: 8.3126e-05
	LOSS [training: 2.597459944172253 | validation: 2.6616500717739875]
	TIME [epoch: 6.39 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.605206622851424		[learning rate: 8.2524e-05]
	Learning Rate: 8.25238e-05
	LOSS [training: 2.605206622851424 | validation: 2.679727508239922]
	TIME [epoch: 6.36 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5837569004861116		[learning rate: 8.1926e-05]
	Learning Rate: 8.19259e-05
	LOSS [training: 2.5837569004861116 | validation: 2.644150943549316]
	TIME [epoch: 6.35 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.594695496458809		[learning rate: 8.1332e-05]
	Learning Rate: 8.13323e-05
	LOSS [training: 2.594695496458809 | validation: 2.6643970088067053]
	TIME [epoch: 6.35 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.604336189037464		[learning rate: 8.0743e-05]
	Learning Rate: 8.07431e-05
	LOSS [training: 2.604336189037464 | validation: 2.6598146577851893]
	TIME [epoch: 6.35 sec]
EPOCH 714/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.593340910936498		[learning rate: 8.0158e-05]
	Learning Rate: 8.01581e-05
	LOSS [training: 2.593340910936498 | validation: 2.728441998028252]
	TIME [epoch: 6.4 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6711857019359417		[learning rate: 7.9577e-05]
	Learning Rate: 7.95774e-05
	LOSS [training: 2.6711857019359417 | validation: 2.6276611810288593]
	TIME [epoch: 6.36 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.587270983380159		[learning rate: 7.9001e-05]
	Learning Rate: 7.90008e-05
	LOSS [training: 2.587270983380159 | validation: 2.738165905947457]
	TIME [epoch: 6.35 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.598234854707982		[learning rate: 7.8428e-05]
	Learning Rate: 7.84285e-05
	LOSS [training: 2.598234854707982 | validation: 2.6036990817882053]
	TIME [epoch: 6.35 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.603034242599173		[learning rate: 7.786e-05]
	Learning Rate: 7.78603e-05
	LOSS [training: 2.603034242599173 | validation: 2.7785262194667624]
	TIME [epoch: 6.35 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6062322066266725		[learning rate: 7.7296e-05]
	Learning Rate: 7.72962e-05
	LOSS [training: 2.6062322066266725 | validation: 2.6883978307877956]
	TIME [epoch: 6.37 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.589171970933574		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 2.589171970933574 | validation: 2.6873148011573207]
	TIME [epoch: 6.38 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.59023443287504		[learning rate: 7.618e-05]
	Learning Rate: 7.61802e-05
	LOSS [training: 2.59023443287504 | validation: 2.6194671460011985]
	TIME [epoch: 6.35 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5959315503867515		[learning rate: 7.5628e-05]
	Learning Rate: 7.56283e-05
	LOSS [training: 2.5959315503867515 | validation: 2.6578917671860074]
	TIME [epoch: 6.36 sec]
EPOCH 723/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.604851292208319		[learning rate: 7.508e-05]
	Learning Rate: 7.50804e-05
	LOSS [training: 2.604851292208319 | validation: 2.7189810768556835]
	TIME [epoch: 6.35 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.593893473965847		[learning rate: 7.4536e-05]
	Learning Rate: 7.45364e-05
	LOSS [training: 2.593893473965847 | validation: 2.727006186537536]
	TIME [epoch: 6.36 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.590027468512389		[learning rate: 7.3996e-05]
	Learning Rate: 7.39964e-05
	LOSS [training: 2.590027468512389 | validation: 2.634480179557399]
	TIME [epoch: 6.41 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5843907822698946		[learning rate: 7.346e-05]
	Learning Rate: 7.34603e-05
	LOSS [training: 2.5843907822698946 | validation: 2.6785862311577215]
	TIME [epoch: 6.36 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5777081264339294		[learning rate: 7.2928e-05]
	Learning Rate: 7.29281e-05
	LOSS [training: 2.5777081264339294 | validation: 2.743066756704743]
	TIME [epoch: 6.35 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5843863547325285		[learning rate: 7.24e-05]
	Learning Rate: 7.23997e-05
	LOSS [training: 2.5843863547325285 | validation: 2.7051031131462833]
	TIME [epoch: 6.35 sec]
EPOCH 729/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5820928328542343		[learning rate: 7.1875e-05]
	Learning Rate: 7.18752e-05
	LOSS [training: 2.5820928328542343 | validation: 2.6529761399846903]
	TIME [epoch: 6.35 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5774240111744806		[learning rate: 7.1354e-05]
	Learning Rate: 7.13545e-05
	LOSS [training: 2.5774240111744806 | validation: 2.68602971067096]
	TIME [epoch: 6.39 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5778777402875184		[learning rate: 7.0838e-05]
	Learning Rate: 7.08375e-05
	LOSS [training: 2.5778777402875184 | validation: 2.7572721795754545]
	TIME [epoch: 6.38 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.577261263692498		[learning rate: 7.0324e-05]
	Learning Rate: 7.03243e-05
	LOSS [training: 2.577261263692498 | validation: 2.6334002262684777]
	TIME [epoch: 6.36 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5806127134031067		[learning rate: 6.9815e-05]
	Learning Rate: 6.98148e-05
	LOSS [training: 2.5806127134031067 | validation: 2.6582337138072596]
	TIME [epoch: 6.35 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.572684535375488		[learning rate: 6.9309e-05]
	Learning Rate: 6.9309e-05
	LOSS [training: 2.572684535375488 | validation: 2.695521790477848]
	TIME [epoch: 6.35 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.572493154391237		[learning rate: 6.8807e-05]
	Learning Rate: 6.88069e-05
	LOSS [training: 2.572493154391237 | validation: 2.703342566149738]
	TIME [epoch: 6.36 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5730825850666883		[learning rate: 6.8308e-05]
	Learning Rate: 6.83084e-05
	LOSS [training: 2.5730825850666883 | validation: 2.6694962295787743]
	TIME [epoch: 6.41 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5820888138566622		[learning rate: 6.7813e-05]
	Learning Rate: 6.78134e-05
	LOSS [training: 2.5820888138566622 | validation: 2.6378660415351822]
	TIME [epoch: 6.36 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5747604102728903		[learning rate: 6.7322e-05]
	Learning Rate: 6.73221e-05
	LOSS [training: 2.5747604102728903 | validation: 2.7026884747233435]
	TIME [epoch: 6.36 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5864531676563614		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 2.5864531676563614 | validation: 2.7255463566214626]
	TIME [epoch: 6.36 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5850693364409487		[learning rate: 6.635e-05]
	Learning Rate: 6.63502e-05
	LOSS [training: 2.5850693364409487 | validation: 2.6657990081203953]
	TIME [epoch: 6.36 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.550896767854479		[learning rate: 6.587e-05]
	Learning Rate: 6.58695e-05
	LOSS [training: 2.550896767854479 | validation: 2.699361229651332]
	TIME [epoch: 6.41 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6042284359934467		[learning rate: 6.5392e-05]
	Learning Rate: 6.53923e-05
	LOSS [training: 2.6042284359934467 | validation: 2.779042992502932]
	TIME [epoch: 6.35 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6006734717320343		[learning rate: 6.4919e-05]
	Learning Rate: 6.49185e-05
	LOSS [training: 2.6006734717320343 | validation: 2.7417666083125374]
	TIME [epoch: 6.35 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5668231636043304		[learning rate: 6.4448e-05]
	Learning Rate: 6.44482e-05
	LOSS [training: 2.5668231636043304 | validation: 2.7165687843733766]
	TIME [epoch: 6.35 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.566184318049086		[learning rate: 6.3981e-05]
	Learning Rate: 6.39813e-05
	LOSS [training: 2.566184318049086 | validation: 2.6952120103622725]
	TIME [epoch: 6.35 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.574281055860558		[learning rate: 6.3518e-05]
	Learning Rate: 6.35177e-05
	LOSS [training: 2.574281055860558 | validation: 2.7237155294772553]
	TIME [epoch: 6.37 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5784084748699336		[learning rate: 6.3058e-05]
	Learning Rate: 6.30575e-05
	LOSS [training: 2.5784084748699336 | validation: 2.7156171010360772]
	TIME [epoch: 6.4 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5679044969680564		[learning rate: 6.2601e-05]
	Learning Rate: 6.26007e-05
	LOSS [training: 2.5679044969680564 | validation: 2.6505316339371436]
	TIME [epoch: 6.36 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5595829561799928		[learning rate: 6.2147e-05]
	Learning Rate: 6.21471e-05
	LOSS [training: 2.5595829561799928 | validation: 2.7067065576964886]
	TIME [epoch: 6.35 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5759872320456547		[learning rate: 6.1697e-05]
	Learning Rate: 6.16969e-05
	LOSS [training: 2.5759872320456547 | validation: 2.6677833575434575]
	TIME [epoch: 6.35 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5955574348813855		[learning rate: 6.125e-05]
	Learning Rate: 6.12499e-05
	LOSS [training: 2.5955574348813855 | validation: 2.6915608579887635]
	TIME [epoch: 6.35 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5464137682611883		[learning rate: 6.0806e-05]
	Learning Rate: 6.08061e-05
	LOSS [training: 2.5464137682611883 | validation: 2.636484363565092]
	TIME [epoch: 6.4 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5570046052629802		[learning rate: 6.0366e-05]
	Learning Rate: 6.03656e-05
	LOSS [training: 2.5570046052629802 | validation: 2.657490460417897]
	TIME [epoch: 6.36 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5551502620791453		[learning rate: 5.9928e-05]
	Learning Rate: 5.99283e-05
	LOSS [training: 2.5551502620791453 | validation: 2.6602595390942705]
	TIME [epoch: 6.35 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.581569759364634		[learning rate: 5.9494e-05]
	Learning Rate: 5.94941e-05
	LOSS [training: 2.581569759364634 | validation: 2.722260335263357]
	TIME [epoch: 6.35 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5733995264143337		[learning rate: 5.9063e-05]
	Learning Rate: 5.9063e-05
	LOSS [training: 2.5733995264143337 | validation: 2.7765924038308647]
	TIME [epoch: 6.35 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.576056109355189		[learning rate: 5.8635e-05]
	Learning Rate: 5.86351e-05
	LOSS [training: 2.576056109355189 | validation: 2.715457247299713]
	TIME [epoch: 6.38 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5743443091859906		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 2.5743443091859906 | validation: 2.739827599052945]
	TIME [epoch: 6.37 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5698913767599474		[learning rate: 5.7789e-05]
	Learning Rate: 5.77886e-05
	LOSS [training: 2.5698913767599474 | validation: 2.739861258296642]
	TIME [epoch: 6.36 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.574152590436632		[learning rate: 5.737e-05]
	Learning Rate: 5.73699e-05
	LOSS [training: 2.574152590436632 | validation: 2.6560668507697693]
	TIME [epoch: 6.35 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.579846863114653		[learning rate: 5.6954e-05]
	Learning Rate: 5.69543e-05
	LOSS [training: 2.579846863114653 | validation: 2.6202141992697205]
	TIME [epoch: 6.35 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.57211918573715		[learning rate: 5.6542e-05]
	Learning Rate: 5.65417e-05
	LOSS [training: 2.57211918573715 | validation: 2.6785036064555525]
	TIME [epoch: 6.35 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5712384252719116		[learning rate: 5.6132e-05]
	Learning Rate: 5.6132e-05
	LOSS [training: 2.5712384252719116 | validation: 2.668080458661254]
	TIME [epoch: 6.4 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5531714766274827		[learning rate: 5.5725e-05]
	Learning Rate: 5.57253e-05
	LOSS [training: 2.5531714766274827 | validation: 2.699237696302858]
	TIME [epoch: 6.35 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.554711253407581		[learning rate: 5.5322e-05]
	Learning Rate: 5.53216e-05
	LOSS [training: 2.554711253407581 | validation: 2.6188839498627683]
	TIME [epoch: 6.35 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.55637234818833		[learning rate: 5.4921e-05]
	Learning Rate: 5.49208e-05
	LOSS [training: 2.55637234818833 | validation: 2.66222800818628]
	TIME [epoch: 6.35 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.554696259697157		[learning rate: 5.4523e-05]
	Learning Rate: 5.45229e-05
	LOSS [training: 2.554696259697157 | validation: 2.7267400608272965]
	TIME [epoch: 6.35 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.555592616249957		[learning rate: 5.4128e-05]
	Learning Rate: 5.41279e-05
	LOSS [training: 2.555592616249957 | validation: 2.664710805098748]
	TIME [epoch: 6.39 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5385739909560927		[learning rate: 5.3736e-05]
	Learning Rate: 5.37357e-05
	LOSS [training: 2.5385739909560927 | validation: 2.635875160376669]
	TIME [epoch: 6.38 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5576020134556456		[learning rate: 5.3346e-05]
	Learning Rate: 5.33464e-05
	LOSS [training: 2.5576020134556456 | validation: 2.667794048521772]
	TIME [epoch: 6.35 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5547052506835044		[learning rate: 5.296e-05]
	Learning Rate: 5.29599e-05
	LOSS [training: 2.5547052506835044 | validation: 2.7230722138018084]
	TIME [epoch: 6.35 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.577165350132112		[learning rate: 5.2576e-05]
	Learning Rate: 5.25762e-05
	LOSS [training: 2.577165350132112 | validation: 2.6272305373276064]
	TIME [epoch: 6.35 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5418967962487327		[learning rate: 5.2195e-05]
	Learning Rate: 5.21953e-05
	LOSS [training: 2.5418967962487327 | validation: 2.6433114601452994]
	TIME [epoch: 6.36 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5535164155896393		[learning rate: 5.1817e-05]
	Learning Rate: 5.18172e-05
	LOSS [training: 2.5535164155896393 | validation: 2.6170322172196823]
	TIME [epoch: 6.4 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5349142542823175		[learning rate: 5.1442e-05]
	Learning Rate: 5.14418e-05
	LOSS [training: 2.5349142542823175 | validation: 2.6749656819489154]
	TIME [epoch: 6.35 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5597519070230303		[learning rate: 5.1069e-05]
	Learning Rate: 5.10691e-05
	LOSS [training: 2.5597519070230303 | validation: 2.764949547922957]
	TIME [epoch: 6.35 sec]
EPOCH 777/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.602308758416519		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 2.602308758416519 | validation: 2.6861784032231513]
	TIME [epoch: 6.35 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5569224206539487		[learning rate: 5.0332e-05]
	Learning Rate: 5.03318e-05
	LOSS [training: 2.5569224206539487 | validation: 2.6330064303786216]
	TIME [epoch: 6.35 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5857442492071296		[learning rate: 4.9967e-05]
	Learning Rate: 4.99671e-05
	LOSS [training: 2.5857442492071296 | validation: 2.7157220092138368]
	TIME [epoch: 6.4 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5446583150329714		[learning rate: 4.9605e-05]
	Learning Rate: 4.96051e-05
	LOSS [training: 2.5446583150329714 | validation: 2.7347191953493937]
	TIME [epoch: 6.36 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5412368731718398		[learning rate: 4.9246e-05]
	Learning Rate: 4.92457e-05
	LOSS [training: 2.5412368731718398 | validation: 2.650340294097445]
	TIME [epoch: 6.35 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.550750167549656		[learning rate: 4.8889e-05]
	Learning Rate: 4.88889e-05
	LOSS [training: 2.550750167549656 | validation: 2.6501954643453365]
	TIME [epoch: 6.35 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5575514990856716		[learning rate: 4.8535e-05]
	Learning Rate: 4.85347e-05
	LOSS [training: 2.5575514990856716 | validation: 2.6931011687055655]
	TIME [epoch: 6.35 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5445666601752563		[learning rate: 4.8183e-05]
	Learning Rate: 4.81831e-05
	LOSS [training: 2.5445666601752563 | validation: 2.6661446028765985]
	TIME [epoch: 6.37 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.544106613657591		[learning rate: 4.7834e-05]
	Learning Rate: 4.7834e-05
	LOSS [training: 2.544106613657591 | validation: 2.7279073573265116]
	TIME [epoch: 6.38 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5556216491744275		[learning rate: 4.7487e-05]
	Learning Rate: 4.74875e-05
	LOSS [training: 2.5556216491744275 | validation: 2.7396047429514203]
	TIME [epoch: 6.36 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.553615825046048		[learning rate: 4.7143e-05]
	Learning Rate: 4.71434e-05
	LOSS [training: 2.553615825046048 | validation: 2.634404948442802]
	TIME [epoch: 6.35 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.540270911683985		[learning rate: 4.6802e-05]
	Learning Rate: 4.68019e-05
	LOSS [training: 2.540270911683985 | validation: 2.743322226794013]
	TIME [epoch: 6.35 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5495320391436516		[learning rate: 4.6463e-05]
	Learning Rate: 4.64628e-05
	LOSS [training: 2.5495320391436516 | validation: 2.655235289332362]
	TIME [epoch: 6.35 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.558392922920449		[learning rate: 4.6126e-05]
	Learning Rate: 4.61262e-05
	LOSS [training: 2.558392922920449 | validation: 2.681689435118593]
	TIME [epoch: 6.41 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.552711746968151		[learning rate: 4.5792e-05]
	Learning Rate: 4.5792e-05
	LOSS [training: 2.552711746968151 | validation: 2.6893574356933554]
	TIME [epoch: 6.35 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.538117543621118		[learning rate: 4.546e-05]
	Learning Rate: 4.54602e-05
	LOSS [training: 2.538117543621118 | validation: 2.7651036181957327]
	TIME [epoch: 6.36 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5556416766366734		[learning rate: 4.5131e-05]
	Learning Rate: 4.51309e-05
	LOSS [training: 2.5556416766366734 | validation: 2.6987487442988556]
	TIME [epoch: 6.34 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5478695548895063		[learning rate: 4.4804e-05]
	Learning Rate: 4.48039e-05
	LOSS [training: 2.5478695548895063 | validation: 2.670538990336863]
	TIME [epoch: 6.35 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.536776398034076		[learning rate: 4.4479e-05]
	Learning Rate: 4.44793e-05
	LOSS [training: 2.536776398034076 | validation: 2.6521845615070827]
	TIME [epoch: 6.38 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.546098554333519		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 2.546098554333519 | validation: 2.6848621236315884]
	TIME [epoch: 6.37 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5412266686413854		[learning rate: 4.3837e-05]
	Learning Rate: 4.38371e-05
	LOSS [training: 2.5412266686413854 | validation: 2.662866473997079]
	TIME [epoch: 6.35 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5279421153388646		[learning rate: 4.352e-05]
	Learning Rate: 4.35195e-05
	LOSS [training: 2.5279421153388646 | validation: 2.6479027829923973]
	TIME [epoch: 6.34 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5519016294231562		[learning rate: 4.3204e-05]
	Learning Rate: 4.32042e-05
	LOSS [training: 2.5519016294231562 | validation: 2.7145665275419257]
	TIME [epoch: 6.35 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.542018673360647		[learning rate: 4.2891e-05]
	Learning Rate: 4.28912e-05
	LOSS [training: 2.542018673360647 | validation: 2.6080834147540406]
	TIME [epoch: 6.35 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.528247870799394		[learning rate: 4.258e-05]
	Learning Rate: 4.25805e-05
	LOSS [training: 2.528247870799394 | validation: 2.675380710622594]
	TIME [epoch: 6.4 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5440501903655024		[learning rate: 4.2272e-05]
	Learning Rate: 4.2272e-05
	LOSS [training: 2.5440501903655024 | validation: 2.6393802786032516]
	TIME [epoch: 6.36 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5209314903136013		[learning rate: 4.1966e-05]
	Learning Rate: 4.19657e-05
	LOSS [training: 2.5209314903136013 | validation: 2.713422882812495]
	TIME [epoch: 6.35 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5470856240780613		[learning rate: 4.1662e-05]
	Learning Rate: 4.16617e-05
	LOSS [training: 2.5470856240780613 | validation: 2.7169914555724706]
	TIME [epoch: 6.35 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5494158095275123		[learning rate: 4.136e-05]
	Learning Rate: 4.13599e-05
	LOSS [training: 2.5494158095275123 | validation: 2.6959090691677856]
	TIME [epoch: 6.35 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.557405674610907		[learning rate: 4.106e-05]
	Learning Rate: 4.10602e-05
	LOSS [training: 2.557405674610907 | validation: 2.6833998497396587]
	TIME [epoch: 6.39 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.549930806163332		[learning rate: 4.0763e-05]
	Learning Rate: 4.07627e-05
	LOSS [training: 2.549930806163332 | validation: 2.67918540724573]
	TIME [epoch: 6.37 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5455892066051704		[learning rate: 4.0467e-05]
	Learning Rate: 4.04674e-05
	LOSS [training: 2.5455892066051704 | validation: 2.6059318926835076]
	TIME [epoch: 6.36 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5382744475162524		[learning rate: 4.0174e-05]
	Learning Rate: 4.01742e-05
	LOSS [training: 2.5382744475162524 | validation: 2.643495731851149]
	TIME [epoch: 6.35 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.536285053724485		[learning rate: 3.9883e-05]
	Learning Rate: 3.98832e-05
	LOSS [training: 2.536285053724485 | validation: 2.654645883169729]
	TIME [epoch: 6.35 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5423562620998577		[learning rate: 3.9594e-05]
	Learning Rate: 3.95942e-05
	LOSS [training: 2.5423562620998577 | validation: 2.6823274790592473]
	TIME [epoch: 6.37 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5497922819370746		[learning rate: 3.9307e-05]
	Learning Rate: 3.93074e-05
	LOSS [training: 2.5497922819370746 | validation: 2.60578283632557]
	TIME [epoch: 6.39 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.540213752994429		[learning rate: 3.9023e-05]
	Learning Rate: 3.90226e-05
	LOSS [training: 2.540213752994429 | validation: 2.6329865576858555]
	TIME [epoch: 6.35 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.524359787950352		[learning rate: 3.874e-05]
	Learning Rate: 3.87399e-05
	LOSS [training: 2.524359787950352 | validation: 2.7075137172954307]
	TIME [epoch: 6.34 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.528545531464448		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 2.528545531464448 | validation: 2.678641119595911]
	TIME [epoch: 6.35 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5440663949643745		[learning rate: 3.8181e-05]
	Learning Rate: 3.81806e-05
	LOSS [training: 2.5440663949643745 | validation: 2.6671072247229386]
	TIME [epoch: 6.35 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.534956091219337		[learning rate: 3.7904e-05]
	Learning Rate: 3.79039e-05
	LOSS [training: 2.534956091219337 | validation: 2.611538281377585]
	TIME [epoch: 6.4 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5386609871056245		[learning rate: 3.7629e-05]
	Learning Rate: 3.76293e-05
	LOSS [training: 2.5386609871056245 | validation: 2.658111755143699]
	TIME [epoch: 6.36 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5197892847982586		[learning rate: 3.7357e-05]
	Learning Rate: 3.73567e-05
	LOSS [training: 2.5197892847982586 | validation: 2.643240500511365]
	TIME [epoch: 6.34 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5164060983656915		[learning rate: 3.7086e-05]
	Learning Rate: 3.70861e-05
	LOSS [training: 2.5164060983656915 | validation: 2.6740854866995294]
	TIME [epoch: 6.34 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5364695913125304		[learning rate: 3.6817e-05]
	Learning Rate: 3.68174e-05
	LOSS [training: 2.5364695913125304 | validation: 2.660828320114099]
	TIME [epoch: 6.35 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5316835392915262		[learning rate: 3.6551e-05]
	Learning Rate: 3.65506e-05
	LOSS [training: 2.5316835392915262 | validation: 2.6244406688693735]
	TIME [epoch: 6.37 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5391089594077414		[learning rate: 3.6286e-05]
	Learning Rate: 3.62858e-05
	LOSS [training: 2.5391089594077414 | validation: 2.592358941437128]
	TIME [epoch: 6.39 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_823.pth
	Model improved!!!
EPOCH 824/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5289396831224242		[learning rate: 3.6023e-05]
	Learning Rate: 3.60229e-05
	LOSS [training: 2.5289396831224242 | validation: 2.700656853133456]
	TIME [epoch: 6.35 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5347203392024156		[learning rate: 3.5762e-05]
	Learning Rate: 3.57619e-05
	LOSS [training: 2.5347203392024156 | validation: 2.662670950661872]
	TIME [epoch: 6.34 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.532266332230318		[learning rate: 3.5503e-05]
	Learning Rate: 3.55029e-05
	LOSS [training: 2.532266332230318 | validation: 2.6032965389156653]
	TIME [epoch: 6.35 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5527763720380694		[learning rate: 3.5246e-05]
	Learning Rate: 3.52456e-05
	LOSS [training: 2.5527763720380694 | validation: 2.65826176867838]
	TIME [epoch: 6.35 sec]
EPOCH 828/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5269113920872424		[learning rate: 3.499e-05]
	Learning Rate: 3.49903e-05
	LOSS [training: 2.5269113920872424 | validation: 2.672847591351265]
	TIME [epoch: 6.4 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.523995013720258		[learning rate: 3.4737e-05]
	Learning Rate: 3.47368e-05
	LOSS [training: 2.523995013720258 | validation: 2.618032547617357]
	TIME [epoch: 6.35 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5255034536727403		[learning rate: 3.4485e-05]
	Learning Rate: 3.44851e-05
	LOSS [training: 2.5255034536727403 | validation: 2.625461737379978]
	TIME [epoch: 6.34 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5340842582648766		[learning rate: 3.4235e-05]
	Learning Rate: 3.42353e-05
	LOSS [training: 2.5340842582648766 | validation: 2.6896508727211557]
	TIME [epoch: 6.35 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.52614314122331		[learning rate: 3.3987e-05]
	Learning Rate: 3.39872e-05
	LOSS [training: 2.52614314122331 | validation: 2.680160744835524]
	TIME [epoch: 6.35 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5207225922973815		[learning rate: 3.3741e-05]
	Learning Rate: 3.3741e-05
	LOSS [training: 2.5207225922973815 | validation: 2.647039739171793]
	TIME [epoch: 6.38 sec]
EPOCH 834/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.519280660972205		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 2.519280660972205 | validation: 2.7002841936778053]
	TIME [epoch: 6.36 sec]
EPOCH 835/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5281032464928095		[learning rate: 3.3254e-05]
	Learning Rate: 3.32539e-05
	LOSS [training: 2.5281032464928095 | validation: 2.6538197327850956]
	TIME [epoch: 6.34 sec]
EPOCH 836/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5183404039703725		[learning rate: 3.3013e-05]
	Learning Rate: 3.3013e-05
	LOSS [training: 2.5183404039703725 | validation: 2.622200822300135]
	TIME [epoch: 6.35 sec]
EPOCH 837/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.518238471518584		[learning rate: 3.2774e-05]
	Learning Rate: 3.27738e-05
	LOSS [training: 2.518238471518584 | validation: 2.610693473453392]
	TIME [epoch: 6.35 sec]
EPOCH 838/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5241015577947126		[learning rate: 3.2536e-05]
	Learning Rate: 3.25363e-05
	LOSS [training: 2.5241015577947126 | validation: 2.6796521624757403]
	TIME [epoch: 6.35 sec]
EPOCH 839/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5289783644398494		[learning rate: 3.2301e-05]
	Learning Rate: 3.23006e-05
	LOSS [training: 2.5289783644398494 | validation: 2.635807168311997]
	TIME [epoch: 6.4 sec]
EPOCH 840/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5104864765442603		[learning rate: 3.2067e-05]
	Learning Rate: 3.20666e-05
	LOSS [training: 2.5104864765442603 | validation: 2.620448465935884]
	TIME [epoch: 6.35 sec]
EPOCH 841/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.530757045840101		[learning rate: 3.1834e-05]
	Learning Rate: 3.18343e-05
	LOSS [training: 2.530757045840101 | validation: 2.610859102315074]
	TIME [epoch: 6.34 sec]
EPOCH 842/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.522555378118912		[learning rate: 3.1604e-05]
	Learning Rate: 3.16036e-05
	LOSS [training: 2.522555378118912 | validation: 2.6621008485961366]
	TIME [epoch: 6.35 sec]
EPOCH 843/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5183257374127086		[learning rate: 3.1375e-05]
	Learning Rate: 3.13747e-05
	LOSS [training: 2.5183257374127086 | validation: 2.6351868580910924]
	TIME [epoch: 6.35 sec]
EPOCH 844/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.507795227902784		[learning rate: 3.1147e-05]
	Learning Rate: 3.11474e-05
	LOSS [training: 2.507795227902784 | validation: 2.7087270107316814]
	TIME [epoch: 6.4 sec]
EPOCH 845/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5476098551528406		[learning rate: 3.0922e-05]
	Learning Rate: 3.09217e-05
	LOSS [training: 2.5476098551528406 | validation: 2.6617508870494726]
	TIME [epoch: 6.36 sec]
EPOCH 846/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.505209092096289		[learning rate: 3.0698e-05]
	Learning Rate: 3.06977e-05
	LOSS [training: 2.505209092096289 | validation: 2.651409310439751]
	TIME [epoch: 6.35 sec]
EPOCH 847/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5200229470981657		[learning rate: 3.0475e-05]
	Learning Rate: 3.04753e-05
	LOSS [training: 2.5200229470981657 | validation: 2.6419859570704443]
	TIME [epoch: 6.35 sec]
EPOCH 848/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.519876598136398		[learning rate: 3.0254e-05]
	Learning Rate: 3.02545e-05
	LOSS [training: 2.519876598136398 | validation: 2.6512371536604076]
	TIME [epoch: 6.35 sec]
EPOCH 849/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.527353570179269		[learning rate: 3.0035e-05]
	Learning Rate: 3.00353e-05
	LOSS [training: 2.527353570179269 | validation: 2.640859320331522]
	TIME [epoch: 6.37 sec]
EPOCH 850/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.530659755269281		[learning rate: 2.9818e-05]
	Learning Rate: 2.98177e-05
	LOSS [training: 2.530659755269281 | validation: 2.619359104179879]
	TIME [epoch: 6.38 sec]
EPOCH 851/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5174543283399418		[learning rate: 2.9602e-05]
	Learning Rate: 2.96017e-05
	LOSS [training: 2.5174543283399418 | validation: 2.627381014151]
	TIME [epoch: 6.35 sec]
EPOCH 852/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5147058248927716		[learning rate: 2.9387e-05]
	Learning Rate: 2.93872e-05
	LOSS [training: 2.5147058248927716 | validation: 2.6383741864370474]
	TIME [epoch: 6.35 sec]
EPOCH 853/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5190980661841014		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 2.5190980661841014 | validation: 2.6509486387143237]
	TIME [epoch: 6.34 sec]
EPOCH 854/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5194912875339055		[learning rate: 2.8963e-05]
	Learning Rate: 2.89629e-05
	LOSS [training: 2.5194912875339055 | validation: 2.6452863260622035]
	TIME [epoch: 6.35 sec]
EPOCH 855/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5154032755508147		[learning rate: 2.8753e-05]
	Learning Rate: 2.87531e-05
	LOSS [training: 2.5154032755508147 | validation: 2.674872657656112]
	TIME [epoch: 6.4 sec]
EPOCH 856/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5150736297372767		[learning rate: 2.8545e-05]
	Learning Rate: 2.85448e-05
	LOSS [training: 2.5150736297372767 | validation: 2.5603189926352554]
	TIME [epoch: 6.35 sec]
	Saving model to: out/model_training/model_phiq_1b_v2_20240503_101338/states/model_phiq_1b_v2_856.pth
	Model improved!!!
EPOCH 857/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5172479060352693		[learning rate: 2.8338e-05]
	Learning Rate: 2.8338e-05
	LOSS [training: 2.5172479060352693 | validation: 2.650239827241392]
	TIME [epoch: 6.36 sec]
EPOCH 858/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5196056627154846		[learning rate: 2.8133e-05]
	Learning Rate: 2.81327e-05
	LOSS [training: 2.5196056627154846 | validation: 2.6292315330662417]
	TIME [epoch: 6.35 sec]
EPOCH 859/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5206103321612234		[learning rate: 2.7929e-05]
	Learning Rate: 2.79288e-05
	LOSS [training: 2.5206103321612234 | validation: 2.6715575707305375]
	TIME [epoch: 6.35 sec]
EPOCH 860/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.513938236557848		[learning rate: 2.7726e-05]
	Learning Rate: 2.77265e-05
	LOSS [training: 2.513938236557848 | validation: 2.5918757331726368]
	TIME [epoch: 6.39 sec]
EPOCH 861/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5080123806881325		[learning rate: 2.7526e-05]
	Learning Rate: 2.75256e-05
	LOSS [training: 2.5080123806881325 | validation: 2.6305866832379268]
	TIME [epoch: 6.37 sec]
EPOCH 862/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5062225436269254		[learning rate: 2.7326e-05]
	Learning Rate: 2.73262e-05
	LOSS [training: 2.5062225436269254 | validation: 2.7003528668377936]
	TIME [epoch: 6.35 sec]
EPOCH 863/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5366858384128887		[learning rate: 2.7128e-05]
	Learning Rate: 2.71282e-05
	LOSS [training: 2.5366858384128887 | validation: 2.6174743784161776]
	TIME [epoch: 6.35 sec]
EPOCH 864/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.527565788684225		[learning rate: 2.6932e-05]
	Learning Rate: 2.69317e-05
	LOSS [training: 2.527565788684225 | validation: 2.683831639991933]
	TIME [epoch: 6.35 sec]
EPOCH 865/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5228725652334623		[learning rate: 2.6737e-05]
	Learning Rate: 2.67365e-05
	LOSS [training: 2.5228725652334623 | validation: 2.631381815708123]
	TIME [epoch: 6.36 sec]
EPOCH 866/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5121644156568887		[learning rate: 2.6543e-05]
	Learning Rate: 2.65428e-05
	LOSS [training: 2.5121644156568887 | validation: 2.6411674123059345]
	TIME [epoch: 6.39 sec]
EPOCH 867/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5197896928307393		[learning rate: 2.6351e-05]
	Learning Rate: 2.63505e-05
	LOSS [training: 2.5197896928307393 | validation: 2.6402898776091037]
	TIME [epoch: 6.36 sec]
EPOCH 868/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.510076533433277		[learning rate: 2.616e-05]
	Learning Rate: 2.61596e-05
	LOSS [training: 2.510076533433277 | validation: 2.6709811704390596]
	TIME [epoch: 6.36 sec]
EPOCH 869/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.511988539513327		[learning rate: 2.597e-05]
	Learning Rate: 2.59701e-05
	LOSS [training: 2.511988539513327 | validation: 2.7002665997381134]
	TIME [epoch: 6.35 sec]
EPOCH 870/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5368440493344853		[learning rate: 2.5782e-05]
	Learning Rate: 2.5782e-05
	LOSS [training: 2.5368440493344853 | validation: 2.658439954857623]
	TIME [epoch: 6.35 sec]
EPOCH 871/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5223121135962616		[learning rate: 2.5595e-05]
	Learning Rate: 2.55952e-05
	LOSS [training: 2.5223121135962616 | validation: 2.6059571563422668]
	TIME [epoch: 6.4 sec]
EPOCH 872/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5146010125651714		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 2.5146010125651714 | validation: 2.6599760932313172]
	TIME [epoch: 6.36 sec]
EPOCH 873/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5068357302924698		[learning rate: 2.5226e-05]
	Learning Rate: 2.52256e-05
	LOSS [training: 2.5068357302924698 | validation: 2.667461906999516]
	TIME [epoch: 6.35 sec]
EPOCH 874/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5180097318908787		[learning rate: 2.5043e-05]
	Learning Rate: 2.50429e-05
	LOSS [training: 2.5180097318908787 | validation: 2.6424509048696407]
	TIME [epoch: 6.35 sec]
EPOCH 875/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.514867345378197		[learning rate: 2.4861e-05]
	Learning Rate: 2.48614e-05
	LOSS [training: 2.514867345378197 | validation: 2.633055572352922]
	TIME [epoch: 6.34 sec]
EPOCH 876/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5082893906235606		[learning rate: 2.4681e-05]
	Learning Rate: 2.46813e-05
	LOSS [training: 2.5082893906235606 | validation: 2.647821483510527]
	TIME [epoch: 6.36 sec]
EPOCH 877/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.52510551409903		[learning rate: 2.4503e-05]
	Learning Rate: 2.45025e-05
	LOSS [training: 2.52510551409903 | validation: 2.6813558374142024]
	TIME [epoch: 6.38 sec]
EPOCH 878/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5142532180868002		[learning rate: 2.4325e-05]
	Learning Rate: 2.4325e-05
	LOSS [training: 2.5142532180868002 | validation: 2.63446704230481]
	TIME [epoch: 6.34 sec]
EPOCH 879/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5253725910799414		[learning rate: 2.4149e-05]
	Learning Rate: 2.41488e-05
	LOSS [training: 2.5253725910799414 | validation: 2.6319050751954114]
	TIME [epoch: 6.34 sec]
EPOCH 880/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.503783885785023		[learning rate: 2.3974e-05]
	Learning Rate: 2.39738e-05
	LOSS [training: 2.503783885785023 | validation: 2.6186362842975783]
	TIME [epoch: 6.34 sec]
EPOCH 881/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.519592581480878		[learning rate: 2.38e-05]
	Learning Rate: 2.38001e-05
	LOSS [training: 2.519592581480878 | validation: 2.6376087694168904]
	TIME [epoch: 6.35 sec]
EPOCH 882/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.497926449081369		[learning rate: 2.3628e-05]
	Learning Rate: 2.36277e-05
	LOSS [training: 2.497926449081369 | validation: 2.6294143653049167]
	TIME [epoch: 6.41 sec]
EPOCH 883/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4925412149096915		[learning rate: 2.3457e-05]
	Learning Rate: 2.34565e-05
	LOSS [training: 2.4925412149096915 | validation: 2.574152990441333]
	TIME [epoch: 6.35 sec]
EPOCH 884/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.499042053962006		[learning rate: 2.3287e-05]
	Learning Rate: 2.32866e-05
	LOSS [training: 2.499042053962006 | validation: 2.6230741785517506]
	TIME [epoch: 6.34 sec]
EPOCH 885/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5068525237782615		[learning rate: 2.3118e-05]
	Learning Rate: 2.31179e-05
	LOSS [training: 2.5068525237782615 | validation: 2.6213445848122627]
	TIME [epoch: 6.35 sec]
EPOCH 886/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5097020807618255		[learning rate: 2.295e-05]
	Learning Rate: 2.29504e-05
	LOSS [training: 2.5097020807618255 | validation: 2.60758277649677]
	TIME [epoch: 6.35 sec]
EPOCH 887/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.498649485633237		[learning rate: 2.2784e-05]
	Learning Rate: 2.27841e-05
	LOSS [training: 2.498649485633237 | validation: 2.6215632469568027]
	TIME [epoch: 6.38 sec]
EPOCH 888/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.513254974566486		[learning rate: 2.2619e-05]
	Learning Rate: 2.2619e-05
	LOSS [training: 2.513254974566486 | validation: 2.6277842308605943]
	TIME [epoch: 6.37 sec]
EPOCH 889/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5020966854954287		[learning rate: 2.2455e-05]
	Learning Rate: 2.24551e-05
	LOSS [training: 2.5020966854954287 | validation: 2.645219596592459]
	TIME [epoch: 6.35 sec]
EPOCH 890/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5149628519408673		[learning rate: 2.2292e-05]
	Learning Rate: 2.22925e-05
	LOSS [training: 2.5149628519408673 | validation: 2.6011909713890047]
	TIME [epoch: 6.35 sec]
EPOCH 891/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5073834379663253		[learning rate: 2.2131e-05]
	Learning Rate: 2.2131e-05
	LOSS [training: 2.5073834379663253 | validation: 2.6010959923012864]
	TIME [epoch: 6.35 sec]
EPOCH 892/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.505938058767294		[learning rate: 2.1971e-05]
	Learning Rate: 2.19706e-05
	LOSS [training: 2.505938058767294 | validation: 2.665116381512418]
	TIME [epoch: 6.36 sec]
EPOCH 893/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.504759309430039		[learning rate: 2.1811e-05]
	Learning Rate: 2.18114e-05
	LOSS [training: 2.504759309430039 | validation: 2.672995609352504]
	TIME [epoch: 6.4 sec]
EPOCH 894/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5206618579666076		[learning rate: 2.1653e-05]
	Learning Rate: 2.16534e-05
	LOSS [training: 2.5206618579666076 | validation: 2.6412093616678387]
	TIME [epoch: 6.36 sec]
EPOCH 895/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5109198120174705		[learning rate: 2.1497e-05]
	Learning Rate: 2.14965e-05
	LOSS [training: 2.5109198120174705 | validation: 2.6318447247047057]
	TIME [epoch: 6.35 sec]
EPOCH 896/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5025061985045816		[learning rate: 2.1341e-05]
	Learning Rate: 2.13408e-05
	LOSS [training: 2.5025061985045816 | validation: 2.6362882081469126]
	TIME [epoch: 6.34 sec]
EPOCH 897/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5098111739470337		[learning rate: 2.1186e-05]
	Learning Rate: 2.11862e-05
	LOSS [training: 2.5098111739470337 | validation: 2.591407714237727]
	TIME [epoch: 6.35 sec]
EPOCH 898/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.513784440851026		[learning rate: 2.1033e-05]
	Learning Rate: 2.10327e-05
	LOSS [training: 2.513784440851026 | validation: 2.6608461244903636]
	TIME [epoch: 6.39 sec]
EPOCH 899/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.503077158348437		[learning rate: 2.088e-05]
	Learning Rate: 2.08803e-05
	LOSS [training: 2.503077158348437 | validation: 2.6063398229407557]
	TIME [epoch: 6.37 sec]
EPOCH 900/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5056390834028495		[learning rate: 2.0729e-05]
	Learning Rate: 2.0729e-05
	LOSS [training: 2.5056390834028495 | validation: 2.6147573434370432]
	TIME [epoch: 6.34 sec]
EPOCH 901/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4961241130096465		[learning rate: 2.0579e-05]
	Learning Rate: 2.05789e-05
	LOSS [training: 2.4961241130096465 | validation: 2.604369192083359]
	TIME [epoch: 6.35 sec]
EPOCH 902/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5007792362065455		[learning rate: 2.043e-05]
	Learning Rate: 2.04298e-05
	LOSS [training: 2.5007792362065455 | validation: 2.6145245836580404]
	TIME [epoch: 6.34 sec]
EPOCH 903/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4978615752386117		[learning rate: 2.0282e-05]
	Learning Rate: 2.02818e-05
	LOSS [training: 2.4978615752386117 | validation: 2.5902515405054567]
	TIME [epoch: 6.36 sec]
EPOCH 904/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4906958379233433		[learning rate: 2.0135e-05]
	Learning Rate: 2.01348e-05
	LOSS [training: 2.4906958379233433 | validation: 2.592253743559435]
	TIME [epoch: 6.4 sec]
EPOCH 905/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5126015740853473		[learning rate: 1.9989e-05]
	Learning Rate: 1.99889e-05
	LOSS [training: 2.5126015740853473 | validation: 2.6113566442600593]
	TIME [epoch: 6.35 sec]
EPOCH 906/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5037426686353474		[learning rate: 1.9844e-05]
	Learning Rate: 1.98441e-05
	LOSS [training: 2.5037426686353474 | validation: 2.6232483759138407]
	TIME [epoch: 6.35 sec]
EPOCH 907/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.508019731676961		[learning rate: 1.97e-05]
	Learning Rate: 1.97003e-05
	LOSS [training: 2.508019731676961 | validation: 2.6295374387659054]
	TIME [epoch: 6.35 sec]
EPOCH 908/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5088680669950527		[learning rate: 1.9558e-05]
	Learning Rate: 1.95576e-05
	LOSS [training: 2.5088680669950527 | validation: 2.630320567885941]
	TIME [epoch: 6.34 sec]
EPOCH 909/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5054625994760036		[learning rate: 1.9416e-05]
	Learning Rate: 1.94159e-05
	LOSS [training: 2.5054625994760036 | validation: 2.6231003293322273]
	TIME [epoch: 6.39 sec]
EPOCH 910/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.496875104504408		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 2.496875104504408 | validation: 2.6114932598150054]
	TIME [epoch: 6.36 sec]
EPOCH 911/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4983146468843325		[learning rate: 1.9136e-05]
	Learning Rate: 1.91356e-05
	LOSS [training: 2.4983146468843325 | validation: 2.615727289684159]
	TIME [epoch: 6.35 sec]
EPOCH 912/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4978720928966944		[learning rate: 1.8997e-05]
	Learning Rate: 1.8997e-05
	LOSS [training: 2.4978720928966944 | validation: 2.623564320756741]
	TIME [epoch: 6.35 sec]
EPOCH 913/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.504718565048432		[learning rate: 1.8859e-05]
	Learning Rate: 1.88593e-05
	LOSS [training: 2.504718565048432 | validation: 2.6241947496508926]
	TIME [epoch: 6.35 sec]
EPOCH 914/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.503751126024885		[learning rate: 1.8723e-05]
	Learning Rate: 1.87227e-05
	LOSS [training: 2.503751126024885 | validation: 2.6508048606858727]
	TIME [epoch: 6.36 sec]
EPOCH 915/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4859914475517795		[learning rate: 1.8587e-05]
	Learning Rate: 1.85871e-05
	LOSS [training: 2.4859914475517795 | validation: 2.6358562981615328]
	TIME [epoch: 6.38 sec]
EPOCH 916/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5116910138080817		[learning rate: 1.8452e-05]
	Learning Rate: 1.84524e-05
	LOSS [training: 2.5116910138080817 | validation: 2.6524660503644615]
	TIME [epoch: 6.35 sec]
EPOCH 917/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5100665372410087		[learning rate: 1.8319e-05]
	Learning Rate: 1.83187e-05
	LOSS [training: 2.5100665372410087 | validation: 2.6112007531929726]
	TIME [epoch: 6.35 sec]
EPOCH 918/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.525396690961942		[learning rate: 1.8186e-05]
	Learning Rate: 1.8186e-05
	LOSS [training: 2.525396690961942 | validation: 2.6044450259219136]
	TIME [epoch: 6.35 sec]
EPOCH 919/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5098674450604066		[learning rate: 1.8054e-05]
	Learning Rate: 1.80542e-05
	LOSS [training: 2.5098674450604066 | validation: 2.588325942176221]
	TIME [epoch: 6.35 sec]
EPOCH 920/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.50776403824608		[learning rate: 1.7923e-05]
	Learning Rate: 1.79234e-05
	LOSS [training: 2.50776403824608 | validation: 2.6074852807541786]
	TIME [epoch: 6.41 sec]
EPOCH 921/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.49892439090776		[learning rate: 1.7794e-05]
	Learning Rate: 1.77936e-05
	LOSS [training: 2.49892439090776 | validation: 2.621821165612525]
	TIME [epoch: 6.36 sec]
EPOCH 922/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5075934186440714		[learning rate: 1.7665e-05]
	Learning Rate: 1.76647e-05
	LOSS [training: 2.5075934186440714 | validation: 2.6277994410545977]
	TIME [epoch: 6.35 sec]
EPOCH 923/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.501550971703739		[learning rate: 1.7537e-05]
	Learning Rate: 1.75367e-05
	LOSS [training: 2.501550971703739 | validation: 2.6196638325442496]
	TIME [epoch: 6.35 sec]
EPOCH 924/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5130433447675578		[learning rate: 1.741e-05]
	Learning Rate: 1.74096e-05
	LOSS [training: 2.5130433447675578 | validation: 2.663583294501992]
	TIME [epoch: 6.35 sec]
EPOCH 925/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4995907288721213		[learning rate: 1.7284e-05]
	Learning Rate: 1.72835e-05
	LOSS [training: 2.4995907288721213 | validation: 2.6222924735206066]
	TIME [epoch: 6.38 sec]
EPOCH 926/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.49885383409624		[learning rate: 1.7158e-05]
	Learning Rate: 1.71583e-05
	LOSS [training: 2.49885383409624 | validation: 2.6260717447702344]
	TIME [epoch: 6.37 sec]
EPOCH 927/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5044135576137943		[learning rate: 1.7034e-05]
	Learning Rate: 1.7034e-05
	LOSS [training: 2.5044135576137943 | validation: 2.599446662590206]
	TIME [epoch: 6.35 sec]
EPOCH 928/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.498800236761876		[learning rate: 1.6911e-05]
	Learning Rate: 1.69106e-05
	LOSS [training: 2.498800236761876 | validation: 2.6261710911299163]
	TIME [epoch: 6.35 sec]
EPOCH 929/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5031641127395554		[learning rate: 1.6788e-05]
	Learning Rate: 1.6788e-05
	LOSS [training: 2.5031641127395554 | validation: 2.6042439977801535]
	TIME [epoch: 6.35 sec]
EPOCH 930/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.489811698315738		[learning rate: 1.6666e-05]
	Learning Rate: 1.66664e-05
	LOSS [training: 2.489811698315738 | validation: 2.5918932896614226]
	TIME [epoch: 6.35 sec]
EPOCH 931/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4897057059841177		[learning rate: 1.6546e-05]
	Learning Rate: 1.65457e-05
	LOSS [training: 2.4897057059841177 | validation: 2.625562891438876]
	TIME [epoch: 6.41 sec]
EPOCH 932/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.494250305259405		[learning rate: 1.6426e-05]
	Learning Rate: 1.64258e-05
	LOSS [training: 2.494250305259405 | validation: 2.6358656306115877]
	TIME [epoch: 6.36 sec]
EPOCH 933/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5033332374057022		[learning rate: 1.6307e-05]
	Learning Rate: 1.63068e-05
	LOSS [training: 2.5033332374057022 | validation: 2.617486707040369]
	TIME [epoch: 6.34 sec]
EPOCH 934/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5001486504905372		[learning rate: 1.6189e-05]
	Learning Rate: 1.61887e-05
	LOSS [training: 2.5001486504905372 | validation: 2.649031164447027]
	TIME [epoch: 6.34 sec]
EPOCH 935/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.503253951712563		[learning rate: 1.6071e-05]
	Learning Rate: 1.60714e-05
	LOSS [training: 2.503253951712563 | validation: 2.651061223362484]
	TIME [epoch: 6.35 sec]
EPOCH 936/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5053092261665943		[learning rate: 1.5955e-05]
	Learning Rate: 1.59549e-05
	LOSS [training: 2.5053092261665943 | validation: 2.6331736138727635]
	TIME [epoch: 6.39 sec]
EPOCH 937/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.494557357366947		[learning rate: 1.5839e-05]
	Learning Rate: 1.58393e-05
	LOSS [training: 2.494557357366947 | validation: 2.6634490231131647]
	TIME [epoch: 6.35 sec]
EPOCH 938/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.479291466152543		[learning rate: 1.5725e-05]
	Learning Rate: 1.57246e-05
	LOSS [training: 2.479291466152543 | validation: 2.596160764233023]
	TIME [epoch: 6.35 sec]
EPOCH 939/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4937771564447266		[learning rate: 1.5611e-05]
	Learning Rate: 1.56107e-05
	LOSS [training: 2.4937771564447266 | validation: 2.6010004224392125]
	TIME [epoch: 6.34 sec]
EPOCH 940/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5024192880351284		[learning rate: 1.5498e-05]
	Learning Rate: 1.54976e-05
	LOSS [training: 2.5024192880351284 | validation: 2.61419896562208]
	TIME [epoch: 6.35 sec]
EPOCH 941/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.495951600807667		[learning rate: 1.5385e-05]
	Learning Rate: 1.53853e-05
	LOSS [training: 2.495951600807667 | validation: 2.5919414215343903]
	TIME [epoch: 6.36 sec]
EPOCH 942/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.504479306437031		[learning rate: 1.5274e-05]
	Learning Rate: 1.52738e-05
	LOSS [training: 2.504479306437031 | validation: 2.639885662293551]
	TIME [epoch: 6.38 sec]
EPOCH 943/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.490852880071644		[learning rate: 1.5163e-05]
	Learning Rate: 1.51632e-05
	LOSS [training: 2.490852880071644 | validation: 2.6190915779644404]
	TIME [epoch: 6.35 sec]
EPOCH 944/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.485225576121014		[learning rate: 1.5053e-05]
	Learning Rate: 1.50533e-05
	LOSS [training: 2.485225576121014 | validation: 2.644508547691098]
	TIME [epoch: 6.34 sec]
EPOCH 945/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5019778723364174		[learning rate: 1.4944e-05]
	Learning Rate: 1.49442e-05
	LOSS [training: 2.5019778723364174 | validation: 2.671211410861411]
	TIME [epoch: 6.34 sec]
EPOCH 946/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.500619906920399		[learning rate: 1.4836e-05]
	Learning Rate: 1.4836e-05
	LOSS [training: 2.500619906920399 | validation: 2.653845250988281]
	TIME [epoch: 6.34 sec]
EPOCH 947/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.485346830221949		[learning rate: 1.4728e-05]
	Learning Rate: 1.47285e-05
	LOSS [training: 2.485346830221949 | validation: 2.662900670452731]
	TIME [epoch: 6.39 sec]
EPOCH 948/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.51005822503981		[learning rate: 1.4622e-05]
	Learning Rate: 1.46218e-05
	LOSS [training: 2.51005822503981 | validation: 2.630561384495806]
	TIME [epoch: 6.35 sec]
EPOCH 949/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.494593858609844		[learning rate: 1.4516e-05]
	Learning Rate: 1.45158e-05
	LOSS [training: 2.494593858609844 | validation: 2.61378681350144]
	TIME [epoch: 6.34 sec]
EPOCH 950/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.495330751133368		[learning rate: 1.4411e-05]
	Learning Rate: 1.44107e-05
	LOSS [training: 2.495330751133368 | validation: 2.616948567141864]
	TIME [epoch: 6.34 sec]
EPOCH 951/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4930331601535163		[learning rate: 1.4306e-05]
	Learning Rate: 1.43063e-05
	LOSS [training: 2.4930331601535163 | validation: 2.642055268309259]
	TIME [epoch: 6.34 sec]
EPOCH 952/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4953141330837676		[learning rate: 1.4203e-05]
	Learning Rate: 1.42026e-05
	LOSS [training: 2.4953141330837676 | validation: 2.6026423390424096]
	TIME [epoch: 6.36 sec]
EPOCH 953/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4860547352215256		[learning rate: 1.41e-05]
	Learning Rate: 1.40997e-05
	LOSS [training: 2.4860547352215256 | validation: 2.642552524466695]
	TIME [epoch: 6.37 sec]
EPOCH 954/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.495897951842428		[learning rate: 1.3998e-05]
	Learning Rate: 1.39976e-05
	LOSS [training: 2.495897951842428 | validation: 2.605016490227679]
	TIME [epoch: 6.34 sec]
EPOCH 955/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.500134955611114		[learning rate: 1.3896e-05]
	Learning Rate: 1.38962e-05
	LOSS [training: 2.500134955611114 | validation: 2.5915964470066073]
	TIME [epoch: 6.34 sec]
EPOCH 956/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4978221423958766		[learning rate: 1.3795e-05]
	Learning Rate: 1.37955e-05
	LOSS [training: 2.4978221423958766 | validation: 2.63107483513582]
	TIME [epoch: 6.34 sec]
EPOCH 957/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4973575007477744		[learning rate: 1.3696e-05]
	Learning Rate: 1.36955e-05
	LOSS [training: 2.4973575007477744 | validation: 2.600188241074412]
	TIME [epoch: 6.35 sec]
EPOCH 958/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5032739407434246		[learning rate: 1.3596e-05]
	Learning Rate: 1.35963e-05
	LOSS [training: 2.5032739407434246 | validation: 2.5785241338803546]
	TIME [epoch: 6.39 sec]
EPOCH 959/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4896851378036438		[learning rate: 1.3498e-05]
	Learning Rate: 1.34978e-05
	LOSS [training: 2.4896851378036438 | validation: 2.5852487057825533]
	TIME [epoch: 6.35 sec]
EPOCH 960/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4966297554808303		[learning rate: 1.34e-05]
	Learning Rate: 1.34e-05
	LOSS [training: 2.4966297554808303 | validation: 2.6193266070545054]
	TIME [epoch: 6.34 sec]
EPOCH 961/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4978702489579394		[learning rate: 1.3303e-05]
	Learning Rate: 1.33029e-05
	LOSS [training: 2.4978702489579394 | validation: 2.6314863854867756]
	TIME [epoch: 6.34 sec]
EPOCH 962/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4983046253832555		[learning rate: 1.3207e-05]
	Learning Rate: 1.32066e-05
	LOSS [training: 2.4983046253832555 | validation: 2.601053125084595]
	TIME [epoch: 6.34 sec]
EPOCH 963/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4953075468131853		[learning rate: 1.3111e-05]
	Learning Rate: 1.31109e-05
	LOSS [training: 2.4953075468131853 | validation: 2.664053609204639]
	TIME [epoch: 6.37 sec]
EPOCH 964/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.491425798411186		[learning rate: 1.3016e-05]
	Learning Rate: 1.30159e-05
	LOSS [training: 2.491425798411186 | validation: 2.612981733173034]
	TIME [epoch: 6.36 sec]
EPOCH 965/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4928341056696737		[learning rate: 1.2922e-05]
	Learning Rate: 1.29216e-05
	LOSS [training: 2.4928341056696737 | validation: 2.612126790951236]
	TIME [epoch: 6.34 sec]
EPOCH 966/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5062769026635436		[learning rate: 1.2828e-05]
	Learning Rate: 1.2828e-05
	LOSS [training: 2.5062769026635436 | validation: 2.6524177493545125]
	TIME [epoch: 6.34 sec]
EPOCH 967/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.501750452648055		[learning rate: 1.2735e-05]
	Learning Rate: 1.2735e-05
	LOSS [training: 2.501750452648055 | validation: 2.6362279570166436]
	TIME [epoch: 6.34 sec]
EPOCH 968/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4981833397090134		[learning rate: 1.2643e-05]
	Learning Rate: 1.26428e-05
	LOSS [training: 2.4981833397090134 | validation: 2.566206084990662]
	TIME [epoch: 6.34 sec]
EPOCH 969/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4870804913627134		[learning rate: 1.2551e-05]
	Learning Rate: 1.25512e-05
	LOSS [training: 2.4870804913627134 | validation: 2.6104174626107257]
	TIME [epoch: 6.4 sec]
EPOCH 970/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4969379691770426		[learning rate: 1.246e-05]
	Learning Rate: 1.24602e-05
	LOSS [training: 2.4969379691770426 | validation: 2.657741485230818]
	TIME [epoch: 6.34 sec]
EPOCH 971/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4916558847909283		[learning rate: 1.237e-05]
	Learning Rate: 1.237e-05
	LOSS [training: 2.4916558847909283 | validation: 2.605307513635794]
	TIME [epoch: 6.34 sec]
EPOCH 972/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.479280998381978		[learning rate: 1.228e-05]
	Learning Rate: 1.22803e-05
	LOSS [training: 2.479280998381978 | validation: 2.646079488233859]
	TIME [epoch: 6.34 sec]
EPOCH 973/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4916023977852046		[learning rate: 1.2191e-05]
	Learning Rate: 1.21914e-05
	LOSS [training: 2.4916023977852046 | validation: 2.616039399916758]
	TIME [epoch: 6.34 sec]
EPOCH 974/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4915865362998284		[learning rate: 1.2103e-05]
	Learning Rate: 1.21031e-05
	LOSS [training: 2.4915865362998284 | validation: 2.60285943872399]
	TIME [epoch: 6.39 sec]
EPOCH 975/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4946292280545084		[learning rate: 1.2015e-05]
	Learning Rate: 1.20154e-05
	LOSS [training: 2.4946292280545084 | validation: 2.650560200048474]
	TIME [epoch: 6.34 sec]
EPOCH 976/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.491918449483535		[learning rate: 1.1928e-05]
	Learning Rate: 1.19283e-05
	LOSS [training: 2.491918449483535 | validation: 2.6400946658351208]
	TIME [epoch: 6.34 sec]
EPOCH 977/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4992132026572653		[learning rate: 1.1842e-05]
	Learning Rate: 1.18419e-05
	LOSS [training: 2.4992132026572653 | validation: 2.6266869207711885]
	TIME [epoch: 6.34 sec]
EPOCH 978/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.484655945852282		[learning rate: 1.1756e-05]
	Learning Rate: 1.17561e-05
	LOSS [training: 2.484655945852282 | validation: 2.6022897393844007]
	TIME [epoch: 6.35 sec]
EPOCH 979/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4843089040947217		[learning rate: 1.1671e-05]
	Learning Rate: 1.16709e-05
	LOSS [training: 2.4843089040947217 | validation: 2.6073754144092263]
	TIME [epoch: 6.36 sec]
EPOCH 980/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4909638463814066		[learning rate: 1.1586e-05]
	Learning Rate: 1.15864e-05
	LOSS [training: 2.4909638463814066 | validation: 2.628527835127139]
	TIME [epoch: 6.38 sec]
EPOCH 981/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4969340458643847		[learning rate: 1.1502e-05]
	Learning Rate: 1.15024e-05
	LOSS [training: 2.4969340458643847 | validation: 2.611920887411844]
	TIME [epoch: 6.35 sec]
EPOCH 982/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.49270697463466		[learning rate: 1.1419e-05]
	Learning Rate: 1.14191e-05
	LOSS [training: 2.49270697463466 | validation: 2.621402863139118]
	TIME [epoch: 6.35 sec]
EPOCH 983/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4803502316049517		[learning rate: 1.1336e-05]
	Learning Rate: 1.13364e-05
	LOSS [training: 2.4803502316049517 | validation: 2.6455770711275663]
	TIME [epoch: 6.35 sec]
EPOCH 984/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.482965550136549		[learning rate: 1.1254e-05]
	Learning Rate: 1.12542e-05
	LOSS [training: 2.482965550136549 | validation: 2.5722529831420724]
	TIME [epoch: 6.35 sec]
EPOCH 985/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.479344030649974		[learning rate: 1.1173e-05]
	Learning Rate: 1.11727e-05
	LOSS [training: 2.479344030649974 | validation: 2.580443790751885]
	TIME [epoch: 6.4 sec]
EPOCH 986/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4915551127507984		[learning rate: 1.1092e-05]
	Learning Rate: 1.10918e-05
	LOSS [training: 2.4915551127507984 | validation: 2.6167715713247226]
	TIME [epoch: 6.35 sec]
EPOCH 987/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.489207893531924		[learning rate: 1.1011e-05]
	Learning Rate: 1.10114e-05
	LOSS [training: 2.489207893531924 | validation: 2.6339377032131948]
	TIME [epoch: 6.34 sec]
EPOCH 988/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.484804037052071		[learning rate: 1.0932e-05]
	Learning Rate: 1.09316e-05
	LOSS [training: 2.484804037052071 | validation: 2.6071110081680544]
	TIME [epoch: 6.34 sec]
EPOCH 989/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4944458102386555		[learning rate: 1.0852e-05]
	Learning Rate: 1.08524e-05
	LOSS [training: 2.4944458102386555 | validation: 2.5934723858280875]
	TIME [epoch: 6.34 sec]
EPOCH 990/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.487122920611017		[learning rate: 1.0774e-05]
	Learning Rate: 1.07738e-05
	LOSS [training: 2.487122920611017 | validation: 2.6578612762158604]
	TIME [epoch: 6.36 sec]
EPOCH 991/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4759502129454143		[learning rate: 1.0696e-05]
	Learning Rate: 1.06957e-05
	LOSS [training: 2.4759502129454143 | validation: 2.60468658807926]
	TIME [epoch: 6.37 sec]
EPOCH 992/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.471680023247831		[learning rate: 1.0618e-05]
	Learning Rate: 1.06182e-05
	LOSS [training: 2.471680023247831 | validation: 2.62249326118185]
	TIME [epoch: 6.34 sec]
EPOCH 993/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4958033304554705		[learning rate: 1.0541e-05]
	Learning Rate: 1.05413e-05
	LOSS [training: 2.4958033304554705 | validation: 2.630324246594026]
	TIME [epoch: 6.34 sec]
EPOCH 994/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4901013818963817		[learning rate: 1.0465e-05]
	Learning Rate: 1.04649e-05
	LOSS [training: 2.4901013818963817 | validation: 2.590399945562056]
	TIME [epoch: 6.34 sec]
EPOCH 995/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4927035240652877		[learning rate: 1.0389e-05]
	Learning Rate: 1.03891e-05
	LOSS [training: 2.4927035240652877 | validation: 2.6372339634530215]
	TIME [epoch: 6.36 sec]
EPOCH 996/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.489688107377444		[learning rate: 1.0314e-05]
	Learning Rate: 1.03139e-05
	LOSS [training: 2.489688107377444 | validation: 2.646160817224964]
	TIME [epoch: 6.4 sec]
EPOCH 997/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4821378265810052		[learning rate: 1.0239e-05]
	Learning Rate: 1.02391e-05
	LOSS [training: 2.4821378265810052 | validation: 2.5966193856752566]
	TIME [epoch: 6.35 sec]
EPOCH 998/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4824167537488537		[learning rate: 1.0165e-05]
	Learning Rate: 1.0165e-05
	LOSS [training: 2.4824167537488537 | validation: 2.565414586536237]
	TIME [epoch: 6.34 sec]
EPOCH 999/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4876924061864996		[learning rate: 1.0091e-05]
	Learning Rate: 1.00913e-05
	LOSS [training: 2.4876924061864996 | validation: 2.606201128467416]
	TIME [epoch: 6.33 sec]
EPOCH 1000/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4958643016884183		[learning rate: 1.0018e-05]
	Learning Rate: 1.00182e-05
	LOSS [training: 2.4958643016884183 | validation: 2.6369604646670672]
	TIME [epoch: 6.34 sec]
Finished training in 6554.240 seconds.
