Args:
Namespace(name='model_phiq_1a_v_mmd3', outdir='out/model_training/model_phiq_1a_v_mmd3', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='warmup_cosine_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1433579324

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_0.pth
EPOCH 1/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 4.774724830871887		[learning rate: 0.01015]
	Learning Rate: 0.01015
	LOSS [training: 4.774724830871887 | validation: 4.72072717635175]
	TIME [epoch: 171 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.692700880491953		[learning rate: 0.01035]
	Learning Rate: 0.01035
	LOSS [training: 4.692700880491953 | validation: 4.642995766610887]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.616936758635919		[learning rate: 0.01055]
	Learning Rate: 0.01055
	LOSS [training: 4.616936758635919 | validation: 4.539907732571788]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.505281784844808		[learning rate: 0.01075]
	Learning Rate: 0.01075
	LOSS [training: 4.505281784844808 | validation: 4.456483422518218]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.407936262114472		[learning rate: 0.01095]
	Learning Rate: 0.01095
	LOSS [training: 4.407936262114472 | validation: 4.4193477126671254]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.321975811736992		[learning rate: 0.01115]
	Learning Rate: 0.01115
	LOSS [training: 4.321975811736992 | validation: 4.838984800004962]
	TIME [epoch: 7.61 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.96340163819561		[learning rate: 0.01135]
	Learning Rate: 0.01135
	LOSS [training: 4.96340163819561 | validation: 4.890928442492699]
	TIME [epoch: 7.57 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.847099967262737		[learning rate: 0.01155]
	Learning Rate: 0.01155
	LOSS [training: 4.847099967262737 | validation: 4.5881627096702085]
	TIME [epoch: 7.57 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.638155246804297		[learning rate: 0.01175]
	Learning Rate: 0.01175
	LOSS [training: 4.638155246804297 | validation: 4.424590461157687]
	TIME [epoch: 7.58 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.152927729270952		[learning rate: 0.01195]
	Learning Rate: 0.01195
	LOSS [training: 4.152927729270952 | validation: 3.8587946312866257]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.749631332834178		[learning rate: 0.01215]
	Learning Rate: 0.01215
	LOSS [training: 3.749631332834178 | validation: 3.451581121430487]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.439942581892751		[learning rate: 0.01235]
	Learning Rate: 0.01235
	LOSS [training: 3.439942581892751 | validation: 3.457716886778119]
	TIME [epoch: 7.58 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.325249524219033		[learning rate: 0.01255]
	Learning Rate: 0.01255
	LOSS [training: 3.325249524219033 | validation: 3.044202239835551]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9846272146889397		[learning rate: 0.01275]
	Learning Rate: 0.01275
	LOSS [training: 2.9846272146889397 | validation: 2.898384855114884]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.745189017099804		[learning rate: 0.01295]
	Learning Rate: 0.01295
	LOSS [training: 2.745189017099804 | validation: 2.6632691002162225]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6146684129251594		[learning rate: 0.01315]
	Learning Rate: 0.01315
	LOSS [training: 2.6146684129251594 | validation: 2.4525134092706096]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3201557469120875		[learning rate: 0.01335]
	Learning Rate: 0.01335
	LOSS [training: 2.3201557469120875 | validation: 2.5283542951098488]
	TIME [epoch: 7.62 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3696407933253236		[learning rate: 0.01355]
	Learning Rate: 0.01355
	LOSS [training: 2.3696407933253236 | validation: 2.3197894128154344]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1665734905766647		[learning rate: 0.01375]
	Learning Rate: 0.01375
	LOSS [training: 2.1665734905766647 | validation: 2.1787125809366366]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0098665536824862		[learning rate: 0.01395]
	Learning Rate: 0.01395
	LOSS [training: 2.0098665536824862 | validation: 1.9712645370907453]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9053046575577057		[learning rate: 0.01415]
	Learning Rate: 0.01415
	LOSS [training: 1.9053046575577057 | validation: 1.893392275587374]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8385908474985313		[learning rate: 0.01435]
	Learning Rate: 0.01435
	LOSS [training: 1.8385908474985313 | validation: 1.9268810706212633]
	TIME [epoch: 7.6 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8053491602751095		[learning rate: 0.01455]
	Learning Rate: 0.01455
	LOSS [training: 1.8053491602751095 | validation: 1.7691549186630646]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7403295737056854		[learning rate: 0.01475]
	Learning Rate: 0.01475
	LOSS [training: 1.7403295737056854 | validation: 1.7191066485552744]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7023997148428827		[learning rate: 0.01495]
	Learning Rate: 0.01495
	LOSS [training: 1.7023997148428827 | validation: 1.6715923200539597]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.644701138127403		[learning rate: 0.01515]
	Learning Rate: 0.01515
	LOSS [training: 1.644701138127403 | validation: 1.6390405830111372]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5960326890045224		[learning rate: 0.01535]
	Learning Rate: 0.01535
	LOSS [training: 1.5960326890045224 | validation: 1.5885482053581856]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5963563885383796		[learning rate: 0.01555]
	Learning Rate: 0.01555
	LOSS [training: 1.5963563885383796 | validation: 1.6030004316032898]
	TIME [epoch: 7.65 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.581620026385743		[learning rate: 0.01575]
	Learning Rate: 0.01575
	LOSS [training: 1.581620026385743 | validation: 1.5217527789634668]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5642998631876088		[learning rate: 0.01595]
	Learning Rate: 0.01595
	LOSS [training: 1.5642998631876088 | validation: 1.5920610175163878]
	TIME [epoch: 7.62 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5324946063775822		[learning rate: 0.01615]
	Learning Rate: 0.01615
	LOSS [training: 1.5324946063775822 | validation: 1.4969110092750715]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4944559618999496		[learning rate: 0.01635]
	Learning Rate: 0.01635
	LOSS [training: 1.4944559618999496 | validation: 1.4785905614140895]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4837657078248088		[learning rate: 0.01655]
	Learning Rate: 0.01655
	LOSS [training: 1.4837657078248088 | validation: 1.4476319765662047]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4212271573075184		[learning rate: 0.01675]
	Learning Rate: 0.01675
	LOSS [training: 1.4212271573075184 | validation: 1.3729115931829088]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3930599452546666		[learning rate: 0.01695]
	Learning Rate: 0.01695
	LOSS [training: 1.3930599452546666 | validation: 1.4082547979522355]
	TIME [epoch: 7.58 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4142331043784016		[learning rate: 0.01715]
	Learning Rate: 0.01715
	LOSS [training: 1.4142331043784016 | validation: 1.367246120573129]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.356603417277729		[learning rate: 0.01735]
	Learning Rate: 0.01735
	LOSS [training: 1.356603417277729 | validation: 1.3684258989463818]
	TIME [epoch: 7.64 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3525905565334368		[learning rate: 0.01755]
	Learning Rate: 0.01755
	LOSS [training: 1.3525905565334368 | validation: 1.311736682229878]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3098206778264845		[learning rate: 0.01775]
	Learning Rate: 0.01775
	LOSS [training: 1.3098206778264845 | validation: 1.3031829624899607]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3046930292516024		[learning rate: 0.01795]
	Learning Rate: 0.01795
	LOSS [training: 1.3046930292516024 | validation: 1.3456192102189934]
	TIME [epoch: 7.59 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3060950160565967		[learning rate: 0.01815]
	Learning Rate: 0.01815
	LOSS [training: 1.3060950160565967 | validation: 1.325193854899513]
	TIME [epoch: 7.62 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2788236107519808		[learning rate: 0.01835]
	Learning Rate: 0.01835
	LOSS [training: 1.2788236107519808 | validation: 1.3018548006718187]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.265688313465171		[learning rate: 0.01855]
	Learning Rate: 0.01855
	LOSS [training: 1.265688313465171 | validation: 1.277860818747744]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2662456646799187		[learning rate: 0.01875]
	Learning Rate: 0.01875
	LOSS [training: 1.2662456646799187 | validation: 1.2896815177204506]
	TIME [epoch: 7.62 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2517776937804574		[learning rate: 0.01895]
	Learning Rate: 0.01895
	LOSS [training: 1.2517776937804574 | validation: 1.242893254996754]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2377310402966708		[learning rate: 0.01915]
	Learning Rate: 0.01915
	LOSS [training: 1.2377310402966708 | validation: 1.2079908728046607]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2462782664025482		[learning rate: 0.01935]
	Learning Rate: 0.01935
	LOSS [training: 1.2462782664025482 | validation: 1.2473580244776161]
	TIME [epoch: 7.61 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2416097252675025		[learning rate: 0.01955]
	Learning Rate: 0.01955
	LOSS [training: 1.2416097252675025 | validation: 1.2695368811104708]
	TIME [epoch: 7.62 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2533883593681936		[learning rate: 0.01975]
	Learning Rate: 0.01975
	LOSS [training: 1.2533883593681936 | validation: 1.2442262019532766]
	TIME [epoch: 7.62 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2397934233636037		[learning rate: 0.01995]
	Learning Rate: 0.01995
	LOSS [training: 1.2397934233636037 | validation: 1.237915345378461]
	TIME [epoch: 7.63 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2349732053697577		[learning rate: 0.02]
	Learning Rate: 0.02
	LOSS [training: 1.2349732053697577 | validation: 1.1872848730992023]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.249413954849738		[learning rate: 0.02]
	Learning Rate: 0.02
	LOSS [training: 1.249413954849738 | validation: 1.199340693914793]
	TIME [epoch: 7.5 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2000039349410252		[learning rate: 0.02]
	Learning Rate: 0.0199999
	LOSS [training: 1.2000039349410252 | validation: 1.1615568308011524]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.211730903104548		[learning rate: 0.02]
	Learning Rate: 0.0199998
	LOSS [training: 1.211730903104548 | validation: 1.2106162176326807]
	TIME [epoch: 7.57 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1968722103991936		[learning rate: 0.02]
	Learning Rate: 0.0199997
	LOSS [training: 1.1968722103991936 | validation: 1.1828313920039903]
	TIME [epoch: 7.6 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1738974686678967		[learning rate: 0.02]
	Learning Rate: 0.0199995
	LOSS [training: 1.1738974686678967 | validation: 1.1643962987504755]
	TIME [epoch: 7.61 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.182706138059751		[learning rate: 0.019999]
	Learning Rate: 0.0199994
	LOSS [training: 1.182706138059751 | validation: 1.1583518305433966]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1967190135863273		[learning rate: 0.019999]
	Learning Rate: 0.0199992
	LOSS [training: 1.1967190135863273 | validation: 1.2632276190946956]
	TIME [epoch: 7.58 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3484601802090452		[learning rate: 0.019999]
	Learning Rate: 0.019999
	LOSS [training: 1.3484601802090452 | validation: 1.2746913068541317]
	TIME [epoch: 7.6 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2391804396330104		[learning rate: 0.019999]
	Learning Rate: 0.0199987
	LOSS [training: 1.2391804396330104 | validation: 1.2182137505872244]
	TIME [epoch: 7.65 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2128303139261505		[learning rate: 0.019998]
	Learning Rate: 0.0199984
	LOSS [training: 1.2128303139261505 | validation: 1.1895736181000818]
	TIME [epoch: 7.55 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2003585657092946		[learning rate: 0.019998]
	Learning Rate: 0.0199981
	LOSS [training: 1.2003585657092946 | validation: 1.1749425350151235]
	TIME [epoch: 7.6 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1818956553192388		[learning rate: 0.019998]
	Learning Rate: 0.0199978
	LOSS [training: 1.1818956553192388 | validation: 1.129827049752173]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2035603714849508		[learning rate: 0.019997]
	Learning Rate: 0.0199974
	LOSS [training: 1.2035603714849508 | validation: 1.1449937083928843]
	TIME [epoch: 7.61 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1611937026626382		[learning rate: 0.019997]
	Learning Rate: 0.019997
	LOSS [training: 1.1611937026626382 | validation: 1.1259978786501632]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1466930230476804		[learning rate: 0.019997]
	Learning Rate: 0.0199966
	LOSS [training: 1.1466930230476804 | validation: 1.204166115045437]
	TIME [epoch: 7.62 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.181212502315426		[learning rate: 0.019996]
	Learning Rate: 0.0199962
	LOSS [training: 1.181212502315426 | validation: 1.1650915942708413]
	TIME [epoch: 7.61 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1559364729702566		[learning rate: 0.019996]
	Learning Rate: 0.0199957
	LOSS [training: 1.1559364729702566 | validation: 1.1563647857494601]
	TIME [epoch: 7.61 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1476411164575113		[learning rate: 0.019995]
	Learning Rate: 0.0199952
	LOSS [training: 1.1476411164575113 | validation: 1.135519786367159]
	TIME [epoch: 7.65 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1384577764025687		[learning rate: 0.019995]
	Learning Rate: 0.0199947
	LOSS [training: 1.1384577764025687 | validation: 1.1221601088910065]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1267382677504085		[learning rate: 0.019994]
	Learning Rate: 0.0199941
	LOSS [training: 1.1267382677504085 | validation: 1.179537085825776]
	TIME [epoch: 7.55 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1571494042657984		[learning rate: 0.019994]
	Learning Rate: 0.0199935
	LOSS [training: 1.1571494042657984 | validation: 1.1284382289432493]
	TIME [epoch: 7.62 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1249058575521063		[learning rate: 0.019993]
	Learning Rate: 0.0199929
	LOSS [training: 1.1249058575521063 | validation: 1.1153946668503287]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.112235983720178		[learning rate: 0.019992]
	Learning Rate: 0.0199923
	LOSS [training: 1.112235983720178 | validation: 1.0959344224890362]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1048617128786407		[learning rate: 0.019992]
	Learning Rate: 0.0199916
	LOSS [training: 1.1048617128786407 | validation: 1.0823345776173385]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.096535442037832		[learning rate: 0.019991]
	Learning Rate: 0.0199909
	LOSS [training: 1.096535442037832 | validation: 1.0600307416233958]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0905059721646488		[learning rate: 0.01999]
	Learning Rate: 0.0199902
	LOSS [training: 1.0905059721646488 | validation: 1.0664622218294044]
	TIME [epoch: 7.6 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1095963569789271		[learning rate: 0.019989]
	Learning Rate: 0.0199895
	LOSS [training: 1.1095963569789271 | validation: 1.0880235133053806]
	TIME [epoch: 7.66 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.075883820477224		[learning rate: 0.019989]
	Learning Rate: 0.0199887
	LOSS [training: 1.075883820477224 | validation: 1.036638790032053]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.047314430235842		[learning rate: 0.019988]
	Learning Rate: 0.0199879
	LOSS [training: 1.047314430235842 | validation: 0.9956533063451505]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.081379835244454		[learning rate: 0.019987]
	Learning Rate: 0.0199871
	LOSS [training: 1.081379835244454 | validation: 1.0981232319394971]
	TIME [epoch: 7.58 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0678268301422746		[learning rate: 0.019986]
	Learning Rate: 0.0199862
	LOSS [training: 1.0678268301422746 | validation: 1.0490037680305528]
	TIME [epoch: 7.58 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0411056875643758		[learning rate: 0.019985]
	Learning Rate: 0.0199853
	LOSS [training: 1.0411056875643758 | validation: 1.0204107565430258]
	TIME [epoch: 7.63 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0304392543746175		[learning rate: 0.019984]
	Learning Rate: 0.0199844
	LOSS [training: 1.0304392543746175 | validation: 0.9932070354254028]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0430377967330289		[learning rate: 0.019984]
	Learning Rate: 0.0199835
	LOSS [training: 1.0430377967330289 | validation: 0.9503094793537221]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9978214328294673		[learning rate: 0.019983]
	Learning Rate: 0.0199825
	LOSS [training: 0.9978214328294673 | validation: 1.016007133165911]
	TIME [epoch: 7.6 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9830423664124287		[learning rate: 0.019982]
	Learning Rate: 0.0199816
	LOSS [training: 0.9830423664124287 | validation: 0.8798319435881679]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9604340724867264		[learning rate: 0.019981]
	Learning Rate: 0.0199805
	LOSS [training: 0.9604340724867264 | validation: 0.9284865908749387]
	TIME [epoch: 7.61 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0059279276157527		[learning rate: 0.019979]
	Learning Rate: 0.0199795
	LOSS [training: 1.0059279276157527 | validation: 1.0833476031986107]
	TIME [epoch: 7.58 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0547122498576362		[learning rate: 0.019978]
	Learning Rate: 0.0199784
	LOSS [training: 1.0547122498576362 | validation: 1.0124526614180618]
	TIME [epoch: 7.61 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9975663320959846		[learning rate: 0.019977]
	Learning Rate: 0.0199773
	LOSS [training: 0.9975663320959846 | validation: 0.9727463158515814]
	TIME [epoch: 7.61 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9606891003849892		[learning rate: 0.019976]
	Learning Rate: 0.0199762
	LOSS [training: 0.9606891003849892 | validation: 0.921343308336771]
	TIME [epoch: 7.65 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9339857232677389		[learning rate: 0.019975]
	Learning Rate: 0.019975
	LOSS [training: 0.9339857232677389 | validation: 0.9549738907470828]
	TIME [epoch: 7.61 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.911438293972478		[learning rate: 0.019974]
	Learning Rate: 0.0199739
	LOSS [training: 0.911438293972478 | validation: 0.9307219138968186]
	TIME [epoch: 7.61 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9377538404722628		[learning rate: 0.019973]
	Learning Rate: 0.0199727
	LOSS [training: 0.9377538404722628 | validation: 0.8824674224209792]
	TIME [epoch: 7.62 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9004892279681858		[learning rate: 0.019971]
	Learning Rate: 0.0199714
	LOSS [training: 0.9004892279681858 | validation: 1.121001291235398]
	TIME [epoch: 7.63 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1034724019983175		[learning rate: 0.01997]
	Learning Rate: 0.0199702
	LOSS [training: 1.1034724019983175 | validation: 0.9686995707852304]
	TIME [epoch: 7.64 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.946826907081836		[learning rate: 0.019969]
	Learning Rate: 0.0199689
	LOSS [training: 0.946826907081836 | validation: 0.9384279061084526]
	TIME [epoch: 7.61 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9186123889640254		[learning rate: 0.019968]
	Learning Rate: 0.0199675
	LOSS [training: 0.9186123889640254 | validation: 0.8998391762746867]
	TIME [epoch: 7.63 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8985407143255071		[learning rate: 0.019966]
	Learning Rate: 0.0199662
	LOSS [training: 0.8985407143255071 | validation: 0.8491181904588474]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9114374299386443		[learning rate: 0.019965]
	Learning Rate: 0.0199648
	LOSS [training: 0.9114374299386443 | validation: 0.829356193102494]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8767625221635731		[learning rate: 0.019963]
	Learning Rate: 0.0199634
	LOSS [training: 0.8767625221635731 | validation: 0.8985748085974936]
	TIME [epoch: 7.64 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.895978437541945		[learning rate: 0.019962]
	Learning Rate: 0.019962
	LOSS [training: 0.895978437541945 | validation: 0.8245516837807909]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.836002336415885		[learning rate: 0.019961]
	Learning Rate: 0.0199606
	LOSS [training: 0.836002336415885 | validation: 0.843389623145963]
	TIME [epoch: 7.62 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8688650815370091		[learning rate: 0.019959]
	Learning Rate: 0.0199591
	LOSS [training: 0.8688650815370091 | validation: 0.7733360607256021]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8584629791473712		[learning rate: 0.019958]
	Learning Rate: 0.0199576
	LOSS [training: 0.8584629791473712 | validation: 0.9746002217982002]
	TIME [epoch: 7.67 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1278108583723006		[learning rate: 0.019956]
	Learning Rate: 0.019956
	LOSS [training: 1.1278108583723006 | validation: 1.1561036669327613]
	TIME [epoch: 7.63 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.034691821964532		[learning rate: 0.019954]
	Learning Rate: 0.0199545
	LOSS [training: 1.034691821964532 | validation: 0.9357677644551248]
	TIME [epoch: 7.63 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9264714893910799		[learning rate: 0.019953]
	Learning Rate: 0.0199529
	LOSS [training: 0.9264714893910799 | validation: 0.91299321552589]
	TIME [epoch: 7.63 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8951163833204765		[learning rate: 0.019951]
	Learning Rate: 0.0199513
	LOSS [training: 0.8951163833204765 | validation: 0.8855493882381973]
	TIME [epoch: 7.64 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9249748267014135		[learning rate: 0.01995]
	Learning Rate: 0.0199496
	LOSS [training: 0.9249748267014135 | validation: 0.902289881535943]
	TIME [epoch: 7.65 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8692530563798242		[learning rate: 0.019948]
	Learning Rate: 0.0199479
	LOSS [training: 0.8692530563798242 | validation: 0.8083581221448912]
	TIME [epoch: 7.62 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8717407222419947		[learning rate: 0.019946]
	Learning Rate: 0.0199462
	LOSS [training: 0.8717407222419947 | validation: 0.7986564165381217]
	TIME [epoch: 7.63 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8110483123535239		[learning rate: 0.019945]
	Learning Rate: 0.0199445
	LOSS [training: 0.8110483123535239 | validation: 0.7819724967453524]
	TIME [epoch: 7.63 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.043948990890638		[learning rate: 0.019943]
	Learning Rate: 0.0199428
	LOSS [training: 1.043948990890638 | validation: 0.9859676354928604]
	TIME [epoch: 7.68 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9231638592165681		[learning rate: 0.019941]
	Learning Rate: 0.019941
	LOSS [training: 0.9231638592165681 | validation: 0.8533740537177117]
	TIME [epoch: 7.63 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8424723193043195		[learning rate: 0.019939]
	Learning Rate: 0.0199392
	LOSS [training: 0.8424723193043195 | validation: 0.7775456543018117]
	TIME [epoch: 7.63 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8429939803681616		[learning rate: 0.019937]
	Learning Rate: 0.0199374
	LOSS [training: 0.8429939803681616 | validation: 0.8191634177945655]
	TIME [epoch: 7.63 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8115065651127286		[learning rate: 0.019935]
	Learning Rate: 0.0199355
	LOSS [training: 0.8115065651127286 | validation: 0.808872346758833]
	TIME [epoch: 7.64 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8460164901524565		[learning rate: 0.019934]
	Learning Rate: 0.0199336
	LOSS [training: 0.8460164901524565 | validation: 0.7852963558086443]
	TIME [epoch: 7.67 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7980708892590955		[learning rate: 0.019932]
	Learning Rate: 0.0199317
	LOSS [training: 0.7980708892590955 | validation: 0.7787643285385566]
	TIME [epoch: 7.62 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8287044951758863		[learning rate: 0.01993]
	Learning Rate: 0.0199297
	LOSS [training: 0.8287044951758863 | validation: 0.7608485772775639]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.768074960111834		[learning rate: 0.019928]
	Learning Rate: 0.0199278
	LOSS [training: 0.768074960111834 | validation: 0.8069075503719132]
	TIME [epoch: 7.62 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8058310682112986		[learning rate: 0.019926]
	Learning Rate: 0.0199258
	LOSS [training: 0.8058310682112986 | validation: 0.6875922302173582]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7205743724149265		[learning rate: 0.019924]
	Learning Rate: 0.0199238
	LOSS [training: 0.7205743724149265 | validation: 0.720813379381334]
	TIME [epoch: 7.53 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8008329865784101		[learning rate: 0.019922]
	Learning Rate: 0.0199217
	LOSS [training: 0.8008329865784101 | validation: 0.7819062205272245]
	TIME [epoch: 7.52 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7731759759785182		[learning rate: 0.01992]
	Learning Rate: 0.0199196
	LOSS [training: 0.7731759759785182 | validation: 0.6809283871388723]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.773702393553118		[learning rate: 0.019918]
	Learning Rate: 0.0199175
	LOSS [training: 0.773702393553118 | validation: 0.7493603020062791]
	TIME [epoch: 7.6 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7084210323988919		[learning rate: 0.019915]
	Learning Rate: 0.0199154
	LOSS [training: 0.7084210323988919 | validation: 0.7405412453408144]
	TIME [epoch: 7.63 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7641150873720202		[learning rate: 0.019913]
	Learning Rate: 0.0199132
	LOSS [training: 0.7641150873720202 | validation: 0.775216124257505]
	TIME [epoch: 7.6 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7772395679551515		[learning rate: 0.019911]
	Learning Rate: 0.019911
	LOSS [training: 0.7772395679551515 | validation: 0.7619332796337193]
	TIME [epoch: 7.59 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.818036891546623		[learning rate: 0.019909]
	Learning Rate: 0.0199088
	LOSS [training: 0.818036891546623 | validation: 0.7849082515239865]
	TIME [epoch: 7.6 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8004384628141996		[learning rate: 0.019907]
	Learning Rate: 0.0199066
	LOSS [training: 0.8004384628141996 | validation: 0.7612168206911696]
	TIME [epoch: 7.62 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7545381330406208		[learning rate: 0.019904]
	Learning Rate: 0.0199043
	LOSS [training: 0.7545381330406208 | validation: 0.6695117714642154]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.763405812889669		[learning rate: 0.019902]
	Learning Rate: 0.019902
	LOSS [training: 0.763405812889669 | validation: 0.7061726524396004]
	TIME [epoch: 7.59 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6662155920868522		[learning rate: 0.0199]
	Learning Rate: 0.0198997
	LOSS [training: 0.6662155920868522 | validation: 0.7204062608986934]
	TIME [epoch: 7.59 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8162509564865664		[learning rate: 0.019897]
	Learning Rate: 0.0198974
	LOSS [training: 0.8162509564865664 | validation: 0.7473569119645781]
	TIME [epoch: 7.6 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7307363754119667		[learning rate: 0.019895]
	Learning Rate: 0.019895
	LOSS [training: 0.7307363754119667 | validation: 0.6773482480627617]
	TIME [epoch: 7.64 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7077224018114264		[learning rate: 0.019893]
	Learning Rate: 0.0198926
	LOSS [training: 0.7077224018114264 | validation: 0.6168247500443749]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6988299689536975		[learning rate: 0.01989]
	Learning Rate: 0.0198901
	LOSS [training: 0.6988299689536975 | validation: 0.6786742371876682]
	TIME [epoch: 7.58 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6733410423191082		[learning rate: 0.019888]
	Learning Rate: 0.0198877
	LOSS [training: 0.6733410423191082 | validation: 0.8206304332521086]
	TIME [epoch: 7.6 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.723475519371125		[learning rate: 0.019885]
	Learning Rate: 0.0198852
	LOSS [training: 0.723475519371125 | validation: 0.6258321850504988]
	TIME [epoch: 7.62 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6655731276515041		[learning rate: 0.019883]
	Learning Rate: 0.0198827
	LOSS [training: 0.6655731276515041 | validation: 0.6818037207522984]
	TIME [epoch: 7.66 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6319476776300691		[learning rate: 0.01988]
	Learning Rate: 0.0198802
	LOSS [training: 0.6319476776300691 | validation: 0.9134327386127523]
	TIME [epoch: 7.62 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7241739584172957		[learning rate: 0.019878]
	Learning Rate: 0.0198776
	LOSS [training: 0.7241739584172957 | validation: 0.6428267768084381]
	TIME [epoch: 7.61 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6914918325275501		[learning rate: 0.019875]
	Learning Rate: 0.019875
	LOSS [training: 0.6914918325275501 | validation: 0.6876807850553245]
	TIME [epoch: 7.63 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.65285646404428		[learning rate: 0.019872]
	Learning Rate: 0.0198724
	LOSS [training: 0.65285646404428 | validation: 0.8702034912661887]
	TIME [epoch: 7.64 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6907707663740694		[learning rate: 0.01987]
	Learning Rate: 0.0198697
	LOSS [training: 0.6907707663740694 | validation: 0.6985067388744908]
	TIME [epoch: 7.64 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6746052742429867		[learning rate: 0.019867]
	Learning Rate: 0.0198671
	LOSS [training: 0.6746052742429867 | validation: 0.8420823118228808]
	TIME [epoch: 7.61 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6100489742853636		[learning rate: 0.019864]
	Learning Rate: 0.0198644
	LOSS [training: 0.6100489742853636 | validation: 0.8188496009046664]
	TIME [epoch: 7.62 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6345372919907992		[learning rate: 0.019862]
	Learning Rate: 0.0198616
	LOSS [training: 0.6345372919907992 | validation: 0.6770332147975004]
	TIME [epoch: 7.61 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7672524984161584		[learning rate: 0.019859]
	Learning Rate: 0.0198589
	LOSS [training: 0.7672524984161584 | validation: 0.8565429508589285]
	TIME [epoch: 7.66 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7266678905733793		[learning rate: 0.019856]
	Learning Rate: 0.0198561
	LOSS [training: 0.7266678905733793 | validation: 0.6540297366507256]
	TIME [epoch: 7.59 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5908981622251395		[learning rate: 0.019853]
	Learning Rate: 0.0198533
	LOSS [training: 0.5908981622251395 | validation: 0.7176637069733338]
	TIME [epoch: 7.59 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6578129919167987		[learning rate: 0.01985]
	Learning Rate: 0.0198505
	LOSS [training: 0.6578129919167987 | validation: 0.5368671101946862]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5758045315282168		[learning rate: 0.019848]
	Learning Rate: 0.0198476
	LOSS [training: 0.5758045315282168 | validation: 0.6318045535048473]
	TIME [epoch: 7.62 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6296421289042198		[learning rate: 0.019845]
	Learning Rate: 0.0198447
	LOSS [training: 0.6296421289042198 | validation: 0.5865986860399846]
	TIME [epoch: 7.66 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6157759798816538		[learning rate: 0.019842]
	Learning Rate: 0.0198418
	LOSS [training: 0.6157759798816538 | validation: 0.6805292781491747]
	TIME [epoch: 7.62 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6817310633122022		[learning rate: 0.019839]
	Learning Rate: 0.0198388
	LOSS [training: 0.6817310633122022 | validation: 0.6354926723174248]
	TIME [epoch: 7.61 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6455317254662437		[learning rate: 0.019836]
	Learning Rate: 0.0198359
	LOSS [training: 0.6455317254662437 | validation: 0.5632587563418254]
	TIME [epoch: 7.62 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5926054079381575		[learning rate: 0.019833]
	Learning Rate: 0.0198329
	LOSS [training: 0.5926054079381575 | validation: 0.6686772849535608]
	TIME [epoch: 7.65 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5997789437039751		[learning rate: 0.01983]
	Learning Rate: 0.0198299
	LOSS [training: 0.5997789437039751 | validation: 0.8409717054265671]
	TIME [epoch: 7.66 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6638905376687996		[learning rate: 0.019827]
	Learning Rate: 0.0198268
	LOSS [training: 0.6638905376687996 | validation: 0.6484149632744711]
	TIME [epoch: 7.62 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6146905477186523		[learning rate: 0.019824]
	Learning Rate: 0.0198237
	LOSS [training: 0.6146905477186523 | validation: 0.5894389918449021]
	TIME [epoch: 7.62 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5413858408100637		[learning rate: 0.019821]
	Learning Rate: 0.0198206
	LOSS [training: 0.5413858408100637 | validation: 0.5361698664675938]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6483892096751351		[learning rate: 0.019818]
	Learning Rate: 0.0198175
	LOSS [training: 0.6483892096751351 | validation: 0.5409041874209146]
	TIME [epoch: 7.67 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5215461276049576		[learning rate: 0.019814]
	Learning Rate: 0.0198143
	LOSS [training: 0.5215461276049576 | validation: 0.9304864913040038]
	TIME [epoch: 7.63 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6823476009627313		[learning rate: 0.019811]
	Learning Rate: 0.0198112
	LOSS [training: 0.6823476009627313 | validation: 0.5617866284500527]
	TIME [epoch: 7.62 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.561028366254464		[learning rate: 0.019808]
	Learning Rate: 0.0198079
	LOSS [training: 0.561028366254464 | validation: 0.5151392976754224]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6445434332787934		[learning rate: 0.019805]
	Learning Rate: 0.0198047
	LOSS [training: 0.6445434332787934 | validation: 0.6915064792477699]
	TIME [epoch: 7.62 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6120194122097051		[learning rate: 0.019801]
	Learning Rate: 0.0198014
	LOSS [training: 0.6120194122097051 | validation: 0.6430983826785166]
	TIME [epoch: 7.65 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.51051234303659		[learning rate: 0.019798]
	Learning Rate: 0.0197982
	LOSS [training: 0.51051234303659 | validation: 0.5416354294061094]
	TIME [epoch: 7.63 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6457763494662675		[learning rate: 0.019795]
	Learning Rate: 0.0197948
	LOSS [training: 0.6457763494662675 | validation: 0.7299850189536378]
	TIME [epoch: 7.62 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6055187279612634		[learning rate: 0.019791]
	Learning Rate: 0.0197915
	LOSS [training: 0.6055187279612634 | validation: 0.6969764588757639]
	TIME [epoch: 7.62 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5910737688217441		[learning rate: 0.019788]
	Learning Rate: 0.0197881
	LOSS [training: 0.5910737688217441 | validation: 0.5512003116722617]
	TIME [epoch: 7.64 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5223752060625697		[learning rate: 0.019785]
	Learning Rate: 0.0197847
	LOSS [training: 0.5223752060625697 | validation: 0.6774143320042796]
	TIME [epoch: 7.65 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5884143986012039		[learning rate: 0.019781]
	Learning Rate: 0.0197813
	LOSS [training: 0.5884143986012039 | validation: 0.6117154516663668]
	TIME [epoch: 7.61 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5284008256504973		[learning rate: 0.019778]
	Learning Rate: 0.0197778
	LOSS [training: 0.5284008256504973 | validation: 0.4989250012282118]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6547585644860268		[learning rate: 0.019774]
	Learning Rate: 0.0197744
	LOSS [training: 0.6547585644860268 | validation: 0.8553138031857179]
	TIME [epoch: 7.62 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6998085191908924		[learning rate: 0.019771]
	Learning Rate: 0.0197709
	LOSS [training: 0.6998085191908924 | validation: 0.6147728095398947]
	TIME [epoch: 7.68 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5881944136467572		[learning rate: 0.019767]
	Learning Rate: 0.0197673
	LOSS [training: 0.5881944136467572 | validation: 0.5142350222635101]
	TIME [epoch: 7.63 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6581110789977302		[learning rate: 0.019764]
	Learning Rate: 0.0197638
	LOSS [training: 0.6581110789977302 | validation: 0.7148928326662349]
	TIME [epoch: 7.62 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6667301057964305		[learning rate: 0.01976]
	Learning Rate: 0.0197602
	LOSS [training: 0.6667301057964305 | validation: 0.5854691457860437]
	TIME [epoch: 7.62 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5286381671756449		[learning rate: 0.019757]
	Learning Rate: 0.0197566
	LOSS [training: 0.5286381671756449 | validation: 0.6033765661929007]
	TIME [epoch: 7.63 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7497358983556648		[learning rate: 0.019753]
	Learning Rate: 0.0197529
	LOSS [training: 0.7497358983556648 | validation: 0.6134933845902057]
	TIME [epoch: 7.66 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6185859208069544		[learning rate: 0.019749]
	Learning Rate: 0.0197493
	LOSS [training: 0.6185859208069544 | validation: 0.5393838266529217]
	TIME [epoch: 7.62 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6001885455125395		[learning rate: 0.019746]
	Learning Rate: 0.0197456
	LOSS [training: 0.6001885455125395 | validation: 0.5678945289073991]
	TIME [epoch: 7.62 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.637558184922916		[learning rate: 0.019742]
	Learning Rate: 0.0197419
	LOSS [training: 0.637558184922916 | validation: 0.5595506925013061]
	TIME [epoch: 7.62 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5399189565656825		[learning rate: 0.019738]
	Learning Rate: 0.0197381
	LOSS [training: 0.5399189565656825 | validation: 0.5336148442193119]
	TIME [epoch: 7.63 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5392616053366057		[learning rate: 0.019734]
	Learning Rate: 0.0197343
	LOSS [training: 0.5392616053366057 | validation: 0.5411038163898034]
	TIME [epoch: 7.65 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4844591754020102		[learning rate: 0.019731]
	Learning Rate: 0.0197305
	LOSS [training: 0.4844591754020102 | validation: 0.6958962582825159]
	TIME [epoch: 7.62 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5846808812359873		[learning rate: 0.019727]
	Learning Rate: 0.0197267
	LOSS [training: 0.5846808812359873 | validation: 0.5315036949744596]
	TIME [epoch: 7.62 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5208783514995085		[learning rate: 0.019723]
	Learning Rate: 0.0197229
	LOSS [training: 0.5208783514995085 | validation: 0.4964421735572665]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5564773999261347		[learning rate: 0.019719]
	Learning Rate: 0.019719
	LOSS [training: 0.5564773999261347 | validation: 0.5224155798877019]
	TIME [epoch: 7.67 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45960827304211643		[learning rate: 0.019715]
	Learning Rate: 0.0197151
	LOSS [training: 0.45960827304211643 | validation: 0.5066359653017312]
	TIME [epoch: 7.63 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7008338114447226		[learning rate: 0.019711]
	Learning Rate: 0.0197112
	LOSS [training: 0.7008338114447226 | validation: 0.5416861060133029]
	TIME [epoch: 7.63 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4966568923980581		[learning rate: 0.019707]
	Learning Rate: 0.0197072
	LOSS [training: 0.4966568923980581 | validation: 0.8783183395778371]
	TIME [epoch: 7.62 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6466741174227532		[learning rate: 0.019703]
	Learning Rate: 0.0197032
	LOSS [training: 0.6466741174227532 | validation: 0.5224637083374984]
	TIME [epoch: 7.62 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5193407210283799		[learning rate: 0.019699]
	Learning Rate: 0.0196992
	LOSS [training: 0.5193407210283799 | validation: 0.5404686294539929]
	TIME [epoch: 7.67 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.508845051064625		[learning rate: 0.019695]
	Learning Rate: 0.0196952
	LOSS [training: 0.508845051064625 | validation: 0.5085957976265192]
	TIME [epoch: 7.63 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5364885324621426		[learning rate: 0.019691]
	Learning Rate: 0.0196911
	LOSS [training: 0.5364885324621426 | validation: 0.47391367687874664]
	TIME [epoch: 122 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5076826734009898		[learning rate: 0.019687]
	Learning Rate: 0.019687
	LOSS [training: 0.5076826734009898 | validation: 0.5924909104649504]
	TIME [epoch: 14.9 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48807077955343925		[learning rate: 0.019683]
	Learning Rate: 0.0196829
	LOSS [training: 0.48807077955343925 | validation: 0.4925526409190396]
	TIME [epoch: 14.9 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5167729962645586		[learning rate: 0.019679]
	Learning Rate: 0.0196788
	LOSS [training: 0.5167729962645586 | validation: 0.4870688327227313]
	TIME [epoch: 14.9 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4701752163790708		[learning rate: 0.019675]
	Learning Rate: 0.0196746
	LOSS [training: 0.4701752163790708 | validation: 0.487270071883786]
	TIME [epoch: 14.9 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4924757278546901		[learning rate: 0.01967]
	Learning Rate: 0.0196704
	LOSS [training: 0.4924757278546901 | validation: 0.6220069103435464]
	TIME [epoch: 14.9 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4909746671081557		[learning rate: 0.019666]
	Learning Rate: 0.0196662
	LOSS [training: 0.4909746671081557 | validation: 0.5677884041846779]
	TIME [epoch: 14.9 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.501804456964095		[learning rate: 0.019662]
	Learning Rate: 0.0196619
	LOSS [training: 0.501804456964095 | validation: 0.4624794681961544]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4932193529736951		[learning rate: 0.019658]
	Learning Rate: 0.0196576
	LOSS [training: 0.4932193529736951 | validation: 0.6286347736447448]
	TIME [epoch: 14.9 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4798037025054373		[learning rate: 0.019653]
	Learning Rate: 0.0196533
	LOSS [training: 0.4798037025054373 | validation: 0.5633620164651107]
	TIME [epoch: 14.9 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4591958203530639		[learning rate: 0.019649]
	Learning Rate: 0.019649
	LOSS [training: 0.4591958203530639 | validation: 0.4444780659837974]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4807330508918603		[learning rate: 0.019645]
	Learning Rate: 0.0196447
	LOSS [training: 0.4807330508918603 | validation: 0.5295786534869671]
	TIME [epoch: 14.9 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46271995816813155		[learning rate: 0.01964]
	Learning Rate: 0.0196403
	LOSS [training: 0.46271995816813155 | validation: 0.40559179652678123]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4977091548877737		[learning rate: 0.019636]
	Learning Rate: 0.0196359
	LOSS [training: 0.4977091548877737 | validation: 0.45752575926308703]
	TIME [epoch: 14.8 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46927762468066625		[learning rate: 0.019631]
	Learning Rate: 0.0196314
	LOSS [training: 0.46927762468066625 | validation: 0.6966014357233352]
	TIME [epoch: 14.8 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.510575591024394		[learning rate: 0.019627]
	Learning Rate: 0.019627
	LOSS [training: 0.510575591024394 | validation: 0.4513674436071576]
	TIME [epoch: 14.9 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46649961144753704		[learning rate: 0.019622]
	Learning Rate: 0.0196225
	LOSS [training: 0.46649961144753704 | validation: 0.4354438089533127]
	TIME [epoch: 14.9 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4420809533741367		[learning rate: 0.019618]
	Learning Rate: 0.019618
	LOSS [training: 0.4420809533741367 | validation: 0.4367464180586065]
	TIME [epoch: 14.9 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47288181730876094		[learning rate: 0.019613]
	Learning Rate: 0.0196134
	LOSS [training: 0.47288181730876094 | validation: 0.396786758893517]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4625273910366122		[learning rate: 0.019609]
	Learning Rate: 0.0196089
	LOSS [training: 0.4625273910366122 | validation: 0.5933358064132713]
	TIME [epoch: 14.9 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46315143082755394		[learning rate: 0.019604]
	Learning Rate: 0.0196043
	LOSS [training: 0.46315143082755394 | validation: 0.4638582554640899]
	TIME [epoch: 14.9 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4765628903522677		[learning rate: 0.0196]
	Learning Rate: 0.0195997
	LOSS [training: 0.4765628903522677 | validation: 0.4622256445170676]
	TIME [epoch: 14.9 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4620861059875824		[learning rate: 0.019595]
	Learning Rate: 0.019595
	LOSS [training: 0.4620861059875824 | validation: 0.4815713396186102]
	TIME [epoch: 14.9 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45506759367607913		[learning rate: 0.01959]
	Learning Rate: 0.0195904
	LOSS [training: 0.45506759367607913 | validation: 0.4122092259587645]
	TIME [epoch: 14.9 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3848612980261175		[learning rate: 0.019586]
	Learning Rate: 0.0195857
	LOSS [training: 0.3848612980261175 | validation: 0.8803639425879062]
	TIME [epoch: 15 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4965126683269074		[learning rate: 0.019581]
	Learning Rate: 0.0195809
	LOSS [training: 0.4965126683269074 | validation: 0.5942979276948959]
	TIME [epoch: 14.8 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4734180431753499		[learning rate: 0.019576]
	Learning Rate: 0.0195762
	LOSS [training: 0.4734180431753499 | validation: 0.5377009766798042]
	TIME [epoch: 15 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4377719465390923		[learning rate: 0.019571]
	Learning Rate: 0.0195714
	LOSS [training: 0.4377719465390923 | validation: 0.44371842579149257]
	TIME [epoch: 14.9 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.421376625188564		[learning rate: 0.019567]
	Learning Rate: 0.0195666
	LOSS [training: 0.421376625188564 | validation: 0.4358304904347444]
	TIME [epoch: 14.8 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42610412999594266		[learning rate: 0.019562]
	Learning Rate: 0.0195618
	LOSS [training: 0.42610412999594266 | validation: 0.46179434926955426]
	TIME [epoch: 14.9 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44934356479010307		[learning rate: 0.019557]
	Learning Rate: 0.0195569
	LOSS [training: 0.44934356479010307 | validation: 0.43473775077176796]
	TIME [epoch: 14.9 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46144741578937915		[learning rate: 0.019552]
	Learning Rate: 0.0195521
	LOSS [training: 0.46144741578937915 | validation: 0.448099871645912]
	TIME [epoch: 14.9 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4065881683695943		[learning rate: 0.019547]
	Learning Rate: 0.0195472
	LOSS [training: 0.4065881683695943 | validation: 0.4700166582812235]
	TIME [epoch: 14.9 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46863848833188215		[learning rate: 0.019542]
	Learning Rate: 0.0195422
	LOSS [training: 0.46863848833188215 | validation: 0.40053358322907345]
	TIME [epoch: 14.9 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4068472280958115		[learning rate: 0.019537]
	Learning Rate: 0.0195373
	LOSS [training: 0.4068472280958115 | validation: 0.4887947184546632]
	TIME [epoch: 14.9 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4477058964751951		[learning rate: 0.019532]
	Learning Rate: 0.0195323
	LOSS [training: 0.4477058964751951 | validation: 0.4285321494055779]
	TIME [epoch: 14.8 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46247116067042193		[learning rate: 0.019527]
	Learning Rate: 0.0195273
	LOSS [training: 0.46247116067042193 | validation: 0.4045891678574937]
	TIME [epoch: 14.9 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44822620763819143		[learning rate: 0.019522]
	Learning Rate: 0.0195222
	LOSS [training: 0.44822620763819143 | validation: 0.5657005710952461]
	TIME [epoch: 14.8 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4984886714236599		[learning rate: 0.019517]
	Learning Rate: 0.0195172
	LOSS [training: 0.4984886714236599 | validation: 0.44444298438520924]
	TIME [epoch: 14.8 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4141041770503214		[learning rate: 0.019512]
	Learning Rate: 0.0195121
	LOSS [training: 0.4141041770503214 | validation: 0.5313788310031669]
	TIME [epoch: 14.9 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4537693817603482		[learning rate: 0.019507]
	Learning Rate: 0.019507
	LOSS [training: 0.4537693817603482 | validation: 0.4639206683631346]
	TIME [epoch: 14.9 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44316478909631163		[learning rate: 0.019502]
	Learning Rate: 0.0195018
	LOSS [training: 0.44316478909631163 | validation: 0.5816140804184825]
	TIME [epoch: 14.9 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45515848045918716		[learning rate: 0.019497]
	Learning Rate: 0.0194967
	LOSS [training: 0.45515848045918716 | validation: 0.5543866218756872]
	TIME [epoch: 14.9 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4591568554042611		[learning rate: 0.019491]
	Learning Rate: 0.0194915
	LOSS [training: 0.4591568554042611 | validation: 0.3992640948189521]
	TIME [epoch: 14.9 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4022109891235307		[learning rate: 0.019486]
	Learning Rate: 0.0194863
	LOSS [training: 0.4022109891235307 | validation: 0.5357801744404502]
	TIME [epoch: 14.9 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4218977950892941		[learning rate: 0.019481]
	Learning Rate: 0.019481
	LOSS [training: 0.4218977950892941 | validation: 0.4785919756525686]
	TIME [epoch: 14.9 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4222502957464036		[learning rate: 0.019476]
	Learning Rate: 0.0194757
	LOSS [training: 0.4222502957464036 | validation: 0.3747202649646267]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4681607315088941		[learning rate: 0.01947]
	Learning Rate: 0.0194705
	LOSS [training: 0.4681607315088941 | validation: 0.4813613269650321]
	TIME [epoch: 14.9 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4484058796576026		[learning rate: 0.019465]
	Learning Rate: 0.0194651
	LOSS [training: 0.4484058796576026 | validation: 0.40800865707509976]
	TIME [epoch: 14.8 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42429474282365426		[learning rate: 0.01946]
	Learning Rate: 0.0194598
	LOSS [training: 0.42429474282365426 | validation: 0.4263512449293729]
	TIME [epoch: 14.8 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42838623084404526		[learning rate: 0.019454]
	Learning Rate: 0.0194544
	LOSS [training: 0.42838623084404526 | validation: 0.41236917165503284]
	TIME [epoch: 14.9 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42422349710002394		[learning rate: 0.019449]
	Learning Rate: 0.019449
	LOSS [training: 0.42422349710002394 | validation: 0.47140492894788744]
	TIME [epoch: 14.9 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4101641751217934		[learning rate: 0.019444]
	Learning Rate: 0.0194436
	LOSS [training: 0.4101641751217934 | validation: 0.3983263640380748]
	TIME [epoch: 14.9 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40662769061767046		[learning rate: 0.019438]
	Learning Rate: 0.0194381
	LOSS [training: 0.40662769061767046 | validation: 0.5657499657361438]
	TIME [epoch: 14.9 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4256027852992497		[learning rate: 0.019433]
	Learning Rate: 0.0194327
	LOSS [training: 0.4256027852992497 | validation: 0.548400205499989]
	TIME [epoch: 14.8 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4190086608869066		[learning rate: 0.019427]
	Learning Rate: 0.0194272
	LOSS [training: 0.4190086608869066 | validation: 0.5732872300060151]
	TIME [epoch: 14.9 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4761176475004164		[learning rate: 0.019422]
	Learning Rate: 0.0194216
	LOSS [training: 0.4761176475004164 | validation: 0.4119097399293221]
	TIME [epoch: 14.9 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4321441552987057		[learning rate: 0.019416]
	Learning Rate: 0.0194161
	LOSS [training: 0.4321441552987057 | validation: 0.479575022131025]
	TIME [epoch: 14.8 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43125436925223126		[learning rate: 0.019411]
	Learning Rate: 0.0194105
	LOSS [training: 0.43125436925223126 | validation: 0.4273839855093258]
	TIME [epoch: 14.9 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4132718246171697		[learning rate: 0.019405]
	Learning Rate: 0.0194049
	LOSS [training: 0.4132718246171697 | validation: 0.42837660317133786]
	TIME [epoch: 14.9 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4164047332645915		[learning rate: 0.019399]
	Learning Rate: 0.0193993
	LOSS [training: 0.4164047332645915 | validation: 0.38974250259409116]
	TIME [epoch: 14.9 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44529480399178595		[learning rate: 0.019394]
	Learning Rate: 0.0193936
	LOSS [training: 0.44529480399178595 | validation: 0.4087928374197537]
	TIME [epoch: 14.9 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43008128454703204		[learning rate: 0.019388]
	Learning Rate: 0.0193879
	LOSS [training: 0.43008128454703204 | validation: 0.42271522454080446]
	TIME [epoch: 14.9 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42806007002793317		[learning rate: 0.019382]
	Learning Rate: 0.0193822
	LOSS [training: 0.42806007002793317 | validation: 0.4389340432178046]
	TIME [epoch: 14.9 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4146895454179993		[learning rate: 0.019376]
	Learning Rate: 0.0193765
	LOSS [training: 0.4146895454179993 | validation: 0.44272313498029825]
	TIME [epoch: 14.8 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.440228834028861		[learning rate: 0.019371]
	Learning Rate: 0.0193707
	LOSS [training: 0.440228834028861 | validation: 0.4683470869475165]
	TIME [epoch: 14.8 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3783116908961255		[learning rate: 0.019365]
	Learning Rate: 0.0193649
	LOSS [training: 0.3783116908961255 | validation: 0.4945467008784941]
	TIME [epoch: 15 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43714461070053506		[learning rate: 0.019359]
	Learning Rate: 0.0193591
	LOSS [training: 0.43714461070053506 | validation: 0.40368042494549744]
	TIME [epoch: 14.9 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42394312065363104		[learning rate: 0.019353]
	Learning Rate: 0.0193533
	LOSS [training: 0.42394312065363104 | validation: 0.6051581919877813]
	TIME [epoch: 14.9 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43662858817916134		[learning rate: 0.019347]
	Learning Rate: 0.0193474
	LOSS [training: 0.43662858817916134 | validation: 0.4098008642208427]
	TIME [epoch: 14.9 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3795127798808971		[learning rate: 0.019342]
	Learning Rate: 0.0193416
	LOSS [training: 0.3795127798808971 | validation: 0.6090841632270068]
	TIME [epoch: 14.9 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42356785093767213		[learning rate: 0.019336]
	Learning Rate: 0.0193356
	LOSS [training: 0.42356785093767213 | validation: 0.4865373366166058]
	TIME [epoch: 14.9 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3728131921391238		[learning rate: 0.01933]
	Learning Rate: 0.0193297
	LOSS [training: 0.3728131921391238 | validation: 0.4498811915789578]
	TIME [epoch: 14.8 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44515111615764336		[learning rate: 0.019324]
	Learning Rate: 0.0193237
	LOSS [training: 0.44515111615764336 | validation: 0.3993640261090107]
	TIME [epoch: 14.9 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4351291416625499		[learning rate: 0.019318]
	Learning Rate: 0.0193178
	LOSS [training: 0.4351291416625499 | validation: 0.400360363025358]
	TIME [epoch: 14.9 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41427190996388785		[learning rate: 0.019312]
	Learning Rate: 0.0193117
	LOSS [training: 0.41427190996388785 | validation: 0.43935619928099034]
	TIME [epoch: 14.9 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3827568401375233		[learning rate: 0.019306]
	Learning Rate: 0.0193057
	LOSS [training: 0.3827568401375233 | validation: 0.49694557001941975]
	TIME [epoch: 14.9 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44529452949454507		[learning rate: 0.0193]
	Learning Rate: 0.0192996
	LOSS [training: 0.44529452949454507 | validation: 0.3957543611754165]
	TIME [epoch: 14.8 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39602001549069793		[learning rate: 0.019294]
	Learning Rate: 0.0192935
	LOSS [training: 0.39602001549069793 | validation: 0.4261490516336166]
	TIME [epoch: 14.8 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3953490069005444		[learning rate: 0.019287]
	Learning Rate: 0.0192874
	LOSS [training: 0.3953490069005444 | validation: 0.363895173295016]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5049543267051156		[learning rate: 0.019281]
	Learning Rate: 0.0192813
	LOSS [training: 0.5049543267051156 | validation: 0.46236342961590476]
	TIME [epoch: 14.9 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4150312569258283		[learning rate: 0.019275]
	Learning Rate: 0.0192751
	LOSS [training: 0.4150312569258283 | validation: 0.41539911525332196]
	TIME [epoch: 14.9 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43890765316439206		[learning rate: 0.019269]
	Learning Rate: 0.0192689
	LOSS [training: 0.43890765316439206 | validation: 0.40395434052375573]
	TIME [epoch: 14.8 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3969730784486899		[learning rate: 0.019263]
	Learning Rate: 0.0192627
	LOSS [training: 0.3969730784486899 | validation: 0.36085351905627516]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39692588377261445		[learning rate: 0.019256]
	Learning Rate: 0.0192565
	LOSS [training: 0.39692588377261445 | validation: 0.37804046704182237]
	TIME [epoch: 14.9 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4233049617419195		[learning rate: 0.01925]
	Learning Rate: 0.0192502
	LOSS [training: 0.4233049617419195 | validation: 0.36005044387508384]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3986313365708568		[learning rate: 0.019244]
	Learning Rate: 0.0192439
	LOSS [training: 0.3986313365708568 | validation: 0.3578881287636674]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3711508219201272		[learning rate: 0.019238]
	Learning Rate: 0.0192376
	LOSS [training: 0.3711508219201272 | validation: 0.38060099415674276]
	TIME [epoch: 14.9 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4215053002855621		[learning rate: 0.019231]
	Learning Rate: 0.0192313
	LOSS [training: 0.4215053002855621 | validation: 0.4295533241661391]
	TIME [epoch: 14.9 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4011117858373069		[learning rate: 0.019225]
	Learning Rate: 0.0192249
	LOSS [training: 0.4011117858373069 | validation: 0.4393983990941408]
	TIME [epoch: 14.9 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3845983139491841		[learning rate: 0.019218]
	Learning Rate: 0.0192185
	LOSS [training: 0.3845983139491841 | validation: 0.4248438084581656]
	TIME [epoch: 14.9 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4263046916490799		[learning rate: 0.019212]
	Learning Rate: 0.0192121
	LOSS [training: 0.4263046916490799 | validation: 0.3647910855617895]
	TIME [epoch: 15 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4191460090616089		[learning rate: 0.019206]
	Learning Rate: 0.0192056
	LOSS [training: 0.4191460090616089 | validation: 0.4237943331622007]
	TIME [epoch: 14.9 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4346652951481506		[learning rate: 0.019199]
	Learning Rate: 0.0191992
	LOSS [training: 0.4346652951481506 | validation: 0.3733273654643042]
	TIME [epoch: 14.9 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4126505414473006		[learning rate: 0.019193]
	Learning Rate: 0.0191927
	LOSS [training: 0.4126505414473006 | validation: 0.40132746578648326]
	TIME [epoch: 14.9 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39879144909798103		[learning rate: 0.019186]
	Learning Rate: 0.0191861
	LOSS [training: 0.39879144909798103 | validation: 0.3811288259071668]
	TIME [epoch: 14.9 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3545132658348456		[learning rate: 0.01918]
	Learning Rate: 0.0191796
	LOSS [training: 0.3545132658348456 | validation: 0.3979418014154569]
	TIME [epoch: 14.9 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3648798189498467		[learning rate: 0.019173]
	Learning Rate: 0.019173
	LOSS [training: 0.3648798189498467 | validation: 0.32496570949558257]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43513901278978184		[learning rate: 0.019166]
	Learning Rate: 0.0191664
	LOSS [training: 0.43513901278978184 | validation: 0.4590162048884949]
	TIME [epoch: 14.9 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4041113330584445		[learning rate: 0.01916]
	Learning Rate: 0.0191598
	LOSS [training: 0.4041113330584445 | validation: 0.35729178522449206]
	TIME [epoch: 14.8 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4120301698348321		[learning rate: 0.019153]
	Learning Rate: 0.0191532
	LOSS [training: 0.4120301698348321 | validation: 0.43840619914971546]
	TIME [epoch: 14.9 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39665288215085526		[learning rate: 0.019146]
	Learning Rate: 0.0191465
	LOSS [training: 0.39665288215085526 | validation: 0.4506645542606412]
	TIME [epoch: 15 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4375083733476224		[learning rate: 0.01914]
	Learning Rate: 0.0191398
	LOSS [training: 0.4375083733476224 | validation: 0.3590355786496511]
	TIME [epoch: 14.9 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3889436404878297		[learning rate: 0.019133]
	Learning Rate: 0.0191331
	LOSS [training: 0.3889436404878297 | validation: 0.40996910982270945]
	TIME [epoch: 14.9 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4031158154361253		[learning rate: 0.019126]
	Learning Rate: 0.0191263
	LOSS [training: 0.4031158154361253 | validation: 0.3743860689202265]
	TIME [epoch: 15 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3732428127879295		[learning rate: 0.01912]
	Learning Rate: 0.0191196
	LOSS [training: 0.3732428127879295 | validation: 0.5200409842147192]
	TIME [epoch: 14.9 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40725740906688584		[learning rate: 0.019113]
	Learning Rate: 0.0191128
	LOSS [training: 0.40725740906688584 | validation: 0.3594568917874378]
	TIME [epoch: 14.9 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3889712088948064		[learning rate: 0.019106]
	Learning Rate: 0.019106
	LOSS [training: 0.3889712088948064 | validation: 0.3453802740805799]
	TIME [epoch: 14.9 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35390007676925384		[learning rate: 0.019099]
	Learning Rate: 0.0190991
	LOSS [training: 0.35390007676925384 | validation: 0.40555833891209003]
	TIME [epoch: 14.9 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4069090420542202		[learning rate: 0.019092]
	Learning Rate: 0.0190922
	LOSS [training: 0.4069090420542202 | validation: 0.3556475850941444]
	TIME [epoch: 15 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3829461820009058		[learning rate: 0.019085]
	Learning Rate: 0.0190853
	LOSS [training: 0.3829461820009058 | validation: 0.4140135963501954]
	TIME [epoch: 14.9 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39165199781312576		[learning rate: 0.019078]
	Learning Rate: 0.0190784
	LOSS [training: 0.39165199781312576 | validation: 0.4500142548874393]
	TIME [epoch: 14.9 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38953378480361794		[learning rate: 0.019071]
	Learning Rate: 0.0190715
	LOSS [training: 0.38953378480361794 | validation: 0.3488706698293489]
	TIME [epoch: 14.9 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3992201828482571		[learning rate: 0.019065]
	Learning Rate: 0.0190645
	LOSS [training: 0.3992201828482571 | validation: 0.37349481671970464]
	TIME [epoch: 14.9 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3905158446554054		[learning rate: 0.019058]
	Learning Rate: 0.0190575
	LOSS [training: 0.3905158446554054 | validation: 0.4086234947034014]
	TIME [epoch: 14.8 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38104387800326656		[learning rate: 0.019051]
	Learning Rate: 0.0190505
	LOSS [training: 0.38104387800326656 | validation: 0.3394516254437635]
	TIME [epoch: 14.9 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41541711783090607		[learning rate: 0.019043]
	Learning Rate: 0.0190435
	LOSS [training: 0.41541711783090607 | validation: 0.4269924685621235]
	TIME [epoch: 14.9 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37695803707841646		[learning rate: 0.019036]
	Learning Rate: 0.0190364
	LOSS [training: 0.37695803707841646 | validation: 0.3639897606835243]
	TIME [epoch: 15 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40106107807904756		[learning rate: 0.019029]
	Learning Rate: 0.0190293
	LOSS [training: 0.40106107807904756 | validation: 0.3898954712365793]
	TIME [epoch: 14.9 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3750343456381993		[learning rate: 0.019022]
	Learning Rate: 0.0190222
	LOSS [training: 0.3750343456381993 | validation: 0.4087859236944412]
	TIME [epoch: 14.9 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40569133384662415		[learning rate: 0.019015]
	Learning Rate: 0.019015
	LOSS [training: 0.40569133384662415 | validation: 0.3984591779894203]
	TIME [epoch: 14.9 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3802771805220466		[learning rate: 0.019008]
	Learning Rate: 0.0190079
	LOSS [training: 0.3802771805220466 | validation: 0.3492492659764097]
	TIME [epoch: 14.8 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38498197416856156		[learning rate: 0.019001]
	Learning Rate: 0.0190007
	LOSS [training: 0.38498197416856156 | validation: 0.383397842907702]
	TIME [epoch: 14.9 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3667605508931646		[learning rate: 0.018993]
	Learning Rate: 0.0189935
	LOSS [training: 0.3667605508931646 | validation: 0.5135230571146832]
	TIME [epoch: 15 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40856760377386325		[learning rate: 0.018986]
	Learning Rate: 0.0189862
	LOSS [training: 0.40856760377386325 | validation: 0.4003215962072969]
	TIME [epoch: 14.9 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35622429951686785		[learning rate: 0.018979]
	Learning Rate: 0.018979
	LOSS [training: 0.35622429951686785 | validation: 0.4392705242740082]
	TIME [epoch: 15 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4147913668746094		[learning rate: 0.018972]
	Learning Rate: 0.0189717
	LOSS [training: 0.4147913668746094 | validation: 0.3553520775260688]
	TIME [epoch: 14.9 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40961928245352985		[learning rate: 0.018964]
	Learning Rate: 0.0189644
	LOSS [training: 0.40961928245352985 | validation: 0.35943781126737967]
	TIME [epoch: 14.9 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36818864675541085		[learning rate: 0.018957]
	Learning Rate: 0.018957
	LOSS [training: 0.36818864675541085 | validation: 0.36625179090744386]
	TIME [epoch: 15 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37826745256260547		[learning rate: 0.01895]
	Learning Rate: 0.0189497
	LOSS [training: 0.37826745256260547 | validation: 0.43449313672567436]
	TIME [epoch: 14.9 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43781859696862047		[learning rate: 0.018942]
	Learning Rate: 0.0189423
	LOSS [training: 0.43781859696862047 | validation: 0.3946544414850508]
	TIME [epoch: 14.9 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39590419521915177		[learning rate: 0.018935]
	Learning Rate: 0.0189349
	LOSS [training: 0.39590419521915177 | validation: 0.35632589165370987]
	TIME [epoch: 15 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3706089352279521		[learning rate: 0.018927]
	Learning Rate: 0.0189274
	LOSS [training: 0.3706089352279521 | validation: 0.40114340414675465]
	TIME [epoch: 14.9 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41002197867158263		[learning rate: 0.01892]
	Learning Rate: 0.01892
	LOSS [training: 0.41002197867158263 | validation: 0.3664659508204603]
	TIME [epoch: 14.9 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.387739200111566		[learning rate: 0.018912]
	Learning Rate: 0.0189125
	LOSS [training: 0.387739200111566 | validation: 0.3722491545491691]
	TIME [epoch: 14.9 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3926738863208118		[learning rate: 0.018905]
	Learning Rate: 0.018905
	LOSS [training: 0.3926738863208118 | validation: 0.3390264795715584]
	TIME [epoch: 14.9 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3824016946647703		[learning rate: 0.018897]
	Learning Rate: 0.0188974
	LOSS [training: 0.3824016946647703 | validation: 0.3865528817596784]
	TIME [epoch: 15 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36082732202746104		[learning rate: 0.01889]
	Learning Rate: 0.0188899
	LOSS [training: 0.36082732202746104 | validation: 0.40579288415161074]
	TIME [epoch: 14.9 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4014842370292325		[learning rate: 0.018882]
	Learning Rate: 0.0188823
	LOSS [training: 0.4014842370292325 | validation: 0.3875772210219083]
	TIME [epoch: 15 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38117314690727633		[learning rate: 0.018875]
	Learning Rate: 0.0188747
	LOSS [training: 0.38117314690727633 | validation: 0.3785748454448289]
	TIME [epoch: 14.8 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3719721642384731		[learning rate: 0.018867]
	Learning Rate: 0.0188671
	LOSS [training: 0.3719721642384731 | validation: 0.43083988856440203]
	TIME [epoch: 14.8 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36642946706343593		[learning rate: 0.018859]
	Learning Rate: 0.0188594
	LOSS [training: 0.36642946706343593 | validation: 0.34244455204523516]
	TIME [epoch: 15 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39635797643178605		[learning rate: 0.018852]
	Learning Rate: 0.0188517
	LOSS [training: 0.39635797643178605 | validation: 0.42741634166350684]
	TIME [epoch: 14.9 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41749356841871943		[learning rate: 0.018844]
	Learning Rate: 0.018844
	LOSS [training: 0.41749356841871943 | validation: 0.38471093071730145]
	TIME [epoch: 14.9 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41255277458697465		[learning rate: 0.018836]
	Learning Rate: 0.0188363
	LOSS [training: 0.41255277458697465 | validation: 0.35486459839153905]
	TIME [epoch: 15 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3652354365298101		[learning rate: 0.018829]
	Learning Rate: 0.0188286
	LOSS [training: 0.3652354365298101 | validation: 0.3713145977717659]
	TIME [epoch: 14.9 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36606486607959293		[learning rate: 0.018821]
	Learning Rate: 0.0188208
	LOSS [training: 0.36606486607959293 | validation: 0.41442119969573865]
	TIME [epoch: 15 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4158862894828519		[learning rate: 0.018813]
	Learning Rate: 0.018813
	LOSS [training: 0.4158862894828519 | validation: 0.3600440489577631]
	TIME [epoch: 14.9 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34381172327283926		[learning rate: 0.018805]
	Learning Rate: 0.0188052
	LOSS [training: 0.34381172327283926 | validation: 0.4917586273600636]
	TIME [epoch: 14.9 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40131476021733103		[learning rate: 0.018797]
	Learning Rate: 0.0187973
	LOSS [training: 0.40131476021733103 | validation: 0.3957806275345219]
	TIME [epoch: 15 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4027458144892768		[learning rate: 0.018789]
	Learning Rate: 0.0187894
	LOSS [training: 0.4027458144892768 | validation: 0.35367114985578063]
	TIME [epoch: 14.8 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3305833745123436		[learning rate: 0.018782]
	Learning Rate: 0.0187815
	LOSS [training: 0.3305833745123436 | validation: 0.4323358375855284]
	TIME [epoch: 14.8 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4261111409550892		[learning rate: 0.018774]
	Learning Rate: 0.0187736
	LOSS [training: 0.4261111409550892 | validation: 0.3460676171802843]
	TIME [epoch: 15 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4222801918017451		[learning rate: 0.018766]
	Learning Rate: 0.0187657
	LOSS [training: 0.4222801918017451 | validation: 0.4138378507655946]
	TIME [epoch: 14.9 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36795867001473453		[learning rate: 0.018758]
	Learning Rate: 0.0187577
	LOSS [training: 0.36795867001473453 | validation: 0.3416977879825282]
	TIME [epoch: 15 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42862594815355937		[learning rate: 0.01875]
	Learning Rate: 0.0187497
	LOSS [training: 0.42862594815355937 | validation: 0.38385194903598485]
	TIME [epoch: 15 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3752393245672847		[learning rate: 0.018742]
	Learning Rate: 0.0187417
	LOSS [training: 0.3752393245672847 | validation: 0.3962485322962267]
	TIME [epoch: 14.9 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35936296178404903		[learning rate: 0.018734]
	Learning Rate: 0.0187337
	LOSS [training: 0.35936296178404903 | validation: 0.3456056955222231]
	TIME [epoch: 14.9 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4051268397466741		[learning rate: 0.018726]
	Learning Rate: 0.0187256
	LOSS [training: 0.4051268397466741 | validation: 0.36632736338928973]
	TIME [epoch: 14.9 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3728094921312154		[learning rate: 0.018718]
	Learning Rate: 0.0187175
	LOSS [training: 0.3728094921312154 | validation: 0.3352025143368448]
	TIME [epoch: 14.8 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39326900063917886		[learning rate: 0.018709]
	Learning Rate: 0.0187094
	LOSS [training: 0.39326900063917886 | validation: 0.3965978183269575]
	TIME [epoch: 15 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38101933762865237		[learning rate: 0.018701]
	Learning Rate: 0.0187013
	LOSS [training: 0.38101933762865237 | validation: 0.34751791249492603]
	TIME [epoch: 14.9 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37700071160065235		[learning rate: 0.018693]
	Learning Rate: 0.0186931
	LOSS [training: 0.37700071160065235 | validation: 0.3479963533473887]
	TIME [epoch: 15 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35103408465499775		[learning rate: 0.018685]
	Learning Rate: 0.0186849
	LOSS [training: 0.35103408465499775 | validation: 0.38336942029714793]
	TIME [epoch: 14.9 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3567902829471128		[learning rate: 0.018677]
	Learning Rate: 0.0186767
	LOSS [training: 0.3567902829471128 | validation: 0.39672149111825006]
	TIME [epoch: 14.9 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3902345748581306		[learning rate: 0.018668]
	Learning Rate: 0.0186685
	LOSS [training: 0.3902345748581306 | validation: 0.4102775135513702]
	TIME [epoch: 15 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37805385828751586		[learning rate: 0.01866]
	Learning Rate: 0.0186602
	LOSS [training: 0.37805385828751586 | validation: 0.46278268381153786]
	TIME [epoch: 14.9 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34493980522328893		[learning rate: 0.018652]
	Learning Rate: 0.018652
	LOSS [training: 0.34493980522328893 | validation: 0.31001975966100714]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4200425090555164		[learning rate: 0.018644]
	Learning Rate: 0.0186437
	LOSS [training: 0.4200425090555164 | validation: 0.38162970527601353]
	TIME [epoch: 15 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37549029297507636		[learning rate: 0.018635]
	Learning Rate: 0.0186353
	LOSS [training: 0.37549029297507636 | validation: 0.3545422895384306]
	TIME [epoch: 14.9 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3720396451002031		[learning rate: 0.018627]
	Learning Rate: 0.018627
	LOSS [training: 0.3720396451002031 | validation: 0.3665390845211835]
	TIME [epoch: 15 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3674866573995184		[learning rate: 0.018619]
	Learning Rate: 0.0186186
	LOSS [training: 0.3674866573995184 | validation: 0.34669397787400946]
	TIME [epoch: 14.9 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3516260357113075		[learning rate: 0.01861]
	Learning Rate: 0.0186102
	LOSS [training: 0.3516260357113075 | validation: 0.34126489507676944]
	TIME [epoch: 14.9 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35949901323242134		[learning rate: 0.018602]
	Learning Rate: 0.0186018
	LOSS [training: 0.35949901323242134 | validation: 0.3596905195786092]
	TIME [epoch: 14.9 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4179454202380671		[learning rate: 0.018593]
	Learning Rate: 0.0185934
	LOSS [training: 0.4179454202380671 | validation: 0.38024527801884866]
	TIME [epoch: 14.8 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3825400944769354		[learning rate: 0.018585]
	Learning Rate: 0.0185849
	LOSS [training: 0.3825400944769354 | validation: 0.3618569652312472]
	TIME [epoch: 15 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.344789858491657		[learning rate: 0.018576]
	Learning Rate: 0.0185764
	LOSS [training: 0.344789858491657 | validation: 0.6325268490616188]
	TIME [epoch: 14.9 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40955137401815445		[learning rate: 0.018568]
	Learning Rate: 0.0185679
	LOSS [training: 0.40955137401815445 | validation: 0.3793680405415791]
	TIME [epoch: 14.9 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.384978715119321		[learning rate: 0.018559]
	Learning Rate: 0.0185594
	LOSS [training: 0.384978715119321 | validation: 0.35460843131517883]
	TIME [epoch: 14.9 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34987035276392026		[learning rate: 0.018551]
	Learning Rate: 0.0185508
	LOSS [training: 0.34987035276392026 | validation: 0.3605823598909625]
	TIME [epoch: 14.9 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3775372285132731		[learning rate: 0.018542]
	Learning Rate: 0.0185422
	LOSS [training: 0.3775372285132731 | validation: 0.36801111916502277]
	TIME [epoch: 15 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34560938118150925		[learning rate: 0.018534]
	Learning Rate: 0.0185336
	LOSS [training: 0.34560938118150925 | validation: 0.33612495312848784]
	TIME [epoch: 14.9 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37851570516165833		[learning rate: 0.018525]
	Learning Rate: 0.018525
	LOSS [training: 0.37851570516165833 | validation: 0.3996551376199399]
	TIME [epoch: 14.8 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3555668890989303		[learning rate: 0.018516]
	Learning Rate: 0.0185163
	LOSS [training: 0.3555668890989303 | validation: 0.435971091858404]
	TIME [epoch: 15 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35019578021381975		[learning rate: 0.018508]
	Learning Rate: 0.0185077
	LOSS [training: 0.35019578021381975 | validation: 0.445804063387533]
	TIME [epoch: 14.9 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3751387034670945		[learning rate: 0.018499]
	Learning Rate: 0.018499
	LOSS [training: 0.3751387034670945 | validation: 0.4300985932340927]
	TIME [epoch: 14.9 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35419666648782194		[learning rate: 0.01849]
	Learning Rate: 0.0184902
	LOSS [training: 0.35419666648782194 | validation: 0.40632908572358756]
	TIME [epoch: 14.9 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3517337921150766		[learning rate: 0.018481]
	Learning Rate: 0.0184815
	LOSS [training: 0.3517337921150766 | validation: 0.4061920854637747]
	TIME [epoch: 14.9 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3412410658472267		[learning rate: 0.018473]
	Learning Rate: 0.0184727
	LOSS [training: 0.3412410658472267 | validation: 0.3210589286772967]
	TIME [epoch: 15 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32181671446424776		[learning rate: 0.018464]
	Learning Rate: 0.0184639
	LOSS [training: 0.32181671446424776 | validation: 0.31353625821569553]
	TIME [epoch: 14.9 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46143705569007093		[learning rate: 0.018455]
	Learning Rate: 0.0184551
	LOSS [training: 0.46143705569007093 | validation: 0.3647083558618689]
	TIME [epoch: 14.9 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36665725176344877		[learning rate: 0.018446]
	Learning Rate: 0.0184463
	LOSS [training: 0.36665725176344877 | validation: 0.5479926771633375]
	TIME [epoch: 14.8 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3968535349673951		[learning rate: 0.018437]
	Learning Rate: 0.0184374
	LOSS [training: 0.3968535349673951 | validation: 0.33257543323657857]
	TIME [epoch: 14.8 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3583431563259121		[learning rate: 0.018429]
	Learning Rate: 0.0184285
	LOSS [training: 0.3583431563259121 | validation: 0.37667775282794436]
	TIME [epoch: 15 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3395558924605145		[learning rate: 0.01842]
	Learning Rate: 0.0184196
	LOSS [training: 0.3395558924605145 | validation: 0.3819944327695356]
	TIME [epoch: 14.9 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36086679514240777		[learning rate: 0.018411]
	Learning Rate: 0.0184107
	LOSS [training: 0.36086679514240777 | validation: 0.3363617558322607]
	TIME [epoch: 14.9 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3750730848709559		[learning rate: 0.018402]
	Learning Rate: 0.0184017
	LOSS [training: 0.3750730848709559 | validation: 0.32753782300336653]
	TIME [epoch: 14.9 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36289945360113846		[learning rate: 0.018393]
	Learning Rate: 0.0183928
	LOSS [training: 0.36289945360113846 | validation: 0.38606107744982243]
	TIME [epoch: 14.9 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.372977854102095		[learning rate: 0.018384]
	Learning Rate: 0.0183838
	LOSS [training: 0.372977854102095 | validation: 0.3920575865071733]
	TIME [epoch: 14.9 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3397668135560673		[learning rate: 0.018375]
	Learning Rate: 0.0183747
	LOSS [training: 0.3397668135560673 | validation: 0.31256409133660185]
	TIME [epoch: 14.9 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3934854237208374		[learning rate: 0.018366]
	Learning Rate: 0.0183657
	LOSS [training: 0.3934854237208374 | validation: 0.4083346915066497]
	TIME [epoch: 14.9 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37754833270726634		[learning rate: 0.018357]
	Learning Rate: 0.0183566
	LOSS [training: 0.37754833270726634 | validation: 0.34092067067744714]
	TIME [epoch: 14.9 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3663164929612608		[learning rate: 0.018348]
	Learning Rate: 0.0183475
	LOSS [training: 0.3663164929612608 | validation: 0.37932831772800657]
	TIME [epoch: 14.9 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3694596936479206		[learning rate: 0.018338]
	Learning Rate: 0.0183384
	LOSS [training: 0.3694596936479206 | validation: 0.37076132860158556]
	TIME [epoch: 15 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33754310707170593		[learning rate: 0.018329]
	Learning Rate: 0.0183293
	LOSS [training: 0.33754310707170593 | validation: 0.3815160487494137]
	TIME [epoch: 14.9 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36407646906081315		[learning rate: 0.01832]
	Learning Rate: 0.0183201
	LOSS [training: 0.36407646906081315 | validation: 0.33623162183512645]
	TIME [epoch: 14.9 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35196932845667084		[learning rate: 0.018311]
	Learning Rate: 0.0183109
	LOSS [training: 0.35196932845667084 | validation: 0.5219171064027307]
	TIME [epoch: 14.9 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4084893813210634		[learning rate: 0.018302]
	Learning Rate: 0.0183017
	LOSS [training: 0.4084893813210634 | validation: 0.3467168171349667]
	TIME [epoch: 14.9 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3137790409314921		[learning rate: 0.018293]
	Learning Rate: 0.0182925
	LOSS [training: 0.3137790409314921 | validation: 0.4051285939452941]
	TIME [epoch: 14.9 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44505402356020807		[learning rate: 0.018283]
	Learning Rate: 0.0182833
	LOSS [training: 0.44505402356020807 | validation: 0.37007885028875376]
	TIME [epoch: 14.9 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.367227047712919		[learning rate: 0.018274]
	Learning Rate: 0.018274
	LOSS [training: 0.367227047712919 | validation: 0.37096761593940897]
	TIME [epoch: 14.9 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3400675123379525		[learning rate: 0.018265]
	Learning Rate: 0.0182647
	LOSS [training: 0.3400675123379525 | validation: 0.4038933629413933]
	TIME [epoch: 15 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3530276506398904		[learning rate: 0.018255]
	Learning Rate: 0.0182554
	LOSS [training: 0.3530276506398904 | validation: 0.32945438988413833]
	TIME [epoch: 14.9 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35275020527504597		[learning rate: 0.018246]
	Learning Rate: 0.018246
	LOSS [training: 0.35275020527504597 | validation: 0.3937777213151257]
	TIME [epoch: 15 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3510499808982505		[learning rate: 0.018237]
	Learning Rate: 0.0182367
	LOSS [training: 0.3510499808982505 | validation: 0.30978232866515226]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35837142180430875		[learning rate: 0.018227]
	Learning Rate: 0.0182273
	LOSS [training: 0.35837142180430875 | validation: 0.3510384431667162]
	TIME [epoch: 14.8 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.378759981169238		[learning rate: 0.018218]
	Learning Rate: 0.0182179
	LOSS [training: 0.378759981169238 | validation: 0.3424792597148398]
	TIME [epoch: 14.9 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3378173814764144		[learning rate: 0.018208]
	Learning Rate: 0.0182085
	LOSS [training: 0.3378173814764144 | validation: 0.3970040469217998]
	TIME [epoch: 14.9 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3717430830346656		[learning rate: 0.018199]
	Learning Rate: 0.018199
	LOSS [training: 0.3717430830346656 | validation: 0.3256167369592229]
	TIME [epoch: 15 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36115366482998995		[learning rate: 0.01819]
	Learning Rate: 0.0181895
	LOSS [training: 0.36115366482998995 | validation: 0.34385593862096286]
	TIME [epoch: 15 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3323703444951829		[learning rate: 0.01818]
	Learning Rate: 0.01818
	LOSS [training: 0.3323703444951829 | validation: 0.4129446345473605]
	TIME [epoch: 14.9 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3620942114631684		[learning rate: 0.018171]
	Learning Rate: 0.0181705
	LOSS [training: 0.3620942114631684 | validation: 0.34034753173620047]
	TIME [epoch: 15 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37816548100516884		[learning rate: 0.018161]
	Learning Rate: 0.018161
	LOSS [training: 0.37816548100516884 | validation: 0.3728124673871409]
	TIME [epoch: 14.8 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32670328350756894		[learning rate: 0.018151]
	Learning Rate: 0.0181514
	LOSS [training: 0.32670328350756894 | validation: 0.4567668397446884]
	TIME [epoch: 14.9 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3811024552870418		[learning rate: 0.018142]
	Learning Rate: 0.0181418
	LOSS [training: 0.3811024552870418 | validation: 0.3233614578947914]
	TIME [epoch: 14.9 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37248230370537094		[learning rate: 0.018132]
	Learning Rate: 0.0181322
	LOSS [training: 0.37248230370537094 | validation: 0.40101912004895834]
	TIME [epoch: 14.9 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32897266837045264		[learning rate: 0.018123]
	Learning Rate: 0.0181226
	LOSS [training: 0.32897266837045264 | validation: 0.30406745075789116]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37077948893699825		[learning rate: 0.018113]
	Learning Rate: 0.0181129
	LOSS [training: 0.37077948893699825 | validation: 0.3296767812170359]
	TIME [epoch: 14.9 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.340073605458149		[learning rate: 0.018103]
	Learning Rate: 0.0181032
	LOSS [training: 0.340073605458149 | validation: 0.32582062758959895]
	TIME [epoch: 15 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.350369102564235		[learning rate: 0.018094]
	Learning Rate: 0.0180936
	LOSS [training: 0.350369102564235 | validation: 0.3952352748940641]
	TIME [epoch: 15 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33561040750509724		[learning rate: 0.018084]
	Learning Rate: 0.0180838
	LOSS [training: 0.33561040750509724 | validation: 0.4509080217111341]
	TIME [epoch: 14.9 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33246855514616436		[learning rate: 0.018074]
	Learning Rate: 0.0180741
	LOSS [training: 0.33246855514616436 | validation: 0.2927803344848087]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.388458502145335		[learning rate: 0.018064]
	Learning Rate: 0.0180643
	LOSS [training: 0.388458502145335 | validation: 0.4400787780444637]
	TIME [epoch: 14.9 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4050550187391277		[learning rate: 0.018055]
	Learning Rate: 0.0180545
	LOSS [training: 0.4050550187391277 | validation: 0.39058795801219937]
	TIME [epoch: 14.8 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34347088397482883		[learning rate: 0.018045]
	Learning Rate: 0.0180447
	LOSS [training: 0.34347088397482883 | validation: 0.3093199154327996]
	TIME [epoch: 14.8 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35144054485549947		[learning rate: 0.018035]
	Learning Rate: 0.0180349
	LOSS [training: 0.35144054485549947 | validation: 0.46990989790601884]
	TIME [epoch: 14.8 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43128650644025157		[learning rate: 0.018025]
	Learning Rate: 0.0180251
	LOSS [training: 0.43128650644025157 | validation: 0.3624786667628588]
	TIME [epoch: 14.8 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3228695616281073		[learning rate: 0.018015]
	Learning Rate: 0.0180152
	LOSS [training: 0.3228695616281073 | validation: 0.3219267984522854]
	TIME [epoch: 14.8 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.439510195216299		[learning rate: 0.018005]
	Learning Rate: 0.0180053
	LOSS [training: 0.439510195216299 | validation: 0.37021931955218157]
	TIME [epoch: 14.8 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.368353640387947		[learning rate: 0.017995]
	Learning Rate: 0.0179954
	LOSS [training: 0.368353640387947 | validation: 0.34371770608991814]
	TIME [epoch: 14.8 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3447820136597223		[learning rate: 0.017985]
	Learning Rate: 0.0179854
	LOSS [training: 0.3447820136597223 | validation: 0.37971009033842507]
	TIME [epoch: 14.8 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3811728865179963		[learning rate: 0.017975]
	Learning Rate: 0.0179755
	LOSS [training: 0.3811728865179963 | validation: 0.36241266462958477]
	TIME [epoch: 14.8 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31490924868383724		[learning rate: 0.017965]
	Learning Rate: 0.0179655
	LOSS [training: 0.31490924868383724 | validation: 0.3070906938442589]
	TIME [epoch: 14.8 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.377562684655658		[learning rate: 0.017955]
	Learning Rate: 0.0179555
	LOSS [training: 0.377562684655658 | validation: 0.3500128422415348]
	TIME [epoch: 14.8 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35320812566094656		[learning rate: 0.017945]
	Learning Rate: 0.0179455
	LOSS [training: 0.35320812566094656 | validation: 0.32193689969173234]
	TIME [epoch: 14.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3569926345481502		[learning rate: 0.017935]
	Learning Rate: 0.0179354
	LOSS [training: 0.3569926345481502 | validation: 0.32283545503515965]
	TIME [epoch: 14.8 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35651406994561247		[learning rate: 0.017925]
	Learning Rate: 0.0179253
	LOSS [training: 0.35651406994561247 | validation: 0.33004934239425643]
	TIME [epoch: 14.8 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3699090800180429		[learning rate: 0.017915]
	Learning Rate: 0.0179152
	LOSS [training: 0.3699090800180429 | validation: 0.33573482927406045]
	TIME [epoch: 14.8 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31099393047258944		[learning rate: 0.017905]
	Learning Rate: 0.0179051
	LOSS [training: 0.31099393047258944 | validation: 0.34187161552285966]
	TIME [epoch: 14.8 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3640864206348707		[learning rate: 0.017895]
	Learning Rate: 0.017895
	LOSS [training: 0.3640864206348707 | validation: 0.37265903557255875]
	TIME [epoch: 14.9 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3886989187436765		[learning rate: 0.017885]
	Learning Rate: 0.0178848
	LOSS [training: 0.3886989187436765 | validation: 0.3279961147725673]
	TIME [epoch: 14.8 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3222357613706123		[learning rate: 0.017875]
	Learning Rate: 0.0178747
	LOSS [training: 0.3222357613706123 | validation: 0.333714656019381]
	TIME [epoch: 14.8 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3708005231130381		[learning rate: 0.017864]
	Learning Rate: 0.0178645
	LOSS [training: 0.3708005231130381 | validation: 0.331937799426653]
	TIME [epoch: 14.9 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3048270982110912		[learning rate: 0.017854]
	Learning Rate: 0.0178542
	LOSS [training: 0.3048270982110912 | validation: 0.3231610464221134]
	TIME [epoch: 14.8 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4072285484155095		[learning rate: 0.017844]
	Learning Rate: 0.017844
	LOSS [training: 0.4072285484155095 | validation: 0.3255597924381282]
	TIME [epoch: 14.9 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30227265472883913		[learning rate: 0.017834]
	Learning Rate: 0.0178337
	LOSS [training: 0.30227265472883913 | validation: 0.3849088624242173]
	TIME [epoch: 14.8 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39072046377592273		[learning rate: 0.017823]
	Learning Rate: 0.0178235
	LOSS [training: 0.39072046377592273 | validation: 0.34372600189384495]
	TIME [epoch: 14.8 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3909033049749427		[learning rate: 0.017813]
	Learning Rate: 0.0178131
	LOSS [training: 0.3909033049749427 | validation: 0.3849378822174524]
	TIME [epoch: 14.9 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3482656559120913		[learning rate: 0.017803]
	Learning Rate: 0.0178028
	LOSS [training: 0.3482656559120913 | validation: 0.3165498505250438]
	TIME [epoch: 14.8 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3513915873290906		[learning rate: 0.017792]
	Learning Rate: 0.0177925
	LOSS [training: 0.3513915873290906 | validation: 0.39613298108511213]
	TIME [epoch: 14.9 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38242105421104294		[learning rate: 0.017782]
	Learning Rate: 0.0177821
	LOSS [training: 0.38242105421104294 | validation: 0.35479777492182846]
	TIME [epoch: 14.9 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.340178868280083		[learning rate: 0.017772]
	Learning Rate: 0.0177717
	LOSS [training: 0.340178868280083 | validation: 0.44398972139747606]
	TIME [epoch: 14.8 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.385123231584618		[learning rate: 0.017761]
	Learning Rate: 0.0177613
	LOSS [training: 0.385123231584618 | validation: 0.327566355967953]
	TIME [epoch: 14.9 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3247536609423153		[learning rate: 0.017751]
	Learning Rate: 0.0177509
	LOSS [training: 0.3247536609423153 | validation: 0.37126148052117053]
	TIME [epoch: 14.8 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3918887664087535		[learning rate: 0.01774]
	Learning Rate: 0.0177404
	LOSS [training: 0.3918887664087535 | validation: 0.3435799991062648]
	TIME [epoch: 14.9 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31787627545029307		[learning rate: 0.01773]
	Learning Rate: 0.0177299
	LOSS [training: 0.31787627545029307 | validation: 0.3000380351415374]
	TIME [epoch: 14.8 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37238391557230927		[learning rate: 0.017719]
	Learning Rate: 0.0177194
	LOSS [training: 0.37238391557230927 | validation: 0.371977615223364]
	TIME [epoch: 14.8 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3314115543299558		[learning rate: 0.017709]
	Learning Rate: 0.0177089
	LOSS [training: 0.3314115543299558 | validation: 0.3511475443933845]
	TIME [epoch: 14.9 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.356293685926606		[learning rate: 0.017698]
	Learning Rate: 0.0176984
	LOSS [training: 0.356293685926606 | validation: 0.3932636485422374]
	TIME [epoch: 14.8 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3813209986470856		[learning rate: 0.017688]
	Learning Rate: 0.0176878
	LOSS [training: 0.3813209986470856 | validation: 0.33984390867202324]
	TIME [epoch: 14.8 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3401188226164251		[learning rate: 0.017677]
	Learning Rate: 0.0176772
	LOSS [training: 0.3401188226164251 | validation: 0.3327109614156186]
	TIME [epoch: 14.8 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3568144767823835		[learning rate: 0.017667]
	Learning Rate: 0.0176666
	LOSS [training: 0.3568144767823835 | validation: 0.3697373110985246]
	TIME [epoch: 14.8 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32559812808377453		[learning rate: 0.017656]
	Learning Rate: 0.017656
	LOSS [training: 0.32559812808377453 | validation: 0.3012418818827568]
	TIME [epoch: 14.9 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3175469311594569		[learning rate: 0.017645]
	Learning Rate: 0.0176454
	LOSS [training: 0.3175469311594569 | validation: 0.45507854795039104]
	TIME [epoch: 14.8 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3676962448771801		[learning rate: 0.017635]
	Learning Rate: 0.0176347
	LOSS [training: 0.3676962448771801 | validation: 0.34299067817248385]
	TIME [epoch: 14.8 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.318587052257001		[learning rate: 0.017624]
	Learning Rate: 0.017624
	LOSS [training: 0.318587052257001 | validation: 0.43926270142147333]
	TIME [epoch: 14.9 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3494012403412004		[learning rate: 0.017613]
	Learning Rate: 0.0176133
	LOSS [training: 0.3494012403412004 | validation: 0.32929780549070203]
	TIME [epoch: 14.8 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3509074298842865		[learning rate: 0.017603]
	Learning Rate: 0.0176026
	LOSS [training: 0.3509074298842865 | validation: 0.38067693620740073]
	TIME [epoch: 14.9 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32883025961091206		[learning rate: 0.017592]
	Learning Rate: 0.0175918
	LOSS [training: 0.32883025961091206 | validation: 0.33116771795484856]
	TIME [epoch: 14.8 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35630323956019133		[learning rate: 0.017581]
	Learning Rate: 0.0175811
	LOSS [training: 0.35630323956019133 | validation: 0.3380268050007209]
	TIME [epoch: 14.8 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3145421237892125		[learning rate: 0.01757]
	Learning Rate: 0.0175703
	LOSS [training: 0.3145421237892125 | validation: 0.3778708385640694]
	TIME [epoch: 14.9 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3491615368088222		[learning rate: 0.017559]
	Learning Rate: 0.0175595
	LOSS [training: 0.3491615368088222 | validation: 0.34268261840048597]
	TIME [epoch: 14.8 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.310031898345552		[learning rate: 0.017549]
	Learning Rate: 0.0175486
	LOSS [training: 0.310031898345552 | validation: 0.33190630613808336]
	TIME [epoch: 14.9 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33714820197395295		[learning rate: 0.017538]
	Learning Rate: 0.0175378
	LOSS [training: 0.33714820197395295 | validation: 0.3639549409682038]
	TIME [epoch: 14.8 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3473868958355765		[learning rate: 0.017527]
	Learning Rate: 0.0175269
	LOSS [training: 0.3473868958355765 | validation: 0.3060276484850027]
	TIME [epoch: 14.8 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3552318846449878		[learning rate: 0.017516]
	Learning Rate: 0.017516
	LOSS [training: 0.3552318846449878 | validation: 0.33523161983185873]
	TIME [epoch: 14.9 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3221665845773716		[learning rate: 0.017505]
	Learning Rate: 0.0175051
	LOSS [training: 0.3221665845773716 | validation: 0.3922451978184121]
	TIME [epoch: 14.8 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35786540937741845		[learning rate: 0.017494]
	Learning Rate: 0.0174942
	LOSS [training: 0.35786540937741845 | validation: 0.34731303283277193]
	TIME [epoch: 14.9 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3461217495798678		[learning rate: 0.017483]
	Learning Rate: 0.0174832
	LOSS [training: 0.3461217495798678 | validation: 0.33443835658397425]
	TIME [epoch: 14.9 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3201295989458361		[learning rate: 0.017472]
	Learning Rate: 0.0174722
	LOSS [training: 0.3201295989458361 | validation: 0.3247227773277636]
	TIME [epoch: 14.9 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32529438137861477		[learning rate: 0.017461]
	Learning Rate: 0.0174612
	LOSS [training: 0.32529438137861477 | validation: 0.3907109312562701]
	TIME [epoch: 14.9 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.360944188513186		[learning rate: 0.01745]
	Learning Rate: 0.0174502
	LOSS [training: 0.360944188513186 | validation: 0.33886191305899976]
	TIME [epoch: 14.8 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3111699508676666		[learning rate: 0.017439]
	Learning Rate: 0.0174392
	LOSS [training: 0.3111699508676666 | validation: 0.31584562820561657]
	TIME [epoch: 14.9 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36158197614064436		[learning rate: 0.017428]
	Learning Rate: 0.0174281
	LOSS [training: 0.36158197614064436 | validation: 0.45042701873464586]
	TIME [epoch: 14.9 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35138041486187527		[learning rate: 0.017417]
	Learning Rate: 0.017417
	LOSS [training: 0.35138041486187527 | validation: 0.31480148852831547]
	TIME [epoch: 14.9 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3600010735404885		[learning rate: 0.017406]
	Learning Rate: 0.0174059
	LOSS [training: 0.3600010735404885 | validation: 0.36063809030911914]
	TIME [epoch: 14.9 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3289476123491828		[learning rate: 0.017395]
	Learning Rate: 0.0173948
	LOSS [training: 0.3289476123491828 | validation: 0.35295317522713654]
	TIME [epoch: 14.9 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30775995066090184		[learning rate: 0.017384]
	Learning Rate: 0.0173837
	LOSS [training: 0.30775995066090184 | validation: 0.32680102302895264]
	TIME [epoch: 14.9 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39340712036794034		[learning rate: 0.017373]
	Learning Rate: 0.0173725
	LOSS [training: 0.39340712036794034 | validation: 0.32704633220729995]
	TIME [epoch: 14.9 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3432437300463458		[learning rate: 0.017361]
	Learning Rate: 0.0173614
	LOSS [training: 0.3432437300463458 | validation: 0.32102810290382916]
	TIME [epoch: 14.8 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3155755630614737		[learning rate: 0.01735]
	Learning Rate: 0.0173502
	LOSS [training: 0.3155755630614737 | validation: 0.32492413291504324]
	TIME [epoch: 139 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3207884874349291		[learning rate: 0.017339]
	Learning Rate: 0.0173389
	LOSS [training: 0.3207884874349291 | validation: 0.28680564806585906]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3279833215079732		[learning rate: 0.017328]
	Learning Rate: 0.0173277
	LOSS [training: 0.3279833215079732 | validation: 0.34261428245470416]
	TIME [epoch: 32.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33806058778518777		[learning rate: 0.017316]
	Learning Rate: 0.0173164
	LOSS [training: 0.33806058778518777 | validation: 0.3528939127333426]
	TIME [epoch: 32.5 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3276185514098679		[learning rate: 0.017305]
	Learning Rate: 0.0173052
	LOSS [training: 0.3276185514098679 | validation: 0.34713570152272005]
	TIME [epoch: 32.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33134411525906105		[learning rate: 0.017294]
	Learning Rate: 0.0172939
	LOSS [training: 0.33134411525906105 | validation: 0.3628210993170192]
	TIME [epoch: 32.7 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34862923398393464		[learning rate: 0.017283]
	Learning Rate: 0.0172826
	LOSS [training: 0.34862923398393464 | validation: 0.34390274443818747]
	TIME [epoch: 32.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31824638149791357		[learning rate: 0.017271]
	Learning Rate: 0.0172712
	LOSS [training: 0.31824638149791357 | validation: 0.3205816261662112]
	TIME [epoch: 32.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35625745223623334		[learning rate: 0.01726]
	Learning Rate: 0.0172599
	LOSS [training: 0.35625745223623334 | validation: 0.32494016342060916]
	TIME [epoch: 32.7 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3223702505618329		[learning rate: 0.017248]
	Learning Rate: 0.0172485
	LOSS [training: 0.3223702505618329 | validation: 0.31403511733275835]
	TIME [epoch: 32.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33286390686587		[learning rate: 0.017237]
	Learning Rate: 0.0172371
	LOSS [training: 0.33286390686587 | validation: 0.3775645928259289]
	TIME [epoch: 32.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31786314932322796		[learning rate: 0.017226]
	Learning Rate: 0.0172257
	LOSS [training: 0.31786314932322796 | validation: 0.29708617631749035]
	TIME [epoch: 32.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31434991870526213		[learning rate: 0.017214]
	Learning Rate: 0.0172142
	LOSS [training: 0.31434991870526213 | validation: 0.40145948778966223]
	TIME [epoch: 32.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.359046334918553		[learning rate: 0.017203]
	Learning Rate: 0.0172028
	LOSS [training: 0.359046334918553 | validation: 0.34228128179397077]
	TIME [epoch: 32.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3069746426910501		[learning rate: 0.017191]
	Learning Rate: 0.0171913
	LOSS [training: 0.3069746426910501 | validation: 0.42537774183913085]
	TIME [epoch: 32.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3640928950506019		[learning rate: 0.01718]
	Learning Rate: 0.0171798
	LOSS [training: 0.3640928950506019 | validation: 0.3103336184798117]
	TIME [epoch: 32.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3262423798956301		[learning rate: 0.017168]
	Learning Rate: 0.0171683
	LOSS [training: 0.3262423798956301 | validation: 0.3260336078682946]
	TIME [epoch: 32.5 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3299466838106061		[learning rate: 0.017157]
	Learning Rate: 0.0171567
	LOSS [training: 0.3299466838106061 | validation: 0.4489100901498374]
	TIME [epoch: 32.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3479949049606003		[learning rate: 0.017145]
	Learning Rate: 0.0171452
	LOSS [training: 0.3479949049606003 | validation: 0.29797250929795466]
	TIME [epoch: 32.5 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.304177527025564		[learning rate: 0.017134]
	Learning Rate: 0.0171336
	LOSS [training: 0.304177527025564 | validation: 0.46867552420627945]
	TIME [epoch: 32.4 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38569221990754643		[learning rate: 0.017122]
	Learning Rate: 0.017122
	LOSS [training: 0.38569221990754643 | validation: 0.4299548487337421]
	TIME [epoch: 32.3 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33240267831008985		[learning rate: 0.01711]
	Learning Rate: 0.0171104
	LOSS [training: 0.33240267831008985 | validation: 0.30007623342504286]
	TIME [epoch: 32.4 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32230053842871886		[learning rate: 0.017099]
	Learning Rate: 0.0170988
	LOSS [training: 0.32230053842871886 | validation: 0.3128249662353742]
	TIME [epoch: 32.3 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3178537471708147		[learning rate: 0.017087]
	Learning Rate: 0.0170871
	LOSS [training: 0.3178537471708147 | validation: 0.29344204817715935]
	TIME [epoch: 32.2 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30972688767943307		[learning rate: 0.017075]
	Learning Rate: 0.0170755
	LOSS [training: 0.30972688767943307 | validation: 0.45266678583001063]
	TIME [epoch: 32.4 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34326946829048366		[learning rate: 0.017064]
	Learning Rate: 0.0170638
	LOSS [training: 0.34326946829048366 | validation: 0.31327688321908664]
	TIME [epoch: 32.4 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34318708912131984		[learning rate: 0.017052]
	Learning Rate: 0.0170521
	LOSS [training: 0.34318708912131984 | validation: 0.35497845458324606]
	TIME [epoch: 32.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33777230419774396		[learning rate: 0.01704]
	Learning Rate: 0.0170403
	LOSS [training: 0.33777230419774396 | validation: 0.34468250876248807]
	TIME [epoch: 32.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33432744208530357		[learning rate: 0.017029]
	Learning Rate: 0.0170286
	LOSS [training: 0.33432744208530357 | validation: 0.3026510869148958]
	TIME [epoch: 32.4 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30758406526863424		[learning rate: 0.017017]
	Learning Rate: 0.0170168
	LOSS [training: 0.30758406526863424 | validation: 0.31381965975910875]
	TIME [epoch: 32.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36180928406910096		[learning rate: 0.017005]
	Learning Rate: 0.017005
	LOSS [training: 0.36180928406910096 | validation: 0.3212248533148611]
	TIME [epoch: 32.5 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3401466990746968		[learning rate: 0.016993]
	Learning Rate: 0.0169932
	LOSS [training: 0.3401466990746968 | validation: 0.30568116222328334]
	TIME [epoch: 32.4 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32392892511723326		[learning rate: 0.016981]
	Learning Rate: 0.0169814
	LOSS [training: 0.32392892511723326 | validation: 0.34314855928044874]
	TIME [epoch: 32.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.316361024013124		[learning rate: 0.01697]
	Learning Rate: 0.0169695
	LOSS [training: 0.316361024013124 | validation: 0.3854845186981958]
	TIME [epoch: 32.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3480372664636161		[learning rate: 0.016958]
	Learning Rate: 0.0169577
	LOSS [training: 0.3480372664636161 | validation: 0.3028067846574427]
	TIME [epoch: 32.4 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30931518120286156		[learning rate: 0.016946]
	Learning Rate: 0.0169458
	LOSS [training: 0.30931518120286156 | validation: 0.3619366782152916]
	TIME [epoch: 32.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31943007221236125		[learning rate: 0.016934]
	Learning Rate: 0.0169339
	LOSS [training: 0.31943007221236125 | validation: 0.33196362970265214]
	TIME [epoch: 32.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3299366913803043		[learning rate: 0.016922]
	Learning Rate: 0.016922
	LOSS [training: 0.3299366913803043 | validation: 0.3028075109856206]
	TIME [epoch: 32.4 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31634076554309853		[learning rate: 0.01691]
	Learning Rate: 0.01691
	LOSS [training: 0.31634076554309853 | validation: 0.32691208828453]
	TIME [epoch: 32.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34239507902491634		[learning rate: 0.016898]
	Learning Rate: 0.0168981
	LOSS [training: 0.34239507902491634 | validation: 0.3360036585055477]
	TIME [epoch: 32.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3273539484736893		[learning rate: 0.016886]
	Learning Rate: 0.0168861
	LOSS [training: 0.3273539484736893 | validation: 0.3487802310574937]
	TIME [epoch: 32.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3243728258353657		[learning rate: 0.016874]
	Learning Rate: 0.0168741
	LOSS [training: 0.3243728258353657 | validation: 0.29909491278034006]
	TIME [epoch: 32.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33852934306225624		[learning rate: 0.016862]
	Learning Rate: 0.0168621
	LOSS [training: 0.33852934306225624 | validation: 0.3234857232877568]
	TIME [epoch: 32.4 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30891250446037927		[learning rate: 0.01685]
	Learning Rate: 0.0168501
	LOSS [training: 0.30891250446037927 | validation: 0.369201177015104]
	TIME [epoch: 32.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3302445798344039		[learning rate: 0.016838]
	Learning Rate: 0.016838
	LOSS [training: 0.3302445798344039 | validation: 0.3158616911699372]
	TIME [epoch: 32.4 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32327795240516893		[learning rate: 0.016826]
	Learning Rate: 0.0168259
	LOSS [training: 0.32327795240516893 | validation: 0.298754027753273]
	TIME [epoch: 32.5 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33672472272813125		[learning rate: 0.016814]
	Learning Rate: 0.0168138
	LOSS [training: 0.33672472272813125 | validation: 0.3224983550287235]
	TIME [epoch: 32.4 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32422884361415477		[learning rate: 0.016802]
	Learning Rate: 0.0168017
	LOSS [training: 0.32422884361415477 | validation: 0.3083658659418671]
	TIME [epoch: 32.5 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3264872545401467		[learning rate: 0.01679]
	Learning Rate: 0.0167896
	LOSS [training: 0.3264872545401467 | validation: 0.3476668554719624]
	TIME [epoch: 32.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31423766803505093		[learning rate: 0.016777]
	Learning Rate: 0.0167775
	LOSS [training: 0.31423766803505093 | validation: 0.2993669942086019]
	TIME [epoch: 32.4 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.334202230670063		[learning rate: 0.016765]
	Learning Rate: 0.0167653
	LOSS [training: 0.334202230670063 | validation: 0.3030891872232253]
	TIME [epoch: 32.5 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3483824463831154		[learning rate: 0.016753]
	Learning Rate: 0.0167531
	LOSS [training: 0.3483824463831154 | validation: 0.34894332881465784]
	TIME [epoch: 32.4 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32017664823633896		[learning rate: 0.016741]
	Learning Rate: 0.0167409
	LOSS [training: 0.32017664823633896 | validation: 0.30332967228279883]
	TIME [epoch: 32.5 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3055993180734128		[learning rate: 0.016729]
	Learning Rate: 0.0167287
	LOSS [training: 0.3055993180734128 | validation: 0.283149615100766]
	TIME [epoch: 32.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30047051441218053		[learning rate: 0.016716]
	Learning Rate: 0.0167165
	LOSS [training: 0.30047051441218053 | validation: 0.29585843257668]
	TIME [epoch: 32.5 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3574416967475844		[learning rate: 0.016704]
	Learning Rate: 0.0167042
	LOSS [training: 0.3574416967475844 | validation: 0.4700741739116391]
	TIME [epoch: 32.4 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34304028333070535		[learning rate: 0.016692]
	Learning Rate: 0.0166919
	LOSS [training: 0.34304028333070535 | validation: 0.3161349870947692]
	TIME [epoch: 32.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34214226369756034		[learning rate: 0.01668]
	Learning Rate: 0.0166796
	LOSS [training: 0.34214226369756034 | validation: 0.30955397652144423]
	TIME [epoch: 32.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3201813698900986		[learning rate: 0.016667]
	Learning Rate: 0.0166673
	LOSS [training: 0.3201813698900986 | validation: 0.3369194821905521]
	TIME [epoch: 32.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3028987311418474		[learning rate: 0.016655]
	Learning Rate: 0.016655
	LOSS [training: 0.3028987311418474 | validation: 0.38980983198563435]
	TIME [epoch: 32.3 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34256392337581737		[learning rate: 0.016643]
	Learning Rate: 0.0166427
	LOSS [training: 0.34256392337581737 | validation: 0.30412750359594765]
	TIME [epoch: 32.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3316247895687823		[learning rate: 0.01663]
	Learning Rate: 0.0166303
	LOSS [training: 0.3316247895687823 | validation: 0.31640771230669823]
	TIME [epoch: 32.5 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3056000000708895		[learning rate: 0.016618]
	Learning Rate: 0.0166179
	LOSS [training: 0.3056000000708895 | validation: 0.2894695912756452]
	TIME [epoch: 32.4 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3317526980030515		[learning rate: 0.016606]
	Learning Rate: 0.0166055
	LOSS [training: 0.3317526980030515 | validation: 0.306022545071008]
	TIME [epoch: 32.5 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3582593597401897		[learning rate: 0.016593]
	Learning Rate: 0.0165931
	LOSS [training: 0.3582593597401897 | validation: 0.3082939061770193]
	TIME [epoch: 32.5 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29643131162474257		[learning rate: 0.016581]
	Learning Rate: 0.0165807
	LOSS [training: 0.29643131162474257 | validation: 0.28164502975830397]
	TIME [epoch: 32.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3105792621145573		[learning rate: 0.016568]
	Learning Rate: 0.0165682
	LOSS [training: 0.3105792621145573 | validation: 0.3519397932385734]
	TIME [epoch: 32.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35874170776886716		[learning rate: 0.016556]
	Learning Rate: 0.0165557
	LOSS [training: 0.35874170776886716 | validation: 0.2919945593505834]
	TIME [epoch: 32.4 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3409025199823465		[learning rate: 0.016543]
	Learning Rate: 0.0165432
	LOSS [training: 0.3409025199823465 | validation: 0.33076539636623503]
	TIME [epoch: 32.2 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3203553094631906		[learning rate: 0.016531]
	Learning Rate: 0.0165307
	LOSS [training: 0.3203553094631906 | validation: 0.29428778247161774]
	TIME [epoch: 32.2 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31227518425328227		[learning rate: 0.016518]
	Learning Rate: 0.0165182
	LOSS [training: 0.31227518425328227 | validation: 0.3008812509568829]
	TIME [epoch: 32.2 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3380546395707669		[learning rate: 0.016506]
	Learning Rate: 0.0165057
	LOSS [training: 0.3380546395707669 | validation: 0.33298869145570476]
	TIME [epoch: 32.2 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30730557458022784		[learning rate: 0.016493]
	Learning Rate: 0.0164931
	LOSS [training: 0.30730557458022784 | validation: 0.37527328737075194]
	TIME [epoch: 32.3 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3298378541060689		[learning rate: 0.016481]
	Learning Rate: 0.0164805
	LOSS [training: 0.3298378541060689 | validation: 0.33670945880219316]
	TIME [epoch: 32.3 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3133185109599213		[learning rate: 0.016468]
	Learning Rate: 0.0164679
	LOSS [training: 0.3133185109599213 | validation: 0.28019505888169666]
	TIME [epoch: 32.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_575.pth
	Model improved!!!
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3105556555453751		[learning rate: 0.016455]
	Learning Rate: 0.0164553
	LOSS [training: 0.3105556555453751 | validation: 0.3509418693080264]
	TIME [epoch: 32.4 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31588248279262865		[learning rate: 0.016443]
	Learning Rate: 0.0164427
	LOSS [training: 0.31588248279262865 | validation: 0.38155102466160085]
	TIME [epoch: 32.4 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3313541029982318		[learning rate: 0.01643]
	Learning Rate: 0.01643
	LOSS [training: 0.3313541029982318 | validation: 0.31397935422905066]
	TIME [epoch: 32.5 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3138772717101555		[learning rate: 0.016417]
	Learning Rate: 0.0164173
	LOSS [training: 0.3138772717101555 | validation: 0.3363865246430654]
	TIME [epoch: 32.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3339522553758174		[learning rate: 0.016405]
	Learning Rate: 0.0164047
	LOSS [training: 0.3339522553758174 | validation: 0.30031335534454817]
	TIME [epoch: 32.4 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3127719811910318		[learning rate: 0.016392]
	Learning Rate: 0.016392
	LOSS [training: 0.3127719811910318 | validation: 0.34017916342069887]
	TIME [epoch: 32.4 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3340804924161219		[learning rate: 0.016379]
	Learning Rate: 0.0163792
	LOSS [training: 0.3340804924161219 | validation: 0.30467016143370507]
	TIME [epoch: 32.4 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31491473152129734		[learning rate: 0.016366]
	Learning Rate: 0.0163665
	LOSS [training: 0.31491473152129734 | validation: 0.37259465474487824]
	TIME [epoch: 32.4 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32270793913746054		[learning rate: 0.016354]
	Learning Rate: 0.0163537
	LOSS [training: 0.32270793913746054 | validation: 0.2964240779675231]
	TIME [epoch: 32.4 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31531383231703686		[learning rate: 0.016341]
	Learning Rate: 0.016341
	LOSS [training: 0.31531383231703686 | validation: 0.3442085417433082]
	TIME [epoch: 32.5 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30054701350562224		[learning rate: 0.016328]
	Learning Rate: 0.0163282
	LOSS [training: 0.30054701350562224 | validation: 0.3158992522465981]
	TIME [epoch: 32.4 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2935867476097088		[learning rate: 0.016315]
	Learning Rate: 0.0163154
	LOSS [training: 0.2935867476097088 | validation: 0.4170458489062629]
	TIME [epoch: 32.4 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3772245442416797		[learning rate: 0.016303]
	Learning Rate: 0.0163025
	LOSS [training: 0.3772245442416797 | validation: 0.34016434955863417]
	TIME [epoch: 32.4 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3255413240930201		[learning rate: 0.01629]
	Learning Rate: 0.0162897
	LOSS [training: 0.3255413240930201 | validation: 0.3208627948541024]
	TIME [epoch: 32.4 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32144954818360705		[learning rate: 0.016277]
	Learning Rate: 0.0162768
	LOSS [training: 0.32144954818360705 | validation: 0.2980654786207677]
	TIME [epoch: 32.4 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3084484882683657		[learning rate: 0.016264]
	Learning Rate: 0.016264
	LOSS [training: 0.3084484882683657 | validation: 0.32182691380540385]
	TIME [epoch: 32.4 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31586980850064905		[learning rate: 0.016251]
	Learning Rate: 0.0162511
	LOSS [training: 0.31586980850064905 | validation: 0.3377471707778703]
	TIME [epoch: 32.4 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2994162740350148		[learning rate: 0.016238]
	Learning Rate: 0.0162382
	LOSS [training: 0.2994162740350148 | validation: 0.32151708529442163]
	TIME [epoch: 32.4 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3316863794564565		[learning rate: 0.016225]
	Learning Rate: 0.0162252
	LOSS [training: 0.3316863794564565 | validation: 0.32556897262729423]
	TIME [epoch: 32.4 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30104580690550986		[learning rate: 0.016212]
	Learning Rate: 0.0162123
	LOSS [training: 0.30104580690550986 | validation: 0.37790499925252874]
	TIME [epoch: 32.4 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31178121280579885		[learning rate: 0.016199]
	Learning Rate: 0.0161993
	LOSS [training: 0.31178121280579885 | validation: 0.3067559324411405]
	TIME [epoch: 32.4 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33989419792019543		[learning rate: 0.016186]
	Learning Rate: 0.0161864
	LOSS [training: 0.33989419792019543 | validation: 0.28458543562907485]
	TIME [epoch: 32.4 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3146487130314231		[learning rate: 0.016173]
	Learning Rate: 0.0161734
	LOSS [training: 0.3146487130314231 | validation: 0.3474048435468168]
	TIME [epoch: 32.3 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31511746340265445		[learning rate: 0.01616]
	Learning Rate: 0.0161603
	LOSS [training: 0.31511746340265445 | validation: 0.2800753221668397]
	TIME [epoch: 32.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_599.pth
	Model improved!!!
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32468344822567585		[learning rate: 0.016147]
	Learning Rate: 0.0161473
	LOSS [training: 0.32468344822567585 | validation: 0.37514572644685856]
	TIME [epoch: 32.6 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3203132674748178		[learning rate: 0.016134]
	Learning Rate: 0.0161343
	LOSS [training: 0.3203132674748178 | validation: 0.38839447941001803]
	TIME [epoch: 32.4 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31955610981900606		[learning rate: 0.016121]
	Learning Rate: 0.0161212
	LOSS [training: 0.31955610981900606 | validation: 0.29344905421212736]
	TIME [epoch: 32.7 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3136795034712235		[learning rate: 0.016108]
	Learning Rate: 0.0161081
	LOSS [training: 0.3136795034712235 | validation: 0.3054373338782923]
	TIME [epoch: 32.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33967479565410996		[learning rate: 0.016095]
	Learning Rate: 0.016095
	LOSS [training: 0.33967479565410996 | validation: 0.3169848280074532]
	TIME [epoch: 32.7 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30981908234110456		[learning rate: 0.016082]
	Learning Rate: 0.0160819
	LOSS [training: 0.30981908234110456 | validation: 0.3036761248733162]
	TIME [epoch: 32.6 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3187280308519616		[learning rate: 0.016069]
	Learning Rate: 0.0160688
	LOSS [training: 0.3187280308519616 | validation: 0.3929702364758776]
	TIME [epoch: 32.6 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3167212710687714		[learning rate: 0.016056]
	Learning Rate: 0.0160556
	LOSS [training: 0.3167212710687714 | validation: 0.28075089639008083]
	TIME [epoch: 32.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3099977104534961		[learning rate: 0.016042]
	Learning Rate: 0.0160425
	LOSS [training: 0.3099977104534961 | validation: 0.3780102627453157]
	TIME [epoch: 32.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3133911082480912		[learning rate: 0.016029]
	Learning Rate: 0.0160293
	LOSS [training: 0.3133911082480912 | validation: 0.29203814193756383]
	TIME [epoch: 32.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30827749814920236		[learning rate: 0.016016]
	Learning Rate: 0.0160161
	LOSS [training: 0.30827749814920236 | validation: 0.278899986627702]
	TIME [epoch: 32.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32880512257916417		[learning rate: 0.016003]
	Learning Rate: 0.0160029
	LOSS [training: 0.32880512257916417 | validation: 0.36664946964243383]
	TIME [epoch: 32.5 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31450494667938256		[learning rate: 0.01599]
	Learning Rate: 0.0159897
	LOSS [training: 0.31450494667938256 | validation: 0.3129968071127004]
	TIME [epoch: 32.4 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3331673329314395		[learning rate: 0.015976]
	Learning Rate: 0.0159764
	LOSS [training: 0.3331673329314395 | validation: 0.29749274164204625]
	TIME [epoch: 32.5 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3141766278805236		[learning rate: 0.015963]
	Learning Rate: 0.0159632
	LOSS [training: 0.3141766278805236 | validation: 0.2909926172961877]
	TIME [epoch: 32.5 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3184379039347849		[learning rate: 0.01595]
	Learning Rate: 0.0159499
	LOSS [training: 0.3184379039347849 | validation: 0.32309252472591193]
	TIME [epoch: 32.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3127735599066914		[learning rate: 0.015937]
	Learning Rate: 0.0159366
	LOSS [training: 0.3127735599066914 | validation: 0.27484837296918885]
	TIME [epoch: 32.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_616.pth
	Model improved!!!
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31003267630147613		[learning rate: 0.015923]
	Learning Rate: 0.0159233
	LOSS [training: 0.31003267630147613 | validation: 0.41219119178902325]
	TIME [epoch: 32.4 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34462178592806		[learning rate: 0.01591]
	Learning Rate: 0.01591
	LOSS [training: 0.34462178592806 | validation: 0.3035429148599801]
	TIME [epoch: 32.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31191335819914345		[learning rate: 0.015897]
	Learning Rate: 0.0158966
	LOSS [training: 0.31191335819914345 | validation: 0.29865648365940317]
	TIME [epoch: 32.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29988549309604723		[learning rate: 0.015883]
	Learning Rate: 0.0158833
	LOSS [training: 0.29988549309604723 | validation: 0.2975250787618347]
	TIME [epoch: 32.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32038060393470513		[learning rate: 0.01587]
	Learning Rate: 0.0158699
	LOSS [training: 0.32038060393470513 | validation: 0.37345919163588914]
	TIME [epoch: 32.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30901613580254206		[learning rate: 0.015856]
	Learning Rate: 0.0158565
	LOSS [training: 0.30901613580254206 | validation: 0.2701604152603407]
	TIME [epoch: 32.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29644777921611065		[learning rate: 0.015843]
	Learning Rate: 0.0158431
	LOSS [training: 0.29644777921611065 | validation: 0.3341755876079695]
	TIME [epoch: 32.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33199952782975417		[learning rate: 0.01583]
	Learning Rate: 0.0158297
	LOSS [training: 0.33199952782975417 | validation: 0.3091023227832065]
	TIME [epoch: 32.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2946487125187735		[learning rate: 0.015816]
	Learning Rate: 0.0158162
	LOSS [training: 0.2946487125187735 | validation: 0.31347189964275135]
	TIME [epoch: 32.6 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34674737552290724		[learning rate: 0.015803]
	Learning Rate: 0.0158028
	LOSS [training: 0.34674737552290724 | validation: 0.2903216269350197]
	TIME [epoch: 32.6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3190969204123398		[learning rate: 0.015789]
	Learning Rate: 0.0157893
	LOSS [training: 0.3190969204123398 | validation: 0.29473191334909654]
	TIME [epoch: 32.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3048554564502417		[learning rate: 0.015776]
	Learning Rate: 0.0157758
	LOSS [training: 0.3048554564502417 | validation: 0.2803315932683789]
	TIME [epoch: 32.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3074890925096978		[learning rate: 0.015762]
	Learning Rate: 0.0157623
	LOSS [training: 0.3074890925096978 | validation: 0.373768509940437]
	TIME [epoch: 32.6 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32649698347876066		[learning rate: 0.015749]
	Learning Rate: 0.0157488
	LOSS [training: 0.32649698347876066 | validation: 0.31740462152521776]
	TIME [epoch: 32.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31132404415259335		[learning rate: 0.015735]
	Learning Rate: 0.0157353
	LOSS [training: 0.31132404415259335 | validation: 0.32989698729367345]
	TIME [epoch: 32.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31655089464428526		[learning rate: 0.015722]
	Learning Rate: 0.0157217
	LOSS [training: 0.31655089464428526 | validation: 0.2887579247728925]
	TIME [epoch: 32.7 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3125093701168287		[learning rate: 0.015708]
	Learning Rate: 0.0157082
	LOSS [training: 0.3125093701168287 | validation: 0.3423251820141143]
	TIME [epoch: 32.7 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30004001857658974		[learning rate: 0.015695]
	Learning Rate: 0.0156946
	LOSS [training: 0.30004001857658974 | validation: 0.3288193194960911]
	TIME [epoch: 32.6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32175039064341804		[learning rate: 0.015681]
	Learning Rate: 0.015681
	LOSS [training: 0.32175039064341804 | validation: 0.32727483769014665]
	TIME [epoch: 32.6 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29917485251733683		[learning rate: 0.015667]
	Learning Rate: 0.0156674
	LOSS [training: 0.29917485251733683 | validation: 0.3035094628381234]
	TIME [epoch: 32.6 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31886273010204463		[learning rate: 0.015654]
	Learning Rate: 0.0156537
	LOSS [training: 0.31886273010204463 | validation: 0.29396611972215037]
	TIME [epoch: 32.6 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3206573273960893		[learning rate: 0.01564]
	Learning Rate: 0.0156401
	LOSS [training: 0.3206573273960893 | validation: 0.3653911027607901]
	TIME [epoch: 32.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30668785460413917		[learning rate: 0.015626]
	Learning Rate: 0.0156264
	LOSS [training: 0.30668785460413917 | validation: 0.2975064871127381]
	TIME [epoch: 32.7 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30418379744728596		[learning rate: 0.015613]
	Learning Rate: 0.0156128
	LOSS [training: 0.30418379744728596 | validation: 0.2865114646724447]
	TIME [epoch: 32.7 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3094180968388991		[learning rate: 0.015599]
	Learning Rate: 0.0155991
	LOSS [training: 0.3094180968388991 | validation: 0.3290527412825438]
	TIME [epoch: 32.6 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30367002283065825		[learning rate: 0.015585]
	Learning Rate: 0.0155854
	LOSS [training: 0.30367002283065825 | validation: 0.340114120673076]
	TIME [epoch: 32.7 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29282130938508777		[learning rate: 0.015572]
	Learning Rate: 0.0155717
	LOSS [training: 0.29282130938508777 | validation: 0.3119177564708696]
	TIME [epoch: 32.7 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31800164171090983		[learning rate: 0.015558]
	Learning Rate: 0.0155579
	LOSS [training: 0.31800164171090983 | validation: 0.40060552578865305]
	TIME [epoch: 32.7 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3356573047161586		[learning rate: 0.015544]
	Learning Rate: 0.0155442
	LOSS [training: 0.3356573047161586 | validation: 0.30190982413600176]
	TIME [epoch: 32.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3177642285539367		[learning rate: 0.01553]
	Learning Rate: 0.0155304
	LOSS [training: 0.3177642285539367 | validation: 0.360599729236885]
	TIME [epoch: 32.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3435107830327279		[learning rate: 0.015517]
	Learning Rate: 0.0155166
	LOSS [training: 0.3435107830327279 | validation: 0.3262841814262294]
	TIME [epoch: 32.7 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29937543800086863		[learning rate: 0.015503]
	Learning Rate: 0.0155028
	LOSS [training: 0.29937543800086863 | validation: 0.33165633438564224]
	TIME [epoch: 32.7 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2937794990543405		[learning rate: 0.015489]
	Learning Rate: 0.015489
	LOSS [training: 0.2937794990543405 | validation: 0.3014373701362407]
	TIME [epoch: 32.7 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33347727479590455		[learning rate: 0.015475]
	Learning Rate: 0.0154752
	LOSS [training: 0.33347727479590455 | validation: 0.3217041482624886]
	TIME [epoch: 32.7 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2974377499062557		[learning rate: 0.015461]
	Learning Rate: 0.0154614
	LOSS [training: 0.2974377499062557 | validation: 0.302881579151963]
	TIME [epoch: 32.7 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3141733864470023		[learning rate: 0.015448]
	Learning Rate: 0.0154475
	LOSS [training: 0.3141733864470023 | validation: 0.27290363772679]
	TIME [epoch: 32.7 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3029222208844268		[learning rate: 0.015434]
	Learning Rate: 0.0154336
	LOSS [training: 0.3029222208844268 | validation: 0.2896422485946414]
	TIME [epoch: 32.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3085991813470103		[learning rate: 0.01542]
	Learning Rate: 0.0154198
	LOSS [training: 0.3085991813470103 | validation: 0.2799716036897185]
	TIME [epoch: 32.7 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3199070815900327		[learning rate: 0.015406]
	Learning Rate: 0.0154059
	LOSS [training: 0.3199070815900327 | validation: 0.3005490613170693]
	TIME [epoch: 32.7 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2828127331935369		[learning rate: 0.015392]
	Learning Rate: 0.0153919
	LOSS [training: 0.2828127331935369 | validation: 0.3252851507715121]
	TIME [epoch: 32.5 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3291184909809223		[learning rate: 0.015378]
	Learning Rate: 0.015378
	LOSS [training: 0.3291184909809223 | validation: 0.295344832736133]
	TIME [epoch: 32.6 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27289474227780414		[learning rate: 0.015364]
	Learning Rate: 0.0153641
	LOSS [training: 0.27289474227780414 | validation: 0.35319553658168934]
	TIME [epoch: 32.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3394455244853773		[learning rate: 0.01535]
	Learning Rate: 0.0153501
	LOSS [training: 0.3394455244853773 | validation: 0.31696932788744697]
	TIME [epoch: 32.6 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.307143443641012		[learning rate: 0.015336]
	Learning Rate: 0.0153361
	LOSS [training: 0.307143443641012 | validation: 0.3079394338081656]
	TIME [epoch: 32.6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32306040306333117		[learning rate: 0.015322]
	Learning Rate: 0.0153222
	LOSS [training: 0.32306040306333117 | validation: 0.3015974420674431]
	TIME [epoch: 32.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2922516207617878		[learning rate: 0.015308]
	Learning Rate: 0.0153082
	LOSS [training: 0.2922516207617878 | validation: 0.2879424414380359]
	TIME [epoch: 32.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3197560395891589		[learning rate: 0.015294]
	Learning Rate: 0.0152941
	LOSS [training: 0.3197560395891589 | validation: 0.30000216486968684]
	TIME [epoch: 32.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30661497311460095		[learning rate: 0.01528]
	Learning Rate: 0.0152801
	LOSS [training: 0.30661497311460095 | validation: 0.4199146851587186]
	TIME [epoch: 32.6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3357320207651996		[learning rate: 0.015266]
	Learning Rate: 0.0152661
	LOSS [training: 0.3357320207651996 | validation: 0.34344511595415483]
	TIME [epoch: 32.5 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2873822209736346		[learning rate: 0.015252]
	Learning Rate: 0.015252
	LOSS [training: 0.2873822209736346 | validation: 0.28242419986538936]
	TIME [epoch: 32.6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3372301874844273		[learning rate: 0.015238]
	Learning Rate: 0.0152379
	LOSS [training: 0.3372301874844273 | validation: 0.29472384950782504]
	TIME [epoch: 32.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2915748513188668		[learning rate: 0.015224]
	Learning Rate: 0.0152238
	LOSS [training: 0.2915748513188668 | validation: 0.33458463413733985]
	TIME [epoch: 32.6 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31044289939351044		[learning rate: 0.01521]
	Learning Rate: 0.0152097
	LOSS [training: 0.31044289939351044 | validation: 0.303200514126836]
	TIME [epoch: 32.5 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32955058247592806		[learning rate: 0.015196]
	Learning Rate: 0.0151956
	LOSS [training: 0.32955058247592806 | validation: 0.2900787671665721]
	TIME [epoch: 32.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2918726310142703		[learning rate: 0.015182]
	Learning Rate: 0.0151815
	LOSS [training: 0.2918726310142703 | validation: 0.2841934438838698]
	TIME [epoch: 32.6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30483847787424967		[learning rate: 0.015167]
	Learning Rate: 0.0151674
	LOSS [training: 0.30483847787424967 | validation: 0.3235896943999181]
	TIME [epoch: 32.5 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32955667147009304		[learning rate: 0.015153]
	Learning Rate: 0.0151532
	LOSS [training: 0.32955667147009304 | validation: 0.32096222783708367]
	TIME [epoch: 32.7 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29285118627469503		[learning rate: 0.015139]
	Learning Rate: 0.015139
	LOSS [training: 0.29285118627469503 | validation: 0.2776893821450286]
	TIME [epoch: 32.6 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2948372074703126		[learning rate: 0.015125]
	Learning Rate: 0.0151248
	LOSS [training: 0.2948372074703126 | validation: 0.2987149495570019]
	TIME [epoch: 32.4 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3127448338183647		[learning rate: 0.015111]
	Learning Rate: 0.0151106
	LOSS [training: 0.3127448338183647 | validation: 0.27578118752254016]
	TIME [epoch: 32.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28807061189454053		[learning rate: 0.015096]
	Learning Rate: 0.0150964
	LOSS [training: 0.28807061189454053 | validation: 0.2847238216023382]
	TIME [epoch: 32.6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3107542073504115		[learning rate: 0.015082]
	Learning Rate: 0.0150822
	LOSS [training: 0.3107542073504115 | validation: 0.331310191957364]
	TIME [epoch: 32.7 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2979318628508268		[learning rate: 0.015068]
	Learning Rate: 0.015068
	LOSS [training: 0.2979318628508268 | validation: 0.275879326847124]
	TIME [epoch: 32.6 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3056213500823502		[learning rate: 0.015054]
	Learning Rate: 0.0150537
	LOSS [training: 0.3056213500823502 | validation: 0.269809334048181]
	TIME [epoch: 32.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2969008811256639		[learning rate: 0.015039]
	Learning Rate: 0.0150394
	LOSS [training: 0.2969008811256639 | validation: 0.33014883306303056]
	TIME [epoch: 32.7 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31804605716021866		[learning rate: 0.015025]
	Learning Rate: 0.0150251
	LOSS [training: 0.31804605716021866 | validation: 0.2775497811471936]
	TIME [epoch: 32.6 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27490257994703005		[learning rate: 0.015011]
	Learning Rate: 0.0150108
	LOSS [training: 0.27490257994703005 | validation: 0.2838697713922279]
	TIME [epoch: 32.6 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3182532429035482		[learning rate: 0.014997]
	Learning Rate: 0.0149965
	LOSS [training: 0.3182532429035482 | validation: 0.3356621922240768]
	TIME [epoch: 32.7 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3040777206073046		[learning rate: 0.014982]
	Learning Rate: 0.0149822
	LOSS [training: 0.3040777206073046 | validation: 0.28129805842327205]
	TIME [epoch: 32.6 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30370287679655206		[learning rate: 0.014968]
	Learning Rate: 0.0149679
	LOSS [training: 0.30370287679655206 | validation: 0.2830813959189412]
	TIME [epoch: 32.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29636331726426135		[learning rate: 0.014954]
	Learning Rate: 0.0149535
	LOSS [training: 0.29636331726426135 | validation: 0.332421488030165]
	TIME [epoch: 32.6 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3127803918956763		[learning rate: 0.014939]
	Learning Rate: 0.0149392
	LOSS [training: 0.3127803918956763 | validation: 0.35249685840195977]
	TIME [epoch: 32.6 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31674354396999427		[learning rate: 0.014925]
	Learning Rate: 0.0149248
	LOSS [training: 0.31674354396999427 | validation: 0.28496106241153374]
	TIME [epoch: 32.7 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3030236533151506		[learning rate: 0.01491]
	Learning Rate: 0.0149104
	LOSS [training: 0.3030236533151506 | validation: 0.2981409801090067]
	TIME [epoch: 32.6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3095491603112981		[learning rate: 0.014896]
	Learning Rate: 0.014896
	LOSS [training: 0.3095491603112981 | validation: 0.3364762465840684]
	TIME [epoch: 32.5 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31881464320101205		[learning rate: 0.014882]
	Learning Rate: 0.0148816
	LOSS [training: 0.31881464320101205 | validation: 0.28378970225730366]
	TIME [epoch: 32.7 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29835407899274435		[learning rate: 0.014867]
	Learning Rate: 0.0148671
	LOSS [training: 0.29835407899274435 | validation: 0.4191927719056422]
	TIME [epoch: 32.7 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33790609673089544		[learning rate: 0.014853]
	Learning Rate: 0.0148527
	LOSS [training: 0.33790609673089544 | validation: 0.28726186645626867]
	TIME [epoch: 32.5 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30834313469243596		[learning rate: 0.014838]
	Learning Rate: 0.0148382
	LOSS [training: 0.30834313469243596 | validation: 0.29576949304152356]
	TIME [epoch: 32.6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.294960642379894		[learning rate: 0.014824]
	Learning Rate: 0.0148237
	LOSS [training: 0.294960642379894 | validation: 0.2979037367305294]
	TIME [epoch: 32.6 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27359110596688996		[learning rate: 0.014809]
	Learning Rate: 0.0148093
	LOSS [training: 0.27359110596688996 | validation: 0.32398623625280226]
	TIME [epoch: 32.6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32548484208688		[learning rate: 0.014795]
	Learning Rate: 0.0147948
	LOSS [training: 0.32548484208688 | validation: 0.34374984317029034]
	TIME [epoch: 32.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3135113762308084		[learning rate: 0.01478]
	Learning Rate: 0.0147803
	LOSS [training: 0.3135113762308084 | validation: 0.27328080220007656]
	TIME [epoch: 32.5 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29242689418030643		[learning rate: 0.014766]
	Learning Rate: 0.0147657
	LOSS [training: 0.29242689418030643 | validation: 0.2646460067475565]
	TIME [epoch: 32.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_700.pth
	Model improved!!!
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28934812081037087		[learning rate: 0.014751]
	Learning Rate: 0.0147512
	LOSS [training: 0.28934812081037087 | validation: 0.3366249771172604]
	TIME [epoch: 32.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3226431474723601		[learning rate: 0.014737]
	Learning Rate: 0.0147366
	LOSS [training: 0.3226431474723601 | validation: 0.30756542930628683]
	TIME [epoch: 32.7 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3277596935858974		[learning rate: 0.014722]
	Learning Rate: 0.0147221
	LOSS [training: 0.3277596935858974 | validation: 0.2941871326394471]
	TIME [epoch: 32.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30546027115321706		[learning rate: 0.014707]
	Learning Rate: 0.0147075
	LOSS [training: 0.30546027115321706 | validation: 0.28698196958728117]
	TIME [epoch: 32.5 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2875141318669833		[learning rate: 0.014693]
	Learning Rate: 0.0146929
	LOSS [training: 0.2875141318669833 | validation: 0.3052193146404094]
	TIME [epoch: 32.5 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30269944584962205		[learning rate: 0.014678]
	Learning Rate: 0.0146783
	LOSS [training: 0.30269944584962205 | validation: 0.28673731935003843]
	TIME [epoch: 32.6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3316353309411876		[learning rate: 0.014664]
	Learning Rate: 0.0146637
	LOSS [training: 0.3316353309411876 | validation: 0.3521286588718687]
	TIME [epoch: 32.6 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29325756094242655		[learning rate: 0.014649]
	Learning Rate: 0.0146491
	LOSS [training: 0.29325756094242655 | validation: 0.30892239991723774]
	TIME [epoch: 32.4 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3181421249784647		[learning rate: 0.014634]
	Learning Rate: 0.0146344
	LOSS [training: 0.3181421249784647 | validation: 0.3162709097295532]
	TIME [epoch: 32.5 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.317658385848828		[learning rate: 0.01462]
	Learning Rate: 0.0146198
	LOSS [training: 0.317658385848828 | validation: 0.32637629383747463]
	TIME [epoch: 32.7 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.312120275859853		[learning rate: 0.014605]
	Learning Rate: 0.0146051
	LOSS [training: 0.312120275859853 | validation: 0.31339314621942427]
	TIME [epoch: 32.6 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3006029308975571		[learning rate: 0.01459]
	Learning Rate: 0.0145904
	LOSS [training: 0.3006029308975571 | validation: 0.2792594466685654]
	TIME [epoch: 32.6 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28692372654937776		[learning rate: 0.014576]
	Learning Rate: 0.0145757
	LOSS [training: 0.28692372654937776 | validation: 0.27021940180738635]
	TIME [epoch: 32.7 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2943075562222468		[learning rate: 0.014561]
	Learning Rate: 0.014561
	LOSS [training: 0.2943075562222468 | validation: 0.28387019503106814]
	TIME [epoch: 32.5 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33530776013495656		[learning rate: 0.014546]
	Learning Rate: 0.0145463
	LOSS [training: 0.33530776013495656 | validation: 0.3146579969530214]
	TIME [epoch: 32.7 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2859684139769951		[learning rate: 0.014532]
	Learning Rate: 0.0145316
	LOSS [training: 0.2859684139769951 | validation: 0.260279798911896]
	TIME [epoch: 32.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_716.pth
	Model improved!!!
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.314583727690836		[learning rate: 0.014517]
	Learning Rate: 0.0145168
	LOSS [training: 0.314583727690836 | validation: 0.32018174613664463]
	TIME [epoch: 32.5 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2849043691405463		[learning rate: 0.014502]
	Learning Rate: 0.0145021
	LOSS [training: 0.2849043691405463 | validation: 0.27579298518157036]
	TIME [epoch: 32.6 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2955012885104529		[learning rate: 0.014487]
	Learning Rate: 0.0144873
	LOSS [training: 0.2955012885104529 | validation: 0.3650725361426248]
	TIME [epoch: 32.7 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3179491593859989		[learning rate: 0.014473]
	Learning Rate: 0.0144726
	LOSS [training: 0.3179491593859989 | validation: 0.2741116842478204]
	TIME [epoch: 32.7 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2883767874561298		[learning rate: 0.014458]
	Learning Rate: 0.0144578
	LOSS [training: 0.2883767874561298 | validation: 0.26282900022576033]
	TIME [epoch: 32.6 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30039454634024443		[learning rate: 0.014443]
	Learning Rate: 0.014443
	LOSS [training: 0.30039454634024443 | validation: 0.29761685991920794]
	TIME [epoch: 32.7 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27613050520758753		[learning rate: 0.014428]
	Learning Rate: 0.0144281
	LOSS [training: 0.27613050520758753 | validation: 0.2732474498944797]
	TIME [epoch: 32.7 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3278363480935603		[learning rate: 0.014413]
	Learning Rate: 0.0144133
	LOSS [training: 0.3278363480935603 | validation: 0.2807260001590801]
	TIME [epoch: 32.6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33480305363742663		[learning rate: 0.014398]
	Learning Rate: 0.0143985
	LOSS [training: 0.33480305363742663 | validation: 0.3296917799579777]
	TIME [epoch: 32.6 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2866801662018175		[learning rate: 0.014384]
	Learning Rate: 0.0143836
	LOSS [training: 0.2866801662018175 | validation: 0.2737422081724717]
	TIME [epoch: 32.7 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2918664158335842		[learning rate: 0.014369]
	Learning Rate: 0.0143688
	LOSS [training: 0.2918664158335842 | validation: 0.28629383208049375]
	TIME [epoch: 32.5 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2914215476351077		[learning rate: 0.014354]
	Learning Rate: 0.0143539
	LOSS [training: 0.2914215476351077 | validation: 0.2759251736650531]
	TIME [epoch: 32.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28028706357678357		[learning rate: 0.014339]
	Learning Rate: 0.014339
	LOSS [training: 0.28028706357678357 | validation: 0.32560793278558775]
	TIME [epoch: 32.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3643112997423934		[learning rate: 0.014324]
	Learning Rate: 0.0143241
	LOSS [training: 0.3643112997423934 | validation: 0.30706403045411823]
	TIME [epoch: 32.7 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2896979076999456		[learning rate: 0.014309]
	Learning Rate: 0.0143092
	LOSS [training: 0.2896979076999456 | validation: 0.27996022291066786]
	TIME [epoch: 32.7 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2952087307716423		[learning rate: 0.014294]
	Learning Rate: 0.0142943
	LOSS [training: 0.2952087307716423 | validation: 0.2764571196656058]
	TIME [epoch: 32.6 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2971261867708953		[learning rate: 0.014279]
	Learning Rate: 0.0142793
	LOSS [training: 0.2971261867708953 | validation: 0.2581837220709021]
	TIME [epoch: 32.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_733.pth
	Model improved!!!
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2836317055459001		[learning rate: 0.014264]
	Learning Rate: 0.0142644
	LOSS [training: 0.2836317055459001 | validation: 0.27786354972337846]
	TIME [epoch: 32.6 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2889562178591279		[learning rate: 0.014249]
	Learning Rate: 0.0142494
	LOSS [training: 0.2889562178591279 | validation: 0.2829108414127368]
	TIME [epoch: 32.8 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28163555513668265		[learning rate: 0.014234]
	Learning Rate: 0.0142345
	LOSS [training: 0.28163555513668265 | validation: 0.28397358482414137]
	TIME [epoch: 32.7 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3151196025264759		[learning rate: 0.014219]
	Learning Rate: 0.0142195
	LOSS [training: 0.3151196025264759 | validation: 0.3081852722039717]
	TIME [epoch: 32.7 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2792136151038878		[learning rate: 0.014205]
	Learning Rate: 0.0142045
	LOSS [training: 0.2792136151038878 | validation: 0.2857965128699217]
	TIME [epoch: 32.6 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2875024875327729		[learning rate: 0.01419]
	Learning Rate: 0.0141895
	LOSS [training: 0.2875024875327729 | validation: 0.28688952547876356]
	TIME [epoch: 32.5 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30599303069918765		[learning rate: 0.014174]
	Learning Rate: 0.0141745
	LOSS [training: 0.30599303069918765 | validation: 0.29486280156549927]
	TIME [epoch: 32.7 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2965619081055051		[learning rate: 0.014159]
	Learning Rate: 0.0141595
	LOSS [training: 0.2965619081055051 | validation: 0.3556978728667781]
	TIME [epoch: 32.7 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29323932459247826		[learning rate: 0.014144]
	Learning Rate: 0.0141444
	LOSS [training: 0.29323932459247826 | validation: 0.2667683976746695]
	TIME [epoch: 32.6 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29946037275794624		[learning rate: 0.014129]
	Learning Rate: 0.0141294
	LOSS [training: 0.29946037275794624 | validation: 0.32763939386404134]
	TIME [epoch: 32.6 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31529779194527086		[learning rate: 0.014114]
	Learning Rate: 0.0141143
	LOSS [training: 0.31529779194527086 | validation: 0.2756623033208628]
	TIME [epoch: 32.6 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27704238961248545		[learning rate: 0.014099]
	Learning Rate: 0.0140992
	LOSS [training: 0.27704238961248545 | validation: 0.270341127981709]
	TIME [epoch: 32.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31543891702459237		[learning rate: 0.014084]
	Learning Rate: 0.0140842
	LOSS [training: 0.31543891702459237 | validation: 0.3071470720244614]
	TIME [epoch: 32.6 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2918687985694227		[learning rate: 0.014069]
	Learning Rate: 0.0140691
	LOSS [training: 0.2918687985694227 | validation: 0.26917351021357]
	TIME [epoch: 32.7 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30871907688705674		[learning rate: 0.014054]
	Learning Rate: 0.014054
	LOSS [training: 0.30871907688705674 | validation: 0.28078162662409056]
	TIME [epoch: 32.7 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2910836425696811		[learning rate: 0.014039]
	Learning Rate: 0.0140389
	LOSS [training: 0.2910836425696811 | validation: 0.27430892883109936]
	TIME [epoch: 32.6 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2750521990471851		[learning rate: 0.014024]
	Learning Rate: 0.0140237
	LOSS [training: 0.2750521990471851 | validation: 0.25820355717641497]
	TIME [epoch: 32.7 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3271278093582765		[learning rate: 0.014009]
	Learning Rate: 0.0140086
	LOSS [training: 0.3271278093582765 | validation: 0.3227203891393532]
	TIME [epoch: 32.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30127196789661065		[learning rate: 0.013993]
	Learning Rate: 0.0139934
	LOSS [training: 0.30127196789661065 | validation: 0.28786140456024734]
	TIME [epoch: 32.7 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2828537400179134		[learning rate: 0.013978]
	Learning Rate: 0.0139783
	LOSS [training: 0.2828537400179134 | validation: 0.2647677585132356]
	TIME [epoch: 32.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28580200557263924		[learning rate: 0.013963]
	Learning Rate: 0.0139631
	LOSS [training: 0.28580200557263924 | validation: 0.38153094666820475]
	TIME [epoch: 32.7 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30705508739109144		[learning rate: 0.013948]
	Learning Rate: 0.0139479
	LOSS [training: 0.30705508739109144 | validation: 0.27040170240424866]
	TIME [epoch: 32.7 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28824980968224556		[learning rate: 0.013933]
	Learning Rate: 0.0139327
	LOSS [training: 0.28824980968224556 | validation: 0.32619665841370216]
	TIME [epoch: 32.7 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3010892410501921		[learning rate: 0.013918]
	Learning Rate: 0.0139175
	LOSS [training: 0.3010892410501921 | validation: 0.2836544408629081]
	TIME [epoch: 32.6 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2978739122565295		[learning rate: 0.013902]
	Learning Rate: 0.0139023
	LOSS [training: 0.2978739122565295 | validation: 0.2641484488659891]
	TIME [epoch: 32.6 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27590615989159284		[learning rate: 0.013887]
	Learning Rate: 0.0138871
	LOSS [training: 0.27590615989159284 | validation: 0.3039919262020935]
	TIME [epoch: 32.7 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2899706038996348		[learning rate: 0.013872]
	Learning Rate: 0.0138719
	LOSS [training: 0.2899706038996348 | validation: 0.2654472660627107]
	TIME [epoch: 32.7 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31957323601295795		[learning rate: 0.013857]
	Learning Rate: 0.0138566
	LOSS [training: 0.31957323601295795 | validation: 0.3036877613779771]
	TIME [epoch: 32.7 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2895723345083518		[learning rate: 0.013841]
	Learning Rate: 0.0138414
	LOSS [training: 0.2895723345083518 | validation: 0.2964288969539737]
	TIME [epoch: 32.6 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28415730170390063		[learning rate: 0.013826]
	Learning Rate: 0.0138261
	LOSS [training: 0.28415730170390063 | validation: 0.2844145378585774]
	TIME [epoch: 32.7 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30757416510639685		[learning rate: 0.013811]
	Learning Rate: 0.0138108
	LOSS [training: 0.30757416510639685 | validation: 0.26960972726837434]
	TIME [epoch: 32.7 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30512070499044613		[learning rate: 0.013796]
	Learning Rate: 0.0137955
	LOSS [training: 0.30512070499044613 | validation: 0.315606589600717]
	TIME [epoch: 32.7 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2852470988883582		[learning rate: 0.01378]
	Learning Rate: 0.0137802
	LOSS [training: 0.2852470988883582 | validation: 0.2924111233304377]
	TIME [epoch: 32.7 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28022930004744634		[learning rate: 0.013765]
	Learning Rate: 0.0137649
	LOSS [training: 0.28022930004744634 | validation: 0.2899946167552737]
	TIME [epoch: 32.6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28282538251013906		[learning rate: 0.01375]
	Learning Rate: 0.0137496
	LOSS [training: 0.28282538251013906 | validation: 0.3215471934810715]
	TIME [epoch: 32.7 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3269809743673425		[learning rate: 0.013734]
	Learning Rate: 0.0137343
	LOSS [training: 0.3269809743673425 | validation: 0.2897549923288366]
	TIME [epoch: 32.8 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2721907022636453		[learning rate: 0.013719]
	Learning Rate: 0.013719
	LOSS [training: 0.2721907022636453 | validation: 0.2568619003855327]
	TIME [epoch: 32.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_770.pth
	Model improved!!!
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3280676956688422		[learning rate: 0.013704]
	Learning Rate: 0.0137036
	LOSS [training: 0.3280676956688422 | validation: 0.3236632300278592]
	TIME [epoch: 32.4 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28718502560917825		[learning rate: 0.013688]
	Learning Rate: 0.0136882
	LOSS [training: 0.28718502560917825 | validation: 0.27817594262110246]
	TIME [epoch: 32.5 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30913408591020763		[learning rate: 0.013673]
	Learning Rate: 0.0136729
	LOSS [training: 0.30913408591020763 | validation: 0.2999169418725479]
	TIME [epoch: 32.5 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28803567601301394		[learning rate: 0.013657]
	Learning Rate: 0.0136575
	LOSS [training: 0.28803567601301394 | validation: 0.2806280094326662]
	TIME [epoch: 32.4 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29986456441901815		[learning rate: 0.013642]
	Learning Rate: 0.0136421
	LOSS [training: 0.29986456441901815 | validation: 0.28068028104905735]
	TIME [epoch: 32.4 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2844301865844908		[learning rate: 0.013627]
	Learning Rate: 0.0136267
	LOSS [training: 0.2844301865844908 | validation: 0.2709910332537302]
	TIME [epoch: 32.5 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2824005640903492		[learning rate: 0.013611]
	Learning Rate: 0.0136113
	LOSS [training: 0.2824005640903492 | validation: 0.2603346092551992]
	TIME [epoch: 32.5 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2904722988076811		[learning rate: 0.013596]
	Learning Rate: 0.0135959
	LOSS [training: 0.2904722988076811 | validation: 0.2724782079220137]
	TIME [epoch: 32.4 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2707606834492118		[learning rate: 0.01358]
	Learning Rate: 0.0135805
	LOSS [training: 0.2707606834492118 | validation: 0.2673186610835382]
	TIME [epoch: 32.5 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29098702298587514		[learning rate: 0.013565]
	Learning Rate: 0.013565
	LOSS [training: 0.29098702298587514 | validation: 0.29428731484619497]
	TIME [epoch: 32.3 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3275667353709421		[learning rate: 0.01355]
	Learning Rate: 0.0135496
	LOSS [training: 0.3275667353709421 | validation: 0.289507572706002]
	TIME [epoch: 32.4 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27858138594594406		[learning rate: 0.013534]
	Learning Rate: 0.0135341
	LOSS [training: 0.27858138594594406 | validation: 0.27413983052613666]
	TIME [epoch: 32.5 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2691111244201492		[learning rate: 0.013519]
	Learning Rate: 0.0135186
	LOSS [training: 0.2691111244201492 | validation: 0.2815278218859]
	TIME [epoch: 32.4 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32023256282576507		[learning rate: 0.013503]
	Learning Rate: 0.0135032
	LOSS [training: 0.32023256282576507 | validation: 0.2915138870016612]
	TIME [epoch: 32.5 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29882101789411636		[learning rate: 0.013488]
	Learning Rate: 0.0134877
	LOSS [training: 0.29882101789411636 | validation: 0.2812712789479697]
	TIME [epoch: 32.4 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2660518285806254		[learning rate: 0.013472]
	Learning Rate: 0.0134722
	LOSS [training: 0.2660518285806254 | validation: 0.26687103467893064]
	TIME [epoch: 32.4 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2906737224097486		[learning rate: 0.013457]
	Learning Rate: 0.0134567
	LOSS [training: 0.2906737224097486 | validation: 0.295046539498702]
	TIME [epoch: 32.5 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2773877298996985		[learning rate: 0.013441]
	Learning Rate: 0.0134412
	LOSS [training: 0.2773877298996985 | validation: 0.26418060239717867]
	TIME [epoch: 32.4 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2901497152623467		[learning rate: 0.013426]
	Learning Rate: 0.0134256
	LOSS [training: 0.2901497152623467 | validation: 0.3222921142835017]
	TIME [epoch: 32.4 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2910989738634566		[learning rate: 0.01341]
	Learning Rate: 0.0134101
	LOSS [training: 0.2910989738634566 | validation: 0.2775755657919695]
	TIME [epoch: 32.5 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.293310968337846		[learning rate: 0.013395]
	Learning Rate: 0.0133946
	LOSS [training: 0.293310968337846 | validation: 0.2626714268829992]
	TIME [epoch: 32.5 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3155315187533675		[learning rate: 0.013379]
	Learning Rate: 0.013379
	LOSS [training: 0.3155315187533675 | validation: 0.2797214309665083]
	TIME [epoch: 32.5 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2801517974571842		[learning rate: 0.013363]
	Learning Rate: 0.0133635
	LOSS [training: 0.2801517974571842 | validation: 0.2583630957609193]
	TIME [epoch: 32.5 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2758252550056408		[learning rate: 0.013348]
	Learning Rate: 0.0133479
	LOSS [training: 0.2758252550056408 | validation: 0.2595312504335975]
	TIME [epoch: 32.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3091513536216463		[learning rate: 0.013332]
	Learning Rate: 0.0133323
	LOSS [training: 0.3091513536216463 | validation: 0.2708041741963017]
	TIME [epoch: 32.5 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2689127949346885		[learning rate: 0.013317]
	Learning Rate: 0.0133167
	LOSS [training: 0.2689127949346885 | validation: 0.29650342475056163]
	TIME [epoch: 32.5 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3152451402872583		[learning rate: 0.013301]
	Learning Rate: 0.0133011
	LOSS [training: 0.3152451402872583 | validation: 0.2812699271781698]
	TIME [epoch: 32.5 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28354525380439266		[learning rate: 0.013286]
	Learning Rate: 0.0132855
	LOSS [training: 0.28354525380439266 | validation: 0.25965499578306805]
	TIME [epoch: 32.4 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2776668551536926		[learning rate: 0.01327]
	Learning Rate: 0.0132699
	LOSS [training: 0.2776668551536926 | validation: 0.2688967064657929]
	TIME [epoch: 32.6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2877758270570688		[learning rate: 0.013254]
	Learning Rate: 0.0132543
	LOSS [training: 0.2877758270570688 | validation: 0.2535161024909217]
	TIME [epoch: 32.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_800.pth
	Model improved!!!
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27454040113592265		[learning rate: 0.013239]
	Learning Rate: 0.0132386
	LOSS [training: 0.27454040113592265 | validation: 0.332612745158272]
	TIME [epoch: 32.4 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30224904417261234		[learning rate: 0.013223]
	Learning Rate: 0.013223
	LOSS [training: 0.30224904417261234 | validation: 0.27604681161191696]
	TIME [epoch: 32.6 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2957129462105526		[learning rate: 0.013207]
	Learning Rate: 0.0132074
	LOSS [training: 0.2957129462105526 | validation: 0.259244280646371]
	TIME [epoch: 32.6 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26824519858238954		[learning rate: 0.013192]
	Learning Rate: 0.0131917
	LOSS [training: 0.26824519858238954 | validation: 0.24872628008898703]
	TIME [epoch: 32.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_804.pth
	Model improved!!!
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3224883745472043		[learning rate: 0.013176]
	Learning Rate: 0.013176
	LOSS [training: 0.3224883745472043 | validation: 0.30881023909477634]
	TIME [epoch: 32.5 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27637059146526904		[learning rate: 0.01316]
	Learning Rate: 0.0131603
	LOSS [training: 0.27637059146526904 | validation: 0.2605233816010788]
	TIME [epoch: 32.6 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28879185653501893		[learning rate: 0.013145]
	Learning Rate: 0.0131447
	LOSS [training: 0.28879185653501893 | validation: 0.2718277234955888]
	TIME [epoch: 32.5 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2692218142143571		[learning rate: 0.013129]
	Learning Rate: 0.013129
	LOSS [training: 0.2692218142143571 | validation: 0.26788681635165407]
	TIME [epoch: 32.7 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2856652420205023		[learning rate: 0.013113]
	Learning Rate: 0.0131133
	LOSS [training: 0.2856652420205023 | validation: 0.33247925209359414]
	TIME [epoch: 32.7 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29185027659634893		[learning rate: 0.013098]
	Learning Rate: 0.0130976
	LOSS [training: 0.29185027659634893 | validation: 0.25818418670519205]
	TIME [epoch: 32.5 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2756963414100079		[learning rate: 0.013082]
	Learning Rate: 0.0130818
	LOSS [training: 0.2756963414100079 | validation: 0.2656539605906312]
	TIME [epoch: 32.7 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3029332264365611		[learning rate: 0.013066]
	Learning Rate: 0.0130661
	LOSS [training: 0.3029332264365611 | validation: 0.26736421056659676]
	TIME [epoch: 32.5 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27974386116771166		[learning rate: 0.01305]
	Learning Rate: 0.0130504
	LOSS [training: 0.27974386116771166 | validation: 0.27614626148509136]
	TIME [epoch: 32.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.272093016878781		[learning rate: 0.013035]
	Learning Rate: 0.0130346
	LOSS [training: 0.272093016878781 | validation: 0.2701437430799989]
	TIME [epoch: 32.5 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30598710823104736		[learning rate: 0.013019]
	Learning Rate: 0.0130189
	LOSS [training: 0.30598710823104736 | validation: 0.3078067679366304]
	TIME [epoch: 32.5 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28003062004033735		[learning rate: 0.013003]
	Learning Rate: 0.0130031
	LOSS [training: 0.28003062004033735 | validation: 0.2461314634834212]
	TIME [epoch: 32.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_816.pth
	Model improved!!!
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2737229018762627		[learning rate: 0.012987]
	Learning Rate: 0.0129873
	LOSS [training: 0.2737229018762627 | validation: 0.2710668091615632]
	TIME [epoch: 32.6 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29074670067845176		[learning rate: 0.012972]
	Learning Rate: 0.0129716
	LOSS [training: 0.29074670067845176 | validation: 0.28995005980468497]
	TIME [epoch: 32.6 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2879874169636109		[learning rate: 0.012956]
	Learning Rate: 0.0129558
	LOSS [training: 0.2879874169636109 | validation: 0.2966463149422177]
	TIME [epoch: 32.5 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2900505937596304		[learning rate: 0.01294]
	Learning Rate: 0.01294
	LOSS [training: 0.2900505937596304 | validation: 0.26685682202738725]
	TIME [epoch: 32.4 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2565418350930555		[learning rate: 0.012924]
	Learning Rate: 0.0129242
	LOSS [training: 0.2565418350930555 | validation: 0.25178817352961086]
	TIME [epoch: 32.6 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30446093289175		[learning rate: 0.012908]
	Learning Rate: 0.0129084
	LOSS [training: 0.30446093289175 | validation: 0.29146993164026]
	TIME [epoch: 32.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3032723126963181		[learning rate: 0.012893]
	Learning Rate: 0.0128926
	LOSS [training: 0.3032723126963181 | validation: 0.2965530278729955]
	TIME [epoch: 32.4 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30263489446819447		[learning rate: 0.012877]
	Learning Rate: 0.0128767
	LOSS [training: 0.30263489446819447 | validation: 0.31997168346163696]
	TIME [epoch: 32.6 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2911724363714649		[learning rate: 0.012861]
	Learning Rate: 0.0128609
	LOSS [training: 0.2911724363714649 | validation: 0.25091974895008884]
	TIME [epoch: 32.6 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2914225938833166		[learning rate: 0.012845]
	Learning Rate: 0.0128451
	LOSS [training: 0.2914225938833166 | validation: 0.2965733533956645]
	TIME [epoch: 32.5 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3096421885904018		[learning rate: 0.012829]
	Learning Rate: 0.0128292
	LOSS [training: 0.3096421885904018 | validation: 0.29243588816000254]
	TIME [epoch: 32.6 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2738986179003503		[learning rate: 0.012813]
	Learning Rate: 0.0128133
	LOSS [training: 0.2738986179003503 | validation: 0.30107035197216836]
	TIME [epoch: 32.6 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28909864795184126		[learning rate: 0.012797]
	Learning Rate: 0.0127975
	LOSS [training: 0.28909864795184126 | validation: 0.2586691060412756]
	TIME [epoch: 32.6 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31674601133776287		[learning rate: 0.012782]
	Learning Rate: 0.0127816
	LOSS [training: 0.31674601133776287 | validation: 0.29475090751362854]
	TIME [epoch: 32.5 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29483067022084225		[learning rate: 0.012766]
	Learning Rate: 0.0127657
	LOSS [training: 0.29483067022084225 | validation: 0.26702127296930517]
	TIME [epoch: 32.6 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27308256832706657		[learning rate: 0.01275]
	Learning Rate: 0.0127498
	LOSS [training: 0.27308256832706657 | validation: 0.29699990189512393]
	TIME [epoch: 32.6 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2899256348508522		[learning rate: 0.012734]
	Learning Rate: 0.0127339
	LOSS [training: 0.2899256348508522 | validation: 0.2742262315933475]
	TIME [epoch: 32.6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2944377602787266		[learning rate: 0.012718]
	Learning Rate: 0.012718
	LOSS [training: 0.2944377602787266 | validation: 0.29000804167415206]
	TIME [epoch: 32.5 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26512025489285124		[learning rate: 0.012702]
	Learning Rate: 0.0127021
	LOSS [training: 0.26512025489285124 | validation: 0.25762961142155294]
	TIME [epoch: 32.5 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.292703094188199		[learning rate: 0.012686]
	Learning Rate: 0.0126862
	LOSS [training: 0.292703094188199 | validation: 0.29519558114412026]
	TIME [epoch: 32.6 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28335651477671797		[learning rate: 0.01267]
	Learning Rate: 0.0126703
	LOSS [training: 0.28335651477671797 | validation: 0.25756416762260576]
	TIME [epoch: 32.6 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26400991192266143		[learning rate: 0.012654]
	Learning Rate: 0.0126544
	LOSS [training: 0.26400991192266143 | validation: 0.2762731004639236]
	TIME [epoch: 32.4 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2943202642940161		[learning rate: 0.012638]
	Learning Rate: 0.0126384
	LOSS [training: 0.2943202642940161 | validation: 0.2684443277404751]
	TIME [epoch: 32.5 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2831343383601432		[learning rate: 0.012622]
	Learning Rate: 0.0126225
	LOSS [training: 0.2831343383601432 | validation: 0.2820705950842463]
	TIME [epoch: 32.6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2754363394470966		[learning rate: 0.012607]
	Learning Rate: 0.0126065
	LOSS [training: 0.2754363394470966 | validation: 0.2914428665318519]
	TIME [epoch: 32.5 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.281463860395732		[learning rate: 0.012591]
	Learning Rate: 0.0125906
	LOSS [training: 0.281463860395732 | validation: 0.2514536986085296]
	TIME [epoch: 32.6 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2820545667290979		[learning rate: 0.012575]
	Learning Rate: 0.0125746
	LOSS [training: 0.2820545667290979 | validation: 0.2672226110631426]
	TIME [epoch: 32.4 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2822717950730985		[learning rate: 0.012559]
	Learning Rate: 0.0125586
	LOSS [training: 0.2822717950730985 | validation: 0.29020514021012844]
	TIME [epoch: 32.5 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29266732332350176		[learning rate: 0.012543]
	Learning Rate: 0.0125426
	LOSS [training: 0.29266732332350176 | validation: 0.2947595104547189]
	TIME [epoch: 32.6 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2802485928834016		[learning rate: 0.012527]
	Learning Rate: 0.0125267
	LOSS [training: 0.2802485928834016 | validation: 0.2557611493747496]
	TIME [epoch: 32.5 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2731947433313372		[learning rate: 0.012511]
	Learning Rate: 0.0125107
	LOSS [training: 0.2731947433313372 | validation: 0.30911512351149706]
	TIME [epoch: 32.4 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2855001381401547		[learning rate: 0.012495]
	Learning Rate: 0.0124947
	LOSS [training: 0.2855001381401547 | validation: 0.2581486130664622]
	TIME [epoch: 32.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26832500151330574		[learning rate: 0.012479]
	Learning Rate: 0.0124786
	LOSS [training: 0.26832500151330574 | validation: 0.3131651592077358]
	TIME [epoch: 32.5 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28831155164294375		[learning rate: 0.012463]
	Learning Rate: 0.0124626
	LOSS [training: 0.28831155164294375 | validation: 0.28083131395647]
	TIME [epoch: 32.5 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32476442577744746		[learning rate: 0.012447]
	Learning Rate: 0.0124466
	LOSS [training: 0.32476442577744746 | validation: 0.2824285885808774]
	TIME [epoch: 32.5 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28369638551120224		[learning rate: 0.012431]
	Learning Rate: 0.0124306
	LOSS [training: 0.28369638551120224 | validation: 0.26899280498558553]
	TIME [epoch: 32.6 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27107605454232775		[learning rate: 0.012415]
	Learning Rate: 0.0124145
	LOSS [training: 0.27107605454232775 | validation: 0.2645168294971373]
	TIME [epoch: 32.5 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2947674234997115		[learning rate: 0.012399]
	Learning Rate: 0.0123985
	LOSS [training: 0.2947674234997115 | validation: 0.2782599539622886]
	TIME [epoch: 32.4 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2885709365336956		[learning rate: 0.012382]
	Learning Rate: 0.0123825
	LOSS [training: 0.2885709365336956 | validation: 0.2881701278186035]
	TIME [epoch: 32.6 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27854174632435186		[learning rate: 0.012366]
	Learning Rate: 0.0123664
	LOSS [training: 0.27854174632435186 | validation: 0.2742568282035477]
	TIME [epoch: 32.6 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2793693453997968		[learning rate: 0.01235]
	Learning Rate: 0.0123503
	LOSS [training: 0.2793693453997968 | validation: 0.280101745922881]
	TIME [epoch: 32.6 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3001259759689695		[learning rate: 0.012334]
	Learning Rate: 0.0123343
	LOSS [training: 0.3001259759689695 | validation: 0.26188534908718264]
	TIME [epoch: 32.4 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2735010911437792		[learning rate: 0.012318]
	Learning Rate: 0.0123182
	LOSS [training: 0.2735010911437792 | validation: 0.2939031146487405]
	TIME [epoch: 32.5 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3122851422622275		[learning rate: 0.012302]
	Learning Rate: 0.0123021
	LOSS [training: 0.3122851422622275 | validation: 0.25108663419977184]
	TIME [epoch: 32.5 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2627433754048501		[learning rate: 0.012286]
	Learning Rate: 0.012286
	LOSS [training: 0.2627433754048501 | validation: 0.2508788772496787]
	TIME [epoch: 32.5 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30315945516266474		[learning rate: 0.01227]
	Learning Rate: 0.0122699
	LOSS [training: 0.30315945516266474 | validation: 0.29899696001367393]
	TIME [epoch: 32.5 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.275136943038812		[learning rate: 0.012254]
	Learning Rate: 0.0122538
	LOSS [training: 0.275136943038812 | validation: 0.28081827265136217]
	TIME [epoch: 32.5 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29734048653101774		[learning rate: 0.012238]
	Learning Rate: 0.0122377
	LOSS [training: 0.29734048653101774 | validation: 0.29948165031816343]
	TIME [epoch: 32.6 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3100441005813919		[learning rate: 0.012222]
	Learning Rate: 0.0122216
	LOSS [training: 0.3100441005813919 | validation: 0.27371334479213866]
	TIME [epoch: 32.6 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27616737493871113		[learning rate: 0.012205]
	Learning Rate: 0.0122055
	LOSS [training: 0.27616737493871113 | validation: 0.2688836977422415]
	TIME [epoch: 32.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26958518866556447		[learning rate: 0.012189]
	Learning Rate: 0.0121894
	LOSS [training: 0.26958518866556447 | validation: 0.300607030164579]
	TIME [epoch: 32.5 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3281389259509735		[learning rate: 0.012173]
	Learning Rate: 0.0121732
	LOSS [training: 0.3281389259509735 | validation: 0.27473626713170013]
	TIME [epoch: 32.5 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2694778611498257		[learning rate: 0.012157]
	Learning Rate: 0.0121571
	LOSS [training: 0.2694778611498257 | validation: 0.25754952670130443]
	TIME [epoch: 32.6 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27691059977189214		[learning rate: 0.012141]
	Learning Rate: 0.012141
	LOSS [training: 0.27691059977189214 | validation: 0.2756500349833131]
	TIME [epoch: 32.6 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2876275407499348		[learning rate: 0.012125]
	Learning Rate: 0.0121248
	LOSS [training: 0.2876275407499348 | validation: 0.2693762072227508]
	TIME [epoch: 32.4 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26422137663164147		[learning rate: 0.012109]
	Learning Rate: 0.0121087
	LOSS [training: 0.26422137663164147 | validation: 0.31515592982180396]
	TIME [epoch: 32.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29594483767877433		[learning rate: 0.012093]
	Learning Rate: 0.0120925
	LOSS [training: 0.29594483767877433 | validation: 0.26082659303954514]
	TIME [epoch: 32.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26517245528113953		[learning rate: 0.012076]
	Learning Rate: 0.0120763
	LOSS [training: 0.26517245528113953 | validation: 0.2586954922191974]
	TIME [epoch: 32.5 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2631946165789728		[learning rate: 0.01206]
	Learning Rate: 0.0120602
	LOSS [training: 0.2631946165789728 | validation: 0.3084042146844994]
	TIME [epoch: 32.5 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3099719103983365		[learning rate: 0.012044]
	Learning Rate: 0.012044
	LOSS [training: 0.3099719103983365 | validation: 0.2601629613258318]
	TIME [epoch: 32.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.274057898247599		[learning rate: 0.012028]
	Learning Rate: 0.0120278
	LOSS [training: 0.274057898247599 | validation: 0.29356401491399103]
	TIME [epoch: 32.5 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27749301652393915		[learning rate: 0.012012]
	Learning Rate: 0.0120116
	LOSS [training: 0.27749301652393915 | validation: 0.2661262099768938]
	TIME [epoch: 32.5 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25908134294385005		[learning rate: 0.011995]
	Learning Rate: 0.0119954
	LOSS [training: 0.25908134294385005 | validation: 0.25843300757969545]
	TIME [epoch: 32.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3152441464574659		[learning rate: 0.011979]
	Learning Rate: 0.0119792
	LOSS [training: 0.3152441464574659 | validation: 0.2991461424989335]
	TIME [epoch: 32.5 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2760152869961847		[learning rate: 0.011963]
	Learning Rate: 0.011963
	LOSS [training: 0.2760152869961847 | validation: 0.2764851449287425]
	TIME [epoch: 32.5 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27554673671199104		[learning rate: 0.011947]
	Learning Rate: 0.0119468
	LOSS [training: 0.27554673671199104 | validation: 0.27001516166225714]
	TIME [epoch: 32.5 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27534284996674174		[learning rate: 0.011931]
	Learning Rate: 0.0119306
	LOSS [training: 0.27534284996674174 | validation: 0.2661085975808132]
	TIME [epoch: 32.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28021947280759796		[learning rate: 0.011914]
	Learning Rate: 0.0119144
	LOSS [training: 0.28021947280759796 | validation: 0.26921912344359167]
	TIME [epoch: 32.5 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2790374939782473		[learning rate: 0.011898]
	Learning Rate: 0.0118982
	LOSS [training: 0.2790374939782473 | validation: 0.2789430702392556]
	TIME [epoch: 32.5 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26535309452606787		[learning rate: 0.011882]
	Learning Rate: 0.0118819
	LOSS [training: 0.26535309452606787 | validation: 0.2373158176379876]
	TIME [epoch: 32.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_886.pth
	Model improved!!!
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.301404690468126		[learning rate: 0.011866]
	Learning Rate: 0.0118657
	LOSS [training: 0.301404690468126 | validation: 0.2926381229540422]
	TIME [epoch: 32.6 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2778256087508476		[learning rate: 0.011849]
	Learning Rate: 0.0118495
	LOSS [training: 0.2778256087508476 | validation: 0.26539477376488424]
	TIME [epoch: 32.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2594752686785312		[learning rate: 0.011833]
	Learning Rate: 0.0118332
	LOSS [training: 0.2594752686785312 | validation: 0.24657900892346676]
	TIME [epoch: 32.6 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30510860006493457		[learning rate: 0.011817]
	Learning Rate: 0.011817
	LOSS [training: 0.30510860006493457 | validation: 0.27139428413768946]
	TIME [epoch: 32.6 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26640240618598776		[learning rate: 0.011801]
	Learning Rate: 0.0118007
	LOSS [training: 0.26640240618598776 | validation: 0.24967432228414627]
	TIME [epoch: 32.5 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2754575703025669		[learning rate: 0.011784]
	Learning Rate: 0.0117844
	LOSS [training: 0.2754575703025669 | validation: 0.35727194945466795]
	TIME [epoch: 32.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31215475444001134		[learning rate: 0.011768]
	Learning Rate: 0.0117682
	LOSS [training: 0.31215475444001134 | validation: 0.2756350069610114]
	TIME [epoch: 32.6 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2752013219956376		[learning rate: 0.011752]
	Learning Rate: 0.0117519
	LOSS [training: 0.2752013219956376 | validation: 0.26587264862179283]
	TIME [epoch: 32.5 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2616330121251107		[learning rate: 0.011736]
	Learning Rate: 0.0117356
	LOSS [training: 0.2616330121251107 | validation: 0.2584339157119101]
	TIME [epoch: 32.6 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29111668395052953		[learning rate: 0.011719]
	Learning Rate: 0.0117194
	LOSS [training: 0.29111668395052953 | validation: 0.3122084061272379]
	TIME [epoch: 32.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2973361432039448		[learning rate: 0.011703]
	Learning Rate: 0.0117031
	LOSS [training: 0.2973361432039448 | validation: 0.2816292515613734]
	TIME [epoch: 32.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2829406680971006		[learning rate: 0.011687]
	Learning Rate: 0.0116868
	LOSS [training: 0.2829406680971006 | validation: 0.2634532463608357]
	TIME [epoch: 32.5 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25918292887757327		[learning rate: 0.01167]
	Learning Rate: 0.0116705
	LOSS [training: 0.25918292887757327 | validation: 0.24265978259111365]
	TIME [epoch: 32.6 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28836888517558235		[learning rate: 0.011654]
	Learning Rate: 0.0116542
	LOSS [training: 0.28836888517558235 | validation: 0.276789877212566]
	TIME [epoch: 32.6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2759212420892714		[learning rate: 0.011638]
	Learning Rate: 0.0116379
	LOSS [training: 0.2759212420892714 | validation: 0.2622925847916837]
	TIME [epoch: 32.5 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2557429793859847		[learning rate: 0.011622]
	Learning Rate: 0.0116216
	LOSS [training: 0.2557429793859847 | validation: 0.25031824483860027]
	TIME [epoch: 32.7 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3073789763153371		[learning rate: 0.011605]
	Learning Rate: 0.0116053
	LOSS [training: 0.3073789763153371 | validation: 0.2658667504740837]
	TIME [epoch: 32.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27821320999627214		[learning rate: 0.011589]
	Learning Rate: 0.011589
	LOSS [training: 0.27821320999627214 | validation: 0.2621421015019683]
	TIME [epoch: 32.7 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27056143945516836		[learning rate: 0.011573]
	Learning Rate: 0.0115726
	LOSS [training: 0.27056143945516836 | validation: 0.25398484119364917]
	TIME [epoch: 32.7 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28253761326307436		[learning rate: 0.011556]
	Learning Rate: 0.0115563
	LOSS [training: 0.28253761326307436 | validation: 0.2761890800239364]
	TIME [epoch: 32.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27732139587595345		[learning rate: 0.01154]
	Learning Rate: 0.01154
	LOSS [training: 0.27732139587595345 | validation: 0.2574906047814004]
	TIME [epoch: 32.5 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27014286070441745		[learning rate: 0.011524]
	Learning Rate: 0.0115237
	LOSS [training: 0.27014286070441745 | validation: 0.25425774292659287]
	TIME [epoch: 32.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27406481311921177		[learning rate: 0.011507]
	Learning Rate: 0.0115073
	LOSS [training: 0.27406481311921177 | validation: 0.24929938497377147]
	TIME [epoch: 32.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27177436696099455		[learning rate: 0.011491]
	Learning Rate: 0.011491
	LOSS [training: 0.27177436696099455 | validation: 0.28231899092526136]
	TIME [epoch: 32.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2812630745779463		[learning rate: 0.011475]
	Learning Rate: 0.0114746
	LOSS [training: 0.2812630745779463 | validation: 0.25800374905641105]
	TIME [epoch: 32.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26026698693081773		[learning rate: 0.011458]
	Learning Rate: 0.0114583
	LOSS [training: 0.26026698693081773 | validation: 0.2635257354029315]
	TIME [epoch: 32.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2724191808995854		[learning rate: 0.011442]
	Learning Rate: 0.0114419
	LOSS [training: 0.2724191808995854 | validation: 0.26940374089537833]
	TIME [epoch: 32.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.265855957592467		[learning rate: 0.011426]
	Learning Rate: 0.0114256
	LOSS [training: 0.265855957592467 | validation: 0.2521227955958009]
	TIME [epoch: 32.7 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29405737239828716		[learning rate: 0.011409]
	Learning Rate: 0.0114092
	LOSS [training: 0.29405737239828716 | validation: 0.26132149577047736]
	TIME [epoch: 32.7 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2658553130019361		[learning rate: 0.011393]
	Learning Rate: 0.0113929
	LOSS [training: 0.2658553130019361 | validation: 0.2520166116124887]
	TIME [epoch: 32.7 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2566121445187518		[learning rate: 0.011376]
	Learning Rate: 0.0113765
	LOSS [training: 0.2566121445187518 | validation: 0.25585644194976265]
	TIME [epoch: 32.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2926154416550264		[learning rate: 0.01136]
	Learning Rate: 0.0113601
	LOSS [training: 0.2926154416550264 | validation: 0.27268436542575286]
	TIME [epoch: 32.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27633470121252235		[learning rate: 0.011344]
	Learning Rate: 0.0113437
	LOSS [training: 0.27633470121252235 | validation: 0.248656282911983]
	TIME [epoch: 32.7 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26149471386662804		[learning rate: 0.011327]
	Learning Rate: 0.0113274
	LOSS [training: 0.26149471386662804 | validation: 0.31581986000631834]
	TIME [epoch: 32.7 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3117516128151231		[learning rate: 0.011311]
	Learning Rate: 0.011311
	LOSS [training: 0.3117516128151231 | validation: 0.2539765589185701]
	TIME [epoch: 32.7 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25968722226896057		[learning rate: 0.011295]
	Learning Rate: 0.0112946
	LOSS [training: 0.25968722226896057 | validation: 0.2497257594179712]
	TIME [epoch: 32.8 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26757010530794556		[learning rate: 0.011278]
	Learning Rate: 0.0112782
	LOSS [training: 0.26757010530794556 | validation: 0.26387789848531196]
	TIME [epoch: 32.8 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2900568759448011		[learning rate: 0.011262]
	Learning Rate: 0.0112618
	LOSS [training: 0.2900568759448011 | validation: 0.2544283659404839]
	TIME [epoch: 32.8 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3059896820896799		[learning rate: 0.011245]
	Learning Rate: 0.0112454
	LOSS [training: 0.3059896820896799 | validation: 0.2743523792856706]
	TIME [epoch: 32.7 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.272468439414812		[learning rate: 0.011229]
	Learning Rate: 0.011229
	LOSS [training: 0.272468439414812 | validation: 0.29616802086913396]
	TIME [epoch: 32.8 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2759524169844352		[learning rate: 0.011213]
	Learning Rate: 0.0112126
	LOSS [training: 0.2759524169844352 | validation: 0.2843772819087149]
	TIME [epoch: 32.7 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.291326737365373		[learning rate: 0.011196]
	Learning Rate: 0.0111962
	LOSS [training: 0.291326737365373 | validation: 0.2639073803339335]
	TIME [epoch: 32.7 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2721763532008237		[learning rate: 0.01118]
	Learning Rate: 0.0111798
	LOSS [training: 0.2721763532008237 | validation: 0.24266826594119467]
	TIME [epoch: 32.8 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2607726811496035		[learning rate: 0.011163]
	Learning Rate: 0.0111634
	LOSS [training: 0.2607726811496035 | validation: 0.24365455743926775]
	TIME [epoch: 32.7 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2688256071259979		[learning rate: 0.011147]
	Learning Rate: 0.011147
	LOSS [training: 0.2688256071259979 | validation: 0.25786982423128507]
	TIME [epoch: 32.8 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2737732220153798		[learning rate: 0.011131]
	Learning Rate: 0.0111305
	LOSS [training: 0.2737732220153798 | validation: 0.2546390622286019]
	TIME [epoch: 32.7 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2645917467977316		[learning rate: 0.011114]
	Learning Rate: 0.0111141
	LOSS [training: 0.2645917467977316 | validation: 0.24630465734888768]
	TIME [epoch: 32.8 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2840852220633299		[learning rate: 0.011098]
	Learning Rate: 0.0110977
	LOSS [training: 0.2840852220633299 | validation: 0.297067365975195]
	TIME [epoch: 32.8 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2785098147779258		[learning rate: 0.011081]
	Learning Rate: 0.0110813
	LOSS [training: 0.2785098147779258 | validation: 0.267896215480672]
	TIME [epoch: 32.7 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28197123649941336		[learning rate: 0.011065]
	Learning Rate: 0.0110648
	LOSS [training: 0.28197123649941336 | validation: 0.2729629633830632]
	TIME [epoch: 32.7 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26762862194948417		[learning rate: 0.011048]
	Learning Rate: 0.0110484
	LOSS [training: 0.26762862194948417 | validation: 0.26831384109900874]
	TIME [epoch: 32.7 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26366823936465117		[learning rate: 0.011032]
	Learning Rate: 0.011032
	LOSS [training: 0.26366823936465117 | validation: 0.25377693533105117]
	TIME [epoch: 32.7 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2960254993113397		[learning rate: 0.011016]
	Learning Rate: 0.0110155
	LOSS [training: 0.2960254993113397 | validation: 0.2594544917100475]
	TIME [epoch: 32.8 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2821249404078343		[learning rate: 0.010999]
	Learning Rate: 0.0109991
	LOSS [training: 0.2821249404078343 | validation: 0.27074046193974516]
	TIME [epoch: 32.7 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25771884251577476		[learning rate: 0.010983]
	Learning Rate: 0.0109826
	LOSS [training: 0.25771884251577476 | validation: 0.28409221500719883]
	TIME [epoch: 32.8 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30392512269649463		[learning rate: 0.010966]
	Learning Rate: 0.0109662
	LOSS [training: 0.30392512269649463 | validation: 0.25116185974532207]
	TIME [epoch: 32.8 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2605268546664534		[learning rate: 0.01095]
	Learning Rate: 0.0109497
	LOSS [training: 0.2605268546664534 | validation: 0.26414548223950224]
	TIME [epoch: 32.7 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2747339412527059		[learning rate: 0.010933]
	Learning Rate: 0.0109333
	LOSS [training: 0.2747339412527059 | validation: 0.2581476901959571]
	TIME [epoch: 32.7 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26764145480604445		[learning rate: 0.010917]
	Learning Rate: 0.0109168
	LOSS [training: 0.26764145480604445 | validation: 0.25210544261455786]
	TIME [epoch: 32.8 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2970606734797451		[learning rate: 0.0109]
	Learning Rate: 0.0109004
	LOSS [training: 0.2970606734797451 | validation: 0.2694862716366132]
	TIME [epoch: 32.7 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2673564877656095		[learning rate: 0.010884]
	Learning Rate: 0.0108839
	LOSS [training: 0.2673564877656095 | validation: 0.26882315782864274]
	TIME [epoch: 32.7 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2742316459844973		[learning rate: 0.010867]
	Learning Rate: 0.0108674
	LOSS [training: 0.2742316459844973 | validation: 0.27745690157041214]
	TIME [epoch: 32.8 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2841037504611173		[learning rate: 0.010851]
	Learning Rate: 0.010851
	LOSS [training: 0.2841037504611173 | validation: 0.3031958190181407]
	TIME [epoch: 32.8 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2873084331615624		[learning rate: 0.010834]
	Learning Rate: 0.0108345
	LOSS [training: 0.2873084331615624 | validation: 0.2532053489451761]
	TIME [epoch: 32.7 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27304259744934106		[learning rate: 0.010818]
	Learning Rate: 0.010818
	LOSS [training: 0.27304259744934106 | validation: 0.26454570197781907]
	TIME [epoch: 32.7 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27048141586738494		[learning rate: 0.010802]
	Learning Rate: 0.0108016
	LOSS [training: 0.27048141586738494 | validation: 0.25469349121814683]
	TIME [epoch: 32.8 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.268044308434418		[learning rate: 0.010785]
	Learning Rate: 0.0107851
	LOSS [training: 0.268044308434418 | validation: 0.26177530921215636]
	TIME [epoch: 32.7 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28180933442507766		[learning rate: 0.010769]
	Learning Rate: 0.0107686
	LOSS [training: 0.28180933442507766 | validation: 0.25354581801418735]
	TIME [epoch: 32.7 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2604051570753557		[learning rate: 0.010752]
	Learning Rate: 0.0107521
	LOSS [training: 0.2604051570753557 | validation: 0.28199572548920193]
	TIME [epoch: 32.7 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28302471916678645		[learning rate: 0.010736]
	Learning Rate: 0.0107356
	LOSS [training: 0.28302471916678645 | validation: 0.24867372230564067]
	TIME [epoch: 32.7 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2577193745244701		[learning rate: 0.010719]
	Learning Rate: 0.0107192
	LOSS [training: 0.2577193745244701 | validation: 0.2622447958770604]
	TIME [epoch: 32.8 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29551746241176435		[learning rate: 0.010703]
	Learning Rate: 0.0107027
	LOSS [training: 0.29551746241176435 | validation: 0.26924500825841036]
	TIME [epoch: 32.8 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2669897216426201		[learning rate: 0.010686]
	Learning Rate: 0.0106862
	LOSS [training: 0.2669897216426201 | validation: 0.2430499614786047]
	TIME [epoch: 32.7 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2578034471414		[learning rate: 0.01067]
	Learning Rate: 0.0106697
	LOSS [training: 0.2578034471414 | validation: 0.2470375193353867]
	TIME [epoch: 32.7 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29450466834008665		[learning rate: 0.010653]
	Learning Rate: 0.0106532
	LOSS [training: 0.29450466834008665 | validation: 0.27932553650272807]
	TIME [epoch: 32.7 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26796091702294217		[learning rate: 0.010637]
	Learning Rate: 0.0106367
	LOSS [training: 0.26796091702294217 | validation: 0.26844188906976374]
	TIME [epoch: 32.7 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2779691783538446		[learning rate: 0.01062]
	Learning Rate: 0.0106202
	LOSS [training: 0.2779691783538446 | validation: 0.2539480098196957]
	TIME [epoch: 32.7 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2592522303158433		[learning rate: 0.010604]
	Learning Rate: 0.0106037
	LOSS [training: 0.2592522303158433 | validation: 0.25648138729474085]
	TIME [epoch: 32.7 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2845535325927155		[learning rate: 0.010587]
	Learning Rate: 0.0105872
	LOSS [training: 0.2845535325927155 | validation: 0.26720726280872215]
	TIME [epoch: 32.8 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.263269683245673		[learning rate: 0.010571]
	Learning Rate: 0.0105707
	LOSS [training: 0.263269683245673 | validation: 0.2458291063435939]
	TIME [epoch: 32.7 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2809323511904739		[learning rate: 0.010554]
	Learning Rate: 0.0105542
	LOSS [training: 0.2809323511904739 | validation: 0.2809825805453646]
	TIME [epoch: 32.8 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2960948457681667		[learning rate: 0.010538]
	Learning Rate: 0.0105377
	LOSS [training: 0.2960948457681667 | validation: 0.2526554443577873]
	TIME [epoch: 32.7 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2584327940260788		[learning rate: 0.010521]
	Learning Rate: 0.0105212
	LOSS [training: 0.2584327940260788 | validation: 0.24845309121093234]
	TIME [epoch: 32.7 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2767268097322554		[learning rate: 0.010505]
	Learning Rate: 0.0105047
	LOSS [training: 0.2767268097322554 | validation: 0.25526174598461204]
	TIME [epoch: 32.7 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25434487677705686		[learning rate: 0.010488]
	Learning Rate: 0.0104882
	LOSS [training: 0.25434487677705686 | validation: 0.23949929357964403]
	TIME [epoch: 32.7 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27199199709356087		[learning rate: 0.010472]
	Learning Rate: 0.0104717
	LOSS [training: 0.27199199709356087 | validation: 0.2695857909905467]
	TIME [epoch: 32.7 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25976088202834335		[learning rate: 0.010455]
	Learning Rate: 0.0104552
	LOSS [training: 0.25976088202834335 | validation: 0.246624029217925]
	TIME [epoch: 32.7 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26409165113999283		[learning rate: 0.010439]
	Learning Rate: 0.0104387
	LOSS [training: 0.26409165113999283 | validation: 0.25959460787523037]
	TIME [epoch: 32.7 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2630124860366523		[learning rate: 0.010422]
	Learning Rate: 0.0104222
	LOSS [training: 0.2630124860366523 | validation: 0.23918399688041236]
	TIME [epoch: 32.6 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2898976175600796		[learning rate: 0.010406]
	Learning Rate: 0.0104057
	LOSS [training: 0.2898976175600796 | validation: 0.25933767662083895]
	TIME [epoch: 32.8 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.264388004570081		[learning rate: 0.010389]
	Learning Rate: 0.0103891
	LOSS [training: 0.264388004570081 | validation: 0.25511600996663913]
	TIME [epoch: 32.8 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2637231437487485		[learning rate: 0.010373]
	Learning Rate: 0.0103726
	LOSS [training: 0.2637231437487485 | validation: 0.2793701889461588]
	TIME [epoch: 32.7 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2945470573997327		[learning rate: 0.010356]
	Learning Rate: 0.0103561
	LOSS [training: 0.2945470573997327 | validation: 0.2482999512459756]
	TIME [epoch: 32.7 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25672546783357475		[learning rate: 0.01034]
	Learning Rate: 0.0103396
	LOSS [training: 0.25672546783357475 | validation: 0.2557035494401556]
	TIME [epoch: 32.7 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2623797225826949		[learning rate: 0.010323]
	Learning Rate: 0.0103231
	LOSS [training: 0.2623797225826949 | validation: 0.2681073149195402]
	TIME [epoch: 32.7 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27592113227829224		[learning rate: 0.010307]
	Learning Rate: 0.0103066
	LOSS [training: 0.27592113227829224 | validation: 0.26637887291685225]
	TIME [epoch: 32.8 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26773379040319295		[learning rate: 0.01029]
	Learning Rate: 0.01029
	LOSS [training: 0.26773379040319295 | validation: 0.26527747519211653]
	TIME [epoch: 32.7 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27421027336003867		[learning rate: 0.010274]
	Learning Rate: 0.0102735
	LOSS [training: 0.27421027336003867 | validation: 0.2630425122493446]
	TIME [epoch: 32.7 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2660252357948394		[learning rate: 0.010257]
	Learning Rate: 0.010257
	LOSS [training: 0.2660252357948394 | validation: 0.24043093276124444]
	TIME [epoch: 32.7 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27408784009719933		[learning rate: 0.01024]
	Learning Rate: 0.0102405
	LOSS [training: 0.27408784009719933 | validation: 0.28159229966179655]
	TIME [epoch: 32.6 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2820569542067597		[learning rate: 0.010224]
	Learning Rate: 0.010224
	LOSS [training: 0.2820569542067597 | validation: 0.24577646189744823]
	TIME [epoch: 32.8 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2549997515351061		[learning rate: 0.010207]
	Learning Rate: 0.0102074
	LOSS [training: 0.2549997515351061 | validation: 0.24763832282728754]
	TIME [epoch: 32.7 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26556504804232883		[learning rate: 0.010191]
	Learning Rate: 0.0101909
	LOSS [training: 0.26556504804232883 | validation: 0.24799928338289723]
	TIME [epoch: 32.7 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29441184849228874		[learning rate: 0.010174]
	Learning Rate: 0.0101744
	LOSS [training: 0.29441184849228874 | validation: 0.26018894973241546]
	TIME [epoch: 32.7 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26212016885964		[learning rate: 0.010158]
	Learning Rate: 0.0101579
	LOSS [training: 0.26212016885964 | validation: 0.24280261318727137]
	TIME [epoch: 32.7 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.259250226465231		[learning rate: 0.010141]
	Learning Rate: 0.0101413
	LOSS [training: 0.259250226465231 | validation: 0.3204416742029343]
	TIME [epoch: 32.8 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28491102633257104		[learning rate: 0.010125]
	Learning Rate: 0.0101248
	LOSS [training: 0.28491102633257104 | validation: 0.25587265036412643]
	TIME [epoch: 32.8 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25871636146172905		[learning rate: 0.010108]
	Learning Rate: 0.0101083
	LOSS [training: 0.25871636146172905 | validation: 0.23828080685586178]
	TIME [epoch: 32.7 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2610661405901394		[learning rate: 0.010092]
	Learning Rate: 0.0100918
	LOSS [training: 0.2610661405901394 | validation: 0.2733697948835469]
	TIME [epoch: 32.7 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2955504246927368		[learning rate: 0.010075]
	Learning Rate: 0.0100752
	LOSS [training: 0.2955504246927368 | validation: 0.24859382056238596]
	TIME [epoch: 32.8 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2580675142578804		[learning rate: 0.010059]
	Learning Rate: 0.0100587
	LOSS [training: 0.2580675142578804 | validation: 0.25211741448717323]
	TIME [epoch: 32.8 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26147027190484445		[learning rate: 0.010042]
	Learning Rate: 0.0100422
	LOSS [training: 0.26147027190484445 | validation: 0.2585141902178839]
	TIME [epoch: 32.6 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27623792162232497		[learning rate: 0.010026]
	Learning Rate: 0.0100257
	LOSS [training: 0.27623792162232497 | validation: 0.26260051720264854]
	TIME [epoch: 32.7 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28250769959226624		[learning rate: 0.010009]
	Learning Rate: 0.0100091
	LOSS [training: 0.28250769959226624 | validation: 0.24788033556435074]
	TIME [epoch: 32.8 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25548900998451196		[learning rate: 0.0099926]
	Learning Rate: 0.00999261
	LOSS [training: 0.25548900998451196 | validation: 0.2658782591043142]
	TIME [epoch: 176 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26301345460107495		[learning rate: 0.0099761]
	Learning Rate: 0.00997608
	LOSS [training: 0.26301345460107495 | validation: 0.24526270328108368]
	TIME [epoch: 69.3 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.259098032210031		[learning rate: 0.0099596]
	Learning Rate: 0.00995955
	LOSS [training: 0.259098032210031 | validation: 0.24995439535319258]
	TIME [epoch: 69.2 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.260458349857048		[learning rate: 0.009943]
	Learning Rate: 0.00994303
	LOSS [training: 0.260458349857048 | validation: 0.2408019170533724]
	TIME [epoch: 69.6 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.276569348094188		[learning rate: 0.0099265]
	Learning Rate: 0.0099265
	LOSS [training: 0.276569348094188 | validation: 0.2892673535795828]
	TIME [epoch: 69.5 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2969825652644479		[learning rate: 0.00991]
	Learning Rate: 0.00990997
	LOSS [training: 0.2969825652644479 | validation: 0.2668175381937612]
	TIME [epoch: 69.6 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27339431144463455		[learning rate: 0.0098934]
	Learning Rate: 0.00989345
	LOSS [training: 0.27339431144463455 | validation: 0.26846051171729135]
	TIME [epoch: 69.9 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2798012402080294		[learning rate: 0.0098769]
	Learning Rate: 0.00987692
	LOSS [training: 0.2798012402080294 | validation: 0.2578899749056516]
	TIME [epoch: 69.8 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25928887899354147		[learning rate: 0.0098604]
	Learning Rate: 0.0098604
	LOSS [training: 0.25928887899354147 | validation: 0.2381978618569074]
	TIME [epoch: 69.6 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2860823984375712		[learning rate: 0.0098439]
	Learning Rate: 0.00984387
	LOSS [training: 0.2860823984375712 | validation: 0.28330460318355183]
	TIME [epoch: 69.9 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2728951980778482		[learning rate: 0.0098274]
	Learning Rate: 0.00982735
	LOSS [training: 0.2728951980778482 | validation: 0.2537031008534969]
	TIME [epoch: 69.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.258919831321927		[learning rate: 0.0098108]
	Learning Rate: 0.00981083
	LOSS [training: 0.258919831321927 | validation: 0.26840368359258693]
	TIME [epoch: 69.9 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28107369745167304		[learning rate: 0.0097943]
	Learning Rate: 0.0097943
	LOSS [training: 0.28107369745167304 | validation: 0.25218289948876615]
	TIME [epoch: 69.6 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2573636313723162		[learning rate: 0.0097778]
	Learning Rate: 0.00977778
	LOSS [training: 0.2573636313723162 | validation: 0.2558967858586695]
	TIME [epoch: 69.5 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25720139530980357		[learning rate: 0.0097613]
	Learning Rate: 0.00976126
	LOSS [training: 0.25720139530980357 | validation: 0.24468817782352786]
	TIME [epoch: 69.2 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2608111205041404		[learning rate: 0.0097447]
	Learning Rate: 0.00974474
	LOSS [training: 0.2608111205041404 | validation: 0.3083325502923572]
	TIME [epoch: 69.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28506784558585835		[learning rate: 0.0097282]
	Learning Rate: 0.00972822
	LOSS [training: 0.28506784558585835 | validation: 0.2467538829292397]
	TIME [epoch: 69.1 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26672501208281857		[learning rate: 0.0097117]
	Learning Rate: 0.0097117
	LOSS [training: 0.26672501208281857 | validation: 0.24850411584467785]
	TIME [epoch: 69.5 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26812474892609406		[learning rate: 0.0096952]
	Learning Rate: 0.00969518
	LOSS [training: 0.26812474892609406 | validation: 0.28039006059149607]
	TIME [epoch: 69.1 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27391166302078934		[learning rate: 0.0096787]
	Learning Rate: 0.00967866
	LOSS [training: 0.27391166302078934 | validation: 0.24225480430696528]
	TIME [epoch: 69.5 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25289165352914206		[learning rate: 0.0096621]
	Learning Rate: 0.00966214
	LOSS [training: 0.25289165352914206 | validation: 0.23895460232894056]
	TIME [epoch: 69.1 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27193262804847573		[learning rate: 0.0096456]
	Learning Rate: 0.00964563
	LOSS [training: 0.27193262804847573 | validation: 0.2634414998427317]
	TIME [epoch: 69.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26111175628722694		[learning rate: 0.0096291]
	Learning Rate: 0.00962911
	LOSS [training: 0.26111175628722694 | validation: 0.23897970076856437]
	TIME [epoch: 69.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2550191837108661		[learning rate: 0.0096126]
	Learning Rate: 0.0096126
	LOSS [training: 0.2550191837108661 | validation: 0.2684513106315463]
	TIME [epoch: 69.4 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2731254715102265		[learning rate: 0.0095961]
	Learning Rate: 0.00959609
	LOSS [training: 0.2731254715102265 | validation: 0.24119641122612379]
	TIME [epoch: 69.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25532734584841854		[learning rate: 0.0095796]
	Learning Rate: 0.00957957
	LOSS [training: 0.25532734584841854 | validation: 0.24439436219147956]
	TIME [epoch: 69.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2753052035703866		[learning rate: 0.0095631]
	Learning Rate: 0.00956306
	LOSS [training: 0.2753052035703866 | validation: 0.2580545533624864]
	TIME [epoch: 69.1 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26031288889713133		[learning rate: 0.0095466]
	Learning Rate: 0.00954655
	LOSS [training: 0.26031288889713133 | validation: 0.24874599495740923]
	TIME [epoch: 69.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2596871473075423		[learning rate: 0.00953]
	Learning Rate: 0.00953004
	LOSS [training: 0.2596871473075423 | validation: 0.26603698864197917]
	TIME [epoch: 69.1 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2719129700332915		[learning rate: 0.0095135]
	Learning Rate: 0.00951354
	LOSS [training: 0.2719129700332915 | validation: 0.25664020881189936]
	TIME [epoch: 69.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26212040196008646		[learning rate: 0.009497]
	Learning Rate: 0.00949703
	LOSS [training: 0.26212040196008646 | validation: 0.24734331356969175]
	TIME [epoch: 69.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2577385678877736		[learning rate: 0.0094805]
	Learning Rate: 0.00948053
	LOSS [training: 0.2577385678877736 | validation: 0.2500113374335053]
	TIME [epoch: 69.1 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2607923529583604		[learning rate: 0.009464]
	Learning Rate: 0.00946402
	LOSS [training: 0.2607923529583604 | validation: 0.25696083910263495]
	TIME [epoch: 69.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2693759884030361		[learning rate: 0.0094475]
	Learning Rate: 0.00944752
	LOSS [training: 0.2693759884030361 | validation: 0.2479503175746438]
	TIME [epoch: 69.1 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2573302372314076		[learning rate: 0.009431]
	Learning Rate: 0.00943102
	LOSS [training: 0.2573302372314076 | validation: 0.2626830004058999]
	TIME [epoch: 69.5 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25953651390690347		[learning rate: 0.0094145]
	Learning Rate: 0.00941452
	LOSS [training: 0.25953651390690347 | validation: 0.2516833690047785]
	TIME [epoch: 69.5 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2746272367634805		[learning rate: 0.009398]
	Learning Rate: 0.00939803
	LOSS [training: 0.2746272367634805 | validation: 0.27989154177255476]
	TIME [epoch: 69.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25907918814281		[learning rate: 0.0093815]
	Learning Rate: 0.00938153
	LOSS [training: 0.25907918814281 | validation: 0.24119414655078864]
	TIME [epoch: 69.1 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.272019624298526		[learning rate: 0.009365]
	Learning Rate: 0.00936504
	LOSS [training: 0.272019624298526 | validation: 0.27233877472500523]
	TIME [epoch: 69.4 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2867680911547268		[learning rate: 0.0093485]
	Learning Rate: 0.00934855
	LOSS [training: 0.2867680911547268 | validation: 0.2537991633274485]
	TIME [epoch: 69.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25559479462095447		[learning rate: 0.0093321]
	Learning Rate: 0.00933206
	LOSS [training: 0.25559479462095447 | validation: 0.24347416465795726]
	TIME [epoch: 69.2 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2498444609708152		[learning rate: 0.0093156]
	Learning Rate: 0.00931557
	LOSS [training: 0.2498444609708152 | validation: 0.24767999875057128]
	TIME [epoch: 69.1 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2661317488907271		[learning rate: 0.0092991]
	Learning Rate: 0.00929908
	LOSS [training: 0.2661317488907271 | validation: 0.23752379781145638]
	TIME [epoch: 69.4 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2624356135179192		[learning rate: 0.0092826]
	Learning Rate: 0.0092826
	LOSS [training: 0.2624356135179192 | validation: 0.2835888885750421]
	TIME [epoch: 69.2 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2703232466719157		[learning rate: 0.0092661]
	Learning Rate: 0.00926612
	LOSS [training: 0.2703232466719157 | validation: 0.26884733062894833]
	TIME [epoch: 69.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26108479596561496		[learning rate: 0.0092496]
	Learning Rate: 0.00924964
	LOSS [training: 0.26108479596561496 | validation: 0.23661369178881414]
	TIME [epoch: 69.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_1046.pth
	Model improved!!!
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2572726443678223		[learning rate: 0.0092332]
	Learning Rate: 0.00923316
	LOSS [training: 0.2572726443678223 | validation: 0.26595651276794613]
	TIME [epoch: 69.6 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2700260098076901		[learning rate: 0.0092167]
	Learning Rate: 0.00921668
	LOSS [training: 0.2700260098076901 | validation: 0.2555501294127759]
	TIME [epoch: 69.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25917819174220713		[learning rate: 0.0092002]
	Learning Rate: 0.00920021
	LOSS [training: 0.25917819174220713 | validation: 0.2664642653782514]
	TIME [epoch: 69.2 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26659769911591263		[learning rate: 0.0091837]
	Learning Rate: 0.00918374
	LOSS [training: 0.26659769911591263 | validation: 0.24573991996645111]
	TIME [epoch: 69.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2609520185935571		[learning rate: 0.0091673]
	Learning Rate: 0.00916727
	LOSS [training: 0.2609520185935571 | validation: 0.23580095376926852]
	TIME [epoch: 69.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_1051.pth
	Model improved!!!
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2512703896047415		[learning rate: 0.0091508]
	Learning Rate: 0.0091508
	LOSS [training: 0.2512703896047415 | validation: 0.246364608311545]
	TIME [epoch: 69.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28179335380046044		[learning rate: 0.0091343]
	Learning Rate: 0.00913434
	LOSS [training: 0.28179335380046044 | validation: 0.24724403996641814]
	TIME [epoch: 69.7 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25127856206466204		[learning rate: 0.0091179]
	Learning Rate: 0.00911787
	LOSS [training: 0.25127856206466204 | validation: 0.24945190833269304]
	TIME [epoch: 69.6 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.255372209167096		[learning rate: 0.0091014]
	Learning Rate: 0.00910141
	LOSS [training: 0.255372209167096 | validation: 0.27361764434122327]
	TIME [epoch: 69.6 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27040645930033574		[learning rate: 0.009085]
	Learning Rate: 0.00908496
	LOSS [training: 0.27040645930033574 | validation: 0.24207117389511454]
	TIME [epoch: 69.7 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26133947619132836		[learning rate: 0.0090685]
	Learning Rate: 0.0090685
	LOSS [training: 0.26133947619132836 | validation: 0.26789215194600446]
	TIME [epoch: 69.6 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.262483589620811		[learning rate: 0.009052]
	Learning Rate: 0.00905205
	LOSS [training: 0.262483589620811 | validation: 0.2455708008750564]
	TIME [epoch: 69.6 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2575493377976759		[learning rate: 0.0090356]
	Learning Rate: 0.0090356
	LOSS [training: 0.2575493377976759 | validation: 0.25501236010014855]
	TIME [epoch: 69.6 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2763132814613108		[learning rate: 0.0090192]
	Learning Rate: 0.00901915
	LOSS [training: 0.2763132814613108 | validation: 0.2900856598238424]
	TIME [epoch: 69.5 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27654709906043173		[learning rate: 0.0090027]
	Learning Rate: 0.00900271
	LOSS [training: 0.27654709906043173 | validation: 0.24165090956726437]
	TIME [epoch: 69.4 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25393257115760015		[learning rate: 0.0089863]
	Learning Rate: 0.00898627
	LOSS [training: 0.25393257115760015 | validation: 0.25202322478490546]
	TIME [epoch: 69.6 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.278274905747957		[learning rate: 0.0089698]
	Learning Rate: 0.00896983
	LOSS [training: 0.278274905747957 | validation: 0.2773678518276643]
	TIME [epoch: 69.6 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2605563542874851		[learning rate: 0.0089534]
	Learning Rate: 0.00895339
	LOSS [training: 0.2605563542874851 | validation: 0.24212233027177343]
	TIME [epoch: 69.6 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24912414026073187		[learning rate: 0.008937]
	Learning Rate: 0.00893696
	LOSS [training: 0.24912414026073187 | validation: 0.24511879777432913]
	TIME [epoch: 69.7 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2949632225796828		[learning rate: 0.0089205]
	Learning Rate: 0.00892053
	LOSS [training: 0.2949632225796828 | validation: 0.2511615018370621]
	TIME [epoch: 69.7 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2593934062394718		[learning rate: 0.0089041]
	Learning Rate: 0.0089041
	LOSS [training: 0.2593934062394718 | validation: 0.2410227752747786]
	TIME [epoch: 69.5 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24912587983541307		[learning rate: 0.0088877]
	Learning Rate: 0.00888767
	LOSS [training: 0.24912587983541307 | validation: 0.2324936055592577]
	TIME [epoch: 69.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_1068.pth
	Model improved!!!
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25937320644731177		[learning rate: 0.0088713]
	Learning Rate: 0.00887125
	LOSS [training: 0.25937320644731177 | validation: 0.27909376585217155]
	TIME [epoch: 69.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26617826413641577		[learning rate: 0.0088548]
	Learning Rate: 0.00885484
	LOSS [training: 0.26617826413641577 | validation: 0.2511074903527552]
	TIME [epoch: 69.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24896720201823175		[learning rate: 0.0088384]
	Learning Rate: 0.00883842
	LOSS [training: 0.24896720201823175 | validation: 0.2443996076013874]
	TIME [epoch: 69.6 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27719014064876407		[learning rate: 0.008822]
	Learning Rate: 0.00882201
	LOSS [training: 0.27719014064876407 | validation: 0.2545530659305781]
	TIME [epoch: 69.9 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25675105369463974		[learning rate: 0.0088056]
	Learning Rate: 0.0088056
	LOSS [training: 0.25675105369463974 | validation: 0.23737405376325527]
	TIME [epoch: 69.7 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25587032258020453		[learning rate: 0.0087892]
	Learning Rate: 0.00878919
	LOSS [training: 0.25587032258020453 | validation: 0.23233867297870453]
	TIME [epoch: 70 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_1074.pth
	Model improved!!!
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2456768848638559		[learning rate: 0.0087728]
	Learning Rate: 0.00877279
	LOSS [training: 0.2456768848638559 | validation: 0.2519504458525124]
	TIME [epoch: 69.8 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2769547792238211		[learning rate: 0.0087564]
	Learning Rate: 0.00875639
	LOSS [training: 0.2769547792238211 | validation: 0.2520207750324536]
	TIME [epoch: 69.8 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2554775432931542		[learning rate: 0.00874]
	Learning Rate: 0.00874
	LOSS [training: 0.2554775432931542 | validation: 0.24143146939629584]
	TIME [epoch: 70.1 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2460845664078587		[learning rate: 0.0087236]
	Learning Rate: 0.00872361
	LOSS [training: 0.2460845664078587 | validation: 0.23742610829816546]
	TIME [epoch: 69.8 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26259207661617767		[learning rate: 0.0087072]
	Learning Rate: 0.00870722
	LOSS [training: 0.26259207661617767 | validation: 0.2438900050137321]
	TIME [epoch: 69.8 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26233193290271567		[learning rate: 0.0086908]
	Learning Rate: 0.00869083
	LOSS [training: 0.26233193290271567 | validation: 0.23620129694261882]
	TIME [epoch: 69.9 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2683886487457357		[learning rate: 0.0086745]
	Learning Rate: 0.00867445
	LOSS [training: 0.2683886487457357 | validation: 0.24882191870517634]
	TIME [epoch: 69.7 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25335343593723875		[learning rate: 0.0086581]
	Learning Rate: 0.00865807
	LOSS [training: 0.25335343593723875 | validation: 0.23289433702328316]
	TIME [epoch: 69.9 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.261604494491129		[learning rate: 0.0086417]
	Learning Rate: 0.0086417
	LOSS [training: 0.261604494491129 | validation: 0.273747844003038]
	TIME [epoch: 69.8 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26073863136651126		[learning rate: 0.0086253]
	Learning Rate: 0.00862533
	LOSS [training: 0.26073863136651126 | validation: 0.24501247336912702]
	TIME [epoch: 69.9 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2616689123878394		[learning rate: 0.008609]
	Learning Rate: 0.00860896
	LOSS [training: 0.2616689123878394 | validation: 0.2498283521735737]
	TIME [epoch: 70 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2555125275012524		[learning rate: 0.0085926]
	Learning Rate: 0.0085926
	LOSS [training: 0.2555125275012524 | validation: 0.24142895860079122]
	TIME [epoch: 69.9 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27064664125027144		[learning rate: 0.0085762]
	Learning Rate: 0.00857624
	LOSS [training: 0.27064664125027144 | validation: 0.25288307476895755]
	TIME [epoch: 69.9 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24692213543253336		[learning rate: 0.0085599]
	Learning Rate: 0.00855989
	LOSS [training: 0.24692213543253336 | validation: 0.23840854034937203]
	TIME [epoch: 70 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26025588034971		[learning rate: 0.0085435]
	Learning Rate: 0.00854354
	LOSS [training: 0.26025588034971 | validation: 0.25158036646379406]
	TIME [epoch: 69.8 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26375192589325175		[learning rate: 0.0085272]
	Learning Rate: 0.00852719
	LOSS [training: 0.26375192589325175 | validation: 0.271373697505436]
	TIME [epoch: 69.9 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2728514927209984		[learning rate: 0.0085108]
	Learning Rate: 0.00851085
	LOSS [training: 0.2728514927209984 | validation: 0.2566220913476673]
	TIME [epoch: 69.9 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2536994673282506		[learning rate: 0.0084945]
	Learning Rate: 0.00849451
	LOSS [training: 0.2536994673282506 | validation: 0.2501247991069053]
	TIME [epoch: 69.6 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2627328168930293		[learning rate: 0.0084782]
	Learning Rate: 0.00847817
	LOSS [training: 0.2627328168930293 | validation: 0.2512265694706252]
	TIME [epoch: 70 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26569620010651396		[learning rate: 0.0084618]
	Learning Rate: 0.00846184
	LOSS [training: 0.26569620010651396 | validation: 0.24743076557794094]
	TIME [epoch: 69.8 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2472098819519051		[learning rate: 0.0084455]
	Learning Rate: 0.00844552
	LOSS [training: 0.2472098819519051 | validation: 0.2437799466939174]
	TIME [epoch: 70 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25871589360889546		[learning rate: 0.0084292]
	Learning Rate: 0.0084292
	LOSS [training: 0.25871589360889546 | validation: 0.2345735597564132]
	TIME [epoch: 69.8 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25003681320252397		[learning rate: 0.0084129]
	Learning Rate: 0.00841288
	LOSS [training: 0.25003681320252397 | validation: 0.259036049527891]
	TIME [epoch: 69.9 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2762692102724971		[learning rate: 0.0083966]
	Learning Rate: 0.00839657
	LOSS [training: 0.2762692102724971 | validation: 0.25140593848390835]
	TIME [epoch: 69.9 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.257502161371423		[learning rate: 0.0083803]
	Learning Rate: 0.00838026
	LOSS [training: 0.257502161371423 | validation: 0.24204238077105583]
	TIME [epoch: 69.8 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25586239752009693		[learning rate: 0.008364]
	Learning Rate: 0.00836395
	LOSS [training: 0.25586239752009693 | validation: 0.26885600246888386]
	TIME [epoch: 70.1 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26396052248384866		[learning rate: 0.0083477]
	Learning Rate: 0.00834765
	LOSS [training: 0.26396052248384866 | validation: 0.2521639536038425]
	TIME [epoch: 69.9 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25314002082080966		[learning rate: 0.0083314]
	Learning Rate: 0.00833136
	LOSS [training: 0.25314002082080966 | validation: 0.2451236627979727]
	TIME [epoch: 70 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.285727346974623		[learning rate: 0.0083151]
	Learning Rate: 0.00831507
	LOSS [training: 0.285727346974623 | validation: 0.2571135513122405]
	TIME [epoch: 70 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.252639270848545		[learning rate: 0.0082988]
	Learning Rate: 0.00829878
	LOSS [training: 0.252639270848545 | validation: 0.246031516171479]
	TIME [epoch: 70.2 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25000970036275144		[learning rate: 0.0082825]
	Learning Rate: 0.0082825
	LOSS [training: 0.25000970036275144 | validation: 0.2548259680340765]
	TIME [epoch: 70 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2714292139715837		[learning rate: 0.0082662]
	Learning Rate: 0.00826622
	LOSS [training: 0.2714292139715837 | validation: 0.25837417047648975]
	TIME [epoch: 70.1 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25865946318934663		[learning rate: 0.0082499]
	Learning Rate: 0.00824995
	LOSS [training: 0.25865946318934663 | validation: 0.2457156758243671]
	TIME [epoch: 70 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2531508656580167		[learning rate: 0.0082337]
	Learning Rate: 0.00823368
	LOSS [training: 0.2531508656580167 | validation: 0.23730657164746935]
	TIME [epoch: 69.9 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24765146328163856		[learning rate: 0.0082174]
	Learning Rate: 0.00821742
	LOSS [training: 0.24765146328163856 | validation: 0.23351954589166837]
	TIME [epoch: 70.2 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2738392201858838		[learning rate: 0.0082012]
	Learning Rate: 0.00820116
	LOSS [training: 0.2738392201858838 | validation: 0.24716674982852632]
	TIME [epoch: 70 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25313185900554264		[learning rate: 0.0081849]
	Learning Rate: 0.00818491
	LOSS [training: 0.25313185900554264 | validation: 0.2314925122695616]
	TIME [epoch: 70.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_1111.pth
	Model improved!!!
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24663523030767925		[learning rate: 0.0081687]
	Learning Rate: 0.00816866
	LOSS [training: 0.24663523030767925 | validation: 0.253835993698205]
	TIME [epoch: 70 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26362359576035055		[learning rate: 0.0081524]
	Learning Rate: 0.00815242
	LOSS [training: 0.26362359576035055 | validation: 0.24476680338093387]
	TIME [epoch: 69.9 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2567629368689518		[learning rate: 0.0081362]
	Learning Rate: 0.00813618
	LOSS [training: 0.2567629368689518 | validation: 0.23544873567908597]
	TIME [epoch: 69.9 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26505999665832514		[learning rate: 0.00812]
	Learning Rate: 0.00811995
	LOSS [training: 0.26505999665832514 | validation: 0.2592291854721459]
	TIME [epoch: 70 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25542753807388463		[learning rate: 0.0081037]
	Learning Rate: 0.00810372
	LOSS [training: 0.25542753807388463 | validation: 0.2431200184568687]
	TIME [epoch: 70.1 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25940547166919914		[learning rate: 0.0080875]
	Learning Rate: 0.0080875
	LOSS [training: 0.25940547166919914 | validation: 0.24474436533228605]
	TIME [epoch: 70 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2551693342372602		[learning rate: 0.0080713]
	Learning Rate: 0.00807128
	LOSS [training: 0.2551693342372602 | validation: 0.24934279691111755]
	TIME [epoch: 70.1 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25466067557313904		[learning rate: 0.0080551]
	Learning Rate: 0.00805507
	LOSS [training: 0.25466067557313904 | validation: 0.27026671400976343]
	TIME [epoch: 70.2 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26664743340142716		[learning rate: 0.0080389]
	Learning Rate: 0.00803887
	LOSS [training: 0.26664743340142716 | validation: 0.2354482878593752]
	TIME [epoch: 70.1 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2527097120285063		[learning rate: 0.0080227]
	Learning Rate: 0.00802267
	LOSS [training: 0.2527097120285063 | validation: 0.25528940560104263]
	TIME [epoch: 70 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2722074330645926		[learning rate: 0.0080065]
	Learning Rate: 0.00800647
	LOSS [training: 0.2722074330645926 | validation: 0.23624985692574163]
	TIME [epoch: 70.1 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24869848555252977		[learning rate: 0.0079903]
	Learning Rate: 0.00799028
	LOSS [training: 0.24869848555252977 | validation: 0.23668860125907817]
	TIME [epoch: 69.8 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2489737654443922		[learning rate: 0.0079741]
	Learning Rate: 0.0079741
	LOSS [training: 0.2489737654443922 | validation: 0.2970886341552109]
	TIME [epoch: 69.6 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2852253220284362		[learning rate: 0.0079579]
	Learning Rate: 0.00795792
	LOSS [training: 0.2852253220284362 | validation: 0.2422692505207653]
	TIME [epoch: 69.7 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24939197293243703		[learning rate: 0.0079417]
	Learning Rate: 0.00794174
	LOSS [training: 0.24939197293243703 | validation: 0.24219916982326306]
	TIME [epoch: 69.8 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2671363134004929		[learning rate: 0.0079256]
	Learning Rate: 0.00792558
	LOSS [training: 0.2671363134004929 | validation: 0.23898684022730754]
	TIME [epoch: 69.8 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2559548245376208		[learning rate: 0.0079094]
	Learning Rate: 0.00790941
	LOSS [training: 0.2559548245376208 | validation: 0.22679406562152377]
	TIME [epoch: 69.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_1128.pth
	Model improved!!!
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25138347260291816		[learning rate: 0.0078933]
	Learning Rate: 0.00789326
	LOSS [training: 0.25138347260291816 | validation: 0.245902840978507]
	TIME [epoch: 69.8 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2670285445630055		[learning rate: 0.0078771]
	Learning Rate: 0.00787711
	LOSS [training: 0.2670285445630055 | validation: 0.2584221064375859]
	TIME [epoch: 69.9 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.257383012250115		[learning rate: 0.007861]
	Learning Rate: 0.00786096
	LOSS [training: 0.257383012250115 | validation: 0.23223745394012435]
	TIME [epoch: 69.6 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25187246949782716		[learning rate: 0.0078448]
	Learning Rate: 0.00784482
	LOSS [training: 0.25187246949782716 | validation: 0.2501936120231803]
	TIME [epoch: 69.9 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2589043713282743		[learning rate: 0.0078287]
	Learning Rate: 0.00782869
	LOSS [training: 0.2589043713282743 | validation: 0.23149604636261883]
	TIME [epoch: 69.8 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2659620569417978		[learning rate: 0.0078126]
	Learning Rate: 0.00781256
	LOSS [training: 0.2659620569417978 | validation: 0.2624155929571502]
	TIME [epoch: 69.8 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2628745647315135		[learning rate: 0.0077964]
	Learning Rate: 0.00779644
	LOSS [training: 0.2628745647315135 | validation: 0.23490377039662383]
	TIME [epoch: 69.9 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2486770461015569		[learning rate: 0.0077803]
	Learning Rate: 0.00778033
	LOSS [training: 0.2486770461015569 | validation: 0.23638604256198856]
	TIME [epoch: 69.9 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2587932104358096		[learning rate: 0.0077642]
	Learning Rate: 0.00776422
	LOSS [training: 0.2587932104358096 | validation: 0.23625630465067066]
	TIME [epoch: 70 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2554672070849478		[learning rate: 0.0077481]
	Learning Rate: 0.00774812
	LOSS [training: 0.2554672070849478 | validation: 0.2439247533027059]
	TIME [epoch: 70.1 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25139875008470924		[learning rate: 0.007732]
	Learning Rate: 0.00773202
	LOSS [training: 0.25139875008470924 | validation: 0.23280887323552457]
	TIME [epoch: 69.7 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24661060572182336		[learning rate: 0.0077159]
	Learning Rate: 0.00771593
	LOSS [training: 0.24661060572182336 | validation: 0.23998379575845097]
	TIME [epoch: 69.7 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26303559116127917		[learning rate: 0.0076998]
	Learning Rate: 0.00769985
	LOSS [training: 0.26303559116127917 | validation: 0.24490312543926673]
	TIME [epoch: 69.8 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25701529662398165		[learning rate: 0.0076838]
	Learning Rate: 0.00768377
	LOSS [training: 0.25701529662398165 | validation: 0.2371265173938336]
	TIME [epoch: 69.9 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2536725128636945		[learning rate: 0.0076677]
	Learning Rate: 0.0076677
	LOSS [training: 0.2536725128636945 | validation: 0.23789587539111898]
	TIME [epoch: 70 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27497183647683293		[learning rate: 0.0076516]
	Learning Rate: 0.00765163
	LOSS [training: 0.27497183647683293 | validation: 0.26659172869819225]
	TIME [epoch: 69.9 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26043095201363187		[learning rate: 0.0076356]
	Learning Rate: 0.00763557
	LOSS [training: 0.26043095201363187 | validation: 0.2516496202417004]
	TIME [epoch: 69.8 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25972052371745347		[learning rate: 0.0076195]
	Learning Rate: 0.00761952
	LOSS [training: 0.25972052371745347 | validation: 0.23875717884968636]
	TIME [epoch: 70.1 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2518983088651668		[learning rate: 0.0076035]
	Learning Rate: 0.00760347
	LOSS [training: 0.2518983088651668 | validation: 0.2488621422492106]
	TIME [epoch: 69.9 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2537096209610774		[learning rate: 0.0075874]
	Learning Rate: 0.00758744
	LOSS [training: 0.2537096209610774 | validation: 0.23354630156974807]
	TIME [epoch: 69.9 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25977756517118206		[learning rate: 0.0075714]
	Learning Rate: 0.0075714
	LOSS [training: 0.25977756517118206 | validation: 0.24668665093400904]
	TIME [epoch: 70 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24879219797619972		[learning rate: 0.0075554]
	Learning Rate: 0.00755538
	LOSS [training: 0.24879219797619972 | validation: 0.24350133963958648]
	TIME [epoch: 70.1 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2576400340953314		[learning rate: 0.0075394]
	Learning Rate: 0.00753936
	LOSS [training: 0.2576400340953314 | validation: 0.2363795813034546]
	TIME [epoch: 70.1 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27153860842656086		[learning rate: 0.0075233]
	Learning Rate: 0.00752335
	LOSS [training: 0.27153860842656086 | validation: 0.2383067984795144]
	TIME [epoch: 70.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24895533843384327		[learning rate: 0.0075073]
	Learning Rate: 0.00750734
	LOSS [training: 0.24895533843384327 | validation: 0.24163289359120774]
	TIME [epoch: 70.1 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2523042108703469		[learning rate: 0.0074913]
	Learning Rate: 0.00749134
	LOSS [training: 0.2523042108703469 | validation: 0.2443222294899195]
	TIME [epoch: 70.1 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2659763607808395		[learning rate: 0.0074754]
	Learning Rate: 0.00747535
	LOSS [training: 0.2659763607808395 | validation: 0.2369224456370535]
	TIME [epoch: 70.2 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24858058549381817		[learning rate: 0.0074594]
	Learning Rate: 0.00745937
	LOSS [training: 0.24858058549381817 | validation: 0.2350117778932158]
	TIME [epoch: 70.2 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2503321304587684		[learning rate: 0.0074434]
	Learning Rate: 0.00744339
	LOSS [training: 0.2503321304587684 | validation: 0.24246029249981885]
	TIME [epoch: 70.2 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26513616296314546		[learning rate: 0.0074274]
	Learning Rate: 0.00742742
	LOSS [training: 0.26513616296314546 | validation: 0.2549797399237016]
	TIME [epoch: 70.2 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26190435603299955		[learning rate: 0.0074115]
	Learning Rate: 0.00741145
	LOSS [training: 0.26190435603299955 | validation: 0.23474982843689748]
	TIME [epoch: 70.2 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2451930641143838		[learning rate: 0.0073955]
	Learning Rate: 0.0073955
	LOSS [training: 0.2451930641143838 | validation: 0.23208175465630532]
	TIME [epoch: 70.2 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2639968058868094		[learning rate: 0.0073795]
	Learning Rate: 0.00737955
	LOSS [training: 0.2639968058868094 | validation: 0.2567292388586949]
	TIME [epoch: 70.2 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2505282345289164		[learning rate: 0.0073636]
	Learning Rate: 0.0073636
	LOSS [training: 0.2505282345289164 | validation: 0.2398652325279181]
	TIME [epoch: 70.2 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25526952074202525		[learning rate: 0.0073477]
	Learning Rate: 0.00734767
	LOSS [training: 0.25526952074202525 | validation: 0.26315950029633606]
	TIME [epoch: 70.1 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2542088423808767		[learning rate: 0.0073317]
	Learning Rate: 0.00733174
	LOSS [training: 0.2542088423808767 | validation: 0.23355849364992287]
	TIME [epoch: 70.2 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2587518373273736		[learning rate: 0.0073158]
	Learning Rate: 0.00731582
	LOSS [training: 0.2587518373273736 | validation: 0.2356043327103861]
	TIME [epoch: 70.1 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2559881045371597		[learning rate: 0.0072999]
	Learning Rate: 0.00729991
	LOSS [training: 0.2559881045371597 | validation: 0.23362849344151312]
	TIME [epoch: 70.1 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24749494040443562		[learning rate: 0.007284]
	Learning Rate: 0.007284
	LOSS [training: 0.24749494040443562 | validation: 0.2434937989298194]
	TIME [epoch: 70.2 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25314056377845784		[learning rate: 0.0072681]
	Learning Rate: 0.0072681
	LOSS [training: 0.25314056377845784 | validation: 0.22817338601003642]
	TIME [epoch: 70.2 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25801035150503915		[learning rate: 0.0072522]
	Learning Rate: 0.00725221
	LOSS [training: 0.25801035150503915 | validation: 0.25171790800645377]
	TIME [epoch: 70.2 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26146002423949677		[learning rate: 0.0072363]
	Learning Rate: 0.00723633
	LOSS [training: 0.26146002423949677 | validation: 0.2348131149755457]
	TIME [epoch: 70.3 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2485007289096677		[learning rate: 0.0072205]
	Learning Rate: 0.00722045
	LOSS [training: 0.2485007289096677 | validation: 0.2310601715031078]
	TIME [epoch: 70.2 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2501198498263819		[learning rate: 0.0072046]
	Learning Rate: 0.00720458
	LOSS [training: 0.2501198498263819 | validation: 0.2358915183641066]
	TIME [epoch: 70.2 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2506520466761465		[learning rate: 0.0071887]
	Learning Rate: 0.00718872
	LOSS [training: 0.2506520466761465 | validation: 0.2615493255620335]
	TIME [epoch: 70.2 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27066420880108033		[learning rate: 0.0071729]
	Learning Rate: 0.00717287
	LOSS [training: 0.27066420880108033 | validation: 0.2454340570576815]
	TIME [epoch: 70.2 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2531509833056347		[learning rate: 0.007157]
	Learning Rate: 0.00715702
	LOSS [training: 0.2531509833056347 | validation: 0.2288741747715597]
	TIME [epoch: 70.2 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24778882987919984		[learning rate: 0.0071412]
	Learning Rate: 0.00714119
	LOSS [training: 0.24778882987919984 | validation: 0.23527095304500942]
	TIME [epoch: 70.3 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2487155585170483		[learning rate: 0.0071254]
	Learning Rate: 0.00712536
	LOSS [training: 0.2487155585170483 | validation: 0.23012616819299292]
	TIME [epoch: 70.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24620375102674608		[learning rate: 0.0071095]
	Learning Rate: 0.00710954
	LOSS [training: 0.24620375102674608 | validation: 0.22669677560232798]
	TIME [epoch: 70.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_1178.pth
	Model improved!!!
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2715697565858487		[learning rate: 0.0070937]
	Learning Rate: 0.00709372
	LOSS [training: 0.2715697565858487 | validation: 0.24243844872209025]
	TIME [epoch: 70.3 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2636566905773888		[learning rate: 0.0070779]
	Learning Rate: 0.00707792
	LOSS [training: 0.2636566905773888 | validation: 0.2336002529201868]
	TIME [epoch: 70.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24647979010491855		[learning rate: 0.0070621]
	Learning Rate: 0.00706212
	LOSS [training: 0.24647979010491855 | validation: 0.23679148999022467]
	TIME [epoch: 70.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26819416100667026		[learning rate: 0.0070463]
	Learning Rate: 0.00704633
	LOSS [training: 0.26819416100667026 | validation: 0.23752249349800275]
	TIME [epoch: 70.2 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24608675003793348		[learning rate: 0.0070305]
	Learning Rate: 0.00703055
	LOSS [training: 0.24608675003793348 | validation: 0.23009345670528975]
	TIME [epoch: 70.3 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24557147769178533		[learning rate: 0.0070148]
	Learning Rate: 0.00701477
	LOSS [training: 0.24557147769178533 | validation: 0.2339204593602698]
	TIME [epoch: 70.3 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24881947629019863		[learning rate: 0.006999]
	Learning Rate: 0.00699901
	LOSS [training: 0.24881947629019863 | validation: 0.2351150198438473]
	TIME [epoch: 70.3 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26242997193297396		[learning rate: 0.0069833]
	Learning Rate: 0.00698325
	LOSS [training: 0.26242997193297396 | validation: 0.24697607577639363]
	TIME [epoch: 70.3 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25075993086491777		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.25075993086491777 | validation: 0.23226590657647433]
	TIME [epoch: 70.2 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25378306291876773		[learning rate: 0.0069518]
	Learning Rate: 0.00695176
	LOSS [training: 0.25378306291876773 | validation: 0.23654424669087554]
	TIME [epoch: 70.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2511622396099382		[learning rate: 0.006936]
	Learning Rate: 0.00693603
	LOSS [training: 0.2511622396099382 | validation: 0.2263800460728815]
	TIME [epoch: 70.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_1189.pth
	Model improved!!!
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24607070844330536		[learning rate: 0.0069203]
	Learning Rate: 0.0069203
	LOSS [training: 0.24607070844330536 | validation: 0.23662571081149092]
	TIME [epoch: 70.3 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26190513692669337		[learning rate: 0.0069046]
	Learning Rate: 0.00690459
	LOSS [training: 0.26190513692669337 | validation: 0.23746281181937656]
	TIME [epoch: 70.4 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2603649502309064		[learning rate: 0.0068889]
	Learning Rate: 0.00688888
	LOSS [training: 0.2603649502309064 | validation: 0.2395238767417907]
	TIME [epoch: 70.4 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2513297399600529		[learning rate: 0.0068732]
	Learning Rate: 0.00687318
	LOSS [training: 0.2513297399600529 | validation: 0.23896109933322715]
	TIME [epoch: 70.5 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2470306985679659		[learning rate: 0.0068575]
	Learning Rate: 0.00685749
	LOSS [training: 0.2470306985679659 | validation: 0.23501529948539363]
	TIME [epoch: 70.3 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2593822875787668		[learning rate: 0.0068418]
	Learning Rate: 0.00684181
	LOSS [training: 0.2593822875787668 | validation: 0.2517403073381643]
	TIME [epoch: 70.4 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24548461936601812		[learning rate: 0.0068261]
	Learning Rate: 0.00682614
	LOSS [training: 0.24548461936601812 | validation: 0.23868683821996411]
	TIME [epoch: 70.4 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25547083153786565		[learning rate: 0.0068105]
	Learning Rate: 0.00681048
	LOSS [training: 0.25547083153786565 | validation: 0.25373151096229624]
	TIME [epoch: 70.4 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2573531430988427		[learning rate: 0.0067948]
	Learning Rate: 0.00679482
	LOSS [training: 0.2573531430988427 | validation: 0.23829707229774194]
	TIME [epoch: 70.4 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25371268482796605		[learning rate: 0.0067792]
	Learning Rate: 0.00677917
	LOSS [training: 0.25371268482796605 | validation: 0.23750979192683247]
	TIME [epoch: 70.3 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2529147017703529		[learning rate: 0.0067635]
	Learning Rate: 0.00676354
	LOSS [training: 0.2529147017703529 | validation: 0.2264055257210354]
	TIME [epoch: 70.4 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2478967948008151		[learning rate: 0.0067479]
	Learning Rate: 0.00674791
	LOSS [training: 0.2478967948008151 | validation: 0.23352759516189975]
	TIME [epoch: 70.3 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25443627824670245		[learning rate: 0.0067323]
	Learning Rate: 0.00673229
	LOSS [training: 0.25443627824670245 | validation: 0.24775057439921]
	TIME [epoch: 70.3 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25299795927386626		[learning rate: 0.0067167]
	Learning Rate: 0.00671668
	LOSS [training: 0.25299795927386626 | validation: 0.23963525737321345]
	TIME [epoch: 70.4 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24984702961996363		[learning rate: 0.0067011]
	Learning Rate: 0.00670108
	LOSS [training: 0.24984702961996363 | validation: 0.23629396007219527]
	TIME [epoch: 70.4 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24921721616702358		[learning rate: 0.0066855]
	Learning Rate: 0.00668548
	LOSS [training: 0.24921721616702358 | validation: 0.23642064125173912]
	TIME [epoch: 70.4 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25338582394393927		[learning rate: 0.0066699]
	Learning Rate: 0.0066699
	LOSS [training: 0.25338582394393927 | validation: 0.24255689766796296]
	TIME [epoch: 70.4 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2557813556107422		[learning rate: 0.0066543]
	Learning Rate: 0.00665432
	LOSS [training: 0.2557813556107422 | validation: 0.24919055702922543]
	TIME [epoch: 70.4 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25536105959775446		[learning rate: 0.0066388]
	Learning Rate: 0.00663876
	LOSS [training: 0.25536105959775446 | validation: 0.24103030894513017]
	TIME [epoch: 70.4 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2442912700364605		[learning rate: 0.0066232]
	Learning Rate: 0.0066232
	LOSS [training: 0.2442912700364605 | validation: 0.24059980108753662]
	TIME [epoch: 70.4 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2692249379026464		[learning rate: 0.0066077]
	Learning Rate: 0.00660765
	LOSS [training: 0.2692249379026464 | validation: 0.23635632273144785]
	TIME [epoch: 70.3 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25016661263621753		[learning rate: 0.0065921]
	Learning Rate: 0.00659212
	LOSS [training: 0.25016661263621753 | validation: 0.24492459914226034]
	TIME [epoch: 70.3 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25045289630303547		[learning rate: 0.0065766]
	Learning Rate: 0.00657659
	LOSS [training: 0.25045289630303547 | validation: 0.23891219535015945]
	TIME [epoch: 70.4 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2554187542632364		[learning rate: 0.0065611]
	Learning Rate: 0.00656107
	LOSS [training: 0.2554187542632364 | validation: 0.23418062715964955]
	TIME [epoch: 70.4 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24403385976156283		[learning rate: 0.0065456]
	Learning Rate: 0.00654556
	LOSS [training: 0.24403385976156283 | validation: 0.23641073682795372]
	TIME [epoch: 70.2 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24500392801605822		[learning rate: 0.0065301]
	Learning Rate: 0.00653006
	LOSS [training: 0.24500392801605822 | validation: 0.23712585508325187]
	TIME [epoch: 70.3 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24858114417317462		[learning rate: 0.0065146]
	Learning Rate: 0.00651457
	LOSS [training: 0.24858114417317462 | validation: 0.24018985991756253]
	TIME [epoch: 70.2 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26303353386993206		[learning rate: 0.0064991]
	Learning Rate: 0.00649909
	LOSS [training: 0.26303353386993206 | validation: 0.2524795224236101]
	TIME [epoch: 70.3 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25106855032123515		[learning rate: 0.0064836]
	Learning Rate: 0.00648362
	LOSS [training: 0.25106855032123515 | validation: 0.23645865246585646]
	TIME [epoch: 70.3 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2547059492868426		[learning rate: 0.0064682]
	Learning Rate: 0.00646815
	LOSS [training: 0.2547059492868426 | validation: 0.23911542465573837]
	TIME [epoch: 70.4 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24718167023944349		[learning rate: 0.0064527]
	Learning Rate: 0.0064527
	LOSS [training: 0.24718167023944349 | validation: 0.23684011012371003]
	TIME [epoch: 70.3 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.254234524231534		[learning rate: 0.0064373]
	Learning Rate: 0.00643726
	LOSS [training: 0.254234524231534 | validation: 0.22889977845130308]
	TIME [epoch: 70.3 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25451590605561625		[learning rate: 0.0064218]
	Learning Rate: 0.00642183
	LOSS [training: 0.25451590605561625 | validation: 0.24564003858014907]
	TIME [epoch: 70.3 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25113577819831867		[learning rate: 0.0064064]
	Learning Rate: 0.0064064
	LOSS [training: 0.25113577819831867 | validation: 0.23517729814067134]
	TIME [epoch: 70.3 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24625090676271674		[learning rate: 0.006391]
	Learning Rate: 0.00639099
	LOSS [training: 0.24625090676271674 | validation: 0.2431713006529757]
	TIME [epoch: 70.3 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24948659467783207		[learning rate: 0.0063756]
	Learning Rate: 0.00637559
	LOSS [training: 0.24948659467783207 | validation: 0.2419256545233975]
	TIME [epoch: 70.3 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2596870324432896		[learning rate: 0.0063602]
	Learning Rate: 0.00636019
	LOSS [training: 0.2596870324432896 | validation: 0.22694209480979916]
	TIME [epoch: 70.3 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24310407299641668		[learning rate: 0.0063448]
	Learning Rate: 0.00634481
	LOSS [training: 0.24310407299641668 | validation: 0.2327333803716707]
	TIME [epoch: 70.4 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25305290838646743		[learning rate: 0.0063294]
	Learning Rate: 0.00632944
	LOSS [training: 0.25305290838646743 | validation: 0.2245934694960996]
	TIME [epoch: 70.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_1228.pth
	Model improved!!!
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25197737590469316		[learning rate: 0.0063141]
	Learning Rate: 0.00631407
	LOSS [training: 0.25197737590469316 | validation: 0.2279487825139077]
	TIME [epoch: 70.3 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2541217110088756		[learning rate: 0.0062987]
	Learning Rate: 0.00629872
	LOSS [training: 0.2541217110088756 | validation: 0.24570317455819288]
	TIME [epoch: 70.4 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.255617738552678		[learning rate: 0.0062834]
	Learning Rate: 0.00628338
	LOSS [training: 0.255617738552678 | validation: 0.23852447879317437]
	TIME [epoch: 70.3 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2482637081329971		[learning rate: 0.006268]
	Learning Rate: 0.00626804
	LOSS [training: 0.2482637081329971 | validation: 0.23117140514511986]
	TIME [epoch: 70.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24816604969052544		[learning rate: 0.0062527]
	Learning Rate: 0.00625272
	LOSS [training: 0.24816604969052544 | validation: 0.2316591284053937]
	TIME [epoch: 70.4 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.243596152077278		[learning rate: 0.0062374]
	Learning Rate: 0.00623741
	LOSS [training: 0.243596152077278 | validation: 0.22237039128659197]
	TIME [epoch: 70.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd3_20240713_103448/states/model_phiq_1a_v_mmd3_1234.pth
	Model improved!!!
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24299877641094597		[learning rate: 0.0062221]
	Learning Rate: 0.00622211
	LOSS [training: 0.24299877641094597 | validation: 0.2407019380214383]
	TIME [epoch: 70.3 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2555049987970907		[learning rate: 0.0062068]
	Learning Rate: 0.00620681
	LOSS [training: 0.2555049987970907 | validation: 0.23463526833243298]
	TIME [epoch: 70.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2604330044326634		[learning rate: 0.0061915]
	Learning Rate: 0.00619153
	LOSS [training: 0.2604330044326634 | validation: 0.2311460546650822]
	TIME [epoch: 70.3 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24484006034317313		[learning rate: 0.0061763]
	Learning Rate: 0.00617626
	LOSS [training: 0.24484006034317313 | validation: 0.2376326806847887]
	TIME [epoch: 70.3 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24442588239665503		[learning rate: 0.006161]
	Learning Rate: 0.006161
	LOSS [training: 0.24442588239665503 | validation: 0.2303544023595214]
	TIME [epoch: 70.2 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2570201297738315		[learning rate: 0.0061458]
	Learning Rate: 0.00614575
	LOSS [training: 0.2570201297738315 | validation: 0.22960509346484126]
	TIME [epoch: 70.3 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2492576072419048		[learning rate: 0.0061305]
	Learning Rate: 0.00613051
	LOSS [training: 0.2492576072419048 | validation: 0.23697656564994024]
	TIME [epoch: 70.3 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2398796592394579		[learning rate: 0.0061153]
	Learning Rate: 0.00611528
	LOSS [training: 0.2398796592394579 | validation: 0.242601281497546]
	TIME [epoch: 70.2 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26162090571003804		[learning rate: 0.0061001]
	Learning Rate: 0.00610007
	LOSS [training: 0.26162090571003804 | validation: 0.23569081036973005]
	TIME [epoch: 70.1 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.246496533373337		[learning rate: 0.0060849]
	Learning Rate: 0.00608486
	LOSS [training: 0.246496533373337 | validation: 0.2368302599967867]
	TIME [epoch: 70.2 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2495730330300179		[learning rate: 0.0060697]
	Learning Rate: 0.00606966
	LOSS [training: 0.2495730330300179 | validation: 0.22998560575773425]
	TIME [epoch: 70.2 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2450478989134278		[learning rate: 0.0060545]
	Learning Rate: 0.00605447
	LOSS [training: 0.2450478989134278 | validation: 0.23529593633622237]
	TIME [epoch: 70.1 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26080597457242616		[learning rate: 0.0060393]
	Learning Rate: 0.0060393
	LOSS [training: 0.26080597457242616 | validation: 0.239100032829991]
	TIME [epoch: 70.3 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2532332804606747		[learning rate: 0.0060241]
	Learning Rate: 0.00602413
	LOSS [training: 0.2532332804606747 | validation: 0.24670401075075088]
	TIME [epoch: 70.2 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24859566621237386		[learning rate: 0.006009]
	Learning Rate: 0.00600898
	LOSS [training: 0.24859566621237386 | validation: 0.23660151878915076]
	TIME [epoch: 70.2 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.241214293755002		[learning rate: 0.0059938]
	Learning Rate: 0.00599384
	LOSS [training: 0.241214293755002 | validation: 0.23565664533101435]
	TIME [epoch: 70.2 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24609622678940068		[learning rate: 0.0059787]
	Learning Rate: 0.00597871
	LOSS [training: 0.24609622678940068 | validation: 0.2422342749465744]
	TIME [epoch: 70.2 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2533335855713252		[learning rate: 0.0059636]
	Learning Rate: 0.00596359
	LOSS [training: 0.2533335855713252 | validation: 0.2401756869907331]
	TIME [epoch: 70.2 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24962932258356563		[learning rate: 0.0059485]
	Learning Rate: 0.00594848
	LOSS [training: 0.24962932258356563 | validation: 0.25171596996177226]
	TIME [epoch: 70.2 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2519568930797192		[learning rate: 0.0059334]
	Learning Rate: 0.00593338
	LOSS [training: 0.2519568930797192 | validation: 0.24247986412521028]
	TIME [epoch: 70.2 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2482835028985849		[learning rate: 0.0059183]
	Learning Rate: 0.00591829
	LOSS [training: 0.2482835028985849 | validation: 0.22437809373170758]
	TIME [epoch: 70.2 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2515195720569313		[learning rate: 0.0059032]
	Learning Rate: 0.00590321
	LOSS [training: 0.2515195720569313 | validation: 0.23759332406172118]
	TIME [epoch: 70.2 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2470635765412448		[learning rate: 0.0058881]
	Learning Rate: 0.00588815
	LOSS [training: 0.2470635765412448 | validation: 0.23295941265434872]
	TIME [epoch: 70.2 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2472295576106325		[learning rate: 0.0058731]
	Learning Rate: 0.00587309
	LOSS [training: 0.2472295576106325 | validation: 0.23303358454930606]
	TIME [epoch: 70.2 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2516516421449464		[learning rate: 0.0058581]
	Learning Rate: 0.00585805
	LOSS [training: 0.2516516421449464 | validation: 0.22970680622761416]
	TIME [epoch: 70.2 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25110626232513006		[learning rate: 0.005843]
	Learning Rate: 0.00584302
	LOSS [training: 0.25110626232513006 | validation: 0.23328529674904597]
	TIME [epoch: 70.1 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2468745075595046		[learning rate: 0.005828]
	Learning Rate: 0.005828
	LOSS [training: 0.2468745075595046 | validation: 0.23250932721011447]
	TIME [epoch: 70.2 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24446104482723763		[learning rate: 0.005813]
	Learning Rate: 0.00581299
	LOSS [training: 0.24446104482723763 | validation: 0.23715942481224803]
	TIME [epoch: 70.2 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24984043747669762		[learning rate: 0.005798]
	Learning Rate: 0.005798
	LOSS [training: 0.24984043747669762 | validation: 0.2419221408991964]
	TIME [epoch: 70.2 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.249121634136377		[learning rate: 0.005783]
	Learning Rate: 0.00578301
	LOSS [training: 0.249121634136377 | validation: 0.23916149647132906]
	TIME [epoch: 70.3 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2577371267541684		[learning rate: 0.005768]
	Learning Rate: 0.00576804
	LOSS [training: 0.2577371267541684 | validation: 0.2331699620798528]
	TIME [epoch: 70.3 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24542344249658335		[learning rate: 0.0057531]
	Learning Rate: 0.00575307
	LOSS [training: 0.24542344249658335 | validation: 0.23210020039742774]
	TIME [epoch: 70.1 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24429690912085134		[learning rate: 0.0057381]
	Learning Rate: 0.00573812
	LOSS [training: 0.24429690912085134 | validation: 0.22622178650261776]
	TIME [epoch: 70.2 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2574140583959562		[learning rate: 0.0057232]
	Learning Rate: 0.00572318
	LOSS [training: 0.2574140583959562 | validation: 0.23926152291816452]
	TIME [epoch: 70.2 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24737808471975514		[learning rate: 0.0057083]
	Learning Rate: 0.00570826
	LOSS [training: 0.24737808471975514 | validation: 0.2344833840580014]
	TIME [epoch: 70.2 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2436821800344001		[learning rate: 0.0056933]
	Learning Rate: 0.00569334
	LOSS [training: 0.2436821800344001 | validation: 0.22718087164435158]
	TIME [epoch: 70.2 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24156483288681552		[learning rate: 0.0056784]
	Learning Rate: 0.00567844
	LOSS [training: 0.24156483288681552 | validation: 0.22803179767381335]
	TIME [epoch: 70.2 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26974014839001104		[learning rate: 0.0056635]
	Learning Rate: 0.00566355
	LOSS [training: 0.26974014839001104 | validation: 0.2391952196187859]
	TIME [epoch: 70.2 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24789063066927908		[learning rate: 0.0056487]
	Learning Rate: 0.00564867
	LOSS [training: 0.24789063066927908 | validation: 0.22954070362475634]
	TIME [epoch: 70.3 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.247963614877886		[learning rate: 0.0056338]
	Learning Rate: 0.0056338
	LOSS [training: 0.247963614877886 | validation: 0.23892843839340572]
	TIME [epoch: 70.2 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24907201353091168		[learning rate: 0.0056189]
	Learning Rate: 0.00561894
	LOSS [training: 0.24907201353091168 | validation: 0.2316519104185741]
	TIME [epoch: 70.2 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24548978167108562		[learning rate: 0.0056041]
	Learning Rate: 0.0056041
	LOSS [training: 0.24548978167108562 | validation: 0.23209869424092044]
	TIME [epoch: 70.3 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2473621696394939		[learning rate: 0.0055893]
	Learning Rate: 0.00558927
	LOSS [training: 0.2473621696394939 | validation: 0.22651343367844873]
	TIME [epoch: 70.3 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24673174408834792		[learning rate: 0.0055744]
	Learning Rate: 0.00557445
	LOSS [training: 0.24673174408834792 | validation: 0.23223192682361438]
	TIME [epoch: 70.2 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2526064528035568		[learning rate: 0.0055596]
	Learning Rate: 0.00555964
	LOSS [training: 0.2526064528035568 | validation: 0.23893880764396846]
	TIME [epoch: 70.3 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2498446642877744		[learning rate: 0.0055448]
	Learning Rate: 0.00554484
	LOSS [training: 0.2498446642877744 | validation: 0.22709998972969267]
	TIME [epoch: 70.3 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24465290225629155		[learning rate: 0.0055301]
	Learning Rate: 0.00553006
	LOSS [training: 0.24465290225629155 | validation: 0.2311507745658657]
	TIME [epoch: 70.3 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24800004834591177		[learning rate: 0.0055153]
	Learning Rate: 0.00551529
	LOSS [training: 0.24800004834591177 | validation: 0.24355578964888341]
	TIME [epoch: 70.3 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24359621051814645		[learning rate: 0.0055005]
	Learning Rate: 0.00550053
	LOSS [training: 0.24359621051814645 | validation: 0.23009815837481434]
	TIME [epoch: 70.3 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2441730616933282		[learning rate: 0.0054858]
	Learning Rate: 0.00548578
	LOSS [training: 0.2441730616933282 | validation: 0.24708526963466854]
	TIME [epoch: 70.3 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24901872531169678		[learning rate: 0.005471]
	Learning Rate: 0.00547105
	LOSS [training: 0.24901872531169678 | validation: 0.23378015882356734]
	TIME [epoch: 70.3 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25150416849923746		[learning rate: 0.0054563]
	Learning Rate: 0.00545632
	LOSS [training: 0.25150416849923746 | validation: 0.24297798882558377]
	TIME [epoch: 70.3 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25554003867182784		[learning rate: 0.0054416]
	Learning Rate: 0.00544161
	LOSS [training: 0.25554003867182784 | validation: 0.2396688397074201]
	TIME [epoch: 70.3 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24610893389168279		[learning rate: 0.0054269]
	Learning Rate: 0.00542692
	LOSS [training: 0.24610893389168279 | validation: 0.23103325714777656]
	TIME [epoch: 70.3 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24219342083868456		[learning rate: 0.0054122]
	Learning Rate: 0.00541223
	LOSS [training: 0.24219342083868456 | validation: 0.22939489951257447]
	TIME [epoch: 70.2 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24176570266318148		[learning rate: 0.0053976]
	Learning Rate: 0.00539756
	LOSS [training: 0.24176570266318148 | validation: 0.2238556436665307]
	TIME [epoch: 70.2 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2546127792479786		[learning rate: 0.0053829]
	Learning Rate: 0.0053829
	LOSS [training: 0.2546127792479786 | validation: 0.24910679692330617]
	TIME [epoch: 70.1 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.250433320442314		[learning rate: 0.0053683]
	Learning Rate: 0.00536825
	LOSS [training: 0.250433320442314 | validation: 0.22930082023776036]
	TIME [epoch: 70.2 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24518117029610498		[learning rate: 0.0053536]
	Learning Rate: 0.00535362
	LOSS [training: 0.24518117029610498 | validation: 0.23042594951361378]
	TIME [epoch: 70.2 sec]
EPOCH 1294/2000:
	Training over batches...
