Args:
Namespace(name='model_phiq_1a_v_mmd7', outdir='out/model_training/model_phiq_1a_v_mmd7', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.0001, 0.001, 0.01, 0.1, 1.0, 10.0], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 99560355

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_0.pth
EPOCH 1/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 2.8773026836371813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8773026836371813 | validation: 2.9064761852065004]
	TIME [epoch: 153 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.734545715694349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.734545715694349 | validation: 2.8085210089986217]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.640644907414838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.640644907414838 | validation: 2.7180052466435054]
	TIME [epoch: 8.01 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.558984913712559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.558984913712559 | validation: 2.6401347021567423]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4176845928027704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4176845928027704 | validation: 2.402674562412101]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2379312951681256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2379312951681256 | validation: 2.050791408935492]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.072193850117798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.072193850117798 | validation: 2.004910734380037]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9730932797572809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9730932797572809 | validation: 1.9736160875635824]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9156202103947573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9156202103947573 | validation: 1.9265432975614325]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855560833810379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.855560833810379 | validation: 1.9040579583216362]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8526215224857971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8526215224857971 | validation: 1.8183427117711242]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7819733250332888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7819733250332888 | validation: 1.814909766501549]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.730417225030282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.730417225030282 | validation: 1.8086114044910242]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6928381135679023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6928381135679023 | validation: 1.7164447874619757]
	TIME [epoch: 8.03 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6293260732184591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6293260732184591 | validation: 1.7116569929713243]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5876677397516785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5876677397516785 | validation: 1.6019331795149743]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5316295568822045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5316295568822045 | validation: 1.6296813942686115]
	TIME [epoch: 7.97 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5099652916870692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5099652916870692 | validation: 1.5291450751964342]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4470093811995584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4470093811995584 | validation: 1.5179156357123276]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4129763888028117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4129763888028117 | validation: 1.4635223791825775]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3833614177908715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3833614177908715 | validation: 1.3900489483969694]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3231811639061495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3231811639061495 | validation: 1.3943973093650905]
	TIME [epoch: 7.98 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3080204870117917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3080204870117917 | validation: 1.3408697130452383]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2453633462461815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2453633462461815 | validation: 1.2770930099491333]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2268813294239356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2268813294239356 | validation: 1.2684551695420572]
	TIME [epoch: 8.02 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1727945541761415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1727945541761415 | validation: 1.279550427878503]
	TIME [epoch: 8.03 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1950914307353167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1950914307353167 | validation: 1.1972930889580526]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1252152907070152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1252152907070152 | validation: 1.1868286296710133]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.093543408788626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.093543408788626 | validation: 1.1395224907525212]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.067267308862351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.067267308862351 | validation: 1.1225330908117006]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0391499174494674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0391499174494674 | validation: 1.0768248415335857]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0046426084181317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0046426084181317 | validation: 1.0421344333130345]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9595195480407066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9595195480407066 | validation: 1.0027819555892794]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9537769247461024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9537769247461024 | validation: 0.9743098284860836]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9335726219778666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9335726219778666 | validation: 1.0392688944949298]
	TIME [epoch: 7.97 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9094282768310935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9094282768310935 | validation: 0.8327184978301014]
	TIME [epoch: 8.01 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8180832751339301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8180832751339301 | validation: 0.7992876015924794]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7931000716938497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7931000716938497 | validation: 0.8441622236592277]
	TIME [epoch: 7.99 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7898319063809697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7898319063809697 | validation: 0.7814946071941694]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7440945355846857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7440945355846857 | validation: 0.749510435029338]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7192463774311613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7192463774311613 | validation: 0.7075201846214755]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6962038823372998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6962038823372998 | validation: 0.7007278517235392]
	TIME [epoch: 8.01 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6728928864419998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6728928864419998 | validation: 0.7268357976016044]
	TIME [epoch: 7.97 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6852066836516078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6852066836516078 | validation: 0.6658489290774458]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6528492871765029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6528492871765029 | validation: 0.6660430671542446]
	TIME [epoch: 7.97 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6336081718758327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6336081718758327 | validation: 0.6103672574470179]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6272653461963394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6272653461963394 | validation: 0.627824001341263]
	TIME [epoch: 8.02 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.607077849473418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.607077849473418 | validation: 0.8417575674430711]
	TIME [epoch: 7.98 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7452899081515939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7452899081515939 | validation: 0.6328686709707902]
	TIME [epoch: 7.97 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6202439343927618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6202439343927618 | validation: 0.6138768245306754]
	TIME [epoch: 7.96 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5906848637945484		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.5906848637945484 | validation: 0.6133232654969396]
	TIME [epoch: 7.97 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5938726437972617		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.5938726437972617 | validation: 0.5814439430138513]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6072981484357873		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.6072981484357873 | validation: 0.6067225053891985]
	TIME [epoch: 8.02 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5799610890318939		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.5799610890318939 | validation: 0.584319446019752]
	TIME [epoch: 7.99 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5647375621411145		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.5647375621411145 | validation: 0.5723460757878985]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.556161108348649		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.556161108348649 | validation: 0.5455961501902407]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5584876933438176		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.5584876933438176 | validation: 0.6053952918703231]
	TIME [epoch: 7.97 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.553708633625065		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.553708633625065 | validation: 0.6653194221856786]
	TIME [epoch: 7.98 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6075183007434852		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.6075183007434852 | validation: 0.5450525751836819]
	TIME [epoch: 8.03 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5437640419214771		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.5437640419214771 | validation: 0.5258401834008377]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5334294235499899		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.5334294235499899 | validation: 0.6214296095068692]
	TIME [epoch: 7.99 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5495879995016142		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.5495879995016142 | validation: 0.5379041318562686]
	TIME [epoch: 7.98 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.674200342020369		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.674200342020369 | validation: 0.9207682034399343]
	TIME [epoch: 7.98 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9170891219843726		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.9170891219843726 | validation: 0.701388339562941]
	TIME [epoch: 8 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7057556231764279		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.7057556231764279 | validation: 0.5623128881513211]
	TIME [epoch: 8.01 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5773952084815142		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.5773952084815142 | validation: 0.5666653562576763]
	TIME [epoch: 7.98 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5718884079327758		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.5718884079327758 | validation: 0.5477713405772366]
	TIME [epoch: 7.98 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5468262189821504		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.5468262189821504 | validation: 0.5345098520512637]
	TIME [epoch: 7.98 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5332275595998462		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.5332275595998462 | validation: 0.5304531535410003]
	TIME [epoch: 7.98 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.520547095266394		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.520547095266394 | validation: 0.5106600355075244]
	TIME [epoch: 8.01 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5129074131767096		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.5129074131767096 | validation: 0.6031963447204118]
	TIME [epoch: 8 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5817157782582641		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.5817157782582641 | validation: 0.6278811231750754]
	TIME [epoch: 7.98 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6258510698622523		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.6258510698622523 | validation: 0.655417128257346]
	TIME [epoch: 7.98 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5934730680857422		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.5934730680857422 | validation: 0.5615011907857885]
	TIME [epoch: 7.98 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5404999268090549		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.5404999268090549 | validation: 0.5520415215025094]
	TIME [epoch: 7.98 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5293341187462395		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.5293341187462395 | validation: 0.5336326853681588]
	TIME [epoch: 8.02 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5205422592236373		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.5205422592236373 | validation: 0.5283656872283363]
	TIME [epoch: 7.99 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5170384811976929		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.5170384811976929 | validation: 0.5381481296052705]
	TIME [epoch: 7.98 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5220015701466271		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.5220015701466271 | validation: 0.7008129953366141]
	TIME [epoch: 7.98 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.85021439439232		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.85021439439232 | validation: 0.7932563595182318]
	TIME [epoch: 7.97 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8623768673492348		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.8623768673492348 | validation: 0.7420519874768479]
	TIME [epoch: 7.98 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7400084970532452		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.7400084970532452 | validation: 0.5943999437282217]
	TIME [epoch: 8.02 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6628866086254608		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.6628866086254608 | validation: 0.5596255699087862]
	TIME [epoch: 7.99 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5892978304965993		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.5892978304965993 | validation: 0.5129547170840415]
	TIME [epoch: 7.98 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5300286188441656		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.5300286188441656 | validation: 0.5050941157341262]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5098912525659387		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.5098912525659387 | validation: 0.49468992373864934]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49965930798608504		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.49965930798608504 | validation: 0.5890465883670257]
	TIME [epoch: 7.97 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5401286898181679		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.5401286898181679 | validation: 0.5447358747933413]
	TIME [epoch: 8.02 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5074448823699336		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.5074448823699336 | validation: 0.49200676965609635]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5093293549317506		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.5093293549317506 | validation: 0.5043695102684904]
	TIME [epoch: 7.97 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5052965505529657		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.5052965505529657 | validation: 0.5172611513361542]
	TIME [epoch: 7.97 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49373606190300967		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.49373606190300967 | validation: 0.4752652744205207]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49630591837999183		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.49630591837999183 | validation: 0.4737142480461779]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5480369087060608		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.5480369087060608 | validation: 0.5302293852359795]
	TIME [epoch: 8 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5079684504684052		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.5079684504684052 | validation: 0.4727886639023786]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48847368451153117		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.48847368451153117 | validation: 0.46575820466561646]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47448243881563373		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.47448243881563373 | validation: 0.4625406622174819]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4746344992495548		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.4746344992495548 | validation: 0.4520439027360155]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47045371895137816		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.47045371895137816 | validation: 0.4664893052491523]
	TIME [epoch: 8.02 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46198932474059196		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.46198932474059196 | validation: 0.4620181917304921]
	TIME [epoch: 7.99 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6515455732799513		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.6515455732799513 | validation: 0.7408157575568519]
	TIME [epoch: 7.98 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6267812632908957		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.6267812632908957 | validation: 0.6509314431679312]
	TIME [epoch: 7.98 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5466748536518226		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.5466748536518226 | validation: 0.5259980050919923]
	TIME [epoch: 7.98 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4750679612003572		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.4750679612003572 | validation: 0.45437283917138477]
	TIME [epoch: 7.98 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45649856555288654		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.45649856555288654 | validation: 0.44861082918073175]
	TIME [epoch: 8.02 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45848149121679427		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.45848149121679427 | validation: 0.45562594295644104]
	TIME [epoch: 7.97 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.515120407378491		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.515120407378491 | validation: 0.5577095219663377]
	TIME [epoch: 7.97 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5766739870111206		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.5766739870111206 | validation: 0.7652786790867471]
	TIME [epoch: 7.97 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6576474055197692		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.6576474055197692 | validation: 0.7425477342111504]
	TIME [epoch: 7.97 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6137981096058697		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.6137981096058697 | validation: 0.6647046392420844]
	TIME [epoch: 7.97 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5526374999858703		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.5526374999858703 | validation: 0.6082636451257935]
	TIME [epoch: 8.01 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5642644107001562		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.5642644107001562 | validation: 0.7369735906243666]
	TIME [epoch: 7.97 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6323905772799656		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.6323905772799656 | validation: 0.7034300030453112]
	TIME [epoch: 7.97 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5691834870777699		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.5691834870777699 | validation: 0.6376947666159885]
	TIME [epoch: 7.97 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.538419700992365		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.538419700992365 | validation: 0.5878872116861992]
	TIME [epoch: 7.97 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5038193204846722		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.5038193204846722 | validation: 0.5391167367160564]
	TIME [epoch: 7.99 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4804037510480572		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.4804037510480572 | validation: 0.5076483735140792]
	TIME [epoch: 8.01 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49567698636545804		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.49567698636545804 | validation: 0.6313845324579302]
	TIME [epoch: 7.97 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6151974239622912		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.6151974239622912 | validation: 0.5832265229141367]
	TIME [epoch: 7.97 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5219273157827465		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.5219273157827465 | validation: 0.5763584476707226]
	TIME [epoch: 7.97 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.518496465830246		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.518496465830246 | validation: 0.528832306091876]
	TIME [epoch: 7.97 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5232409592876536		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.5232409592876536 | validation: 0.5892955014826738]
	TIME [epoch: 7.99 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48626134894477263		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.48626134894477263 | validation: 0.47337980379307665]
	TIME [epoch: 8 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44740290266024835		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.44740290266024835 | validation: 0.462337481314996]
	TIME [epoch: 7.97 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44042471739509576		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.44042471739509576 | validation: 0.4346976285630497]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45727378949259473		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.45727378949259473 | validation: 0.45770320217218496]
	TIME [epoch: 7.97 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.542373753381925		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.542373753381925 | validation: 0.6796031818865622]
	TIME [epoch: 7.97 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5274247553429798		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.5274247553429798 | validation: 0.453002645413031]
	TIME [epoch: 8 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5008130216170734		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.5008130216170734 | validation: 0.732961997689216]
	TIME [epoch: 7.99 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7678539777562102		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.7678539777562102 | validation: 0.6448821547250914]
	TIME [epoch: 7.97 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.60722860672894		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.60722860672894 | validation: 0.4976105238433119]
	TIME [epoch: 7.97 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5042425977114441		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.5042425977114441 | validation: 0.46294617503556457]
	TIME [epoch: 7.97 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47944008183733516		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.47944008183733516 | validation: 0.4500777844999937]
	TIME [epoch: 7.97 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4652119933028239		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.4652119933028239 | validation: 0.43802689341958573]
	TIME [epoch: 8 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4505872699447283		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.4505872699447283 | validation: 0.43810782281524496]
	TIME [epoch: 7.98 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4435723806920184		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.4435723806920184 | validation: 0.4333533262367354]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43776831187427073		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.43776831187427073 | validation: 0.4241337436213988]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43381129554319775		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.43381129554319775 | validation: 0.42978070447393035]
	TIME [epoch: 7.97 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45174095187148		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.45174095187148 | validation: 0.4351418519440679]
	TIME [epoch: 7.97 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43238989786508725		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.43238989786508725 | validation: 0.414088953510981]
	TIME [epoch: 8.02 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41856355641786974		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.41856355641786974 | validation: 0.4266840952359325]
	TIME [epoch: 7.98 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41442475440379056		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.41442475440379056 | validation: 0.40337781787247173]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4099724501226725		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.4099724501226725 | validation: 0.3850744298736516]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4034538008453187		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.4034538008453187 | validation: 0.3931250270258565]
	TIME [epoch: 7.99 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4059963696443874		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.4059963696443874 | validation: 0.39547235122388236]
	TIME [epoch: 8 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39379383875095925		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.39379383875095925 | validation: 0.37129884173193173]
	TIME [epoch: 8.02 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38195625279810547		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.38195625279810547 | validation: 0.419377910940972]
	TIME [epoch: 7.98 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39007044171335614		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.39007044171335614 | validation: 0.36829872698081834]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3857468082242602		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.3857468082242602 | validation: 0.35810838180747523]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37482052539463023		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.37482052539463023 | validation: 0.3763716831661885]
	TIME [epoch: 7.97 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3913295719131178		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.3913295719131178 | validation: 0.37811103544109687]
	TIME [epoch: 8.01 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.364862381353605		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.364862381353605 | validation: 0.3641292250687468]
	TIME [epoch: 7.99 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3502618636974305		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.3502618636974305 | validation: 0.3615517652297454]
	TIME [epoch: 7.97 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36851566716696693		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.36851566716696693 | validation: 0.34082200580556365]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36249762286657433		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.36249762286657433 | validation: 0.39635232615197336]
	TIME [epoch: 7.97 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36240041567350423		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.36240041567350423 | validation: 0.3744241069585127]
	TIME [epoch: 7.97 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36268606035838147		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.36268606035838147 | validation: 0.31256228728245683]
	TIME [epoch: 8.01 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35870906600305635		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.35870906600305635 | validation: 0.38363668773359494]
	TIME [epoch: 7.98 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37491778851612234		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.37491778851612234 | validation: 0.3684189778491135]
	TIME [epoch: 7.95 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3620123399215152		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.3620123399215152 | validation: 0.3118606415506683]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33451132192976507		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.33451132192976507 | validation: 0.36751604053592557]
	TIME [epoch: 7.97 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3836062508781866		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.3836062508781866 | validation: 0.3374110141862312]
	TIME [epoch: 7.97 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34068221719173286		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.34068221719173286 | validation: 0.3542824526326265]
	TIME [epoch: 8 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35269415769732415		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.35269415769732415 | validation: 0.32065569609929107]
	TIME [epoch: 7.97 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3191355152870085		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.3191355152870085 | validation: 0.3516726396313884]
	TIME [epoch: 7.96 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3260361248750682		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.3260361248750682 | validation: 0.31744706470580436]
	TIME [epoch: 7.97 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.321449486929787		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.321449486929787 | validation: 0.32569080529919836]
	TIME [epoch: 7.97 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3125425035034063		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.3125425035034063 | validation: 0.2806147525878836]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33267567766248257		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.33267567766248257 | validation: 0.5064930676128655]
	TIME [epoch: 8.02 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37500008842909194		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.37500008842909194 | validation: 0.32507307006195985]
	TIME [epoch: 7.97 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3209620207672943		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.3209620207672943 | validation: 0.3517334214856822]
	TIME [epoch: 7.96 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30188004093047077		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.30188004093047077 | validation: 0.3655138253584516]
	TIME [epoch: 7.97 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3234645845677898		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.3234645845677898 | validation: 0.28266669511829834]
	TIME [epoch: 7.96 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2902721047965281		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.2902721047965281 | validation: 0.35859175795082754]
	TIME [epoch: 7.97 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.359758763978714		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.359758763978714 | validation: 0.2909242408163627]
	TIME [epoch: 7.99 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2941272393667623		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.2941272393667623 | validation: 0.38938370575614634]
	TIME [epoch: 7.96 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31444121783946655		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.31444121783946655 | validation: 0.325117229818242]
	TIME [epoch: 7.95 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3553181223223684		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.3553181223223684 | validation: 0.30020516405346576]
	TIME [epoch: 7.96 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3014923683751368		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.3014923683751368 | validation: 0.30551786788508856]
	TIME [epoch: 7.95 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2863088154286979		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.2863088154286979 | validation: 0.3369156220482229]
	TIME [epoch: 7.98 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3025300768042204		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.3025300768042204 | validation: 0.3630526327519572]
	TIME [epoch: 7.99 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3364776877751121		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.3364776877751121 | validation: 0.2909023340164819]
	TIME [epoch: 7.97 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2990797582247963		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.2990797582247963 | validation: 0.2817855919570078]
	TIME [epoch: 7.96 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27969441346342994		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.27969441346342994 | validation: 0.32619964246015243]
	TIME [epoch: 7.97 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31634472538442016		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.31634472538442016 | validation: 0.27082174393729064]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2854859444929043		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.2854859444929043 | validation: 0.4529656308857086]
	TIME [epoch: 8.02 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38786163273824026		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.38786163273824026 | validation: 0.36038220773022445]
	TIME [epoch: 7.98 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35662775969678756		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.35662775969678756 | validation: 0.3442076643688372]
	TIME [epoch: 7.95 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3233253305567525		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.3233253305567525 | validation: 0.3070152809883415]
	TIME [epoch: 7.97 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3394500057260088		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.3394500057260088 | validation: 0.3333046804861822]
	TIME [epoch: 7.95 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3274491509640256		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.3274491509640256 | validation: 0.36562353900277755]
	TIME [epoch: 7.95 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35453383868728844		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.35453383868728844 | validation: 0.34653115668215906]
	TIME [epoch: 7.99 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33651458828001624		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.33651458828001624 | validation: 0.31874077169736303]
	TIME [epoch: 7.98 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31040068654859243		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.31040068654859243 | validation: 0.37227033596141945]
	TIME [epoch: 7.97 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3254158604984367		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.3254158604984367 | validation: 0.30539373479614695]
	TIME [epoch: 7.95 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30655771797206166		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.30655771797206166 | validation: 0.35773190751173123]
	TIME [epoch: 7.95 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31444437421489035		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.31444437421489035 | validation: 0.31295436077565664]
	TIME [epoch: 7.95 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29825154380625796		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.29825154380625796 | validation: 0.2904754900326649]
	TIME [epoch: 8.01 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35735389229656095		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.35735389229656095 | validation: 0.2901258526054586]
	TIME [epoch: 7.96 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3074831116030304		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.3074831116030304 | validation: 0.2958359026486778]
	TIME [epoch: 7.96 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27957182540430076		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.27957182540430076 | validation: 0.28370517771777326]
	TIME [epoch: 108 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3057319318337207		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.3057319318337207 | validation: 0.27650319019878133]
	TIME [epoch: 15.8 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30059123584380326		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.30059123584380326 | validation: 0.3142006558871764]
	TIME [epoch: 15.7 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29315447441980186		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.29315447441980186 | validation: 0.36588552702033217]
	TIME [epoch: 15.8 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27949436852814336		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.27949436852814336 | validation: 0.30982302171903986]
	TIME [epoch: 15.8 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2684251304641929		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.2684251304641929 | validation: 0.33510482057939583]
	TIME [epoch: 15.7 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.284042263889009		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.284042263889009 | validation: 0.3050700965905592]
	TIME [epoch: 15.7 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2633786435596262		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.2633786435596262 | validation: 0.32780769971780116]
	TIME [epoch: 15.8 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26195541767935465		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.26195541767935465 | validation: 0.3109700112719078]
	TIME [epoch: 15.7 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2737567885040364		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.2737567885040364 | validation: 0.30211830649021604]
	TIME [epoch: 15.7 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2629975232212914		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.2629975232212914 | validation: 0.3626495733777443]
	TIME [epoch: 15.8 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.275052630441904		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.275052630441904 | validation: 0.23144370511191287]
	TIME [epoch: 15.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.257469051761512		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.257469051761512 | validation: 0.2512336073260822]
	TIME [epoch: 15.7 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23545711515473922		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.23545711515473922 | validation: 0.38296194791962934]
	TIME [epoch: 15.8 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28813476432249313		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.28813476432249313 | validation: 0.26987541830979755]
	TIME [epoch: 15.7 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2535639450251664		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.2535639450251664 | validation: 0.2729276065930001]
	TIME [epoch: 15.7 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25433552631366374		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.25433552631366374 | validation: 0.35028938691050093]
	TIME [epoch: 15.8 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3200150191487826		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.3200150191487826 | validation: 0.40019772879161897]
	TIME [epoch: 15.8 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31046944799920106		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.31046944799920106 | validation: 0.253798676447426]
	TIME [epoch: 15.8 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2529220545955835		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.2529220545955835 | validation: 0.2734776676072963]
	TIME [epoch: 15.8 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24840728698530995		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.24840728698530995 | validation: 0.24861092018417857]
	TIME [epoch: 15.8 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26050159744365486		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.26050159744365486 | validation: 0.24241173177687575]
	TIME [epoch: 15.8 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24506398120342565		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.24506398120342565 | validation: 0.2855360799995679]
	TIME [epoch: 15.8 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25480183535383655		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.25480183535383655 | validation: 0.24798287377191558]
	TIME [epoch: 15.8 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2853261210380507		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.2853261210380507 | validation: 0.3173504451946747]
	TIME [epoch: 15.8 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2508619802512175		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.2508619802512175 | validation: 0.2729603875499114]
	TIME [epoch: 15.7 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24028167997969446		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.24028167997969446 | validation: 0.3127410365624389]
	TIME [epoch: 15.8 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24430180076486452		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.24430180076486452 | validation: 0.2591386538142139]
	TIME [epoch: 15.7 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2545061911738873		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.2545061911738873 | validation: 0.26476306028213603]
	TIME [epoch: 15.7 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2524267669560563		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.2524267669560563 | validation: 0.2433873827563311]
	TIME [epoch: 15.8 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2592037230910247		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.2592037230910247 | validation: 0.2307588181866519]
	TIME [epoch: 15.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25272214859237824		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.25272214859237824 | validation: 0.2557694138968219]
	TIME [epoch: 15.7 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23849746036773062		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.23849746036773062 | validation: 0.2797333705016649]
	TIME [epoch: 15.7 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25677808334255264		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.25677808334255264 | validation: 0.27940728894432854]
	TIME [epoch: 15.8 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25444207104615213		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.25444207104615213 | validation: 0.2213780161047293]
	TIME [epoch: 15.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2248603230830357		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.2248603230830357 | validation: 0.2627045354973852]
	TIME [epoch: 15.7 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23870786538404257		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.23870786538404257 | validation: 0.4119734148170857]
	TIME [epoch: 15.8 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.287439492353078		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.287439492353078 | validation: 0.32204009770165754]
	TIME [epoch: 15.7 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2797665159190318		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.2797665159190318 | validation: 0.2637628279600659]
	TIME [epoch: 15.7 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23429011120398846		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.23429011120398846 | validation: 0.2626119031759966]
	TIME [epoch: 15.8 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27570242062992745		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.27570242062992745 | validation: 0.30990582153576685]
	TIME [epoch: 15.7 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24505432017081302		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.24505432017081302 | validation: 0.23765762931546475]
	TIME [epoch: 15.7 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22564867416908352		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.22564867416908352 | validation: 0.2911015767284808]
	TIME [epoch: 15.8 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24813838656659032		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.24813838656659032 | validation: 0.23646770024903968]
	TIME [epoch: 15.7 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23292815739779524		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.23292815739779524 | validation: 0.2534487846452751]
	TIME [epoch: 15.7 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23487710311294102		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.23487710311294102 | validation: 0.2506748181090322]
	TIME [epoch: 15.8 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21447772601619824		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.21447772601619824 | validation: 0.3156098493410203]
	TIME [epoch: 15.8 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25067271670532143		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.25067271670532143 | validation: 0.24093220441731922]
	TIME [epoch: 15.7 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23653833626003484		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.23653833626003484 | validation: 0.2914569640676776]
	TIME [epoch: 15.7 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24230087202009193		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.24230087202009193 | validation: 0.24460367825539092]
	TIME [epoch: 15.8 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20805424154658192		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.20805424154658192 | validation: 0.2792921359717908]
	TIME [epoch: 15.7 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23130560174425457		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.23130560174425457 | validation: 0.25082342571435573]
	TIME [epoch: 15.7 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2143317047001616		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.2143317047001616 | validation: 0.21479801892617756]
	TIME [epoch: 15.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23460521380818827		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.23460521380818827 | validation: 0.2315312755127689]
	TIME [epoch: 15.7 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23176432612057896		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.23176432612057896 | validation: 0.2064488047369849]
	TIME [epoch: 15.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22642643369807963		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.22642643369807963 | validation: 0.2378836460111997]
	TIME [epoch: 15.8 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2274417590818244		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.2274417590818244 | validation: 0.2570364992121701]
	TIME [epoch: 15.7 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2125920776763936		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.2125920776763936 | validation: 0.24072312453860345]
	TIME [epoch: 15.7 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22186279399984765		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.22186279399984765 | validation: 0.29381968784623913]
	TIME [epoch: 15.8 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25344164638518923		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.25344164638518923 | validation: 0.2512669374554408]
	TIME [epoch: 15.7 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21206113253242853		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.21206113253242853 | validation: 0.2565112065821204]
	TIME [epoch: 15.7 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22110890792436125		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.22110890792436125 | validation: 0.24789299276001628]
	TIME [epoch: 15.8 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23416664011839458		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.23416664011839458 | validation: 0.25755158584980953]
	TIME [epoch: 15.8 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2264994947195983		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.2264994947195983 | validation: 0.23306584973067487]
	TIME [epoch: 15.7 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2251385276162996		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.2251385276162996 | validation: 0.2316118724547504]
	TIME [epoch: 15.8 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20583436199002964		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.20583436199002964 | validation: 0.24284833565762975]
	TIME [epoch: 15.8 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23585646525533258		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.23585646525533258 | validation: 0.2405732852244059]
	TIME [epoch: 15.7 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20340544632464674		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.20340544632464674 | validation: 0.2438278282375576]
	TIME [epoch: 15.8 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24152045007898587		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.24152045007898587 | validation: 0.2362586660689323]
	TIME [epoch: 15.8 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22130280420994244		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.22130280420994244 | validation: 0.19708319441957609]
	TIME [epoch: 15.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20645859986331747		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.20645859986331747 | validation: 0.29893744671591704]
	TIME [epoch: 15.8 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21744458860630447		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.21744458860630447 | validation: 0.2812693156309338]
	TIME [epoch: 15.8 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2177483203334338		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.2177483203334338 | validation: 0.2869878708818633]
	TIME [epoch: 15.7 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21869912676465283		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.21869912676465283 | validation: 0.20753874221986018]
	TIME [epoch: 15.7 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20817912058179203		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.20817912058179203 | validation: 0.23128299515119555]
	TIME [epoch: 15.8 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22365850148479666		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.22365850148479666 | validation: 0.24476268133237183]
	TIME [epoch: 15.7 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21181947867281878		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.21181947867281878 | validation: 0.26157168244533346]
	TIME [epoch: 15.7 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2176781673424213		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.2176781673424213 | validation: 0.28388047367235997]
	TIME [epoch: 15.8 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20129236585628485		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.20129236585628485 | validation: 0.20632556811504105]
	TIME [epoch: 15.7 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21255783926120875		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.21255783926120875 | validation: 0.43615697083916644]
	TIME [epoch: 15.7 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.247846060688224		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.247846060688224 | validation: 0.2145278366475726]
	TIME [epoch: 15.8 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19071973529644376		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.19071973529644376 | validation: 0.23554053098083805]
	TIME [epoch: 15.7 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20661314812151443		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.20661314812151443 | validation: 0.23681109161742422]
	TIME [epoch: 15.7 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20424013316411155		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.20424013316411155 | validation: 0.22287829380326743]
	TIME [epoch: 15.8 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2134707097833803		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.2134707097833803 | validation: 0.2502012990388964]
	TIME [epoch: 15.7 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2405600805229		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.2405600805229 | validation: 0.20857140695418666]
	TIME [epoch: 15.7 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21207189380863758		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.21207189380863758 | validation: 0.20526212455464637]
	TIME [epoch: 15.8 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20362876326272566		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.20362876326272566 | validation: 0.24213338034464632]
	TIME [epoch: 15.7 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19626239925725564		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.19626239925725564 | validation: 0.23834356764956793]
	TIME [epoch: 15.7 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21008275476365745		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.21008275476365745 | validation: 0.2185521085606285]
	TIME [epoch: 15.8 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19605301998298555		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.19605301998298555 | validation: 0.26308175487601976]
	TIME [epoch: 15.7 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24407703901710898		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.24407703901710898 | validation: 0.26678237630458457]
	TIME [epoch: 15.7 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2294123999697204		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.2294123999697204 | validation: 0.24669302504892837]
	TIME [epoch: 15.8 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20447855504242635		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.20447855504242635 | validation: 0.23261115971601395]
	TIME [epoch: 15.7 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19459339400340045		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.19459339400340045 | validation: 0.20314420698699787]
	TIME [epoch: 15.7 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19381015648661915		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.19381015648661915 | validation: 0.19367941064341027]
	TIME [epoch: 15.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2073735300164547		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.2073735300164547 | validation: 0.21420707621173035]
	TIME [epoch: 15.7 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20579584177682386		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.20579584177682386 | validation: 0.23465150516256356]
	TIME [epoch: 15.7 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19658356422011747		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.19658356422011747 | validation: 0.1879782183393462]
	TIME [epoch: 15.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20989218016520317		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.20989218016520317 | validation: 0.34610385041499325]
	TIME [epoch: 15.7 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21805673489864902		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.21805673489864902 | validation: 0.29252985240567214]
	TIME [epoch: 15.7 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20598830684193842		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.20598830684193842 | validation: 0.23559055166437354]
	TIME [epoch: 15.8 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2030039710913556		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.2030039710913556 | validation: 0.20164700766462773]
	TIME [epoch: 15.7 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1973533886214581		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.1973533886214581 | validation: 0.2705111689333197]
	TIME [epoch: 15.7 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19360690744753983		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.19360690744753983 | validation: 0.20421189451506827]
	TIME [epoch: 15.8 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18797312767897256		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.18797312767897256 | validation: 0.21858689880548537]
	TIME [epoch: 15.7 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20061295285029296		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.20061295285029296 | validation: 0.2847028311036123]
	TIME [epoch: 15.7 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22553445427933164		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.22553445427933164 | validation: 0.25173419241969724]
	TIME [epoch: 15.8 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19854855525186896		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.19854855525186896 | validation: 0.1974070897294247]
	TIME [epoch: 15.7 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19850949603512003		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.19850949603512003 | validation: 0.20022739208251256]
	TIME [epoch: 15.7 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18246229719800275		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.18246229719800275 | validation: 0.20449385315347118]
	TIME [epoch: 15.8 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22073532441495503		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.22073532441495503 | validation: 0.20678854143004743]
	TIME [epoch: 15.8 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20609015306093792		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.20609015306093792 | validation: 0.20280125219496886]
	TIME [epoch: 15.7 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18228965647973383		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.18228965647973383 | validation: 0.20778734726227188]
	TIME [epoch: 15.8 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19475163106766744		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.19475163106766744 | validation: 0.23875497309611143]
	TIME [epoch: 15.8 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21103328145313577		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.21103328145313577 | validation: 0.2079555650853535]
	TIME [epoch: 15.8 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1845632471542742		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.1845632471542742 | validation: 0.2098212812113625]
	TIME [epoch: 15.8 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18161897728130255		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.18161897728130255 | validation: 0.18381463960928082]
	TIME [epoch: 15.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2003631384180659		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.2003631384180659 | validation: 0.34346450814872503]
	TIME [epoch: 15.7 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20556130796112138		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.20556130796112138 | validation: 0.21666700116061902]
	TIME [epoch: 15.8 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19916223675584477		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.19916223675584477 | validation: 0.21286594888636626]
	TIME [epoch: 15.7 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18384426398690526		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.18384426398690526 | validation: 0.21746371748398413]
	TIME [epoch: 15.7 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1903948581502545		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.1903948581502545 | validation: 0.2039427831941416]
	TIME [epoch: 15.8 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20257362398983164		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.20257362398983164 | validation: 0.19250598564096508]
	TIME [epoch: 15.8 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18021391361775588		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.18021391361775588 | validation: 0.1971253690393681]
	TIME [epoch: 15.7 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18910798786409483		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.18910798786409483 | validation: 0.20496196179175202]
	TIME [epoch: 15.8 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19154483226262092		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.19154483226262092 | validation: 0.25641842765324924]
	TIME [epoch: 15.8 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18109251758629874		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.18109251758629874 | validation: 0.24690925340711067]
	TIME [epoch: 15.7 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2022787006891743		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.2022787006891743 | validation: 0.18999548026935237]
	TIME [epoch: 15.7 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18025539277078453		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.18025539277078453 | validation: 0.22306446719287465]
	TIME [epoch: 15.8 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1912138913514961		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.1912138913514961 | validation: 0.184456386837483]
	TIME [epoch: 15.7 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18193166480316658		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.18193166480316658 | validation: 0.2077757094292837]
	TIME [epoch: 15.7 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1835193822459355		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.1835193822459355 | validation: 0.2826612394354098]
	TIME [epoch: 15.8 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18680158984867698		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.18680158984867698 | validation: 0.19823333760822773]
	TIME [epoch: 15.7 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18842513436171485		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.18842513436171485 | validation: 0.2256523959373018]
	TIME [epoch: 15.7 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19090906530196855		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.19090906530196855 | validation: 0.19331028813649415]
	TIME [epoch: 15.8 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1753029440408024		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.1753029440408024 | validation: 0.23359397594896286]
	TIME [epoch: 15.7 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18049525631441263		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.18049525631441263 | validation: 0.21579827818209202]
	TIME [epoch: 15.7 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17551557830482145		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.17551557830482145 | validation: 0.20763357141597277]
	TIME [epoch: 15.8 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18117404246384575		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.18117404246384575 | validation: 0.24682370454466962]
	TIME [epoch: 15.7 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19825426427967763		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.19825426427967763 | validation: 0.18788845676170568]
	TIME [epoch: 15.7 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18255776757215786		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.18255776757215786 | validation: 0.1661495096943275]
	TIME [epoch: 15.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16863344425017757		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.16863344425017757 | validation: 0.22541444470732835]
	TIME [epoch: 15.7 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21490002775602066		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.21490002775602066 | validation: 0.19919588289647783]
	TIME [epoch: 15.7 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17352374116280872		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.17352374116280872 | validation: 0.18494974917535448]
	TIME [epoch: 15.8 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17233042803350865		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.17233042803350865 | validation: 0.21648518583449405]
	TIME [epoch: 15.7 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18163661976118864		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.18163661976118864 | validation: 0.23544838356190134]
	TIME [epoch: 15.8 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18308083917524678		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.18308083917524678 | validation: 0.20519035950175182]
	TIME [epoch: 15.8 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1843557522387474		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.1843557522387474 | validation: 0.19510689233039408]
	TIME [epoch: 15.8 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17480779963507748		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.17480779963507748 | validation: 0.173966288911865]
	TIME [epoch: 15.7 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1796181870535256		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.1796181870535256 | validation: 0.18955828899245714]
	TIME [epoch: 15.8 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16630244666896005		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.16630244666896005 | validation: 0.19866630138299618]
	TIME [epoch: 15.7 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1846073703229117		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.1846073703229117 | validation: 0.17006233938018117]
	TIME [epoch: 15.7 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21274125542070374		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.21274125542070374 | validation: 0.2275018654217088]
	TIME [epoch: 15.8 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18749740672444648		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.18749740672444648 | validation: 0.1922311791548835]
	TIME [epoch: 15.8 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17349411890045696		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.17349411890045696 | validation: 0.1884844016946073]
	TIME [epoch: 15.7 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1817476546448211		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.1817476546448211 | validation: 0.21751531994630968]
	TIME [epoch: 15.8 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18509000733458075		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.18509000733458075 | validation: 0.17738882697075153]
	TIME [epoch: 15.7 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17344031595845505		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.17344031595845505 | validation: 0.19539834226220698]
	TIME [epoch: 15.8 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17672777122740524		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.17672777122740524 | validation: 0.19976838786896778]
	TIME [epoch: 15.8 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18154766794164962		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.18154766794164962 | validation: 0.17118598723415576]
	TIME [epoch: 15.8 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1661375073387718		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.1661375073387718 | validation: 0.17834106076005424]
	TIME [epoch: 15.7 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17626915275570804		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.17626915275570804 | validation: 0.17383397791364133]
	TIME [epoch: 15.8 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17697438584138467		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.17697438584138467 | validation: 0.2038119155289549]
	TIME [epoch: 15.8 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16823535294067146		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.16823535294067146 | validation: 0.17132179427548117]
	TIME [epoch: 15.7 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17464396804624116		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.17464396804624116 | validation: 0.17229503036250926]
	TIME [epoch: 15.8 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17271398229383525		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.17271398229383525 | validation: 0.251003824898069]
	TIME [epoch: 15.8 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20998020240736828		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.20998020240736828 | validation: 0.19071991050729267]
	TIME [epoch: 15.7 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.175477840026731		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.175477840026731 | validation: 0.176167748686348]
	TIME [epoch: 15.8 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17043596150544404		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.17043596150544404 | validation: 0.17245912354046847]
	TIME [epoch: 15.8 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16580351492134934		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.16580351492134934 | validation: 0.18647497691992987]
	TIME [epoch: 15.7 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1651439878515155		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.1651439878515155 | validation: 0.1779898590996597]
	TIME [epoch: 15.7 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16720496913039884		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.16720496913039884 | validation: 0.17275611379494715]
	TIME [epoch: 15.8 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1939939957571631		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.1939939957571631 | validation: 0.17660142189509773]
	TIME [epoch: 15.7 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17095186265502188		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.17095186265502188 | validation: 0.18240759606223247]
	TIME [epoch: 15.8 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17404924214948675		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.17404924214948675 | validation: 0.22196828157441612]
	TIME [epoch: 15.8 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19218688925704666		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.19218688925704666 | validation: 0.1726834277276167]
	TIME [epoch: 15.7 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16840945318266415		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.16840945318266415 | validation: 0.1687621676955154]
	TIME [epoch: 15.7 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16974954434607364		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.16974954434607364 | validation: 0.19254202407961904]
	TIME [epoch: 15.8 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1707083984409528		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.1707083984409528 | validation: 0.18858842293902148]
	TIME [epoch: 15.7 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16645545546941673		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.16645545546941673 | validation: 0.18478669800272673]
	TIME [epoch: 15.7 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1817741992268925		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.1817741992268925 | validation: 0.20240528039119735]
	TIME [epoch: 15.8 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18050940199986443		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.18050940199986443 | validation: 0.16418997575847377]
	TIME [epoch: 15.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1605684083174556		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.1605684083174556 | validation: 0.1744082556116373]
	TIME [epoch: 15.7 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16161704031863192		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.16161704031863192 | validation: 0.18710666482775107]
	TIME [epoch: 15.8 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19159958072600677		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.19159958072600677 | validation: 0.18422027317157103]
	TIME [epoch: 15.7 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16490661809185714		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.16490661809185714 | validation: 0.20551559328300095]
	TIME [epoch: 15.7 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15874504825146654		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.15874504825146654 | validation: 0.2305413096476546]
	TIME [epoch: 15.7 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17805347409495276		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.17805347409495276 | validation: 0.17440485400364505]
	TIME [epoch: 15.7 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1637981003026615		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.1637981003026615 | validation: 0.21387197310916753]
	TIME [epoch: 15.7 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1780363859035188		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.1780363859035188 | validation: 0.19613829759545037]
	TIME [epoch: 15.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17934390157061975		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.17934390157061975 | validation: 0.18713363406490957]
	TIME [epoch: 15.7 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17250672836145367		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.17250672836145367 | validation: 0.17791244006266205]
	TIME [epoch: 15.7 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16329421600400687		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.16329421600400687 | validation: 0.16531367326295107]
	TIME [epoch: 15.8 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16003309697610246		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.16003309697610246 | validation: 0.1833523301583936]
	TIME [epoch: 15.7 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1650245911176734		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.1650245911176734 | validation: 0.16549543419222495]
	TIME [epoch: 15.7 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16285663370868475		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.16285663370868475 | validation: 0.16135132271289646]
	TIME [epoch: 15.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16097990405525572		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.16097990405525572 | validation: 0.19886928323621547]
	TIME [epoch: 15.7 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1916994240453706		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.1916994240453706 | validation: 0.18129478816802475]
	TIME [epoch: 15.7 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15827733552400525		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.15827733552400525 | validation: 0.1730182068294614]
	TIME [epoch: 15.8 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15014105016383494		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.15014105016383494 | validation: 0.1893024322951197]
	TIME [epoch: 15.7 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15985070258853104		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.15985070258853104 | validation: 0.1802014137776105]
	TIME [epoch: 15.7 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15704267464526783		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.15704267464526783 | validation: 0.1797478002011284]
	TIME [epoch: 15.8 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15919940318145015		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.15919940318145015 | validation: 0.1847059580155846]
	TIME [epoch: 15.7 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18576744934825815		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.18576744934825815 | validation: 0.1886635946036755]
	TIME [epoch: 15.7 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1696075415936599		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.1696075415936599 | validation: 0.16802582046070336]
	TIME [epoch: 15.8 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15826859640446841		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.15826859640446841 | validation: 0.1677781582220625]
	TIME [epoch: 15.7 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1512647763249424		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.1512647763249424 | validation: 0.1937479228314197]
	TIME [epoch: 15.7 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1804303500587104		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.1804303500587104 | validation: 0.1559382325465537]
	TIME [epoch: 15.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16291372907548146		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.16291372907548146 | validation: 0.16406987698144793]
	TIME [epoch: 15.7 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14879887498123234		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.14879887498123234 | validation: 0.15265291299782913]
	TIME [epoch: 15.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15511329264036183		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.15511329264036183 | validation: 0.15688749186500073]
	TIME [epoch: 15.8 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15535529151067987		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.15535529151067987 | validation: 0.1973150300536885]
	TIME [epoch: 15.7 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1645427225284446		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.1645427225284446 | validation: 0.21121009516095984]
	TIME [epoch: 15.7 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16188376280402236		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.16188376280402236 | validation: 0.18662846707206904]
	TIME [epoch: 15.8 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16550671193340727		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.16550671193340727 | validation: 0.17359665511995725]
	TIME [epoch: 15.7 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16023692005114598		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.16023692005114598 | validation: 0.16332218092306477]
	TIME [epoch: 15.7 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1626895341085517		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.1626895341085517 | validation: 0.1475152282625489]
	TIME [epoch: 15.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15761568723042182		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.15761568723042182 | validation: 0.18784457154971707]
	TIME [epoch: 15.8 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16388031322216293		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.16388031322216293 | validation: 0.19462947024990285]
	TIME [epoch: 15.8 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18598948447785102		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.18598948447785102 | validation: 0.15638993799511428]
	TIME [epoch: 15.8 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15204758412117111		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.15204758412117111 | validation: 0.17246494419924618]
	TIME [epoch: 15.8 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15935973323438318		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.15935973323438318 | validation: 0.1547611913753707]
	TIME [epoch: 15.8 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1552706346371166		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.1552706346371166 | validation: 0.1492443966705254]
	TIME [epoch: 15.8 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15093684967857418		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.15093684967857418 | validation: 0.19200429931934587]
	TIME [epoch: 15.8 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15275930874852842		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.15275930874852842 | validation: 0.16917628991690695]
	TIME [epoch: 15.8 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17102881382560714		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.17102881382560714 | validation: 0.16595379696330725]
	TIME [epoch: 15.8 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1600858627417066		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.1600858627417066 | validation: 0.1583620761887799]
	TIME [epoch: 15.8 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.155922365339292		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.155922365339292 | validation: 0.14295435486587788]
	TIME [epoch: 15.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15123481242638628		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.15123481242638628 | validation: 0.1651285895364883]
	TIME [epoch: 15.8 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16721570120680646		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.16721570120680646 | validation: 0.15609795132041196]
	TIME [epoch: 15.8 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15634654734059653		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.15634654734059653 | validation: 0.15858353753182766]
	TIME [epoch: 15.8 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1588801092259404		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.1588801092259404 | validation: 0.14392205491943388]
	TIME [epoch: 15.8 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1477262667594903		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.1477262667594903 | validation: 0.15732739968491238]
	TIME [epoch: 15.8 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1492049612083532		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.1492049612083532 | validation: 0.20214093392061783]
	TIME [epoch: 15.8 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16822593243724837		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.16822593243724837 | validation: 0.16117620272127842]
	TIME [epoch: 15.8 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1529850917075592		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.1529850917075592 | validation: 0.16850763738972155]
	TIME [epoch: 15.8 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1581673755694091		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.1581673755694091 | validation: 0.1582642601342712]
	TIME [epoch: 15.8 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16155617482417775		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.16155617482417775 | validation: 0.17153730074588677]
	TIME [epoch: 15.8 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16080162244076784		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.16080162244076784 | validation: 0.1608634730062576]
	TIME [epoch: 15.8 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1654019073780564		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.1654019073780564 | validation: 0.17182338022110338]
	TIME [epoch: 15.8 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15357659771880078		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.15357659771880078 | validation: 0.1449392919890905]
	TIME [epoch: 15.8 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15253865278224074		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.15253865278224074 | validation: 0.16313699383814778]
	TIME [epoch: 15.8 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15669253797218194		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.15669253797218194 | validation: 0.19065026349796788]
	TIME [epoch: 15.7 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16236887232430403		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.16236887232430403 | validation: 0.16432945516825154]
	TIME [epoch: 15.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16035721066833605		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.16035721066833605 | validation: 0.1788693394971901]
	TIME [epoch: 15.8 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1514159991255052		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.1514159991255052 | validation: 0.16191438934913388]
	TIME [epoch: 15.8 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1541403371674846		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.1541403371674846 | validation: 0.14672005637774407]
	TIME [epoch: 15.8 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14695542668748732		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.14695542668748732 | validation: 0.19297515735979787]
	TIME [epoch: 15.8 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15737970920015454		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.15737970920015454 | validation: 0.1708162492930302]
	TIME [epoch: 15.8 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15061994836140044		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.15061994836140044 | validation: 0.15849478299376218]
	TIME [epoch: 15.8 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14937361096158933		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.14937361096158933 | validation: 0.16391600118050917]
	TIME [epoch: 15.8 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15800907415511994		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.15800907415511994 | validation: 0.15222695504549055]
	TIME [epoch: 15.8 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15143063238499727		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.15143063238499727 | validation: 0.15271146501153546]
	TIME [epoch: 15.8 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14560244861199062		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.14560244861199062 | validation: 0.14106777353059657]
	TIME [epoch: 15.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13945935997370587		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.13945935997370587 | validation: 0.17144013402342606]
	TIME [epoch: 15.8 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15326978736766045		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.15326978736766045 | validation: 0.17220145210365592]
	TIME [epoch: 15.7 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1472471008319704		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.1472471008319704 | validation: 0.18665208150224838]
	TIME [epoch: 15.8 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1829934133209638		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.1829934133209638 | validation: 0.15665091018852795]
	TIME [epoch: 15.8 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14554155693065174		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.14554155693065174 | validation: 0.19045445824044138]
	TIME [epoch: 15.7 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14884619963017706		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.14884619963017706 | validation: 0.14876089284271096]
	TIME [epoch: 15.8 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14291337875583865		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.14291337875583865 | validation: 0.14971268369426416]
	TIME [epoch: 15.8 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1475841629946379		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.1475841629946379 | validation: 0.15371755837142492]
	TIME [epoch: 15.7 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14541499088305954		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.14541499088305954 | validation: 0.17664636653040355]
	TIME [epoch: 15.8 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16751928162898322		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.16751928162898322 | validation: 0.16929541719272612]
	TIME [epoch: 15.8 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1516946927296022		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.1516946927296022 | validation: 0.15082265001631281]
	TIME [epoch: 15.7 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16635459374353065		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.16635459374353065 | validation: 0.18920202614444154]
	TIME [epoch: 15.8 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1512841646181175		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.1512841646181175 | validation: 0.17094396783051924]
	TIME [epoch: 15.8 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1571356237785742		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.1571356237785742 | validation: 0.16190922304159494]
	TIME [epoch: 15.7 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15591384105113418		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.15591384105113418 | validation: 0.14614828739762203]
	TIME [epoch: 15.8 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13658984493181348		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.13658984493181348 | validation: 0.15536952638220683]
	TIME [epoch: 15.8 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14511683743438314		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.14511683743438314 | validation: 0.17540030201008566]
	TIME [epoch: 15.7 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1612200136651771		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.1612200136651771 | validation: 0.16373671467483597]
	TIME [epoch: 15.8 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1562648631755737		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.1562648631755737 | validation: 0.1513139904071754]
	TIME [epoch: 15.8 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1511431213054024		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.1511431213054024 | validation: 0.15925499834044748]
	TIME [epoch: 15.7 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1459445709592946		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.1459445709592946 | validation: 0.16432278842705428]
	TIME [epoch: 15.8 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14333053532189294		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.14333053532189294 | validation: 0.14253958850519377]
	TIME [epoch: 15.8 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1461888290142807		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.1461888290142807 | validation: 0.1670739889882909]
	TIME [epoch: 15.8 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14673825057282566		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.14673825057282566 | validation: 0.14400866275013]
	TIME [epoch: 15.8 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14488328896885833		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.14488328896885833 | validation: 0.16337687425974773]
	TIME [epoch: 15.8 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15657983725595334		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.15657983725595334 | validation: 0.16424112018150833]
	TIME [epoch: 15.8 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1581784907480152		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.1581784907480152 | validation: 0.13924044516182302]
	TIME [epoch: 15.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1394595609038349		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.1394595609038349 | validation: 0.14547252410763678]
	TIME [epoch: 15.8 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13735264792169444		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.13735264792169444 | validation: 0.14492575468383706]
	TIME [epoch: 15.7 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1360840482772647		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.1360840482772647 | validation: 0.15068561899880917]
	TIME [epoch: 15.8 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16278067698641419		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.16278067698641419 | validation: 0.1652179681323966]
	TIME [epoch: 15.8 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13936995556971765		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.13936995556971765 | validation: 0.14719805640123296]
	TIME [epoch: 15.8 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1525788845974992		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.1525788845974992 | validation: 0.15967302182933008]
	TIME [epoch: 15.8 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14051189689024532		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.14051189689024532 | validation: 0.15316001513964875]
	TIME [epoch: 15.8 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14248160120786385		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.14248160120786385 | validation: 0.17605322489502112]
	TIME [epoch: 15.8 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1498489002104999		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.1498489002104999 | validation: 0.15096096108183243]
	TIME [epoch: 15.8 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14476456672072482		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.14476456672072482 | validation: 0.13953718583401753]
	TIME [epoch: 15.8 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14242940684749275		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.14242940684749275 | validation: 0.13555929246663279]
	TIME [epoch: 15.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1421407366681021		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.1421407366681021 | validation: 0.14118510686170238]
	TIME [epoch: 15.8 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13903398229965525		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.13903398229965525 | validation: 0.15887215994092832]
	TIME [epoch: 15.8 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14272075316403357		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.14272075316403357 | validation: 0.13428430991109352]
	TIME [epoch: 15.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_496.pth
	Model improved!!!
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13702657543806687		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.13702657543806687 | validation: 0.13782923069961256]
	TIME [epoch: 15.8 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16476323596420028		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.16476323596420028 | validation: 0.1740249845705899]
	TIME [epoch: 15.8 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14494056566284177		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.14494056566284177 | validation: 0.14527383624537177]
	TIME [epoch: 15.7 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14101973253954248		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.14101973253954248 | validation: 0.1457937471674033]
	TIME [epoch: 15.7 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14023216389804843		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.14023216389804843 | validation: 0.1424440663809333]
	TIME [epoch: 126 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13887842741757903		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.13887842741757903 | validation: 0.13108028343503317]
	TIME [epoch: 34.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1471919346361091		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.1471919346361091 | validation: 0.1491409590002309]
	TIME [epoch: 34.2 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14493127386292096		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.14493127386292096 | validation: 0.14581387769510978]
	TIME [epoch: 34.1 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1433985901049837		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.1433985901049837 | validation: 0.1647085588812011]
	TIME [epoch: 34.2 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14057656322038042		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.14057656322038042 | validation: 0.15210931199768668]
	TIME [epoch: 34.2 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13474290728197502		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.13474290728197502 | validation: 0.1424994675259311]
	TIME [epoch: 34.1 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14372698621304347		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.14372698621304347 | validation: 0.15966910717907407]
	TIME [epoch: 34.1 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1508930993436272		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.1508930993436272 | validation: 0.13734283167158204]
	TIME [epoch: 34.2 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13717616248329859		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.13717616248329859 | validation: 0.1522903024277352]
	TIME [epoch: 34.2 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15772779725951597		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.15772779725951597 | validation: 0.13664554074423624]
	TIME [epoch: 34.1 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13204873375072027		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.13204873375072027 | validation: 0.15504908153183017]
	TIME [epoch: 34.2 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1408391746967994		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.1408391746967994 | validation: 0.14744805264621408]
	TIME [epoch: 34.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14738908339937462		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.14738908339937462 | validation: 0.14671976340021597]
	TIME [epoch: 34.1 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14619702377451366		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.14619702377451366 | validation: 0.15524955393779333]
	TIME [epoch: 34.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13608773798988819		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.13608773798988819 | validation: 0.1528864451959836]
	TIME [epoch: 34.2 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13617337381289585		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.13617337381289585 | validation: 0.15551218791584526]
	TIME [epoch: 34.2 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15126811777032592		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.15126811777032592 | validation: 0.15375752798037873]
	TIME [epoch: 34.2 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14121001670257238		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.14121001670257238 | validation: 0.13999633153210433]
	TIME [epoch: 34.2 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13879142960755342		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.13879142960755342 | validation: 0.14464041458486537]
	TIME [epoch: 34.2 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14575328657855152		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.14575328657855152 | validation: 0.13527152976336246]
	TIME [epoch: 34.2 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13862334248382718		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.13862334248382718 | validation: 0.13876157218636817]
	TIME [epoch: 34.2 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13510913891336399		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.13510913891336399 | validation: 0.17462898639485147]
	TIME [epoch: 34.2 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14499150464274888		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.14499150464274888 | validation: 0.15211757512635216]
	TIME [epoch: 34.2 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13483633915045298		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.13483633915045298 | validation: 0.17074563423030784]
	TIME [epoch: 34.2 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14244542589604914		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.14244542589604914 | validation: 0.14622980139838299]
	TIME [epoch: 34.2 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13496383559020633		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.13496383559020633 | validation: 0.15026929661669747]
	TIME [epoch: 34.2 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12932495348653614		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.12932495348653614 | validation: 0.15655521915338066]
	TIME [epoch: 34.2 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1347842823212532		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.1347842823212532 | validation: 0.15364891568865344]
	TIME [epoch: 34.2 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16222465136763775		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.16222465136763775 | validation: 0.16128849438372228]
	TIME [epoch: 34.2 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14438477814040024		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.14438477814040024 | validation: 0.1332075239597605]
	TIME [epoch: 34.1 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1338386558501481		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.1338386558501481 | validation: 0.13757180711612094]
	TIME [epoch: 34.2 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13804669603445724		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.13804669603445724 | validation: 0.13438617120652951]
	TIME [epoch: 34.2 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13573068532816193		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.13573068532816193 | validation: 0.19017513757497628]
	TIME [epoch: 34.1 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14074923174791976		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.14074923174791976 | validation: 0.14109175334062518]
	TIME [epoch: 34.2 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14096848205489962		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.14096848205489962 | validation: 0.13790891102435654]
	TIME [epoch: 34.2 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12580739057487095		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.12580739057487095 | validation: 0.15089534439882324]
	TIME [epoch: 34.1 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13841420548175717		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.13841420548175717 | validation: 0.13988638678877852]
	TIME [epoch: 34.1 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13454830506650867		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.13454830506650867 | validation: 0.13795376813543447]
	TIME [epoch: 34.2 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13393618579877972		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.13393618579877972 | validation: 0.1750071274472349]
	TIME [epoch: 34.3 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15249842668416685		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.15249842668416685 | validation: 0.16144180486782073]
	TIME [epoch: 34.1 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14075763455295923		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.14075763455295923 | validation: 0.13222910802965507]
	TIME [epoch: 34.3 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13553504464392932		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.13553504464392932 | validation: 0.13536770056500558]
	TIME [epoch: 34.3 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1256288442830309		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.1256288442830309 | validation: 0.14053152980060457]
	TIME [epoch: 34.1 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13335353086122828		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.13335353086122828 | validation: 0.14781734502046295]
	TIME [epoch: 34.1 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1284368002972751		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.1284368002972751 | validation: 0.16319439281961703]
	TIME [epoch: 34.2 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13447814211628756		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.13447814211628756 | validation: 0.14323983995679762]
	TIME [epoch: 34.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14447334333668993		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.14447334333668993 | validation: 0.1762624822155439]
	TIME [epoch: 34.1 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13883491145412716		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.13883491145412716 | validation: 0.13139236023198428]
	TIME [epoch: 34.2 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13361343625789682		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.13361343625789682 | validation: 0.1316356908960468]
	TIME [epoch: 34.2 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12856104756299278		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.12856104756299278 | validation: 0.17039950485759747]
	TIME [epoch: 34.1 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14285622382893984		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.14285622382893984 | validation: 0.15352184189844414]
	TIME [epoch: 34.2 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1422706314959108		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.1422706314959108 | validation: 0.12935066174265908]
	TIME [epoch: 34.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1288169727492978		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.1288169727492978 | validation: 0.13601853656803703]
	TIME [epoch: 34.1 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13248833840834207		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.13248833840834207 | validation: 0.1406448271471799]
	TIME [epoch: 34.1 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13065039844813028		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.13065039844813028 | validation: 0.13503271361477914]
	TIME [epoch: 34.2 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1404328330280567		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.1404328330280567 | validation: 0.1330281242130265]
	TIME [epoch: 34.1 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12954197702981382		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.12954197702981382 | validation: 0.136056005300867]
	TIME [epoch: 34.1 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13427749599512584		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.13427749599512584 | validation: 0.1406735520566213]
	TIME [epoch: 34.2 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12997955980254944		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.12997955980254944 | validation: 0.1443194130299986]
	TIME [epoch: 34.2 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12917113800107316		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.12917113800107316 | validation: 0.15262381275515435]
	TIME [epoch: 34.1 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13582841403066293		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.13582841403066293 | validation: 0.15166943608067368]
	TIME [epoch: 34.2 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13420814637498174		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.13420814637498174 | validation: 0.14348303399749263]
	TIME [epoch: 34.2 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12680893612367364		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.12680893612367364 | validation: 0.14733581263061593]
	TIME [epoch: 34.1 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12764555198661795		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.12764555198661795 | validation: 0.15757140544013748]
	TIME [epoch: 34.1 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13861490644024219		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.13861490644024219 | validation: 0.13326784827087848]
	TIME [epoch: 34.2 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1253347426061809		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.1253347426061809 | validation: 0.14752246862404028]
	TIME [epoch: 34.2 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1433984773805158		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.1433984773805158 | validation: 0.13828885481827916]
	TIME [epoch: 34.1 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1275975194558588		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.1275975194558588 | validation: 0.13481720761700194]
	TIME [epoch: 34.2 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14058127935124606		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.14058127935124606 | validation: 0.13317979974563057]
	TIME [epoch: 34.2 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13266180478932307		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.13266180478932307 | validation: 0.12985460730862197]
	TIME [epoch: 34.1 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12978571018495055		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.12978571018495055 | validation: 0.13425471087739455]
	TIME [epoch: 34.2 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14161224589963567		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.14161224589963567 | validation: 0.1378392277570535]
	TIME [epoch: 34.2 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1328143079700323		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.1328143079700323 | validation: 0.13693680625985327]
	TIME [epoch: 34.1 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1291785921128558		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.1291785921128558 | validation: 0.14061174613457872]
	TIME [epoch: 34.2 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13063128609539848		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.13063128609539848 | validation: 0.12366413599600384]
	TIME [epoch: 34.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_576.pth
	Model improved!!!
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1314707057095738		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.1314707057095738 | validation: 0.13912307819971764]
	TIME [epoch: 34.2 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13373826489218665		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.13373826489218665 | validation: 0.13379943354676707]
	TIME [epoch: 34.1 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12815477438108538		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.12815477438108538 | validation: 0.14367145310195145]
	TIME [epoch: 34.2 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12604919983838347		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.12604919983838347 | validation: 0.1629397936328194]
	TIME [epoch: 34.2 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13248575284854716		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.13248575284854716 | validation: 0.13351695964318258]
	TIME [epoch: 34.1 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1307259853402977		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.1307259853402977 | validation: 0.1252899271487528]
	TIME [epoch: 34.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12687600030467705		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.12687600030467705 | validation: 0.15006643916181256]
	TIME [epoch: 34.2 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14193280300283748		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.14193280300283748 | validation: 0.13151091393408487]
	TIME [epoch: 34.1 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13691995621855904		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.13691995621855904 | validation: 0.1567530831203186]
	TIME [epoch: 34.2 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.128337767762452		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.128337767762452 | validation: 0.1403870532217776]
	TIME [epoch: 34.2 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1251935008691214		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.1251935008691214 | validation: 0.1303287498284772]
	TIME [epoch: 34.2 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1265880898062222		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.1265880898062222 | validation: 0.12745134610223674]
	TIME [epoch: 34.1 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13176937731046498		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.13176937731046498 | validation: 0.13067160449891457]
	TIME [epoch: 34.2 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12768764256001383		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.12768764256001383 | validation: 0.1450257614576158]
	TIME [epoch: 34.2 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12869153834264282		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.12869153834264282 | validation: 0.1312290258042449]
	TIME [epoch: 34.1 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1277614254203258		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.1277614254203258 | validation: 0.12657039237159812]
	TIME [epoch: 34.2 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12137464674747125		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.12137464674747125 | validation: 0.1224394817692195]
	TIME [epoch: 34.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12465224626001899		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.12465224626001899 | validation: 0.13592901870000995]
	TIME [epoch: 34.1 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13872711064669715		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.13872711064669715 | validation: 0.1343484566790128]
	TIME [epoch: 34.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13097684583493144		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.13097684583493144 | validation: 0.12357325739337144]
	TIME [epoch: 34.2 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13139972393938848		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.13139972393938848 | validation: 0.1265716514734723]
	TIME [epoch: 34.2 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12483529150629831		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.12483529150629831 | validation: 0.13259642998030502]
	TIME [epoch: 34.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1247438587138376		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.1247438587138376 | validation: 0.1356555370228884]
	TIME [epoch: 34.2 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12601040454673168		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.12601040454673168 | validation: 0.13785171945922567]
	TIME [epoch: 34.2 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12721235347360216		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.12721235347360216 | validation: 0.13394108186279402]
	TIME [epoch: 34.1 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12319317387094286		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.12319317387094286 | validation: 0.12539614325523862]
	TIME [epoch: 34.2 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12452919350124302		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.12452919350124302 | validation: 0.12530102135277915]
	TIME [epoch: 34.1 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13175481450414558		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.13175481450414558 | validation: 0.11773305662376465]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13272247688624325		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.13272247688624325 | validation: 0.13129138601625367]
	TIME [epoch: 34.1 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12027806874521522		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.12027806874521522 | validation: 0.12823539305412207]
	TIME [epoch: 34.2 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12544052106954792		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.12544052106954792 | validation: 0.13013387671055637]
	TIME [epoch: 34.2 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12946885987141252		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.12946885987141252 | validation: 0.12333897333016466]
	TIME [epoch: 34.1 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12073867606682624		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.12073867606682624 | validation: 0.1280537836820666]
	TIME [epoch: 34.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1241797475304636		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.1241797475304636 | validation: 0.12531239174557113]
	TIME [epoch: 34.1 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12463745147291849		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.12463745147291849 | validation: 0.1285557135649868]
	TIME [epoch: 34.1 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13171114113791563		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.13171114113791563 | validation: 0.16983009949319716]
	TIME [epoch: 34 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13627690381876684		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.13627690381876684 | validation: 0.1406867565660544]
	TIME [epoch: 34.1 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1274931242252697		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.1274931242252697 | validation: 0.12610378663581848]
	TIME [epoch: 34 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12336044725341458		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.12336044725341458 | validation: 0.13481883488191104]
	TIME [epoch: 34.1 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12521052171655017		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.12521052171655017 | validation: 0.12667344682035636]
	TIME [epoch: 34.1 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11764730935614764		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.11764730935614764 | validation: 0.16457498094684367]
	TIME [epoch: 34.2 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13621385223507199		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.13621385223507199 | validation: 0.12093008414635778]
	TIME [epoch: 34 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12115562259477478		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.12115562259477478 | validation: 0.12487864750198702]
	TIME [epoch: 34.1 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12302522151216996		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.12302522151216996 | validation: 0.1441188271520193]
	TIME [epoch: 34.1 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13207432680869968		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.13207432680869968 | validation: 0.12364638917994134]
	TIME [epoch: 34.1 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13279440521136276		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.13279440521136276 | validation: 0.12331993011285336]
	TIME [epoch: 34.1 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12102584993003573		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.12102584993003573 | validation: 0.12051726825092163]
	TIME [epoch: 34.1 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12349567642668736		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.12349567642668736 | validation: 0.11981730835096666]
	TIME [epoch: 34.1 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11779495036219445		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.11779495036219445 | validation: 0.13128929905471953]
	TIME [epoch: 34.1 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12258953298991274		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.12258953298991274 | validation: 0.14565304003187096]
	TIME [epoch: 34.1 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1269883134633426		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.1269883134633426 | validation: 0.1576649377192617]
	TIME [epoch: 34.1 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13160199858482435		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.13160199858482435 | validation: 0.14037206438251315]
	TIME [epoch: 34.1 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12169691974037963		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.12169691974037963 | validation: 0.12055441247273696]
	TIME [epoch: 34.2 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11810285210159033		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.11810285210159033 | validation: 0.12100570392907326]
	TIME [epoch: 34.1 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12015982918123155		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.12015982918123155 | validation: 0.12882925808536216]
	TIME [epoch: 34 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1238214900784316		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.1238214900784316 | validation: 0.13642904241398457]
	TIME [epoch: 34.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12881814454218296		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.12881814454218296 | validation: 0.11997621504422121]
	TIME [epoch: 34.1 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12670276884418133		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.12670276884418133 | validation: 0.13726849242634148]
	TIME [epoch: 34.1 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1300366479091953		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.1300366479091953 | validation: 0.13193867813333462]
	TIME [epoch: 34.1 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12245322167821182		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.12245322167821182 | validation: 0.13250517756873023]
	TIME [epoch: 34.1 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12368986949636827		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.12368986949636827 | validation: 0.1210089354704208]
	TIME [epoch: 34.1 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12119113896288125		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.12119113896288125 | validation: 0.13238627410410486]
	TIME [epoch: 34.1 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12088206584326451		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.12088206584326451 | validation: 0.13133677623545]
	TIME [epoch: 34.1 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12403374264115419		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.12403374264115419 | validation: 0.11758976540229282]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_640.pth
	Model improved!!!
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11996438807279108		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.11996438807279108 | validation: 0.12211619022723721]
	TIME [epoch: 34.1 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12267458625670627		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.12267458625670627 | validation: 0.11683125642850596]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12269014515831547		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.12269014515831547 | validation: 0.14605088448758102]
	TIME [epoch: 34.2 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12654668804494767		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.12654668804494767 | validation: 0.12925218565595675]
	TIME [epoch: 34 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11717091591223273		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.11717091591223273 | validation: 0.12997053413645004]
	TIME [epoch: 34.1 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12334042169893614		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.12334042169893614 | validation: 0.1188152679554571]
	TIME [epoch: 34.1 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12259832873287918		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.12259832873287918 | validation: 0.12080505014216934]
	TIME [epoch: 34.1 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11820713977480503		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.11820713977480503 | validation: 0.11885651201481759]
	TIME [epoch: 34.1 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12066822553403564		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.12066822553403564 | validation: 0.1171136692126625]
	TIME [epoch: 34.1 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12373405413693372		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.12373405413693372 | validation: 0.11652154998658909]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12292306142494405		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.12292306142494405 | validation: 0.12218264320458572]
	TIME [epoch: 34.1 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12498249879611814		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.12498249879611814 | validation: 0.11729067345335331]
	TIME [epoch: 34.1 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1153922022875219		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.1153922022875219 | validation: 0.11984560352947765]
	TIME [epoch: 34 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13755782087553053		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.13755782087553053 | validation: 0.12624560015676423]
	TIME [epoch: 34.1 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11986875105594488		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.11986875105594488 | validation: 0.12073631030172488]
	TIME [epoch: 34.1 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12078474066887432		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.12078474066887432 | validation: 0.12100421524267832]
	TIME [epoch: 34 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12367637441857939		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.12367637441857939 | validation: 0.1331619842089687]
	TIME [epoch: 34.1 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12612269626319417		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.12612269626319417 | validation: 0.11787818629647756]
	TIME [epoch: 34.1 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11612571825061839		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.11612571825061839 | validation: 0.12992862448306636]
	TIME [epoch: 34 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12063833192248682		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.12063833192248682 | validation: 0.11969591700500969]
	TIME [epoch: 34.1 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12499971209473378		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.12499971209473378 | validation: 0.12401368710269262]
	TIME [epoch: 34.1 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11604663147839589		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.11604663147839589 | validation: 0.12219874054051175]
	TIME [epoch: 34.1 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11773346450174324		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.11773346450174324 | validation: 0.12935765042674635]
	TIME [epoch: 34.1 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12418761400282195		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.12418761400282195 | validation: 0.12508649337176214]
	TIME [epoch: 34.1 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11448741787981687		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.11448741787981687 | validation: 0.13392025389389828]
	TIME [epoch: 34 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12579721041692263		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.12579721041692263 | validation: 0.12145966967423004]
	TIME [epoch: 34.1 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11679264796277991		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.11679264796277991 | validation: 0.11844832695073312]
	TIME [epoch: 34.1 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11310215495258681		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.11310215495258681 | validation: 0.11780575028685653]
	TIME [epoch: 34.1 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11810824767112155		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.11810824767112155 | validation: 0.12269439004573512]
	TIME [epoch: 34.1 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12627377298714884		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.12627377298714884 | validation: 0.13597556636388886]
	TIME [epoch: 34.1 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1266202034818235		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.1266202034818235 | validation: 0.12243890770855756]
	TIME [epoch: 34.1 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12165314648554328		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.12165314648554328 | validation: 0.11800397988271777]
	TIME [epoch: 34.1 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1191818967987412		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.1191818967987412 | validation: 0.1438191771933705]
	TIME [epoch: 34.1 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12492316317099034		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.12492316317099034 | validation: 0.12276482663911384]
	TIME [epoch: 34.1 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1220422704986257		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.1220422704986257 | validation: 0.13025216057716105]
	TIME [epoch: 34.1 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11714895135753062		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.11714895135753062 | validation: 0.11343514508963465]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_676.pth
	Model improved!!!
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11909215827987904		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.11909215827987904 | validation: 0.11798522390531516]
	TIME [epoch: 34.1 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12272103124681		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.12272103124681 | validation: 0.11574108438840776]
	TIME [epoch: 34.1 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11386794784593175		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.11386794784593175 | validation: 0.12476719290391505]
	TIME [epoch: 34.1 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11931443310647435		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.11931443310647435 | validation: 0.11516042348317432]
	TIME [epoch: 34 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12071177336960637		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.12071177336960637 | validation: 0.11882963107615163]
	TIME [epoch: 34.1 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1219911898313085		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.1219911898313085 | validation: 0.126975654822654]
	TIME [epoch: 34.1 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11573018266030188		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.11573018266030188 | validation: 0.12063848052631038]
	TIME [epoch: 34.1 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11726908545737497		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.11726908545737497 | validation: 0.11256356073876995]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_684.pth
	Model improved!!!
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11766352927530806		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.11766352927530806 | validation: 0.12273665312486665]
	TIME [epoch: 34.1 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12014606597805719		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.12014606597805719 | validation: 0.11942429112129284]
	TIME [epoch: 34.1 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11813557692212394		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.11813557692212394 | validation: 0.11773558020315833]
	TIME [epoch: 34.1 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1173035964526439		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.1173035964526439 | validation: 0.1194482369329957]
	TIME [epoch: 34.1 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11600349552519959		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.11600349552519959 | validation: 0.11597269746195624]
	TIME [epoch: 34.1 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11199965285245964		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.11199965285245964 | validation: 0.12073302704342453]
	TIME [epoch: 34.1 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12422925598043513		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.12422925598043513 | validation: 0.13796279762606095]
	TIME [epoch: 34.1 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12407446355405498		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.12407446355405498 | validation: 0.11967604881515198]
	TIME [epoch: 34.1 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11743359254872344		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.11743359254872344 | validation: 0.11973669895082739]
	TIME [epoch: 34.1 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11526401235107579		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.11526401235107579 | validation: 0.1229408156683824]
	TIME [epoch: 34.1 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11699872688504478		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.11699872688504478 | validation: 0.11290295787091026]
	TIME [epoch: 34.1 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1178285350046254		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.1178285350046254 | validation: 0.1131655883862068]
	TIME [epoch: 34.1 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11880243207280992		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.11880243207280992 | validation: 0.12105055295792866]
	TIME [epoch: 34.1 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12513092381783675		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.12513092381783675 | validation: 0.12303102735866744]
	TIME [epoch: 34.1 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11858649733672726		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.11858649733672726 | validation: 0.11481825524757315]
	TIME [epoch: 34.1 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11224975205994828		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.11224975205994828 | validation: 0.11366447290210414]
	TIME [epoch: 34 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11262269396159023		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.11262269396159023 | validation: 0.1252448386015969]
	TIME [epoch: 34.1 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1264612884902298		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.1264612884902298 | validation: 0.1387743741620026]
	TIME [epoch: 34.1 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12596432643441874		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.12596432643441874 | validation: 0.11568745564707555]
	TIME [epoch: 34 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11278148215719971		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.11278148215719971 | validation: 0.11788154368387002]
	TIME [epoch: 34.1 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12310549910102109		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.12310549910102109 | validation: 0.11396478909407148]
	TIME [epoch: 34.1 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11556062127458812		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.11556062127458812 | validation: 0.12750825663377513]
	TIME [epoch: 34 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1204134846161187		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.1204134846161187 | validation: 0.11208718588732977]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_707.pth
	Model improved!!!
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11570412399028664		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.11570412399028664 | validation: 0.11855317319435388]
	TIME [epoch: 34.1 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11836296439686549		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.11836296439686549 | validation: 0.11164634206792604]
	TIME [epoch: 34 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_709.pth
	Model improved!!!
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11579829973212571		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.11579829973212571 | validation: 0.12449035982970452]
	TIME [epoch: 34.1 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11627216721429037		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.11627216721429037 | validation: 0.12051931214373296]
	TIME [epoch: 34.1 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11309330333983382		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.11309330333983382 | validation: 0.11445155227910146]
	TIME [epoch: 34 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11829851428903118		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.11829851428903118 | validation: 0.12422165688586925]
	TIME [epoch: 34.1 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11718756458213744		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.11718756458213744 | validation: 0.12893379635586721]
	TIME [epoch: 34.1 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12097812973192522		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.12097812973192522 | validation: 0.1163799538902471]
	TIME [epoch: 34 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11433883857185353		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.11433883857185353 | validation: 0.11303404643721582]
	TIME [epoch: 34.1 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11350978414559071		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.11350978414559071 | validation: 0.11452830631471089]
	TIME [epoch: 34.1 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11281584430028167		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.11281584430028167 | validation: 0.11705134249886615]
	TIME [epoch: 34 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11364494562302269		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.11364494562302269 | validation: 0.11652330324367606]
	TIME [epoch: 34.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11503738009999573		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.11503738009999573 | validation: 0.12453987045746737]
	TIME [epoch: 34.1 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1156108885105017		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.1156108885105017 | validation: 0.11542818413443896]
	TIME [epoch: 34 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11191899335402061		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.11191899335402061 | validation: 0.13014535204337305]
	TIME [epoch: 34.1 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11710861447605224		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.11710861447605224 | validation: 0.1322072936066945]
	TIME [epoch: 34 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12221027511437023		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.12221027511437023 | validation: 0.11525161464322774]
	TIME [epoch: 34 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1171507439319252		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.1171507439319252 | validation: 0.11738752452403192]
	TIME [epoch: 34.1 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11316594745696912		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.11316594745696912 | validation: 0.11517741496463027]
	TIME [epoch: 34 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11274278655679296		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.11274278655679296 | validation: 0.11313753157262298]
	TIME [epoch: 34.1 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11831743909744882		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.11831743909744882 | validation: 0.12312733375268793]
	TIME [epoch: 34.1 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1208331321207316		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.1208331321207316 | validation: 0.11330257430664081]
	TIME [epoch: 34.1 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11493048015635628		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.11493048015635628 | validation: 0.11663561994027788]
	TIME [epoch: 34 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10923315652633185		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.10923315652633185 | validation: 0.11437240917223099]
	TIME [epoch: 34.1 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1137137490054759		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.1137137490054759 | validation: 0.11906721088921199]
	TIME [epoch: 34.1 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11685587345765422		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.11685587345765422 | validation: 0.13316298246073605]
	TIME [epoch: 34.1 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12114940879822715		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.12114940879822715 | validation: 0.11559761042765851]
	TIME [epoch: 34.1 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10984967610864514		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.10984967610864514 | validation: 0.11732565966672537]
	TIME [epoch: 34 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11404275494948106		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.11404275494948106 | validation: 0.1145145931079643]
	TIME [epoch: 34 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1127204780344645		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.1127204780344645 | validation: 0.11970623621010582]
	TIME [epoch: 34.1 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11433879502315802		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.11433879502315802 | validation: 0.1167652478018499]
	TIME [epoch: 34 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11389931276981088		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.11389931276981088 | validation: 0.11716390402215807]
	TIME [epoch: 34.1 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10868785497684086		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.10868785497684086 | validation: 0.11657593744177885]
	TIME [epoch: 34.1 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1137904306096561		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.1137904306096561 | validation: 0.11154697256118465]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_741.pth
	Model improved!!!
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11513019151600583		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.11513019151600583 | validation: 0.12916450543852256]
	TIME [epoch: 34.1 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11974857868180687		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.11974857868180687 | validation: 0.11095635836979248]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_743.pth
	Model improved!!!
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1104250885558588		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.1104250885558588 | validation: 0.11897690782078925]
	TIME [epoch: 34 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11273284764220276		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.11273284764220276 | validation: 0.10963004615896371]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_745.pth
	Model improved!!!
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11178790360190409		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.11178790360190409 | validation: 0.11215382371410552]
	TIME [epoch: 34.1 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11372910483411135		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.11372910483411135 | validation: 0.11968206195823292]
	TIME [epoch: 34 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.120083018191695		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.120083018191695 | validation: 0.11485441404840671]
	TIME [epoch: 34.1 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10937386094410084		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.10937386094410084 | validation: 0.11642066461378764]
	TIME [epoch: 34.1 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1137901763538866		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.1137901763538866 | validation: 0.11667587128765247]
	TIME [epoch: 34.1 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11548761921866113		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.11548761921866113 | validation: 0.1133635055841598]
	TIME [epoch: 34.1 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11254974140878543		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.11254974140878543 | validation: 0.11304854959380836]
	TIME [epoch: 34.1 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11493968607975227		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.11493968607975227 | validation: 0.11284614677642088]
	TIME [epoch: 34 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11132602773709374		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.11132602773709374 | validation: 0.11124466153219106]
	TIME [epoch: 34.1 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11150083422151319		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.11150083422151319 | validation: 0.11698210127718008]
	TIME [epoch: 34.1 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11119180403354059		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.11119180403354059 | validation: 0.11662330714919891]
	TIME [epoch: 34 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11348042698518482		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.11348042698518482 | validation: 0.13706596964223408]
	TIME [epoch: 34.1 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12100621487770585		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.12100621487770585 | validation: 0.10885383817137118]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11168843527439476		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.11168843527439476 | validation: 0.11389226826121676]
	TIME [epoch: 34.1 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1087520027848283		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.1087520027848283 | validation: 0.11412243995242159]
	TIME [epoch: 34.1 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1137326088100089		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.1137326088100089 | validation: 0.12336565257679397]
	TIME [epoch: 34.1 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11842710349566458		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.11842710349566458 | validation: 0.11372536234207063]
	TIME [epoch: 34 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11177908109816173		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.11177908109816173 | validation: 0.11218671433125858]
	TIME [epoch: 34.1 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1097323231398052		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.1097323231398052 | validation: 0.10946973243153668]
	TIME [epoch: 34.1 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11316130071418581		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.11316130071418581 | validation: 0.1105008415654552]
	TIME [epoch: 34.1 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11219230074032745		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.11219230074032745 | validation: 0.11212965122393058]
	TIME [epoch: 34.1 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11340896539008151		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.11340896539008151 | validation: 0.11086743783867961]
	TIME [epoch: 34.1 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10906857664055539		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.10906857664055539 | validation: 0.10866793807427881]
	TIME [epoch: 34 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_768.pth
	Model improved!!!
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11218424732842693		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.11218424732842693 | validation: 0.1260049128503536]
	TIME [epoch: 34.1 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11728477914200638		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.11728477914200638 | validation: 0.10996605559868622]
	TIME [epoch: 34.1 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10923341056971991		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.10923341056971991 | validation: 0.11178786112867306]
	TIME [epoch: 34.1 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10960649636593947		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.10960649636593947 | validation: 0.11267614906605303]
	TIME [epoch: 34.1 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11241735561759879		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.11241735561759879 | validation: 0.11286510355191762]
	TIME [epoch: 34.1 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11271606703364981		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.11271606703364981 | validation: 0.11163579926982126]
	TIME [epoch: 34.1 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1088109793574131		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.1088109793574131 | validation: 0.10595303433681307]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_775.pth
	Model improved!!!
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1105349044243043		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.1105349044243043 | validation: 0.10947201386625052]
	TIME [epoch: 34.1 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10954735795323323		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.10954735795323323 | validation: 0.11239513863783679]
	TIME [epoch: 34.1 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11082521674964553		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.11082521674964553 | validation: 0.12222170678704797]
	TIME [epoch: 34.1 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11465919639088923		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.11465919639088923 | validation: 0.10903793243760991]
	TIME [epoch: 34.1 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11066608868108409		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.11066608868108409 | validation: 0.11173628237029626]
	TIME [epoch: 34.1 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11413298669609917		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.11413298669609917 | validation: 0.1192650340867861]
	TIME [epoch: 34 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11094177271924771		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.11094177271924771 | validation: 0.11664934311935264]
	TIME [epoch: 34.1 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11350943603963701		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.11350943603963701 | validation: 0.11306394642792661]
	TIME [epoch: 34.1 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1111812812084548		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.1111812812084548 | validation: 0.11376926706386274]
	TIME [epoch: 34.1 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11517888434487301		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.11517888434487301 | validation: 0.10589729984892113]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10955824904695538		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.10955824904695538 | validation: 0.11081420188121792]
	TIME [epoch: 34.1 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10873563597601674		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.10873563597601674 | validation: 0.11245260640470145]
	TIME [epoch: 34.1 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10796472524805112		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.10796472524805112 | validation: 0.11057784062054021]
	TIME [epoch: 34.1 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11396452897783843		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.11396452897783843 | validation: 0.11103302492617559]
	TIME [epoch: 34.1 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10695405915455256		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.10695405915455256 | validation: 0.11655527918692331]
	TIME [epoch: 34.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11133253031463428		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.11133253031463428 | validation: 0.10641348256952676]
	TIME [epoch: 34.1 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11141966360177072		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.11141966360177072 | validation: 0.11069645302372613]
	TIME [epoch: 34.1 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10964035046933122		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.10964035046933122 | validation: 0.10620613809733456]
	TIME [epoch: 34.1 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1112342902943934		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.1112342902943934 | validation: 0.11073398381024764]
	TIME [epoch: 34 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10809306574248441		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.10809306574248441 | validation: 0.11236095184886861]
	TIME [epoch: 34 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11072735441193912		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.11072735441193912 | validation: 0.11883895080073069]
	TIME [epoch: 34.1 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11255793177049875		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.11255793177049875 | validation: 0.10814659956648326]
	TIME [epoch: 34.1 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10857886625969873		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.10857886625969873 | validation: 0.11369886645726063]
	TIME [epoch: 34.1 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11261155334330293		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.11261155334330293 | validation: 0.11221021481964537]
	TIME [epoch: 34.1 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10917201954124237		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.10917201954124237 | validation: 0.11268725221041112]
	TIME [epoch: 34 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10883301565842803		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.10883301565842803 | validation: 0.11365155218487014]
	TIME [epoch: 34.1 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11088118879840635		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.11088118879840635 | validation: 0.12590319843910702]
	TIME [epoch: 34.1 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11512664135257163		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.11512664135257163 | validation: 0.11428561334939331]
	TIME [epoch: 34 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10921496520170779		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.10921496520170779 | validation: 0.11348428901040364]
	TIME [epoch: 34.1 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10999746871641297		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.10999746871641297 | validation: 0.10610618775018957]
	TIME [epoch: 34.1 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11029741080372721		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.11029741080372721 | validation: 0.10709362775679482]
	TIME [epoch: 34.1 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10871216137166881		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.10871216137166881 | validation: 0.10941586720522367]
	TIME [epoch: 34.1 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11200448794185863		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.11200448794185863 | validation: 0.10632987763068247]
	TIME [epoch: 34.1 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10962874760173753		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.10962874760173753 | validation: 0.11894263799153473]
	TIME [epoch: 34 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11045612493002091		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.11045612493002091 | validation: 0.10815051447483089]
	TIME [epoch: 34.1 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11063470452342643		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.11063470452342643 | validation: 0.11332776660254207]
	TIME [epoch: 34.1 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1092046886921057		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.1092046886921057 | validation: 0.10788046352839689]
	TIME [epoch: 34.1 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10668845745199336		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.10668845745199336 | validation: 0.10958625969005656]
	TIME [epoch: 34.1 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10917062112669654		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.10917062112669654 | validation: 0.11526443562574429]
	TIME [epoch: 34.1 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11139435781386181		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.11139435781386181 | validation: 0.11007723259972797]
	TIME [epoch: 34.1 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10854323183533215		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.10854323183533215 | validation: 0.11321982125508101]
	TIME [epoch: 34.1 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11002390712061516		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.11002390712061516 | validation: 0.10705893576428804]
	TIME [epoch: 34.1 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10930975823438138		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.10930975823438138 | validation: 0.10931603634290865]
	TIME [epoch: 34.1 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11151958168521078		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.11151958168521078 | validation: 0.11450944364779377]
	TIME [epoch: 34.1 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11173932662880659		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.11173932662880659 | validation: 0.10838065013213058]
	TIME [epoch: 34.1 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11241971707953785		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.11241971707953785 | validation: 0.10581705160912488]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_821.pth
	Model improved!!!
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.109454893629262		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.109454893629262 | validation: 0.11525504909233988]
	TIME [epoch: 34.1 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11161755198188512		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.11161755198188512 | validation: 0.11263036570503869]
	TIME [epoch: 34 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11123111815596173		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.11123111815596173 | validation: 0.10911122577955856]
	TIME [epoch: 34.1 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10769003379367952		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.10769003379367952 | validation: 0.11228278608823655]
	TIME [epoch: 34.1 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10906288020217832		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.10906288020217832 | validation: 0.10929686175497332]
	TIME [epoch: 34 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1068085607723366		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.1068085607723366 | validation: 0.10407826626630934]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_827.pth
	Model improved!!!
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10992110723458548		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.10992110723458548 | validation: 0.11031974932133842]
	TIME [epoch: 34.2 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11036130823476378		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.11036130823476378 | validation: 0.10899310082760041]
	TIME [epoch: 34.2 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10883844806551969		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.10883844806551969 | validation: 0.10753909445962065]
	TIME [epoch: 34.2 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1074537303004951		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.1074537303004951 | validation: 0.10972504434139675]
	TIME [epoch: 34.2 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11105478986339495		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.11105478986339495 | validation: 0.11774255719492421]
	TIME [epoch: 34.2 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11252244483991071		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.11252244483991071 | validation: 0.11135820295459864]
	TIME [epoch: 34.2 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10904002743018543		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.10904002743018543 | validation: 0.11029052496761127]
	TIME [epoch: 34.2 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10645892996521922		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.10645892996521922 | validation: 0.11067504570408297]
	TIME [epoch: 34.2 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10788404178320908		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.10788404178320908 | validation: 0.10796968871452836]
	TIME [epoch: 34.2 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11052355143959008		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.11052355143959008 | validation: 0.108681824348752]
	TIME [epoch: 34.2 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10864035789988265		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.10864035789988265 | validation: 0.11114234516401136]
	TIME [epoch: 34.2 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10896468144487331		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.10896468144487331 | validation: 0.10766283246198713]
	TIME [epoch: 34.1 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10680927250012748		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.10680927250012748 | validation: 0.1076067501021174]
	TIME [epoch: 34.2 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10817793162161195		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.10817793162161195 | validation: 0.10404968459379682]
	TIME [epoch: 34.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_841.pth
	Model improved!!!
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11188824494552656		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.11188824494552656 | validation: 0.11555263605381744]
	TIME [epoch: 34.1 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10952718010285871		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.10952718010285871 | validation: 0.10832215418526392]
	TIME [epoch: 34.1 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10979258880420838		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.10979258880420838 | validation: 0.11348416180407386]
	TIME [epoch: 34.2 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10962902749929943		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.10962902749929943 | validation: 0.10665623340414485]
	TIME [epoch: 34.1 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10518619683600339		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.10518619683600339 | validation: 0.10646745022554888]
	TIME [epoch: 34.1 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10500644172350992		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.10500644172350992 | validation: 0.10534789297281395]
	TIME [epoch: 34.2 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10722295758917288		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.10722295758917288 | validation: 0.10521272236198911]
	TIME [epoch: 34.2 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10842366633343903		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.10842366633343903 | validation: 0.10487494044063397]
	TIME [epoch: 34.1 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11038111849466756		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.11038111849466756 | validation: 0.10640806979020662]
	TIME [epoch: 34.2 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11336717469951736		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.11336717469951736 | validation: 0.11422380183402414]
	TIME [epoch: 34.2 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11050006237216867		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.11050006237216867 | validation: 0.10749874397627698]
	TIME [epoch: 34.1 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10486799893794786		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.10486799893794786 | validation: 0.10527124387034789]
	TIME [epoch: 34.2 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10897187526912404		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.10897187526912404 | validation: 0.11607818393959822]
	TIME [epoch: 34.2 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10861313749140257		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.10861313749140257 | validation: 0.10944756718841954]
	TIME [epoch: 34.1 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10897168935044102		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.10897168935044102 | validation: 0.10490395270555616]
	TIME [epoch: 34.1 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1051290075008508		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.1051290075008508 | validation: 0.10397541471161653]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10599863667077133		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.10599863667077133 | validation: 0.11194311384121838]
	TIME [epoch: 34.1 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11006188444171199		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.11006188444171199 | validation: 0.10629791259968474]
	TIME [epoch: 34.1 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10542670970459012		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.10542670970459012 | validation: 0.10828585681542119]
	TIME [epoch: 34.2 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10544792204784931		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.10544792204784931 | validation: 0.10855371374948547]
	TIME [epoch: 34.2 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10791002048586865		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.10791002048586865 | validation: 0.10437338722644382]
	TIME [epoch: 34.1 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10480220927745595		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.10480220927745595 | validation: 0.10393769556592333]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_863.pth
	Model improved!!!
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10765062156409298		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.10765062156409298 | validation: 0.11260182218133324]
	TIME [epoch: 34.2 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10986680224839876		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.10986680224839876 | validation: 0.10729810445499481]
	TIME [epoch: 34.1 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10732979375977075		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.10732979375977075 | validation: 0.10698354701166837]
	TIME [epoch: 34.2 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10556281967970527		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.10556281967970527 | validation: 0.10514948479576748]
	TIME [epoch: 34.2 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10817383504432339		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.10817383504432339 | validation: 0.10386812109479063]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_868.pth
	Model improved!!!
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1069151967690807		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.1069151967690807 | validation: 0.10954296092460514]
	TIME [epoch: 34.1 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10736888539582753		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.10736888539582753 | validation: 0.1111375300960496]
	TIME [epoch: 34.1 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1061127423657216		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.1061127423657216 | validation: 0.10438454245152082]
	TIME [epoch: 34.1 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10684906858136517		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.10684906858136517 | validation: 0.10628770533707274]
	TIME [epoch: 34.1 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10660308722748227		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.10660308722748227 | validation: 0.10849629914747821]
	TIME [epoch: 34.2 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1078193404027047		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.1078193404027047 | validation: 0.1085298117072174]
	TIME [epoch: 34.1 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10562077038256999		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.10562077038256999 | validation: 0.10687431287550554]
	TIME [epoch: 34.1 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1067874117313362		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.1067874117313362 | validation: 0.10824106298902797]
	TIME [epoch: 34.1 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1106853418022892		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.1106853418022892 | validation: 0.10701447327668526]
	TIME [epoch: 34.1 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10555481499297201		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.10555481499297201 | validation: 0.10866620515174391]
	TIME [epoch: 34.1 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10574781727610916		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.10574781727610916 | validation: 0.10589419897962687]
	TIME [epoch: 34.1 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10790442253410429		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.10790442253410429 | validation: 0.11156033981903521]
	TIME [epoch: 34.2 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10551435823679067		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.10551435823679067 | validation: 0.10295122175815738]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_881.pth
	Model improved!!!
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10872212905035585		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.10872212905035585 | validation: 0.10680921924974393]
	TIME [epoch: 34.1 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10586789276080869		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.10586789276080869 | validation: 0.10799067559730618]
	TIME [epoch: 34.1 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10680787306114983		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.10680787306114983 | validation: 0.10793520493995415]
	TIME [epoch: 34.1 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10659002040353688		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.10659002040353688 | validation: 0.1032586283527966]
	TIME [epoch: 34.1 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10518773703124756		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.10518773703124756 | validation: 0.10615624314577418]
	TIME [epoch: 34.1 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10722461704030045		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.10722461704030045 | validation: 0.10426164521749906]
	TIME [epoch: 34.1 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10634439978683058		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.10634439978683058 | validation: 0.104211017348213]
	TIME [epoch: 34.1 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10750343959767623		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.10750343959767623 | validation: 0.10853052723050294]
	TIME [epoch: 34.1 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10684262617830603		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.10684262617830603 | validation: 0.10486802363216699]
	TIME [epoch: 34.1 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1072903686485492		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.1072903686485492 | validation: 0.10529987601628246]
	TIME [epoch: 34.1 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1076980050883409		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.1076980050883409 | validation: 0.10469698791239951]
	TIME [epoch: 34.1 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10669924746998496		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.10669924746998496 | validation: 0.10318144149440586]
	TIME [epoch: 34.1 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10675385670013163		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.10675385670013163 | validation: 0.10528650093607249]
	TIME [epoch: 34.1 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10577928057348025		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.10577928057348025 | validation: 0.10508881951452657]
	TIME [epoch: 34.1 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10550255248171753		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.10550255248171753 | validation: 0.10879551932637443]
	TIME [epoch: 34.1 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10659517932384316		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.10659517932384316 | validation: 0.1041194184825556]
	TIME [epoch: 34.1 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10326692451353653		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.10326692451353653 | validation: 0.10490521871410269]
	TIME [epoch: 34.1 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10387837975823568		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.10387837975823568 | validation: 0.10593249474961913]
	TIME [epoch: 34.1 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10769714453474143		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.10769714453474143 | validation: 0.10553272972354953]
	TIME [epoch: 34.1 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10577642675267403		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.10577642675267403 | validation: 0.11083844348713767]
	TIME [epoch: 34.1 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10701576353451718		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.10701576353451718 | validation: 0.10652666087394738]
	TIME [epoch: 34.1 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10685139574722703		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.10685139574722703 | validation: 0.10427531352309477]
	TIME [epoch: 34.1 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10589631834378391		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.10589631834378391 | validation: 0.10297940773225109]
	TIME [epoch: 34.1 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1045401175370352		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.1045401175370352 | validation: 0.1053931835192351]
	TIME [epoch: 34 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1051092205097256		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.1051092205097256 | validation: 0.10383679821758653]
	TIME [epoch: 34.1 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1061959645474023		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.1061959645474023 | validation: 0.1030969294342913]
	TIME [epoch: 34 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10612957510278825		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.10612957510278825 | validation: 0.10544644169121872]
	TIME [epoch: 34.1 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10572935083745573		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.10572935083745573 | validation: 0.1078223397692218]
	TIME [epoch: 34.1 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10703540996960002		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.10703540996960002 | validation: 0.10540074573115077]
	TIME [epoch: 34.1 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10730858631024001		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.10730858631024001 | validation: 0.10598756912329288]
	TIME [epoch: 34.1 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10755826235420195		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.10755826235420195 | validation: 0.10699667587163333]
	TIME [epoch: 34.1 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10560678114982984		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.10560678114982984 | validation: 0.10525784313307515]
	TIME [epoch: 34.2 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10470256928340038		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.10470256928340038 | validation: 0.10249209228095055]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_914.pth
	Model improved!!!
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10649932729153513		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.10649932729153513 | validation: 0.10494256837953449]
	TIME [epoch: 34.1 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10446965235742109		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.10446965235742109 | validation: 0.10248271151693344]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_916.pth
	Model improved!!!
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10496057851494392		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.10496057851494392 | validation: 0.10521431851942006]
	TIME [epoch: 34.1 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10487564303648694		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.10487564303648694 | validation: 0.10338119315732675]
	TIME [epoch: 34.1 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10497819556940667		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.10497819556940667 | validation: 0.10122998364733923]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10530392160345554		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.10530392160345554 | validation: 0.1012221517986566]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_920.pth
	Model improved!!!
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10281477983342954		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.10281477983342954 | validation: 0.10557857596161663]
	TIME [epoch: 34.1 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10755185607026532		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.10755185607026532 | validation: 0.10332653784270066]
	TIME [epoch: 34.1 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10408142530779446		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.10408142530779446 | validation: 0.1073765162373447]
	TIME [epoch: 34.1 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10644521076521055		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.10644521076521055 | validation: 0.10396994069141043]
	TIME [epoch: 34.1 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10621019721524921		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.10621019721524921 | validation: 0.10826031313157902]
	TIME [epoch: 34.2 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10667856066706809		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.10667856066706809 | validation: 0.10171901572458636]
	TIME [epoch: 34.2 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1045713456725832		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.1045713456725832 | validation: 0.1030134750619496]
	TIME [epoch: 34 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10621242686696868		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.10621242686696868 | validation: 0.1065886994999129]
	TIME [epoch: 34.1 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10692129208744672		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.10692129208744672 | validation: 0.10345276305996393]
	TIME [epoch: 34.1 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10373487900386118		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.10373487900386118 | validation: 0.10614815649825354]
	TIME [epoch: 34.1 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10557347541027653		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.10557347541027653 | validation: 0.10739817323432575]
	TIME [epoch: 34.1 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10442963458710339		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.10442963458710339 | validation: 0.1093017912905697]
	TIME [epoch: 34.1 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10488070357296901		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.10488070357296901 | validation: 0.10743925660639857]
	TIME [epoch: 34.1 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10605469585274035		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.10605469585274035 | validation: 0.10231136433265117]
	TIME [epoch: 34 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10506895744390912		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.10506895744390912 | validation: 0.1081893078707111]
	TIME [epoch: 34.1 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10730208445401256		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.10730208445401256 | validation: 0.10415546406387641]
	TIME [epoch: 34.2 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10388849001548811		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.10388849001548811 | validation: 0.10184590420563097]
	TIME [epoch: 34 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10512522074147485		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.10512522074147485 | validation: 0.10464392329421154]
	TIME [epoch: 34.1 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10495673658516191		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.10495673658516191 | validation: 0.10806471077585594]
	TIME [epoch: 34.1 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10677153227418813		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.10677153227418813 | validation: 0.10279504151561272]
	TIME [epoch: 34.1 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10616095286247457		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.10616095286247457 | validation: 0.10621330052392054]
	TIME [epoch: 34.2 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1048586769751492		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.1048586769751492 | validation: 0.1047005881687509]
	TIME [epoch: 34.1 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10536725749351515		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.10536725749351515 | validation: 0.10381203514113643]
	TIME [epoch: 34.1 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10318283224133795		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.10318283224133795 | validation: 0.10300210844830049]
	TIME [epoch: 34.1 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10341211168369793		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.10341211168369793 | validation: 0.1016385694343982]
	TIME [epoch: 34.2 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1044042962805379		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.1044042962805379 | validation: 0.10392089272999891]
	TIME [epoch: 34.1 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10667429520241173		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.10667429520241173 | validation: 0.10701430330239606]
	TIME [epoch: 34.1 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10588157435838848		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.10588157435838848 | validation: 0.10256650799168399]
	TIME [epoch: 34.1 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10532286011031818		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.10532286011031818 | validation: 0.10289729533734357]
	TIME [epoch: 34.2 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10438805527246305		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.10438805527246305 | validation: 0.10304058121955798]
	TIME [epoch: 34.1 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10459423685612793		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.10459423685612793 | validation: 0.10147330907041584]
	TIME [epoch: 34.1 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10373180411516947		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.10373180411516947 | validation: 0.10254594586884822]
	TIME [epoch: 34.1 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10303204545047309		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.10303204545047309 | validation: 0.10290700776269043]
	TIME [epoch: 34.1 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10474995836568274		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.10474995836568274 | validation: 0.1028867319441093]
	TIME [epoch: 34.2 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10399755430506114		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.10399755430506114 | validation: 0.10352034411387787]
	TIME [epoch: 34.1 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10722561243012194		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.10722561243012194 | validation: 0.10174343241696798]
	TIME [epoch: 34.1 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10443384868389055		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.10443384868389055 | validation: 0.10172507978332718]
	TIME [epoch: 34.1 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1041585960350482		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.1041585960350482 | validation: 0.10261297634203756]
	TIME [epoch: 34.1 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1035932376723971		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.1035932376723971 | validation: 0.10432634203445537]
	TIME [epoch: 34.1 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10383540179481408		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.10383540179481408 | validation: 0.10730425896688268]
	TIME [epoch: 34.1 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1055674176006761		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.1055674176006761 | validation: 0.10408786060231595]
	TIME [epoch: 34.1 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10319519127330952		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.10319519127330952 | validation: 0.10348958885991097]
	TIME [epoch: 34.1 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10777270214937669		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.10777270214937669 | validation: 0.1016707332788803]
	TIME [epoch: 34 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1033737927480898		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.1033737927480898 | validation: 0.10284193564536814]
	TIME [epoch: 34.1 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10250463119552038		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.10250463119552038 | validation: 0.10438743805027975]
	TIME [epoch: 34.1 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10615918539515218		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.10615918539515218 | validation: 0.10591400255173124]
	TIME [epoch: 34 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10509864620183511		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.10509864620183511 | validation: 0.10425279778090239]
	TIME [epoch: 34.1 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1058409369455521		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.1058409369455521 | validation: 0.1024398806139803]
	TIME [epoch: 34.1 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10339099665490234		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.10339099665490234 | validation: 0.10026353910205169]
	TIME [epoch: 34.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_969.pth
	Model improved!!!
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10437763326293947		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.10437763326293947 | validation: 0.10129698370020404]
	TIME [epoch: 34.1 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10411150557937285		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.10411150557937285 | validation: 0.10485588747505395]
	TIME [epoch: 34.1 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10359424839238417		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.10359424839238417 | validation: 0.10313333148659795]
	TIME [epoch: 34 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10408606080695051		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.10408606080695051 | validation: 0.1040883941973096]
	TIME [epoch: 34.1 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1036090020370055		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.1036090020370055 | validation: 0.1055330356421845]
	TIME [epoch: 34.1 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10503212438633092		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.10503212438633092 | validation: 0.10247688269281094]
	TIME [epoch: 34.1 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10390910384313205		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.10390910384313205 | validation: 0.10174239984663612]
	TIME [epoch: 34 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1054206200865786		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.1054206200865786 | validation: 0.1005372139096023]
	TIME [epoch: 34.1 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10382922311283561		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.10382922311283561 | validation: 0.0997219452132869]
	TIME [epoch: 34 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_978.pth
	Model improved!!!
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10275096606787126		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.10275096606787126 | validation: 0.10194337535259163]
	TIME [epoch: 34.1 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10362243765375063		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.10362243765375063 | validation: 0.10008280534134953]
	TIME [epoch: 34.1 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10316929502856327		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.10316929502856327 | validation: 0.10199335969524037]
	TIME [epoch: 34.1 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10338485224918349		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.10338485224918349 | validation: 0.1024124165966802]
	TIME [epoch: 34.1 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1040307109301624		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.1040307109301624 | validation: 0.10161749305799517]
	TIME [epoch: 34.1 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10476925852768282		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.10476925852768282 | validation: 0.10056071407439017]
	TIME [epoch: 34 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10323118021389854		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.10323118021389854 | validation: 0.10527509956603204]
	TIME [epoch: 34 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10493733775736108		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.10493733775736108 | validation: 0.10237981522631279]
	TIME [epoch: 34 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10435791851232398		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.10435791851232398 | validation: 0.10247537096214435]
	TIME [epoch: 34.1 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1035077216898619		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.1035077216898619 | validation: 0.10151976808149893]
	TIME [epoch: 34.1 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10397402003208997		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.10397402003208997 | validation: 0.10468717060863177]
	TIME [epoch: 34 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10583274198101045		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.10583274198101045 | validation: 0.10494588891181174]
	TIME [epoch: 34.1 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10326570997499518		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.10326570997499518 | validation: 0.10326951657862687]
	TIME [epoch: 34.1 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10362533302905416		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.10362533302905416 | validation: 0.1027727486991688]
	TIME [epoch: 34.1 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10492179190937997		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.10492179190937997 | validation: 0.10000973928198248]
	TIME [epoch: 34.1 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10288159417833878		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.10288159417833878 | validation: 0.10041778948168016]
	TIME [epoch: 34.1 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10361521901251125		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.10361521901251125 | validation: 0.1027079408779089]
	TIME [epoch: 34 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10447360884746008		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.10447360884746008 | validation: 0.10306867027234554]
	TIME [epoch: 34 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10328897825833559		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.10328897825833559 | validation: 0.1040200378827411]
	TIME [epoch: 34 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10410165222288957		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.10410165222288957 | validation: 0.10288710220977529]
	TIME [epoch: 34 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10316103900336807		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.10316103900336807 | validation: 0.10326136010279535]
	TIME [epoch: 34.1 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10317837772868924		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.10317837772868924 | validation: 0.10228764058697826]
	TIME [epoch: 34.1 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10424864513499757		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.10424864513499757 | validation: 0.09975341224558149]
	TIME [epoch: 165 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10446958193734211		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.10446958193734211 | validation: 0.10039217898965883]
	TIME [epoch: 73.2 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10300118642952838		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.10300118642952838 | validation: 0.10077618785344511]
	TIME [epoch: 73 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10381068936022375		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.10381068936022375 | validation: 0.10110181460689466]
	TIME [epoch: 73 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10343547040005893		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.10343547040005893 | validation: 0.1016142170704431]
	TIME [epoch: 73 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10357583545681352		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.10357583545681352 | validation: 0.10149262929806008]
	TIME [epoch: 73 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10349486603411008		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.10349486603411008 | validation: 0.10145266586113566]
	TIME [epoch: 73 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10252947222705289		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.10252947222705289 | validation: 0.09976555811000129]
	TIME [epoch: 73 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10357882612935967		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.10357882612935967 | validation: 0.10192666190888441]
	TIME [epoch: 73 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10411675534829254		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.10411675534829254 | validation: 0.10375441254413842]
	TIME [epoch: 73 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10384690719083553		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.10384690719083553 | validation: 0.10453439727826058]
	TIME [epoch: 73 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10593622330575775		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.10593622330575775 | validation: 0.0999919669514453]
	TIME [epoch: 73 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10171945106363998		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.10171945106363998 | validation: 0.10203820189040068]
	TIME [epoch: 73.1 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10242475245767522		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.10242475245767522 | validation: 0.1011569964848254]
	TIME [epoch: 73.1 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10543992338201363		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.10543992338201363 | validation: 0.10245333114602362]
	TIME [epoch: 73.1 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10372615081044512		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.10372615081044512 | validation: 0.10162565893497535]
	TIME [epoch: 73.1 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10378113321988372		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.10378113321988372 | validation: 0.10185201287575871]
	TIME [epoch: 73.1 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10295198843710993		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.10295198843710993 | validation: 0.09887828407777915]
	TIME [epoch: 73.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_1018.pth
	Model improved!!!
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10236168956083		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.10236168956083 | validation: 0.10227885686510171]
	TIME [epoch: 73.1 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10285972980873767		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.10285972980873767 | validation: 0.10238983608443167]
	TIME [epoch: 73.1 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10349604416203882		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.10349604416203882 | validation: 0.10014647808077362]
	TIME [epoch: 73.2 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10682303724295397		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.10682303724295397 | validation: 0.1005167416151944]
	TIME [epoch: 73.1 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10482332180499031		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.10482332180499031 | validation: 0.1002071234841158]
	TIME [epoch: 73.2 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1020095356699863		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.1020095356699863 | validation: 0.10086698892664658]
	TIME [epoch: 73.1 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10237716369117984		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.10237716369117984 | validation: 0.10144153685737986]
	TIME [epoch: 73.2 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10311855969819994		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.10311855969819994 | validation: 0.10179824521512852]
	TIME [epoch: 73.1 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10270347459895632		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.10270347459895632 | validation: 0.10006760651805925]
	TIME [epoch: 73.1 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10378775975327427		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.10378775975327427 | validation: 0.10036111171196253]
	TIME [epoch: 73.1 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10569237247478239		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.10569237247478239 | validation: 0.09884262740812821]
	TIME [epoch: 73.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_1029.pth
	Model improved!!!
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10203025581154328		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.10203025581154328 | validation: 0.09756929309373749]
	TIME [epoch: 73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_1030.pth
	Model improved!!!
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10341094709937726		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.10341094709937726 | validation: 0.10355480628161312]
	TIME [epoch: 73 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10237681728449598		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.10237681728449598 | validation: 0.09906939805793655]
	TIME [epoch: 73 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10342428398061779		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.10342428398061779 | validation: 0.10176365151853724]
	TIME [epoch: 73 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10288730002593312		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.10288730002593312 | validation: 0.09937712976772958]
	TIME [epoch: 73 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10331933797782579		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.10331933797782579 | validation: 0.10456985435660993]
	TIME [epoch: 73 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10420277448908066		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.10420277448908066 | validation: 0.10000223127110022]
	TIME [epoch: 73 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10266686897260094		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.10266686897260094 | validation: 0.10040082741218777]
	TIME [epoch: 73 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10185287050328808		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.10185287050328808 | validation: 0.09998663959936877]
	TIME [epoch: 73 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10187285060637151		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.10187285060637151 | validation: 0.10295427228578746]
	TIME [epoch: 73 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10314012466230396		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.10314012466230396 | validation: 0.09938453108709658]
	TIME [epoch: 73 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10295906828231915		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.10295906828231915 | validation: 0.10128294226355178]
	TIME [epoch: 73 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10326888465967414		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.10326888465967414 | validation: 0.09918695042065086]
	TIME [epoch: 73 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10377610023026386		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.10377610023026386 | validation: 0.10043956239026908]
	TIME [epoch: 73 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10218867726215611		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.10218867726215611 | validation: 0.10216766864551288]
	TIME [epoch: 73 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10239792450016404		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.10239792450016404 | validation: 0.09963817524874097]
	TIME [epoch: 73 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10282951214629893		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.10282951214629893 | validation: 0.09904219826931826]
	TIME [epoch: 73 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10232726823177488		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.10232726823177488 | validation: 0.10168710516592736]
	TIME [epoch: 73 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10319284527033475		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.10319284527033475 | validation: 0.10138521394850544]
	TIME [epoch: 73 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10193918125667176		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.10193918125667176 | validation: 0.10277202816546557]
	TIME [epoch: 73 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10302055507158674		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.10302055507158674 | validation: 0.10075126342642292]
	TIME [epoch: 73 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10392642121840494		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.10392642121840494 | validation: 0.101010458028958]
	TIME [epoch: 73 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10420247734933286		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.10420247734933286 | validation: 0.10277571308890099]
	TIME [epoch: 73 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10342701162831743		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.10342701162831743 | validation: 0.09966384077791755]
	TIME [epoch: 73 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10197133654378392		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.10197133654378392 | validation: 0.10403066731482538]
	TIME [epoch: 73 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10405159099646831		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.10405159099646831 | validation: 0.09990750148391717]
	TIME [epoch: 73 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1013592797921781		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.1013592797921781 | validation: 0.10080178704210047]
	TIME [epoch: 73 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10371411463934038		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.10371411463934038 | validation: 0.10189982491235722]
	TIME [epoch: 73 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10346290959285025		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.10346290959285025 | validation: 0.09929010017912138]
	TIME [epoch: 73 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10316229562723617		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.10316229562723617 | validation: 0.09753195782758856]
	TIME [epoch: 73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_1059.pth
	Model improved!!!
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10357802791703911		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.10357802791703911 | validation: 0.10081942274132447]
	TIME [epoch: 73 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1028844232238152		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.1028844232238152 | validation: 0.10059604505611479]
	TIME [epoch: 73 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10170468545750486		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.10170468545750486 | validation: 0.0999759523269027]
	TIME [epoch: 73 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10353910021997925		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.10353910021997925 | validation: 0.10024704530414072]
	TIME [epoch: 72.9 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10267064094148834		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.10267064094148834 | validation: 0.10086744691364528]
	TIME [epoch: 73 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10239099458634186		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.10239099458634186 | validation: 0.09864826241085116]
	TIME [epoch: 73 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10377887517711633		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.10377887517711633 | validation: 0.09817164024668162]
	TIME [epoch: 73 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10126639986462488		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.10126639986462488 | validation: 0.10028786312740356]
	TIME [epoch: 73 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10265744860812714		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.10265744860812714 | validation: 0.10018824071608125]
	TIME [epoch: 73.1 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10253787836526987		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.10253787836526987 | validation: 0.10113890637904496]
	TIME [epoch: 73.1 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10307588759543437		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.10307588759543437 | validation: 0.09963816673808512]
	TIME [epoch: 73 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10282801479629677		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.10282801479629677 | validation: 0.09993718599728793]
	TIME [epoch: 73.1 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10340036281520741		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.10340036281520741 | validation: 0.10193023935640586]
	TIME [epoch: 73 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10428844948624577		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.10428844948624577 | validation: 0.10222850594081533]
	TIME [epoch: 73 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10212142753058984		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.10212142753058984 | validation: 0.09878011269190787]
	TIME [epoch: 73 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10207714820024348		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.10207714820024348 | validation: 0.09940366816939174]
	TIME [epoch: 73 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10171233295996432		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.10171233295996432 | validation: 0.09935865446259093]
	TIME [epoch: 72.9 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10341520317187572		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.10341520317187572 | validation: 0.09888080571625998]
	TIME [epoch: 73 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10142635735682237		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.10142635735682237 | validation: 0.1009239015317466]
	TIME [epoch: 73 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10384116153700976		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.10384116153700976 | validation: 0.09875410011164226]
	TIME [epoch: 73 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1029036982722136		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.1029036982722136 | validation: 0.10047663349562978]
	TIME [epoch: 72.9 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10154041478338433		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.10154041478338433 | validation: 0.09870590268852836]
	TIME [epoch: 73 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10149057039460727		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.10149057039460727 | validation: 0.0989205610206244]
	TIME [epoch: 73 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1033104269214877		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.1033104269214877 | validation: 0.0995361121307051]
	TIME [epoch: 73 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10346461747801133		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.10346461747801133 | validation: 0.0990564939262243]
	TIME [epoch: 72.9 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10249460689216193		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.10249460689216193 | validation: 0.09820651914309778]
	TIME [epoch: 73 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10184909027162194		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.10184909027162194 | validation: 0.09962017780711137]
	TIME [epoch: 73 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10180569144100696		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.10180569144100696 | validation: 0.10048176260835168]
	TIME [epoch: 73 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10300255228931865		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.10300255228931865 | validation: 0.09821077349304869]
	TIME [epoch: 73 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1026997637620894		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.1026997637620894 | validation: 0.09816543324286228]
	TIME [epoch: 73 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10223102405400807		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.10223102405400807 | validation: 0.10083408140317959]
	TIME [epoch: 73 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1015603698713433		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.1015603698713433 | validation: 0.09800164652825]
	TIME [epoch: 73 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10212289082883896		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.10212289082883896 | validation: 0.09944875322895244]
	TIME [epoch: 73 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1023567559704122		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.1023567559704122 | validation: 0.0988226805123617]
	TIME [epoch: 73 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10138088606672324		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.10138088606672324 | validation: 0.10036615606852377]
	TIME [epoch: 73 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10287993207615054		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.10287993207615054 | validation: 0.10089319100027835]
	TIME [epoch: 73 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1019484134672861		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.1019484134672861 | validation: 0.09964356826856866]
	TIME [epoch: 73 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10249721123409508		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.10249721123409508 | validation: 0.10196963795785201]
	TIME [epoch: 73 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10082405335953265		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.10082405335953265 | validation: 0.09945576718590679]
	TIME [epoch: 73 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10265000067401404		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.10265000067401404 | validation: 0.10166899320363942]
	TIME [epoch: 73.1 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10340070081552483		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.10340070081552483 | validation: 0.09870474606917903]
	TIME [epoch: 73.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10158233086312088		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.10158233086312088 | validation: 0.09775885569989876]
	TIME [epoch: 73.2 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10168063221137799		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.10168063221137799 | validation: 0.10038305942678502]
	TIME [epoch: 73.2 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10293891231247035		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.10293891231247035 | validation: 0.09856479421462813]
	TIME [epoch: 73.2 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10217670769337248		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.10217670769337248 | validation: 0.10080862660908381]
	TIME [epoch: 73.2 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10185889508070245		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.10185889508070245 | validation: 0.09988789492486247]
	TIME [epoch: 73.2 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10121812803276582		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.10121812803276582 | validation: 0.10018029728212124]
	TIME [epoch: 73.2 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10402579989371423		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.10402579989371423 | validation: 0.10080361317983111]
	TIME [epoch: 73.2 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1018596758037531		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.1018596758037531 | validation: 0.0982586828145558]
	TIME [epoch: 73 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10146610187665236		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.10146610187665236 | validation: 0.09998125088446796]
	TIME [epoch: 73 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10209315813937361		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.10209315813937361 | validation: 0.09936158599969708]
	TIME [epoch: 73 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10087019185397333		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.10087019185397333 | validation: 0.09678365800310605]
	TIME [epoch: 73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_1111.pth
	Model improved!!!
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1018397695962856		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.1018397695962856 | validation: 0.0996215521079899]
	TIME [epoch: 73 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10187213603661949		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.10187213603661949 | validation: 0.09784579637735119]
	TIME [epoch: 73.2 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1023656516436029		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.1023656516436029 | validation: 0.09978036729715198]
	TIME [epoch: 73.2 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10200734827930358		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.10200734827930358 | validation: 0.09695723436825768]
	TIME [epoch: 73.3 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10119735633023126		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.10119735633023126 | validation: 0.0994814123657776]
	TIME [epoch: 73.2 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10143842877011215		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.10143842877011215 | validation: 0.09903403198485286]
	TIME [epoch: 73.2 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10209928507887622		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.10209928507887622 | validation: 0.09945261210838258]
	TIME [epoch: 73.2 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1024921550733961		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.1024921550733961 | validation: 0.10051954940266995]
	TIME [epoch: 73.2 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10256244713043497		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.10256244713043497 | validation: 0.09900489626437209]
	TIME [epoch: 73.2 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10212608685155851		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.10212608685155851 | validation: 0.09760572073712245]
	TIME [epoch: 73.1 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10103916817394117		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.10103916817394117 | validation: 0.09606398345131793]
	TIME [epoch: 73.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_1122.pth
	Model improved!!!
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10228070621709964		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.10228070621709964 | validation: 0.10082614293745992]
	TIME [epoch: 73.3 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10236982838727406		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.10236982838727406 | validation: 0.09883657256197863]
	TIME [epoch: 73.2 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10162340581932143		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.10162340581932143 | validation: 0.0983251512521358]
	TIME [epoch: 73.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10186209804061336		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.10186209804061336 | validation: 0.09711333756411958]
	TIME [epoch: 73.2 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10175726429397743		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.10175726429397743 | validation: 0.09804311611685246]
	TIME [epoch: 73.1 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10136638166499817		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.10136638166499817 | validation: 0.10068536906119381]
	TIME [epoch: 73 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10184942129991904		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.10184942129991904 | validation: 0.09812758653792239]
	TIME [epoch: 73 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10197616059628148		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.10197616059628148 | validation: 0.0995604777447627]
	TIME [epoch: 73.1 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10148563298283315		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.10148563298283315 | validation: 0.09939849264400019]
	TIME [epoch: 73 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1024678107106608		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.1024678107106608 | validation: 0.09747038356821978]
	TIME [epoch: 73.1 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10057548453992622		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.10057548453992622 | validation: 0.10009110770787116]
	TIME [epoch: 73 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1017509035944538		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.1017509035944538 | validation: 0.09961694053196286]
	TIME [epoch: 73 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10048601866885865		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.10048601866885865 | validation: 0.09984870633344232]
	TIME [epoch: 72.9 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1018033168070231		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.1018033168070231 | validation: 0.09831595325862162]
	TIME [epoch: 73.1 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10085535302789045		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.10085535302789045 | validation: 0.0977124540420079]
	TIME [epoch: 73 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10230023417733379		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.10230023417733379 | validation: 0.09769568195706003]
	TIME [epoch: 73 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10188787399739324		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.10188787399739324 | validation: 0.09871240624323373]
	TIME [epoch: 73 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10115108559178738		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.10115108559178738 | validation: 0.0980400528010773]
	TIME [epoch: 73.1 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10209344410483658		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.10209344410483658 | validation: 0.09791684223631994]
	TIME [epoch: 73.1 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10105032046966532		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.10105032046966532 | validation: 0.10042341328149768]
	TIME [epoch: 73.1 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10229868830399047		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.10229868830399047 | validation: 0.09641816630958558]
	TIME [epoch: 73 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1013131783867478		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.1013131783867478 | validation: 0.09801144188966092]
	TIME [epoch: 73 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10182686653543846		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.10182686653543846 | validation: 0.09820347622131535]
	TIME [epoch: 73 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10124188622949282		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.10124188622949282 | validation: 0.09953807967522832]
	TIME [epoch: 73 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10229556065465041		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.10229556065465041 | validation: 0.10079346048683893]
	TIME [epoch: 73.1 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10216890691173193		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.10216890691173193 | validation: 0.1003264156675784]
	TIME [epoch: 73.1 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10127731466563682		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.10127731466563682 | validation: 0.09843792678997791]
	TIME [epoch: 73.1 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10254529658786259		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.10254529658786259 | validation: 0.10042039165080821]
	TIME [epoch: 73.1 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10123624836702441		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.10123624836702441 | validation: 0.09885481132519339]
	TIME [epoch: 73.1 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10161816732286229		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.10161816732286229 | validation: 0.09774604165584576]
	TIME [epoch: 73 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10119804977484409		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.10119804977484409 | validation: 0.09941683157025416]
	TIME [epoch: 73 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10150964432280227		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.10150964432280227 | validation: 0.0984379635275074]
	TIME [epoch: 73.1 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10155998822907078		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.10155998822907078 | validation: 0.09921933714443859]
	TIME [epoch: 73 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10102166758771897		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.10102166758771897 | validation: 0.09745527276055677]
	TIME [epoch: 73 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10074729310155682		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.10074729310155682 | validation: 0.09895465901700644]
	TIME [epoch: 73.1 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10109764486586942		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.10109764486586942 | validation: 0.09909435239590425]
	TIME [epoch: 73.1 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10158479542959514		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.10158479542959514 | validation: 0.09947812845906853]
	TIME [epoch: 73.1 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10172452838399683		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.10172452838399683 | validation: 0.09824130069667078]
	TIME [epoch: 73 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1021045731016994		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.1021045731016994 | validation: 0.0977524563566509]
	TIME [epoch: 73.1 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10040385915279289		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.10040385915279289 | validation: 0.09944390471677753]
	TIME [epoch: 73 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1008495261883634		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.1008495261883634 | validation: 0.10095575405382987]
	TIME [epoch: 73.1 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10169430627125711		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.10169430627125711 | validation: 0.09996920042909396]
	TIME [epoch: 73.1 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10140524693943415		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.10140524693943415 | validation: 0.09832361927128208]
	TIME [epoch: 73.1 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1013705474985689		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.1013705474985689 | validation: 0.0969380869850312]
	TIME [epoch: 72.9 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10094608926514921		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.10094608926514921 | validation: 0.09824257590679981]
	TIME [epoch: 73.1 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10133812680397147		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.10133812680397147 | validation: 0.09905988797765913]
	TIME [epoch: 73.1 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10334886492639905		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 0.10334886492639905 | validation: 0.0979617403911675]
	TIME [epoch: 73 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10115852966307384		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.10115852966307384 | validation: 0.10041566859373272]
	TIME [epoch: 73.1 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10145039974808792		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.10145039974808792 | validation: 0.09769832849031432]
	TIME [epoch: 73.1 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1015669276554132		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.1015669276554132 | validation: 0.09743785767703732]
	TIME [epoch: 73.1 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10089510495289555		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.10089510495289555 | validation: 0.09783257278770796]
	TIME [epoch: 73 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1011618277659337		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.1011618277659337 | validation: 0.09841208629408776]
	TIME [epoch: 73 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10173732458402476		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.10173732458402476 | validation: 0.09888099590161117]
	TIME [epoch: 72.9 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10091032928756333		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.10091032928756333 | validation: 0.100032388897739]
	TIME [epoch: 73 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10108912601281735		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.10108912601281735 | validation: 0.09766166474464703]
	TIME [epoch: 73 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10098121810391789		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.10098121810391789 | validation: 0.09917208982984824]
	TIME [epoch: 73 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10014151010892663		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.10014151010892663 | validation: 0.09743649952464518]
	TIME [epoch: 72.9 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10126551023222283		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.10126551023222283 | validation: 0.09850537011549931]
	TIME [epoch: 73 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10109819669683769		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.10109819669683769 | validation: 0.0965657147850513]
	TIME [epoch: 72.9 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10120628322390156		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.10120628322390156 | validation: 0.09975722167301582]
	TIME [epoch: 72.9 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10136885849197481		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.10136885849197481 | validation: 0.09727305796053906]
	TIME [epoch: 72.9 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10042516504852655		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.10042516504852655 | validation: 0.099229001801479]
	TIME [epoch: 73 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10148315193986215		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.10148315193986215 | validation: 0.09916579493494591]
	TIME [epoch: 72.9 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10075356081392331		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.10075356081392331 | validation: 0.0985300928964214]
	TIME [epoch: 72.9 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10115713266556553		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.10115713266556553 | validation: 0.09964909895035344]
	TIME [epoch: 73 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1024822747781772		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.1024822747781772 | validation: 0.09824858973404016]
	TIME [epoch: 72.9 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1009688848043894		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.1009688848043894 | validation: 0.09898949863828455]
	TIME [epoch: 73 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10144454826101283		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.10144454826101283 | validation: 0.09785718793050956]
	TIME [epoch: 72.9 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10087536461765656		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.10087536461765656 | validation: 0.09680495309198818]
	TIME [epoch: 73 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1005186364271515		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.1005186364271515 | validation: 0.0968662435116734]
	TIME [epoch: 73 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10178751464929037		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.10178751464929037 | validation: 0.09824914531054199]
	TIME [epoch: 73 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10067568362011435		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.10067568362011435 | validation: 0.09585943732141519]
	TIME [epoch: 73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_1194.pth
	Model improved!!!
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10097729032193427		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.10097729032193427 | validation: 0.09652826574342102]
	TIME [epoch: 72.9 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1010917572874106		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.1010917572874106 | validation: 0.09807814932738435]
	TIME [epoch: 72.9 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10171791763353605		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.10171791763353605 | validation: 0.0995816022480919]
	TIME [epoch: 73 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10194669311785656		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.10194669311785656 | validation: 0.0958979723401581]
	TIME [epoch: 73 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10000998361437355		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.10000998361437355 | validation: 0.0987339005231834]
	TIME [epoch: 73 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10180959866062682		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.10180959866062682 | validation: 0.09787621282586961]
	TIME [epoch: 73 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10088673672572965		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.10088673672572965 | validation: 0.09682311616769088]
	TIME [epoch: 73 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10119089232917051		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.10119089232917051 | validation: 0.09694877668565585]
	TIME [epoch: 73 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09983429700358047		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.09983429700358047 | validation: 0.09651294736063697]
	TIME [epoch: 73 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10138948803644705		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.10138948803644705 | validation: 0.09766560808852273]
	TIME [epoch: 73 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10190864456536243		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.10190864456536243 | validation: 0.09788724099161453]
	TIME [epoch: 73 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10138591003292693		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.10138591003292693 | validation: 0.09787078756000986]
	TIME [epoch: 73 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10015853365773704		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.10015853365773704 | validation: 0.09976910801786057]
	TIME [epoch: 73 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10120474434464277		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.10120474434464277 | validation: 0.09573826344520031]
	TIME [epoch: 73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_1208.pth
	Model improved!!!
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10098888173798895		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.10098888173798895 | validation: 0.09690804063957578]
	TIME [epoch: 73.1 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1021931459493467		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.1021931459493467 | validation: 0.09567195049884283]
	TIME [epoch: 73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd7_20240713_103448/states/model_phiq_1a_v_mmd7_1210.pth
	Model improved!!!
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10146406841370001		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.10146406841370001 | validation: 0.09644206393172655]
	TIME [epoch: 73 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10116010913804		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.10116010913804 | validation: 0.09722114399523243]
	TIME [epoch: 73 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10126058181436041		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.10126058181436041 | validation: 0.098097786948947]
	TIME [epoch: 72.9 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10144911671686868		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.10144911671686868 | validation: 0.10061937212553175]
	TIME [epoch: 73 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10179811580660388		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.10179811580660388 | validation: 0.09650009470888855]
	TIME [epoch: 73 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1005009932274459		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.1005009932274459 | validation: 0.09668483302331565]
	TIME [epoch: 72.9 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1007702580635784		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.1007702580635784 | validation: 0.09910710348089355]
	TIME [epoch: 73 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10124737819998869		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.10124737819998869 | validation: 0.09727183051319888]
	TIME [epoch: 73 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10039507408126963		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.10039507408126963 | validation: 0.09718808859239546]
	TIME [epoch: 73 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10040578892559326		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.10040578892559326 | validation: 0.09652133277019048]
	TIME [epoch: 73 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10035275128998647		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.10035275128998647 | validation: 0.09588335362465601]
	TIME [epoch: 73 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10073378486941578		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.10073378486941578 | validation: 0.09773615430992012]
	TIME [epoch: 73 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10156521501349408		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.10156521501349408 | validation: 0.10000269810601975]
	TIME [epoch: 73 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10088074708415615		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.10088074708415615 | validation: 0.09700730752308398]
	TIME [epoch: 73 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10046458986254406		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.10046458986254406 | validation: 0.09596816582622386]
	TIME [epoch: 73 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10123424979978915		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.10123424979978915 | validation: 0.09756821209184494]
	TIME [epoch: 73 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10082701350816231		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.10082701350816231 | validation: 0.09899317275965669]
	TIME [epoch: 73 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10184581560732996		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.10184581560732996 | validation: 0.09842654807636203]
	TIME [epoch: 73 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10084585265764837		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.10084585265764837 | validation: 0.09878015476502673]
	TIME [epoch: 73 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10066715049122663		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.10066715049122663 | validation: 0.09722682031292787]
	TIME [epoch: 73 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10077389968366945		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.10077389968366945 | validation: 0.09776983213314408]
	TIME [epoch: 73 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10099718966349251		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.10099718966349251 | validation: 0.09618291717573065]
	TIME [epoch: 73 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1014581495018644		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.1014581495018644 | validation: 0.09581669873846238]
	TIME [epoch: 73 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10063502767689397		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.10063502767689397 | validation: 0.09692325952087294]
	TIME [epoch: 73 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1010380193462349		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.1010380193462349 | validation: 0.09732648452267509]
	TIME [epoch: 73 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10112331143110703		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.10112331143110703 | validation: 0.09799179399288983]
	TIME [epoch: 73 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10020527607258821		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.10020527607258821 | validation: 0.0964623918139298]
	TIME [epoch: 73 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10120041190084687		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.10120041190084687 | validation: 0.09719802715609678]
	TIME [epoch: 73 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10115439201701344		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.10115439201701344 | validation: 0.10087218810733518]
	TIME [epoch: 73 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10097627037954661		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.10097627037954661 | validation: 0.09740458030059973]
	TIME [epoch: 73 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10052417369579195		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.10052417369579195 | validation: 0.09701230833137009]
	TIME [epoch: 73 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1008634568908619		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.1008634568908619 | validation: 0.09640560615143799]
	TIME [epoch: 73 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10102295506890811		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.10102295506890811 | validation: 0.0969031153471645]
	TIME [epoch: 73 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10073626630874066		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.10073626630874066 | validation: 0.09778368696624648]
	TIME [epoch: 73 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10113178786170819		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.10113178786170819 | validation: 0.09585441138287235]
	TIME [epoch: 73 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10113398063410833		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.10113398063410833 | validation: 0.0969865448119284]
	TIME [epoch: 73 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.100556052832768		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.100556052832768 | validation: 0.0961833216208005]
	TIME [epoch: 73 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10163691943885243		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.10163691943885243 | validation: 0.09784286645430038]
	TIME [epoch: 73 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10012552876932358		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.10012552876932358 | validation: 0.09609385346638902]
	TIME [epoch: 73 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10056574692732391		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.10056574692732391 | validation: 0.0979238946775208]
	TIME [epoch: 73 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10146313180481478		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.10146313180481478 | validation: 0.09696311340191707]
	TIME [epoch: 72.9 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10062898664057893		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.10062898664057893 | validation: 0.09754194914802874]
	TIME [epoch: 72.9 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10107256376788921		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.10107256376788921 | validation: 0.09637611077705993]
	TIME [epoch: 73 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10096601319578521		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.10096601319578521 | validation: 0.0983560116022208]
	TIME [epoch: 73 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10086954022198519		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.10086954022198519 | validation: 0.0981285689437159]
	TIME [epoch: 73 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10084171085295128		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.10084171085295128 | validation: 0.09832428844557528]
	TIME [epoch: 73 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10152890355231925		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.10152890355231925 | validation: 0.09778399523938644]
	TIME [epoch: 73 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10056973740615929		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.10056973740615929 | validation: 0.09893632832105394]
	TIME [epoch: 73 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.101042320374743		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.101042320374743 | validation: 0.0977197502875476]
	TIME [epoch: 73 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09994634507491815		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.09994634507491815 | validation: 0.09733394247228633]
	TIME [epoch: 73 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1012718918987125		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.1012718918987125 | validation: 0.09735689373407193]
	TIME [epoch: 73 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10089956956951407		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.10089956956951407 | validation: 0.09744569973383217]
	TIME [epoch: 73 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10038363611949278		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.10038363611949278 | validation: 0.09668016044734046]
	TIME [epoch: 73 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10064740542349407		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.10064740542349407 | validation: 0.09685503011587832]
	TIME [epoch: 73 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10125204996567164		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.10125204996567164 | validation: 0.09718146130672253]
	TIME [epoch: 73 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1000393038276591		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.1000393038276591 | validation: 0.09661812163366783]
	TIME [epoch: 73 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10025440303503974		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.10025440303503974 | validation: 0.0970075430318578]
	TIME [epoch: 73 sec]
EPOCH 1268/2000:
	Training over batches...
