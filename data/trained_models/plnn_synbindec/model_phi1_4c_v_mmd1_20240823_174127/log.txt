Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2200252400

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.7020938219925545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.7020938219925545 | validation: 6.260401993096749]
	TIME [epoch: 46.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.233153184584799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.233153184584799 | validation: 5.1580025818863335]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.202816248779352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.202816248779352 | validation: 4.397718472271172]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.462208265076123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.462208265076123 | validation: 5.464697146642935]
	TIME [epoch: 3.77 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.949532300437554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.949532300437554 | validation: 4.417090288290786]
	TIME [epoch: 3.78 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.217011979837336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.217011979837336 | validation: 4.50185091455977]
	TIME [epoch: 3.76 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5293080203787275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5293080203787275 | validation: 4.70437235035907]
	TIME [epoch: 3.77 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.667573401133228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.667573401133228 | validation: 4.2361717234727285]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.211555299206051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.211555299206051 | validation: 4.406826688709542]
	TIME [epoch: 3.75 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.352889245042048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.352889245042048 | validation: 4.289983853224569]
	TIME [epoch: 3.75 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.065147242790723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.065147242790723 | validation: 4.164497130977722]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.015402816436099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.015402816436099 | validation: 4.071315871704406]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9835006756057125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9835006756057125 | validation: 4.046878899508705]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9400604902034604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9400604902034604 | validation: 4.053018109291261]
	TIME [epoch: 3.78 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.914083601134419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.914083601134419 | validation: 4.024099454901872]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.886176185274389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.886176185274389 | validation: 3.9849400287771086]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.859280233599482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.859280233599482 | validation: 3.9503348988647207]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.83246329210774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.83246329210774 | validation: 3.944650550948498]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.804608766507229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.804608766507229 | validation: 3.9149452327060636]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.789077537024989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.789077537024989 | validation: 3.9456737128808506]
	TIME [epoch: 3.77 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8116834937822204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8116834937822204 | validation: 4.076899552041297]
	TIME [epoch: 3.77 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9728927158797958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9728927158797958 | validation: 4.191765252528602]
	TIME [epoch: 3.78 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.037047621669544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.037047621669544 | validation: 3.859891242599234]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7170762803248873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7170762803248873 | validation: 3.979223651399329]
	TIME [epoch: 3.77 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.886893105796405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.886893105796405 | validation: 3.9243615510530625]
	TIME [epoch: 3.77 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.772283279429361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.772283279429361 | validation: 3.8097967819479734]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6655431767864073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6655431767864073 | validation: 3.8017246541877743]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.697067121255438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.697067121255438 | validation: 3.8063048646671067]
	TIME [epoch: 3.79 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.652994259961074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.652994259961074 | validation: 3.7382858263149163]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6017213231680194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6017213231680194 | validation: 3.7109558880717284]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.591557345568234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.591557345568234 | validation: 3.724559280740447]
	TIME [epoch: 3.78 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5769964943299932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5769964943299932 | validation: 3.680240615826739]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5616879582724636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5616879582724636 | validation: 3.6826510346395995]
	TIME [epoch: 3.8 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5538658361916733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5538658361916733 | validation: 3.6710916287999424]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5475379556269004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5475379556269004 | validation: 3.686563439831809]
	TIME [epoch: 3.77 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5601124365829966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5601124365829966 | validation: 3.618901223816176]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.495663112079543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.495663112079543 | validation: 3.606327662314788]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4595678242469736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4595678242469736 | validation: 3.562849656899228]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.422916964465128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.422916964465128 | validation: 3.5584303556014194]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.410718995156387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.410718995156387 | validation: 3.5187302234149516]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.412835552830957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.412835552830957 | validation: 3.6127643773641296]
	TIME [epoch: 3.77 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.452792764181568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.452792764181568 | validation: 3.499505455180912]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.39683878021553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.39683878021553 | validation: 3.489306120951545]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3588071980105907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3588071980105907 | validation: 3.4369677332563167]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3081075716632418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3081075716632418 | validation: 3.4195629802160625]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.283957606566427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.283957606566427 | validation: 3.3751817112852915]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.265807875921514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.265807875921514 | validation: 3.3926344991220576]
	TIME [epoch: 3.75 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.252843759791528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.252843759791528 | validation: 3.3376093779592004]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2430588732424326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2430588732424326 | validation: 3.383597547113467]
	TIME [epoch: 3.76 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2531954428049192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2531954428049192 | validation: 3.3477709704766223]
	TIME [epoch: 3.76 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.279886789026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.279886789026 | validation: 3.4424388855323738]
	TIME [epoch: 3.76 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2916388598736566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2916388598736566 | validation: 3.263144290819436]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1709475803566636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1709475803566636 | validation: 3.267226190590924]
	TIME [epoch: 3.77 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1514026119277116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1514026119277116 | validation: 3.242587640284302]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1378112964449714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1378112964449714 | validation: 3.2131656486902695]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1065075881292574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1065075881292574 | validation: 3.269192169902735]
	TIME [epoch: 3.78 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1171077814171655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1171077814171655 | validation: 3.279754026226117]
	TIME [epoch: 3.77 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2532473394199006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2532473394199006 | validation: 3.247191660704779]
	TIME [epoch: 3.77 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1554162365278318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1554162365278318 | validation: 3.1710898125790816]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.046744233417052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.046744233417052 | validation: 3.181385712092682]
	TIME [epoch: 3.78 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.124427318629343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.124427318629343 | validation: 3.3555817113810087]
	TIME [epoch: 3.77 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1829793987190795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1829793987190795 | validation: 3.087126308550456]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.046192460777572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.046192460777572 | validation: 3.0802068912101364]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9917269498111465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9917269498111465 | validation: 3.178931692108298]
	TIME [epoch: 3.76 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0168929794875035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0168929794875035 | validation: 3.056905975956933]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0280012163663264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0280012163663264 | validation: 3.094791246957863]
	TIME [epoch: 3.76 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9742817652038083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9742817652038083 | validation: 3.018039993161463]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9290999118750576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9290999118750576 | validation: 3.0841328427694363]
	TIME [epoch: 3.77 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9479700375061704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9479700375061704 | validation: 3.1378418184774914]
	TIME [epoch: 3.78 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1159715769040157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1159715769040157 | validation: 3.083783721009947]
	TIME [epoch: 3.77 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9746136694159193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9746136694159193 | validation: 2.952213909377381]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8771633495983355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8771633495983355 | validation: 2.9722978606939927]
	TIME [epoch: 3.76 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.872369781763961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.872369781763961 | validation: 2.9506056576758044]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8407246218934925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8407246218934925 | validation: 2.921464228657599]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8794654458223885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8794654458223885 | validation: 4.0699196257900665]
	TIME [epoch: 3.76 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7604569972535486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7604569972535486 | validation: 2.9702148483270516]
	TIME [epoch: 3.76 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9493735794990052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9493735794990052 | validation: 2.980385021391017]
	TIME [epoch: 3.76 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9536043429715564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9536043429715564 | validation: 2.95122784460281]
	TIME [epoch: 3.77 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.86240837633489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.86240837633489 | validation: 2.9904016444609223]
	TIME [epoch: 3.76 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.868489525196709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.868489525196709 | validation: 2.9155154315013965]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.842967378003583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.842967378003583 | validation: 2.918357961929647]
	TIME [epoch: 3.77 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8313231673805013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8313231673805013 | validation: 2.90384062516679]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.816244259116281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.816244259116281 | validation: 2.8871024266490437]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.808775997299213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.808775997299213 | validation: 2.879548739529371]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.787877255384425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.787877255384425 | validation: 2.863434418334469]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7821601612954088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7821601612954088 | validation: 2.8947615528322497]
	TIME [epoch: 3.76 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7855322221603536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7855322221603536 | validation: 3.0941821496496296]
	TIME [epoch: 3.77 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0711876395355877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0711876395355877 | validation: 3.162871242681138]
	TIME [epoch: 3.76 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.975036731734168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.975036731734168 | validation: 2.861088377764586]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8305351493156823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8305351493156823 | validation: 2.8355836769022895]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7772795751204664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7772795751204664 | validation: 2.9183745387057143]
	TIME [epoch: 3.77 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.784365831139057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.784365831139057 | validation: 2.8501544523929887]
	TIME [epoch: 3.77 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.817859928026284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.817859928026284 | validation: 2.9829892886842]
	TIME [epoch: 3.78 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8448578230922767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8448578230922767 | validation: 2.839599966651136]
	TIME [epoch: 3.78 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.813699108766789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.813699108766789 | validation: 2.8613742076639177]
	TIME [epoch: 3.77 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7600989825831785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7600989825831785 | validation: 2.822159252540809]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7406674810266076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7406674810266076 | validation: 2.812511723903178]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7394581907222704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7394581907222704 | validation: 2.8491112460494863]
	TIME [epoch: 3.77 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7388924645870474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7388924645870474 | validation: 2.8220525249712445]
	TIME [epoch: 3.77 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7761219924450327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7761219924450327 | validation: 3.064563523911172]
	TIME [epoch: 3.77 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8929357569170953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8929357569170953 | validation: 2.8672812942149455]
	TIME [epoch: 3.75 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8329472053530913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8329472053530913 | validation: 2.78750119765634]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.717729308558601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.717729308558601 | validation: 2.7469065632710836]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6648528889945133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6648528889945133 | validation: 2.5059719154642544]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.399098436309842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.399098436309842 | validation: 2.2649643184099193]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1011892571383965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1011892571383965 | validation: 1.3186204307661051]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3950761646142444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3950761646142444 | validation: 1.651747519560415]
	TIME [epoch: 3.78 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5733349738147597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5733349738147597 | validation: 1.2718993377004733]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3798225777183362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3798225777183362 | validation: 1.749883536082114]
	TIME [epoch: 3.78 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6758824293253738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6758824293253738 | validation: 1.3149929782022607]
	TIME [epoch: 3.78 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.337435719792006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.337435719792006 | validation: 1.1052077513633414]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2141311596044457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2141311596044457 | validation: 0.9920909261616521]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0990148764863041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0990148764863041 | validation: 1.1608343445134321]
	TIME [epoch: 3.78 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.166433843791627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.166433843791627 | validation: 1.0534408215596394]
	TIME [epoch: 3.78 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1858464973368157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1858464973368157 | validation: 0.9656146844225199]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0633708755746099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0633708755746099 | validation: 1.0346933342226126]
	TIME [epoch: 3.76 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.061769621566707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.061769621566707 | validation: 0.9282195636571622]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9989256087229943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9989256087229943 | validation: 0.8863305447967442]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0145502737187753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0145502737187753 | validation: 0.9655773340756971]
	TIME [epoch: 3.76 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.019991448132642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.019991448132642 | validation: 0.8439602917350545]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9580699589249878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9580699589249878 | validation: 0.8468199371245526]
	TIME [epoch: 3.76 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9454138827244641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9454138827244641 | validation: 0.8760516448998242]
	TIME [epoch: 3.75 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9533424784770351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9533424784770351 | validation: 0.8255969648598627]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9349511044748667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9349511044748667 | validation: 0.8178550459172048]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9343770513777523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9343770513777523 | validation: 0.7975037016194149]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9171029158683408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9171029158683408 | validation: 0.8087906216072362]
	TIME [epoch: 3.77 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9249042220103075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9249042220103075 | validation: 0.9085725813219088]
	TIME [epoch: 3.77 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0031704953219245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0031704953219245 | validation: 0.8511209094476175]
	TIME [epoch: 3.77 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9491477138445592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9491477138445592 | validation: 0.8710319367963287]
	TIME [epoch: 3.77 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.006907968268104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.006907968268104 | validation: 0.8868780826913053]
	TIME [epoch: 3.77 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9798075186152848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9798075186152848 | validation: 0.8630174329921445]
	TIME [epoch: 3.77 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0045624649189688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0045624649189688 | validation: 0.8155452674247003]
	TIME [epoch: 3.77 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9257955284060024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9257955284060024 | validation: 0.8400166677853429]
	TIME [epoch: 3.77 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.952830767452549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.952830767452549 | validation: 0.9132920666168662]
	TIME [epoch: 3.79 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0858860165529638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0858860165529638 | validation: 0.8593783319966368]
	TIME [epoch: 3.78 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9520887671697489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9520887671697489 | validation: 0.7513454599176056]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8929487716287892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8929487716287892 | validation: 0.7828221286536912]
	TIME [epoch: 3.77 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8931129923843838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8931129923843838 | validation: 0.782098904248827]
	TIME [epoch: 3.77 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8967845872960163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8967845872960163 | validation: 0.7784830339802347]
	TIME [epoch: 3.77 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8822862711683012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8822862711683012 | validation: 0.7753102672722278]
	TIME [epoch: 3.77 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8803861678251458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8803861678251458 | validation: 0.7638199083910087]
	TIME [epoch: 3.77 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8729070190540087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8729070190540087 | validation: 0.8143745148627897]
	TIME [epoch: 3.77 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9181599902504466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9181599902504466 | validation: 0.9205588269993386]
	TIME [epoch: 3.77 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0792126048352808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0792126048352808 | validation: 0.7681499989825158]
	TIME [epoch: 3.77 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8847976399508675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8847976399508675 | validation: 0.8127566594336799]
	TIME [epoch: 3.79 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9117680613306596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9117680613306596 | validation: 0.9082114394417702]
	TIME [epoch: 3.78 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0679449133822139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0679449133822139 | validation: 0.7712932739855408]
	TIME [epoch: 3.77 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8572859073742586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8572859073742586 | validation: 0.7985568882333622]
	TIME [epoch: 3.77 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831588418136231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8831588418136231 | validation: 0.8716742179820166]
	TIME [epoch: 3.77 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9949419726011717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9949419726011717 | validation: 0.8120062398217418]
	TIME [epoch: 3.77 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9053947736117253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9053947736117253 | validation: 0.780107230177232]
	TIME [epoch: 3.77 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8578519737826502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8578519737826502 | validation: 0.7300148589065443]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8357547121529001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8357547121529001 | validation: 0.7530434351136283]
	TIME [epoch: 3.77 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168032561818431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8168032561818431 | validation: 0.7967340710582118]
	TIME [epoch: 3.76 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.826417891889907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.826417891889907 | validation: 0.7907999080207584]
	TIME [epoch: 3.77 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8738332999632149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8738332999632149 | validation: 1.3625154035774845]
	TIME [epoch: 3.78 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3127190954147228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3127190954147228 | validation: 0.7740212763841365]
	TIME [epoch: 3.77 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8607476282314928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8607476282314928 | validation: 0.8368303775475902]
	TIME [epoch: 4.04 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9471559402016153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9471559402016153 | validation: 0.7398307202276135]
	TIME [epoch: 3.77 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.823572717275157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.823572717275157 | validation: 0.7402801281992814]
	TIME [epoch: 3.77 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8296490413820929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8296490413820929 | validation: 0.9100715905358989]
	TIME [epoch: 3.76 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9352121545270976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9352121545270976 | validation: 0.976453594288511]
	TIME [epoch: 3.76 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.143498292699196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.143498292699196 | validation: 0.8300217501735585]
	TIME [epoch: 3.76 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8631872949805908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8631872949805908 | validation: 0.7277489945647364]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8283546826617821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8283546826617821 | validation: 0.7298357660952255]
	TIME [epoch: 3.78 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8228563220205718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8228563220205718 | validation: 0.735640986102704]
	TIME [epoch: 3.8 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7970421713678907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7970421713678907 | validation: 0.727327113786092]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7756062873514756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7756062873514756 | validation: 0.7255404685127959]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7716460670110743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7716460670110743 | validation: 0.7554796999018538]
	TIME [epoch: 3.77 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7780131027245326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7780131027245326 | validation: 0.8454046062062173]
	TIME [epoch: 3.77 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8508109016284939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8508109016284939 | validation: 0.9814633740120556]
	TIME [epoch: 3.77 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0165489400167698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0165489400167698 | validation: 0.8815304841562516]
	TIME [epoch: 3.78 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0346726923183254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0346726923183254 | validation: 0.7708841518131155]
	TIME [epoch: 3.77 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.816977951446855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.816977951446855 | validation: 0.7195002534455925]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859194452151439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7859194452151439 | validation: 0.7964510405684623]
	TIME [epoch: 3.75 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9206214784030982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9206214784030982 | validation: 0.7645743798265631]
	TIME [epoch: 3.75 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7782220583101015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7782220583101015 | validation: 0.7245047845601168]
	TIME [epoch: 3.77 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8096681300112397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8096681300112397 | validation: 0.7882661214512198]
	TIME [epoch: 3.76 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8107255018191485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8107255018191485 | validation: 0.7806263698455711]
	TIME [epoch: 3.75 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8763798829793886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8763798829793886 | validation: 0.8794052961649097]
	TIME [epoch: 3.75 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9747915133612344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9747915133612344 | validation: 0.6804411895469057]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7529102674538278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7529102674538278 | validation: 0.697869004970296]
	TIME [epoch: 3.76 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7842199860720793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7842199860720793 | validation: 0.7228147388853737]
	TIME [epoch: 3.76 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7713629423799082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7713629423799082 | validation: 0.6777609237442968]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7778056012014731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7778056012014731 | validation: 0.7742489418576304]
	TIME [epoch: 3.76 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896778745143948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7896778745143948 | validation: 0.6928694944345055]
	TIME [epoch: 3.77 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8062889828862555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8062889828862555 | validation: 0.8196408939897948]
	TIME [epoch: 3.77 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8212391317170734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8212391317170734 | validation: 0.7863904477758219]
	TIME [epoch: 3.84 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8623497804764673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8623497804764673 | validation: 0.7917310509692371]
	TIME [epoch: 3.78 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7616714083665995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7616714083665995 | validation: 0.7029706765762559]
	TIME [epoch: 3.75 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7068316266898613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7068316266898613 | validation: 0.689905422676708]
	TIME [epoch: 3.76 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7596187629886195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7596187629886195 | validation: 0.9349447265608172]
	TIME [epoch: 3.76 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.029781511115997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.029781511115997 | validation: 0.6516331430243061]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7225228759851284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7225228759851284 | validation: 0.6379940497932446]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7215043826904483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7215043826904483 | validation: 0.7315199900873393]
	TIME [epoch: 3.78 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7394957197698958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7394957197698958 | validation: 0.6830063851301078]
	TIME [epoch: 3.77 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7697942476600795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7697942476600795 | validation: 0.8212397893202358]
	TIME [epoch: 3.78 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8240518505478202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8240518505478202 | validation: 0.6610960306220295]
	TIME [epoch: 3.78 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.739185709124409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.739185709124409 | validation: 0.6441237762559457]
	TIME [epoch: 3.78 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.68287074816703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.68287074816703 | validation: 0.6835637768147689]
	TIME [epoch: 3.77 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6877782214655369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6877782214655369 | validation: 0.708009613984133]
	TIME [epoch: 49.5 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.812285433140176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.812285433140176 | validation: 0.7566610938296585]
	TIME [epoch: 8.18 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7627955950243385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7627955950243385 | validation: 0.6553850429472574]
	TIME [epoch: 8.17 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7738041072292446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7738041072292446 | validation: 0.7522447561865718]
	TIME [epoch: 8.19 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7788885373991907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7788885373991907 | validation: 0.63802727070775]
	TIME [epoch: 8.11 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7108220926411201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7108220926411201 | validation: 0.6777642480498951]
	TIME [epoch: 8.18 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.714061075501187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.714061075501187 | validation: 1.0101920995239002]
	TIME [epoch: 8.2 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9192860330226786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9192860330226786 | validation: 0.740016212377927]
	TIME [epoch: 8.14 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8073375678107066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8073375678107066 | validation: 0.6858282903158012]
	TIME [epoch: 8.15 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7101441918890177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7101441918890177 | validation: 0.6677039647250806]
	TIME [epoch: 8.15 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6762710653953183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6762710653953183 | validation: 0.6241001050044]
	TIME [epoch: 8.16 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6665665302217026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6665665302217026 | validation: 0.7464592647625253]
	TIME [epoch: 8.23 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.737008796041975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.737008796041975 | validation: 0.9061413977995056]
	TIME [epoch: 8.22 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0879308996029609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0879308996029609 | validation: 0.777704460867223]
	TIME [epoch: 8.21 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8417162737015268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8417162737015268 | validation: 0.6663965677473741]
	TIME [epoch: 8.21 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.710428276719636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.710428276719636 | validation: 0.6602251183388193]
	TIME [epoch: 8.2 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7228338217982466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7228338217982466 | validation: 0.6151661509956488]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6512967208528819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6512967208528819 | validation: 0.6383125071109881]
	TIME [epoch: 8.19 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6489044288098796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6489044288098796 | validation: 0.5966660950264802]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6959884662540309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6959884662540309 | validation: 0.7644647068508292]
	TIME [epoch: 8.16 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085947704050992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7085947704050992 | validation: 0.617150422345455]
	TIME [epoch: 8.15 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6965073484847645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6965073484847645 | validation: 0.6923789308323225]
	TIME [epoch: 8.16 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6702116172997962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6702116172997962 | validation: 0.6582246820852682]
	TIME [epoch: 8.16 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6917045442927562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6917045442927562 | validation: 0.5975676565728093]
	TIME [epoch: 8.17 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6166130816056428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6166130816056428 | validation: 0.6109640391506098]
	TIME [epoch: 8.16 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6294152320139729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6294152320139729 | validation: 0.6553895827556491]
	TIME [epoch: 8.15 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7498811274019487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7498811274019487 | validation: 0.8007542580357685]
	TIME [epoch: 8.16 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8835905053302895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8835905053302895 | validation: 0.5689066229470987]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459742828821504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6459742828821504 | validation: 0.5590896952979801]
	TIME [epoch: 8.15 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5926551544809437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5926551544809437 | validation: 0.6303189803154696]
	TIME [epoch: 8.17 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.622536853720159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.622536853720159 | validation: 0.6975031762756599]
	TIME [epoch: 8.19 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7034890645036112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7034890645036112 | validation: 0.8254129178941492]
	TIME [epoch: 8.19 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8596466683623447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8596466683623447 | validation: 0.6153788268434426]
	TIME [epoch: 8.19 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6733947017369696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6733947017369696 | validation: 0.9829040023060227]
	TIME [epoch: 8.19 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.93967373810385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.93967373810385 | validation: 0.5834737080565757]
	TIME [epoch: 8.17 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6430052069384045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6430052069384045 | validation: 0.6239574273603117]
	TIME [epoch: 8.2 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6462291009535674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6462291009535674 | validation: 0.5873924055917924]
	TIME [epoch: 8.16 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.584328370266929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.584328370266929 | validation: 0.6128127929363156]
	TIME [epoch: 8.22 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5810481659385005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5810481659385005 | validation: 0.5842560341091062]
	TIME [epoch: 8.18 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6485181414273545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6485181414273545 | validation: 0.7597105555948771]
	TIME [epoch: 8.18 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900707973527068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7900707973527068 | validation: 0.6288758116143489]
	TIME [epoch: 8.19 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6805384900027704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6805384900027704 | validation: 0.6083099458165381]
	TIME [epoch: 8.18 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6155680886550521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6155680886550521 | validation: 0.5622205579506009]
	TIME [epoch: 8.21 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5652922517847158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5652922517847158 | validation: 0.5297402741841908]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5430980618905542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5430980618905542 | validation: 0.733515999672282]
	TIME [epoch: 8.19 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6617482952558362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6617482952558362 | validation: 0.7819975015027927]
	TIME [epoch: 8.18 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8316075296441834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8316075296441834 | validation: 0.6690390329960616]
	TIME [epoch: 8.2 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057154947099707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8057154947099707 | validation: 0.731953094854101]
	TIME [epoch: 8.22 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7256710027136429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7256710027136429 | validation: 0.5645620236109892]
	TIME [epoch: 8.2 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5665912088238754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5665912088238754 | validation: 0.6032383469336825]
	TIME [epoch: 8.19 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5703967136156654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5703967136156654 | validation: 0.6861537303043586]
	TIME [epoch: 8.19 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7520461970404188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7520461970404188 | validation: 0.8142330266380988]
	TIME [epoch: 8.18 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.010667329815341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.010667329815341 | validation: 0.6177713582979787]
	TIME [epoch: 8.19 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7169838906173962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7169838906173962 | validation: 0.7540358469063909]
	TIME [epoch: 8.2 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9345509060081261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9345509060081261 | validation: 0.6066305503710435]
	TIME [epoch: 8.18 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6832716975206556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6832716975206556 | validation: 0.6514266854307573]
	TIME [epoch: 8.21 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6570538813294582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6570538813294582 | validation: 0.5121578001372126]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5570809158765401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5570809158765401 | validation: 0.6571795044642292]
	TIME [epoch: 8.19 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6343025866099886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6343025866099886 | validation: 0.6425019559707222]
	TIME [epoch: 8.18 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7009614589928356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7009614589928356 | validation: 0.518453034169443]
	TIME [epoch: 8.16 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.545420380015197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.545420380015197 | validation: 0.5964622547019366]
	TIME [epoch: 8.17 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5711567114536236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5711567114536236 | validation: 0.5427826503711924]
	TIME [epoch: 8.16 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5866548967266887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5866548967266887 | validation: 0.513721736271772]
	TIME [epoch: 8.18 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5087225566136858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5087225566136858 | validation: 0.5597295975087621]
	TIME [epoch: 8.19 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5232955133604295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5232955133604295 | validation: 0.48941733907620855]
	TIME [epoch: 8.17 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5342940580382104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5342940580382104 | validation: 0.49486982379345773]
	TIME [epoch: 8.15 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5001274965811203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5001274965811203 | validation: 0.47379524261792644]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49517897374734454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49517897374734454 | validation: 0.5115438700605534]
	TIME [epoch: 8.2 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49909997910738796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49909997910738796 | validation: 0.6654746885546088]
	TIME [epoch: 8.19 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7863460756766341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7863460756766341 | validation: 0.7114193560441315]
	TIME [epoch: 8.17 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8605821819798735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8605821819798735 | validation: 0.6125964194145722]
	TIME [epoch: 8.18 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6405820304755754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6405820304755754 | validation: 0.5051848222958698]
	TIME [epoch: 8.18 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5632734748814751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5632734748814751 | validation: 0.6826431925452199]
	TIME [epoch: 8.17 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6191990969640201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6191990969640201 | validation: 0.6813391239248943]
	TIME [epoch: 8.2 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7651601884972244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7651601884972244 | validation: 0.5209644216848359]
	TIME [epoch: 8.18 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5725626779834617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5725626779834617 | validation: 0.5079781230687436]
	TIME [epoch: 8.19 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5381922030649888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5381922030649888 | validation: 0.497142790567195]
	TIME [epoch: 8.19 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4929976936674626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4929976936674626 | validation: 0.4901863334858718]
	TIME [epoch: 8.18 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4954289267642629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4954289267642629 | validation: 0.5683386785684519]
	TIME [epoch: 8.19 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.588568985644311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.588568985644311 | validation: 0.6232179755758698]
	TIME [epoch: 8.19 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6897126667996013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6897126667996013 | validation: 0.5098950650917161]
	TIME [epoch: 8.19 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5682742496578422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5682742496578422 | validation: 0.5353976555709133]
	TIME [epoch: 8.18 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.521891662737445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.521891662737445 | validation: 0.4774658681287609]
	TIME [epoch: 8.19 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.603323242761919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.603323242761919 | validation: 0.7515991636545452]
	TIME [epoch: 8.18 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6381510730674956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6381510730674956 | validation: 0.5835493064996907]
	TIME [epoch: 8.2 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6626584620779469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6626584620779469 | validation: 0.48321435318178896]
	TIME [epoch: 8.19 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5677232592798293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5677232592798293 | validation: 0.519320494243722]
	TIME [epoch: 8.18 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4955411840149433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4955411840149433 | validation: 0.4409342552630827]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4333869174754048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4333869174754048 | validation: 0.5340116749608163]
	TIME [epoch: 8.19 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45662266194546436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45662266194546436 | validation: 0.44695844167364013]
	TIME [epoch: 8.18 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4939163672268835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4939163672268835 | validation: 0.4582764434832953]
	TIME [epoch: 8.22 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46267257296533937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46267257296533937 | validation: 0.3913920433638143]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40850512204598033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40850512204598033 | validation: 0.5659542647802219]
	TIME [epoch: 8.21 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4891483606988205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4891483606988205 | validation: 0.5003185737851009]
	TIME [epoch: 8.21 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6164987251399091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6164987251399091 | validation: 0.45475081506107706]
	TIME [epoch: 8.21 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5066520320641411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5066520320641411 | validation: 0.7213464484843691]
	TIME [epoch: 8.21 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6054123494995157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6054123494995157 | validation: 0.46831292637672145]
	TIME [epoch: 8.24 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5179505092398684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5179505092398684 | validation: 0.594143104609793]
	TIME [epoch: 8.19 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6491047255911992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6491047255911992 | validation: 0.5409196564361384]
	TIME [epoch: 8.21 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6457703138619013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6457703138619013 | validation: 0.39045438007330224]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3866225537771365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3866225537771365 | validation: 0.4636673375619725]
	TIME [epoch: 8.21 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43357162789422227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43357162789422227 | validation: 0.3685744726432673]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37969927030898837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37969927030898837 | validation: 0.3526339351823368]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35999587353454937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35999587353454937 | validation: 0.3704010537790701]
	TIME [epoch: 8.19 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3576772303761975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3576772303761975 | validation: 0.43311739526184034]
	TIME [epoch: 8.23 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3820152285011143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3820152285011143 | validation: 0.47557055246996716]
	TIME [epoch: 8.21 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5862996243384784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5862996243384784 | validation: 0.7068922777366948]
	TIME [epoch: 8.22 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6368740160198881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6368740160198881 | validation: 0.4417543619655246]
	TIME [epoch: 8.23 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5065678661942495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5065678661942495 | validation: 0.42303034075116097]
	TIME [epoch: 8.22 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44196122586813275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44196122586813275 | validation: 0.38483209938921686]
	TIME [epoch: 8.2 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3473570972865597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3473570972865597 | validation: 0.2949885749281222]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.373089392072255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.373089392072255 | validation: 0.582236067600061]
	TIME [epoch: 8.19 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44030649348428214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44030649348428214 | validation: 0.40899634291176645]
	TIME [epoch: 8.21 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5346794322497551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5346794322497551 | validation: 0.5392496356963856]
	TIME [epoch: 8.19 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42559133054194037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42559133054194037 | validation: 0.35339215271961527]
	TIME [epoch: 8.18 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3409979121155982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3409979121155982 | validation: 0.44230516773112466]
	TIME [epoch: 8.17 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5168279426141007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5168279426141007 | validation: 0.4422281696487902]
	TIME [epoch: 8.19 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38475615154030374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38475615154030374 | validation: 0.42183951846114126]
	TIME [epoch: 8.17 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4555937815084463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4555937815084463 | validation: 0.5096893810931316]
	TIME [epoch: 8.21 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4905088963264849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4905088963264849 | validation: 0.31818619711448765]
	TIME [epoch: 8.17 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3507756573371082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3507756573371082 | validation: 0.4565206715447845]
	TIME [epoch: 8.19 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3455513439902904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3455513439902904 | validation: 0.45203299750885595]
	TIME [epoch: 8.17 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5631604880087983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5631604880087983 | validation: 0.6608081830570907]
	TIME [epoch: 8.21 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5261101285784442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5261101285784442 | validation: 0.35722328061602404]
	TIME [epoch: 8.2 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35612721197643815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35612721197643815 | validation: 0.39811529699479453]
	TIME [epoch: 8.2 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47679181974636203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47679181974636203 | validation: 0.4635889364289205]
	TIME [epoch: 8.18 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3855908923760471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3855908923760471 | validation: 0.38347005849389837]
	TIME [epoch: 8.19 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3789216210712527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3789216210712527 | validation: 0.36655635751138355]
	TIME [epoch: 8.15 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42877460862558425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42877460862558425 | validation: 0.3492444460969389]
	TIME [epoch: 8.18 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29735587145090586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29735587145090586 | validation: 0.27272411553773107]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3113783839537113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3113783839537113 | validation: 0.49497998814269634]
	TIME [epoch: 8.18 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3763512856177874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3763512856177874 | validation: 0.28321373799065114]
	TIME [epoch: 8.17 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36863786085990446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36863786085990446 | validation: 0.5304918302082539]
	TIME [epoch: 8.17 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3766510673089615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3766510673089615 | validation: 0.2941121755023097]
	TIME [epoch: 8.18 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3740336109302422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3740336109302422 | validation: 0.42106042102726615]
	TIME [epoch: 8.19 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3270991407687995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3270991407687995 | validation: 0.30451053080707147]
	TIME [epoch: 8.18 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34542070669976555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34542070669976555 | validation: 0.2886313335187297]
	TIME [epoch: 8.17 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37796286728825323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37796286728825323 | validation: 0.8776733494103581]
	TIME [epoch: 8.15 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7126822516454877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7126822516454877 | validation: 0.5809937766222636]
	TIME [epoch: 8.18 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5955213073280917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5955213073280917 | validation: 0.4513195157859684]
	TIME [epoch: 8.17 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5660923195306061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5660923195306061 | validation: 0.3696223288654764]
	TIME [epoch: 8.2 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3425685973235635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3425685973235635 | validation: 0.4141635842872795]
	TIME [epoch: 8.15 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.382207602250544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.382207602250544 | validation: 0.3073032890260196]
	TIME [epoch: 8.18 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27495651455225933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27495651455225933 | validation: 0.2923341941484349]
	TIME [epoch: 8.17 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2909305653316526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2909305653316526 | validation: 0.25212738327063716]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28597791668246625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28597791668246625 | validation: 0.4214736829158498]
	TIME [epoch: 8.19 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2997960706525921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2997960706525921 | validation: 0.23462130959088703]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28169940966414425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28169940966414425 | validation: 0.35541211323544447]
	TIME [epoch: 8.18 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27270252688439467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27270252688439467 | validation: 0.3153336903828707]
	TIME [epoch: 8.18 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3574124941976983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3574124941976983 | validation: 0.2271800426148286]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26583646440091047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26583646440091047 | validation: 0.5174114210910216]
	TIME [epoch: 8.19 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35463271786618733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35463271786618733 | validation: 0.2113993869798624]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3001327473681507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3001327473681507 | validation: 0.43419419429993167]
	TIME [epoch: 8.19 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30114427756699697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30114427756699697 | validation: 0.28167204184990363]
	TIME [epoch: 8.19 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28734298005152153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28734298005152153 | validation: 0.37709946093284624]
	TIME [epoch: 8.21 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3971951052352625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3971951052352625 | validation: 0.442910169208211]
	TIME [epoch: 8.21 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5199338165542298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5199338165542298 | validation: 0.22315034190831798]
	TIME [epoch: 8.18 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29203129893346164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29203129893346164 | validation: 0.5049531112938178]
	TIME [epoch: 8.22 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36811470024601667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36811470024601667 | validation: 0.3232709814075967]
	TIME [epoch: 8.2 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38674554446538595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38674554446538595 | validation: 0.4033273127093945]
	TIME [epoch: 8.19 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32820628359799825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32820628359799825 | validation: 0.2323195051660231]
	TIME [epoch: 8.2 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23318226322744454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23318226322744454 | validation: 0.22495321492666132]
	TIME [epoch: 8.18 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24528629596147244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24528629596147244 | validation: 0.30241831807517183]
	TIME [epoch: 8.21 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23969862576781953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23969862576781953 | validation: 0.18565798984064563]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23180681065449357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23180681065449357 | validation: 0.31437801775839935]
	TIME [epoch: 8.17 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23006790155462584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23006790155462584 | validation: 0.18362488891293807]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23815819210495912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23815819210495912 | validation: 0.3683392588197049]
	TIME [epoch: 8.17 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.258666841462977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.258666841462977 | validation: 0.24371616840400373]
	TIME [epoch: 8.19 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3106151918576455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3106151918576455 | validation: 0.38626601440035574]
	TIME [epoch: 8.2 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25812998299909734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25812998299909734 | validation: 0.19359939612432792]
	TIME [epoch: 8.2 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24155958331666988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24155958331666988 | validation: 0.30858052934252067]
	TIME [epoch: 8.17 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2169478210912753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2169478210912753 | validation: 0.4030292371546648]
	TIME [epoch: 8.19 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38527701827936667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38527701827936667 | validation: 0.45713525488264595]
	TIME [epoch: 8.18 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.604868338862056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.604868338862056 | validation: 0.3995805959810077]
	TIME [epoch: 8.2 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5261321175645196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5261321175645196 | validation: 0.21998866383543533]
	TIME [epoch: 8.2 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30986811148433924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30986811148433924 | validation: 0.6507012587643918]
	TIME [epoch: 8.2 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5083215841842597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5083215841842597 | validation: 0.39231245996747727]
	TIME [epoch: 8.2 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.487532978808099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.487532978808099 | validation: 0.2736539406117799]
	TIME [epoch: 8.19 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27978140082873315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27978140082873315 | validation: 0.28845919638317336]
	TIME [epoch: 8.18 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2574347705104475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2574347705104475 | validation: 0.21506414594325715]
	TIME [epoch: 8.21 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21674506612824457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21674506612824457 | validation: 0.25690235888455115]
	TIME [epoch: 8.21 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19377542167986003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19377542167986003 | validation: 0.19682896751832285]
	TIME [epoch: 8.19 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1912504582794852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1912504582794852 | validation: 0.20421925621306194]
	TIME [epoch: 8.18 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17744880622540377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17744880622540377 | validation: 0.18397966431868895]
	TIME [epoch: 8.2 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17338899728692872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17338899728692872 | validation: 0.2821066513554013]
	TIME [epoch: 8.17 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18894355935199517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18894355935199517 | validation: 0.24325340910193405]
	TIME [epoch: 8.22 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3464731889503358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3464731889503358 | validation: 0.593824278734642]
	TIME [epoch: 8.19 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43782603260928143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43782603260928143 | validation: 0.2445661369327405]
	TIME [epoch: 8.19 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24399776347437396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24399776347437396 | validation: 0.21294556209889492]
	TIME [epoch: 8.2 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2646022322192442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2646022322192442 | validation: 0.3685343098210079]
	TIME [epoch: 8.2 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2961950203006354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2961950203006354 | validation: 0.4802901773389776]
	TIME [epoch: 8.2 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5137978855125476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5137978855125476 | validation: 0.24942576524691562]
	TIME [epoch: 8.19 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3346331693745269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3346331693745269 | validation: 0.3871073152285714]
	TIME [epoch: 8.19 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30020912280648565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30020912280648565 | validation: 0.23751654677998585]
	TIME [epoch: 8.2 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24962639968281408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24962639968281408 | validation: 0.18780898832222342]
	TIME [epoch: 8.19 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1676222369747603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1676222369747603 | validation: 0.17822910503426553]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17104088793169867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17104088793169867 | validation: 0.17674370458859917]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15728704335983845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15728704335983845 | validation: 0.16542417601079823]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16160145792223052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16160145792223052 | validation: 0.1869835934027462]
	TIME [epoch: 8.2 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1795146393144202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1795146393144202 | validation: 0.29120380840922655]
	TIME [epoch: 8.19 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21176723619281246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21176723619281246 | validation: 0.24775924100360652]
	TIME [epoch: 8.16 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31538039684403407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31538039684403407 | validation: 0.3174327801449912]
	TIME [epoch: 8.18 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22888261976542684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22888261976542684 | validation: 0.17723191699076982]
	TIME [epoch: 8.17 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20630865120010328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20630865120010328 | validation: 1.8417662175490912]
	TIME [epoch: 8.16 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.450431456291024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.450431456291024 | validation: 1.1262829117549082]
	TIME [epoch: 8.16 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8216531191723602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8216531191723602 | validation: 0.387128423981637]
	TIME [epoch: 8.19 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31000698781266783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31000698781266783 | validation: 0.23628383570661682]
	TIME [epoch: 8.17 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.302375685296413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.302375685296413 | validation: 0.2474355217033248]
	TIME [epoch: 8.21 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24340057685514183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24340057685514183 | validation: 0.2495340990829496]
	TIME [epoch: 8.18 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20794485750155706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20794485750155706 | validation: 0.2340210725976284]
	TIME [epoch: 8.19 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1854070698447175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1854070698447175 | validation: 0.19422492653487297]
	TIME [epoch: 8.18 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18023322069772701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18023322069772701 | validation: 0.2138900813567303]
	TIME [epoch: 8.18 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16701081426636233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16701081426636233 | validation: 0.16056988057605243]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15740078446943342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15740078446943342 | validation: 0.20165139467504237]
	TIME [epoch: 8.19 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15045608237316482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15045608237316482 | validation: 0.13536278344142624]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16327734075466546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16327734075466546 | validation: 0.23455581336919937]
	TIME [epoch: 8.2 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16083280848479894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16083280848479894 | validation: 0.17737793973330576]
	TIME [epoch: 8.21 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17943934860210412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17943934860210412 | validation: 0.24109971169240368]
	TIME [epoch: 8.2 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22502785287222246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22502785287222246 | validation: 0.24638687732557313]
	TIME [epoch: 8.23 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20612121680932372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20612121680932372 | validation: 0.11569434506048015]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14501961294076623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14501961294076623 | validation: 0.22010632092063442]
	TIME [epoch: 8.17 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14506147369554875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14506147369554875 | validation: 0.1318113525520937]
	TIME [epoch: 8.17 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17179120675858997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17179120675858997 | validation: 0.3487028178863681]
	TIME [epoch: 8.17 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20701542913470491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20701542913470491 | validation: 0.12474470986237739]
	TIME [epoch: 8.17 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21875861734399543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21875861734399543 | validation: 0.303349624480131]
	TIME [epoch: 8.16 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1802326358312906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1802326358312906 | validation: 0.1414820112309072]
	TIME [epoch: 8.16 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12895517746608703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12895517746608703 | validation: 0.11842099549225665]
	TIME [epoch: 8.17 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13130889882613742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13130889882613742 | validation: 0.20325727637206492]
	TIME [epoch: 8.17 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15578860328725605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15578860328725605 | validation: 0.2436693617318305]
	TIME [epoch: 8.16 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2429835261537017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2429835261537017 | validation: 0.2913769518565033]
	TIME [epoch: 8.18 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26702259392100886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26702259392100886 | validation: 0.23673912769946553]
	TIME [epoch: 8.13 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20415537220033542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20415537220033542 | validation: 0.14342811523844043]
	TIME [epoch: 8.17 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13769921709691257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13769921709691257 | validation: 0.2326602053322716]
	TIME [epoch: 8.14 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14335871579001622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14335871579001622 | validation: 0.09806837943098273]
	TIME [epoch: 8.17 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15087464141276402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15087464141276402 | validation: 0.432186158325667]
	TIME [epoch: 8.15 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2869802828374852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2869802828374852 | validation: 0.22726754265449545]
	TIME [epoch: 8.17 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2970102784056248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2970102784056248 | validation: 0.2231248989777604]
	TIME [epoch: 8.17 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18675599443761484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18675599443761484 | validation: 0.20049877335956762]
	TIME [epoch: 8.18 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16354775694012966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16354775694012966 | validation: 0.16669790525754685]
	TIME [epoch: 8.17 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16782890674480183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16782890674480183 | validation: 0.21636525782921462]
	TIME [epoch: 8.16 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14963720734163788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14963720734163788 | validation: 0.09566868566250478]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12389166149421843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12389166149421843 | validation: 0.1765882024355464]
	TIME [epoch: 8.16 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12064677606639163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12064677606639163 | validation: 0.10362198132842126]
	TIME [epoch: 8.17 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12805906739193568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12805906739193568 | validation: 0.3339730549093557]
	TIME [epoch: 8.16 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19009235629532556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19009235629532556 | validation: 0.1348941234855776]
	TIME [epoch: 8.18 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21644080616502745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21644080616502745 | validation: 0.3104730155122661]
	TIME [epoch: 8.17 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18395284671628254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18395284671628254 | validation: 0.15277088653282245]
	TIME [epoch: 8.2 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14365980546754464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14365980546754464 | validation: 0.14613697656005212]
	TIME [epoch: 8.15 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15380926161975095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15380926161975095 | validation: 0.3072558928432325]
	TIME [epoch: 8.17 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19073224581621598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19073224581621598 | validation: 0.19149714114964347]
	TIME [epoch: 8.15 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1978751229637998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1978751229637998 | validation: 0.20444111149727498]
	TIME [epoch: 8.17 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19058740375202668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19058740375202668 | validation: 0.15451875836944953]
	TIME [epoch: 8.17 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11645657199132298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11645657199132298 | validation: 0.18305822386861065]
	TIME [epoch: 8.18 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14232359425220298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14232359425220298 | validation: 0.14056205225390808]
	TIME [epoch: 8.16 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15093300287917058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15093300287917058 | validation: 0.22178218818757484]
	TIME [epoch: 8.17 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15361855042811867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15361855042811867 | validation: 0.13221865966165783]
	TIME [epoch: 8.15 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1762964718554117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1762964718554117 | validation: 0.3809206976325381]
	TIME [epoch: 8.2 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2122318816729517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2122318816729517 | validation: 0.15601262255064638]
	TIME [epoch: 8.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21434741214360176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21434741214360176 | validation: 0.22349952676434107]
	TIME [epoch: 8.17 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12945116070770843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12945116070770843 | validation: 0.17075802815627172]
	TIME [epoch: 8.17 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11711941598776018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11711941598776018 | validation: 0.13277465400917485]
	TIME [epoch: 8.18 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14954439886530968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14954439886530968 | validation: 0.2458557557291127]
	TIME [epoch: 8.17 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21541673818770668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21541673818770668 | validation: 0.22641052119900926]
	TIME [epoch: 8.16 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19084499412845668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19084499412845668 | validation: 0.1474760381404264]
	TIME [epoch: 8.17 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11908082447694916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11908082447694916 | validation: 0.16137793113934948]
	TIME [epoch: 8.15 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1828738528115006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1828738528115006 | validation: 0.16496542079032614]
	TIME [epoch: 8.17 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1482431561734842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1482431561734842 | validation: 0.15384285381453258]
	TIME [epoch: 8.15 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13528752834354976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13528752834354976 | validation: 0.10759008602356844]
	TIME [epoch: 8.16 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1176802318112743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1176802318112743 | validation: 0.1759714741700712]
	TIME [epoch: 8.17 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10592394495886005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10592394495886005 | validation: 0.113371653744376]
	TIME [epoch: 8.17 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16045342955794084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16045342955794084 | validation: 0.5602879791399461]
	TIME [epoch: 8.16 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3420803182324012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3420803182324012 | validation: 0.22571040015542462]
	TIME [epoch: 8.16 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2238369044942023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2238369044942023 | validation: 0.1722212557964901]
	TIME [epoch: 8.16 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.237602002089737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.237602002089737 | validation: 0.6128098134466776]
	TIME [epoch: 8.17 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3963017129722037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3963017129722037 | validation: 0.2228215944710261]
	TIME [epoch: 8.15 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19385413929810974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19385413929810974 | validation: 0.15775764662944913]
	TIME [epoch: 8.14 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17189904352724633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17189904352724633 | validation: 0.23543497559628335]
	TIME [epoch: 8.17 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13805582429823787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13805582429823787 | validation: 0.20036571830995636]
	TIME [epoch: 8.16 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.169100147482909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.169100147482909 | validation: 0.1964029521843982]
	TIME [epoch: 8.18 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15318811242279234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15318811242279234 | validation: 0.10764217759261299]
	TIME [epoch: 8.17 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1285785009093398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1285785009093398 | validation: 0.14086405012684178]
	TIME [epoch: 8.18 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11050715552769116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11050715552769116 | validation: 0.12545410828640585]
	TIME [epoch: 8.16 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10462298448345099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10462298448345099 | validation: 0.11604085867024938]
	TIME [epoch: 8.17 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0951829215927714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0951829215927714 | validation: 0.08762381665535608]
	TIME [epoch: 8.15 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09201420336746996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09201420336746996 | validation: 0.14314900974436826]
	TIME [epoch: 8.19 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09669034020358583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09669034020358583 | validation: 0.11578491316077916]
	TIME [epoch: 8.2 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13042793120995774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13042793120995774 | validation: 0.3441492527181178]
	TIME [epoch: 8.18 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2310605442244647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2310605442244647 | validation: 0.13890033950093852]
	TIME [epoch: 8.21 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17690978471497296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17690978471497296 | validation: 0.20728131808054284]
	TIME [epoch: 8.21 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12974048118628384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12974048118628384 | validation: 0.08827377700982329]
	TIME [epoch: 8.2 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11850789877901707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11850789877901707 | validation: 0.23888864110691332]
	TIME [epoch: 8.21 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1296103026580867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1296103026580867 | validation: 0.08088563014415046]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10727088411223601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10727088411223601 | validation: 0.15602420565223735]
	TIME [epoch: 8.21 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10045096381445592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10045096381445592 | validation: 0.09068174861202562]
	TIME [epoch: 8.19 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09146103689547115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09146103689547115 | validation: 0.1284895418633885]
	TIME [epoch: 8.21 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09183022685465275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09183022685465275 | validation: 0.09602140925568671]
	TIME [epoch: 8.16 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10679594202429166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10679594202429166 | validation: 0.26165707666438476]
	TIME [epoch: 8.21 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21414467659323147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21414467659323147 | validation: 0.2361872494807914]
	TIME [epoch: 8.17 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2380992591229836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2380992591229836 | validation: 0.18935998676230756]
	TIME [epoch: 8.16 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17944671150835323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17944671150835323 | validation: 0.4517051172711042]
	TIME [epoch: 8.16 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2866943135572813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2866943135572813 | validation: 0.1384662924144123]
	TIME [epoch: 59.1 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17895149531847623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17895149531847623 | validation: 0.34712443413547905]
	TIME [epoch: 17.5 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20870722920127002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20870722920127002 | validation: 0.12614046911202195]
	TIME [epoch: 17.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14577401967334003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14577401967334003 | validation: 0.16887147803198232]
	TIME [epoch: 17.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11774716447661032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11774716447661032 | validation: 0.09667586561806527]
	TIME [epoch: 17.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09486886931722943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09486886931722943 | validation: 0.148440771180127]
	TIME [epoch: 17.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09538279080273988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09538279080273988 | validation: 0.08304424093319206]
	TIME [epoch: 17.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09457339495151283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09457339495151283 | validation: 0.15319583722812355]
	TIME [epoch: 17.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10821192213430485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10821192213430485 | validation: 0.09036516752549217]
	TIME [epoch: 17.6 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11947216207700638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11947216207700638 | validation: 0.3505406868206604]
	TIME [epoch: 17.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18463928462487908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18463928462487908 | validation: 0.10671247254009418]
	TIME [epoch: 17.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1639887348778172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1639887348778172 | validation: 0.24371933130504236]
	TIME [epoch: 17.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14816239362301534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14816239362301534 | validation: 0.14681004349569912]
	TIME [epoch: 17.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11894088254967758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11894088254967758 | validation: 0.11782999733486706]
	TIME [epoch: 17.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12122791493947625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12122791493947625 | validation: 0.09819561257789662]
	TIME [epoch: 17.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09013129522127844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09013129522127844 | validation: 0.16757937747149343]
	TIME [epoch: 17.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09886461042679194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09886461042679194 | validation: 0.09646022460675836]
	TIME [epoch: 17.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11007879884106606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11007879884106606 | validation: 0.21590980817571329]
	TIME [epoch: 17.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1562185018410144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1562185018410144 | validation: 0.12036911172467223]
	TIME [epoch: 17.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15383911375421291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15383911375421291 | validation: 0.24162463928527933]
	TIME [epoch: 17.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13079782611586926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13079782611586926 | validation: 0.08600565018671519]
	TIME [epoch: 17.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1196088471689918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1196088471689918 | validation: 0.21861325618694322]
	TIME [epoch: 17.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11546374898393427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11546374898393427 | validation: 0.09428235401577251]
	TIME [epoch: 17.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1237437053270751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1237437053270751 | validation: 0.3189690456227275]
	TIME [epoch: 17.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1688635439312821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1688635439312821 | validation: 0.10584509863457799]
	TIME [epoch: 17.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12072053197258356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12072053197258356 | validation: 0.10518552234393388]
	TIME [epoch: 17.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10584695503369566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10584695503369566 | validation: 0.14066962876734487]
	TIME [epoch: 17.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11236426027861349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11236426027861349 | validation: 0.13195233018943037]
	TIME [epoch: 17.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10946461035072823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10946461035072823 | validation: 0.1350915421980203]
	TIME [epoch: 17.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259037075423749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1259037075423749 | validation: 0.2044369931313184]
	TIME [epoch: 17.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17579256922181855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17579256922181855 | validation: 0.10920283634337075]
	TIME [epoch: 17.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14206808465050696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14206808465050696 | validation: 0.2147717443095452]
	TIME [epoch: 17.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11274122023713705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11274122023713705 | validation: 0.10238731232965007]
	TIME [epoch: 17.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14616213951485832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14616213951485832 | validation: 0.39453544766262366]
	TIME [epoch: 17.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19249155715308186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19249155715308186 | validation: 0.10520324283293095]
	TIME [epoch: 17.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11987526703060077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11987526703060077 | validation: 0.11360055541703061]
	TIME [epoch: 17.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09684425719213671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09684425719213671 | validation: 0.10705044454802787]
	TIME [epoch: 17.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0870807920817905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0870807920817905 | validation: 0.08305507159917155]
	TIME [epoch: 17.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07688396181836486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07688396181836486 | validation: 0.07298086906359207]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07614043012179161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07614043012179161 | validation: 0.1346393492426673]
	TIME [epoch: 17.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11672933182741138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11672933182741138 | validation: 0.24100787709748792]
	TIME [epoch: 17.6 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22417915787309275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22417915787309275 | validation: 0.22424814672063775]
	TIME [epoch: 17.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1595859218676399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1595859218676399 | validation: 0.06655981735535392]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_543.pth
	Model improved!!!
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12308132097244343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12308132097244343 | validation: 0.36213495010838265]
	TIME [epoch: 17.5 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21075823714229674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21075823714229674 | validation: 0.0831469730845238]
	TIME [epoch: 17.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12096569881271521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12096569881271521 | validation: 0.09706972898163622]
	TIME [epoch: 17.5 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07878624875565517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07878624875565517 | validation: 0.08581227415251652]
	TIME [epoch: 17.5 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07828193492624194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07828193492624194 | validation: 0.09098639092023358]
	TIME [epoch: 17.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07570863919660391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07570863919660391 | validation: 0.08984779005957906]
	TIME [epoch: 17.5 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10134227869624461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10134227869624461 | validation: 0.29975270206822996]
	TIME [epoch: 17.5 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18644484508001583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18644484508001583 | validation: 0.15633483754931973]
	TIME [epoch: 17.5 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17368627603421832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17368627603421832 | validation: 0.17674528902537756]
	TIME [epoch: 17.5 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10636264755497415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10636264755497415 | validation: 0.1016653791404033]
	TIME [epoch: 17.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10670216891436717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10670216891436717 | validation: 0.06478041560164788]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07630661963274885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07630661963274885 | validation: 0.11176537385351802]
	TIME [epoch: 17.5 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07241018827729763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07241018827729763 | validation: 0.06412423768109936]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0723054357100965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0723054357100965 | validation: 0.1361392803412745]
	TIME [epoch: 17.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07931890888291677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07931890888291677 | validation: 0.0845744011966571]
	TIME [epoch: 17.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11875361135488886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11875361135488886 | validation: 0.4378982551388849]
	TIME [epoch: 17.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2215089767321471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2215089767321471 | validation: 0.0975296084692979]
	TIME [epoch: 17.5 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11441767887589738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11441767887589738 | validation: 0.09266552026222984]
	TIME [epoch: 17.5 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10603998683788644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10603998683788644 | validation: 0.11449501404926539]
	TIME [epoch: 17.5 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1075164225052574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1075164225052574 | validation: 0.16670577703490627]
	TIME [epoch: 17.5 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12096231803370992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12096231803370992 | validation: 0.21492405205293375]
	TIME [epoch: 17.6 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22746803090159712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22746803090159712 | validation: 0.15339797412648004]
	TIME [epoch: 17.5 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13019310670269096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13019310670269096 | validation: 0.06278825997782532]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07381422382322378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07381422382322378 | validation: 0.15471903840690338]
	TIME [epoch: 17.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09104849976025627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09104849976025627 | validation: 0.06462164072992145]
	TIME [epoch: 17.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08029179745892781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08029179745892781 | validation: 0.15855101357539525]
	TIME [epoch: 17.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09775764736314518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09775764736314518 | validation: 0.09133670973772384]
	TIME [epoch: 17.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12490237313586203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12490237313586203 | validation: 0.28739403104997063]
	TIME [epoch: 17.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1607789237635663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1607789237635663 | validation: 0.07553231238389]
	TIME [epoch: 17.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1047374607588575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1047374607588575 | validation: 0.11010601287742966]
	TIME [epoch: 17.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08976533952805178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08976533952805178 | validation: 0.08321010385427441]
	TIME [epoch: 17.5 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07459944259640826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07459944259640826 | validation: 0.09188419214250348]
	TIME [epoch: 17.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07678931640020249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07678931640020249 | validation: 0.07082012240669729]
	TIME [epoch: 17.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08301901235414853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08301901235414853 | validation: 0.13242174598869982]
	TIME [epoch: 17.6 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10231219433809251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10231219433809251 | validation: 0.11731130634765657]
	TIME [epoch: 17.5 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.149315835704243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.149315835704243 | validation: 0.26314275728458447]
	TIME [epoch: 17.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16066149528342968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16066149528342968 | validation: 0.06485793128788336]
	TIME [epoch: 17.5 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0958023518504535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0958023518504535 | validation: 0.2727456700044579]
	TIME [epoch: 17.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.151217605087908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.151217605087908 | validation: 0.12308842296929168]
	TIME [epoch: 17.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1308160335681666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1308160335681666 | validation: 0.09140261643914005]
	TIME [epoch: 17.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06966183643654265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06966183643654265 | validation: 0.0771749676144869]
	TIME [epoch: 17.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06665004205280632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06665004205280632 | validation: 0.055329809009072306]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07250572371977546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07250572371977546 | validation: 0.23731387610554122]
	TIME [epoch: 17.5 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12758660959497792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12758660959497792 | validation: 0.16455601885409532]
	TIME [epoch: 17.4 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20939668915123696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20939668915123696 | validation: 0.2927409090386978]
	TIME [epoch: 17.5 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13183537867678102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13183537867678102 | validation: 0.1148775559868439]
	TIME [epoch: 17.5 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09864974843774064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09864974843774064 | validation: 0.06952566727171121]
	TIME [epoch: 17.6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08287650983875895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08287650983875895 | validation: 0.14044843213966826]
	TIME [epoch: 17.5 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07665065936572342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07665065936572342 | validation: 0.08247304395221072]
	TIME [epoch: 17.5 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07319780605985686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07319780605985686 | validation: 0.07343189846391547]
	TIME [epoch: 17.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06963908989686406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06963908989686406 | validation: 0.10443157905720249]
	TIME [epoch: 17.5 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09567660408450944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09567660408450944 | validation: 0.10030186379507261]
	TIME [epoch: 17.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11950298378651741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11950298378651741 | validation: 0.26304399084563695]
	TIME [epoch: 17.5 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17922581083485742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17922581083485742 | validation: 0.11478978571378094]
	TIME [epoch: 17.5 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14613134432304115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14613134432304115 | validation: 0.18208794556380214]
	TIME [epoch: 17.5 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12248523414322769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12248523414322769 | validation: 0.07020231022457356]
	TIME [epoch: 17.5 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08755089779877258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08755089779877258 | validation: 0.20404891770132272]
	TIME [epoch: 17.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10732248824013745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10732248824013745 | validation: 0.10105369307807642]
	TIME [epoch: 17.5 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12709423416110732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12709423416110732 | validation: 0.30550098151205024]
	TIME [epoch: 17.5 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14908345233796166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14908345233796166 | validation: 0.06407362690852418]
	TIME [epoch: 17.5 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0781945705557702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0781945705557702 | validation: 0.17893235197532037]
	TIME [epoch: 17.5 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1992078797202987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1992078797202987 | validation: 0.23225157010286368]
	TIME [epoch: 17.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13587905368644965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13587905368644965 | validation: 0.07736308037025708]
	TIME [epoch: 17.5 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0744668657655239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0744668657655239 | validation: 0.12561329337727498]
	TIME [epoch: 17.5 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10232505153227774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10232505153227774 | validation: 0.1147978884965319]
	TIME [epoch: 17.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14908509626203162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14908509626203162 | validation: 0.18964858685911748]
	TIME [epoch: 17.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1268788342488381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1268788342488381 | validation: 0.051668529015655884]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06426548431142085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06426548431142085 | validation: 0.06806708015035177]
	TIME [epoch: 17.5 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06785441864769066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06785441864769066 | validation: 0.08448432358875337]
	TIME [epoch: 17.6 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06786005430150914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06786005430150914 | validation: 0.048979245890237645]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05974104946918202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05974104946918202 | validation: 0.09421393478662698]
	TIME [epoch: 17.6 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07001870761094602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07001870761094602 | validation: 0.07892856007027832]
	TIME [epoch: 17.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10222121363489745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10222121363489745 | validation: 0.28566452830536576]
	TIME [epoch: 17.5 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16133355932961185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16133355932961185 | validation: 0.05955232128770016]
	TIME [epoch: 17.5 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10383807775983556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10383807775983556 | validation: 0.16598291440308896]
	TIME [epoch: 17.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0883442049382904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0883442049382904 | validation: 0.11733997013484752]
	TIME [epoch: 17.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12496896226305783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12496896226305783 | validation: 0.13120815831134244]
	TIME [epoch: 17.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11593036982806361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11593036982806361 | validation: 0.07044681736758888]
	TIME [epoch: 17.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07856348054506063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07856348054506063 | validation: 0.08956392624940743]
	TIME [epoch: 17.6 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0803128413077092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0803128413077092 | validation: 0.049513687260232045]
	TIME [epoch: 17.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06850315321077874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06850315321077874 | validation: 0.207202626424686]
	TIME [epoch: 17.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10892219416085935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10892219416085935 | validation: 0.07543180086162173]
	TIME [epoch: 17.5 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10917903977132593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10917903977132593 | validation: 0.19542556126819896]
	TIME [epoch: 17.6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1056857605373369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1056857605373369 | validation: 0.06854054541597981]
	TIME [epoch: 17.5 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.088213950214828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.088213950214828 | validation: 0.16391591580380474]
	TIME [epoch: 17.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10068241720036078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10068241720036078 | validation: 0.05944717744662266]
	TIME [epoch: 17.5 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06650285290736212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06650285290736212 | validation: 0.1056351183298089]
	TIME [epoch: 17.5 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09925880286429674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09925880286429674 | validation: 0.07909434510934914]
	TIME [epoch: 17.5 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0982438025427429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0982438025427429 | validation: 0.11755288481904524]
	TIME [epoch: 17.5 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0970461221462672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0970461221462672 | validation: 0.07769098241322564]
	TIME [epoch: 17.5 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0947538543607476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0947538543607476 | validation: 0.14297780420528317]
	TIME [epoch: 17.5 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10141759725758619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10141759725758619 | validation: 0.1310709714162659]
	TIME [epoch: 17.5 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23119019794428183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23119019794428183 | validation: 0.26962162234712544]
	TIME [epoch: 17.5 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14782818815447626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14782818815447626 | validation: 0.0988760451659206]
	TIME [epoch: 17.4 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07263367033444137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07263367033444137 | validation: 0.051230481224309316]
	TIME [epoch: 17.5 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06719155531186495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06719155531186495 | validation: 0.07829285275110949]
	TIME [epoch: 17.5 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06798398836744189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06798398836744189 | validation: 0.18907074839370236]
	TIME [epoch: 17.5 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13989891943839755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13989891943839755 | validation: 0.11369352323130416]
	TIME [epoch: 17.5 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14574427920920818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14574427920920818 | validation: 0.15432995509974923]
	TIME [epoch: 17.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10528415207553916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10528415207553916 | validation: 0.0819169433417143]
	TIME [epoch: 17.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08675980545076119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08675980545076119 | validation: 0.09115336094334824]
	TIME [epoch: 17.5 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0775187579746823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0775187579746823 | validation: 0.05147398081456959]
	TIME [epoch: 17.5 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06686040073229269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06686040073229269 | validation: 0.06597495286019399]
	TIME [epoch: 17.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05053087479522834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05053087479522834 | validation: 0.04545919031853405]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_647.pth
	Model improved!!!
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058179235802470364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058179235802470364 | validation: 0.18396157098541926]
	TIME [epoch: 17.7 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11076928484532605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11076928484532605 | validation: 0.10691675155526668]
	TIME [epoch: 17.6 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17159871185036799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17159871185036799 | validation: 0.5296385968552736]
	TIME [epoch: 17.6 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2851368259526281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2851368259526281 | validation: 0.20344976212671073]
	TIME [epoch: 17.6 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11839868303948269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11839868303948269 | validation: 0.1619478429623069]
	TIME [epoch: 17.6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1919070754929647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1919070754929647 | validation: 0.09986858597796083]
	TIME [epoch: 17.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09630978919272686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09630978919272686 | validation: 0.06474077775620345]
	TIME [epoch: 17.6 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07507331372399938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07507331372399938 | validation: 0.06540946454253335]
	TIME [epoch: 17.6 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05954537980323383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05954537980323383 | validation: 0.059909381800246436]
	TIME [epoch: 17.6 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051742807759084776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051742807759084776 | validation: 0.059756868010064614]
	TIME [epoch: 17.6 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05146114503386382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05146114503386382 | validation: 0.06935324570141617]
	TIME [epoch: 17.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05530046044752588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05530046044752588 | validation: 0.0582422355000239]
	TIME [epoch: 17.6 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07474981194821011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07474981194821011 | validation: 0.11073869561568982]
	TIME [epoch: 17.6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0898242819439735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0898242819439735 | validation: 0.07104008267693517]
	TIME [epoch: 17.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08861774550350082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08861774550350082 | validation: 0.12727266075923974]
	TIME [epoch: 17.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10483808364357679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10483808364357679 | validation: 0.07049645702679384]
	TIME [epoch: 17.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08713289839818247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08713289839818247 | validation: 0.12437662752442205]
	TIME [epoch: 17.6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08331598236339444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08331598236339444 | validation: 0.04674799308182142]
	TIME [epoch: 17.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07474418618459266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07474418618459266 | validation: 0.134354676729153]
	TIME [epoch: 17.6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07615439570252824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07615439570252824 | validation: 0.05518190658473429]
	TIME [epoch: 17.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08010862555537038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08010862555537038 | validation: 0.23816863025841398]
	TIME [epoch: 17.6 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11039469950702872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11039469950702872 | validation: 0.08506565177387777]
	TIME [epoch: 17.6 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10987556838659387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10987556838659387 | validation: 0.3178972237225168]
	TIME [epoch: 17.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1588901871865753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1588901871865753 | validation: 0.07998256339664996]
	TIME [epoch: 17.6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04951111139200707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04951111139200707 | validation: 0.05598590843251943]
	TIME [epoch: 17.6 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06980200054674375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06980200054674375 | validation: 0.150539836270198]
	TIME [epoch: 17.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08139608086667548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08139608086667548 | validation: 0.06754951780745642]
	TIME [epoch: 17.6 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06913113753157317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06913113753157317 | validation: 0.059817704216570525]
	TIME [epoch: 17.6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060320115707861534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060320115707861534 | validation: 0.08165929705226248]
	TIME [epoch: 17.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09645675093893044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09645675093893044 | validation: 0.27941762385957974]
	TIME [epoch: 17.6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22118734580751734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22118734580751734 | validation: 0.08567757223233963]
	TIME [epoch: 17.6 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.093533832636262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.093533832636262 | validation: 0.0641859851680296]
	TIME [epoch: 17.6 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055438599114437145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055438599114437145 | validation: 0.03879472093743767]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0503759818114506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0503759818114506 | validation: 0.09407792169622471]
	TIME [epoch: 17.5 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061787775691983494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061787775691983494 | validation: 0.05263072683437362]
	TIME [epoch: 17.5 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07775549338287961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07775549338287961 | validation: 0.19830956926876686]
	TIME [epoch: 17.5 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.111386638862645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.111386638862645 | validation: 0.06500886770167567]
	TIME [epoch: 17.5 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09529442269083938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09529442269083938 | validation: 0.16117397649767604]
	TIME [epoch: 17.5 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08139603349008309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08139603349008309 | validation: 0.04607436275912944]
	TIME [epoch: 17.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06171780933662305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06171780933662305 | validation: 0.08057108635052318]
	TIME [epoch: 17.6 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056843079917675045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056843079917675045 | validation: 0.04109310716124055]
	TIME [epoch: 17.6 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049677391128247345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049677391128247345 | validation: 0.060224743425164644]
	TIME [epoch: 17.6 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048585186671844756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048585186671844756 | validation: 0.05630130620770029]
	TIME [epoch: 17.6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05302541491546091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05302541491546091 | validation: 0.11301532075223644]
	TIME [epoch: 17.6 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1128010472328237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1128010472328237 | validation: 0.1956582443363713]
	TIME [epoch: 17.5 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2250991989308506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2250991989308506 | validation: 0.19366274201519318]
	TIME [epoch: 17.5 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14326711340310372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14326711340310372 | validation: 0.17180698140833817]
	TIME [epoch: 17.5 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18617578213492947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18617578213492947 | validation: 0.29093680397273974]
	TIME [epoch: 17.5 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1569658010461574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1569658010461574 | validation: 0.10900770294907686]
	TIME [epoch: 17.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13839999783167645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13839999783167645 | validation: 0.1198368610267885]
	TIME [epoch: 17.5 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07795594481236653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07795594481236653 | validation: 0.07720452655978276]
	TIME [epoch: 17.5 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06094622180709768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06094622180709768 | validation: 0.04034268598663196]
	TIME [epoch: 17.5 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05907082283999443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05907082283999443 | validation: 0.07145445379392308]
	TIME [epoch: 17.5 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05135987557475586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05135987557475586 | validation: 0.035636289628430795]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_701.pth
	Model improved!!!
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04940582734953527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04940582734953527 | validation: 0.0699101396718964]
	TIME [epoch: 17.6 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0697731994435187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0697731994435187 | validation: 0.06334869154466069]
	TIME [epoch: 17.5 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557272906802882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07557272906802882 | validation: 0.10475647673845297]
	TIME [epoch: 17.6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06868251908786803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06868251908786803 | validation: 0.062400921111110036]
	TIME [epoch: 17.6 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10162354077536126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10162354077536126 | validation: 0.26368316331148406]
	TIME [epoch: 17.6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13573284034342933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13573284034342933 | validation: 0.05689522382129617]
	TIME [epoch: 17.5 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07698015069459545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07698015069459545 | validation: 0.05127487747773424]
	TIME [epoch: 17.5 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05380371478802243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05380371478802243 | validation: 0.03793296415035065]
	TIME [epoch: 17.6 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04274173993421051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04274173993421051 | validation: 0.040134697843452485]
	TIME [epoch: 17.6 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03711288179100056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03711288179100056 | validation: 0.04372237155623512]
	TIME [epoch: 17.6 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03861846368141223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03861846368141223 | validation: 0.03928022807202779]
	TIME [epoch: 17.6 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04632526463381261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04632526463381261 | validation: 0.08066702093528898]
	TIME [epoch: 17.6 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08320623994753323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08320623994753323 | validation: 0.14024349445057216]
	TIME [epoch: 17.5 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16570558675781555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16570558675781555 | validation: 0.2686186863186462]
	TIME [epoch: 17.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19581894080718162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19581894080718162 | validation: 0.193226875979087]
	TIME [epoch: 17.6 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2161632916755962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2161632916755962 | validation: 0.10119861959262827]
	TIME [epoch: 17.6 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07922420476304869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07922420476304869 | validation: 0.049148081344749295]
	TIME [epoch: 17.5 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06690944173552867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06690944173552867 | validation: 0.09088381527049927]
	TIME [epoch: 17.6 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08098981641615931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08098981641615931 | validation: 0.04958037401463999]
	TIME [epoch: 17.5 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06728711489040523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06728711489040523 | validation: 0.057076553509091514]
	TIME [epoch: 17.6 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057467849317034275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057467849317034275 | validation: 0.038378995757604596]
	TIME [epoch: 17.6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04320444731229782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04320444731229782 | validation: 0.0558996903572127]
	TIME [epoch: 17.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05244311857750985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05244311857750985 | validation: 0.06787287255528762]
	TIME [epoch: 17.6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0917861741287761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0917861741287761 | validation: 0.22895307586395774]
	TIME [epoch: 17.5 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.129550132094594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.129550132094594 | validation: 0.07188994704626782]
	TIME [epoch: 17.6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09691513912609143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09691513912609143 | validation: 0.07305318063201402]
	TIME [epoch: 17.6 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06358114442143689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06358114442143689 | validation: 0.053262176308648206]
	TIME [epoch: 17.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05630077147846017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05630077147846017 | validation: 0.04003448807143287]
	TIME [epoch: 17.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039695858261169885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039695858261169885 | validation: 0.03061775581848225]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_730.pth
	Model improved!!!
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03300907373165395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03300907373165395 | validation: 0.03444897089901883]
	TIME [epoch: 17.4 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034368789474634794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034368789474634794 | validation: 0.05044476388337396]
	TIME [epoch: 17.5 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06090521838039523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06090521838039523 | validation: 0.13517888107379392]
	TIME [epoch: 17.5 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10338463457805656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10338463457805656 | validation: 0.15099388666247152]
	TIME [epoch: 17.5 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1717862865747171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1717862865747171 | validation: 0.30981564839768805]
	TIME [epoch: 17.5 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16128399102772556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16128399102772556 | validation: 0.08275071365972589]
	TIME [epoch: 17.5 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0727284887202421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0727284887202421 | validation: 0.12063415372195121]
	TIME [epoch: 17.5 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11687995720741397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11687995720741397 | validation: 0.14744731137771439]
	TIME [epoch: 17.5 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16269588180505934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16269588180505934 | validation: 0.04185882344089325]
	TIME [epoch: 17.5 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04773469193277834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04773469193277834 | validation: 0.08457067902980128]
	TIME [epoch: 17.5 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05777818359799831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05777818359799831 | validation: 0.056195754573422886]
	TIME [epoch: 17.5 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07073550196165013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07073550196165013 | validation: 0.11766663532072404]
	TIME [epoch: 17.5 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09976772940725356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09976772940725356 | validation: 0.05739914007638326]
	TIME [epoch: 17.5 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06774118323527395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06774118323527395 | validation: 0.07170329172999554]
	TIME [epoch: 17.5 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04891817412137561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04891817412137561 | validation: 0.035973285950760506]
	TIME [epoch: 17.5 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04598019862610037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04598019862610037 | validation: 0.06434796952412991]
	TIME [epoch: 17.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0496862538232134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0496862538232134 | validation: 0.04207133660617632]
	TIME [epoch: 17.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0664504902907785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0664504902907785 | validation: 0.18087882149566503]
	TIME [epoch: 17.5 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1123677873821018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1123677873821018 | validation: 0.07590781756134725]
	TIME [epoch: 17.5 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039791926561303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1039791926561303 | validation: 0.089119404982133]
	TIME [epoch: 17.5 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09149980822922686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09149980822922686 | validation: 0.08925621703535258]
	TIME [epoch: 17.5 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06775550873393246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06775550873393246 | validation: 0.05123233178810258]
	TIME [epoch: 17.5 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059090440577051144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059090440577051144 | validation: 0.08947037880251243]
	TIME [epoch: 17.5 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05720528811562238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05720528811562238 | validation: 0.044741227690972475]
	TIME [epoch: 17.5 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061559033918011166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061559033918011166 | validation: 0.0976692899963492]
	TIME [epoch: 17.5 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07951446874051041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07951446874051041 | validation: 0.05771677176505705]
	TIME [epoch: 17.5 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08596287932905342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08596287932905342 | validation: 0.13046169758021678]
	TIME [epoch: 17.5 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08646364735874673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08646364735874673 | validation: 0.035809644177330034]
	TIME [epoch: 17.5 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051497432638539564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051497432638539564 | validation: 0.08253382263540773]
	TIME [epoch: 17.5 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05218325593415624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05218325593415624 | validation: 0.0487814631069256]
	TIME [epoch: 17.5 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06793132614526536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06793132614526536 | validation: 0.1656634492779645]
	TIME [epoch: 17.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09449698610551518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09449698610551518 | validation: 0.059279229687333915]
	TIME [epoch: 17.5 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07741430258536522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07741430258536522 | validation: 0.14673160841079397]
	TIME [epoch: 17.5 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08216591598273679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08216591598273679 | validation: 0.0522535737925293]
	TIME [epoch: 17.5 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05821938958326638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05821938958326638 | validation: 0.05665052100365745]
	TIME [epoch: 17.5 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0746372977059996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0746372977059996 | validation: 0.1019181074243708]
	TIME [epoch: 17.5 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08772437610865336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08772437610865336 | validation: 0.055934673531296414]
	TIME [epoch: 17.5 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07840614675412283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07840614675412283 | validation: 0.15742362534047546]
	TIME [epoch: 17.5 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08977796042150829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08977796042150829 | validation: 0.042055603046594905]
	TIME [epoch: 17.5 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051156728609107346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051156728609107346 | validation: 0.03729136602138925]
	TIME [epoch: 17.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03864321766472904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03864321766472904 | validation: 0.0383171088240473]
	TIME [epoch: 17.5 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03771020273368127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03771020273368127 | validation: 0.05969616082259338]
	TIME [epoch: 17.5 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06619036043315708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06619036043315708 | validation: 0.07527074189087456]
	TIME [epoch: 17.5 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09767948104953023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09767948104953023 | validation: 0.13635372685600267]
	TIME [epoch: 17.5 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11853433877843399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11853433877843399 | validation: 0.06869203355653962]
	TIME [epoch: 17.5 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08810091822940969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08810091822940969 | validation: 0.08947786906213145]
	TIME [epoch: 17.5 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06588639958381096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06588639958381096 | validation: 0.057281098527777956]
	TIME [epoch: 17.5 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07863043243871345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07863043243871345 | validation: 0.24862716744286673]
	TIME [epoch: 17.5 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11991046932127318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11991046932127318 | validation: 0.05705246504595719]
	TIME [epoch: 17.5 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07488883059466256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07488883059466256 | validation: 0.051478387028590505]
	TIME [epoch: 17.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04664812172943455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04664812172943455 | validation: 0.03965948283385961]
	TIME [epoch: 17.5 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03770217931958761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03770217931958761 | validation: 0.034430323496405506]
	TIME [epoch: 17.5 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051854382479547205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051854382479547205 | validation: 0.16962659956830067]
	TIME [epoch: 17.5 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10254856153799397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10254856153799397 | validation: 0.06714597666596236]
	TIME [epoch: 17.5 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09180143464997764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09180143464997764 | validation: 0.09393963569504862]
	TIME [epoch: 17.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08121281255447232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08121281255447232 | validation: 0.0518254973490651]
	TIME [epoch: 17.5 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07173588302090822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07173588302090822 | validation: 0.05274699385268509]
	TIME [epoch: 17.5 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046238809739232944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046238809739232944 | validation: 0.034104181926163345]
	TIME [epoch: 17.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0439947363316748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0439947363316748 | validation: 0.06186626969503574]
	TIME [epoch: 17.5 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06475673473734414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06475673473734414 | validation: 0.23766424322545943]
	TIME [epoch: 17.5 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14484709444066066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14484709444066066 | validation: 0.12527195750469802]
	TIME [epoch: 17.5 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12646052562877538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12646052562877538 | validation: 0.05463335958058953]
	TIME [epoch: 17.5 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09834211558898882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09834211558898882 | validation: 0.22192345625847396]
	TIME [epoch: 17.5 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1367911552450747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1367911552450747 | validation: 0.036372124756766946]
	TIME [epoch: 17.6 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04546987786426288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04546987786426288 | validation: 0.03357015493251897]
	TIME [epoch: 17.5 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040333932255410064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040333932255410064 | validation: 0.10968735499051047]
	TIME [epoch: 17.6 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06565613685456503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06565613685456503 | validation: 0.04832943266978525]
	TIME [epoch: 17.5 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06383082609085161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06383082609085161 | validation: 0.061321850867352104]
	TIME [epoch: 17.5 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05315360910258963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05315360910258963 | validation: 0.030830835849197713]
	TIME [epoch: 17.5 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030001711332833737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030001711332833737 | validation: 0.03231715041627155]
	TIME [epoch: 17.6 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028425799686553788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028425799686553788 | validation: 0.038986058876276264]
	TIME [epoch: 17.5 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039337340875215905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039337340875215905 | validation: 0.07248011048338235]
	TIME [epoch: 17.5 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0774353057188349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0774353057188349 | validation: 0.1188102196090066]
	TIME [epoch: 17.4 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13561062261908774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13561062261908774 | validation: 0.03991205852888923]
	TIME [epoch: 17.5 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039293409702384416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039293409702384416 | validation: 0.05065773811410414]
	TIME [epoch: 17.5 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06346311385598884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06346311385598884 | validation: 0.15509095077533908]
	TIME [epoch: 17.5 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10328603923243274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10328603923243274 | validation: 0.09585390789852145]
	TIME [epoch: 17.5 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15111849564737972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15111849564737972 | validation: 0.3354165692119766]
	TIME [epoch: 17.4 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18511527075760764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18511527075760764 | validation: 0.05973422478477867]
	TIME [epoch: 17.4 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04866480392163994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04866480392163994 | validation: 0.04503870602713725]
	TIME [epoch: 17.5 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06678899939255258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06678899939255258 | validation: 0.062092137297521416]
	TIME [epoch: 17.4 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05739508271579602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05739508271579602 | validation: 0.07598801197400043]
	TIME [epoch: 17.5 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07067435759011495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07067435759011495 | validation: 0.04398410863044244]
	TIME [epoch: 17.5 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04059566475035023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04059566475035023 | validation: 0.031221188331511954]
	TIME [epoch: 17.5 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03187630865390801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03187630865390801 | validation: 0.03850626696430325]
	TIME [epoch: 17.5 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03437556057348482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03437556057348482 | validation: 0.04865773547307503]
	TIME [epoch: 17.5 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0553065562075871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0553065562075871 | validation: 0.06076487620718363]
	TIME [epoch: 17.5 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050304550095184565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050304550095184565 | validation: 0.03120480625516884]
	TIME [epoch: 17.5 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03954724284728907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03954724284728907 | validation: 0.021610783940932788]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_819.pth
	Model improved!!!
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03229592717763443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03229592717763443 | validation: 0.044599437879295725]
	TIME [epoch: 17.6 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04275520953109038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04275520953109038 | validation: 0.07104657202370225]
	TIME [epoch: 17.6 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09141852787718298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09141852787718298 | validation: 0.29361103913354675]
	TIME [epoch: 17.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19940848378534562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19940848378534562 | validation: 0.08171993147659662]
	TIME [epoch: 17.6 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10704647523775813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10704647523775813 | validation: 0.072343611493092]
	TIME [epoch: 17.6 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06616344167711956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06616344167711956 | validation: 0.0571539166613228]
	TIME [epoch: 17.6 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05445004755707029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05445004755707029 | validation: 0.03296058252625276]
	TIME [epoch: 17.6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04262787252038068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04262787252038068 | validation: 0.028747024978938863]
	TIME [epoch: 17.5 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033572776306409854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033572776306409854 | validation: 0.027158178412775125]
	TIME [epoch: 17.6 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031222240006030196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031222240006030196 | validation: 0.04982477022553471]
	TIME [epoch: 17.6 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04372987611058882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04372987611058882 | validation: 0.04121795706461413]
	TIME [epoch: 17.6 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04974708360325791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04974708360325791 | validation: 0.07264084196233843]
	TIME [epoch: 17.6 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05135630909544994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05135630909544994 | validation: 0.04157146808625306]
	TIME [epoch: 17.5 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06322769545003533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06322769545003533 | validation: 0.19563809022997322]
	TIME [epoch: 17.5 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10569139855705015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10569139855705015 | validation: 0.0940183227422736]
	TIME [epoch: 17.5 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11091576425706975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11091576425706975 | validation: 0.09247386049526556]
	TIME [epoch: 17.5 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09623187482624729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09623187482624729 | validation: 0.12183126407101569]
	TIME [epoch: 17.6 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1382308339150246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1382308339150246 | validation: 0.06790750011159623]
	TIME [epoch: 17.6 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07611816226735424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07611816226735424 | validation: 0.04841168865254064]
	TIME [epoch: 17.6 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055240016316871754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055240016316871754 | validation: 0.02915570902131658]
	TIME [epoch: 17.6 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04670385501986496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04670385501986496 | validation: 0.07209808015892424]
	TIME [epoch: 17.6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05693828609632672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05693828609632672 | validation: 0.07650397553849954]
	TIME [epoch: 17.6 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10596050732978991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10596050732978991 | validation: 0.19910494086262756]
	TIME [epoch: 17.6 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11698007369247655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11698007369247655 | validation: 0.045340131167201675]
	TIME [epoch: 17.6 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04612773650243462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04612773650243462 | validation: 0.03210325980456864]
	TIME [epoch: 17.6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04547117885131762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04547117885131762 | validation: 0.043962367260290104]
	TIME [epoch: 18.1 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045765613105863495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045765613105863495 | validation: 0.03379723875671048]
	TIME [epoch: 17.5 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03644514798459463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03644514798459463 | validation: 0.04264749776186825]
	TIME [epoch: 17.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03211832642153933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03211832642153933 | validation: 0.028940795222603302]
	TIME [epoch: 17.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034912203660333624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034912203660333624 | validation: 0.06528508242925782]
	TIME [epoch: 17.6 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05676361066073788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05676361066073788 | validation: 0.07672831786209339]
	TIME [epoch: 17.6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09736983179492387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09736983179492387 | validation: 0.10679055037938866]
	TIME [epoch: 17.6 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08688097680380676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08688097680380676 | validation: 0.07948984508441917]
	TIME [epoch: 17.6 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08728276106320347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08728276106320347 | validation: 0.18087281328482968]
	TIME [epoch: 17.6 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09401729049298119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09401729049298119 | validation: 0.03082662049027576]
	TIME [epoch: 17.6 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04110320701532506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04110320701532506 | validation: 0.03149423664886682]
	TIME [epoch: 17.5 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03407333671215075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03407333671215075 | validation: 0.04770718419068044]
	TIME [epoch: 17.5 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03183650588640414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03183650588640414 | validation: 0.018058564337049988]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02864513574737851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02864513574737851 | validation: 0.0329486836634556]
	TIME [epoch: 17.5 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028912149540943705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028912149540943705 | validation: 0.07775494303075799]
	TIME [epoch: 17.5 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059749998626334205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059749998626334205 | validation: 0.10209182755489798]
	TIME [epoch: 17.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10239938962976236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10239938962976236 | validation: 0.07800297777874865]
	TIME [epoch: 17.5 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10338886832577515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10338886832577515 | validation: 0.14065846957466582]
	TIME [epoch: 17.5 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10889749705161318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10889749705161318 | validation: 0.10210535062910349]
	TIME [epoch: 17.5 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12411080474937526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12411080474937526 | validation: 0.27906988363925905]
	TIME [epoch: 17.5 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14567413335016385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14567413335016385 | validation: 0.11012139469698373]
	TIME [epoch: 17.5 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0735728687280872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0735728687280872 | validation: 0.04423780966545922]
	TIME [epoch: 17.5 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044113869768270274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044113869768270274 | validation: 0.0395588494871363]
	TIME [epoch: 17.5 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04137641772283683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04137641772283683 | validation: 0.029596907835560994]
	TIME [epoch: 17.5 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03359582171551617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03359582171551617 | validation: 0.022746408173081933]
	TIME [epoch: 17.5 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02559102077055043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02559102077055043 | validation: 0.023935041101777344]
	TIME [epoch: 17.5 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03567827306821646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03567827306821646 | validation: 0.14596974502435384]
	TIME [epoch: 17.5 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11536836166174784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11536836166174784 | validation: 0.049366732143173275]
	TIME [epoch: 17.5 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07217650774517592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07217650774517592 | validation: 0.05181462920155047]
	TIME [epoch: 17.5 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04704059369135646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04704059369135646 | validation: 0.04096671836693131]
	TIME [epoch: 17.5 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04393479250407557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04393479250407557 | validation: 0.04026827757105276]
	TIME [epoch: 17.5 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041880255551081824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041880255551081824 | validation: 0.05531190984212034]
	TIME [epoch: 17.5 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05467332680899418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05467332680899418 | validation: 0.042108806597300166]
	TIME [epoch: 17.5 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05118382979267508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05118382979267508 | validation: 0.035965917617897004]
	TIME [epoch: 17.5 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040532305611211183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040532305611211183 | validation: 0.02346131819482901]
	TIME [epoch: 17.5 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027863693892948724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027863693892948724 | validation: 0.021423749079478572]
	TIME [epoch: 17.5 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02285508094862922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02285508094862922 | validation: 0.016212629726270945]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_881.pth
	Model improved!!!
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026035173625095633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026035173625095633 | validation: 0.0991613744398511]
	TIME [epoch: 17.5 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06583301022602661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06583301022602661 | validation: 0.16771193920409286]
	TIME [epoch: 17.5 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24406826063306092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24406826063306092 | validation: 0.2938187595124233]
	TIME [epoch: 17.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16771560014077871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16771560014077871 | validation: 0.16883216382136787]
	TIME [epoch: 17.5 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12194592655976866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12194592655976866 | validation: 0.042087163937165106]
	TIME [epoch: 17.5 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056952288270169654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056952288270169654 | validation: 0.04405879188331344]
	TIME [epoch: 17.5 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051402484793332905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051402484793332905 | validation: 0.03062293078783849]
	TIME [epoch: 17.5 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03483536898809467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03483536898809467 | validation: 0.05089849685769256]
	TIME [epoch: 17.5 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030026427811843337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030026427811843337 | validation: 0.02697030630196262]
	TIME [epoch: 17.5 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027699581761378322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027699581761378322 | validation: 0.02467485759181476]
	TIME [epoch: 17.5 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031059793045883986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031059793045883986 | validation: 0.022578374130620527]
	TIME [epoch: 17.5 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02884631554485078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02884631554485078 | validation: 0.03287709575453686]
	TIME [epoch: 17.5 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034584967135976995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034584967135976995 | validation: 0.05393718200594816]
	TIME [epoch: 17.5 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055758868339257237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055758868339257237 | validation: 0.06266407185077162]
	TIME [epoch: 17.5 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06968409776307437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06968409776307437 | validation: 0.04152596408087102]
	TIME [epoch: 17.5 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054212901230780604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054212901230780604 | validation: 0.064037806324196]
	TIME [epoch: 17.5 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053039749891287395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053039749891287395 | validation: 0.062351048225575]
	TIME [epoch: 17.5 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07714349052768603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07714349052768603 | validation: 0.2024519505610738]
	TIME [epoch: 17.5 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11111261468818862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11111261468818862 | validation: 0.029634786246968883]
	TIME [epoch: 17.5 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04058476430367399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04058476430367399 | validation: 0.057644373984972276]
	TIME [epoch: 17.5 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034029429752894164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034029429752894164 | validation: 0.01783488908415456]
	TIME [epoch: 17.5 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027958924727365008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027958924727365008 | validation: 0.02969992296479165]
	TIME [epoch: 17.5 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040579326275626555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040579326275626555 | validation: 0.10280514176743832]
	TIME [epoch: 17.5 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09780200717289969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09780200717289969 | validation: 0.09796934712112187]
	TIME [epoch: 17.5 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12192771889102957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12192771889102957 | validation: 0.0989611121524821]
	TIME [epoch: 17.5 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07352991836949503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07352991836949503 | validation: 0.06299277413914645]
	TIME [epoch: 17.5 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0707903362296253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0707903362296253 | validation: 0.06313246123998852]
	TIME [epoch: 17.5 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05303150651016599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05303150651016599 | validation: 0.025124748854214986]
	TIME [epoch: 17.5 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03277128875267884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03277128875267884 | validation: 0.03386967115934205]
	TIME [epoch: 17.5 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03619513729418318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03619513729418318 | validation: 0.027159500492664048]
	TIME [epoch: 17.5 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03952996716321621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03952996716321621 | validation: 0.18365055618135548]
	TIME [epoch: 17.5 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11562592525881663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11562592525881663 | validation: 0.03459685218749092]
	TIME [epoch: 17.5 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04652342107683154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04652342107683154 | validation: 0.04184898232994593]
	TIME [epoch: 17.5 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04962099100096366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04962099100096366 | validation: 0.044843275743017746]
	TIME [epoch: 17.5 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04872809847551803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04872809847551803 | validation: 0.042856940704726114]
	TIME [epoch: 17.5 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04270188236228393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04270188236228393 | validation: 0.024455773734143305]
	TIME [epoch: 17.5 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03294502771647673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03294502771647673 | validation: 0.04848277791972799]
	TIME [epoch: 17.5 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030044486249900148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030044486249900148 | validation: 0.03507365508342491]
	TIME [epoch: 17.5 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046016114255658803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046016114255658803 | validation: 0.14106388304310294]
	TIME [epoch: 17.5 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10932568020234389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10932568020234389 | validation: 0.09623141031529724]
	TIME [epoch: 17.5 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12829587215731306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12829587215731306 | validation: 0.0717019933452117]
	TIME [epoch: 17.5 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08016345882513441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08016345882513441 | validation: 0.022175354864419097]
	TIME [epoch: 17.5 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023949041622686552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023949041622686552 | validation: 0.018301745061065435]
	TIME [epoch: 17.5 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024614348407044524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024614348407044524 | validation: 0.03072521291187759]
	TIME [epoch: 17.5 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02938000876285321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02938000876285321 | validation: 0.031955691190463424]
	TIME [epoch: 17.5 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03829978745382761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03829978745382761 | validation: 0.05506366034541132]
	TIME [epoch: 17.5 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04765181591629041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04765181591629041 | validation: 0.05406428577829778]
	TIME [epoch: 17.5 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06137469538621559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06137469538621559 | validation: 0.14158996672165555]
	TIME [epoch: 17.5 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08233667120136895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08233667120136895 | validation: 0.026223078239159883]
	TIME [epoch: 17.5 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0392557928043627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0392557928043627 | validation: 0.04399773837685233]
	TIME [epoch: 17.5 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03864307556989009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03864307556989009 | validation: 0.03087627336521577]
	TIME [epoch: 17.5 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034736895269625645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034736895269625645 | validation: 0.042792676538463637]
	TIME [epoch: 17.5 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05115600472498876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05115600472498876 | validation: 0.08787836214383407]
	TIME [epoch: 17.5 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09605243820879913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09605243820879913 | validation: 0.06618402822373143]
	TIME [epoch: 17.5 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08567846319721382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08567846319721382 | validation: 0.10774470780499157]
	TIME [epoch: 17.5 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08335407308813311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08335407308813311 | validation: 0.09764565695751481]
	TIME [epoch: 17.5 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12789300441264614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12789300441264614 | validation: 0.1155886628144488]
	TIME [epoch: 17.5 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08192055952102376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08192055952102376 | validation: 0.06901586857455777]
	TIME [epoch: 17.5 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058976466478853844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058976466478853844 | validation: 0.03673213396611687]
	TIME [epoch: 17.5 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036144547963404126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036144547963404126 | validation: 0.01502066995458753]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_941.pth
	Model improved!!!
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02930066116323446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02930066116323446 | validation: 0.03479892417982246]
	TIME [epoch: 17.5 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029876605284995535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029876605284995535 | validation: 0.01920233660802685]
	TIME [epoch: 17.5 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02861235589586064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02861235589586064 | validation: 0.06371696945335212]
	TIME [epoch: 17.5 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04142158103333573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04142158103333573 | validation: 0.04664892791827305]
	TIME [epoch: 17.5 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06674399064840802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06674399064840802 | validation: 0.1451060370517747]
	TIME [epoch: 17.5 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1156351310389018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1156351310389018 | validation: 0.04440508604708831]
	TIME [epoch: 17.5 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05210860735566505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05210860735566505 | validation: 0.05169238519797157]
	TIME [epoch: 17.5 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06067667632282272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06067667632282272 | validation: 0.07870353899463663]
	TIME [epoch: 17.5 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0516545879278504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0516545879278504 | validation: 0.04694606750164823]
	TIME [epoch: 17.5 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05356636482397926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05356636482397926 | validation: 0.03361833307765499]
	TIME [epoch: 17.5 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04031902407643122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04031902407643122 | validation: 0.01738011273863338]
	TIME [epoch: 17.5 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0297280660322045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0297280660322045 | validation: 0.03701634723150741]
	TIME [epoch: 17.5 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03931045844558046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03931045844558046 | validation: 0.045282846315249786]
	TIME [epoch: 17.5 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062180688184781396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062180688184781396 | validation: 0.0880272723367327]
	TIME [epoch: 17.5 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07736499714654825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07736499714654825 | validation: 0.030384089044227226]
	TIME [epoch: 17.5 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038635438637922105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038635438637922105 | validation: 0.0341373162728763]
	TIME [epoch: 17.5 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04097862998903216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04097862998903216 | validation: 0.0413192036497594]
	TIME [epoch: 17.5 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0453271406529197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0453271406529197 | validation: 0.041632007838549895]
	TIME [epoch: 17.5 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03486978807824188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03486978807824188 | validation: 0.029516849391763744]
	TIME [epoch: 17.5 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036518419781446945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036518419781446945 | validation: 0.1946506759685579]
	TIME [epoch: 17.5 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12626271446201892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12626271446201892 | validation: 0.036083595511328725]
	TIME [epoch: 17.5 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04778053151782784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04778053151782784 | validation: 0.03986464228221889]
	TIME [epoch: 17.5 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04087558572637777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04087558572637777 | validation: 0.044292204161414744]
	TIME [epoch: 17.5 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035588764384696575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035588764384696575 | validation: 0.034690535679655486]
	TIME [epoch: 17.4 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04877202398346361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04877202398346361 | validation: 0.10476668557663285]
	TIME [epoch: 17.5 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1109163180845887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1109163180845887 | validation: 0.09650855219149186]
	TIME [epoch: 17.5 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11416241896140973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11416241896140973 | validation: 0.08105844464101498]
	TIME [epoch: 17.5 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049573492912863684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049573492912863684 | validation: 0.057575551625122005]
	TIME [epoch: 17.5 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057177823342113944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057177823342113944 | validation: 0.07836576841602866]
	TIME [epoch: 17.5 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06089146462297278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06089146462297278 | validation: 0.053357422601437034]
	TIME [epoch: 17.5 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059873564494686775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059873564494686775 | validation: 0.06783698388097671]
	TIME [epoch: 17.5 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07407992796489396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07407992796489396 | validation: 0.13835560518689508]
	TIME [epoch: 17.5 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16017683080013748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16017683080013748 | validation: 0.22348597298629158]
	TIME [epoch: 17.5 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1534626757669549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1534626757669549 | validation: 0.09081302646861973]
	TIME [epoch: 17.5 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05790198960112576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05790198960112576 | validation: 0.03831803666693421]
	TIME [epoch: 17.5 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05090916500340648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05090916500340648 | validation: 0.03533174536475965]
	TIME [epoch: 17.5 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036643443636180204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036643443636180204 | validation: 0.024348227709008952]
	TIME [epoch: 17.5 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03426459337744798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03426459337744798 | validation: 0.026199222288759073]
	TIME [epoch: 17.5 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029447410757035657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029447410757035657 | validation: 0.016951327174496277]
	TIME [epoch: 17.5 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022000585364167758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022000585364167758 | validation: 0.02567173477049557]
	TIME [epoch: 17.5 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02157359581396524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02157359581396524 | validation: 0.017833522805193524]
	TIME [epoch: 17.5 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022070634903418932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022070634903418932 | validation: 0.02705250690646025]
	TIME [epoch: 17.5 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02540765784197509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02540765784197509 | validation: 0.037446701509905256]
	TIME [epoch: 17.5 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044134037395612814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044134037395612814 | validation: 0.07523419646718302]
	TIME [epoch: 17.5 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07679520572962976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07679520572962976 | validation: 0.11442454020199666]
	TIME [epoch: 17.5 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13814337352126313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13814337352126313 | validation: 0.11219059398617391]
	TIME [epoch: 17.5 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07583667836981246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07583667836981246 | validation: 0.018690578366674183]
	TIME [epoch: 17.5 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024243113116186044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024243113116186044 | validation: 0.024297655206186132]
	TIME [epoch: 17.5 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03378703630967885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03378703630967885 | validation: 0.03935862634170345]
	TIME [epoch: 17.5 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0362149818460923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0362149818460923 | validation: 0.04262078715117976]
	TIME [epoch: 17.5 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04098106625305868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04098106625305868 | validation: 0.03044850844405035]
	TIME [epoch: 17.5 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0377831142783469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0377831142783469 | validation: 0.02997265092841136]
	TIME [epoch: 17.5 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0341023434460876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0341023434460876 | validation: 0.01961644052231314]
	TIME [epoch: 17.5 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033996728907391346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033996728907391346 | validation: 0.07400987896935186]
	TIME [epoch: 17.5 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051164285216255996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051164285216255996 | validation: 0.046097724764523944]
	TIME [epoch: 17.5 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06857342540856089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06857342540856089 | validation: 0.07096595879661667]
	TIME [epoch: 17.5 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06890089398929866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06890089398929866 | validation: 0.04532009209171422]
	TIME [epoch: 17.5 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05723487214563014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05723487214563014 | validation: 0.04057170107623888]
	TIME [epoch: 17.5 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048241579754270576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048241579754270576 | validation: 0.6304585949482148]
	TIME [epoch: 17.5 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8528209976795518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8528209976795518 | validation: 0.5367673764092892]
	TIME [epoch: 76.5 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7596873890828922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7596873890828922 | validation: 0.30307166864984575]
	TIME [epoch: 36.2 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34763513022395415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34763513022395415 | validation: 0.318998355692211]
	TIME [epoch: 36.2 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35351334309704185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35351334309704185 | validation: 0.18465123770633746]
	TIME [epoch: 36.2 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22543800394005956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22543800394005956 | validation: 0.14425398986818758]
	TIME [epoch: 36.2 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17451887765124052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17451887765124052 | validation: 0.12776883222923296]
	TIME [epoch: 36.2 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15222851684717723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15222851684717723 | validation: 0.06566952027723401]
	TIME [epoch: 36.2 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08481680163436851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08481680163436851 | validation: 0.05981073143795568]
	TIME [epoch: 36.2 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049293350379954276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049293350379954276 | validation: 0.06963459922862114]
	TIME [epoch: 36.2 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06424953141421183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06424953141421183 | validation: 0.1265120825285727]
	TIME [epoch: 36.2 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12105312182501127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12105312182501127 | validation: 0.05837794890184821]
	TIME [epoch: 36.1 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05553766390865524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05553766390865524 | validation: 0.04117126047172165]
	TIME [epoch: 36.2 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05358222420070027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05358222420070027 | validation: 0.03883663776501241]
	TIME [epoch: 36.1 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04298515090434239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04298515090434239 | validation: 0.028133649471893854]
	TIME [epoch: 36.2 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03399665268493887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03399665268493887 | validation: 0.019906900986598832]
	TIME [epoch: 36.1 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02361145689416501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02361145689416501 | validation: 0.014765793331922095]
	TIME [epoch: 36.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_1016.pth
	Model improved!!!
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02019761653378855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02019761653378855 | validation: 0.015494029228175832]
	TIME [epoch: 36.2 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019236379169607024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019236379169607024 | validation: 0.018872222256278427]
	TIME [epoch: 36.1 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018509152421753796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018509152421753796 | validation: 0.017833527033718165]
	TIME [epoch: 36.2 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017038192192323363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017038192192323363 | validation: 0.01757531510843997]
	TIME [epoch: 36.1 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017683682316674217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017683682316674217 | validation: 0.025788221000375446]
	TIME [epoch: 36.2 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027375609357910013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027375609357910013 | validation: 0.04902824699603926]
	TIME [epoch: 36.1 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04885063702825966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04885063702825966 | validation: 0.054016011202263306]
	TIME [epoch: 36.2 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05878631240934928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05878631240934928 | validation: 0.04504668323857591]
	TIME [epoch: 36.1 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04859111766876999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04859111766876999 | validation: 0.08378877393688249]
	TIME [epoch: 36.1 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0773322921059966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0773322921059966 | validation: 0.13490993621294264]
	TIME [epoch: 36.2 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15086150832053086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15086150832053086 | validation: 0.05747159965412042]
	TIME [epoch: 36.2 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059540624665968524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059540624665968524 | validation: 0.028765013066328494]
	TIME [epoch: 36.2 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0232351080606215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0232351080606215 | validation: 0.025632192778017163]
	TIME [epoch: 36.2 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028752551047051815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028752551047051815 | validation: 0.02668613711421394]
	TIME [epoch: 36.1 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030987990142454406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030987990142454406 | validation: 0.034062826672161074]
	TIME [epoch: 36.2 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03431840787648798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03431840787648798 | validation: 0.03782034738486584]
	TIME [epoch: 36.1 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04281112152893217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04281112152893217 | validation: 0.0751566372834053]
	TIME [epoch: 36.2 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05321351261848982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05321351261848982 | validation: 0.044320348494375356]
	TIME [epoch: 36.1 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04476080434639009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04476080434639009 | validation: 0.0734913400629706]
	TIME [epoch: 36.2 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04637889630848288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04637889630848288 | validation: 0.06945925113688636]
	TIME [epoch: 36.1 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07231560433606396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07231560433606396 | validation: 0.041939231649899905]
	TIME [epoch: 36.2 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04557932709593368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04557932709593368 | validation: 0.035931568163599124]
	TIME [epoch: 36.2 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037336965829080165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037336965829080165 | validation: 0.062128251731933704]
	TIME [epoch: 36.1 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06031109000404225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06031109000404225 | validation: 0.04624800204666049]
	TIME [epoch: 36.1 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05660790369635593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05660790369635593 | validation: 0.07867822113359926]
	TIME [epoch: 36.1 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06589028991893267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06589028991893267 | validation: 0.03600072634946435]
	TIME [epoch: 36.1 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035335215078195335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035335215078195335 | validation: 0.030377000753748898]
	TIME [epoch: 36.2 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0363974143269222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0363974143269222 | validation: 0.08537253154772485]
	TIME [epoch: 36.1 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0496344694418108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0496344694418108 | validation: 0.025548865024883474]
	TIME [epoch: 36.2 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032266125196953904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032266125196953904 | validation: 0.02796980871332068]
	TIME [epoch: 36.2 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037610165450574815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037610165450574815 | validation: 0.08901641100189475]
	TIME [epoch: 36.1 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05553245200320145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05553245200320145 | validation: 0.0678463780352138]
	TIME [epoch: 36.2 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08358194541447095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08358194541447095 | validation: 0.1500045616607913]
	TIME [epoch: 36.1 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08498741667627209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08498741667627209 | validation: 0.05329739418247751]
	TIME [epoch: 36.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05122558960709375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05122558960709375 | validation: 0.021288725893288063]
	TIME [epoch: 36.2 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030028850423655364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030028850423655364 | validation: 0.05965462774151023]
	TIME [epoch: 36.2 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06749894012689703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06749894012689703 | validation: 0.0724564963733292]
	TIME [epoch: 36.1 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08300675471395189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08300675471395189 | validation: 0.031131274042034432]
	TIME [epoch: 36.1 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03372634172726171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03372634172726171 | validation: 0.04286127875599488]
	TIME [epoch: 36.1 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03632455050633782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03632455050633782 | validation: 0.0144725250584969]
	TIME [epoch: 36.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_1056.pth
	Model improved!!!
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01997584674748741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01997584674748741 | validation: 0.017259025541527553]
	TIME [epoch: 36.1 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021468262364213297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021468262364213297 | validation: 0.01177270356114758]
	TIME [epoch: 36.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_1058.pth
	Model improved!!!
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017667077319502938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017667077319502938 | validation: 0.0099707270107541]
	TIME [epoch: 36.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_1059.pth
	Model improved!!!
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015348865921334042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015348865921334042 | validation: 0.014687951432235703]
	TIME [epoch: 36.2 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020428907098185945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020428907098185945 | validation: 0.04494983939356413]
	TIME [epoch: 36.1 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04175363955305279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04175363955305279 | validation: 0.13326437065301683]
	TIME [epoch: 36.2 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09236978592379153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09236978592379153 | validation: 0.06156497030720501]
	TIME [epoch: 36.2 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0726094724057371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0726094724057371 | validation: 0.21435559778665303]
	TIME [epoch: 36.1 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10497394261181281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10497394261181281 | validation: 0.12332401410459089]
	TIME [epoch: 36.1 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09280377663610143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09280377663610143 | validation: 0.02515535548157216]
	TIME [epoch: 36.1 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04064890531432231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04064890531432231 | validation: 0.05913959794344401]
	TIME [epoch: 36.1 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0631213356583192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0631213356583192 | validation: 0.04947635744826804]
	TIME [epoch: 36.1 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05453967785706522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05453967785706522 | validation: 0.05960242094750005]
	TIME [epoch: 36.1 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04022017799237187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04022017799237187 | validation: 0.03745927822848295]
	TIME [epoch: 36.1 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03291305083336254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03291305083336254 | validation: 0.05849066981517175]
	TIME [epoch: 36.1 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07045494362890681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07045494362890681 | validation: 0.07750327018834273]
	TIME [epoch: 36.1 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0688056744696096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0688056744696096 | validation: 0.03167411082332287]
	TIME [epoch: 36.1 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04037602570618798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04037602570618798 | validation: 0.031028710803039008]
	TIME [epoch: 36.1 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03588294093019199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03588294093019199 | validation: 0.029436492628333467]
	TIME [epoch: 36.1 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026734416767848834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026734416767848834 | validation: 0.019369643506233804]
	TIME [epoch: 36.2 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019767451018735675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019767451018735675 | validation: 0.01849751266870947]
	TIME [epoch: 36.2 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025126979632442537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025126979632442537 | validation: 0.022271261646711584]
	TIME [epoch: 36.1 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02573444767463335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02573444767463335 | validation: 0.04254101131679982]
	TIME [epoch: 36.1 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05572649860029207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05572649860029207 | validation: 0.03622997088011836]
	TIME [epoch: 36.1 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04276240861035942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04276240861035942 | validation: 0.04291008249637493]
	TIME [epoch: 36.1 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045412978242597574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045412978242597574 | validation: 0.050969146572705154]
	TIME [epoch: 36.1 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05999985016870801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05999985016870801 | validation: 0.09013067926991072]
	TIME [epoch: 36.1 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06696496307619458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06696496307619458 | validation: 0.02843393720669847]
	TIME [epoch: 36.1 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04028312501052978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04028312501052978 | validation: 0.04830611464668341]
	TIME [epoch: 36.1 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033742950569080526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033742950569080526 | validation: 0.03507650400993528]
	TIME [epoch: 36.1 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03736645587311359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03736645587311359 | validation: 0.06250398739484121]
	TIME [epoch: 36.1 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07258469066521758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07258469066521758 | validation: 0.09703418493335486]
	TIME [epoch: 36.2 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08125639424307454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08125639424307454 | validation: 0.05794945072702262]
	TIME [epoch: 36.2 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054743548091745656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054743548091745656 | validation: 0.037922421260528784]
	TIME [epoch: 36.1 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03930368613450585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03930368613450585 | validation: 0.033323447074114866]
	TIME [epoch: 36.2 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03666814241456522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03666814241456522 | validation: 0.015819386036638516]
	TIME [epoch: 36.1 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02026321740848466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02026321740848466 | validation: 0.009317527388377001]
	TIME [epoch: 36.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_1093.pth
	Model improved!!!
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012948343072683421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012948343072683421 | validation: 0.010925452037425499]
	TIME [epoch: 36.2 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0152122983749463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0152122983749463 | validation: 0.03076021651411083]
	TIME [epoch: 36.1 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0516332833442601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0516332833442601 | validation: 0.16969863368381236]
	TIME [epoch: 36.2 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08410688046235139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08410688046235139 | validation: 0.025812834525242537]
	TIME [epoch: 36.2 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02972574150022549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02972574150022549 | validation: 0.04315925433002123]
	TIME [epoch: 36.2 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04949684992600009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04949684992600009 | validation: 0.047819412686620114]
	TIME [epoch: 36.1 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05121225946222568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05121225946222568 | validation: 0.04187915622648555]
	TIME [epoch: 36.1 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04382226270163441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04382226270163441 | validation: 0.06286552700147152]
	TIME [epoch: 36.2 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07016862097472074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07016862097472074 | validation: 0.07765825562321292]
	TIME [epoch: 36.1 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08735721083046609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08735721083046609 | validation: 0.05542862734408075]
	TIME [epoch: 36.2 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04714160560592807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04714160560592807 | validation: 0.023359029004673626]
	TIME [epoch: 36.2 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024640822203806586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024640822203806586 | validation: 0.05121580590059053]
	TIME [epoch: 36.2 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042283861156667005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042283861156667005 | validation: 0.03861246163846682]
	TIME [epoch: 36.2 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042205070091180755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042205070091180755 | validation: 0.026013092160547726]
	TIME [epoch: 36.2 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030745370919341916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030745370919341916 | validation: 0.02331285774849815]
	TIME [epoch: 36.2 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030943168479459986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030943168479459986 | validation: 0.09861138966902429]
	TIME [epoch: 36.2 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06657444929992133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06657444929992133 | validation: 0.04608981293199015]
	TIME [epoch: 36.1 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06005874591594079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06005874591594079 | validation: 0.09399857701496507]
	TIME [epoch: 36.2 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05513719466055392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05513719466055392 | validation: 0.041955984200578056]
	TIME [epoch: 36.2 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03491658332606025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03491658332606025 | validation: 0.03364312924748821]
	TIME [epoch: 36.2 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035779222474546575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035779222474546575 | validation: 0.032698982245150665]
	TIME [epoch: 36.1 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044117656462147326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044117656462147326 | validation: 0.028070834988434958]
	TIME [epoch: 36.1 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03554711500872214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03554711500872214 | validation: 0.02450234254267271]
	TIME [epoch: 36.1 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026916741622713857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026916741622713857 | validation: 0.028118572041833913]
	TIME [epoch: 36.2 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029812131173056252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029812131173056252 | validation: 0.028472760800348586]
	TIME [epoch: 36.2 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024003249409252796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024003249409252796 | validation: 0.02037347577598163]
	TIME [epoch: 36.2 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024581921103738136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024581921103738136 | validation: 0.09437618994299105]
	TIME [epoch: 36.2 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058572988734082065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058572988734082065 | validation: 0.17254074820590665]
	TIME [epoch: 36.2 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.235593422268728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.235593422268728 | validation: 0.16700600355558587]
	TIME [epoch: 36.1 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09596202609553534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09596202609553534 | validation: 0.08054850310185792]
	TIME [epoch: 36.2 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048582330566491354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048582330566491354 | validation: 0.03598631026677884]
	TIME [epoch: 36.1 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04945769206386167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04945769206386167 | validation: 0.030771888834510697]
	TIME [epoch: 36.2 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03227990596546734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03227990596546734 | validation: 0.04010545660793167]
	TIME [epoch: 36.1 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042265678575692835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042265678575692835 | validation: 0.04427476938043973]
	TIME [epoch: 36.2 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04804941704598447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04804941704598447 | validation: 0.0318779296957676]
	TIME [epoch: 36.1 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03976074819053223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03976074819053223 | validation: 0.02465560614239667]
	TIME [epoch: 36.2 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029250869862524675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029250869862524675 | validation: 0.0192284318183376]
	TIME [epoch: 36.2 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020853736411497775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020853736411497775 | validation: 0.012270335906972641]
	TIME [epoch: 36.2 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01402446224924089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01402446224924089 | validation: 0.010444610540668309]
	TIME [epoch: 36.2 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020130717092647698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020130717092647698 | validation: 0.023466325689760738]
	TIME [epoch: 36.1 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023948525709396217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023948525709396217 | validation: 0.02684653643122166]
	TIME [epoch: 36.2 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033430574255787826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033430574255787826 | validation: 0.06295133594603364]
	TIME [epoch: 36.2 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06224411774588081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06224411774588081 | validation: 0.07253223179064637]
	TIME [epoch: 36.2 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07724638309222048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07724638309222048 | validation: 0.031027993026008574]
	TIME [epoch: 36.2 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03640415692307946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03640415692307946 | validation: 0.009628809595626453]
	TIME [epoch: 36.1 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016925274335787697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016925274335787697 | validation: 0.02501035722416536]
	TIME [epoch: 36.2 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02713821407859638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02713821407859638 | validation: 0.026467070191027366]
	TIME [epoch: 36.2 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031126427142248725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031126427142248725 | validation: 0.040555053322969006]
	TIME [epoch: 36.1 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048668059467159706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048668059467159706 | validation: 0.09151609840260155]
	TIME [epoch: 36.2 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08895729667466712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08895729667466712 | validation: 0.06898923941804048]
	TIME [epoch: 36.1 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08216405270750915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08216405270750915 | validation: 0.10174864410655729]
	TIME [epoch: 36.2 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049585937929270185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049585937929270185 | validation: 0.04102783183622482]
	TIME [epoch: 36.1 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048574380439534916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048574380439534916 | validation: 0.03842533656661365]
	TIME [epoch: 36.2 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04674053029796337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04674053029796337 | validation: 0.016395538858010473]
	TIME [epoch: 36.1 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02107242971521946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02107242971521946 | validation: 0.011905133240873379]
	TIME [epoch: 36.2 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018088415830508246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018088415830508246 | validation: 0.009041519478183336]
	TIME [epoch: 36.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_1149.pth
	Model improved!!!
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015716558670157903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015716558670157903 | validation: 0.016957849089380316]
	TIME [epoch: 36.1 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023404658385659395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023404658385659395 | validation: 0.012995839642705254]
	TIME [epoch: 36.1 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019805308765820314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019805308765820314 | validation: 0.02042158952751102]
	TIME [epoch: 36.1 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026227387595606708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026227387595606708 | validation: 0.048330714112199635]
	TIME [epoch: 36.1 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04050463613014063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04050463613014063 | validation: 0.06168785800558593]
	TIME [epoch: 36 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06082748896167036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06082748896167036 | validation: 0.11270254797269881]
	TIME [epoch: 36 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0724942858677573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0724942858677573 | validation: 0.05016350427460497]
	TIME [epoch: 36.1 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05844439501209549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05844439501209549 | validation: 0.04680114978024966]
	TIME [epoch: 36.1 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05035930810477282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05035930810477282 | validation: 0.05425676960697312]
	TIME [epoch: 36.1 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050256163019371095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050256163019371095 | validation: 0.04535855956254092]
	TIME [epoch: 36.1 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05530525946894865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05530525946894865 | validation: 0.03908045844965681]
	TIME [epoch: 36 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04796392046059191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04796392046059191 | validation: 0.03536657219750711]
	TIME [epoch: 36.1 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03460394049024899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03460394049024899 | validation: 0.0140946594863543]
	TIME [epoch: 37.2 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018842219492287232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018842219492287232 | validation: 0.018375284741474852]
	TIME [epoch: 36 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018012810073192272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018012810073192272 | validation: 0.04240806621026916]
	TIME [epoch: 36.1 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034385547148971005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034385547148971005 | validation: 0.04011356566059479]
	TIME [epoch: 36.2 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060146931378602726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060146931378602726 | validation: 0.22715201941018695]
	TIME [epoch: 36.2 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11265383057537108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11265383057537108 | validation: 0.05590407516606042]
	TIME [epoch: 36.2 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0574450183682792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0574450183682792 | validation: 0.04657372698604187]
	TIME [epoch: 36.1 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060943721069874286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060943721069874286 | validation: 0.029171040312449072]
	TIME [epoch: 36.1 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03789188313087464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03789188313087464 | validation: 0.046649620571320605]
	TIME [epoch: 36 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03896889240954637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03896889240954637 | validation: 0.03278190195036295]
	TIME [epoch: 36.2 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03902878050027317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03902878050027317 | validation: 0.028941213386229805]
	TIME [epoch: 35.9 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03863898133680688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03863898133680688 | validation: 0.06705081972940058]
	TIME [epoch: 36.1 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060275482263315014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060275482263315014 | validation: 0.0772328396394869]
	TIME [epoch: 35.9 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04349940272429437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04349940272429437 | validation: 0.039025655554925626]
	TIME [epoch: 36.1 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03954319977882666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03954319977882666 | validation: 0.04407076747086089]
	TIME [epoch: 35.9 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04767090401533448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04767090401533448 | validation: 0.04220617139186426]
	TIME [epoch: 35.9 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04933284044463953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04933284044463953 | validation: 0.04715886266977242]
	TIME [epoch: 36 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049336137292777096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049336137292777096 | validation: 0.039643579027110326]
	TIME [epoch: 35.9 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05269400013550599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05269400013550599 | validation: 0.03792800784923625]
	TIME [epoch: 35.9 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03980581608215277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03980581608215277 | validation: 0.03076943510782756]
	TIME [epoch: 36.1 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03417589898331784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03417589898331784 | validation: 0.039437052638553696]
	TIME [epoch: 36.1 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036462976100759756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036462976100759756 | validation: 0.015738932019065333]
	TIME [epoch: 36.1 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024328790871597887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024328790871597887 | validation: 0.012069736235048934]
	TIME [epoch: 35.9 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018341990669045224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018341990669045224 | validation: 0.008752414626945882]
	TIME [epoch: 35.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_1185.pth
	Model improved!!!
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018900296697370655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018900296697370655 | validation: 0.08672594131128869]
	TIME [epoch: 36.2 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05654887574403278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05654887574403278 | validation: 0.04647220131822616]
	TIME [epoch: 36 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04602162165489963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04602162165489963 | validation: 0.03206672463641186]
	TIME [epoch: 36.2 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04470688272293796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04470688272293796 | validation: 0.053056052235677637]
	TIME [epoch: 36 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05964095101647896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05964095101647896 | validation: 0.019715661413716747]
	TIME [epoch: 36 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02871564861832213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02871564861832213 | validation: 0.03909139683794125]
	TIME [epoch: 35.9 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045358872378228145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045358872378228145 | validation: 0.06566566018520409]
	TIME [epoch: 36 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06452169954924777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06452169954924777 | validation: 0.04907897892375969]
	TIME [epoch: 36.1 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05315477711686263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05315477711686263 | validation: 0.020689270108427783]
	TIME [epoch: 35.9 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02822040528105866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02822040528105866 | validation: 0.022419392065082047]
	TIME [epoch: 36.1 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023041255829328575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023041255829328575 | validation: 0.03219942276357194]
	TIME [epoch: 35.9 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042235880339821674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042235880339821674 | validation: 0.07073421979552628]
	TIME [epoch: 35.9 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04694951610544727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04694951610544727 | validation: 0.11472648431433119]
	TIME [epoch: 36.1 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1366286189283577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1366286189283577 | validation: 0.15623965959123842]
	TIME [epoch: 35.9 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09797444564025351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09797444564025351 | validation: 0.0716984548147551]
	TIME [epoch: 36 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040070892138836856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040070892138836856 | validation: 0.017376648371883196]
	TIME [epoch: 36.1 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02862866964210629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02862866964210629 | validation: 0.014445372088472408]
	TIME [epoch: 36.1 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024857215271804657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024857215271804657 | validation: 0.014158445495879836]
	TIME [epoch: 36.1 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02738340161170448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02738340161170448 | validation: 0.026913812874609422]
	TIME [epoch: 36.1 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03947964789544968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03947964789544968 | validation: 0.06003594528499732]
	TIME [epoch: 36.1 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05938789262896219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05938789262896219 | validation: 0.03553555718022607]
	TIME [epoch: 36 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04325658091799525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04325658091799525 | validation: 0.009500592376014161]
	TIME [epoch: 36 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015058088402576877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015058088402576877 | validation: 0.009957623013547724]
	TIME [epoch: 36.2 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013745563581348682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013745563581348682 | validation: 0.01007221412543784]
	TIME [epoch: 36.1 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016419676711555503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016419676711555503 | validation: 0.015426416463691096]
	TIME [epoch: 36 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01979254161557023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01979254161557023 | validation: 0.06113709996318652]
	TIME [epoch: 35.9 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04195301065195865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04195301065195865 | validation: 0.2930581326291785]
	TIME [epoch: 36.1 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33135691776127135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33135691776127135 | validation: 0.32126088478980774]
	TIME [epoch: 36.1 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34753966953253906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34753966953253906 | validation: 0.21903921460992282]
	TIME [epoch: 36.2 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2521735608259418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2521735608259418 | validation: 0.20444227191720843]
	TIME [epoch: 36 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2371553518365564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2371553518365564 | validation: 0.17796888883481865]
	TIME [epoch: 36.1 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13291310668875123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13291310668875123 | validation: 0.05899923973976227]
	TIME [epoch: 36 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06051666266789501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06051666266789501 | validation: 0.07153064182155477]
	TIME [epoch: 36.1 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06420109886412342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06420109886412342 | validation: 0.26962187462189263]
	TIME [epoch: 36 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19740309922649843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19740309922649843 | validation: 0.0873866683750596]
	TIME [epoch: 36.1 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08518536205352033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08518536205352033 | validation: 0.04711907104180673]
	TIME [epoch: 36 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050410997289833503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050410997289833503 | validation: 0.03895868472488924]
	TIME [epoch: 36.1 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04020790699616468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04020790699616468 | validation: 0.042122957265945615]
	TIME [epoch: 36 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03778264308498402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03778264308498402 | validation: 0.029916613082273693]
	TIME [epoch: 36 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028043587990082167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028043587990082167 | validation: 0.021022327272888566]
	TIME [epoch: 36.2 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02040279781097557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02040279781097557 | validation: 0.018189353317201753]
	TIME [epoch: 36.1 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01826434028558729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01826434028558729 | validation: 0.015057091006845736]
	TIME [epoch: 36 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018796080054403645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018796080054403645 | validation: 0.013438373838083396]
	TIME [epoch: 36 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015035922195959011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015035922195959011 | validation: 0.0737840348452]
	TIME [epoch: 36.1 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06683501032585176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06683501032585176 | validation: 0.023819383348996415]
	TIME [epoch: 36.1 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032269929163847494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032269929163847494 | validation: 0.039387301389249965]
	TIME [epoch: 36.2 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04695516080683032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04695516080683032 | validation: 0.03279949715093268]
	TIME [epoch: 36.2 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03811713688108145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03811713688108145 | validation: 0.0651068543012526]
	TIME [epoch: 36.1 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05101880821928259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05101880821928259 | validation: 0.03856967001719861]
	TIME [epoch: 36 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037700038143934175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037700038143934175 | validation: 0.02024362800777022]
	TIME [epoch: 36 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027828345351009386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027828345351009386 | validation: 0.0137761938971133]
	TIME [epoch: 36.1 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019171766651828213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019171766651828213 | validation: 0.02433923468162649]
	TIME [epoch: 36.2 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0228075759047886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0228075759047886 | validation: 0.028991068324776628]
	TIME [epoch: 36.3 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032198167854288064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032198167854288064 | validation: 0.037369875575214005]
	TIME [epoch: 36.1 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04533930232551907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04533930232551907 | validation: 0.044655805969726216]
	TIME [epoch: 36.2 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05115289417413654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05115289417413654 | validation: 0.02276625429259793]
	TIME [epoch: 36.1 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029167084268346986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029167084268346986 | validation: 0.010947259297696145]
	TIME [epoch: 36 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01553169738700144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01553169738700144 | validation: 0.15552094883494774]
	TIME [epoch: 35.9 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18393671413778934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18393671413778934 | validation: 0.11147310052217321]
	TIME [epoch: 36.1 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10060148785611236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10060148785611236 | validation: 0.16803903515288884]
	TIME [epoch: 36.2 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06610138301913913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06610138301913913 | validation: 0.1533093042609336]
	TIME [epoch: 36.1 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08473618911085012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08473618911085012 | validation: 0.11691018591520282]
	TIME [epoch: 36.2 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06152770669681654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06152770669681654 | validation: 0.09231409233994425]
	TIME [epoch: 36.2 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056820794568299596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056820794568299596 | validation: 0.11591562958868971]
	TIME [epoch: 36.1 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07218026344345858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07218026344345858 | validation: 0.1406683504890961]
	TIME [epoch: 36 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09590624120637359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09590624120637359 | validation: 0.05154626883723029]
	TIME [epoch: 36.2 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04672897932286938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04672897932286938 | validation: 0.0391538709251459]
	TIME [epoch: 36.2 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029464789408256135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029464789408256135 | validation: 0.03250313708444063]
	TIME [epoch: 36.2 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023733240632871598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023733240632871598 | validation: 0.014662944127370249]
	TIME [epoch: 36.2 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02034079730941106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02034079730941106 | validation: 0.014398602563953445]
	TIME [epoch: 36.1 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019884819744278345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019884819744278345 | validation: 0.018791934172615765]
	TIME [epoch: 36.2 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018752501097956596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018752501097956596 | validation: 0.01770482008543254]
	TIME [epoch: 36.1 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02329358005993509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02329358005993509 | validation: 0.029848147345908384]
	TIME [epoch: 36.1 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03625767797473594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03625767797473594 | validation: 0.05447865996493876]
	TIME [epoch: 36 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05115969851026032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05115969851026032 | validation: 0.03094788528420869]
	TIME [epoch: 36.2 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03755835439689613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03755835439689613 | validation: 0.044328598018639656]
	TIME [epoch: 36.1 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03794707798046723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03794707798046723 | validation: 0.019120032833337654]
	TIME [epoch: 36.2 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03013335196738308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03013335196738308 | validation: 0.03201864447730678]
	TIME [epoch: 36 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028885667971650517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028885667971650517 | validation: 0.025705662779545647]
	TIME [epoch: 36.1 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034100310879663996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034100310879663996 | validation: 0.03426971488384297]
	TIME [epoch: 36 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0389765211148969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0389765211148969 | validation: 0.03466186097922467]
	TIME [epoch: 35.9 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038763694901161355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038763694901161355 | validation: 0.013583805244913705]
	TIME [epoch: 36 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022816062239351394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022816062239351394 | validation: 0.04025489608238991]
	TIME [epoch: 36 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044594159880237835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044594159880237835 | validation: 0.06433100281564395]
	TIME [epoch: 36.1 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0381248999949189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0381248999949189 | validation: 0.05556825701144789]
	TIME [epoch: 36 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06654839958310327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06654839958310327 | validation: 0.0501976255647964]
	TIME [epoch: 36.2 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06259400177780416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06259400177780416 | validation: 0.04822003319070555]
	TIME [epoch: 36 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04356009340810773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04356009340810773 | validation: 0.0749471893516465]
	TIME [epoch: 36.2 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07420381552768628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07420381552768628 | validation: 0.0821095203733388]
	TIME [epoch: 36.1 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06874445910682894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06874445910682894 | validation: 0.11202524653183549]
	TIME [epoch: 36.2 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07240751580978849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07240751580978849 | validation: 0.1754174078591417]
	TIME [epoch: 36.2 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15146322958426664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15146322958426664 | validation: 0.07537743062523242]
	TIME [epoch: 36.1 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07934343995329593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07934343995329593 | validation: 0.05170606262402064]
	TIME [epoch: 36.1 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048034011848982504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048034011848982504 | validation: 0.04815678464055352]
	TIME [epoch: 36.1 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032458440007058144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032458440007058144 | validation: 0.02404606799056055]
	TIME [epoch: 36.1 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02919555438904662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02919555438904662 | validation: 0.02522296854378683]
	TIME [epoch: 36.2 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022631125793883013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022631125793883013 | validation: 0.02732121197542403]
	TIME [epoch: 36.2 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028291189591108656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028291189591108656 | validation: 0.04579570383942941]
	TIME [epoch: 36.1 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047601965763034625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047601965763034625 | validation: 0.038818430643607675]
	TIME [epoch: 36.2 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044493203009062655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044493203009062655 | validation: 0.029754880876596404]
	TIME [epoch: 36.1 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03125987367992678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03125987367992678 | validation: 0.0154314302487235]
	TIME [epoch: 36.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_174127/states/model_phi1_4c_v_mmd1_1286.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 22642.028 seconds.
