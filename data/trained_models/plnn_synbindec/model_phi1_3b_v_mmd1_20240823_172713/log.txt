Args:
Namespace(name='model_phi1_3b_v_mmd1', outdir='out/model_training/model_phi1_3b_v_mmd1', training_data='data/training_data/data_phi1_3b/training', validation_data='data/training_data/data_phi1_3b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3147820178

Training model...

Saving initial model state to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.888516324277259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.888516324277259 | validation: 5.1604251186121175]
	TIME [epoch: 31.3 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.525238594287556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.525238594287556 | validation: 5.143535474228818]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.217820810848673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.217820810848673 | validation: 4.901038052041944]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.105556141994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.105556141994 | validation: 4.823033407476986]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.05676087235858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.05676087235858 | validation: 4.84142798484686]
	TIME [epoch: 1.77 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.995010493639846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.995010493639846 | validation: 4.709669675306343]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.864281331036336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.864281331036336 | validation: 4.235356589653328]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.438445228894021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.438445228894021 | validation: 2.605484236252189]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6029648109884533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6029648109884533 | validation: 2.6985814815476954]
	TIME [epoch: 1.78 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.254979430266491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.254979430266491 | validation: 2.845540738871086]
	TIME [epoch: 1.77 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3881193896863757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3881193896863757 | validation: 3.8883059184363216]
	TIME [epoch: 1.77 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0425625266034286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0425625266034286 | validation: 1.6005865861510065]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.79091595263105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.79091595263105 | validation: 1.5677345381982146]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6920046764152221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6920046764152221 | validation: 1.3991035382020565]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3676310332367623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3676310332367623 | validation: 1.1066168860646763]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0896958575411275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0896958575411275 | validation: 1.0772647943923548]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155343449563296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.155343449563296 | validation: 1.5201396897175234]
	TIME [epoch: 1.78 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4490622614927577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4490622614927577 | validation: 1.1617136258874659]
	TIME [epoch: 1.79 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2291654724118042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2291654724118042 | validation: 1.074605785334968]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0597475787831427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0597475787831427 | validation: 1.0445146300787056]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0195963639621286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0195963639621286 | validation: 0.970628771283775]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.021677589254387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.021677589254387 | validation: 0.9825651472442509]
	TIME [epoch: 1.77 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0052704935606593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0052704935606593 | validation: 0.9633541343509734]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9795213962430682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9795213962430682 | validation: 1.0034949102927546]
	TIME [epoch: 1.77 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9841804586637336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9841804586637336 | validation: 0.9166489468224607]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9607730739435953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9607730739435953 | validation: 0.9417464525528305]
	TIME [epoch: 1.77 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9516454612474433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9516454612474433 | validation: 0.9363061310870769]
	TIME [epoch: 1.77 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9470791370652611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9470791370652611 | validation: 0.985790719457718]
	TIME [epoch: 1.77 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.970707401441998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.970707401441998 | validation: 0.9424358222534942]
	TIME [epoch: 1.77 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9732195811225262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9732195811225262 | validation: 1.033700592867812]
	TIME [epoch: 1.77 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9888183778954845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9888183778954845 | validation: 0.9821881308307543]
	TIME [epoch: 1.77 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.994851915129035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.994851915129035 | validation: 1.134296636856929]
	TIME [epoch: 1.77 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0224249796988503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0224249796988503 | validation: 0.9654309082224377]
	TIME [epoch: 1.77 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9578603968555917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9578603968555917 | validation: 0.9456350201406611]
	TIME [epoch: 1.79 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.905894022280923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.905894022280923 | validation: 0.8695237402045859]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8738109153997651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8738109153997651 | validation: 0.8775936508820568]
	TIME [epoch: 1.78 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.866072467203411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.866072467203411 | validation: 0.8576365852275529]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859014436027443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.859014436027443 | validation: 0.9067174061728709]
	TIME [epoch: 1.78 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859887531093662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.859887531093662 | validation: 0.8706049177406471]
	TIME [epoch: 1.78 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8706671853709347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8706671853709347 | validation: 0.9681991902564013]
	TIME [epoch: 1.78 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.901658265115911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.901658265115911 | validation: 0.9308498383797632]
	TIME [epoch: 1.77 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9171699382307733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9171699382307733 | validation: 0.9735834643298951]
	TIME [epoch: 1.78 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.889130981913975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.889130981913975 | validation: 0.8380364936713939]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.826128372671181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.826128372671181 | validation: 0.859977444633633]
	TIME [epoch: 1.77 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8195385598280138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8195385598280138 | validation: 0.9773619859582987]
	TIME [epoch: 1.77 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8724309363017554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8724309363017554 | validation: 1.0064922069302564]
	TIME [epoch: 1.78 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0656682586384048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0656682586384048 | validation: 1.200955584460574]
	TIME [epoch: 1.78 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0003833900461272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0003833900461272 | validation: 0.8790181797281624]
	TIME [epoch: 1.77 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8137501786751861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8137501786751861 | validation: 0.8768717321721422]
	TIME [epoch: 1.78 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8813191390015829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8813191390015829 | validation: 0.8477622885735931]
	TIME [epoch: 1.77 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.798499625910851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.798499625910851 | validation: 0.9107887233186625]
	TIME [epoch: 1.77 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8265673516348403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8265673516348403 | validation: 0.8709085513632866]
	TIME [epoch: 1.78 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7992552004632207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7992552004632207 | validation: 0.8055412122670232]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7708910518795636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7708910518795636 | validation: 0.8234015256538116]
	TIME [epoch: 1.78 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764824115827384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.764824115827384 | validation: 0.8076091303118867]
	TIME [epoch: 1.78 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7580129777399964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7580129777399964 | validation: 0.8095538200132257]
	TIME [epoch: 1.77 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.753062218547782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.753062218547782 | validation: 0.7932481315299108]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7451462646664299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7451462646664299 | validation: 0.783011073813979]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7575650220876622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7575650220876622 | validation: 0.9120794788437886]
	TIME [epoch: 1.77 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7976988217579337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7976988217579337 | validation: 0.8266123813551328]
	TIME [epoch: 1.77 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7976914373064021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7976914373064021 | validation: 0.8136015437170173]
	TIME [epoch: 1.78 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7535560263721192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7535560263721192 | validation: 0.7955591055702119]
	TIME [epoch: 1.77 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7378286932748225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7378286932748225 | validation: 0.8382371947871695]
	TIME [epoch: 1.77 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7967056888957033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7967056888957033 | validation: 1.0763732636966399]
	TIME [epoch: 1.77 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9901690630235279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9901690630235279 | validation: 0.8073792645422817]
	TIME [epoch: 1.77 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7492759364788566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7492759364788566 | validation: 0.7798710942200293]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7383377032836012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7383377032836012 | validation: 0.8301468820926754]
	TIME [epoch: 1.77 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7782610801846765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7782610801846765 | validation: 0.7702718098974211]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7185418242187072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7185418242187072 | validation: 0.7916377716846553]
	TIME [epoch: 1.77 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254258708825507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7254258708825507 | validation: 0.772068085808292]
	TIME [epoch: 1.79 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.730338049236239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.730338049236239 | validation: 0.8881993357462922]
	TIME [epoch: 1.78 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7727329053398142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7727329053398142 | validation: 0.824085304192715]
	TIME [epoch: 1.78 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8239484782996215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8239484782996215 | validation: 0.8411607060744285]
	TIME [epoch: 1.78 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7442169292570031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7442169292570031 | validation: 0.7730116147286454]
	TIME [epoch: 1.77 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7284185420306633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7284185420306633 | validation: 0.8189992729878451]
	TIME [epoch: 1.77 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7514680550336286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7514680550336286 | validation: 0.7960626795448962]
	TIME [epoch: 1.77 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7428302163063419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7428302163063419 | validation: 0.7503466910739398]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7148093780723721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7148093780723721 | validation: 0.7446829515833149]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7054170349187671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7054170349187671 | validation: 0.7810951482269591]
	TIME [epoch: 1.77 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7138418207947719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7138418207947719 | validation: 0.7565841841925423]
	TIME [epoch: 1.77 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7440684720215077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7440684720215077 | validation: 0.9491014334775358]
	TIME [epoch: 1.77 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941170569653662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7941170569653662 | validation: 0.768457897395657]
	TIME [epoch: 1.77 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7661182126074215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7661182126074215 | validation: 0.8231617129267342]
	TIME [epoch: 1.77 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305910748535894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7305910748535894 | validation: 0.7387338052543878]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7134759295187416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7134759295187416 | validation: 0.8062853367553879]
	TIME [epoch: 1.77 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7398324476365378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7398324476365378 | validation: 0.8051817109017216]
	TIME [epoch: 1.77 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7536646134389383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7536646134389383 | validation: 0.8256072747353342]
	TIME [epoch: 1.77 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7748118241549332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7748118241549332 | validation: 0.7555152776886707]
	TIME [epoch: 1.77 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7103082660234273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7103082660234273 | validation: 0.8068008444742963]
	TIME [epoch: 1.78 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7151168782750442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7151168782750442 | validation: 0.741726558560877]
	TIME [epoch: 1.77 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173965714466839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7173965714466839 | validation: 0.8009478995030475]
	TIME [epoch: 1.77 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7194643629573088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7194643629573088 | validation: 0.7312249595204923]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7098997368198302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7098997368198302 | validation: 0.7717222419516097]
	TIME [epoch: 1.77 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7069366815461254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7069366815461254 | validation: 0.7202222324111262]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066388981041989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066388981041989 | validation: 0.8113307916130221]
	TIME [epoch: 1.77 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7185206851748583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7185206851748583 | validation: 0.7635182871584318]
	TIME [epoch: 1.77 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7673112249209553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7673112249209553 | validation: 0.986415480103829]
	TIME [epoch: 1.77 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8272913308001194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8272913308001194 | validation: 0.7372487313542037]
	TIME [epoch: 1.77 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7229957397660555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7229957397660555 | validation: 0.7627585356943669]
	TIME [epoch: 1.78 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7125722319005189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7125722319005189 | validation: 0.8049927718619281]
	TIME [epoch: 1.78 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7297652201653729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7297652201653729 | validation: 0.8149400813107601]
	TIME [epoch: 1.77 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047056098069857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8047056098069857 | validation: 0.7800156361504572]
	TIME [epoch: 1.77 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7178238635280698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7178238635280698 | validation: 0.738590521409033]
	TIME [epoch: 1.77 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6943915279588094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6943915279588094 | validation: 0.7191407823999659]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7067554696300087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7067554696300087 | validation: 0.8304115562322676]
	TIME [epoch: 1.77 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7344436321053905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7344436321053905 | validation: 0.7387087472043667]
	TIME [epoch: 1.77 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7318128116008886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7318128116008886 | validation: 0.7896079798498938]
	TIME [epoch: 1.79 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.725100594172125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.725100594172125 | validation: 0.734420067060991]
	TIME [epoch: 1.78 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182241364879789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7182241364879789 | validation: 0.8173301679171525]
	TIME [epoch: 1.77 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7711168250527497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7711168250527497 | validation: 0.805744160501914]
	TIME [epoch: 1.77 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7383232621767744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7383232621767744 | validation: 0.7453823730622332]
	TIME [epoch: 1.77 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7198716070085496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7198716070085496 | validation: 0.7626892062091336]
	TIME [epoch: 1.77 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6992525661955002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6992525661955002 | validation: 0.7291022842934523]
	TIME [epoch: 1.77 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.701681662921956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.701681662921956 | validation: 0.7802539892841865]
	TIME [epoch: 1.77 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7078461339411142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7078461339411142 | validation: 0.7471937869146]
	TIME [epoch: 1.77 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7148177333669645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7148177333669645 | validation: 0.8547960686759808]
	TIME [epoch: 1.77 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7436560343571105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7436560343571105 | validation: 0.7410374125945353]
	TIME [epoch: 1.77 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7433135782445052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7433135782445052 | validation: 0.8338845133163583]
	TIME [epoch: 1.77 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7357803591360776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7357803591360776 | validation: 0.7526820161729042]
	TIME [epoch: 1.77 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299565785380027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7299565785380027 | validation: 0.7911958376315402]
	TIME [epoch: 1.77 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7140757400313139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7140757400313139 | validation: 0.7473660163627152]
	TIME [epoch: 1.78 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7031438146389667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7031438146389667 | validation: 0.7400911490946859]
	TIME [epoch: 1.77 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7023194102408985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7023194102408985 | validation: 0.7982736130837877]
	TIME [epoch: 1.77 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7128795665678641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7128795665678641 | validation: 0.7126667603742842]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7017489963029511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7017489963029511 | validation: 0.7664818670625102]
	TIME [epoch: 1.78 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7056293036314439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7056293036314439 | validation: 0.7176466509367541]
	TIME [epoch: 1.79 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918357299146911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6918357299146911 | validation: 0.7724269393979091]
	TIME [epoch: 1.78 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7253919880318682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7253919880318682 | validation: 0.8259551552045743]
	TIME [epoch: 1.78 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7588648856588285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7588648856588285 | validation: 0.8148859077648313]
	TIME [epoch: 1.78 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8068115458601056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8068115458601056 | validation: 0.7791530884587905]
	TIME [epoch: 1.78 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7161013092544448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7161013092544448 | validation: 0.7432380484310297]
	TIME [epoch: 1.78 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.735024688973293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.735024688973293 | validation: 0.8092702622861744]
	TIME [epoch: 1.78 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7274704022761211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7274704022761211 | validation: 0.7463467123394942]
	TIME [epoch: 1.78 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6927392145959734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6927392145959734 | validation: 0.7334038123343941]
	TIME [epoch: 1.78 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712421010772365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.712421010772365 | validation: 0.8757357240965469]
	TIME [epoch: 1.77 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7543462134478236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7543462134478236 | validation: 0.7492450451156437]
	TIME [epoch: 1.78 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7386359577889414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7386359577889414 | validation: 0.8066729484846333]
	TIME [epoch: 1.78 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7013720407944264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7013720407944264 | validation: 0.7225698262210024]
	TIME [epoch: 1.78 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6855503019004285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6855503019004285 | validation: 0.735694598770565]
	TIME [epoch: 1.77 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6812052316886132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6812052316886132 | validation: 0.7423736862091986]
	TIME [epoch: 1.78 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687229088262388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687229088262388 | validation: 0.7300323351856792]
	TIME [epoch: 1.78 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6849782813544704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6849782813544704 | validation: 0.7621296280787624]
	TIME [epoch: 1.77 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6927208909739696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6927208909739696 | validation: 0.7511782452667328]
	TIME [epoch: 1.77 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7346855781028708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7346855781028708 | validation: 0.9016135250267191]
	TIME [epoch: 1.77 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597196195104016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7597196195104016 | validation: 0.7418909423231045]
	TIME [epoch: 1.78 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7544964884892416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7544964884892416 | validation: 0.7605720402718239]
	TIME [epoch: 1.79 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6964443877981847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6964443877981847 | validation: 0.7755775963667451]
	TIME [epoch: 1.78 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7374160041516723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7374160041516723 | validation: 0.8256697101527938]
	TIME [epoch: 1.78 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564097135852916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7564097135852916 | validation: 0.7329662537138724]
	TIME [epoch: 1.78 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6865006766505607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6865006766505607 | validation: 0.7931314740910234]
	TIME [epoch: 1.77 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7179844274510987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7179844274510987 | validation: 0.7529678269185119]
	TIME [epoch: 1.78 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7488974479156975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7488974479156975 | validation: 0.7649167460870461]
	TIME [epoch: 1.78 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6915635704190843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6915635704190843 | validation: 0.7683552940565255]
	TIME [epoch: 1.77 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.706277819716856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.706277819716856 | validation: 0.7657443510148374]
	TIME [epoch: 1.77 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7070892857395031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7070892857395031 | validation: 0.7385068751259125]
	TIME [epoch: 1.77 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6883730062197659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6883730062197659 | validation: 0.7302873024560672]
	TIME [epoch: 1.77 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6879949817504527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6879949817504527 | validation: 0.925269714579871]
	TIME [epoch: 1.77 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7751479429679126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7751479429679126 | validation: 0.7758924974837976]
	TIME [epoch: 1.77 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.768177322654376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.768177322654376 | validation: 0.8120241762540316]
	TIME [epoch: 1.77 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7225747632258512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7225747632258512 | validation: 0.7469448707317844]
	TIME [epoch: 1.77 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6966987101146555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6966987101146555 | validation: 0.7348691022701961]
	TIME [epoch: 1.77 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7180552994973696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7180552994973696 | validation: 0.7732366430556911]
	TIME [epoch: 1.77 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6957915755811368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6957915755811368 | validation: 0.7285447378081851]
	TIME [epoch: 1.77 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6878638310348049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6878638310348049 | validation: 0.7365498740462058]
	TIME [epoch: 1.77 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.695741063061706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.695741063061706 | validation: 0.7388568754049689]
	TIME [epoch: 1.78 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6795126443476218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6795126443476218 | validation: 0.7483264426856863]
	TIME [epoch: 1.79 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.678708045990508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.678708045990508 | validation: 0.7217995615721678]
	TIME [epoch: 1.78 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6793273871562957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6793273871562957 | validation: 0.7256406488821215]
	TIME [epoch: 1.78 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6767783011650752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6767783011650752 | validation: 0.7252641569226569]
	TIME [epoch: 1.77 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6819715342474819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6819715342474819 | validation: 0.7624096748033015]
	TIME [epoch: 1.77 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6933129625135621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6933129625135621 | validation: 0.7566180751666735]
	TIME [epoch: 1.77 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7531598118910401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7531598118910401 | validation: 1.1664314866924848]
	TIME [epoch: 1.77 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9284162112204503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9284162112204503 | validation: 0.7803025129498]
	TIME [epoch: 1.77 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7435945862951928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7435945862951928 | validation: 0.762548019724111]
	TIME [epoch: 1.77 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.750801490111715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.750801490111715 | validation: 0.8124658499917417]
	TIME [epoch: 1.77 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7203705517595357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7203705517595357 | validation: 0.733675264507639]
	TIME [epoch: 1.77 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6883065596286605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6883065596286605 | validation: 0.7264552362462422]
	TIME [epoch: 1.77 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6887498150867767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6887498150867767 | validation: 0.8303870116195604]
	TIME [epoch: 1.77 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305929390782572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7305929390782572 | validation: 0.7554288855986336]
	TIME [epoch: 1.78 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7571437669413584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7571437669413584 | validation: 0.7328344159512025]
	TIME [epoch: 1.77 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6800724750775089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6800724750775089 | validation: 0.7673571791865004]
	TIME [epoch: 1.77 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7012573733443904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7012573733443904 | validation: 0.7251743247699182]
	TIME [epoch: 1.77 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6925536404156557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6925536404156557 | validation: 0.7205561968394455]
	TIME [epoch: 1.77 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6786189757471756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6786189757471756 | validation: 0.7430541162959575]
	TIME [epoch: 1.78 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6835417587463121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6835417587463121 | validation: 0.7257514007094722]
	TIME [epoch: 1.79 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7027423074665652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7027423074665652 | validation: 0.782931935753712]
	TIME [epoch: 1.78 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003940021358754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7003940021358754 | validation: 0.7372641914174205]
	TIME [epoch: 1.78 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7069128135996069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7069128135996069 | validation: 0.7693875980526607]
	TIME [epoch: 1.78 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6888998244841505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6888998244841505 | validation: 0.7079120684886551]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6933855708011138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6933855708011138 | validation: 0.7659493005367273]
	TIME [epoch: 1.77 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7025576088744291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7025576088744291 | validation: 0.7624619656190561]
	TIME [epoch: 1.77 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7133978041618098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7133978041618098 | validation: 0.7428995539572232]
	TIME [epoch: 1.77 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7196127518562432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7196127518562432 | validation: 0.7524535704199791]
	TIME [epoch: 1.77 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6809232118139571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6809232118139571 | validation: 0.7256382513620463]
	TIME [epoch: 1.77 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.700174289285887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.700174289285887 | validation: 0.809569474974698]
	TIME [epoch: 1.77 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7298670825851056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7298670825851056 | validation: 0.724719987640869]
	TIME [epoch: 1.77 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7020635987861583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7020635987861583 | validation: 0.7670651450733685]
	TIME [epoch: 1.77 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6927905521420215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6927905521420215 | validation: 0.7148928788450863]
	TIME [epoch: 1.77 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7128049000613466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7128049000613466 | validation: 0.7849959878664544]
	TIME [epoch: 1.77 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7013142945251244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7013142945251244 | validation: 0.7107963976363884]
	TIME [epoch: 1.77 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6884851113703048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6884851113703048 | validation: 0.7582015974867791]
	TIME [epoch: 29.7 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6815180722962778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6815180722962778 | validation: 0.7198973016872182]
	TIME [epoch: 3.51 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6876157998545083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6876157998545083 | validation: 0.7433785977752838]
	TIME [epoch: 3.51 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6827333459044757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6827333459044757 | validation: 0.7200585828957821]
	TIME [epoch: 3.51 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.681965823856859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.681965823856859 | validation: 0.7693238902256939]
	TIME [epoch: 3.51 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6923212248430131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6923212248430131 | validation: 0.7390367127117186]
	TIME [epoch: 3.51 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7309527646379027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7309527646379027 | validation: 0.9790776136223279]
	TIME [epoch: 3.51 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7934691537914723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7934691537914723 | validation: 0.7251231550277439]
	TIME [epoch: 3.53 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254163943763712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7254163943763712 | validation: 0.7136555219196172]
	TIME [epoch: 3.51 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.680099709356774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.680099709356774 | validation: 0.8129545351826185]
	TIME [epoch: 3.51 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7176359577981012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7176359577981012 | validation: 0.7132881096963661]
	TIME [epoch: 3.51 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6922381223772799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6922381223772799 | validation: 0.7336477757859101]
	TIME [epoch: 3.51 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6785254471085986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6785254471085986 | validation: 0.7195005695349788]
	TIME [epoch: 3.51 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6801898081079508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6801898081079508 | validation: 0.7443630619246311]
	TIME [epoch: 3.5 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6911951724147669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6911951724147669 | validation: 0.7624998936970631]
	TIME [epoch: 3.51 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6864782621169967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6864782621169967 | validation: 0.7141532158111343]
	TIME [epoch: 3.51 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6886645103874568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6886645103874568 | validation: 0.73391948154292]
	TIME [epoch: 3.51 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6729364076510711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6729364076510711 | validation: 0.729146722407555]
	TIME [epoch: 3.51 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6786491206235706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6786491206235706 | validation: 0.7262281161144104]
	TIME [epoch: 3.51 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6639313295192028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6639313295192028 | validation: 0.7236579759121775]
	TIME [epoch: 3.51 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6785985930772295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6785985930772295 | validation: 0.7738610517574827]
	TIME [epoch: 3.51 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6831820088945793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6831820088945793 | validation: 0.7644500688434074]
	TIME [epoch: 3.51 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77842802848032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.77842802848032 | validation: 0.9660585252846439]
	TIME [epoch: 3.51 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8174916315286807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8174916315286807 | validation: 0.7171488642744932]
	TIME [epoch: 3.5 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6738658638645992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6738658638645992 | validation: 0.7118256589145115]
	TIME [epoch: 3.51 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7145388805001993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7145388805001993 | validation: 0.8696058121799013]
	TIME [epoch: 3.51 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7376889984773323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7376889984773323 | validation: 0.7512811300832754]
	TIME [epoch: 3.52 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7017801887014747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7017801887014747 | validation: 0.7247481081486707]
	TIME [epoch: 3.52 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6733021941717595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6733021941717595 | validation: 0.7579718470902472]
	TIME [epoch: 3.52 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6881969099242872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6881969099242872 | validation: 0.7199114599812115]
	TIME [epoch: 3.51 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6826828417077496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6826828417077496 | validation: 0.7203862869676524]
	TIME [epoch: 3.51 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6647998396318741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6647998396318741 | validation: 0.7293020217877568]
	TIME [epoch: 3.51 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6790912301856495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6790912301856495 | validation: 0.707059035782864]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6755059282181841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6755059282181841 | validation: 0.7626404566072204]
	TIME [epoch: 3.53 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.686566589431287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.686566589431287 | validation: 0.7174166497073271]
	TIME [epoch: 3.53 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7193895586305237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7193895586305237 | validation: 0.8123565640935482]
	TIME [epoch: 3.53 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7264976146224883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7264976146224883 | validation: 0.7175928281909569]
	TIME [epoch: 3.53 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685244794075627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.685244794075627 | validation: 0.7011284666926696]
	TIME [epoch: 3.54 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6708817762155682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6708817762155682 | validation: 0.7363141479540825]
	TIME [epoch: 3.51 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6746768125669477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6746768125669477 | validation: 0.7057275606203546]
	TIME [epoch: 3.51 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6628305234662335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6628305234662335 | validation: 0.7423904589870033]
	TIME [epoch: 3.51 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6844519735512882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6844519735512882 | validation: 0.7196001550669665]
	TIME [epoch: 3.51 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969594990349615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6969594990349615 | validation: 0.8588370446849928]
	TIME [epoch: 3.51 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7405239724687087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7405239724687087 | validation: 0.7586560380699778]
	TIME [epoch: 3.51 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576963286781636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7576963286781636 | validation: 0.7803092723175784]
	TIME [epoch: 3.51 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6987538212745273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6987538212745273 | validation: 0.7167686130493947]
	TIME [epoch: 3.51 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6714099743499066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6714099743499066 | validation: 0.7069614118564911]
	TIME [epoch: 3.54 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.665075697353238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.665075697353238 | validation: 0.7200631257102845]
	TIME [epoch: 3.52 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6673182940308203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6673182940308203 | validation: 0.7045086673868008]
	TIME [epoch: 3.51 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6798517602898175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6798517602898175 | validation: 0.783288808477102]
	TIME [epoch: 3.5 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7024753036122882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7024753036122882 | validation: 0.7176820779139572]
	TIME [epoch: 3.51 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6984777662290964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6984777662290964 | validation: 0.7333496582545052]
	TIME [epoch: 3.5 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6733206315509646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6733206315509646 | validation: 0.7053265853037551]
	TIME [epoch: 3.51 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6743540741928018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6743540741928018 | validation: 0.7576251736920006]
	TIME [epoch: 3.5 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6828918119942633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6828918119942633 | validation: 0.7523492846576636]
	TIME [epoch: 3.51 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6964045278318858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6964045278318858 | validation: 0.7491332433667599]
	TIME [epoch: 3.5 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7184166592756194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7184166592756194 | validation: 0.7824332061983899]
	TIME [epoch: 3.51 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6929023724073308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6929023724073308 | validation: 0.7109490141142932]
	TIME [epoch: 3.51 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6767564193626134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6767564193626134 | validation: 0.7725817114083838]
	TIME [epoch: 3.52 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6855414107955793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6855414107955793 | validation: 0.7202302007387056]
	TIME [epoch: 3.51 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.697046795419503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.697046795419503 | validation: 0.7713398210680866]
	TIME [epoch: 3.51 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6890800001411063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6890800001411063 | validation: 0.704867591483726]
	TIME [epoch: 3.5 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.666889323941563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.666889323941563 | validation: 0.7232322916905464]
	TIME [epoch: 3.51 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6559813571197677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6559813571197677 | validation: 0.7118572130170342]
	TIME [epoch: 3.5 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6590639058626804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6590639058626804 | validation: 0.7131026937767315]
	TIME [epoch: 3.51 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6590207047727049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6590207047727049 | validation: 0.7516195804972599]
	TIME [epoch: 3.53 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6797037436225954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6797037436225954 | validation: 0.7677309919074198]
	TIME [epoch: 3.51 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.787157061657274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.787157061657274 | validation: 0.9728132609094061]
	TIME [epoch: 3.51 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.799405141930236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.799405141930236 | validation: 0.718355888864791]
	TIME [epoch: 3.52 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6644185779622199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6644185779622199 | validation: 0.7081917158778324]
	TIME [epoch: 3.51 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6912027436591595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6912027436591595 | validation: 0.7915005290889315]
	TIME [epoch: 3.5 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6921715654021001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6921715654021001 | validation: 0.6987827668815773]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6722722847133379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6722722847133379 | validation: 0.7280214526139691]
	TIME [epoch: 3.5 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6615201624357107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6615201624357107 | validation: 0.7179657872096679]
	TIME [epoch: 3.5 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6675622224129835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6675622224129835 | validation: 0.7106986503102325]
	TIME [epoch: 3.51 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6669110815833479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6669110815833479 | validation: 0.7035787419689332]
	TIME [epoch: 3.5 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6580085897202892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6580085897202892 | validation: 0.7112173495009686]
	TIME [epoch: 3.5 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.662287324499808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.662287324499808 | validation: 0.7043349891420516]
	TIME [epoch: 3.51 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6708939815836018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6708939815836018 | validation: 0.7320670435273938]
	TIME [epoch: 3.52 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6810562458576291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6810562458576291 | validation: 0.7619335352582359]
	TIME [epoch: 3.51 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7386806527723079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7386806527723079 | validation: 0.7995747831275071]
	TIME [epoch: 3.51 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6944159795138642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6944159795138642 | validation: 0.688163466199936]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6967393681441321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6967393681441321 | validation: 0.8486931869461767]
	TIME [epoch: 3.51 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7357772066433208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7357772066433208 | validation: 0.7041687038998609]
	TIME [epoch: 3.51 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6851074445206465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6851074445206465 | validation: 0.7200051741925358]
	TIME [epoch: 3.51 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6775977727907702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6775977727907702 | validation: 0.7013821178105949]
	TIME [epoch: 3.52 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6601435811189887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6601435811189887 | validation: 0.7110779349077561]
	TIME [epoch: 3.51 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6595049086617664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6595049086617664 | validation: 0.7008921885963297]
	TIME [epoch: 3.51 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6547237230933385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6547237230933385 | validation: 0.717134064659931]
	TIME [epoch: 3.53 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6586439576704587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6586439576704587 | validation: 0.703670142197746]
	TIME [epoch: 3.52 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.682854391700599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.682854391700599 | validation: 0.795734566055422]
	TIME [epoch: 3.51 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7029874802359194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7029874802359194 | validation: 0.7090509435052043]
	TIME [epoch: 3.51 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7247905689519513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7247905689519513 | validation: 0.7910754921636011]
	TIME [epoch: 3.51 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6983895605721055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6983895605721055 | validation: 0.6995119164142918]
	TIME [epoch: 3.52 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6874140344374925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6874140344374925 | validation: 0.7798362487963533]
	TIME [epoch: 3.52 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6897790244432036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6897790244432036 | validation: 0.7031324181043834]
	TIME [epoch: 3.51 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6726619621523318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6726619621523318 | validation: 0.7274253546917611]
	TIME [epoch: 3.52 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6577428575799807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6577428575799807 | validation: 0.6938747153996404]
	TIME [epoch: 3.51 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6730026402613267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6730026402613267 | validation: 0.7252107142664349]
	TIME [epoch: 3.52 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6717142247231503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6717142247231503 | validation: 0.7031197426689485]
	TIME [epoch: 3.53 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6705912920463912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6705912920463912 | validation: 0.6878696130743759]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6481211114947363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6481211114947363 | validation: 0.6902353590097455]
	TIME [epoch: 3.51 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6643002100103746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6643002100103746 | validation: 0.828024698417361]
	TIME [epoch: 3.51 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7318736743167793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7318736743167793 | validation: 0.7157370773478707]
	TIME [epoch: 3.51 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6987942024502556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6987942024502556 | validation: 0.7257640427474172]
	TIME [epoch: 3.51 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6621166069141978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6621166069141978 | validation: 0.6877963291720947]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6591313667618051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6591313667618051 | validation: 0.6911165230499972]
	TIME [epoch: 3.51 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6587069342732979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6587069342732979 | validation: 0.7049783763175883]
	TIME [epoch: 3.5 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6615147738725529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6615147738725529 | validation: 0.7127652273692444]
	TIME [epoch: 3.52 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.695963600005663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.695963600005663 | validation: 0.8212724468640739]
	TIME [epoch: 3.52 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7099364272239096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7099364272239096 | validation: 0.6941949984107985]
	TIME [epoch: 3.5 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.698077558308616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.698077558308616 | validation: 0.7493323495735004]
	TIME [epoch: 3.51 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6939205456052122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6939205456052122 | validation: 0.7294094671085317]
	TIME [epoch: 3.5 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6726654096332368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6726654096332368 | validation: 0.6866664067043327]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.672757439133889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.672757439133889 | validation: 0.7515870761890646]
	TIME [epoch: 3.5 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6718082586958093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6718082586958093 | validation: 0.7097972287185078]
	TIME [epoch: 3.5 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6625062735727678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6625062735727678 | validation: 0.7247650164415852]
	TIME [epoch: 3.5 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.670395784834158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.670395784834158 | validation: 0.7294802241691568]
	TIME [epoch: 3.5 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6806971051879914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6806971051879914 | validation: 0.7261402782901326]
	TIME [epoch: 3.51 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6734078801834801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6734078801834801 | validation: 0.7135243540525424]
	TIME [epoch: 3.52 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6829200642709202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6829200642709202 | validation: 0.8765266853360996]
	TIME [epoch: 3.51 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7344735390037173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7344735390037173 | validation: 0.7166228059684862]
	TIME [epoch: 3.51 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7132472091918823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7132472091918823 | validation: 0.7283696075027895]
	TIME [epoch: 3.51 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569047513871276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6569047513871276 | validation: 0.7047163873162104]
	TIME [epoch: 3.51 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6512823729082804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6512823729082804 | validation: 0.6981168348492759]
	TIME [epoch: 3.51 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459121264466459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6459121264466459 | validation: 0.709921139853234]
	TIME [epoch: 3.5 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6546275103484162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6546275103484162 | validation: 0.6949943491088706]
	TIME [epoch: 3.5 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6488685076847329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6488685076847329 | validation: 0.7100755285660937]
	TIME [epoch: 3.51 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6534927167949457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6534927167949457 | validation: 0.6798971353620794]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6467739880584057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6467739880584057 | validation: 0.6988900447261517]
	TIME [epoch: 3.53 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6490182999829222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6490182999829222 | validation: 0.6859785248543622]
	TIME [epoch: 3.5 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503008168454865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6503008168454865 | validation: 0.7047284302940474]
	TIME [epoch: 3.5 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664313136079694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.664313136079694 | validation: 0.7245485258342813]
	TIME [epoch: 3.5 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6862564785385038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6862564785385038 | validation: 0.7956758306772469]
	TIME [epoch: 3.5 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.779772434520542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.779772434520542 | validation: 0.9243307444357769]
	TIME [epoch: 3.5 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7774206482440067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7774206482440067 | validation: 0.725432139905642]
	TIME [epoch: 3.52 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7069513244796043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7069513244796043 | validation: 0.7673772459706873]
	TIME [epoch: 3.5 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6951287149408519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6951287149408519 | validation: 0.6922824650993062]
	TIME [epoch: 3.5 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.655487325702816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.655487325702816 | validation: 0.7169709784314705]
	TIME [epoch: 3.5 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658031900619202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.658031900619202 | validation: 0.6865608152157513]
	TIME [epoch: 3.51 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6461064392706389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6461064392706389 | validation: 0.7079330227662575]
	TIME [epoch: 3.51 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6554127918748737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6554127918748737 | validation: 0.6794658987454936]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6565086491034944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6565086491034944 | validation: 0.7411463189480534]
	TIME [epoch: 3.5 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6599232854453768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6599232854453768 | validation: 0.681363441128918]
	TIME [epoch: 3.49 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6595683379122904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6595683379122904 | validation: 0.7071439487500274]
	TIME [epoch: 3.49 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.655230074293481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.655230074293481 | validation: 0.6918340743162483]
	TIME [epoch: 3.5 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6430439366879347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6430439366879347 | validation: 0.6886134321797105]
	TIME [epoch: 3.5 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6363634707580604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6363634707580604 | validation: 0.70464638826815]
	TIME [epoch: 3.5 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6473227660875872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6473227660875872 | validation: 0.6747791249886661]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6774171634654768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6774171634654768 | validation: 0.9061362133661585]
	TIME [epoch: 3.51 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7586215237431679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7586215237431679 | validation: 0.7566150117164988]
	TIME [epoch: 3.51 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7910686069116415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7910686069116415 | validation: 0.716613445970147]
	TIME [epoch: 3.49 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6557940804447989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6557940804447989 | validation: 0.7360229952810576]
	TIME [epoch: 3.5 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.676888419897183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.676888419897183 | validation: 0.7158748570704321]
	TIME [epoch: 3.49 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6910210923129173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6910210923129173 | validation: 0.7098710667249541]
	TIME [epoch: 3.5 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6497305943219394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6497305943219394 | validation: 0.768591670352542]
	TIME [epoch: 3.49 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.693529486742672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.693529486742672 | validation: 0.7225885663359375]
	TIME [epoch: 3.5 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6756314180364644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6756314180364644 | validation: 0.7252719732152827]
	TIME [epoch: 3.5 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592962551739389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6592962551739389 | validation: 0.7188988890055396]
	TIME [epoch: 3.5 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6658925738541757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6658925738541757 | validation: 0.7013337019290549]
	TIME [epoch: 3.51 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6613910964974378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6613910964974378 | validation: 0.7247892624299223]
	TIME [epoch: 3.51 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6598641850491711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6598641850491711 | validation: 0.6870333888415859]
	TIME [epoch: 3.5 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6924362932034811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6924362932034811 | validation: 0.7834090845729333]
	TIME [epoch: 3.5 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6833270548115137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6833270548115137 | validation: 0.6747041247196429]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.648658331273455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.648658331273455 | validation: 0.6963205931519452]
	TIME [epoch: 3.5 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6520483938860983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6520483938860983 | validation: 0.7609460843730942]
	TIME [epoch: 3.5 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6640483582867063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6640483582867063 | validation: 0.6874686201667585]
	TIME [epoch: 3.5 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6620147537490008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6620147537490008 | validation: 0.7309231602064157]
	TIME [epoch: 3.5 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6565800119795374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6565800119795374 | validation: 0.7073384653943773]
	TIME [epoch: 3.5 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6642326475912478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6642326475912478 | validation: 0.741711009200685]
	TIME [epoch: 3.5 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6610942955692269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6610942955692269 | validation: 0.7094311608121494]
	TIME [epoch: 3.51 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.643996456322697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.643996456322697 | validation: 0.6963832168184587]
	TIME [epoch: 3.5 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6388964779606593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6388964779606593 | validation: 0.699160260864551]
	TIME [epoch: 3.5 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6404629987866542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6404629987866542 | validation: 0.6857311602335441]
	TIME [epoch: 3.49 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6371826344871275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6371826344871275 | validation: 0.7049717263036099]
	TIME [epoch: 3.49 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6378137620046783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6378137620046783 | validation: 0.6953261882422883]
	TIME [epoch: 3.5 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650898687905494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.650898687905494 | validation: 0.8487532091035824]
	TIME [epoch: 3.5 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7273269516624356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7273269516624356 | validation: 0.8547077597254269]
	TIME [epoch: 3.5 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8901643204338933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8901643204338933 | validation: 0.7878001184792311]
	TIME [epoch: 3.5 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6674467485466066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6674467485466066 | validation: 0.7350208699056721]
	TIME [epoch: 3.5 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6722129534873099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6722129534873099 | validation: 0.708238899666522]
	TIME [epoch: 3.5 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6752463072076746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6752463072076746 | validation: 0.7110484080986477]
	TIME [epoch: 3.51 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.638917586668544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.638917586668544 | validation: 0.7397839886731669]
	TIME [epoch: 3.49 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6711304925345593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6711304925345593 | validation: 0.6928465874274755]
	TIME [epoch: 3.49 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6472420503041054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6472420503041054 | validation: 0.67739812967616]
	TIME [epoch: 3.5 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6416577925413464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6416577925413464 | validation: 0.6992846344896112]
	TIME [epoch: 3.5 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6437628564862803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6437628564862803 | validation: 0.6951961769765158]
	TIME [epoch: 3.5 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6451703212392979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6451703212392979 | validation: 0.6957987909371557]
	TIME [epoch: 3.5 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6362258868994335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6362258868994335 | validation: 0.6968104138364577]
	TIME [epoch: 3.52 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6681068416456165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6681068416456165 | validation: 0.922786787299452]
	TIME [epoch: 3.5 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7747906194138935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7747906194138935 | validation: 0.7126625572508971]
	TIME [epoch: 3.51 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816946301202065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6816946301202065 | validation: 0.7036105898309111]
	TIME [epoch: 3.51 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6468494568232888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6468494568232888 | validation: 0.7046568524524258]
	TIME [epoch: 3.5 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6397090254380606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6397090254380606 | validation: 0.6909010286817976]
	TIME [epoch: 3.5 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459675837755355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6459675837755355 | validation: 0.6792448343659919]
	TIME [epoch: 3.5 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6357889977335107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6357889977335107 | validation: 0.6922972456081355]
	TIME [epoch: 3.5 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427952110947374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6427952110947374 | validation: 0.6924275461289757]
	TIME [epoch: 3.5 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.66049786248195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.66049786248195 | validation: 0.7976802875155772]
	TIME [epoch: 3.49 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6966005483191485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6966005483191485 | validation: 0.7389095651770401]
	TIME [epoch: 3.5 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7137722109233913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7137722109233913 | validation: 0.6916490855787956]
	TIME [epoch: 3.5 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.65281097182753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.65281097182753 | validation: 0.7136714121087154]
	TIME [epoch: 3.51 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6552670952918999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6552670952918999 | validation: 0.6992792510257053]
	TIME [epoch: 3.51 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6516200549332107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6516200549332107 | validation: 0.7254388073934784]
	TIME [epoch: 3.5 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6378241635627317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6378241635627317 | validation: 0.6782660678898864]
	TIME [epoch: 3.5 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657799956008863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657799956008863 | validation: 0.7810531380573394]
	TIME [epoch: 3.5 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6764387895817668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6764387895817668 | validation: 0.6641434710720262]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6542247697384467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6542247697384467 | validation: 0.685761464298154]
	TIME [epoch: 3.5 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6395085483025581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6395085483025581 | validation: 0.6841222355255168]
	TIME [epoch: 3.49 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6367650641796663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6367650641796663 | validation: 0.7740998997669365]
	TIME [epoch: 3.5 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6826815487913896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6826815487913896 | validation: 0.7039185650943094]
	TIME [epoch: 3.49 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7048104850822806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7048104850822806 | validation: 0.7312745684263685]
	TIME [epoch: 3.5 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6639245733762291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6639245733762291 | validation: 0.674755774913065]
	TIME [epoch: 3.52 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.631471250682362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.631471250682362 | validation: 0.6688413793600954]
	TIME [epoch: 3.49 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6305903487185281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6305903487185281 | validation: 0.699628170047583]
	TIME [epoch: 3.5 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6295018800695776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6295018800695776 | validation: 0.6874754032979672]
	TIME [epoch: 3.49 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6415877040376043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6415877040376043 | validation: 0.7848774451589389]
	TIME [epoch: 3.49 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6990929242689987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6990929242689987 | validation: 0.779425742277598]
	TIME [epoch: 3.5 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7914832810485725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7914832810485725 | validation: 0.7430731111294891]
	TIME [epoch: 3.49 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.669848261768874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.669848261768874 | validation: 0.6871649496245116]
	TIME [epoch: 3.49 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6374002632036883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6374002632036883 | validation: 0.7049628518296385]
	TIME [epoch: 3.5 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6411701336662304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6411701336662304 | validation: 0.697075828091761]
	TIME [epoch: 3.5 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429341495405837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6429341495405837 | validation: 0.6923875022906947]
	TIME [epoch: 3.5 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.645406396388807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.645406396388807 | validation: 0.7275509767194858]
	TIME [epoch: 3.5 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6374888690019236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6374888690019236 | validation: 0.6872971593255344]
	TIME [epoch: 3.5 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6444749561346729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6444749561346729 | validation: 0.7119001691037371]
	TIME [epoch: 3.49 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6409913697117002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6409913697117002 | validation: 0.6829828491332732]
	TIME [epoch: 3.5 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6461368354729043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6461368354729043 | validation: 0.7601780951899633]
	TIME [epoch: 3.5 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6864160272189773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6864160272189773 | validation: 0.7055631958819936]
	TIME [epoch: 3.5 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6627286581385964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6627286581385964 | validation: 0.7134336963021137]
	TIME [epoch: 3.49 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6376679010575468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6376679010575468 | validation: 0.7138164458239988]
	TIME [epoch: 3.5 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6319834986600144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6319834986600144 | validation: 0.6797684283832459]
	TIME [epoch: 3.5 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6330256160924151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6330256160924151 | validation: 0.7244504653965606]
	TIME [epoch: 3.52 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6461767526451334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6461767526451334 | validation: 0.7107175292296104]
	TIME [epoch: 3.51 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7165946753590745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7165946753590745 | validation: 0.7971553813012306]
	TIME [epoch: 3.51 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6776139368410197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6776139368410197 | validation: 0.7007559427188689]
	TIME [epoch: 3.5 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6606156408393414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6606156408393414 | validation: 0.7404675968040645]
	TIME [epoch: 3.5 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6516687715573949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6516687715573949 | validation: 0.6772313154427879]
	TIME [epoch: 3.49 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6198254678501331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6198254678501331 | validation: 0.6491555788055186]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6374689972949402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6374689972949402 | validation: 0.7233134810160391]
	TIME [epoch: 3.51 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6460120923964875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6460120923964875 | validation: 0.7312535484791511]
	TIME [epoch: 3.51 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6824449961394348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6824449961394348 | validation: 0.7593956815458431]
	TIME [epoch: 3.51 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6828900935591153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6828900935591153 | validation: 0.6932714495814712]
	TIME [epoch: 3.52 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.632039815616336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.632039815616336 | validation: 0.7040070416760207]
	TIME [epoch: 3.54 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6319070936712774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6319070936712774 | validation: 0.6790946566068569]
	TIME [epoch: 3.51 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6479396477334516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6479396477334516 | validation: 0.7342175700933297]
	TIME [epoch: 3.51 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6466921785124918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6466921785124918 | validation: 0.6702120560350684]
	TIME [epoch: 3.51 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6506453665636356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6506453665636356 | validation: 0.7493130550534199]
	TIME [epoch: 3.51 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6557656439618802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6557656439618802 | validation: 0.6724316742706558]
	TIME [epoch: 3.51 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6595221768011965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6595221768011965 | validation: 0.7520059788185016]
	TIME [epoch: 3.51 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6470384164110702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6470384164110702 | validation: 0.6555446675494934]
	TIME [epoch: 3.51 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293672022337544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6293672022337544 | validation: 0.6921425354253663]
	TIME [epoch: 3.51 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6282775172821111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6282775172821111 | validation: 0.6976092482175771]
	TIME [epoch: 3.52 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6428833270070565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6428833270070565 | validation: 0.7512434776750498]
	TIME [epoch: 3.52 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7357007503505643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7357007503505643 | validation: 0.7318811073484934]
	TIME [epoch: 3.53 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6493360492031465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6493360492031465 | validation: 0.7234863419456631]
	TIME [epoch: 3.51 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6634248859728206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6634248859728206 | validation: 0.7023424551878139]
	TIME [epoch: 3.51 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6964013975947129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6964013975947129 | validation: 0.7345432615816483]
	TIME [epoch: 3.51 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6387479991558616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6387479991558616 | validation: 0.7063459537402715]
	TIME [epoch: 3.51 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427965862944458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6427965862944458 | validation: 0.6830251966382037]
	TIME [epoch: 3.51 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.624945125432501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.624945125432501 | validation: 0.7147649250152226]
	TIME [epoch: 3.51 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6314613404233769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6314613404233769 | validation: 0.6752814850879747]
	TIME [epoch: 3.51 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6221521294162102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6221521294162102 | validation: 0.7066883013152788]
	TIME [epoch: 3.53 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6303527057397607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6303527057397607 | validation: 0.7202113553621244]
	TIME [epoch: 3.52 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6557510977747976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6557510977747976 | validation: 0.7775387394132061]
	TIME [epoch: 3.52 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7190689741400127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7190689741400127 | validation: 0.721876055927305]
	TIME [epoch: 3.51 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6499735755606538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6499735755606538 | validation: 0.6631397002646418]
	TIME [epoch: 3.5 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6268750338991216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6268750338991216 | validation: 0.7655973317543596]
	TIME [epoch: 3.51 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6574629377214504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6574629377214504 | validation: 0.7123987690832133]
	TIME [epoch: 3.5 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869354610657274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6869354610657274 | validation: 0.7182085944188845]
	TIME [epoch: 3.5 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6192453430918506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6192453430918506 | validation: 0.680564594036092]
	TIME [epoch: 3.51 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6147087203424658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6147087203424658 | validation: 0.6937198268660199]
	TIME [epoch: 3.51 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6358145996474882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6358145996474882 | validation: 0.7680996156082767]
	TIME [epoch: 3.51 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6466746846176799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6466746846176799 | validation: 0.6745581782157045]
	TIME [epoch: 3.52 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6584339790672955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6584339790672955 | validation: 0.7632569491792333]
	TIME [epoch: 3.52 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.665353675077794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.665353675077794 | validation: 0.6768550947131495]
	TIME [epoch: 3.51 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6473992920964773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6473992920964773 | validation: 0.7250030369822957]
	TIME [epoch: 3.51 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6619969580065659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6619969580065659 | validation: 0.7559560935161478]
	TIME [epoch: 3.5 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.665779176065505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.665779176065505 | validation: 0.6899200761553088]
	TIME [epoch: 3.51 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6263118501524718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6263118501524718 | validation: 0.6818450493937518]
	TIME [epoch: 3.5 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6495066374038468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6495066374038468 | validation: 0.7257982700571762]
	TIME [epoch: 3.5 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.612200699931939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.612200699931939 | validation: 0.6657404877401236]
	TIME [epoch: 3.49 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6105680256980204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6105680256980204 | validation: 0.6740554550254405]
	TIME [epoch: 3.5 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6048700498550315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6048700498550315 | validation: 0.66293499507751]
	TIME [epoch: 3.5 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6155510751430808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6155510751430808 | validation: 0.7081888827260633]
	TIME [epoch: 3.51 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6373262806366319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6373262806366319 | validation: 0.7310900740925222]
	TIME [epoch: 3.51 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6928846825314551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6928846825314551 | validation: 0.779309443064162]
	TIME [epoch: 3.5 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6847413450819184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6847413450819184 | validation: 0.6715756774799158]
	TIME [epoch: 3.5 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6426674219560516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6426674219560516 | validation: 0.7496000400012273]
	TIME [epoch: 3.5 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6373966335882881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6373966335882881 | validation: 0.6975826593784623]
	TIME [epoch: 3.5 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6686819625706844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6686819625706844 | validation: 0.7462429448006989]
	TIME [epoch: 3.5 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6280019428144028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6280019428144028 | validation: 0.6446344716430013]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6042692844869094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6042692844869094 | validation: 0.6719913180987098]
	TIME [epoch: 3.51 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6013768233412189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6013768233412189 | validation: 0.665913658859356]
	TIME [epoch: 3.51 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6008494025244953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6008494025244953 | validation: 0.6773289614326548]
	TIME [epoch: 3.52 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6046875616743734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6046875616743734 | validation: 0.6508957445984027]
	TIME [epoch: 3.52 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6083627429669044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6083627429669044 | validation: 0.7162916559155375]
	TIME [epoch: 3.51 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6356033972469264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6356033972469264 | validation: 0.6765766475251458]
	TIME [epoch: 3.51 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579977283491388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6579977283491388 | validation: 0.731385725514781]
	TIME [epoch: 3.5 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6515089449799146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6515089449799146 | validation: 0.7684587253959829]
	TIME [epoch: 3.51 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6454974538485508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6454974538485508 | validation: 0.7173820227607626]
	TIME [epoch: 3.51 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7270646935975311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7270646935975311 | validation: 0.7198323017672967]
	TIME [epoch: 33.6 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.617510142898843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.617510142898843 | validation: 0.6930626830337325]
	TIME [epoch: 7.62 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6048142037884678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6048142037884678 | validation: 0.6440707500475918]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_503.pth
	Model improved!!!
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6321065284634346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6321065284634346 | validation: 0.7073122099410788]
	TIME [epoch: 7.64 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6128133684405743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6128133684405743 | validation: 0.6638866765585478]
	TIME [epoch: 7.65 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6073679221449837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6073679221449837 | validation: 0.6680000533165249]
	TIME [epoch: 7.61 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6072460115040573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6072460115040573 | validation: 0.685254249093235]
	TIME [epoch: 7.64 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6236917566448338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6236917566448338 | validation: 0.700285586487109]
	TIME [epoch: 7.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6900702291517399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6900702291517399 | validation: 0.7760105602852182]
	TIME [epoch: 7.61 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7014745596710449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7014745596710449 | validation: 0.7242279077082303]
	TIME [epoch: 7.62 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6605508352025234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6605508352025234 | validation: 0.6865834835310509]
	TIME [epoch: 7.61 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6261934076330163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6261934076330163 | validation: 0.7417825152814125]
	TIME [epoch: 7.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6150229806692185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6150229806692185 | validation: 0.6782242903476737]
	TIME [epoch: 7.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6022308308305858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6022308308305858 | validation: 0.6751969602304689]
	TIME [epoch: 7.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5930149908229615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5930149908229615 | validation: 0.6548956487902539]
	TIME [epoch: 7.62 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.591681348781691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.591681348781691 | validation: 0.6597846275375536]
	TIME [epoch: 7.62 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5940904183318536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5940904183318536 | validation: 0.6875433063279992]
	TIME [epoch: 7.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5960479159119929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5960479159119929 | validation: 0.6600542659615249]
	TIME [epoch: 7.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264500508586387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6264500508586387 | validation: 0.8816179883469233]
	TIME [epoch: 7.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7154571761296521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7154571761296521 | validation: 0.6891001042248601]
	TIME [epoch: 7.61 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6705269334128673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6705269334128673 | validation: 0.6906806952317962]
	TIME [epoch: 7.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5973911347681374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5973911347681374 | validation: 0.7050337927873953]
	TIME [epoch: 7.59 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6034219337554302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6034219337554302 | validation: 0.6876187310788454]
	TIME [epoch: 7.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6093265916155407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6093265916155407 | validation: 0.733216371599943]
	TIME [epoch: 7.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6547675609717123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6547675609717123 | validation: 0.7369858125361288]
	TIME [epoch: 7.61 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579102034854089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6579102034854089 | validation: 0.642585248121344]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_526.pth
	Model improved!!!
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.60058005575733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.60058005575733 | validation: 0.66902175106852]
	TIME [epoch: 7.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5856503386965056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5856503386965056 | validation: 0.6486267571522628]
	TIME [epoch: 7.59 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6414329978971071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6414329978971071 | validation: 0.8030427462842797]
	TIME [epoch: 7.61 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6413218345374141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6413218345374141 | validation: 0.654409473516866]
	TIME [epoch: 7.61 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6038187844638288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6038187844638288 | validation: 0.7254517303311255]
	TIME [epoch: 7.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6061935931615342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6061935931615342 | validation: 0.6333628729740799]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_532.pth
	Model improved!!!
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5827594100979561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5827594100979561 | validation: 0.6838824907704172]
	TIME [epoch: 7.65 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5887911140169204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5887911140169204 | validation: 0.6336279386663818]
	TIME [epoch: 7.62 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5842666078298154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5842666078298154 | validation: 0.7140528017913073]
	TIME [epoch: 7.61 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6265387740807551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6265387740807551 | validation: 0.744258902617749]
	TIME [epoch: 7.59 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830055737507784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6830055737507784 | validation: 0.7647033281840482]
	TIME [epoch: 7.61 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6605033762141304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6605033762141304 | validation: 0.7183031606919061]
	TIME [epoch: 7.59 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6105851874173844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6105851874173844 | validation: 0.6507367657940945]
	TIME [epoch: 7.62 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5850515659595852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5850515659595852 | validation: 0.6595339586322218]
	TIME [epoch: 7.59 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5849807123610573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5849807123610573 | validation: 0.6414192707999264]
	TIME [epoch: 7.61 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.578038891316237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.578038891316237 | validation: 0.6468298998695703]
	TIME [epoch: 7.59 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6193098529230341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6193098529230341 | validation: 0.8591933066699724]
	TIME [epoch: 7.61 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6863345546324005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6863345546324005 | validation: 0.654699262598143]
	TIME [epoch: 7.61 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6292494748242163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6292494748242163 | validation: 0.6500065705232165]
	TIME [epoch: 7.61 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5669612889400828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5669612889400828 | validation: 0.7213741310290399]
	TIME [epoch: 7.59 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5928947556602323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5928947556602323 | validation: 0.6432642813681264]
	TIME [epoch: 7.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.598032423791081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.598032423791081 | validation: 0.6973140777223759]
	TIME [epoch: 7.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5857428541472238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5857428541472238 | validation: 0.6843169389418775]
	TIME [epoch: 7.62 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6037435367081662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6037435367081662 | validation: 0.7781004203546056]
	TIME [epoch: 7.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6809660350264355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6809660350264355 | validation: 0.7286435027158277]
	TIME [epoch: 7.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6927342319409366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6927342319409366 | validation: 0.6407056583822577]
	TIME [epoch: 7.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5689487516329294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5689487516329294 | validation: 0.6989377825731569]
	TIME [epoch: 7.61 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5724732182450001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5724732182450001 | validation: 0.6184389613928487]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5753594175523635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5753594175523635 | validation: 0.6616646178177845]
	TIME [epoch: 7.64 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5549307772945201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5549307772945201 | validation: 0.64381075469831]
	TIME [epoch: 7.59 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.554642556602826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.554642556602826 | validation: 0.6505579244286919]
	TIME [epoch: 7.59 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5537175270441641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5537175270441641 | validation: 0.7456791358350872]
	TIME [epoch: 7.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5690942412064677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5690942412064677 | validation: 0.6470242218512192]
	TIME [epoch: 7.61 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6401095993746586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6401095993746586 | validation: 0.8472707458510649]
	TIME [epoch: 7.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6602794949741144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6602794949741144 | validation: 0.6429343232710999]
	TIME [epoch: 7.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5407671533704597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5407671533704597 | validation: 0.6546692560291497]
	TIME [epoch: 7.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5337641333297259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5337641333297259 | validation: 0.7584875611345208]
	TIME [epoch: 7.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5911800165439918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5911800165439918 | validation: 0.7684380594461703]
	TIME [epoch: 7.62 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7103640967393255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7103640967393255 | validation: 0.6530995518017598]
	TIME [epoch: 7.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5229559103391266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5229559103391266 | validation: 0.9241219801605601]
	TIME [epoch: 7.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9654993260511193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9654993260511193 | validation: 0.6675581829417281]
	TIME [epoch: 7.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6092072668762846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6092072668762846 | validation: 0.5880740374780434]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5559788692074954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5559788692074954 | validation: 0.601044995240907]
	TIME [epoch: 7.64 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49748965391370464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49748965391370464 | validation: 0.6357415144275849]
	TIME [epoch: 7.62 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48699778410220795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48699778410220795 | validation: 0.5920666362173519]
	TIME [epoch: 7.61 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49590625660590987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49590625660590987 | validation: 0.5950253576574974]
	TIME [epoch: 7.62 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4627146421072263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4627146421072263 | validation: 0.5634459332494879]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_573.pth
	Model improved!!!
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4417356407194007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4417356407194007 | validation: 0.5429610884250519]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_574.pth
	Model improved!!!
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42449326245808344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42449326245808344 | validation: 1.9607463307562858]
	TIME [epoch: 7.61 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8223286998698371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8223286998698371 | validation: 0.6526344213216737]
	TIME [epoch: 7.62 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6636959657756047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6636959657756047 | validation: 0.5973134649659676]
	TIME [epoch: 7.62 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6913473837083447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6913473837083447 | validation: 0.5762171056474783]
	TIME [epoch: 7.64 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6609065611220328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6609065611220328 | validation: 0.538775555973122]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5539122042750588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5539122042750588 | validation: 0.5739205662281929]
	TIME [epoch: 7.63 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5363794261594999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5363794261594999 | validation: 0.5963252682020214]
	TIME [epoch: 7.63 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5202033553596509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5202033553596509 | validation: 0.5878913528155931]
	TIME [epoch: 7.62 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.501560400546146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.501560400546146 | validation: 0.5587733630566177]
	TIME [epoch: 7.64 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4870176203464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4870176203464 | validation: 0.5539867442330625]
	TIME [epoch: 7.62 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4565515154357048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4565515154357048 | validation: 0.5153958287287036]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41976045550386326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41976045550386326 | validation: 0.5326602397345276]
	TIME [epoch: 7.63 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3990264626174729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3990264626174729 | validation: 0.47198458671587074]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_587.pth
	Model improved!!!
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38410866627047113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38410866627047113 | validation: 0.48240006012487574]
	TIME [epoch: 7.64 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37142177988095887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37142177988095887 | validation: 0.42796842365542087]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3948112176012395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3948112176012395 | validation: 0.44196759537610464]
	TIME [epoch: 7.6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37858352042801413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37858352042801413 | validation: 0.4404509502364262]
	TIME [epoch: 7.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43262925562871546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43262925562871546 | validation: 0.4614425479980764]
	TIME [epoch: 7.6 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4176565031173085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4176565031173085 | validation: 0.41971215634735926]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3642627086499155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3642627086499155 | validation: 0.4165745457869891]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_594.pth
	Model improved!!!
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3961642741633744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3961642741633744 | validation: 0.38011659508628104]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3146682141842651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3146682141842651 | validation: 0.5076977529356105]
	TIME [epoch: 7.64 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45843137298210834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45843137298210834 | validation: 0.4407172017176321]
	TIME [epoch: 7.65 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43560065768564393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43560065768564393 | validation: 0.4991011671473583]
	TIME [epoch: 7.62 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4934738080346597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4934738080346597 | validation: 0.5189243834710162]
	TIME [epoch: 7.62 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49255004053602564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49255004053602564 | validation: 0.40498350450447096]
	TIME [epoch: 7.62 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3650864247065215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3650864247065215 | validation: 0.6574005225510491]
	TIME [epoch: 7.62 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6751115292273767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6751115292273767 | validation: 0.447599931320374]
	TIME [epoch: 7.64 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4136865104277668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4136865104277668 | validation: 0.41065630335270387]
	TIME [epoch: 7.63 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39817165256905657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39817165256905657 | validation: 0.40846204388660007]
	TIME [epoch: 7.62 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3063655455545649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3063655455545649 | validation: 0.340750245304134]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_605.pth
	Model improved!!!
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2806990483689042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2806990483689042 | validation: 0.33051461203818877]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27625087005668614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27625087005668614 | validation: 0.6670434925011093]
	TIME [epoch: 7.64 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.720432886703249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.720432886703249 | validation: 0.600514907075572]
	TIME [epoch: 7.65 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48074485358905444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48074485358905444 | validation: 0.45233098147527884]
	TIME [epoch: 7.62 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4915892307893375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4915892307893375 | validation: 0.3363539049254203]
	TIME [epoch: 7.62 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2682195736167139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2682195736167139 | validation: 0.4045735963778636]
	TIME [epoch: 7.62 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3449211511513293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3449211511513293 | validation: 0.36121236631435405]
	TIME [epoch: 7.64 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2758651584897185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2758651584897185 | validation: 0.35828817973066857]
	TIME [epoch: 7.62 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2880350008768901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2880350008768901 | validation: 0.3973841918974411]
	TIME [epoch: 7.62 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36130858809817723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36130858809817723 | validation: 0.3247785467811699]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2875013060596782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2875013060596782 | validation: 0.45346497560358706]
	TIME [epoch: 7.63 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4613869271984363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4613869271984363 | validation: 0.3979050260542759]
	TIME [epoch: 7.66 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3541813566243839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3541813566243839 | validation: 0.3644431671417822]
	TIME [epoch: 7.63 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3610552187381353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3610552187381353 | validation: 0.361405115729057]
	TIME [epoch: 7.62 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2946493535871694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2946493535871694 | validation: 0.2816537706101782]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2515657650787461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2515657650787461 | validation: 0.29002434805312366]
	TIME [epoch: 7.63 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21978142521423444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21978142521423444 | validation: 0.27638372926441507]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23292044744875107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23292044744875107 | validation: 0.32486442520297826]
	TIME [epoch: 7.63 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25909906185510984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25909906185510984 | validation: 0.41809238643786845]
	TIME [epoch: 7.63 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4643474324513521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4643474324513521 | validation: 0.43253105755108]
	TIME [epoch: 7.62 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3786554780428565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3786554780428565 | validation: 0.26318026026174957]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24516649379269662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24516649379269662 | validation: 0.29472461334119493]
	TIME [epoch: 7.63 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25416421624417834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25416421624417834 | validation: 0.35424575708981876]
	TIME [epoch: 7.62 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32754371921491127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32754371921491127 | validation: 0.26809984098678635]
	TIME [epoch: 7.63 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2113284508222488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2113284508222488 | validation: 0.27418683928968346]
	TIME [epoch: 7.62 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2596842678018221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2596842678018221 | validation: 0.37624061796398345]
	TIME [epoch: 7.64 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3483274353423824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3483274353423824 | validation: 0.28215738489529124]
	TIME [epoch: 7.62 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24418147417659045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24418147417659045 | validation: 0.3690997636105693]
	TIME [epoch: 7.64 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35558139021462765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35558139021462765 | validation: 0.34451320482809583]
	TIME [epoch: 7.62 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2943059326464549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2943059326464549 | validation: 0.25971125859307403]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22010008076358267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22010008076358267 | validation: 0.32973117629380266]
	TIME [epoch: 7.64 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32147999424058277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32147999424058277 | validation: 0.3194078601882664]
	TIME [epoch: 7.62 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30344205478427044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30344205478427044 | validation: 0.22474700658353142]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20909063955397855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20909063955397855 | validation: 0.3591861337665734]
	TIME [epoch: 7.61 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3640994002260555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3640994002260555 | validation: 0.4581686501342861]
	TIME [epoch: 7.61 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38450783299000507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38450783299000507 | validation: 0.3880786809533634]
	TIME [epoch: 7.63 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4402048670174897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4402048670174897 | validation: 0.3230158426842386]
	TIME [epoch: 7.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2714086326459885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2714086326459885 | validation: 0.2529340027736554]
	TIME [epoch: 7.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19931083515521142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19931083515521142 | validation: 0.215654941282398]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_644.pth
	Model improved!!!
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19806519461188896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19806519461188896 | validation: 0.2726738339161385]
	TIME [epoch: 7.61 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23671169342458115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23671169342458115 | validation: 0.2291227662438602]
	TIME [epoch: 7.62 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24383691709007177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24383691709007177 | validation: 0.26280180811314974]
	TIME [epoch: 7.61 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2049188695235964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2049188695235964 | validation: 0.2916955841840057]
	TIME [epoch: 7.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3132912455763996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3132912455763996 | validation: 0.44096310997054733]
	TIME [epoch: 7.61 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44184851883818954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44184851883818954 | validation: 0.27374744002541707]
	TIME [epoch: 7.63 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2620909777874526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2620909777874526 | validation: 0.32761795887137535]
	TIME [epoch: 7.64 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3536258771397037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3536258771397037 | validation: 0.31082748702659607]
	TIME [epoch: 7.62 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2490796855423833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2490796855423833 | validation: 0.2274175435948211]
	TIME [epoch: 7.62 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2221335713781012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2221335713781012 | validation: 0.31338409215185636]
	TIME [epoch: 7.62 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29292177994717705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29292177994717705 | validation: 0.23573291887287173]
	TIME [epoch: 7.64 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2165819996391117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2165819996391117 | validation: 0.20409588056588224]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_656.pth
	Model improved!!!
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18771794689930016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18771794689930016 | validation: 0.252379223467166]
	TIME [epoch: 7.61 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20257889053297087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20257889053297087 | validation: 0.201793889187469]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_658.pth
	Model improved!!!
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2115825067332081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2115825067332081 | validation: 0.24610028707527187]
	TIME [epoch: 7.61 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20663382006914982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20663382006914982 | validation: 0.1837854685228611]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_660.pth
	Model improved!!!
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1892533179751927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1892533179751927 | validation: 0.2603462032054454]
	TIME [epoch: 7.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20889546850361762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20889546850361762 | validation: 0.2245466587708357]
	TIME [epoch: 7.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23510016006127316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23510016006127316 | validation: 0.32902467968413046]
	TIME [epoch: 7.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27838471727525244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27838471727525244 | validation: 0.32831429659739997]
	TIME [epoch: 7.6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37379361749148954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37379361749148954 | validation: 0.3522170733899328]
	TIME [epoch: 7.62 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3235340625225451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3235340625225451 | validation: 0.3375316007407138]
	TIME [epoch: 7.61 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2756538609074381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2756538609074381 | validation: 0.2496130269063398]
	TIME [epoch: 7.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24335727619825062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24335727619825062 | validation: 0.2079855329555146]
	TIME [epoch: 7.59 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17583540504713172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17583540504713172 | validation: 0.18038105389405415]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1476619361132821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1476619361132821 | validation: 0.16745923417316774]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_670.pth
	Model improved!!!
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14140634242027414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14140634242027414 | validation: 0.1811760354502155]
	TIME [epoch: 7.6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16836592912291848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16836592912291848 | validation: 0.2794722924540096]
	TIME [epoch: 7.59 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2634780043073106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2634780043073106 | validation: 0.2044566612435301]
	TIME [epoch: 7.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19348007299772374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19348007299772374 | validation: 0.424436981521158]
	TIME [epoch: 7.6 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36924796543783683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36924796543783683 | validation: 0.288935770097969]
	TIME [epoch: 7.62 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32639182823749396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32639182823749396 | validation: 0.18738632719458265]
	TIME [epoch: 7.59 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14574106014465038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14574106014465038 | validation: 0.18276299076362268]
	TIME [epoch: 7.6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14681392543214725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14681392543214725 | validation: 0.2260660181571606]
	TIME [epoch: 7.59 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22385171637786314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22385171637786314 | validation: 0.3668070322261376]
	TIME [epoch: 7.62 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3481152319629914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3481152319629914 | validation: 0.2889790579487334]
	TIME [epoch: 7.6 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23722021769962218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23722021769962218 | validation: 0.29663131337292675]
	TIME [epoch: 7.6 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32248802072449295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32248802072449295 | validation: 0.27264879616592924]
	TIME [epoch: 7.59 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24981911090390108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24981911090390108 | validation: 0.22453217670699585]
	TIME [epoch: 7.59 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23431018180410704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23431018180410704 | validation: 0.2450151397975831]
	TIME [epoch: 7.61 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20118779371177817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20118779371177817 | validation: 0.1743121444798146]
	TIME [epoch: 7.59 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18552446397928996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18552446397928996 | validation: 0.17548451284978558]
	TIME [epoch: 7.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14042685742277136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14042685742277136 | validation: 0.16436443940316853]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_687.pth
	Model improved!!!
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14474315817351285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14474315817351285 | validation: 0.18383539775321928]
	TIME [epoch: 7.6 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16683229876557107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16683229876557107 | validation: 0.21828288043953076]
	TIME [epoch: 7.62 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21780057372937392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21780057372937392 | validation: 0.1669159941511033]
	TIME [epoch: 7.6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1627855225802756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1627855225802756 | validation: 0.3269096407180271]
	TIME [epoch: 7.6 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2747057518079782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2747057518079782 | validation: 0.3209121437074958]
	TIME [epoch: 7.61 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3528525257184388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3528525257184388 | validation: 0.23696917216262703]
	TIME [epoch: 7.6 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17699347239341215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17699347239341215 | validation: 0.17223692407559224]
	TIME [epoch: 7.61 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15370213135828278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15370213135828278 | validation: 0.26832849859430125]
	TIME [epoch: 7.6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2878795666538063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2878795666538063 | validation: 0.19649266708115043]
	TIME [epoch: 7.6 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14749270631080955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14749270631080955 | validation: 0.31253177970149126]
	TIME [epoch: 7.59 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.382653129166544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.382653129166544 | validation: 0.2887371228476615]
	TIME [epoch: 7.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28304637481353573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28304637481353573 | validation: 0.18702501083694323]
	TIME [epoch: 7.62 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19037031890314587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19037031890314587 | validation: 0.24783765507642214]
	TIME [epoch: 7.6 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24491371637480633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24491371637480633 | validation: 0.1438935358512782]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_701.pth
	Model improved!!!
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15164094010102416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15164094010102416 | validation: 0.142958341009057]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_702.pth
	Model improved!!!
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12367262696589607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12367262696589607 | validation: 0.17051716408459777]
	TIME [epoch: 7.63 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14592760631140173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14592760631140173 | validation: 0.2376130967530739]
	TIME [epoch: 7.63 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27882243077819574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27882243077819574 | validation: 0.41600552762989906]
	TIME [epoch: 7.62 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38432055692768285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38432055692768285 | validation: 0.19186965192963468]
	TIME [epoch: 7.62 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19001084467199458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19001084467199458 | validation: 0.2110132501237957]
	TIME [epoch: 7.63 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1942302883128243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1942302883128243 | validation: 0.2242980494714802]
	TIME [epoch: 7.63 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24648584347754196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24648584347754196 | validation: 0.14327322476847268]
	TIME [epoch: 7.63 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13239848384161465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13239848384161465 | validation: 0.22256884116931755]
	TIME [epoch: 7.62 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2403222726390365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2403222726390365 | validation: 0.2290368907025478]
	TIME [epoch: 7.61 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2106827642957429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2106827642957429 | validation: 0.1510917463178333]
	TIME [epoch: 7.62 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14494605334076532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14494605334076532 | validation: 0.25018407154741346]
	TIME [epoch: 7.62 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23089251798394445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23089251798394445 | validation: 0.16408330602526908]
	TIME [epoch: 7.61 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1776997606590833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1776997606590833 | validation: 0.2894464896599087]
	TIME [epoch: 7.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22319600831988917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22319600831988917 | validation: 0.2484717212978083]
	TIME [epoch: 7.6 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27693638513496477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27693638513496477 | validation: 0.3466118021377591]
	TIME [epoch: 7.59 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31587572907356193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31587572907356193 | validation: 0.19093690433887522]
	TIME [epoch: 7.62 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14046658468004317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14046658468004317 | validation: 0.15946362570234693]
	TIME [epoch: 7.61 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1499538106653714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1499538106653714 | validation: 0.23235496466357863]
	TIME [epoch: 7.6 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2571896522033048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2571896522033048 | validation: 0.1674626121210111]
	TIME [epoch: 7.6 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14673644941490316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14673644941490316 | validation: 0.12802334173692867]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_722.pth
	Model improved!!!
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10567120955407909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10567120955407909 | validation: 0.12641461802178983]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1072386559256212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1072386559256212 | validation: 0.114802354877693]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_724.pth
	Model improved!!!
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09981518997940722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09981518997940722 | validation: 0.14297699015179818]
	TIME [epoch: 7.61 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12235776636288848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12235776636288848 | validation: 0.20360421936784495]
	TIME [epoch: 7.6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2530340168470951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2530340168470951 | validation: 0.4015579728952247]
	TIME [epoch: 7.6 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37889499929709625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37889499929709625 | validation: 0.3805835087109604]
	TIME [epoch: 7.64 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39034713033620977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39034713033620977 | validation: 0.24221443496058256]
	TIME [epoch: 7.61 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2135375289704961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2135375289704961 | validation: 0.14469106545974567]
	TIME [epoch: 7.6 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14879510596197798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14879510596197798 | validation: 0.16052261031912343]
	TIME [epoch: 7.6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11140976351348787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11140976351348787 | validation: 0.14220936036421095]
	TIME [epoch: 7.61 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13480293525660259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13480293525660259 | validation: 0.19402662065504012]
	TIME [epoch: 7.62 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16945901274036757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16945901274036757 | validation: 0.20364091609380652]
	TIME [epoch: 7.6 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23390764156322189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23390764156322189 | validation: 0.19883659883709648]
	TIME [epoch: 7.61 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16116910368175424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16116910368175424 | validation: 0.14493284453713548]
	TIME [epoch: 7.61 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13618752650101582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13618752650101582 | validation: 0.29595789540301193]
	TIME [epoch: 7.61 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2457104403752414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2457104403752414 | validation: 0.2047127220813256]
	TIME [epoch: 7.63 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22179586722249398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22179586722249398 | validation: 0.2974451501937914]
	TIME [epoch: 7.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25329865943409474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25329865943409474 | validation: 0.14273122273817776]
	TIME [epoch: 7.61 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11747124423314852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11747124423314852 | validation: 0.12419855826579505]
	TIME [epoch: 7.6 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10289712565111568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10289712565111568 | validation: 0.14436120018065923]
	TIME [epoch: 7.61 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1029523895587989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1029523895587989 | validation: 0.13160808269875268]
	TIME [epoch: 7.62 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11670444285112677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11670444285112677 | validation: 0.1734986251212333]
	TIME [epoch: 7.61 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15412241595086423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15412241595086423 | validation: 0.1904990039999065]
	TIME [epoch: 7.62 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20510334612369222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20510334612369222 | validation: 0.24732136347042233]
	TIME [epoch: 7.62 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1901669846829644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1901669846829644 | validation: 0.27263641605207395]
	TIME [epoch: 7.62 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3142021966850627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3142021966850627 | validation: 0.38524655394560153]
	TIME [epoch: 7.63 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3955201943821722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3955201943821722 | validation: 0.19156906557220255]
	TIME [epoch: 7.6 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15261503768671392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15261503768671392 | validation: 0.31249151873785214]
	TIME [epoch: 7.61 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3854194474110962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3854194474110962 | validation: 0.22107766047471575]
	TIME [epoch: 7.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18971097688114724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18971097688114724 | validation: 0.16504703144756136]
	TIME [epoch: 7.61 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12778938568506523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12778938568506523 | validation: 0.1601784736738541]
	TIME [epoch: 7.62 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13822004073710606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13822004073710606 | validation: 0.1420755213449274]
	TIME [epoch: 7.6 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1218514993216938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1218514993216938 | validation: 0.13143490684469375]
	TIME [epoch: 7.61 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10172771847352202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10172771847352202 | validation: 0.1094554737132417]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_756.pth
	Model improved!!!
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09469238789064789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09469238789064789 | validation: 0.14177622995948777]
	TIME [epoch: 7.66 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10613429214172111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10613429214172111 | validation: 0.21704615593170049]
	TIME [epoch: 7.61 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2138669888936418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2138669888936418 | validation: 0.3992999478764019]
	TIME [epoch: 7.61 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36396056539959304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36396056539959304 | validation: 0.1676112254766683]
	TIME [epoch: 7.61 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14643530489755996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14643530489755996 | validation: 0.12654695751858303]
	TIME [epoch: 7.61 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09796352888352555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09796352888352555 | validation: 0.18442502151112247]
	TIME [epoch: 7.63 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16802451684503494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16802451684503494 | validation: 0.21826925113976747]
	TIME [epoch: 7.62 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25387165999614086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25387165999614086 | validation: 0.1530888511837754]
	TIME [epoch: 7.61 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11438713372095037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11438713372095037 | validation: 0.13541545128725452]
	TIME [epoch: 7.61 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15375256633145723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15375256633145723 | validation: 0.2640556450296671]
	TIME [epoch: 7.62 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28722833862212344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28722833862212344 | validation: 0.18141942040045703]
	TIME [epoch: 7.63 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2009693445751743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2009693445751743 | validation: 0.2703438852934219]
	TIME [epoch: 7.61 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26238546263184837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26238546263184837 | validation: 0.12374151884324743]
	TIME [epoch: 7.61 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10547135136144645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10547135136144645 | validation: 0.10973326174052234]
	TIME [epoch: 7.61 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09315499037044092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09315499037044092 | validation: 0.14534390254301308]
	TIME [epoch: 7.62 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11719690116939926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11719690116939926 | validation: 0.15105680789837678]
	TIME [epoch: 7.63 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1444383184771789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1444383184771789 | validation: 0.25944371615110934]
	TIME [epoch: 7.62 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19362286729753636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19362286729753636 | validation: 0.18081930873902838]
	TIME [epoch: 7.61 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19528447737291152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19528447737291152 | validation: 0.3234367239404597]
	TIME [epoch: 7.61 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3239501664597497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3239501664597497 | validation: 0.18488741773377496]
	TIME [epoch: 7.61 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1391031409842175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1391031409842175 | validation: 0.2715451992351556]
	TIME [epoch: 7.63 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2996424799715248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2996424799715248 | validation: 0.25300962387228004]
	TIME [epoch: 7.61 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25273553232994717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25273553232994717 | validation: 0.15779474399730536]
	TIME [epoch: 7.61 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12447408486960931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12447408486960931 | validation: 0.15019824787590103]
	TIME [epoch: 7.61 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11819538345262284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11819538345262284 | validation: 0.15650674954634913]
	TIME [epoch: 7.61 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13185789377000692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13185789377000692 | validation: 0.13437911194266047]
	TIME [epoch: 7.63 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1090767068674208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1090767068674208 | validation: 0.12453775542178327]
	TIME [epoch: 7.61 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08393848589377143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08393848589377143 | validation: 0.11516349237718795]
	TIME [epoch: 7.61 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10131135906240457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10131135906240457 | validation: 0.2999857376392863]
	TIME [epoch: 7.61 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24336831290270436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24336831290270436 | validation: 0.20720158259632107]
	TIME [epoch: 7.61 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2129828735958196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2129828735958196 | validation: 0.14400511883838862]
	TIME [epoch: 7.63 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09827009182431465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09827009182431465 | validation: 0.12095781716784315]
	TIME [epoch: 7.61 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09662914300044487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09662914300044487 | validation: 0.13609823961268513]
	TIME [epoch: 7.61 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1051893899590921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1051893899590921 | validation: 0.1788963952256099]
	TIME [epoch: 7.61 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1831480257397147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1831480257397147 | validation: 0.23254006562533772]
	TIME [epoch: 7.61 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2385666079230828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2385666079230828 | validation: 0.17475096588059813]
	TIME [epoch: 7.62 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12769858131120276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12769858131120276 | validation: 0.12265896768628425]
	TIME [epoch: 7.61 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12049101358177862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12049101358177862 | validation: 0.3555776245899101]
	TIME [epoch: 7.6 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28434196318747157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28434196318747157 | validation: 0.2214142442375751]
	TIME [epoch: 7.61 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2266701278575266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2266701278575266 | validation: 0.1925330995195647]
	TIME [epoch: 7.61 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1424246009397112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1424246009397112 | validation: 0.15607808209241975]
	TIME [epoch: 7.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1351585161612707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1351585161612707 | validation: 0.1858436467057354]
	TIME [epoch: 7.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1539322588205628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1539322588205628 | validation: 0.1378559323463655]
	TIME [epoch: 7.61 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10153316560999014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10153316560999014 | validation: 0.1038254919518845]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_800.pth
	Model improved!!!
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08242124957323103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08242124957323103 | validation: 0.11466792332525721]
	TIME [epoch: 7.61 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08616704581585498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08616704581585498 | validation: 0.11097762146312098]
	TIME [epoch: 7.61 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10929487483475078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10929487483475078 | validation: 0.19335011814103736]
	TIME [epoch: 7.6 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1676718075338277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1676718075338277 | validation: 0.17292397166477283]
	TIME [epoch: 7.6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18618236945239566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18618236945239566 | validation: 0.27640448514236643]
	TIME [epoch: 7.61 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22026735004355558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22026735004355558 | validation: 0.17655220550252101]
	TIME [epoch: 7.62 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11359472549527493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11359472549527493 | validation: 0.14891367580840384]
	TIME [epoch: 7.6 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13940683291470768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13940683291470768 | validation: 0.21343564144168614]
	TIME [epoch: 7.6 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2112402566038576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2112402566038576 | validation: 0.13084826690570325]
	TIME [epoch: 7.6 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11140305516745738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11140305516745738 | validation: 0.1090113811857275]
	TIME [epoch: 7.6 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08054158597138106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08054158597138106 | validation: 0.10980482586202023]
	TIME [epoch: 7.62 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09634664719529427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09634664719529427 | validation: 0.12729981169630672]
	TIME [epoch: 7.6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12052435408411068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12052435408411068 | validation: 0.11357522321032153]
	TIME [epoch: 7.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10549400929782803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10549400929782803 | validation: 0.1411933527372125]
	TIME [epoch: 7.6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10002757984702196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10002757984702196 | validation: 0.17125266300351744]
	TIME [epoch: 7.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.140796182481769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.140796182481769 | validation: 0.275605974582888]
	TIME [epoch: 7.61 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22885667460953202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22885667460953202 | validation: 0.23146367353324326]
	TIME [epoch: 7.61 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24349449113287022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24349449113287022 | validation: 0.17548813863487037]
	TIME [epoch: 7.6 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13553408803035696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13553408803035696 | validation: 0.16064711155822936]
	TIME [epoch: 7.6 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12992858688685213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12992858688685213 | validation: 0.1725934032803763]
	TIME [epoch: 7.61 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14095262059589875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14095262059589875 | validation: 0.15254232180403116]
	TIME [epoch: 7.62 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11808472269396804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11808472269396804 | validation: 0.11094474233380668]
	TIME [epoch: 7.61 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08604257999271156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08604257999271156 | validation: 0.10985610163803457]
	TIME [epoch: 7.6 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07976621934175722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07976621934175722 | validation: 0.1257450695685691]
	TIME [epoch: 7.6 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0772448214849367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0772448214849367 | validation: 0.1091155895950045]
	TIME [epoch: 7.63 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10192422651469238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10192422651469238 | validation: 0.2333299773501203]
	TIME [epoch: 7.62 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1891526653613827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1891526653613827 | validation: 0.18140859928302058]
	TIME [epoch: 7.6 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17172393040576328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17172393040576328 | validation: 0.1746768766179327]
	TIME [epoch: 7.6 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11903731237714645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11903731237714645 | validation: 0.13482102167484245]
	TIME [epoch: 7.6 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10740706433797331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10740706433797331 | validation: 0.14789507432193869]
	TIME [epoch: 7.6 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13752948812992716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13752948812992716 | validation: 0.1781889114265282]
	TIME [epoch: 7.61 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14276538565378727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14276538565378727 | validation: 0.10516011938889536]
	TIME [epoch: 7.6 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07939995529973874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07939995529973874 | validation: 0.12383242584597468]
	TIME [epoch: 7.6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08849081306908942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08849081306908942 | validation: 0.11363022386332569]
	TIME [epoch: 7.59 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1090415262147302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1090415262147302 | validation: 0.27426631440249577]
	TIME [epoch: 7.61 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22030898975204363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22030898975204363 | validation: 0.1435291807198048]
	TIME [epoch: 7.61 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1484675582718666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1484675582718666 | validation: 0.1306662670138755]
	TIME [epoch: 7.62 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08912900626032137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08912900626032137 | validation: 0.11709239584700853]
	TIME [epoch: 7.6 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07175443871434857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07175443871434857 | validation: 0.1259921995021815]
	TIME [epoch: 7.61 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09752026238167438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09752026238167438 | validation: 0.14348844109191983]
	TIME [epoch: 7.6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12724182886371943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12724182886371943 | validation: 0.12404216833499913]
	TIME [epoch: 7.63 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10908821057699923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10908821057699923 | validation: 0.11280328462065273]
	TIME [epoch: 7.6 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07269126764686823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07269126764686823 | validation: 0.12314722727613035]
	TIME [epoch: 7.62 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11817655078273502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11817655078273502 | validation: 0.24863697612713243]
	TIME [epoch: 7.6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20802569024777845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20802569024777845 | validation: 0.1847375218876496]
	TIME [epoch: 7.61 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17825120109508713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17825120109508713 | validation: 0.11577130626384598]
	TIME [epoch: 7.61 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08359243598767091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08359243598767091 | validation: 0.10187435912687191]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_847.pth
	Model improved!!!
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09067890638029123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09067890638029123 | validation: 0.11109424931646146]
	TIME [epoch: 7.59 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1171065581490439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1171065581490439 | validation: 0.19502306195126595]
	TIME [epoch: 7.61 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2019498001390594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2019498001390594 | validation: 0.1308288076657882]
	TIME [epoch: 7.6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1398070070507454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1398070070507454 | validation: 0.10696850504673772]
	TIME [epoch: 7.62 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09921365911896192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09921365911896192 | validation: 0.14780273960687557]
	TIME [epoch: 7.6 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1239482571745685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1239482571745685 | validation: 0.11068710975234722]
	TIME [epoch: 7.61 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1059470681814891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1059470681814891 | validation: 0.18198259586238286]
	TIME [epoch: 7.6 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14605786385628403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14605786385628403 | validation: 0.15413584172219238]
	TIME [epoch: 7.62 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13074612545830416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13074612545830416 | validation: 0.12314582122370027]
	TIME [epoch: 7.61 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08711887622786293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08711887622786293 | validation: 0.07613159902774058]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0599999143044198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0599999143044198 | validation: 0.07612725918491522]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_858.pth
	Model improved!!!
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05526683518668432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05526683518668432 | validation: 0.07532253388969579]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_859.pth
	Model improved!!!
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05206449920806978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05206449920806978 | validation: 0.07314197981320357]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_860.pth
	Model improved!!!
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061764157926968974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061764157926968974 | validation: 0.14938998812193974]
	TIME [epoch: 7.61 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11807589649398838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11807589649398838 | validation: 0.17241149567579456]
	TIME [epoch: 7.61 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15556135158142317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15556135158142317 | validation: 0.1545801487127113]
	TIME [epoch: 7.6 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11615135729554496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11615135729554496 | validation: 0.13543038593559284]
	TIME [epoch: 7.61 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10187824248185436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10187824248185436 | validation: 0.13052004573590456]
	TIME [epoch: 7.62 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12405205836929456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12405205836929456 | validation: 0.27638047007779915]
	TIME [epoch: 7.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21976865110103042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21976865110103042 | validation: 0.15881267825662518]
	TIME [epoch: 7.6 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15367450199341473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15367450199341473 | validation: 0.1560433845469971]
	TIME [epoch: 7.6 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12226948554818941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12226948554818941 | validation: 0.11968247873232872]
	TIME [epoch: 7.61 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08021218991285575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08021218991285575 | validation: 0.10570253465550294]
	TIME [epoch: 7.62 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0844240180488623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0844240180488623 | validation: 0.11037930629492104]
	TIME [epoch: 7.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08003310040335436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08003310040335436 | validation: 0.07973905178582624]
	TIME [epoch: 7.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08144613278115545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08144613278115545 | validation: 0.12660257713134646]
	TIME [epoch: 7.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09653160099667062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09653160099667062 | validation: 0.08959178120099671]
	TIME [epoch: 7.61 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08566940306404884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08566940306404884 | validation: 0.14202794377961886]
	TIME [epoch: 7.62 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10669976461936088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10669976461936088 | validation: 0.1069928461379217]
	TIME [epoch: 7.61 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11474458058830314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11474458058830314 | validation: 0.20746206715111626]
	TIME [epoch: 7.6 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19502681720076306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19502681720076306 | validation: 0.20159010349978]
	TIME [epoch: 7.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1285073678874995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1285073678874995 | validation: 0.1806566916602444]
	TIME [epoch: 7.61 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15559195484488403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15559195484488403 | validation: 0.2460595323644133]
	TIME [epoch: 7.62 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20127334457837365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20127334457837365 | validation: 0.11006557069091]
	TIME [epoch: 7.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11941933153491906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11941933153491906 | validation: 0.08565773796377334]
	TIME [epoch: 7.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06073785795532892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06073785795532892 | validation: 0.11553205757790663]
	TIME [epoch: 7.61 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0719990154076979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0719990154076979 | validation: 0.0942159052874127]
	TIME [epoch: 7.61 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07561033849982036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07561033849982036 | validation: 0.09453623226816674]
	TIME [epoch: 7.62 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06304888472119555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06304888472119555 | validation: 0.11526008635208143]
	TIME [epoch: 7.61 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09120345396304565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09120345396304565 | validation: 0.14448992995539944]
	TIME [epoch: 7.61 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.178454424009919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.178454424009919 | validation: 0.11991549628104359]
	TIME [epoch: 7.61 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09383753082369399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09383753082369399 | validation: 0.10672650491459766]
	TIME [epoch: 7.61 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08760420268121256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08760420268121256 | validation: 0.0945937964569187]
	TIME [epoch: 7.62 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10721009884373196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10721009884373196 | validation: 0.16977737007344806]
	TIME [epoch: 7.6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13422870944882115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13422870944882115 | validation: 0.20123264192718437]
	TIME [epoch: 7.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1894372163481716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1894372163481716 | validation: 0.2157301297193623]
	TIME [epoch: 7.6 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1699331036492046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1699331036492046 | validation: 0.12145149006534176]
	TIME [epoch: 7.63 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08755820217481769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08755820217481769 | validation: 0.13231438511565416]
	TIME [epoch: 7.6 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13697617948116736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13697617948116736 | validation: 0.17620269763202426]
	TIME [epoch: 7.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12906133181981072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12906133181981072 | validation: 0.15836377071897967]
	TIME [epoch: 7.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10784668769135172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10784668769135172 | validation: 0.10752433044410013]
	TIME [epoch: 7.6 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08230279632969023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08230279632969023 | validation: 0.1037689630491836]
	TIME [epoch: 7.62 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06888321270315728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06888321270315728 | validation: 0.08160319858987869]
	TIME [epoch: 7.61 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0684031463124245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0684031463124245 | validation: 0.08186527027297552]
	TIME [epoch: 7.6 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05854182548998732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05854182548998732 | validation: 0.06834010715637373]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_902.pth
	Model improved!!!
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05613114896177547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05613114896177547 | validation: 0.10492225664936645]
	TIME [epoch: 7.61 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06074949755365481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06074949755365481 | validation: 0.08265521055246866]
	TIME [epoch: 7.62 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07866955840655253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07866955840655253 | validation: 0.16681986178765704]
	TIME [epoch: 7.61 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12928368263814272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12928368263814272 | validation: 0.1699718442422239]
	TIME [epoch: 7.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19016081240832938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19016081240832938 | validation: 0.15531843170322848]
	TIME [epoch: 7.61 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10372180074738227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10372180074738227 | validation: 0.14452510140244298]
	TIME [epoch: 7.61 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11306744013711965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11306744013711965 | validation: 0.1641058096519045]
	TIME [epoch: 7.61 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312367155683273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1312367155683273 | validation: 0.10366106389549948]
	TIME [epoch: 7.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08124381841093553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08124381841093553 | validation: 0.107284901648279]
	TIME [epoch: 7.59 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07741957904871988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07741957904871988 | validation: 0.09156894061083816]
	TIME [epoch: 7.62 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08840110619205385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08840110619205385 | validation: 0.1891756589658048]
	TIME [epoch: 7.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16275811684599206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16275811684599206 | validation: 0.144529237528869]
	TIME [epoch: 7.62 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10038443421758718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10038443421758718 | validation: 0.11536255972294839]
	TIME [epoch: 7.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08317200587010054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08317200587010054 | validation: 0.12120494848773339]
	TIME [epoch: 7.6 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07064999999819699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07064999999819699 | validation: 0.08297549252266025]
	TIME [epoch: 7.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07942245122058461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07942245122058461 | validation: 0.15091609622451063]
	TIME [epoch: 7.61 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09973163772769424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09973163772769424 | validation: 0.10449953507603751]
	TIME [epoch: 7.62 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08756517981850669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08756517981850669 | validation: 0.12019030804122577]
	TIME [epoch: 7.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08739912233705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08739912233705 | validation: 0.14608251337508796]
	TIME [epoch: 7.59 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11268920678238317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11268920678238317 | validation: 0.1795322153092141]
	TIME [epoch: 7.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1849298063245648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1849298063245648 | validation: 0.2576991410342793]
	TIME [epoch: 7.61 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22027922014287146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22027922014287146 | validation: 0.12731067588051467]
	TIME [epoch: 7.61 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08893606291906324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08893606291906324 | validation: 0.1429764526607242]
	TIME [epoch: 7.61 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10694676027975226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10694676027975226 | validation: 0.1239723434261725]
	TIME [epoch: 7.61 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09164767947590725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09164767947590725 | validation: 0.08061889843169218]
	TIME [epoch: 7.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07030493327956906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07030493327956906 | validation: 0.12919414509339533]
	TIME [epoch: 7.62 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09704615634867185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09704615634867185 | validation: 0.07301914990526125]
	TIME [epoch: 7.62 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04890039660884799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04890039660884799 | validation: 0.07037945801610426]
	TIME [epoch: 7.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05587450357774745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05587450357774745 | validation: 0.11836096160970483]
	TIME [epoch: 7.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09005058251405944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09005058251405944 | validation: 0.09013484183653402]
	TIME [epoch: 7.61 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09137490718312051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09137490718312051 | validation: 0.1099287981919026]
	TIME [epoch: 7.61 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07842076928286591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07842076928286591 | validation: 0.11730853221057874]
	TIME [epoch: 7.62 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10165716371248965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10165716371248965 | validation: 0.22159256528260915]
	TIME [epoch: 7.6 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16830959446405358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16830959446405358 | validation: 0.10333401420219862]
	TIME [epoch: 7.61 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0790899437125787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0790899437125787 | validation: 0.09582979186178313]
	TIME [epoch: 7.6 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06388705860724893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06388705860724893 | validation: 0.07744840578605992]
	TIME [epoch: 7.62 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04709772097650288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04709772097650288 | validation: 0.06496665451388954]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_939.pth
	Model improved!!!
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041152776214733996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041152776214733996 | validation: 0.08558596308191047]
	TIME [epoch: 7.64 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04841231796162775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04841231796162775 | validation: 0.09572895099720324]
	TIME [epoch: 7.63 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11886720312585008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11886720312585008 | validation: 0.3190483602152915]
	TIME [epoch: 7.64 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23971481177014822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23971481177014822 | validation: 0.17997553370176927]
	TIME [epoch: 7.65 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14438251216635548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14438251216635548 | validation: 0.14143295702989123]
	TIME [epoch: 7.64 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10792034741911105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10792034741911105 | validation: 0.13747087936071187]
	TIME [epoch: 7.63 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09888243213819013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09888243213819013 | validation: 0.11624532510727481]
	TIME [epoch: 7.64 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11178964914738795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11178964914738795 | validation: 0.09744309038727869]
	TIME [epoch: 7.63 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0746242543851548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0746242543851548 | validation: 0.09238035668106853]
	TIME [epoch: 7.68 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06730437608559574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06730437608559574 | validation: 0.07250445592239714]
	TIME [epoch: 7.61 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0662721970535229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0662721970535229 | validation: 0.13635820862432232]
	TIME [epoch: 7.61 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09706594000901379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09706594000901379 | validation: 0.17423499418344257]
	TIME [epoch: 7.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14845626802272574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14845626802272574 | validation: 0.18677216619428402]
	TIME [epoch: 7.6 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15539377139321708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15539377139321708 | validation: 0.09665840831743265]
	TIME [epoch: 7.62 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08298755437019498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08298755437019498 | validation: 0.0799660034250756]
	TIME [epoch: 7.6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053885434597427916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053885434597427916 | validation: 0.0829119498270108]
	TIME [epoch: 7.59 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04935891021525325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04935891021525325 | validation: 0.07208673422658739]
	TIME [epoch: 7.6 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05502431684842441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05502431684842441 | validation: 0.0621043128389182]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_957.pth
	Model improved!!!
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04085992438055251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04085992438055251 | validation: 0.06613422556842533]
	TIME [epoch: 7.64 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04340333848423773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04340333848423773 | validation: 0.08176383270432822]
	TIME [epoch: 7.63 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05114882844328694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05114882844328694 | validation: 0.07164480787672414]
	TIME [epoch: 7.63 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07138936023309121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07138936023309121 | validation: 0.266280732419785]
	TIME [epoch: 7.63 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21544808339719745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21544808339719745 | validation: 0.16593449391071782]
	TIME [epoch: 7.63 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18708916923757474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18708916923757474 | validation: 0.23839996329434357]
	TIME [epoch: 7.65 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.274113432450506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.274113432450506 | validation: 0.17493232759881047]
	TIME [epoch: 7.63 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1642646601462044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1642646601462044 | validation: 0.0888250930492621]
	TIME [epoch: 7.64 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06968110911640231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06968110911640231 | validation: 0.08189906366294403]
	TIME [epoch: 7.63 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06631463926436035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06631463926436035 | validation: 0.09419954351157372]
	TIME [epoch: 7.64 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06514140789394059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06514140789394059 | validation: 0.08212433166275762]
	TIME [epoch: 7.64 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06851509845740823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06851509845740823 | validation: 0.09233444171311636]
	TIME [epoch: 7.63 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07242081074470559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07242081074470559 | validation: 0.0892536373162952]
	TIME [epoch: 7.63 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0825440197785848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0825440197785848 | validation: 0.1277300379764889]
	TIME [epoch: 7.63 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08500579297327794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08500579297327794 | validation: 0.11415076698327115]
	TIME [epoch: 7.63 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11569592603288005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11569592603288005 | validation: 0.1457162431347571]
	TIME [epoch: 7.64 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11293492349426154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11293492349426154 | validation: 0.15886092363065255]
	TIME [epoch: 7.62 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12277682943370677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12277682943370677 | validation: 0.188873287093715]
	TIME [epoch: 7.62 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17161257684732134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17161257684732134 | validation: 0.10437548786264338]
	TIME [epoch: 7.63 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08114966528411617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08114966528411617 | validation: 0.0848237143891887]
	TIME [epoch: 7.63 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06140965049443651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06140965049443651 | validation: 0.08839142738414396]
	TIME [epoch: 7.64 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06314075576840603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06314075576840603 | validation: 0.07814245254810363]
	TIME [epoch: 7.63 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05582359891862515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05582359891862515 | validation: 0.10129708686521784]
	TIME [epoch: 7.63 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07219042901911968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07219042901911968 | validation: 0.10621305548865873]
	TIME [epoch: 7.63 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10147478653027428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10147478653027428 | validation: 0.17381285499341362]
	TIME [epoch: 7.64 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14401753201367756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14401753201367756 | validation: 0.11493479713369342]
	TIME [epoch: 7.63 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10772796195363128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10772796195363128 | validation: 0.07198686755886648]
	TIME [epoch: 7.63 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05381594635173281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05381594635173281 | validation: 0.10125780992976541]
	TIME [epoch: 7.61 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0691316274303986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0691316274303986 | validation: 0.07200575123668472]
	TIME [epoch: 7.64 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07532658143014115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07532658143014115 | validation: 0.12447614313762943]
	TIME [epoch: 7.64 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09133753066235378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09133753066235378 | validation: 0.0924709858355634]
	TIME [epoch: 7.64 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10686949215035146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10686949215035146 | validation: 0.10260938230753791]
	TIME [epoch: 7.62 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07106847640663069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07106847640663069 | validation: 0.05164571984245212]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_990.pth
	Model improved!!!
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04389101840552919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04389101840552919 | validation: 0.05918336253272242]
	TIME [epoch: 7.61 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05078236401451529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05078236401451529 | validation: 0.049285437307834794]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_992.pth
	Model improved!!!
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04715714893834671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04715714893834671 | validation: 0.07870216459298378]
	TIME [epoch: 7.61 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06450966373349219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06450966373349219 | validation: 0.10984608932947906]
	TIME [epoch: 7.6 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11654575913806589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11654575913806589 | validation: 0.1978941893371191]
	TIME [epoch: 7.6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16346516584654178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16346516584654178 | validation: 0.18343355040712805]
	TIME [epoch: 7.61 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.175058637090146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.175058637090146 | validation: 0.20817702117312387]
	TIME [epoch: 7.62 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16901152878390246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16901152878390246 | validation: 0.10250939805506741]
	TIME [epoch: 7.6 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09528698486445757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09528698486445757 | validation: 0.13832233180917625]
	TIME [epoch: 7.59 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11480728741194021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11480728741194021 | validation: 0.11026121924853327]
	TIME [epoch: 7.6 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09678497681679046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09678497681679046 | validation: 0.11198041887750505]
	TIME [epoch: 41.9 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08380198052182561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08380198052182561 | validation: 0.0898793324281515]
	TIME [epoch: 16.4 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08087696645541534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08087696645541534 | validation: 0.08905871360176132]
	TIME [epoch: 16.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05610733619921897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05610733619921897 | validation: 0.0879319126483532]
	TIME [epoch: 16.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1035919159869848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1035919159869848 | validation: 0.16581154883907698]
	TIME [epoch: 16.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13068721579736284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13068721579736284 | validation: 0.12727620490752473]
	TIME [epoch: 16.3 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08187742083024377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08187742083024377 | validation: 0.11460993263582041]
	TIME [epoch: 16.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11053149642471294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11053149642471294 | validation: 0.18383793466019563]
	TIME [epoch: 16.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13957775663965746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13957775663965746 | validation: 0.09492233882366818]
	TIME [epoch: 16.4 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08539822942720722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08539822942720722 | validation: 0.07885827058454564]
	TIME [epoch: 16.4 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05657976396836375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05657976396836375 | validation: 0.14334204426918173]
	TIME [epoch: 16.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10321854419258392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10321854419258392 | validation: 0.09118868682195921]
	TIME [epoch: 16.3 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07900571999946022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07900571999946022 | validation: 0.09351949764693085]
	TIME [epoch: 16.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06671823422809603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06671823422809603 | validation: 0.09436521259618584]
	TIME [epoch: 16.3 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0636169720193036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0636169720193036 | validation: 0.08309570290104842]
	TIME [epoch: 16.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06654434854923214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06654434854923214 | validation: 0.12448844281952592]
	TIME [epoch: 16.3 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08960974949114339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08960974949114339 | validation: 0.10957398457777862]
	TIME [epoch: 16.4 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10594490931308415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10594490931308415 | validation: 0.11685856939642508]
	TIME [epoch: 16.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09555475438420873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09555475438420873 | validation: 0.07163635566361036]
	TIME [epoch: 16.4 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05779547495344468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05779547495344468 | validation: 0.08002532299954168]
	TIME [epoch: 16.4 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0540801658729985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0540801658729985 | validation: 0.06916048750466433]
	TIME [epoch: 16.3 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05677189099844189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05677189099844189 | validation: 0.08483994929452637]
	TIME [epoch: 16.4 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061735321684572755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061735321684572755 | validation: 0.0650511278999653]
	TIME [epoch: 16.4 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06995224058649792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06995224058649792 | validation: 0.08141314786232945]
	TIME [epoch: 16.4 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06129483604208275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06129483604208275 | validation: 0.0904598756035152]
	TIME [epoch: 16.4 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09469656389773828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09469656389773828 | validation: 0.12064256078815816]
	TIME [epoch: 16.4 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039770259113207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1039770259113207 | validation: 0.09505577097694032]
	TIME [epoch: 16.4 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0859954351315795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0859954351315795 | validation: 0.12790703113235669]
	TIME [epoch: 16.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08622177051294966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08622177051294966 | validation: 0.1340382703758346]
	TIME [epoch: 16.4 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11414069425371474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11414069425371474 | validation: 0.16556077754779583]
	TIME [epoch: 16.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11559978400036071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11559978400036071 | validation: 0.0699662489400637]
	TIME [epoch: 16.4 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059366424018205244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059366424018205244 | validation: 0.07357315538662028]
	TIME [epoch: 16.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039335302847265184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039335302847265184 | validation: 0.07217516649444261]
	TIME [epoch: 16.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050904429915285515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050904429915285515 | validation: 0.1119947788719473]
	TIME [epoch: 16.4 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08975869048508078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08975869048508078 | validation: 0.1433626401096826]
	TIME [epoch: 16.4 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12920045119177898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12920045119177898 | validation: 0.14020831800971423]
	TIME [epoch: 16.4 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11841329401240737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11841329401240737 | validation: 0.13261464532405884]
	TIME [epoch: 16.4 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14120666559433454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14120666559433454 | validation: 0.07557250064258889]
	TIME [epoch: 16.4 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06320749305972563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06320749305972563 | validation: 0.09261012347877363]
	TIME [epoch: 16.4 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07686887855749852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07686887855749852 | validation: 0.07334039848006299]
	TIME [epoch: 16.4 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06953880484369677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06953880484369677 | validation: 0.08201530238622495]
	TIME [epoch: 16.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058966481060817755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058966481060817755 | validation: 0.07252839784256256]
	TIME [epoch: 16.4 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042558520112261945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042558520112261945 | validation: 0.06349853207302003]
	TIME [epoch: 16.4 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04365295466598171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04365295466598171 | validation: 0.09489169414081583]
	TIME [epoch: 16.4 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06662175510099269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06662175510099269 | validation: 0.12885155989294797]
	TIME [epoch: 16.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1115334379516969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1115334379516969 | validation: 0.10586755527172345]
	TIME [epoch: 16.4 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08595432440523672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08595432440523672 | validation: 0.07935805060146414]
	TIME [epoch: 16.4 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0658634075881974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0658634075881974 | validation: 0.08439462670099002]
	TIME [epoch: 16.4 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06347676829325741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06347676829325741 | validation: 0.12491818673420557]
	TIME [epoch: 16.4 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09370975123228352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09370975123228352 | validation: 0.22511537802079185]
	TIME [epoch: 16.4 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2537969586881516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2537969586881516 | validation: 0.11816633501111179]
	TIME [epoch: 16.4 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11974376985585643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11974376985585643 | validation: 0.08112273872195204]
	TIME [epoch: 16.3 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07050789320893784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07050789320893784 | validation: 0.10599650777327324]
	TIME [epoch: 16.4 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12456246457676469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12456246457676469 | validation: 0.12634616439739232]
	TIME [epoch: 16.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1254635518873267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1254635518873267 | validation: 0.06723373021529273]
	TIME [epoch: 16.4 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05731438712480765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05731438712480765 | validation: 0.06464941600202159]
	TIME [epoch: 16.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06779575365104515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06779575365104515 | validation: 0.06289875539706238]
	TIME [epoch: 16.4 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05506995468111809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05506995468111809 | validation: 0.07629018909833048]
	TIME [epoch: 16.4 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05537050932378113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05537050932378113 | validation: 0.0647165110890886]
	TIME [epoch: 16.4 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04982241764403562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04982241764403562 | validation: 0.06606916166555607]
	TIME [epoch: 16.4 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04690125272715808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04690125272715808 | validation: 0.07087605348019198]
	TIME [epoch: 16.4 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05665056079874222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05665056079874222 | validation: 0.12745244367011663]
	TIME [epoch: 16.4 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11158984509089777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11158984509089777 | validation: 0.1363022643987439]
	TIME [epoch: 16.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1156788981179842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1156788981179842 | validation: 0.10512855821914063]
	TIME [epoch: 16.4 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08371133603533103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08371133603533103 | validation: 0.06052997181376964]
	TIME [epoch: 16.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06023933825522155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06023933825522155 | validation: 0.049970092955346934]
	TIME [epoch: 16.4 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03670848387961164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03670848387961164 | validation: 0.045030358881645555]
	TIME [epoch: 16.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_1067.pth
	Model improved!!!
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0315643366211775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0315643366211775 | validation: 0.04131746351410739]
	TIME [epoch: 16.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_1068.pth
	Model improved!!!
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03385264702263855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03385264702263855 | validation: 0.1015571156916113]
	TIME [epoch: 16.4 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15027919906002027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15027919906002027 | validation: 0.22917631329449362]
	TIME [epoch: 16.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1655665990563286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1655665990563286 | validation: 0.2723146952768302]
	TIME [epoch: 16.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24190800039285212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24190800039285212 | validation: 0.10219194056905107]
	TIME [epoch: 16.4 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08142362103092295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08142362103092295 | validation: 0.07056568987420525]
	TIME [epoch: 16.4 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06453018278005854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06453018278005854 | validation: 0.10598249150256396]
	TIME [epoch: 16.4 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07329647268631406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07329647268631406 | validation: 0.12338689083846383]
	TIME [epoch: 16.4 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09925040431743565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09925040431743565 | validation: 0.1251546356921504]
	TIME [epoch: 16.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15467931745001093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15467931745001093 | validation: 0.09498369764894593]
	TIME [epoch: 16.4 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08658661003378741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08658661003378741 | validation: 0.0743569569724005]
	TIME [epoch: 16.4 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05251353689962839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05251353689962839 | validation: 0.0685303663640393]
	TIME [epoch: 16.4 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06815918577049612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06815918577049612 | validation: 0.12574755057360393]
	TIME [epoch: 16.4 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08824688494339146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08824688494339146 | validation: 0.09017196085700735]
	TIME [epoch: 16.4 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06926343407947254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06926343407947254 | validation: 0.07365842748736462]
	TIME [epoch: 16.4 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04973736007424157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04973736007424157 | validation: 0.06907885642677274]
	TIME [epoch: 16.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050127288763783806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050127288763783806 | validation: 0.06763692353189889]
	TIME [epoch: 16.4 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06052502231804897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06052502231804897 | validation: 0.16672531242026745]
	TIME [epoch: 16.4 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14060988707503766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14060988707503766 | validation: 0.17232142789505517]
	TIME [epoch: 16.4 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13830427383245256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13830427383245256 | validation: 0.16124736847092316]
	TIME [epoch: 16.4 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11748476093556626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11748476093556626 | validation: 0.0842113829587707]
	TIME [epoch: 16.4 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10616670849835781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10616670849835781 | validation: 0.07971068784466372]
	TIME [epoch: 16.4 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051226522717230466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051226522717230466 | validation: 0.0732143030541678]
	TIME [epoch: 16.4 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0433836893600749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0433836893600749 | validation: 0.06065146357845589]
	TIME [epoch: 16.4 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05207782001658826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05207782001658826 | validation: 0.11760440532310766]
	TIME [epoch: 16.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07864006097220573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07864006097220573 | validation: 0.06626191088973786]
	TIME [epoch: 16.4 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054865599944421126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054865599944421126 | validation: 0.09656492542955945]
	TIME [epoch: 16.4 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06466678917582294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06466678917582294 | validation: 0.08350375784313988]
	TIME [epoch: 16.4 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0742721657873623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0742721657873623 | validation: 0.11533344476794234]
	TIME [epoch: 16.4 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08338475064743889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08338475064743889 | validation: 0.08452647003948831]
	TIME [epoch: 16.4 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0943325909359579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0943325909359579 | validation: 0.13004576081291194]
	TIME [epoch: 16.4 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07935426848770086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07935426848770086 | validation: 0.09310635113751525]
	TIME [epoch: 16.4 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0831289325354114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0831289325354114 | validation: 0.1306500766718233]
	TIME [epoch: 16.4 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09475538474547808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09475538474547808 | validation: 0.1281843468088755]
	TIME [epoch: 16.4 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11015103919225311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11015103919225311 | validation: 0.10657884047609115]
	TIME [epoch: 16.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07708139557268286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07708139557268286 | validation: 0.06742804273770381]
	TIME [epoch: 16.4 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04844686635211419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04844686635211419 | validation: 0.06205748992399993]
	TIME [epoch: 16.4 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06472692397580286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06472692397580286 | validation: 0.08838233286700944]
	TIME [epoch: 16.4 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06211731864446078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06211731864446078 | validation: 0.07584141625368275]
	TIME [epoch: 16.4 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06246702120443807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06246702120443807 | validation: 0.09658582352851504]
	TIME [epoch: 16.4 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06513022066740479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06513022066740479 | validation: 0.07342692441843049]
	TIME [epoch: 16.4 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06772243623104732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06772243623104732 | validation: 0.09199530728462033]
	TIME [epoch: 16.4 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06169082150568967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06169082150568967 | validation: 0.06848827631664764]
	TIME [epoch: 16.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06215324886302563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06215324886302563 | validation: 0.09386256650711233]
	TIME [epoch: 16.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05966089445021074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05966089445021074 | validation: 0.0921247165913246]
	TIME [epoch: 16.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06603869825467615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06603869825467615 | validation: 0.11026713224927871]
	TIME [epoch: 16.4 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09113081521531947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09113081521531947 | validation: 0.11603135649289338]
	TIME [epoch: 16.4 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08496233359197718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08496233359197718 | validation: 0.06709988398358273]
	TIME [epoch: 16.3 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06386069833626805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06386069833626805 | validation: 0.09437798608091517]
	TIME [epoch: 16.4 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06969642553984547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06969642553984547 | validation: 0.07740563953759329]
	TIME [epoch: 16.4 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06835478833528699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06835478833528699 | validation: 0.11067542988335455]
	TIME [epoch: 16.4 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07579623325687673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07579623325687673 | validation: 0.08576790827896123]
	TIME [epoch: 16.3 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07475895231842326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07475895231842326 | validation: 0.08627330491563759]
	TIME [epoch: 16.4 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059008074336856606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059008074336856606 | validation: 0.05477412595733151]
	TIME [epoch: 16.4 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041871442753657825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041871442753657825 | validation: 0.06357184594346689]
	TIME [epoch: 16.4 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04830569556123056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04830569556123056 | validation: 0.11771620822749856]
	TIME [epoch: 16.4 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08914392135442233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08914392135442233 | validation: 0.08024642609440702]
	TIME [epoch: 16.4 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06772816246948725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06772816246948725 | validation: 0.06255239493995819]
	TIME [epoch: 16.4 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05785550546673861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05785550546673861 | validation: 0.05904343208177133]
	TIME [epoch: 16.4 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058593942587089214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058593942587089214 | validation: 0.09435838822426085]
	TIME [epoch: 16.4 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08694007251980704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08694007251980704 | validation: 0.0781791321109312]
	TIME [epoch: 16.4 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08049575846796808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08049575846796808 | validation: 0.10703495421779984]
	TIME [epoch: 16.4 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08794504258912048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08794504258912048 | validation: 0.10699900873391358]
	TIME [epoch: 16.4 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09514046388784664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09514046388784664 | validation: 0.1206382644941073]
	TIME [epoch: 16.4 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09202385739278433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09202385739278433 | validation: 0.10328780833216918]
	TIME [epoch: 16.4 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08818666406900631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08818666406900631 | validation: 0.0661951543903139]
	TIME [epoch: 16.4 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054330937597474975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054330937597474975 | validation: 0.06672263036837833]
	TIME [epoch: 16.4 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04638570879768316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04638570879768316 | validation: 0.050738714589551454]
	TIME [epoch: 16.4 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04302194782380938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04302194782380938 | validation: 0.07913060390506199]
	TIME [epoch: 16.4 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05409543489559897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05409543489559897 | validation: 0.06850518591772599]
	TIME [epoch: 16.4 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05702646656966618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05702646656966618 | validation: 0.10645394660294678]
	TIME [epoch: 16.4 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07863280234627715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07863280234627715 | validation: 0.1099322795149063]
	TIME [epoch: 16.4 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10449849152821727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10449849152821727 | validation: 0.07790151536458094]
	TIME [epoch: 16.4 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13020795945848548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13020795945848548 | validation: 0.10565914446595462]
	TIME [epoch: 16.4 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0997842634538196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0997842634538196 | validation: 0.09129262278537165]
	TIME [epoch: 16.4 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08927695473783846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08927695473783846 | validation: 0.08542497727686715]
	TIME [epoch: 16.4 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06959912540497885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06959912540497885 | validation: 0.05810397524199379]
	TIME [epoch: 16.4 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04809739334421053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04809739334421053 | validation: 0.07012137309639992]
	TIME [epoch: 16.4 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045960414726078355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045960414726078355 | validation: 0.048938878321332306]
	TIME [epoch: 16.4 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037090099810528285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037090099810528285 | validation: 0.06467388677778994]
	TIME [epoch: 16.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03821359861867501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03821359861867501 | validation: 0.0682590148971089]
	TIME [epoch: 16.4 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05186212779275011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05186212779275011 | validation: 0.12453868377371485]
	TIME [epoch: 16.4 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.083885657994098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.083885657994098 | validation: 0.10000120934201838]
	TIME [epoch: 16.4 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08308657723385178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08308657723385178 | validation: 0.10496208904103099]
	TIME [epoch: 16.4 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07393816770089578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07393816770089578 | validation: 0.09098269464080866]
	TIME [epoch: 16.4 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08853494560732138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08853494560732138 | validation: 0.10259974835896296]
	TIME [epoch: 16.4 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07587855103798524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07587855103798524 | validation: 0.07032838709920425]
	TIME [epoch: 16.4 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05714506506201447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05714506506201447 | validation: 0.06302072507775645]
	TIME [epoch: 16.4 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05374914671650288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05374914671650288 | validation: 0.10107900916330351]
	TIME [epoch: 16.4 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06877405819771347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06877405819771347 | validation: 0.11099505813590063]
	TIME [epoch: 16.4 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09615672985188478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09615672985188478 | validation: 0.13456570086026007]
	TIME [epoch: 16.4 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0946503518453915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0946503518453915 | validation: 0.06897407459756259]
	TIME [epoch: 16.4 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04802005142130357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04802005142130357 | validation: 0.07235563163481364]
	TIME [epoch: 16.4 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049789471317801565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049789471317801565 | validation: 0.10720277893988728]
	TIME [epoch: 16.4 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08855107283774097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08855107283774097 | validation: 0.09840760857971348]
	TIME [epoch: 16.4 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09165369363288953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09165369363288953 | validation: 0.0794078519870839]
	TIME [epoch: 16.4 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06298684661574079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06298684661574079 | validation: 0.08942076227191381]
	TIME [epoch: 16.4 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11444298821149826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11444298821149826 | validation: 0.1036325370768346]
	TIME [epoch: 16.4 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08329045074480249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08329045074480249 | validation: 0.13183531054147543]
	TIME [epoch: 16.4 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09853687979786731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09853687979786731 | validation: 0.07592343517079347]
	TIME [epoch: 16.4 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0528461551506628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0528461551506628 | validation: 0.06410858019876704]
	TIME [epoch: 16.4 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035871468946873195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035871468946873195 | validation: 0.06796628979971468]
	TIME [epoch: 16.3 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172713/states/model_phi1_3b_v_mmd1_1169.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 8203.251 seconds.
