Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1408963554

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.595527087819977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.595527087819977 | validation: 6.362095331406684]
	TIME [epoch: 45.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.351746237066564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.351746237066564 | validation: 6.237824053297395]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.126770641970547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.126770641970547 | validation: 4.479847784660025]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.580264315978411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.580264315978411 | validation: 5.818868231928504]
	TIME [epoch: 3.76 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.0858239565295404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0858239565295404 | validation: 6.455902265018931]
	TIME [epoch: 3.73 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.482430544110285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.482430544110285 | validation: 5.560204026626408]
	TIME [epoch: 3.73 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.889839208988256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.889839208988256 | validation: 4.5617263954386145]
	TIME [epoch: 3.73 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.025878411405797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.025878411405797 | validation: 4.905719546765656]
	TIME [epoch: 3.73 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.943822510473077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.943822510473077 | validation: 4.582097604958415]
	TIME [epoch: 3.72 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.662772121943609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.662772121943609 | validation: 3.865815553471916]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.239310096459074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.239310096459074 | validation: 3.8109462174282975]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3159393128083945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3159393128083945 | validation: 3.947109198792091]
	TIME [epoch: 3.73 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.19716420784913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.19716420784913 | validation: 3.9217532579477012]
	TIME [epoch: 3.73 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.17761729659437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.17761729659437 | validation: 3.764308464992242]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.144699873609161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.144699873609161 | validation: 3.7825147499150655]
	TIME [epoch: 3.75 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.117244767249857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.117244767249857 | validation: 3.7819683628054053]
	TIME [epoch: 3.73 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.093425153844798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.093425153844798 | validation: 3.7357214863412596]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.072688968007214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.072688968007214 | validation: 3.720582813587364]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0584716811984185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0584716811984185 | validation: 3.699030715955575]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.038220151544391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.038220151544391 | validation: 3.699347890512752]
	TIME [epoch: 3.72 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.02435296630223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.02435296630223 | validation: 3.6342283930899977]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.029643479313505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.029643479313505 | validation: 3.8518813489225523]
	TIME [epoch: 3.73 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.114080531892228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.114080531892228 | validation: 3.7046556240413895]
	TIME [epoch: 3.73 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.15627165849675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.15627165849675 | validation: 3.842624187203724]
	TIME [epoch: 3.73 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.090559724215955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.090559724215955 | validation: 3.5706215538476087]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9559703371125385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9559703371125385 | validation: 3.557723106387801]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.947020222073984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.947020222073984 | validation: 3.6382913037822107]
	TIME [epoch: 3.74 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.954960812872516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.954960812872516 | validation: 3.534840691781234]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9521048139810446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9521048139810446 | validation: 3.640612032759286]
	TIME [epoch: 3.73 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.956466415589752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.956466415589752 | validation: 3.5062815182584695]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9175537380717027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9175537380717027 | validation: 3.562448656863589]
	TIME [epoch: 3.72 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8989263683096578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8989263683096578 | validation: 3.468047199451391]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.87758579040162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.87758579040162 | validation: 3.532143597600471]
	TIME [epoch: 3.71 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.870060265183076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.870060265183076 | validation: 3.445368413528921]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8538331989860706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8538331989860706 | validation: 3.5076877077911393]
	TIME [epoch: 3.75 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8545213250286623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8545213250286623 | validation: 3.409158209260186]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8260180729064737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8260180729064737 | validation: 3.4674926545486224]
	TIME [epoch: 3.74 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.809857370041842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.809857370041842 | validation: 3.356659037616706]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7653271944756823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7653271944756823 | validation: 3.346197667219]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.707489155480208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.707489155480208 | validation: 3.264774225783971]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6683562637701335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6683562637701335 | validation: 3.302579991450663]
	TIME [epoch: 3.74 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6568828447194774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6568828447194774 | validation: 3.2621673688557777]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6368648007439868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6368648007439868 | validation: 3.2917143712841184]
	TIME [epoch: 3.72 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6520989850876697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6520989850876697 | validation: 3.364616681136413]
	TIME [epoch: 3.72 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6934734982837307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6934734982837307 | validation: 3.270721020749359]
	TIME [epoch: 3.73 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.637916484335077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.637916484335077 | validation: 3.2684993298934986]
	TIME [epoch: 3.73 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.605913983040695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.605913983040695 | validation: 3.13226385213807]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5421991345497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5421991345497 | validation: 3.120746503613768]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5094377529318206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5094377529318206 | validation: 3.113453459037273]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4987281382007343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4987281382007343 | validation: 3.093698170592985]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.484338547899852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.484338547899852 | validation: 3.113271412348849]
	TIME [epoch: 3.74 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4784837277127343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4784837277127343 | validation: 3.1864506191767803]
	TIME [epoch: 3.73 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5507269304760394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5507269304760394 | validation: 3.329632127242947]
	TIME [epoch: 3.73 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.69524138649458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.69524138649458 | validation: 3.1455532525739995]
	TIME [epoch: 3.73 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.502381630209659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.502381630209659 | validation: 3.128218969808114]
	TIME [epoch: 3.74 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4915526892535955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4915526892535955 | validation: 3.106652707924585]
	TIME [epoch: 3.75 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4475500346525836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4475500346525836 | validation: 3.0117746922880104]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.377785983592354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.377785983592354 | validation: 3.0063167484577566]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3840082670158633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3840082670158633 | validation: 2.99709582664857]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.359363987629986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.359363987629986 | validation: 2.9670430433348245]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3211017758860226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3211017758860226 | validation: 2.9402719005833995]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.304948159094404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.304948159094404 | validation: 2.935897115540263]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2951389943019227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2951389943019227 | validation: 2.909050388475218]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.271404197080753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.271404197080753 | validation: 2.9767640197318324]
	TIME [epoch: 3.71 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.321737233418543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.321737233418543 | validation: 3.168348810773468]
	TIME [epoch: 3.71 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.515955716388953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.515955716388953 | validation: 2.9628975538562963]
	TIME [epoch: 3.73 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.324574508034837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.324574508034837 | validation: 2.955804355606775]
	TIME [epoch: 3.73 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2975153674644084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2975153674644084 | validation: 2.9103979074404616]
	TIME [epoch: 3.73 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2666243292735393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2666243292735393 | validation: 2.874769732122502]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.209152894012985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.209152894012985 | validation: 2.8392458501228504]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1732182206350013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1732182206350013 | validation: 2.8108329237266543]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1587433485940766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1587433485940766 | validation: 2.8296829407142856]
	TIME [epoch: 3.73 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.153022576442839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.153022576442839 | validation: 2.893473021910077]
	TIME [epoch: 3.73 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2348572562729414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2348572562729414 | validation: 2.8352047353616365]
	TIME [epoch: 3.73 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.172365591098552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.172365591098552 | validation: 2.735476282416729]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0617950710845494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0617950710845494 | validation: 2.6077706795443265]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9293882171835492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9293882171835492 | validation: 2.4464442923558796]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7293970903559863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7293970903559863 | validation: 2.039459064159266]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.354079213483499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.354079213483499 | validation: 1.6124933060585669]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9447566587862113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9447566587862113 | validation: 1.2589280250170678]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5463189478054704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5463189478054704 | validation: 2.16250050874198]
	TIME [epoch: 3.73 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.475420397231035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.475420397231035 | validation: 1.3710831868606135]
	TIME [epoch: 3.73 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6344431099609114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6344431099609114 | validation: 1.591891048849862]
	TIME [epoch: 3.73 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.069981548136751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.069981548136751 | validation: 0.9401255251602763]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1882561168394674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1882561168394674 | validation: 0.9828997668940066]
	TIME [epoch: 3.74 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.225931787496564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.225931787496564 | validation: 0.8209549960178952]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0626829326846967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0626829326846967 | validation: 0.7606515836361686]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9921312172724709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9921312172724709 | validation: 0.780850306352634]
	TIME [epoch: 3.73 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.992047671035856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.992047671035856 | validation: 0.79673999376618]
	TIME [epoch: 3.74 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9884354297439749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9884354297439749 | validation: 0.8632843802862796]
	TIME [epoch: 3.74 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0842363542749913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0842363542749913 | validation: 0.8450509189266331]
	TIME [epoch: 3.74 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.04523916139204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.04523916139204 | validation: 0.7467779826570224]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9381922384103232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9381922384103232 | validation: 0.7368405499445791]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9353783179734945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9353783179734945 | validation: 0.7247503459098027]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9270410140104506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9270410140104506 | validation: 0.74058733741053]
	TIME [epoch: 3.72 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9239842023485679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9239842023485679 | validation: 0.739406145475332]
	TIME [epoch: 3.72 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9224641644240198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9224641644240198 | validation: 0.74695201410449]
	TIME [epoch: 3.74 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9492923784826894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9492923784826894 | validation: 0.814662105567495]
	TIME [epoch: 3.73 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0194244512920192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0194244512920192 | validation: 0.8765717684218605]
	TIME [epoch: 3.71 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0985851101441617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0985851101441617 | validation: 0.7958357174368464]
	TIME [epoch: 3.72 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9986205686030055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9986205686030055 | validation: 0.7571289824331496]
	TIME [epoch: 3.72 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9266304466920552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9266304466920552 | validation: 0.7768814343703556]
	TIME [epoch: 3.73 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9372160369825594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9372160369825594 | validation: 0.7730412479502822]
	TIME [epoch: 3.73 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.945857387346388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.945857387346388 | validation: 0.8505452741412969]
	TIME [epoch: 3.73 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0540027022518725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0540027022518725 | validation: 0.8085714720454424]
	TIME [epoch: 3.73 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9967164688085073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9967164688085073 | validation: 0.7364392785414566]
	TIME [epoch: 3.73 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8992568578148665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8992568578148665 | validation: 0.7690882941322916]
	TIME [epoch: 3.73 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9092060074885775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9092060074885775 | validation: 0.804821977000623]
	TIME [epoch: 3.74 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9749613683127538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9749613683127538 | validation: 0.7932081619422914]
	TIME [epoch: 3.74 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9725199160267062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9725199160267062 | validation: 0.8895168644370343]
	TIME [epoch: 3.73 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0837866481678753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0837866481678753 | validation: 0.859729589716854]
	TIME [epoch: 3.73 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0407414829634736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0407414829634736 | validation: 0.7784329577370009]
	TIME [epoch: 3.73 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9298871473045083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9298871473045083 | validation: 0.7810184065276199]
	TIME [epoch: 3.72 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.94110169698043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.94110169698043 | validation: 0.8343712484685528]
	TIME [epoch: 3.72 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0254437899776718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0254437899776718 | validation: 0.7345510914808724]
	TIME [epoch: 3.72 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8936511833046944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8936511833046944 | validation: 0.7788965917908299]
	TIME [epoch: 3.72 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9183457132736706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9183457132736706 | validation: 0.7673103612319164]
	TIME [epoch: 3.73 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9019884016735408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9019884016735408 | validation: 0.7704382749170904]
	TIME [epoch: 3.73 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9155513264438286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9155513264438286 | validation: 0.7872857202807372]
	TIME [epoch: 3.74 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9512286835274238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9512286835274238 | validation: 0.7464445406309097]
	TIME [epoch: 3.74 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8991861910959617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8991861910959617 | validation: 0.7391769334557027]
	TIME [epoch: 3.73 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8902123095054808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8902123095054808 | validation: 0.7536626140682936]
	TIME [epoch: 3.73 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8925234207328556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8925234207328556 | validation: 0.8085853548422833]
	TIME [epoch: 3.73 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9411977916170179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9411977916170179 | validation: 0.8381166115135832]
	TIME [epoch: 3.73 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9816884468416374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9816884468416374 | validation: 0.7722565100004257]
	TIME [epoch: 3.73 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9191800577585744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9191800577585744 | validation: 0.7910413178459553]
	TIME [epoch: 3.73 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9609447004981074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9609447004981074 | validation: 0.9888043702223549]
	TIME [epoch: 3.72 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.177139537312042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.177139537312042 | validation: 0.759193577398429]
	TIME [epoch: 3.73 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9173106600881872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9173106600881872 | validation: 0.9451747291322488]
	TIME [epoch: 3.73 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.138551864243926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.138551864243926 | validation: 0.8416206571914433]
	TIME [epoch: 3.74 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.013753216310702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.013753216310702 | validation: 0.7508622809799435]
	TIME [epoch: 3.74 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9439877850503993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9439877850503993 | validation: 0.784687263100042]
	TIME [epoch: 3.72 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9436708053783229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9436708053783229 | validation: 0.7462286664534954]
	TIME [epoch: 3.73 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9039193807954353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9039193807954353 | validation: 0.7425465834659388]
	TIME [epoch: 3.73 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8891227813989806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8891227813989806 | validation: 0.7457895492214904]
	TIME [epoch: 3.73 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8934118870497015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8934118870497015 | validation: 0.7819542911649371]
	TIME [epoch: 3.72 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9312004828965289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9312004828965289 | validation: 0.7369729957531965]
	TIME [epoch: 3.73 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8899871027295972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8899871027295972 | validation: 0.7805665172125199]
	TIME [epoch: 3.72 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9410096654262219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9410096654262219 | validation: 0.7849802823029816]
	TIME [epoch: 3.78 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9342243656864707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9342243656864707 | validation: 0.8051917837422601]
	TIME [epoch: 3.72 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9848850170000697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9848850170000697 | validation: 0.7951822637621982]
	TIME [epoch: 3.74 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9198154979655234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9198154979655234 | validation: 0.800806435830759]
	TIME [epoch: 3.73 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9506331387965442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9506331387965442 | validation: 0.7445085540163267]
	TIME [epoch: 3.73 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8984697627330556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8984697627330556 | validation: 0.7345447777100313]
	TIME [epoch: 3.72 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8858379043412654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8858379043412654 | validation: 0.7416909247355342]
	TIME [epoch: 3.73 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8912598823023893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8912598823023893 | validation: 0.8128488765639571]
	TIME [epoch: 3.72 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9461152363972173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9461152363972173 | validation: 0.8034611884983474]
	TIME [epoch: 3.73 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.942819908592175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.942819908592175 | validation: 0.7630980372269651]
	TIME [epoch: 3.73 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8863276271137599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8863276271137599 | validation: 0.734364938585588]
	TIME [epoch: 3.72 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8948552811339053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8948552811339053 | validation: 0.7542435283332626]
	TIME [epoch: 3.72 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887771945431655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.887771945431655 | validation: 0.7550784599273683]
	TIME [epoch: 3.73 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8909409014402945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8909409014402945 | validation: 0.7760617635386864]
	TIME [epoch: 3.72 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9603961740516732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9603961740516732 | validation: 0.8631315274997968]
	TIME [epoch: 3.74 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.043695206566443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.043695206566443 | validation: 0.8046703417139363]
	TIME [epoch: 3.73 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9788068072837232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9788068072837232 | validation: 0.7314059177018545]
	TIME [epoch: 3.72 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8709689138356983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8709689138356983 | validation: 0.7817838617298541]
	TIME [epoch: 3.73 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9210675913411075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9210675913411075 | validation: 0.7755390600381568]
	TIME [epoch: 3.72 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.931710256040669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.931710256040669 | validation: 0.7640678300764635]
	TIME [epoch: 3.73 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.886182975509766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.886182975509766 | validation: 0.7677175796983848]
	TIME [epoch: 3.72 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8869596315834879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8869596315834879 | validation: 0.7975168076437475]
	TIME [epoch: 3.72 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9177186952734063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9177186952734063 | validation: 0.8613320070034591]
	TIME [epoch: 3.72 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9843116989150851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9843116989150851 | validation: 0.7911149125526955]
	TIME [epoch: 3.73 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8960076517906534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8960076517906534 | validation: 0.7720704727246873]
	TIME [epoch: 3.73 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9165020068396595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9165020068396595 | validation: 0.7838396496868656]
	TIME [epoch: 3.74 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9279802544827643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9279802544827643 | validation: 0.7983152332058927]
	TIME [epoch: 3.74 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9337895783005283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9337895783005283 | validation: 0.7479291695888064]
	TIME [epoch: 3.73 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9075127671084549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9075127671084549 | validation: 0.76895110943875]
	TIME [epoch: 3.73 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8914741583771151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8914741583771151 | validation: 0.7670383213345119]
	TIME [epoch: 3.72 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9049055758398029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9049055758398029 | validation: 0.7858629413975995]
	TIME [epoch: 3.72 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8837611151587109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8837611151587109 | validation: 0.7742302293839461]
	TIME [epoch: 3.72 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9203652837773791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9203652837773791 | validation: 0.7979873065581083]
	TIME [epoch: 3.72 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.916814995439844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.916814995439844 | validation: 0.7575550528780977]
	TIME [epoch: 3.73 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9047651891221109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9047651891221109 | validation: 0.7555941997160871]
	TIME [epoch: 3.72 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8702128012352861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8702128012352861 | validation: 0.7836995723779892]
	TIME [epoch: 3.73 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9003264963189509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9003264963189509 | validation: 0.9716789573899032]
	TIME [epoch: 3.74 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0754241593606058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0754241593606058 | validation: 0.7470979508849682]
	TIME [epoch: 3.74 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649654311533083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8649654311533083 | validation: 0.7984438100603979]
	TIME [epoch: 3.72 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9531870446208336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9531870446208336 | validation: 0.8268355709925315]
	TIME [epoch: 3.73 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9420607734478824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9420607734478824 | validation: 0.7437750346738303]
	TIME [epoch: 3.73 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859316624491544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.859316624491544 | validation: 0.7804101884380615]
	TIME [epoch: 3.73 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9307231427865125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9307231427865125 | validation: 0.782316538193979]
	TIME [epoch: 3.73 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9139736351668618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9139736351668618 | validation: 0.7678222712821765]
	TIME [epoch: 3.73 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9104076510273202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9104076510273202 | validation: 0.930141591092932]
	TIME [epoch: 3.73 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.971668811730037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.971668811730037 | validation: 0.9318968680601533]
	TIME [epoch: 3.73 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0247630435223882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0247630435223882 | validation: 0.748330895596987]
	TIME [epoch: 3.73 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8624290009537819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8624290009537819 | validation: 0.844060232190183]
	TIME [epoch: 3.74 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.920052390522514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.920052390522514 | validation: 0.8556295811799874]
	TIME [epoch: 3.74 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0318865715858863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0318865715858863 | validation: 0.748551791484464]
	TIME [epoch: 3.73 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8558442682348546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8558442682348546 | validation: 0.8390579653182689]
	TIME [epoch: 3.72 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9492421637159908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9492421637159908 | validation: 0.8297230736844861]
	TIME [epoch: 3.72 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9774396220826058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9774396220826058 | validation: 0.7439615981979115]
	TIME [epoch: 3.73 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8537637504671753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8537637504671753 | validation: 0.8220958605934361]
	TIME [epoch: 3.73 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9375277629014883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9375277629014883 | validation: 0.7490877070681478]
	TIME [epoch: 3.73 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8841654480090207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8841654480090207 | validation: 0.7434984835514376]
	TIME [epoch: 3.73 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8520617739179415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8520617739179415 | validation: 0.7970926589301754]
	TIME [epoch: 3.73 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.873886009394295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.873886009394295 | validation: 0.7814733427845967]
	TIME [epoch: 3.72 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8993643416985906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8993643416985906 | validation: 0.768579187240781]
	TIME [epoch: 3.73 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8453519542570108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8453519542570108 | validation: 0.7867735065950506]
	TIME [epoch: 3.75 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8644716885592345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8644716885592345 | validation: 1.0602365737685429]
	TIME [epoch: 3.73 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0975195607742223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0975195607742223 | validation: 0.9979795432631917]
	TIME [epoch: 3.72 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.065588122917841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.065588122917841 | validation: 0.8007604070667409]
	TIME [epoch: 48 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8674206206665117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8674206206665117 | validation: 0.7495028184950706]
	TIME [epoch: 8.09 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8505258723227549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8505258723227549 | validation: 0.7758209754900053]
	TIME [epoch: 8.08 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8463466505780528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8463466505780528 | validation: 0.7461875236435754]
	TIME [epoch: 8.08 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8294887871438588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8294887871438588 | validation: 0.7724067805068926]
	TIME [epoch: 8.09 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8456789572251302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8456789572251302 | validation: 0.796383051155098]
	TIME [epoch: 8.08 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9320375463052943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9320375463052943 | validation: 0.7954805879135002]
	TIME [epoch: 8.09 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8427887121007679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8427887121007679 | validation: 0.7318628542723561]
	TIME [epoch: 8.1 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8137147536603249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8137147536603249 | validation: 0.7544510769410954]
	TIME [epoch: 8.09 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.799827727759253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.799827727759253 | validation: 0.7335126300859389]
	TIME [epoch: 8.08 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7919787401137999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7919787401137999 | validation: 0.820805976504987]
	TIME [epoch: 8.08 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8680823291840463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8680823291840463 | validation: 1.2459298563299175]
	TIME [epoch: 8.09 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2374240778092238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2374240778092238 | validation: 0.8466055345374639]
	TIME [epoch: 8.12 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.909481766669038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.909481766669038 | validation: 0.7557394760697461]
	TIME [epoch: 8.09 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8926585862498214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8926585862498214 | validation: 0.7628043463662699]
	TIME [epoch: 8.04 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8353452823861943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8353452823861943 | validation: 0.7505608030679227]
	TIME [epoch: 8.07 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8224921545767575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8224921545767575 | validation: 0.7985901083334737]
	TIME [epoch: 8.05 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9444218812514374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9444218812514374 | validation: 0.7399599026883978]
	TIME [epoch: 8.06 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7831685522829718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7831685522829718 | validation: 0.7160775721245959]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7731456895286966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7731456895286966 | validation: 0.7152586308921267]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8100118355563243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8100118355563243 | validation: 0.8322385745086263]
	TIME [epoch: 8.07 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8417215704676986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8417215704676986 | validation: 0.8257234548324771]
	TIME [epoch: 8.1 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9656736603787561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9656736603787561 | validation: 0.7544835939846283]
	TIME [epoch: 8.12 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8108550874243917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8108550874243917 | validation: 0.8642178027009841]
	TIME [epoch: 8.13 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8669673807217041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8669673807217041 | validation: 0.7900136610720816]
	TIME [epoch: 8.13 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8728781605439038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8728781605439038 | validation: 0.8172781254959429]
	TIME [epoch: 8.12 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8514406809242503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8514406809242503 | validation: 0.8209849791068149]
	TIME [epoch: 8.12 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8355018747569268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8355018747569268 | validation: 0.7288411764059579]
	TIME [epoch: 8.12 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8309171294373704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8309171294373704 | validation: 0.7062781899084092]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7539471657863147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7539471657863147 | validation: 0.7047262449305727]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7226920194040919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7226920194040919 | validation: 0.6693950611903041]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7253711269681904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7253711269681904 | validation: 0.7291156877817994]
	TIME [epoch: 8.1 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.736731396297873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.736731396297873 | validation: 0.7035613540805291]
	TIME [epoch: 8.1 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7937135876503767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7937135876503767 | validation: 0.7551739085365211]
	TIME [epoch: 8.1 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7635768923697098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7635768923697098 | validation: 0.6982513905800976]
	TIME [epoch: 8.11 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8013972258753966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8013972258753966 | validation: 0.8253453854603281]
	TIME [epoch: 8.11 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8163381098169991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8163381098169991 | validation: 0.7889046893622614]
	TIME [epoch: 8.1 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9239401793671793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9239401793671793 | validation: 0.8996624539598905]
	TIME [epoch: 8.1 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9973198559838471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9973198559838471 | validation: 0.9423584503202097]
	TIME [epoch: 8.1 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9560904078056675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9560904078056675 | validation: 0.65858792389283]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7270753075795778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7270753075795778 | validation: 0.7472036172016847]
	TIME [epoch: 8.11 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7942413877047354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7942413877047354 | validation: 0.7347605811546423]
	TIME [epoch: 8.12 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7554737833760973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7554737833760973 | validation: 0.7422070697950206]
	TIME [epoch: 8.09 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7418369349907903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7418369349907903 | validation: 0.6847090726141896]
	TIME [epoch: 8.1 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7778401839070953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7778401839070953 | validation: 0.7276095659150272]
	TIME [epoch: 8.09 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305439620898663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7305439620898663 | validation: 0.6548327939345345]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7235699813856448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7235699813856448 | validation: 0.7158522695251934]
	TIME [epoch: 8.14 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266118857192446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7266118857192446 | validation: 0.7629606547279055]
	TIME [epoch: 8.12 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7953298042740969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7953298042740969 | validation: 0.917926812322882]
	TIME [epoch: 8.13 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8993203783493047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8993203783493047 | validation: 0.6921129574454248]
	TIME [epoch: 8.12 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871416031735774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7871416031735774 | validation: 0.7006279877633044]
	TIME [epoch: 8.14 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7051189766703678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7051189766703678 | validation: 0.6640941467890429]
	TIME [epoch: 8.13 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6764965829413023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6764965829413023 | validation: 0.6334873245736405]
	TIME [epoch: 8.15 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6593805029180553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6593805029180553 | validation: 0.6701473188821157]
	TIME [epoch: 8.08 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6661146870556149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6661146870556149 | validation: 0.6613021869274855]
	TIME [epoch: 8.09 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7373767543552526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7373767543552526 | validation: 0.9817322089394995]
	TIME [epoch: 8.08 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9778655401239914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9778655401239914 | validation: 0.7513894533776733]
	TIME [epoch: 8.1 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8663493986110348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8663493986110348 | validation: 0.643186669888482]
	TIME [epoch: 8.09 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.693844633807201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.693844633807201 | validation: 0.8132860555011883]
	TIME [epoch: 8.1 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7911561855879741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7911561855879741 | validation: 0.6644426106799284]
	TIME [epoch: 8.08 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7519798729362951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7519798729362951 | validation: 0.6354654928107872]
	TIME [epoch: 8.09 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446271467847876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6446271467847876 | validation: 0.6875196695667025]
	TIME [epoch: 8.08 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6709278746619052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6709278746619052 | validation: 0.6246861098326412]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7031100839533353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7031100839533353 | validation: 0.7100931664274482]
	TIME [epoch: 8.11 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7009283616819915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7009283616819915 | validation: 0.8060625990172572]
	TIME [epoch: 8.12 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8158245926717977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8158245926717977 | validation: 0.7192169817771555]
	TIME [epoch: 8.11 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8033927447192839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8033927447192839 | validation: 0.7100802122725128]
	TIME [epoch: 8.11 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7052429738569255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7052429738569255 | validation: 0.6007374221252854]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6651045604040493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6651045604040493 | validation: 0.6466329761464864]
	TIME [epoch: 8.12 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.632453410444686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.632453410444686 | validation: 0.5859101798357316]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6359549149712102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6359549149712102 | validation: 0.6530108157035441]
	TIME [epoch: 8.08 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.633515429668213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.633515429668213 | validation: 0.5903345635261039]
	TIME [epoch: 8.07 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6374173415988081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6374173415988081 | validation: 0.7228517892533516]
	TIME [epoch: 8.07 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6930578685034982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6930578685034982 | validation: 0.7922487896379609]
	TIME [epoch: 8.07 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8906336157465661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8906336157465661 | validation: 0.83291961582613]
	TIME [epoch: 8.09 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8438816375891304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8438816375891304 | validation: 0.638291329030074]
	TIME [epoch: 8.08 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6700924567820542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6700924567820542 | validation: 0.5854077440538065]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.610769739556272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.610769739556272 | validation: 0.6401820202019456]
	TIME [epoch: 8.06 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6457202858740627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6457202858740627 | validation: 0.6377988189812971]
	TIME [epoch: 8.07 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6961550176152878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6961550176152878 | validation: 0.7598600723919495]
	TIME [epoch: 8.07 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7378540677870876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7378540677870876 | validation: 0.6114216991328881]
	TIME [epoch: 8.09 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6694556206664555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6694556206664555 | validation: 0.6023861567220735]
	TIME [epoch: 8.09 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6228650464959291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6228650464959291 | validation: 0.6697868108844217]
	TIME [epoch: 8.08 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6440283009902282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6440283009902282 | validation: 0.5982811338309338]
	TIME [epoch: 8.09 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6730535704349592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6730535704349592 | validation: 0.6501648745125044]
	TIME [epoch: 8.09 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.636212922750149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.636212922750149 | validation: 0.5648669309540978]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6477369664936304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6477369664936304 | validation: 0.7068157287448642]
	TIME [epoch: 8.09 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7167625818921438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7167625818921438 | validation: 0.6347779827726151]
	TIME [epoch: 8.08 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6977565547532392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6977565547532392 | validation: 0.5940257940254489]
	TIME [epoch: 8.08 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5983179078696423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5983179078696423 | validation: 0.5540123526182388]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5626314989143381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5626314989143381 | validation: 0.5362420806820575]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5740537653910736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5740537653910736 | validation: 0.5358024574633569]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5496916401580911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5496916401580911 | validation: 0.5704262474330755]
	TIME [epoch: 8.09 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5749523296698272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5749523296698272 | validation: 0.6045073563442676]
	TIME [epoch: 8.08 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6260694856999862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6260694856999862 | validation: 0.7187644907451054]
	TIME [epoch: 8.07 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7431920945592967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7431920945592967 | validation: 0.6725838023283273]
	TIME [epoch: 8.07 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7593415537661531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7593415537661531 | validation: 0.5269526359342533]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5411289973966081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5411289973966081 | validation: 0.5467417220916808]
	TIME [epoch: 8.1 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.542246563318106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.542246563318106 | validation: 0.5146917788552617]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5781663964774382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5781663964774382 | validation: 0.5328703952303958]
	TIME [epoch: 8.08 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5299643212449217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5299643212449217 | validation: 0.482426038930482]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4993422749012946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4993422749012946 | validation: 0.4809343001944063]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5353854392465525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5353854392465525 | validation: 0.6683764320811909]
	TIME [epoch: 8.09 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6280805914047423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6280805914047423 | validation: 0.5886107025898806]
	TIME [epoch: 8.09 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6360353576778242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6360353576778242 | validation: 0.47078880587436234]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5203230760037247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5203230760037247 | validation: 0.6600098015170347]
	TIME [epoch: 8.08 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6285992077944133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6285992077944133 | validation: 0.652918079018144]
	TIME [epoch: 8.08 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.780866049413071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.780866049413071 | validation: 0.6072988014487051]
	TIME [epoch: 8.06 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5936664430006766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5936664430006766 | validation: 0.47010388824507454]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4953830123515195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4953830123515195 | validation: 0.4491855853926985]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44947810992934034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44947810992934034 | validation: 0.5075269591792564]
	TIME [epoch: 8.06 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4841687645348998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4841687645348998 | validation: 0.44545713323651825]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4834306550825905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4834306550825905 | validation: 0.5126332559597228]
	TIME [epoch: 8.08 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49270977617016765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49270977617016765 | validation: 0.47305045550011526]
	TIME [epoch: 8.08 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5255733233508526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5255733233508526 | validation: 0.4929595271744672]
	TIME [epoch: 8.11 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49411766966881376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49411766966881376 | validation: 0.5623893104997142]
	TIME [epoch: 8.07 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5206578817959573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5206578817959573 | validation: 0.5858717887555798]
	TIME [epoch: 8.08 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6423710042786198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6423710042786198 | validation: 0.42424590058247474]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45203078476397507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45203078476397507 | validation: 0.5777512642724613]
	TIME [epoch: 8.08 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.528592400312465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.528592400312465 | validation: 0.4472633840064278]
	TIME [epoch: 8.1 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4512739436949209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4512739436949209 | validation: 0.5033267360715022]
	TIME [epoch: 8.07 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47022502466894744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47022502466894744 | validation: 0.46423468882166835]
	TIME [epoch: 8.08 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5185353780083116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5185353780083116 | validation: 0.4851009778093035]
	TIME [epoch: 8.06 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48618855308237435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48618855308237435 | validation: 0.44292227644970084]
	TIME [epoch: 8.08 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.443369943158475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.443369943158475 | validation: 0.44340599142335535]
	TIME [epoch: 8.07 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47049990037831474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47049990037831474 | validation: 0.8104657827139259]
	TIME [epoch: 8.1 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6983830205911786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6983830205911786 | validation: 0.44899824642074826]
	TIME [epoch: 8.07 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4109053373226926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4109053373226926 | validation: 0.4294438591051411]
	TIME [epoch: 8.08 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4960130489801668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4960130489801668 | validation: 0.5166228233200237]
	TIME [epoch: 8.07 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.501717568782196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.501717568782196 | validation: 0.5176959210810836]
	TIME [epoch: 8.08 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5464068228803132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5464068228803132 | validation: 0.43485789461025315]
	TIME [epoch: 8.07 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.430679458388986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.430679458388986 | validation: 0.3709129537499317]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36434583595663894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36434583595663894 | validation: 0.36340646578538444]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3491907306516959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3491907306516959 | validation: 0.3469890337677184]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33554673443802124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33554673443802124 | validation: 0.32459300008688796]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32940330643069826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32940330643069826 | validation: 0.40645907317808255]
	TIME [epoch: 8.1 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36634233405362693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36634233405362693 | validation: 0.46706361193584955]
	TIME [epoch: 8.11 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5723362634320907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5723362634320907 | validation: 0.577067309156649]
	TIME [epoch: 8.09 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5072570851225737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5072570851225737 | validation: 0.3223777027036021]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3722062395634608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3722062395634608 | validation: 0.4036384095771439]
	TIME [epoch: 8.08 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3617579629544893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3617579629544893 | validation: 0.4674381177960994]
	TIME [epoch: 8.09 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4503613282136739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4503613282136739 | validation: 0.32563023369594274]
	TIME [epoch: 8.06 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33235079721119015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33235079721119015 | validation: 0.29406376556489716]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27521337345674335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27521337345674335 | validation: 0.2891798839338648]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_344.pth
	Model improved!!!
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2736322580384678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2736322580384678 | validation: 0.3400399935997817]
	TIME [epoch: 8.09 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28515492299078476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28515492299078476 | validation: 0.3416246886055934]
	TIME [epoch: 8.1 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42441133423752564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42441133423752564 | validation: 0.7804043794160416]
	TIME [epoch: 8.09 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657663574465286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657663574465286 | validation: 0.49750212875139077]
	TIME [epoch: 8.12 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47107677848149304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47107677848149304 | validation: 0.5497061829669402]
	TIME [epoch: 8.09 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5783125912231659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5783125912231659 | validation: 0.36804640644538955]
	TIME [epoch: 8.1 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38786749320958125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38786749320958125 | validation: 0.3550541582005662]
	TIME [epoch: 8.09 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3762285912016594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3762285912016594 | validation: 0.3109364663420167]
	TIME [epoch: 8.1 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29370624721266225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29370624721266225 | validation: 0.30436513178024277]
	TIME [epoch: 8.09 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2646143926605883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2646143926605883 | validation: 0.2278252167923569]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24725331236699796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24725331236699796 | validation: 0.26316897562422703]
	TIME [epoch: 8.09 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2373685802899104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2373685802899104 | validation: 0.3238415881101949]
	TIME [epoch: 8.09 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.323588557993196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.323588557993196 | validation: 0.5193049091913658]
	TIME [epoch: 8.1 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.523132922587775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.523132922587775 | validation: 0.4744169073615281]
	TIME [epoch: 8.1 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.632234203152768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.632234203152768 | validation: 0.6204749929239348]
	TIME [epoch: 8.11 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48503891779893515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48503891779893515 | validation: 0.5017314071655811]
	TIME [epoch: 8.12 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4721439183544418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4721439183544418 | validation: 0.26976507953633777]
	TIME [epoch: 8.1 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3036324115464692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3036324115464692 | validation: 0.3074782337968778]
	TIME [epoch: 8.1 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31088749116744413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31088749116744413 | validation: 0.29432560147203707]
	TIME [epoch: 8.09 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2860287171271399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2860287171271399 | validation: 0.26463269842386]
	TIME [epoch: 8.1 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2494467004080827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2494467004080827 | validation: 0.2716691148437235]
	TIME [epoch: 8.1 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24008614117179455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24008614117179455 | validation: 0.2571604823531037]
	TIME [epoch: 8.12 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2733738007006635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2733738007006635 | validation: 0.3697279922385701]
	TIME [epoch: 8.08 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30780484763044674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30780484763044674 | validation: 0.31961859060052433]
	TIME [epoch: 8.08 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38498541570058964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38498541570058964 | validation: 0.4900624012562531]
	TIME [epoch: 8.08 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34154426967533724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34154426967533724 | validation: 0.20977636946164946]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23960016529832953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23960016529832953 | validation: 0.264026343590233]
	TIME [epoch: 8.09 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22115298127284788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22115298127284788 | validation: 0.23277704450551676]
	TIME [epoch: 8.07 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.228481287311905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.228481287311905 | validation: 0.2982216006465326]
	TIME [epoch: 8.07 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26062480620607004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26062480620607004 | validation: 0.4501424742085902]
	TIME [epoch: 8.07 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3974200858000507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3974200858000507 | validation: 0.2746132706984676]
	TIME [epoch: 8.07 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33963805641333544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33963805641333544 | validation: 0.482583376725626]
	TIME [epoch: 8.07 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33082779759147085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33082779759147085 | validation: 0.22405321618288065]
	TIME [epoch: 8.1 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2255352492937556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2255352492937556 | validation: 0.22222801356455513]
	TIME [epoch: 8.08 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21763567771119235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21763567771119235 | validation: 0.2981042673903894]
	TIME [epoch: 8.07 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26146718584685935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26146718584685935 | validation: 0.28954428970278934]
	TIME [epoch: 8.07 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31094060152957326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31094060152957326 | validation: 0.3837967578355983]
	TIME [epoch: 8.07 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3100314598420724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3100314598420724 | validation: 0.22006577374602482]
	TIME [epoch: 8.08 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.282238295243043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.282238295243043 | validation: 0.3944344965726212]
	TIME [epoch: 8.1 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2790004353645202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2790004353645202 | validation: 0.18666316896241553]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21028446777156845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21028446777156845 | validation: 0.1817985928523995]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17425495146815095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17425495146815095 | validation: 0.2178320370577838]
	TIME [epoch: 8.1 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18628122633512273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18628122633512273 | validation: 0.2441566655599047]
	TIME [epoch: 8.11 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27291769902968815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27291769902968815 | validation: 0.24889287139198718]
	TIME [epoch: 8.13 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19872052381094002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19872052381094002 | validation: 0.18706789905829524]
	TIME [epoch: 8.11 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22359248000306559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22359248000306559 | validation: 0.43911055709483415]
	TIME [epoch: 8.11 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2918126581523198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2918126581523198 | validation: 0.2695906126608016]
	TIME [epoch: 8.09 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2935873864690195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2935873864690195 | validation: 0.39782696667806317]
	TIME [epoch: 8.1 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3385594571567762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3385594571567762 | validation: 0.5505018458936427]
	TIME [epoch: 8.1 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.544033251129095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.544033251129095 | validation: 0.24822438707895841]
	TIME [epoch: 8.13 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25505994395889003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25505994395889003 | validation: 0.18775253231175004]
	TIME [epoch: 8.1 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18023756378280703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18023756378280703 | validation: 0.26252064482536924]
	TIME [epoch: 8.11 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2807231590518944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2807231590518944 | validation: 0.6007982896809397]
	TIME [epoch: 8.12 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5086837570782806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5086837570782806 | validation: 0.1897972511631056]
	TIME [epoch: 8.11 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18869112888208506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18869112888208506 | validation: 0.24042681650371125]
	TIME [epoch: 8.11 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29173810069536493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29173810069536493 | validation: 0.3215149350600015]
	TIME [epoch: 8.12 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2550719512670035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2550719512670035 | validation: 0.1497009520752203]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15350428913016356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15350428913016356 | validation: 0.14235695735518827]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16062050273397055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16062050273397055 | validation: 0.1602716435168194]
	TIME [epoch: 8.09 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15920940088189542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15920940088189542 | validation: 0.31704137002997856]
	TIME [epoch: 8.08 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2768392061862953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2768392061862953 | validation: 0.2699625786349868]
	TIME [epoch: 8.1 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2785013701818841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2785013701818841 | validation: 0.26066250893461224]
	TIME [epoch: 8.09 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2349042249769901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2349042249769901 | validation: 0.20417575595536797]
	TIME [epoch: 8.08 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1575071547061322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1575071547061322 | validation: 0.15933939934946714]
	TIME [epoch: 8.08 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19527073690423777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19527073690423777 | validation: 0.48426648430560637]
	TIME [epoch: 8.08 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.310502836421072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.310502836421072 | validation: 0.23333818137625573]
	TIME [epoch: 8.07 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2575483652520042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2575483652520042 | validation: 0.1700489951783639]
	TIME [epoch: 8.1 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1616579117923372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1616579117923372 | validation: 0.14301761347333697]
	TIME [epoch: 8.07 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1422343435723637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1422343435723637 | validation: 0.12057374178876788]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11782826154531517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11782826154531517 | validation: 0.11513576773542065]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11379383119105177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11379383119105177 | validation: 0.19392362556109333]
	TIME [epoch: 8.07 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12539292486345271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12539292486345271 | validation: 0.1387412715562081]
	TIME [epoch: 8.09 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18500097962632914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18500097962632914 | validation: 0.5674638536925002]
	TIME [epoch: 8.12 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40156748823851757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40156748823851757 | validation: 0.6643791051352393]
	TIME [epoch: 8.08 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8346035555409762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8346035555409762 | validation: 0.27582567449303913]
	TIME [epoch: 8.1 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31856012097175196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31856012097175196 | validation: 0.3593622032260375]
	TIME [epoch: 8.08 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27726779705202464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27726779705202464 | validation: 0.2638446127040745]
	TIME [epoch: 8.1 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25787231250179776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25787231250179776 | validation: 0.6190224937613504]
	TIME [epoch: 8.1 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6043640624116722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6043640624116722 | validation: 0.15498289810894753]
	TIME [epoch: 8.1 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14871531585937553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14871531585937553 | validation: 0.31083424942393406]
	TIME [epoch: 8.08 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3468186631201306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3468186631201306 | validation: 0.20938613659175787]
	TIME [epoch: 8.09 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19616288468904408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19616288468904408 | validation: 0.1465441685730223]
	TIME [epoch: 8.08 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13287744832437112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13287744832437112 | validation: 0.12648302636563272]
	TIME [epoch: 8.1 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1305848607574725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1305848607574725 | validation: 0.19189959263389053]
	TIME [epoch: 8.1 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1463340926692462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1463340926692462 | validation: 0.16865990003172668]
	TIME [epoch: 8.1 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2098160914723896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2098160914723896 | validation: 0.43901517074245044]
	TIME [epoch: 8.08 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3376811807824361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3376811807824361 | validation: 0.31576461533959416]
	TIME [epoch: 8.09 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23977404737941194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23977404737941194 | validation: 0.18950084633167702]
	TIME [epoch: 8.08 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24719934900765225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24719934900765225 | validation: 0.2790033423893216]
	TIME [epoch: 8.11 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18682620000505523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18682620000505523 | validation: 0.1795879403024328]
	TIME [epoch: 8.1 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18904427055656947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18904427055656947 | validation: 0.31586888370983135]
	TIME [epoch: 8.1 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2769591014945107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2769591014945107 | validation: 0.2554994742415399]
	TIME [epoch: 8.1 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26474155077226497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26474155077226497 | validation: 0.12938647923005298]
	TIME [epoch: 8.1 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13077082837609236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13077082837609236 | validation: 0.23997584123435658]
	TIME [epoch: 8.1 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18021908845296467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18021908845296467 | validation: 0.2699669585265976]
	TIME [epoch: 8.12 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3051923803536708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3051923803536708 | validation: 0.22054706260019596]
	TIME [epoch: 8.09 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2103325066101313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2103325066101313 | validation: 0.13484128490820108]
	TIME [epoch: 8.09 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12844648093193908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12844648093193908 | validation: 0.120255918998725]
	TIME [epoch: 8.09 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14389078528978808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14389078528978808 | validation: 0.4442421038671661]
	TIME [epoch: 8.09 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3060573207623156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3060573207623156 | validation: 0.15937620799805108]
	TIME [epoch: 8.1 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23690621304295056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23690621304295056 | validation: 0.1776816839127228]
	TIME [epoch: 8.12 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1511729097783634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1511729097783634 | validation: 0.17269589003451524]
	TIME [epoch: 8.09 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17799165487503843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17799165487503843 | validation: 0.2057838991815109]
	TIME [epoch: 8.08 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22389950149605745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22389950149605745 | validation: 0.2374401808358098]
	TIME [epoch: 8.09 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18699098738487366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18699098738487366 | validation: 0.14797542800768035]
	TIME [epoch: 8.09 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14228651735896733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14228651735896733 | validation: 0.17947657303095693]
	TIME [epoch: 8.1 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14926328729625718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14926328729625718 | validation: 0.2515412180692162]
	TIME [epoch: 8.12 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2273827987373349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2273827987373349 | validation: 0.4028304118880727]
	TIME [epoch: 8.09 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44591556617600114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44591556617600114 | validation: 0.23665785394017488]
	TIME [epoch: 8.09 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1600168132872657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1600168132872657 | validation: 0.11893528470278808]
	TIME [epoch: 8.09 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11654869897974193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11654869897974193 | validation: 0.12673855192132008]
	TIME [epoch: 8.1 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10795559853622728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10795559853622728 | validation: 0.15037992346922194]
	TIME [epoch: 8.11 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14223177796708592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14223177796708592 | validation: 0.3413531787014084]
	TIME [epoch: 8.11 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3198859833197756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3198859833197756 | validation: 0.6302133863357983]
	TIME [epoch: 8.09 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9577217261382804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9577217261382804 | validation: 1.1200934213562328]
	TIME [epoch: 8.09 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.078835501004816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.078835501004816 | validation: 0.45542459114649314]
	TIME [epoch: 8.09 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4864960260020584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4864960260020584 | validation: 0.24747606188725946]
	TIME [epoch: 8.1 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26119562762937704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26119562762937704 | validation: 0.8641627849147181]
	TIME [epoch: 8.11 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688863729250204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7688863729250204 | validation: 0.5817833257372581]
	TIME [epoch: 8.1 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47552378769137804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47552378769137804 | validation: 0.3004352214156152]
	TIME [epoch: 8.09 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2858300034558826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2858300034558826 | validation: 0.22057770759806866]
	TIME [epoch: 8.1 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2357581820010066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2357581820010066 | validation: 0.2133571632908131]
	TIME [epoch: 8.09 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2273616503930824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2273616503930824 | validation: 0.26724542281455044]
	TIME [epoch: 8.1 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2468524037184135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2468524037184135 | validation: 0.2255333532449841]
	TIME [epoch: 8.11 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22146042071417063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22146042071417063 | validation: 0.19472459776942508]
	TIME [epoch: 8.1 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19988997740173717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19988997740173717 | validation: 0.18969327951850337]
	TIME [epoch: 8.09 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1744151303130907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1744151303130907 | validation: 0.1630923526416681]
	TIME [epoch: 8.09 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18192099249608212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18192099249608212 | validation: 0.411489802989033]
	TIME [epoch: 8.09 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26657181691384796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26657181691384796 | validation: 0.189847627275406]
	TIME [epoch: 8.1 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1610381413277101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1610381413277101 | validation: 0.16782191797470447]
	TIME [epoch: 8.11 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2210115505145035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2210115505145035 | validation: 0.34565958456797236]
	TIME [epoch: 8.1 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22449149337739818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22449149337739818 | validation: 0.2249796881243384]
	TIME [epoch: 8.09 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20321524435741667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20321524435741667 | validation: 0.1814799920251976]
	TIME [epoch: 8.1 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15951384337463487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15951384337463487 | validation: 0.16200961381090961]
	TIME [epoch: 8.09 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1447622992985095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1447622992985095 | validation: 0.1694346988497113]
	TIME [epoch: 8.11 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15855352061867162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15855352061867162 | validation: 0.1339433606379917]
	TIME [epoch: 8.11 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17580402684090488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17580402684090488 | validation: 0.27392308733121257]
	TIME [epoch: 8.1 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17832263952663296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17832263952663296 | validation: 0.1474856178920912]
	TIME [epoch: 8.09 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1582032455461395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1582032455461395 | validation: 0.19191668529111305]
	TIME [epoch: 8.1 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14274408784801781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14274408784801781 | validation: 0.14271456969990526]
	TIME [epoch: 8.1 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.147390847697808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.147390847697808 | validation: 0.2927236088587745]
	TIME [epoch: 8.12 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2950860336787233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2950860336787233 | validation: 0.19824968615468275]
	TIME [epoch: 8.1 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17476059580398892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17476059580398892 | validation: 0.15345211043044482]
	TIME [epoch: 8.1 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16513143621301488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16513143621301488 | validation: 0.15109178581787688]
	TIME [epoch: 8.09 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13742396186644512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13742396186644512 | validation: 0.20807257348968777]
	TIME [epoch: 8.09 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19143393557333224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19143393557333224 | validation: 0.24678158225774008]
	TIME [epoch: 8.1 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20781052306896147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20781052306896147 | validation: 0.5364549833100193]
	TIME [epoch: 8.12 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6747424071398603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6747424071398603 | validation: 0.6140924171943523]
	TIME [epoch: 8.1 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.553355542090126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.553355542090126 | validation: 0.24119215399878008]
	TIME [epoch: 8.09 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20785387622400095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20785387622400095 | validation: 0.19133853337095252]
	TIME [epoch: 8.1 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1651035836943307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1651035836943307 | validation: 0.23121118577469213]
	TIME [epoch: 8.1 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2271537704413987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2271537704413987 | validation: 0.15831113861379875]
	TIME [epoch: 8.1 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16443408911161705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16443408911161705 | validation: 0.12387002511808252]
	TIME [epoch: 8.11 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11977375795951074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11977375795951074 | validation: 0.11418747444872888]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10900380033324218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10900380033324218 | validation: 0.14169483934948063]
	TIME [epoch: 8.06 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10506693085629035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10506693085629035 | validation: 0.09737190285849835]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1152306023462457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1152306023462457 | validation: 0.24111267287951532]
	TIME [epoch: 57.5 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15444702919162917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15444702919162917 | validation: 0.15658889590410566]
	TIME [epoch: 17.4 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19055170375384348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19055170375384348 | validation: 0.2338274377983877]
	TIME [epoch: 17.4 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14153487775908427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14153487775908427 | validation: 0.10944795821546163]
	TIME [epoch: 17.3 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11674250283092627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11674250283092627 | validation: 0.1655736807271306]
	TIME [epoch: 17.4 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11541459304363713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11541459304363713 | validation: 0.5272899874035778]
	TIME [epoch: 17.4 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44113329479061736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44113329479061736 | validation: 0.18884087951771344]
	TIME [epoch: 17.4 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14130877761719848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14130877761719848 | validation: 0.16487773987831686]
	TIME [epoch: 17.4 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2050315073652809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2050315073652809 | validation: 0.4207218805344095]
	TIME [epoch: 17.4 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2944046670252136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2944046670252136 | validation: 0.12416705594491098]
	TIME [epoch: 17.3 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1575655456020234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1575655456020234 | validation: 0.14856455381466846]
	TIME [epoch: 17.4 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12885453472004924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12885453472004924 | validation: 0.2578727273340726]
	TIME [epoch: 17.4 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16518198158308586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16518198158308586 | validation: 0.14770437151999727]
	TIME [epoch: 17.3 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19450517804531028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19450517804531028 | validation: 0.4174368461031966]
	TIME [epoch: 17.4 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4557122345935173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4557122345935173 | validation: 0.4851208951968059]
	TIME [epoch: 17.4 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37876205011868735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37876205011868735 | validation: 0.17165305849864734]
	TIME [epoch: 17.3 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1878866710881551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1878866710881551 | validation: 0.1579132377510613]
	TIME [epoch: 17.4 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17792840639451704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17792840639451704 | validation: 0.17056226012401293]
	TIME [epoch: 17.3 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15137213343647804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15137213343647804 | validation: 0.11729283253593033]
	TIME [epoch: 17.4 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1206522020630008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1206522020630008 | validation: 0.12676308416301624]
	TIME [epoch: 17.4 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13276784752295387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13276784752295387 | validation: 0.15135276955860852]
	TIME [epoch: 17.4 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14560065784542284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14560065784542284 | validation: 0.20625009416269424]
	TIME [epoch: 17.3 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21468953390963155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21468953390963155 | validation: 0.24273178528985123]
	TIME [epoch: 17.4 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16103531878426963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16103531878426963 | validation: 0.1652274183855026]
	TIME [epoch: 17.3 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18171485625618977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18171485625618977 | validation: 0.16350271737362423]
	TIME [epoch: 17.4 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15503137642384235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15503137642384235 | validation: 0.8184661014228944]
	TIME [epoch: 17.4 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1599325378438443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1599325378438443 | validation: 0.6957019782792934]
	TIME [epoch: 17.4 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9696253704300055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9696253704300055 | validation: 0.36940272416755593]
	TIME [epoch: 17.3 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39702396426698533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39702396426698533 | validation: 0.26966498910332615]
	TIME [epoch: 17.4 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27602988608720114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27602988608720114 | validation: 0.26800877163273057]
	TIME [epoch: 17.3 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2575709782798273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2575709782798273 | validation: 0.3062468105661404]
	TIME [epoch: 17.3 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3333206220185715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3333206220185715 | validation: 0.2939694817414715]
	TIME [epoch: 17.4 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2723285138701832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2723285138701832 | validation: 0.1705488486098057]
	TIME [epoch: 17.4 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1748280677655928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1748280677655928 | validation: 0.2139256364124468]
	TIME [epoch: 17.3 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18771553004790192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18771553004790192 | validation: 0.14016230354731216]
	TIME [epoch: 17.4 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15562127916932167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15562127916932167 | validation: 0.12974946897031137]
	TIME [epoch: 17.4 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14940233990430193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14940233990430193 | validation: 0.2534885783820988]
	TIME [epoch: 17.4 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17817531995829625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17817531995829625 | validation: 0.1436817355612254]
	TIME [epoch: 17.4 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13551390933840282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13551390933840282 | validation: 0.18733036937982664]
	TIME [epoch: 17.4 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15957964698329621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15957964698329621 | validation: 0.13859329170827564]
	TIME [epoch: 17.4 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14094009498430574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14094009498430574 | validation: 0.13309073486984693]
	TIME [epoch: 17.3 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11052639602875643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11052639602875643 | validation: 0.151468750263131]
	TIME [epoch: 17.3 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14492585033755073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14492585033755073 | validation: 0.2642868289169751]
	TIME [epoch: 17.4 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23730702181855726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23730702181855726 | validation: 0.18784367860161805]
	TIME [epoch: 17.4 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1825725218276662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1825725218276662 | validation: 0.13139476999527375]
	TIME [epoch: 17.3 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15403207149563589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15403207149563589 | validation: 0.1267614211342865]
	TIME [epoch: 17.4 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10702533687673597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10702533687673597 | validation: 0.10528819875370249]
	TIME [epoch: 17.3 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11480584802831682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11480584802831682 | validation: 0.2019455520763722]
	TIME [epoch: 17.4 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13419285116716642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13419285116716642 | validation: 0.14776583603649782]
	TIME [epoch: 17.4 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1598284734347553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1598284734347553 | validation: 0.2210871857726101]
	TIME [epoch: 17.3 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16062806906379465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16062806906379465 | validation: 0.1361737338768148]
	TIME [epoch: 17.3 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1552583607216649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1552583607216649 | validation: 0.16742952585844617]
	TIME [epoch: 17.4 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312150570687075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1312150570687075 | validation: 0.1518855937077416]
	TIME [epoch: 17.3 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1462093418102173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1462093418102173 | validation: 0.4890376173578358]
	TIME [epoch: 17.3 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5583088437260587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5583088437260587 | validation: 0.3153774349151098]
	TIME [epoch: 17.4 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2573093562698272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2573093562698272 | validation: 0.6267539014124022]
	TIME [epoch: 17.4 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5883508122310571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5883508122310571 | validation: 0.37754860280520836]
	TIME [epoch: 17.3 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32015726374348286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32015726374348286 | validation: 1.5746967657212378]
	TIME [epoch: 17.4 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8005890563266942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8005890563266942 | validation: 1.4037082973608415]
	TIME [epoch: 17.3 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.717671943621785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.717671943621785 | validation: 0.8130324921863888]
	TIME [epoch: 17.4 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0436288568536736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0436288568536736 | validation: 0.4867045281723817]
	TIME [epoch: 17.4 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5735722838290829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5735722838290829 | validation: 0.45946903673186146]
	TIME [epoch: 17.4 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5381964439016323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5381964439016323 | validation: 0.3467023300453167]
	TIME [epoch: 17.4 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4461564141763967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4461564141763967 | validation: 0.4760420387819322]
	TIME [epoch: 17.4 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40759876289750424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40759876289750424 | validation: 0.23890085246529527]
	TIME [epoch: 17.3 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22125394038481005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22125394038481005 | validation: 0.9400262866182777]
	TIME [epoch: 17.4 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7263594768601698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7263594768601698 | validation: 0.6983209216431038]
	TIME [epoch: 17.4 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5176147583878845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5176147583878845 | validation: 0.4744425129871099]
	TIME [epoch: 17.4 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37086245231312676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37086245231312676 | validation: 0.3843099025567549]
	TIME [epoch: 17.2 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34601972958256594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34601972958256594 | validation: 0.33345067929062466]
	TIME [epoch: 17.3 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30556832652001514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30556832652001514 | validation: 0.2815551221709153]
	TIME [epoch: 17.2 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2562610600467915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2562610600467915 | validation: 0.25335659316966525]
	TIME [epoch: 17.3 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24114470589578058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24114470589578058 | validation: 0.23427040217477046]
	TIME [epoch: 17.2 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22491559089756488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22491559089756488 | validation: 0.23306572304837359]
	TIME [epoch: 17.2 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20586708950614807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20586708950614807 | validation: 0.21128233258314869]
	TIME [epoch: 17.1 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18802263588826407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18802263588826407 | validation: 0.21374685762063717]
	TIME [epoch: 17.1 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17945335130754564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17945335130754564 | validation: 0.20068850920326464]
	TIME [epoch: 17.1 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1646865620328074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1646865620328074 | validation: 0.18361117648981873]
	TIME [epoch: 17.1 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15638384837440236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15638384837440236 | validation: 0.19807314826387687]
	TIME [epoch: 17.1 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.151828099012985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.151828099012985 | validation: 0.24001552959959618]
	TIME [epoch: 17.1 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25190047540060556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25190047540060556 | validation: 0.19806175409198598]
	TIME [epoch: 17.1 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21312584625477546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21312584625477546 | validation: 0.3891882263779067]
	TIME [epoch: 17.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39489170645006794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39489170645006794 | validation: 0.23599753434124998]
	TIME [epoch: 17.1 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22378106632340797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22378106632340797 | validation: 0.17525100786089723]
	TIME [epoch: 17.1 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1968717026857159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1968717026857159 | validation: 0.18788881497446847]
	TIME [epoch: 17.1 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17273993026508896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17273993026508896 | validation: 0.17348250162470347]
	TIME [epoch: 17.2 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16193185757532091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16193185757532091 | validation: 0.11562337266472121]
	TIME [epoch: 17.1 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1492462427973584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1492462427973584 | validation: 0.10623287050541835]
	TIME [epoch: 17.1 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13304605643159223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13304605643159223 | validation: 0.1438644297755057]
	TIME [epoch: 17 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13109632154807008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13109632154807008 | validation: 0.14501898142039812]
	TIME [epoch: 17.1 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13690885710025222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13690885710025222 | validation: 0.14395388558936784]
	TIME [epoch: 17.1 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16459912829397802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16459912829397802 | validation: 0.4749386341818356]
	TIME [epoch: 17.1 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30463772544776446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30463772544776446 | validation: 0.2678805200141555]
	TIME [epoch: 17.1 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26417488634673614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26417488634673614 | validation: 0.2973202037235729]
	TIME [epoch: 17.1 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3256599387272695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3256599387272695 | validation: 0.16749338614988601]
	TIME [epoch: 17.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1615425898583719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1615425898583719 | validation: 0.24725844406874936]
	TIME [epoch: 17.1 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21662541777994265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21662541777994265 | validation: 0.18940209955797782]
	TIME [epoch: 17 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1804000304666328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1804000304666328 | validation: 0.10957884790439931]
	TIME [epoch: 17.2 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11273109389574713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11273109389574713 | validation: 0.11122117426673453]
	TIME [epoch: 17.1 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10111154474053648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10111154474053648 | validation: 0.11184487907885843]
	TIME [epoch: 17.1 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10001876192167934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10001876192167934 | validation: 0.15227316995106088]
	TIME [epoch: 17.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_144956/states/model_phi1_4c_v_mmd1_601.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5113.046 seconds.
