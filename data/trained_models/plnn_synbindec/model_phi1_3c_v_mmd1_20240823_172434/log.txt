Args:
Namespace(name='model_phi1_3c_v_mmd1', outdir='out/model_training/model_phi1_3c_v_mmd1', training_data='data/training_data/data_phi1_3c/training', validation_data='data/training_data/data_phi1_3c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2734571042

Training model...

Saving initial model state to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.425765686341937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.425765686341937 | validation: 4.618279651996521]
	TIME [epoch: 28.3 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.7486759898565865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7486759898565865 | validation: 3.8242463779943976]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.036495456163272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.036495456163272 | validation: 5.439756917026801]
	TIME [epoch: 3.78 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.511693249325345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.511693249325345 | validation: 4.8341660149524035]
	TIME [epoch: 3.78 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.943220302580503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.943220302580503 | validation: 4.361116134333801]
	TIME [epoch: 3.78 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.7000650298755655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7000650298755655 | validation: 3.6651181803770307]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8352627170129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8352627170129 | validation: 3.535435687113831]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7070413960561552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7070413960561552 | validation: 3.142105068966829]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.325362026440857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.325362026440857 | validation: 3.1805895385206497]
	TIME [epoch: 3.78 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4587857153479957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4587857153479957 | validation: 3.0935768156020163]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.30957449162623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.30957449162623 | validation: 3.110347467510664]
	TIME [epoch: 3.78 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3270542793397566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3270542793397566 | validation: 3.060996081039443]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2831648477442834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2831648477442834 | validation: 3.038667204228477]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.27436495271637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.27436495271637 | validation: 3.0424831398375733]
	TIME [epoch: 3.79 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.264828049325832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.264828049325832 | validation: 3.0392263830916955]
	TIME [epoch: 3.78 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2729761088119758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2729761088119758 | validation: 3.0630129253324356]
	TIME [epoch: 3.78 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.278464785745896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.278464785745896 | validation: 3.0558920496811535]
	TIME [epoch: 3.79 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2927590028772684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2927590028772684 | validation: 3.05931358006411]
	TIME [epoch: 3.81 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2729456151061207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2729456151061207 | validation: 3.037439077453499]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2635810880965677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2635810880965677 | validation: 3.0043396728283027]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.223543587953728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.223543587953728 | validation: 2.9843417126794947]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2035836578025907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2035836578025907 | validation: 2.964462213287316]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.191049775176955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.191049775176955 | validation: 2.9867821256261733]
	TIME [epoch: 3.79 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2095624429419853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2095624429419853 | validation: 3.0103759463905675]
	TIME [epoch: 3.79 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2308663924618015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2308663924618015 | validation: 3.0291815350460887]
	TIME [epoch: 3.78 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2769896686608524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2769896686608524 | validation: 2.966454884937409]
	TIME [epoch: 3.78 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1781774252066484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1781774252066484 | validation: 2.9390589967995626]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.139013536669653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.139013536669653 | validation: 2.900283665929869]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.117292806350333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.117292806350333 | validation: 2.8909388979393014]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1081705619640076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1081705619640076 | validation: 2.8761990559488506]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1034678210486244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1034678210486244 | validation: 2.9012061673769605]
	TIME [epoch: 3.81 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1263990321978516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1263990321978516 | validation: 2.954050923931554]
	TIME [epoch: 3.79 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.169289071622451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.169289071622451 | validation: 2.953234272705016]
	TIME [epoch: 3.78 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.235683175677185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.235683175677185 | validation: 2.8240511919201534]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.045080483578683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.045080483578683 | validation: 2.845404226595124]
	TIME [epoch: 3.78 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.069275209299006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.069275209299006 | validation: 2.8023406665347887]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0894402413643616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0894402413643616 | validation: 2.7055533613731066]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.935687854858402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.935687854858402 | validation: 2.70113192983438]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9427087777180523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9427087777180523 | validation: 2.705210868950447]
	TIME [epoch: 3.81 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9345607002091665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9345607002091665 | validation: 2.4754147722721465]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.584905037431551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.584905037431551 | validation: 2.0474274659352]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2638837732140518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2638837732140518 | validation: 1.5872171518540783]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.819863409543449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.819863409543449 | validation: 1.2885299300877913]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4630221868369375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4630221868369375 | validation: 1.4872973610410751]
	TIME [epoch: 3.79 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6301848126860414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6301848126860414 | validation: 1.2278179262512943]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2555238611475434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2555238611475434 | validation: 0.903994391808293]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0620969231852326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0620969231852326 | validation: 1.1238353046787075]
	TIME [epoch: 3.79 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2588080217895772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2588080217895772 | validation: 0.7986870649139339]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8642796398158632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8642796398158632 | validation: 0.8719771335097182]
	TIME [epoch: 3.79 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0695129935521164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0695129935521164 | validation: 0.7059580588894919]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.75895189932375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.75895189932375 | validation: 0.7473926423056647]
	TIME [epoch: 3.78 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7186832859872567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7186832859872567 | validation: 0.7628620282942242]
	TIME [epoch: 3.78 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7069566200369907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7069566200369907 | validation: 0.7330093752461853]
	TIME [epoch: 3.78 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6820496732049094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6820496732049094 | validation: 0.7050808988346322]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7059379092626145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7059379092626145 | validation: 0.8011482346173633]
	TIME [epoch: 3.79 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8645643467126445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8645643467126445 | validation: 0.8272388524909436]
	TIME [epoch: 3.79 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8855658824148549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8855658824148549 | validation: 0.6803554674199868]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7239459272460715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7239459272460715 | validation: 0.7639628618424237]
	TIME [epoch: 3.79 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7352505186212165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7352505186212165 | validation: 0.6970438300665031]
	TIME [epoch: 3.78 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6764726904493631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6764726904493631 | validation: 0.7147055667151525]
	TIME [epoch: 3.78 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6622963879543403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6622963879543403 | validation: 0.7235983564952632]
	TIME [epoch: 3.8 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6727909365413345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6727909365413345 | validation: 0.7314071236364069]
	TIME [epoch: 3.78 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6700555267858704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6700555267858704 | validation: 0.7399973928737904]
	TIME [epoch: 3.8 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6817515191013913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6817515191013913 | validation: 0.7592946983288832]
	TIME [epoch: 3.79 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7419037420301744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7419037420301744 | validation: 0.7569574916204378]
	TIME [epoch: 3.79 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7488487235617503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7488487235617503 | validation: 0.833066323955671]
	TIME [epoch: 3.78 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9067982739673878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9067982739673878 | validation: 0.7032310083601649]
	TIME [epoch: 3.78 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.747016142307557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.747016142307557 | validation: 0.816542159561206]
	TIME [epoch: 3.77 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8112403261101626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8112403261101626 | validation: 0.7841352889405002]
	TIME [epoch: 3.78 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.78662720125482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.78662720125482 | validation: 0.723509352017246]
	TIME [epoch: 3.78 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6979961120867109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6979961120867109 | validation: 0.7541143655478415]
	TIME [epoch: 3.79 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7009601524145128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7009601524145128 | validation: 0.7144468322769989]
	TIME [epoch: 3.79 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699761905976099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6699761905976099 | validation: 0.7324241273033429]
	TIME [epoch: 3.8 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.678450780771369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.678450780771369 | validation: 0.7424281433185321]
	TIME [epoch: 3.79 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6844781270926084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6844781270926084 | validation: 0.7338868546632368]
	TIME [epoch: 3.78 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6608141976218673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6608141976218673 | validation: 0.6993407302692631]
	TIME [epoch: 3.78 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6650427377671133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6650427377671133 | validation: 0.7228331182965958]
	TIME [epoch: 3.78 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6673114975798078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6673114975798078 | validation: 0.7030630465733863]
	TIME [epoch: 3.78 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6870540731087169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6870540731087169 | validation: 0.7327422268387794]
	TIME [epoch: 3.79 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7100729754590657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7100729754590657 | validation: 0.8688706709563206]
	TIME [epoch: 3.78 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888022195496498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8888022195496498 | validation: 0.7809066965511943]
	TIME [epoch: 3.79 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7779298182343343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7779298182343343 | validation: 0.8136301428665942]
	TIME [epoch: 3.78 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8240923460072549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8240923460072549 | validation: 0.7325183982440479]
	TIME [epoch: 3.79 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7284558532603461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7284558532603461 | validation: 0.758282505915261]
	TIME [epoch: 3.79 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7497322968194327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7497322968194327 | validation: 0.708039023211188]
	TIME [epoch: 3.79 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6690778200556176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6690778200556176 | validation: 0.7346026313966467]
	TIME [epoch: 3.79 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066236045790327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066236045790327 | validation: 0.7850927841883405]
	TIME [epoch: 3.78 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7487185926487792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7487185926487792 | validation: 0.7172203408101191]
	TIME [epoch: 3.78 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7206812636200405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7206812636200405 | validation: 0.7419068874835847]
	TIME [epoch: 3.78 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6991134740235271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6991134740235271 | validation: 0.7033730344380311]
	TIME [epoch: 3.78 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6684405818818422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6684405818818422 | validation: 0.7208459556159474]
	TIME [epoch: 3.78 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6659668775653897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6659668775653897 | validation: 0.7157102310262041]
	TIME [epoch: 3.78 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6661081358571193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6661081358571193 | validation: 0.7106777058911997]
	TIME [epoch: 3.77 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592108207869367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6592108207869367 | validation: 0.696684329376549]
	TIME [epoch: 3.78 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6487197703818643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6487197703818643 | validation: 0.690753226178973]
	TIME [epoch: 3.78 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6545167697470171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6545167697470171 | validation: 0.6925698609336095]
	TIME [epoch: 3.8 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6505701033758209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6505701033758209 | validation: 0.6954964595149257]
	TIME [epoch: 3.78 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6550870369539229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6550870369539229 | validation: 0.8613734987510983]
	TIME [epoch: 3.79 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9222837817409375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9222837817409375 | validation: 1.085351577594715]
	TIME [epoch: 3.78 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0933000603800067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0933000603800067 | validation: 0.7768851631214266]
	TIME [epoch: 3.79 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547809222174665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7547809222174665 | validation: 0.8573860545301207]
	TIME [epoch: 3.8 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8531322157344008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8531322157344008 | validation: 0.7153405114328184]
	TIME [epoch: 3.79 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6939266814479166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6939266814479166 | validation: 0.7331954280380736]
	TIME [epoch: 3.79 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7034148424053875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7034148424053875 | validation: 0.709955453929004]
	TIME [epoch: 3.8 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6770710250269676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6770710250269676 | validation: 0.7384639088643508]
	TIME [epoch: 3.79 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6735836787877588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6735836787877588 | validation: 0.7275647210555036]
	TIME [epoch: 3.81 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6920386536530592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6920386536530592 | validation: 0.7242238435836413]
	TIME [epoch: 3.8 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7045839634498617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7045839634498617 | validation: 0.7442399394766318]
	TIME [epoch: 3.81 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6862863242731129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6862863242731129 | validation: 0.704271667828678]
	TIME [epoch: 3.81 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6652909201931505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6652909201931505 | validation: 0.7025487965804214]
	TIME [epoch: 3.8 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6535959976647476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6535959976647476 | validation: 0.7280041227847955]
	TIME [epoch: 3.8 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7154149309977281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7154149309977281 | validation: 0.7058374125013994]
	TIME [epoch: 3.8 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6836921762300971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6836921762300971 | validation: 0.6887954792073965]
	TIME [epoch: 3.79 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6962501675256283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6962501675256283 | validation: 0.708006044129456]
	TIME [epoch: 3.8 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6537021511506694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6537021511506694 | validation: 0.7431788737700494]
	TIME [epoch: 3.81 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8194202604459768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8194202604459768 | validation: 0.8016409141658285]
	TIME [epoch: 3.8 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8330466054579809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8330466054579809 | validation: 0.801880721093453]
	TIME [epoch: 3.79 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015607586351265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8015607586351265 | validation: 0.7495236117143879]
	TIME [epoch: 3.79 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7322232258026924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7322232258026924 | validation: 0.7508958903794609]
	TIME [epoch: 3.8 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7313091041157731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7313091041157731 | validation: 0.69009835235329]
	TIME [epoch: 3.81 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6974399014679321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6974399014679321 | validation: 0.7385828516304528]
	TIME [epoch: 3.8 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7051493265352378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7051493265352378 | validation: 0.7156662620931226]
	TIME [epoch: 3.8 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607619699845038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6607619699845038 | validation: 0.7541360156706198]
	TIME [epoch: 3.81 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7627681390223171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7627681390223171 | validation: 0.7022954791415796]
	TIME [epoch: 3.8 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7107734559607051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7107734559607051 | validation: 0.729770963352028]
	TIME [epoch: 3.79 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7099098016109067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7099098016109067 | validation: 0.6990777831930354]
	TIME [epoch: 3.8 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.671859407132572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.671859407132572 | validation: 0.731245774871913]
	TIME [epoch: 3.81 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6850803366919824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6850803366919824 | validation: 0.7002306606671789]
	TIME [epoch: 3.79 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6800488187749992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6800488187749992 | validation: 0.7202662071961505]
	TIME [epoch: 3.8 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6581026685798368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6581026685798368 | validation: 0.7372237910651723]
	TIME [epoch: 3.79 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403353822487796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7403353822487796 | validation: 0.711051686939654]
	TIME [epoch: 3.79 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7098927177626083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7098927177626083 | validation: 0.6940217454705492]
	TIME [epoch: 3.8 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.67978447579212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.67978447579212 | validation: 0.7708901130327394]
	TIME [epoch: 3.79 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7192417656778216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7192417656778216 | validation: 0.7049379715627465]
	TIME [epoch: 3.81 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7175638005647931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7175638005647931 | validation: 0.6967053618471097]
	TIME [epoch: 3.8 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781707436827753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6781707436827753 | validation: 0.712773552213265]
	TIME [epoch: 3.79 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6698367568640363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6698367568640363 | validation: 0.7029401581714105]
	TIME [epoch: 3.8 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6543054077497399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6543054077497399 | validation: 0.6801825211882614]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524805557992153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6524805557992153 | validation: 0.7068160326913431]
	TIME [epoch: 3.78 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6516677685512113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6516677685512113 | validation: 0.684602645848464]
	TIME [epoch: 3.79 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6572595174822914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6572595174822914 | validation: 0.6968718957463557]
	TIME [epoch: 3.78 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6832410034904984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6832410034904984 | validation: 0.7356344769150124]
	TIME [epoch: 3.8 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7651386292852165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7651386292852165 | validation: 0.8303009268383079]
	TIME [epoch: 3.82 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8237363272864613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8237363272864613 | validation: 0.7777373968447782]
	TIME [epoch: 3.81 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7659395191286847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7659395191286847 | validation: 0.6805918404550335]
	TIME [epoch: 3.81 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6837753936129336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6837753936129336 | validation: 0.691685029985126]
	TIME [epoch: 3.79 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918691755934248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6918691755934248 | validation: 0.7248089715354423]
	TIME [epoch: 3.81 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6613540695082523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6613540695082523 | validation: 0.7134836819541314]
	TIME [epoch: 3.8 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6629144890497635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6629144890497635 | validation: 0.6881886411407437]
	TIME [epoch: 3.8 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6470134918642708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6470134918642708 | validation: 0.7003623643050185]
	TIME [epoch: 3.81 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6444572054028662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6444572054028662 | validation: 0.7092263044650845]
	TIME [epoch: 3.81 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6861465466353136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6861465466353136 | validation: 0.7794679018583841]
	TIME [epoch: 3.78 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.76466337762102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.76466337762102 | validation: 0.708333783451337]
	TIME [epoch: 3.78 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.669075484957516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.669075484957516 | validation: 0.6852162163636439]
	TIME [epoch: 3.79 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.696808543541389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.696808543541389 | validation: 0.7028351375567223]
	TIME [epoch: 3.82 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6585766803703268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6585766803703268 | validation: 0.7425538601505473]
	TIME [epoch: 3.79 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7047662027854696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7047662027854696 | validation: 0.6913288257032705]
	TIME [epoch: 3.79 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6953317670192375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6953317670192375 | validation: 0.6930510977149682]
	TIME [epoch: 3.79 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6597741365108134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6597741365108134 | validation: 0.7180364732008925]
	TIME [epoch: 3.79 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7059115548068976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7059115548068976 | validation: 0.6809344795129832]
	TIME [epoch: 3.8 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6658274987501095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6658274987501095 | validation: 0.6873694111662043]
	TIME [epoch: 3.79 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6543184620906781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6543184620906781 | validation: 0.7186388363358802]
	TIME [epoch: 3.78 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6936538439337712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6936538439337712 | validation: 0.7021642493211897]
	TIME [epoch: 3.79 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.649885957655834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.649885957655834 | validation: 0.6857789405783361]
	TIME [epoch: 3.79 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521072614693244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6521072614693244 | validation: 0.6880031380789773]
	TIME [epoch: 3.79 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6509534364920307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6509534364920307 | validation: 0.7025012001502153]
	TIME [epoch: 3.8 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6597164383599775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6597164383599775 | validation: 0.7404597040595038]
	TIME [epoch: 3.79 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6933602110594465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6933602110594465 | validation: 0.7572902087981161]
	TIME [epoch: 3.78 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7319718917413612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7319718917413612 | validation: 0.6921692656928008]
	TIME [epoch: 3.79 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6537968071226965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6537968071226965 | validation: 0.6734300026503833]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346655876425001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6346655876425001 | validation: 0.6863190671861139]
	TIME [epoch: 3.81 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6858752297811563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6858752297811563 | validation: 0.6781895768381763]
	TIME [epoch: 3.79 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.668874204086693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.668874204086693 | validation: 0.6854514219737116]
	TIME [epoch: 3.79 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6424252262611307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6424252262611307 | validation: 0.6830623324645908]
	TIME [epoch: 3.79 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6530614764202948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6530614764202948 | validation: 0.6914665427404056]
	TIME [epoch: 3.79 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7099484504649458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7099484504649458 | validation: 0.6953827874222833]
	TIME [epoch: 3.79 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427118372587346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6427118372587346 | validation: 0.68665560675093]
	TIME [epoch: 3.79 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6432555295960682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6432555295960682 | validation: 0.7258530782116758]
	TIME [epoch: 3.8 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7503223207104468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7503223207104468 | validation: 0.8282929613753659]
	TIME [epoch: 3.8 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963849959103549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7963849959103549 | validation: 0.7155965877666453]
	TIME [epoch: 3.81 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6946165205492342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6946165205492342 | validation: 0.7271865996330544]
	TIME [epoch: 3.81 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7220913052044097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7220913052044097 | validation: 0.6847405977724915]
	TIME [epoch: 3.81 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6934320332114902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6934320332114902 | validation: 0.7181548256508409]
	TIME [epoch: 3.81 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830542770314062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6830542770314062 | validation: 0.6997264411334623]
	TIME [epoch: 3.8 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6534891737135936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6534891737135936 | validation: 0.6798627378591909]
	TIME [epoch: 3.8 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6389376659266394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6389376659266394 | validation: 0.6894817067462091]
	TIME [epoch: 3.8 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6398563319295103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6398563319295103 | validation: 0.689497561141472]
	TIME [epoch: 3.8 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6643642683175236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6643642683175236 | validation: 0.7008084133181728]
	TIME [epoch: 3.81 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6457698923581362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6457698923581362 | validation: 0.682919334007678]
	TIME [epoch: 3.83 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6400357548830912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6400357548830912 | validation: 0.7039666280502972]
	TIME [epoch: 3.82 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6489424917663226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6489424917663226 | validation: 0.705873222631031]
	TIME [epoch: 3.8 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.697550477269209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.697550477269209 | validation: 0.7877731335241158]
	TIME [epoch: 3.8 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7378653363050448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7378653363050448 | validation: 0.7983281277365163]
	TIME [epoch: 3.81 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8272007235919194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8272007235919194 | validation: 0.6892530096275026]
	TIME [epoch: 3.8 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7184885194256666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7184885194256666 | validation: 0.7886727009173096]
	TIME [epoch: 3.8 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789164177402531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.789164177402531 | validation: 0.6965472372783665]
	TIME [epoch: 3.81 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6729389388374378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6729389388374378 | validation: 0.6994342524701258]
	TIME [epoch: 3.81 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6622871802323056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6622871802323056 | validation: 0.7121387480856038]
	TIME [epoch: 3.8 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6529519553974156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6529519553974156 | validation: 0.679631948100982]
	TIME [epoch: 3.79 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6412618989718906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6412618989718906 | validation: 0.6921539136072781]
	TIME [epoch: 3.81 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6363539644748538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6363539644748538 | validation: 0.6768540033797527]
	TIME [epoch: 30.8 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6345226691046746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6345226691046746 | validation: 0.6849878220044021]
	TIME [epoch: 8.25 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6354136092801789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6354136092801789 | validation: 0.6632732970213288]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6462022031299036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6462022031299036 | validation: 0.6846291319009772]
	TIME [epoch: 8.25 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6389636345576178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6389636345576178 | validation: 0.6760275168319636]
	TIME [epoch: 8.29 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650349413786906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.650349413786906 | validation: 0.6916433599402121]
	TIME [epoch: 8.26 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6782812666048228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6782812666048228 | validation: 0.6808148466892827]
	TIME [epoch: 8.25 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7088940118636566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7088940118636566 | validation: 0.6833687640783863]
	TIME [epoch: 8.27 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6449618917263095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6449618917263095 | validation: 0.6953164903057849]
	TIME [epoch: 8.26 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6583293852409043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6583293852409043 | validation: 0.696698967732233]
	TIME [epoch: 8.27 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6642342533125937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6642342533125937 | validation: 0.7315908675026469]
	TIME [epoch: 8.27 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7006359777722304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7006359777722304 | validation: 0.750655747954244]
	TIME [epoch: 8.26 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6979784600603651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6979784600603651 | validation: 0.6807722921045749]
	TIME [epoch: 8.25 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6820272440353551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6820272440353551 | validation: 0.6816115709283888]
	TIME [epoch: 8.27 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6243638169443755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6243638169443755 | validation: 0.6814685111831719]
	TIME [epoch: 8.26 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6434703874328355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6434703874328355 | validation: 0.6666316363395048]
	TIME [epoch: 8.27 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6366260850099379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6366260850099379 | validation: 0.682265954968719]
	TIME [epoch: 8.27 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332987893569323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6332987893569323 | validation: 0.6844343821028792]
	TIME [epoch: 8.28 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607789946504926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6607789946504926 | validation: 0.7010051511354949]
	TIME [epoch: 8.25 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6621595770138253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6621595770138253 | validation: 0.7191084225463675]
	TIME [epoch: 8.27 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.722196470789292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.722196470789292 | validation: 0.6661700848401811]
	TIME [epoch: 8.27 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6376375117005902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6376375117005902 | validation: 0.6833368311433602]
	TIME [epoch: 8.24 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.64655579654279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.64655579654279 | validation: 0.7305306497100738]
	TIME [epoch: 8.25 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6994091897902545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6994091897902545 | validation: 0.7097345677312714]
	TIME [epoch: 8.25 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6620497100658199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6620497100658199 | validation: 0.7005652640882899]
	TIME [epoch: 8.28 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439899737921075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7439899737921075 | validation: 0.6637347353370937]
	TIME [epoch: 8.26 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6341369962330358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6341369962330358 | validation: 0.7242038439535965]
	TIME [epoch: 8.27 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7188071216642963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7188071216642963 | validation: 0.678815042299631]
	TIME [epoch: 8.27 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7136005081432807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7136005081432807 | validation: 0.7147052185315855]
	TIME [epoch: 8.28 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003794493365885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7003794493365885 | validation: 0.7159460710321283]
	TIME [epoch: 8.25 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7002397159875499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7002397159875499 | validation: 0.6805317035454542]
	TIME [epoch: 8.26 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6444548546793659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6444548546793659 | validation: 0.6899614289955661]
	TIME [epoch: 8.28 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6562663206115323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6562663206115323 | validation: 0.6691626692586135]
	TIME [epoch: 8.26 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6456477249017303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6456477249017303 | validation: 0.7109925822145453]
	TIME [epoch: 8.25 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6572741746773226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6572741746773226 | validation: 0.6667482337605656]
	TIME [epoch: 8.26 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.653293403621075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.653293403621075 | validation: 0.6949687062752761]
	TIME [epoch: 8.26 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6303321869588114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6303321869588114 | validation: 0.6945748480256588]
	TIME [epoch: 8.29 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6517670548974036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6517670548974036 | validation: 0.6880227387176769]
	TIME [epoch: 8.26 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6274055418693726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6274055418693726 | validation: 0.6565251850698776]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.640655385232435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.640655385232435 | validation: 0.6969411726131213]
	TIME [epoch: 8.24 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6437190119172462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6437190119172462 | validation: 0.7567908891275179]
	TIME [epoch: 8.27 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7447608250324049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7447608250324049 | validation: 0.7169738451643443]
	TIME [epoch: 8.27 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6616197821150742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6616197821150742 | validation: 0.6483895479251023]
	TIME [epoch: 8.28 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6602024581088365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6602024581088365 | validation: 0.666930603508722]
	TIME [epoch: 8.55 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6227250063486209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6227250063486209 | validation: 0.6471187535323186]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6091011342474143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6091011342474143 | validation: 0.6540489243141203]
	TIME [epoch: 8.26 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6212766704768702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6212766704768702 | validation: 0.6542262884135788]
	TIME [epoch: 8.26 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6163702991791753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6163702991791753 | validation: 0.6590698051832511]
	TIME [epoch: 8.24 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6391701110520763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6391701110520763 | validation: 0.6638344233874011]
	TIME [epoch: 8.29 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6375789531310416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6375789531310416 | validation: 0.6681856363350156]
	TIME [epoch: 8.28 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.619277492004862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.619277492004862 | validation: 0.6588880835418779]
	TIME [epoch: 8.26 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6410858692608211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6410858692608211 | validation: 0.813971373699002]
	TIME [epoch: 8.25 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.87757705896924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.87757705896924 | validation: 0.73121880371707]
	TIME [epoch: 8.27 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8100827947696496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8100827947696496 | validation: 0.6992726727913215]
	TIME [epoch: 8.28 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7331713377635554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7331713377635554 | validation: 0.6824909153685403]
	TIME [epoch: 8.28 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6358417415401274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6358417415401274 | validation: 0.7144594029567255]
	TIME [epoch: 8.27 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6686836016653974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6686836016653974 | validation: 0.6409372561368637]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6386448180053332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6386448180053332 | validation: 0.6986949374992245]
	TIME [epoch: 8.24 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6289123493576895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6289123493576895 | validation: 0.6629389017354566]
	TIME [epoch: 8.28 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6181673407188941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6181673407188941 | validation: 0.6848565541397174]
	TIME [epoch: 8.27 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6034688986238693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6034688986238693 | validation: 0.6486039495862145]
	TIME [epoch: 8.24 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6057043021479578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6057043021479578 | validation: 0.6880769618063859]
	TIME [epoch: 8.27 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6151105126963219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6151105126963219 | validation: 0.6813838160952743]
	TIME [epoch: 8.27 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6758407678009423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6758407678009423 | validation: 0.7710980756372666]
	TIME [epoch: 8.25 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7134459157388762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7134459157388762 | validation: 0.6999903453076457]
	TIME [epoch: 8.28 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6949283670627736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6949283670627736 | validation: 0.6635301642154728]
	TIME [epoch: 8.25 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6065066489523473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6065066489523473 | validation: 0.6389073758892115]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5873835094634704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5873835094634704 | validation: 0.6366038333389243]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5756642421021421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5756642421021421 | validation: 0.6312622105858426]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5764812958227884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5764812958227884 | validation: 0.6210710029720993]
	TIME [epoch: 8.29 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5845377360547371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5845377360547371 | validation: 0.7317852575818699]
	TIME [epoch: 8.26 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6646058946671144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6646058946671144 | validation: 0.8542747811225295]
	TIME [epoch: 8.25 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9527653464763252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9527653464763252 | validation: 0.6927236821029104]
	TIME [epoch: 8.24 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7061888070136593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7061888070136593 | validation: 0.658798329287826]
	TIME [epoch: 8.24 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6067070460559557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6067070460559557 | validation: 0.630288672350444]
	TIME [epoch: 8.27 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6204906166193892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6204906166193892 | validation: 0.6769084322180996]
	TIME [epoch: 8.27 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6137908007482582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6137908007482582 | validation: 0.6034209418790676]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5932002081983528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5932002081983528 | validation: 0.6187377495043038]
	TIME [epoch: 8.25 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5580329055211403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5580329055211403 | validation: 0.5845057669538877]
	TIME [epoch: 8.29 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5438708796822705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5438708796822705 | validation: 0.6274671936753555]
	TIME [epoch: 8.23 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5372403752098924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5372403752098924 | validation: 0.6086551205369685]
	TIME [epoch: 8.25 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5842925637489406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5842925637489406 | validation: 0.8408379461036031]
	TIME [epoch: 8.24 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7922254058304995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7922254058304995 | validation: 0.8024020725345828]
	TIME [epoch: 8.24 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8930451331452655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8930451331452655 | validation: 0.6404367062643626]
	TIME [epoch: 8.24 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5910327063161104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5910327063161104 | validation: 0.5957676940743049]
	TIME [epoch: 8.24 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5577266914135176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5577266914135176 | validation: 0.5816853449534305]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5718170320658066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5718170320658066 | validation: 0.6124523170587959]
	TIME [epoch: 8.25 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5503613377589557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5503613377589557 | validation: 0.5819781992696955]
	TIME [epoch: 8.24 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.526980934017492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.526980934017492 | validation: 0.5700395682162059]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5103614633738927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5103614633738927 | validation: 0.5734894112283957]
	TIME [epoch: 8.23 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5044726434565281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5044726434565281 | validation: 0.5502415781700458]
	TIME [epoch: 8.27 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5038747919316955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5038747919316955 | validation: 0.5982877721039584]
	TIME [epoch: 8.24 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5057793816683996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5057793816683996 | validation: 0.6352602301767082]
	TIME [epoch: 8.23 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6999802982112939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6999802982112939 | validation: 0.8978554202492852]
	TIME [epoch: 8.24 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0348565223359356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0348565223359356 | validation: 0.548395459261546]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5429510530658034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5429510530658034 | validation: 0.6176744456842095]
	TIME [epoch: 8.22 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.653058824690058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.653058824690058 | validation: 0.5830845345912268]
	TIME [epoch: 8.25 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5375729270164163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5375729270164163 | validation: 0.5424743155809679]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4923710869570374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4923710869570374 | validation: 0.5340613507889886]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47499355468619786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47499355468619786 | validation: 0.6057203026669596]
	TIME [epoch: 8.21 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49297877578756955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49297877578756955 | validation: 0.6336523427185861]
	TIME [epoch: 8.26 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6683225071883919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6683225071883919 | validation: 0.8223430269801391]
	TIME [epoch: 8.27 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9277714020812872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9277714020812872 | validation: 0.5378045966737928]
	TIME [epoch: 8.28 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49542347914985163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49542347914985163 | validation: 0.5379909638759525]
	TIME [epoch: 8.25 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5523909008082922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5523909008082922 | validation: 0.6134596481062948]
	TIME [epoch: 8.27 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5308520053357617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5308520053357617 | validation: 0.48859245413707486]
	TIME [epoch: 8.27 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4545832720695813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4545832720695813 | validation: 0.48275939074914387]
	TIME [epoch: 8.27 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4383105725525782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4383105725525782 | validation: 0.4987252022553308]
	TIME [epoch: 8.28 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4330396795302904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4330396795302904 | validation: 0.4988716277254739]
	TIME [epoch: 8.26 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48167711475598374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48167711475598374 | validation: 0.7320863306488006]
	TIME [epoch: 8.25 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439570678259938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7439570678259938 | validation: 0.4774809023812321]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49613159307105975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49613159307105975 | validation: 0.4401338078198343]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4393421331631396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4393421331631396 | validation: 0.4808664606750423]
	TIME [epoch: 8.24 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42563666375871306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42563666375871306 | validation: 0.48876101867744187]
	TIME [epoch: 8.23 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41538981621446086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41538981621446086 | validation: 0.45877394152951734]
	TIME [epoch: 8.22 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4278192235988651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4278192235988651 | validation: 0.6231518695391219]
	TIME [epoch: 8.23 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6206628846499418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6206628846499418 | validation: 0.5788013259358024]
	TIME [epoch: 8.23 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5898728085622118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5898728085622118 | validation: 0.5665888438242]
	TIME [epoch: 8.24 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6101225112683493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6101225112683493 | validation: 0.4330118651736438]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44826549958159706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44826549958159706 | validation: 0.4244876128019954]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3845999626186207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3845999626186207 | validation: 0.5302133976490947]
	TIME [epoch: 8.22 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.456206767734743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.456206767734743 | validation: 0.5569565934141777]
	TIME [epoch: 8.25 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5497099985915243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5497099985915243 | validation: 0.6812181235309347]
	TIME [epoch: 8.25 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8295322111552855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8295322111552855 | validation: 0.5193647262887691]
	TIME [epoch: 8.24 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4616650003577379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4616650003577379 | validation: 0.5039558315562148]
	TIME [epoch: 8.24 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44886595218004033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44886595218004033 | validation: 0.5473039695221171]
	TIME [epoch: 8.23 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45970506053297594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45970506053297594 | validation: 0.5087921286668143]
	TIME [epoch: 8.23 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4488826981713177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4488826981713177 | validation: 0.5511091351124933]
	TIME [epoch: 8.22 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5122489905083295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5122489905083295 | validation: 0.4342112566505469]
	TIME [epoch: 8.23 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4120735449111016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4120735449111016 | validation: 0.40725628941703307]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3652818352522265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3652818352522265 | validation: 0.4231019000985759]
	TIME [epoch: 8.22 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34319782219663725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34319782219663725 | validation: 0.4260404237138893]
	TIME [epoch: 8.24 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3492903743729525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3492903743729525 | validation: 0.42226284202698716]
	TIME [epoch: 8.24 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3915966229726439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3915966229726439 | validation: 0.499065470816477]
	TIME [epoch: 8.25 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4446021791221083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4446021791221083 | validation: 0.5420616521118241]
	TIME [epoch: 8.25 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.587532396801488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.587532396801488 | validation: 0.4307119846080677]
	TIME [epoch: 8.24 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3935376903994703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3935376903994703 | validation: 0.36315214409856766]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3203110571272939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3203110571272939 | validation: 0.4001557606465861]
	TIME [epoch: 8.26 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3096756779997312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3096756779997312 | validation: 0.3497412069547102]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2963060954763859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2963060954763859 | validation: 0.356868335619668]
	TIME [epoch: 8.24 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2948654241523487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2948654241523487 | validation: 0.3416666688203604]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29851544713963596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29851544713963596 | validation: 0.5203108249940506]
	TIME [epoch: 8.21 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44245031309174493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44245031309174493 | validation: 0.7234361794969666]
	TIME [epoch: 8.24 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0307407274956701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0307407274956701 | validation: 0.5907773325859352]
	TIME [epoch: 8.23 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9586082162376554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9586082162376554 | validation: 0.9131675737472232]
	TIME [epoch: 8.24 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0112871903191758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0112871903191758 | validation: 0.7481636901037053]
	TIME [epoch: 8.25 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7297108937019569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7297108937019569 | validation: 0.44289476400550964]
	TIME [epoch: 8.23 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38302382112721917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38302382112721917 | validation: 0.3531882204695537]
	TIME [epoch: 8.22 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32510089747871435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32510089747871435 | validation: 0.393482716828853]
	TIME [epoch: 8.22 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31830336473374404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31830336473374404 | validation: 0.3492618677095798]
	TIME [epoch: 8.25 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3112375723692202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3112375723692202 | validation: 0.43138887949875077]
	TIME [epoch: 8.27 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4098202203531913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4098202203531913 | validation: 0.4962004689584924]
	TIME [epoch: 8.23 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5075004124386046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5075004124386046 | validation: 0.37683000455425886]
	TIME [epoch: 8.23 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3056513274163746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3056513274163746 | validation: 0.3792003675776871]
	TIME [epoch: 8.21 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27740079006631885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27740079006631885 | validation: 0.4150994743285762]
	TIME [epoch: 8.21 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3733959419077229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3733959419077229 | validation: 0.5343317405939855]
	TIME [epoch: 8.22 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4469905261929279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4469905261929279 | validation: 0.44324389975294737]
	TIME [epoch: 8.22 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5551466554435082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5551466554435082 | validation: 0.31925865876266823]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3871334547278588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3871334547278588 | validation: 0.455794133528608]
	TIME [epoch: 8.21 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37395546080251313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37395546080251313 | validation: 0.4277138808852434]
	TIME [epoch: 8.21 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36118837568534806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36118837568534806 | validation: 0.30310430506375]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24812621519517947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24812621519517947 | validation: 0.3343270353951773]
	TIME [epoch: 8.25 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24402532641926697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24402532641926697 | validation: 0.3255072818963407]
	TIME [epoch: 8.24 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27104319824785444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27104319824785444 | validation: 0.4424666147136478]
	TIME [epoch: 8.23 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33253671759815717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33253671759815717 | validation: 0.4498607556942929]
	TIME [epoch: 8.23 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5211782702286595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5211782702286595 | validation: 0.38577569745463464]
	TIME [epoch: 8.22 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3941099878132159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3941099878132159 | validation: 0.3687558495278409]
	TIME [epoch: 8.24 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32919538507487645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32919538507487645 | validation: 0.3663196140625905]
	TIME [epoch: 8.24 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23821797355296884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23821797355296884 | validation: 0.3231791307745804]
	TIME [epoch: 8.22 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21598388888581255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21598388888581255 | validation: 0.3546031002785335]
	TIME [epoch: 8.22 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2689445466185728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2689445466185728 | validation: 0.42636112662816467]
	TIME [epoch: 8.23 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.422418335790948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.422418335790948 | validation: 0.42018081323351847]
	TIME [epoch: 8.24 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2979158037749411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2979158037749411 | validation: 0.2937038318375933]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2558524957369654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2558524957369654 | validation: 0.3642398718525592]
	TIME [epoch: 8.24 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22854212589488157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22854212589488157 | validation: 0.2574549535889425]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_375.pth
	Model improved!!!
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2625828150190028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2625828150190028 | validation: 0.33623441228276585]
	TIME [epoch: 8.23 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2327220553605012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2327220553605012 | validation: 0.4461820815436468]
	TIME [epoch: 8.23 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3627024833750656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3627024833750656 | validation: 0.6338534208807095]
	TIME [epoch: 8.25 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4901883287035097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4901883287035097 | validation: 0.42949101321072886]
	TIME [epoch: 8.25 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6462560889001563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6462560889001563 | validation: 0.47590664191342313]
	TIME [epoch: 8.23 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6842565096240827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6842565096240827 | validation: 0.7772519389254694]
	TIME [epoch: 8.23 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6903518729681537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6903518729681537 | validation: 0.4371312711242099]
	TIME [epoch: 8.24 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30349665777565654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30349665777565654 | validation: 0.40547582598643256]
	TIME [epoch: 8.24 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3678788674750452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3678788674750452 | validation: 0.3149445755463243]
	TIME [epoch: 8.25 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2599145244211964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2599145244211964 | validation: 0.2862924941249289]
	TIME [epoch: 8.23 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2052852826362704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2052852826362704 | validation: 0.3015187992087358]
	TIME [epoch: 8.24 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19560719200226787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19560719200226787 | validation: 0.26863197243511355]
	TIME [epoch: 8.23 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18515775674025065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18515775674025065 | validation: 0.2563098521938452]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20866986205386476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20866986205386476 | validation: 0.9442805097265317]
	TIME [epoch: 8.24 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2076209377438478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2076209377438478 | validation: 0.9394268181243588]
	TIME [epoch: 8.24 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791186433870902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.791186433870902 | validation: 0.31917991263740664]
	TIME [epoch: 8.22 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3319207218089333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3319207218089333 | validation: 0.35783210420562517]
	TIME [epoch: 8.23 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37323979647563155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37323979647563155 | validation: 0.23345768820330967]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20177517013212018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20177517013212018 | validation: 0.3065976941400068]
	TIME [epoch: 8.23 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2076821115584321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2076821115584321 | validation: 0.2827097806570774]
	TIME [epoch: 8.25 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19563651241138658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19563651241138658 | validation: 0.28522268670110257]
	TIME [epoch: 8.27 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24392996350277676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24392996350277676 | validation: 0.2649378671190728]
	TIME [epoch: 8.23 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20370575178014996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20370575178014996 | validation: 0.25546931890619823]
	TIME [epoch: 8.24 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17687804364466622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17687804364466622 | validation: 0.2918713640873434]
	TIME [epoch: 8.22 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18219771832600684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18219771832600684 | validation: 0.29100076169065936]
	TIME [epoch: 8.24 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23017412559873657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23017412559873657 | validation: 0.43661546007881696]
	TIME [epoch: 8.24 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32735154896739815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32735154896739815 | validation: 0.3643733318618553]
	TIME [epoch: 8.23 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4139464685419427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4139464685419427 | validation: 0.25881363388245754]
	TIME [epoch: 8.23 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25475754047647065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25475754047647065 | validation: 0.25486451171325203]
	TIME [epoch: 8.22 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18771559264033794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18771559264033794 | validation: 0.2750543023437584]
	TIME [epoch: 8.23 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1815873146154487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1815873146154487 | validation: 0.2546968151792463]
	TIME [epoch: 8.24 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15865511687243694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15865511687243694 | validation: 0.22647088821387334]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19615852338028192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19615852338028192 | validation: 0.41659614415161106]
	TIME [epoch: 8.23 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31825460998905825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31825460998905825 | validation: 0.4295880915604192]
	TIME [epoch: 8.23 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4817898328173789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4817898328173789 | validation: 0.3945146369317694]
	TIME [epoch: 8.23 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37110639275732865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37110639275732865 | validation: 0.851429643737523]
	TIME [epoch: 8.25 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7714305647068901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7714305647068901 | validation: 0.24270896017297422]
	TIME [epoch: 8.22 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36823504966466636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36823504966466636 | validation: 0.4908948940822441]
	TIME [epoch: 8.23 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5001562790485499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5001562790485499 | validation: 0.24662455197549013]
	TIME [epoch: 8.23 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24176064875801584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24176064875801584 | validation: 0.20921295606117904]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17463210771306234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17463210771306234 | validation: 0.2975981934709932]
	TIME [epoch: 8.22 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.225059799611325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.225059799611325 | validation: 0.20193328682516165]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_417.pth
	Model improved!!!
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16848014475394693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16848014475394693 | validation: 0.20111537972678814]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1700809046429885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1700809046429885 | validation: 0.2300985423436838]
	TIME [epoch: 8.22 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15354927240649777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15354927240649777 | validation: 0.2185055236974888]
	TIME [epoch: 8.22 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20879543330903536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20879543330903536 | validation: 0.25609079794403206]
	TIME [epoch: 8.22 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17289707381678554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17289707381678554 | validation: 0.2334260044658211]
	TIME [epoch: 8.23 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28625135413812886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28625135413812886 | validation: 0.304852014821887]
	TIME [epoch: 8.2 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23807119719209927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23807119719209927 | validation: 0.24555280718563416]
	TIME [epoch: 8.22 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3381496846803792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3381496846803792 | validation: 0.23719255879337517]
	TIME [epoch: 8.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2845228123201752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2845228123201752 | validation: 0.22822391234599393]
	TIME [epoch: 8.21 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13899071208863265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13899071208863265 | validation: 0.2902036184172753]
	TIME [epoch: 8.2 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1975463112852065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1975463112852065 | validation: 0.2449797862752409]
	TIME [epoch: 8.23 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3017294417003241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3017294417003241 | validation: 0.3000114537210632]
	TIME [epoch: 8.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31393899326169705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31393899326169705 | validation: 0.22655587640663233]
	TIME [epoch: 8.21 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14888892587766722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14888892587766722 | validation: 0.39131244118930186]
	TIME [epoch: 8.2 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3063434794515534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3063434794515534 | validation: 1.0459976141122596]
	TIME [epoch: 8.22 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154470137957979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.154470137957979 | validation: 0.682299121831721]
	TIME [epoch: 8.22 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9509298783396524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9509298783396524 | validation: 0.7379232775486244]
	TIME [epoch: 8.22 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6889789666725343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6889789666725343 | validation: 0.44530697425608107]
	TIME [epoch: 8.21 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3596369180378943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3596369180378943 | validation: 0.4161839790708882]
	TIME [epoch: 8.22 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4403089124732739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4403089124732739 | validation: 0.3060038320676887]
	TIME [epoch: 8.21 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3305287765708178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3305287765708178 | validation: 0.2711343896686766]
	TIME [epoch: 8.23 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2133428450302597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2133428450302597 | validation: 0.2686795101791862]
	TIME [epoch: 8.22 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16721328697281201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16721328697281201 | validation: 0.23824666435670735]
	TIME [epoch: 8.21 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16662751074446483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16662751074446483 | validation: 0.26818779248082214]
	TIME [epoch: 8.2 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18300934350095438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18300934350095438 | validation: 0.32590187725443365]
	TIME [epoch: 8.21 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3325743926622198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3325743926622198 | validation: 0.2611785150255177]
	TIME [epoch: 8.21 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3257593901281375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3257593901281375 | validation: 0.21801973350628223]
	TIME [epoch: 8.21 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2332952567765435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2332952567765435 | validation: 0.3788770116688607]
	TIME [epoch: 8.21 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.266970700049725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.266970700049725 | validation: 0.5968258887501261]
	TIME [epoch: 8.2 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6182944319797627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6182944319797627 | validation: 1.1818161382600636]
	TIME [epoch: 8.21 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.669987257634804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.669987257634804 | validation: 0.9013563444366688]
	TIME [epoch: 8.2 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3702613087269035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3702613087269035 | validation: 0.6030003848905947]
	TIME [epoch: 8.2 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5658605301076503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5658605301076503 | validation: 0.44684274389803846]
	TIME [epoch: 8.22 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3720480875593976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3720480875593976 | validation: 0.321001736511888]
	TIME [epoch: 8.21 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32066448091277266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32066448091277266 | validation: 0.2827310133174737]
	TIME [epoch: 8.2 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2609708366679049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2609708366679049 | validation: 0.3343866439443852]
	TIME [epoch: 8.21 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21676362427839735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21676362427839735 | validation: 0.28920671267465886]
	TIME [epoch: 8.21 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18503062764171027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18503062764171027 | validation: 0.25789578763180665]
	TIME [epoch: 8.23 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1615583003173699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1615583003173699 | validation: 0.23152640832434546]
	TIME [epoch: 8.2 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15309570197995806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15309570197995806 | validation: 0.22163091547341995]
	TIME [epoch: 8.21 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16051499584348095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16051499584348095 | validation: 0.21918917005283833]
	TIME [epoch: 8.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17104337323552385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17104337323552385 | validation: 0.23225242289443115]
	TIME [epoch: 8.21 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17029300073499606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17029300073499606 | validation: 0.22286174488815202]
	TIME [epoch: 8.22 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18128795023133926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18128795023133926 | validation: 0.19954477879583862]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_461.pth
	Model improved!!!
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14273320337800777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14273320337800777 | validation: 0.20051988054589642]
	TIME [epoch: 8.22 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13344020032984444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13344020032984444 | validation: 0.2139431330583571]
	TIME [epoch: 8.21 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13884416940872604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13884416940872604 | validation: 0.22040109612206518]
	TIME [epoch: 8.21 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18692933527890682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18692933527890682 | validation: 0.2830032129731913]
	TIME [epoch: 8.21 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21081047981438994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21081047981438994 | validation: 0.23921337544751037]
	TIME [epoch: 8.23 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29486395583578057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29486395583578057 | validation: 0.22884978893130753]
	TIME [epoch: 8.21 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21571629612955703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21571629612955703 | validation: 0.23399134531231278]
	TIME [epoch: 8.2 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14453861704256565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14453861704256565 | validation: 0.18849510579255457]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1384899164731891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1384899164731891 | validation: 0.16201723562776177]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11505141452374865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11505141452374865 | validation: 0.16411405147762675]
	TIME [epoch: 8.22 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12065967095384471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12065967095384471 | validation: 0.16013094412623893]
	TIME [epoch: 8.32 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11398355968011238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11398355968011238 | validation: 0.16679832123265317]
	TIME [epoch: 8.21 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11174987602255723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11174987602255723 | validation: 0.20312029641721316]
	TIME [epoch: 8.21 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1558893762153464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1558893762153464 | validation: 0.5545134502804933]
	TIME [epoch: 8.2 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5098060965098243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5098060965098243 | validation: 0.8255383943832005]
	TIME [epoch: 8.21 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.02997727880269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.02997727880269 | validation: 1.3358435004830451]
	TIME [epoch: 8.22 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9067960770017456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9067960770017456 | validation: 1.499091229280024]
	TIME [epoch: 8.2 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.181313868422294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.181313868422294 | validation: 1.7845114728816072]
	TIME [epoch: 8.2 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.523306810747142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.523306810747142 | validation: 1.8970176804861396]
	TIME [epoch: 8.21 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.623778001442241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.623778001442241 | validation: 1.822271403170511]
	TIME [epoch: 8.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5632489668505447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5632489668505447 | validation: 1.8348940885980292]
	TIME [epoch: 8.21 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.602027122974287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.602027122974287 | validation: 1.8049359193608787]
	TIME [epoch: 8.21 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5693126751930255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5693126751930255 | validation: 1.8380175729499868]
	TIME [epoch: 8.21 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5762221401525274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5762221401525274 | validation: 1.8612248006560705]
	TIME [epoch: 8.2 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5812573844004927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5812573844004927 | validation: 1.8270732854687297]
	TIME [epoch: 8.21 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.610526839696452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.610526839696452 | validation: 1.7722775644789939]
	TIME [epoch: 8.21 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.528672868569964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.528672868569964 | validation: 1.7884006434346784]
	TIME [epoch: 8.23 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.516481719889817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.516481719889817 | validation: 1.720871764775836]
	TIME [epoch: 8.21 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4645426724291504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4645426724291504 | validation: 1.6574215969579553]
	TIME [epoch: 8.21 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4802203914913448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4802203914913448 | validation: 1.635600520461285]
	TIME [epoch: 8.21 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.386343333157336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.386343333157336 | validation: 1.7323626819274254]
	TIME [epoch: 8.22 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4506284808742143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4506284808742143 | validation: 1.4631954584513895]
	TIME [epoch: 8.21 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1660855107702366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1660855107702366 | validation: 1.2043404863278266]
	TIME [epoch: 8.23 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.715417727115684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.715417727115684 | validation: 0.7912137083575006]
	TIME [epoch: 8.2 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1863192000183969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1863192000183969 | validation: 1.0078308601323804]
	TIME [epoch: 8.21 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2133527564705922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2133527564705922 | validation: 0.6326915579138724]
	TIME [epoch: 8.2 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6635581762455789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6635581762455789 | validation: 0.6031519356016981]
	TIME [epoch: 8.22 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5094159245282652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5094159245282652 | validation: 0.5541459164258192]
	TIME [epoch: 8.22 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7267189006732115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7267189006732115 | validation: 0.42657273891508496]
	TIME [epoch: 8.22 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4497568730124094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4497568730124094 | validation: 0.2936843542383884]
	TIME [epoch: 40.1 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4244115451228075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4244115451228075 | validation: 0.8522305765322676]
	TIME [epoch: 17.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8782404539844135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8782404539844135 | validation: 0.47098146415099573]
	TIME [epoch: 17.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3948849360066727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3948849360066727 | validation: 0.5184724900775683]
	TIME [epoch: 17.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4525264979742052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4525264979742052 | validation: 0.33865439312661216]
	TIME [epoch: 17.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25531760141297555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25531760141297555 | validation: 0.3796413451312929]
	TIME [epoch: 17.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34032311617348654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34032311617348654 | validation: 0.2696642372030871]
	TIME [epoch: 17.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21929678653189605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21929678653189605 | validation: 0.2873101979837113]
	TIME [epoch: 17.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24226912920738286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24226912920738286 | validation: 0.28555187100746177]
	TIME [epoch: 17.6 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22388284071164485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22388284071164485 | validation: 0.26901988133390325]
	TIME [epoch: 17.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2021765434640856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2021765434640856 | validation: 0.2576610954126625]
	TIME [epoch: 17.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19052575302630936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19052575302630936 | validation: 0.258159003303775]
	TIME [epoch: 17.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18430587778462929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18430587778462929 | validation: 0.2514231746308337]
	TIME [epoch: 17.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17740793298559304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17740793298559304 | validation: 0.24155690438583965]
	TIME [epoch: 17.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1756523465001387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1756523465001387 | validation: 0.24510582329999658]
	TIME [epoch: 17.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19053908775378348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19053908775378348 | validation: 0.30684377315522293]
	TIME [epoch: 17.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23894601028282608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23894601028282608 | validation: 0.315388874765792]
	TIME [epoch: 17.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32704375896810034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32704375896810034 | validation: 0.2675740650104153]
	TIME [epoch: 17.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22130651523998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22130651523998 | validation: 0.26609385028744115]
	TIME [epoch: 17.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2039453751622218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2039453751622218 | validation: 0.4244710084417383]
	TIME [epoch: 17.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4125623345136509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4125623345136509 | validation: 0.2452587865317793]
	TIME [epoch: 17.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2181934071163715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2181934071163715 | validation: 0.25169387593968223]
	TIME [epoch: 17.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1863509316890397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1863509316890397 | validation: 0.3204626498080618]
	TIME [epoch: 17.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2739998960912249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2739998960912249 | validation: 0.44743276309952784]
	TIME [epoch: 17.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33284776382906445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33284776382906445 | validation: 0.4263617437710888]
	TIME [epoch: 17.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.509703887745861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.509703887745861 | validation: 0.4989841965002736]
	TIME [epoch: 17.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6237556115360279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6237556115360279 | validation: 0.5507598101590577]
	TIME [epoch: 17.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4768373415222981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4768373415222981 | validation: 0.27894719401796103]
	TIME [epoch: 17.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22396286744515131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22396286744515131 | validation: 0.2954373244359838]
	TIME [epoch: 17.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21235493820347956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21235493820347956 | validation: 0.3083221709866002]
	TIME [epoch: 17.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21505277630517136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21505277630517136 | validation: 0.3076098137508568]
	TIME [epoch: 17.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3353567147405556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3353567147405556 | validation: 0.26042217693045183]
	TIME [epoch: 17.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27747637564029803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27747637564029803 | validation: 0.23482435187589729]
	TIME [epoch: 17.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19807127021654686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19807127021654686 | validation: 0.24848992111246734]
	TIME [epoch: 17.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17296324071573752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17296324071573752 | validation: 0.22933149939135433]
	TIME [epoch: 17.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1720331813630726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1720331813630726 | validation: 0.35211071369355795]
	TIME [epoch: 17.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26855269502489526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26855269502489526 | validation: 0.34343694141664804]
	TIME [epoch: 17.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36601615925746284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36601615925746284 | validation: 0.4583378466043108]
	TIME [epoch: 17.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32578542268299066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32578542268299066 | validation: 0.3316834885814622]
	TIME [epoch: 17.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20685412395224984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20685412395224984 | validation: 0.26335959494098043]
	TIME [epoch: 17.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22673553919655834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22673553919655834 | validation: 0.30287653478452586]
	TIME [epoch: 17.6 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20409455457952938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20409455457952938 | validation: 0.29140541438242723]
	TIME [epoch: 17.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3072835420582415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3072835420582415 | validation: 0.24654221311969568]
	TIME [epoch: 17.6 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24934981409570534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24934981409570534 | validation: 0.2294574033466693]
	TIME [epoch: 17.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1396411041971172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1396411041971172 | validation: 0.2978299415629903]
	TIME [epoch: 17.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23086963578229225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23086963578229225 | validation: 0.31098604508351063]
	TIME [epoch: 17.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3687666588579722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3687666588579722 | validation: 0.3252047468568464]
	TIME [epoch: 17.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38224278741885614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38224278741885614 | validation: 0.2655233755427695]
	TIME [epoch: 17.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19276739918360938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19276739918360938 | validation: 0.2957548394947628]
	TIME [epoch: 17.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2038401217448396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2038401217448396 | validation: 0.2148042797793835]
	TIME [epoch: 17.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14399121915798074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14399121915798074 | validation: 0.2225428595925641]
	TIME [epoch: 17.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15599807694042528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15599807694042528 | validation: 0.20372896028138002]
	TIME [epoch: 17.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14250411364312085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14250411364312085 | validation: 0.4133726297326189]
	TIME [epoch: 17.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3388951654414315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3388951654414315 | validation: 0.8132776337718202]
	TIME [epoch: 17.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.897047853526475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.897047853526475 | validation: 1.0595732787473882]
	TIME [epoch: 17.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.190349718670537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.190349718670537 | validation: 0.7159885230996865]
	TIME [epoch: 17.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756644851245344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.756644851245344 | validation: 0.3082988017465611]
	TIME [epoch: 17.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33435915868423566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33435915868423566 | validation: 0.22273176401643047]
	TIME [epoch: 17.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24954395176433758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24954395176433758 | validation: 0.5860008051552292]
	TIME [epoch: 17.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5484510914462343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5484510914462343 | validation: 0.47191881483575604]
	TIME [epoch: 17.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5001683413371358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5001683413371358 | validation: 0.23438247986614089]
	TIME [epoch: 17.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33427466737358563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33427466737358563 | validation: 0.8382208266392634]
	TIME [epoch: 17.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069734230855553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8069734230855553 | validation: 0.49907471903232353]
	TIME [epoch: 17.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48396682265399904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48396682265399904 | validation: 0.2703289498842178]
	TIME [epoch: 17.6 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26162815012236446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26162815012236446 | validation: 0.31134712285863486]
	TIME [epoch: 17.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21918943564350968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21918943564350968 | validation: 0.3212975475665046]
	TIME [epoch: 17.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24963098660396063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24963098660396063 | validation: 0.21995599584133452]
	TIME [epoch: 17.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21297872205316892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21297872205316892 | validation: 0.20662295725127955]
	TIME [epoch: 17.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16072490542984902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16072490542984902 | validation: 0.2165610965326238]
	TIME [epoch: 17.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14874806760372314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14874806760372314 | validation: 0.20064382491429134]
	TIME [epoch: 17.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14702473874401378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14702473874401378 | validation: 0.1699508100871755]
	TIME [epoch: 17.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14136305674760563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14136305674760563 | validation: 0.18567033226612176]
	TIME [epoch: 17.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14342209520725516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14342209520725516 | validation: 0.20903078771247063]
	TIME [epoch: 17.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172434/states/model_phi1_3c_v_mmd1_573.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4645.470 seconds.
