Args:
Namespace(name='model_phi2_1a_v_mmd2', outdir='out/model_training/model_phi2_1a_v_mmd2', training_data='data/training_data/data_phi2_1a/training', validation_data='data/training_data/data_phi2_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=10.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1156997225

Training model...

Saving initial model state to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4000390089012003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4000390089012003 | validation: 4.427978429367688]
	TIME [epoch: 127 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.883437758226288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.883437758226288 | validation: 4.095959745741922]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.365256476840101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.365256476840101 | validation: 2.006357894548869]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1483860884332087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1483860884332087 | validation: 2.0482955556751534]
	TIME [epoch: 7.59 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.482098770625239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.482098770625239 | validation: 1.6518052898968505]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2745662561806361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2745662561806361 | validation: 1.1233204782301915]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3415252390863648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3415252390863648 | validation: 1.4522854000121053]
	TIME [epoch: 7.55 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0105084509216724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0105084509216724 | validation: 0.9781839507718213]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1908130691535923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1908130691535923 | validation: 1.7005296602188624]
	TIME [epoch: 7.62 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6086802310418675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6086802310418675 | validation: 0.9170597644762589]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0122658773550426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0122658773550426 | validation: 0.669988035964374]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8439411268381806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8439411268381806 | validation: 0.9628075210032131]
	TIME [epoch: 7.56 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8853114411597975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8853114411597975 | validation: 1.232446737332805]
	TIME [epoch: 7.61 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7788688361206333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7788688361206333 | validation: 0.5599878173186967]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8421982680759726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8421982680759726 | validation: 0.5370271693297667]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6816368175099605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6816368175099605 | validation: 0.6547311550311017]
	TIME [epoch: 7.58 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.637281721904278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.637281721904278 | validation: 0.5618012120895208]
	TIME [epoch: 7.62 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7366478615745194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7366478615745194 | validation: 0.5528046771976639]
	TIME [epoch: 7.59 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5797776401075411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5797776401075411 | validation: 0.8122804270894242]
	TIME [epoch: 7.59 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7675016239914543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7675016239914543 | validation: 0.6063845782086283]
	TIME [epoch: 7.6 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.61247227584396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.61247227584396 | validation: 0.8915600440945788]
	TIME [epoch: 7.6 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6204806956309428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6204806956309428 | validation: 0.7608992629952436]
	TIME [epoch: 7.65 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6697601752245567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6697601752245567 | validation: 0.49627070276896323]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5562075291537305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5562075291537305 | validation: 0.9611245977817817]
	TIME [epoch: 7.53 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.69535620236566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.69535620236566 | validation: 0.5115474866304391]
	TIME [epoch: 7.53 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49786513379366676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49786513379366676 | validation: 0.4446701953565694]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6402682665489637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6402682665489637 | validation: 0.6748124573966894]
	TIME [epoch: 7.59 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5572520436098204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5572520436098204 | validation: 0.4422693150996877]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.758327542796181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.758327542796181 | validation: 0.44908141796706874]
	TIME [epoch: 7.57 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47260496425799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47260496425799 | validation: 0.4241467641983434]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5249864124031057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5249864124031057 | validation: 0.544760704345525]
	TIME [epoch: 7.64 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5560458338020619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5560458338020619 | validation: 0.39969953463324465]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46109605366152706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46109605366152706 | validation: 0.5307448743203158]
	TIME [epoch: 7.59 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6189045581650621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6189045581650621 | validation: 0.3836878878070318]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4211434400732109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4211434400732109 | validation: 0.43394212983057756]
	TIME [epoch: 7.64 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6498033927932843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6498033927932843 | validation: 0.40470377884962905]
	TIME [epoch: 7.58 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4097990125747345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4097990125747345 | validation: 0.35088018858949754]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5171001318423076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5171001318423076 | validation: 0.4735580761428153]
	TIME [epoch: 7.59 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4193218015668606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4193218015668606 | validation: 0.32596658158397485]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.481199819556802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.481199819556802 | validation: 0.4164492632184771]
	TIME [epoch: 7.56 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3819500465563612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3819500465563612 | validation: 0.338437498642946]
	TIME [epoch: 7.52 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35268563774721107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35268563774721107 | validation: 0.41610638783207105]
	TIME [epoch: 7.52 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5247604036349611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5247604036349611 | validation: 0.3580801648758031]
	TIME [epoch: 7.52 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36831682899758283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36831682899758283 | validation: 0.36295773239790785]
	TIME [epoch: 7.58 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3582936943236591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3582936943236591 | validation: 0.380592237865195]
	TIME [epoch: 7.53 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3987688919838879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3987688919838879 | validation: 0.3793026422122979]
	TIME [epoch: 7.52 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3793846181924626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3793846181924626 | validation: 0.36230947588312]
	TIME [epoch: 7.52 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36031771677732943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36031771677732943 | validation: 0.34472890781834353]
	TIME [epoch: 7.54 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32515564102845784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32515564102845784 | validation: 0.25463666997846346]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.325290821386037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.325290821386037 | validation: 0.7249009898273678]
	TIME [epoch: 7.56 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4651090880707566		[learning rate: 0.0099823]
	Learning Rate: 0.0099823
	LOSS [training: 0.4651090880707566 | validation: 0.3474546680404953]
	TIME [epoch: 7.55 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3307485963948186		[learning rate: 0.0099588]
	Learning Rate: 0.00995876
	LOSS [training: 0.3307485963948186 | validation: 0.23680773583169323]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28137967482244924		[learning rate: 0.0099353]
	Learning Rate: 0.00993527
	LOSS [training: 0.28137967482244924 | validation: 0.2501325557986271]
	TIME [epoch: 7.62 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38077442936311523		[learning rate: 0.0099118]
	Learning Rate: 0.00991183
	LOSS [training: 0.38077442936311523 | validation: 0.34632799701091505]
	TIME [epoch: 7.56 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3006727256335616		[learning rate: 0.0098884]
	Learning Rate: 0.00988845
	LOSS [training: 0.3006727256335616 | validation: 0.21876065414799561]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2751169748237119		[learning rate: 0.0098651]
	Learning Rate: 0.00986512
	LOSS [training: 0.2751169748237119 | validation: 0.23746685602484058]
	TIME [epoch: 7.57 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.392483358006888		[learning rate: 0.0098419]
	Learning Rate: 0.00984185
	LOSS [training: 0.392483358006888 | validation: 0.22037412069859352]
	TIME [epoch: 7.63 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.286474862733873		[learning rate: 0.0098186]
	Learning Rate: 0.00981864
	LOSS [training: 0.286474862733873 | validation: 0.26194164774958817]
	TIME [epoch: 7.6 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28795474996932846		[learning rate: 0.0097955]
	Learning Rate: 0.00979548
	LOSS [training: 0.28795474996932846 | validation: 0.22967320434167426]
	TIME [epoch: 7.58 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2782613397453666		[learning rate: 0.0097724]
	Learning Rate: 0.00977237
	LOSS [training: 0.2782613397453666 | validation: 0.3975294234137599]
	TIME [epoch: 7.59 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3533006608619952		[learning rate: 0.0097493]
	Learning Rate: 0.00974932
	LOSS [training: 0.3533006608619952 | validation: 0.2320259346190457]
	TIME [epoch: 7.58 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2850101537882575		[learning rate: 0.0097263]
	Learning Rate: 0.00972632
	LOSS [training: 0.2850101537882575 | validation: 0.18517501464771421]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2504066277230343		[learning rate: 0.0097034]
	Learning Rate: 0.00970338
	LOSS [training: 0.2504066277230343 | validation: 0.18967272477501762]
	TIME [epoch: 7.59 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27147163560216797		[learning rate: 0.0096805]
	Learning Rate: 0.00968049
	LOSS [training: 0.27147163560216797 | validation: 0.4840033899222054]
	TIME [epoch: 7.59 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33169784489893833		[learning rate: 0.0096577]
	Learning Rate: 0.00965766
	LOSS [training: 0.33169784489893833 | validation: 0.2461891780814328]
	TIME [epoch: 7.59 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2619601875007379		[learning rate: 0.0096349]
	Learning Rate: 0.00963488
	LOSS [training: 0.2619601875007379 | validation: 0.2215538685218264]
	TIME [epoch: 7.61 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26513143641539794		[learning rate: 0.0096121]
	Learning Rate: 0.00961215
	LOSS [training: 0.26513143641539794 | validation: 0.18362506260274644]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2513586058499281		[learning rate: 0.0095895]
	Learning Rate: 0.00958948
	LOSS [training: 0.2513586058499281 | validation: 0.22881348702900223]
	TIME [epoch: 7.61 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2578308813758023		[learning rate: 0.0095669]
	Learning Rate: 0.00956686
	LOSS [training: 0.2578308813758023 | validation: 0.20874460439128611]
	TIME [epoch: 7.64 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28933565411527173		[learning rate: 0.0095443]
	Learning Rate: 0.00954429
	LOSS [training: 0.28933565411527173 | validation: 0.17911010430672092]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24497026777419745		[learning rate: 0.0095218]
	Learning Rate: 0.00952177
	LOSS [training: 0.24497026777419745 | validation: 0.1988252127773098]
	TIME [epoch: 7.68 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23921285975007783		[learning rate: 0.0094993]
	Learning Rate: 0.00949931
	LOSS [training: 0.23921285975007783 | validation: 0.19201211941521285]
	TIME [epoch: 7.62 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2795557676796639		[learning rate: 0.0094769]
	Learning Rate: 0.00947691
	LOSS [training: 0.2795557676796639 | validation: 0.190316432287002]
	TIME [epoch: 7.63 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24783872291695394		[learning rate: 0.0094546]
	Learning Rate: 0.00945455
	LOSS [training: 0.24783872291695394 | validation: 0.2710936294054951]
	TIME [epoch: 7.63 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2589023108390251		[learning rate: 0.0094323]
	Learning Rate: 0.00943225
	LOSS [training: 0.2589023108390251 | validation: 0.17175318289417593]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23490829938227376		[learning rate: 0.00941]
	Learning Rate: 0.00941
	LOSS [training: 0.23490829938227376 | validation: 0.17219241662508453]
	TIME [epoch: 7.64 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2463577781211738		[learning rate: 0.0093878]
	Learning Rate: 0.00938781
	LOSS [training: 0.2463577781211738 | validation: 0.19103197772741431]
	TIME [epoch: 7.92 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23686277065267888		[learning rate: 0.0093657]
	Learning Rate: 0.00936566
	LOSS [training: 0.23686277065267888 | validation: 0.16248818754934333]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2284384935819907		[learning rate: 0.0093436]
	Learning Rate: 0.00934357
	LOSS [training: 0.2284384935819907 | validation: 0.15614893656659715]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23462771984658823		[learning rate: 0.0093215]
	Learning Rate: 0.00932153
	LOSS [training: 0.23462771984658823 | validation: 0.37915791862751624]
	TIME [epoch: 7.6 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3129677049037879		[learning rate: 0.0092995]
	Learning Rate: 0.00929954
	LOSS [training: 0.3129677049037879 | validation: 0.17864580849038847]
	TIME [epoch: 7.55 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2399973825108063		[learning rate: 0.0092776]
	Learning Rate: 0.00927761
	LOSS [training: 0.2399973825108063 | validation: 0.1572384015422693]
	TIME [epoch: 7.56 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22096333905318316		[learning rate: 0.0092557]
	Learning Rate: 0.00925572
	LOSS [training: 0.22096333905318316 | validation: 0.17510500553216177]
	TIME [epoch: 7.56 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2463020022121466		[learning rate: 0.0092339]
	Learning Rate: 0.00923389
	LOSS [training: 0.2463020022121466 | validation: 0.15666893295366052]
	TIME [epoch: 7.6 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2367298959091953		[learning rate: 0.0092121]
	Learning Rate: 0.00921211
	LOSS [training: 0.2367298959091953 | validation: 0.16032337598961985]
	TIME [epoch: 7.56 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22270598387724821		[learning rate: 0.0091904]
	Learning Rate: 0.00919038
	LOSS [training: 0.22270598387724821 | validation: 0.1657408844335172]
	TIME [epoch: 7.55 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22072102880185576		[learning rate: 0.0091687]
	Learning Rate: 0.0091687
	LOSS [training: 0.22072102880185576 | validation: 0.19717287387852037]
	TIME [epoch: 7.56 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2459092334771501		[learning rate: 0.0091471]
	Learning Rate: 0.00914707
	LOSS [training: 0.2459092334771501 | validation: 0.1749180807127108]
	TIME [epoch: 7.56 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22013848387412188		[learning rate: 0.0091255]
	Learning Rate: 0.00912549
	LOSS [training: 0.22013848387412188 | validation: 0.15497628629099963]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21012851808457125		[learning rate: 0.009104]
	Learning Rate: 0.00910397
	LOSS [training: 0.21012851808457125 | validation: 0.1630791678928774]
	TIME [epoch: 7.6 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2371095610898318		[learning rate: 0.0090825]
	Learning Rate: 0.00908249
	LOSS [training: 0.2371095610898318 | validation: 0.15599452714031423]
	TIME [epoch: 7.6 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21370981802778402		[learning rate: 0.0090611]
	Learning Rate: 0.00906107
	LOSS [training: 0.21370981802778402 | validation: 0.16715725432287964]
	TIME [epoch: 7.6 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2451378624283571		[learning rate: 0.0090397]
	Learning Rate: 0.0090397
	LOSS [training: 0.2451378624283571 | validation: 0.15492527627352384]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22286552218529032		[learning rate: 0.0090184]
	Learning Rate: 0.00901837
	LOSS [training: 0.22286552218529032 | validation: 0.15730525240678245]
	TIME [epoch: 7.61 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21713486746133864		[learning rate: 0.0089971]
	Learning Rate: 0.0089971
	LOSS [training: 0.21713486746133864 | validation: 0.19371515413968327]
	TIME [epoch: 7.59 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23667469808463484		[learning rate: 0.0089759]
	Learning Rate: 0.00897588
	LOSS [training: 0.23667469808463484 | validation: 0.15572362380162022]
	TIME [epoch: 7.59 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.208308179915405		[learning rate: 0.0089547]
	Learning Rate: 0.0089547
	LOSS [training: 0.208308179915405 | validation: 0.14285635025916726]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1991176746715527		[learning rate: 0.0089336]
	Learning Rate: 0.00893358
	LOSS [training: 0.1991176746715527 | validation: 0.1708638138903334]
	TIME [epoch: 7.64 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24346569760082998		[learning rate: 0.0089125]
	Learning Rate: 0.00891251
	LOSS [training: 0.24346569760082998 | validation: 0.159911411904272]
	TIME [epoch: 7.63 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20742169997025617		[learning rate: 0.0088915]
	Learning Rate: 0.00889149
	LOSS [training: 0.20742169997025617 | validation: 0.1603139999801499]
	TIME [epoch: 7.6 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1947784931664484		[learning rate: 0.0088705]
	Learning Rate: 0.00887051
	LOSS [training: 0.1947784931664484 | validation: 0.1470480739332639]
	TIME [epoch: 7.64 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19109851022954538		[learning rate: 0.0088496]
	Learning Rate: 0.00884959
	LOSS [training: 0.19109851022954538 | validation: 0.1535432896779266]
	TIME [epoch: 7.68 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2006387744475561		[learning rate: 0.0088287]
	Learning Rate: 0.00882871
	LOSS [training: 0.2006387744475561 | validation: 0.1396792591270491]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22173036646135683		[learning rate: 0.0088079]
	Learning Rate: 0.00880789
	LOSS [training: 0.22173036646135683 | validation: 0.17338915491912024]
	TIME [epoch: 7.64 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18859404598889284		[learning rate: 0.0087871]
	Learning Rate: 0.00878711
	LOSS [training: 0.18859404598889284 | validation: 0.1335419542275967]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17058740666679506		[learning rate: 0.0087664]
	Learning Rate: 0.00876638
	LOSS [training: 0.17058740666679506 | validation: 0.1778953624666601]
	TIME [epoch: 7.55 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20666929494865674		[learning rate: 0.0087457]
	Learning Rate: 0.00874571
	LOSS [training: 0.20666929494865674 | validation: 0.2125903645937849]
	TIME [epoch: 7.57 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20161156554669057		[learning rate: 0.0087251]
	Learning Rate: 0.00872508
	LOSS [training: 0.20161156554669057 | validation: 0.12946157690137394]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15319525829203867		[learning rate: 0.0087045]
	Learning Rate: 0.00870449
	LOSS [training: 0.15319525829203867 | validation: 0.20529347730256864]
	TIME [epoch: 7.55 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17252745034326183		[learning rate: 0.008684]
	Learning Rate: 0.00868396
	LOSS [training: 0.17252745034326183 | validation: 0.17560426401886461]
	TIME [epoch: 7.55 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1603589359288121		[learning rate: 0.0086635]
	Learning Rate: 0.00866348
	LOSS [training: 0.1603589359288121 | validation: 0.11249190384836855]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16677808712648645		[learning rate: 0.008643]
	Learning Rate: 0.00864304
	LOSS [training: 0.16677808712648645 | validation: 0.27772358908490663]
	TIME [epoch: 7.57 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2021005178851502		[learning rate: 0.0086227]
	Learning Rate: 0.00862265
	LOSS [training: 0.2021005178851502 | validation: 0.15557874700292107]
	TIME [epoch: 7.56 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1765945533263316		[learning rate: 0.0086023]
	Learning Rate: 0.00860232
	LOSS [training: 0.1765945533263316 | validation: 0.0969976452800796]
	TIME [epoch: 7.56 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14278134986270186		[learning rate: 0.008582]
	Learning Rate: 0.00858202
	LOSS [training: 0.14278134986270186 | validation: 0.09612741393662338]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13414462036124858		[learning rate: 0.0085618]
	Learning Rate: 0.00856178
	LOSS [training: 0.13414462036124858 | validation: 0.1086524462426335]
	TIME [epoch: 7.64 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1253263877946341		[learning rate: 0.0085416]
	Learning Rate: 0.00854158
	LOSS [training: 0.1253263877946341 | validation: 0.148340665059219]
	TIME [epoch: 7.64 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14689146215658644		[learning rate: 0.0085214]
	Learning Rate: 0.00852144
	LOSS [training: 0.14689146215658644 | validation: 0.12287601235951663]
	TIME [epoch: 7.63 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1440732913802579		[learning rate: 0.0085013]
	Learning Rate: 0.00850134
	LOSS [training: 0.1440732913802579 | validation: 0.08279348133904796]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1547065556403485		[learning rate: 0.0084813]
	Learning Rate: 0.00848128
	LOSS [training: 0.1547065556403485 | validation: 0.0902771736919851]
	TIME [epoch: 7.68 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11215374065096938		[learning rate: 0.0084613]
	Learning Rate: 0.00846128
	LOSS [training: 0.11215374065096938 | validation: 0.10493171462971335]
	TIME [epoch: 7.65 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13618388485236055		[learning rate: 0.0084413]
	Learning Rate: 0.00844132
	LOSS [training: 0.13618388485236055 | validation: 0.13263430678169122]
	TIME [epoch: 7.64 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11781132341418943		[learning rate: 0.0084214]
	Learning Rate: 0.00842141
	LOSS [training: 0.11781132341418943 | validation: 0.07775147288395462]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15370187205972768		[learning rate: 0.0084015]
	Learning Rate: 0.00840154
	LOSS [training: 0.15370187205972768 | validation: 0.3215105349485168]
	TIME [epoch: 7.7 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1657597959522601		[learning rate: 0.0083817]
	Learning Rate: 0.00838172
	LOSS [training: 0.1657597959522601 | validation: 0.10176225819434881]
	TIME [epoch: 7.66 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11440716768887912		[learning rate: 0.008362]
	Learning Rate: 0.00836195
	LOSS [training: 0.11440716768887912 | validation: 0.09643589342763448]
	TIME [epoch: 7.64 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1213634793837251		[learning rate: 0.0083422]
	Learning Rate: 0.00834223
	LOSS [training: 0.1213634793837251 | validation: 0.0719348844777829]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13747206582402452		[learning rate: 0.0083225]
	Learning Rate: 0.00832255
	LOSS [training: 0.13747206582402452 | validation: 0.13631853860194731]
	TIME [epoch: 7.54 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10822658382489986		[learning rate: 0.0083029]
	Learning Rate: 0.00830292
	LOSS [training: 0.10822658382489986 | validation: 0.10599457524806702]
	TIME [epoch: 7.58 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12460003812258524		[learning rate: 0.0082833]
	Learning Rate: 0.00828333
	LOSS [training: 0.12460003812258524 | validation: 0.08545413670092203]
	TIME [epoch: 7.54 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14920241103102327		[learning rate: 0.0082638]
	Learning Rate: 0.00826379
	LOSS [training: 0.14920241103102327 | validation: 0.08279252519920965]
	TIME [epoch: 7.53 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1230958247164656		[learning rate: 0.0082443]
	Learning Rate: 0.0082443
	LOSS [training: 0.1230958247164656 | validation: 0.15290885664203113]
	TIME [epoch: 7.54 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11697811160094382		[learning rate: 0.0082249]
	Learning Rate: 0.00822485
	LOSS [training: 0.11697811160094382 | validation: 0.07180200179444125]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11136403587839654		[learning rate: 0.0082055]
	Learning Rate: 0.00820545
	LOSS [training: 0.11136403587839654 | validation: 0.08542062036744127]
	TIME [epoch: 7.57 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11585015378424798		[learning rate: 0.0081861]
	Learning Rate: 0.0081861
	LOSS [training: 0.11585015378424798 | validation: 0.10442196766013095]
	TIME [epoch: 7.56 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08713546395661084		[learning rate: 0.0081668]
	Learning Rate: 0.00816679
	LOSS [training: 0.08713546395661084 | validation: 0.12001472498792143]
	TIME [epoch: 7.56 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1192860399538051		[learning rate: 0.0081475]
	Learning Rate: 0.00814752
	LOSS [training: 0.1192860399538051 | validation: 0.11097286509050691]
	TIME [epoch: 7.55 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11836861621022371		[learning rate: 0.0081283]
	Learning Rate: 0.0081283
	LOSS [training: 0.11836861621022371 | validation: 0.13987012092474227]
	TIME [epoch: 7.61 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15887568750515124		[learning rate: 0.0081091]
	Learning Rate: 0.00810913
	LOSS [training: 0.15887568750515124 | validation: 0.08225187748824414]
	TIME [epoch: 7.57 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08775533334818557		[learning rate: 0.00809]
	Learning Rate: 0.00809
	LOSS [training: 0.08775533334818557 | validation: 0.14434222541443653]
	TIME [epoch: 7.57 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11161207855361122		[learning rate: 0.0080709]
	Learning Rate: 0.00807092
	LOSS [training: 0.11161207855361122 | validation: 0.07015409308018647]
	TIME [epoch: 7.55 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06589484252133901		[learning rate: 0.0080519]
	Learning Rate: 0.00805188
	LOSS [training: 0.06589484252133901 | validation: 0.04609982076170819]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15270381246967504		[learning rate: 0.0080329]
	Learning Rate: 0.00803289
	LOSS [training: 0.15270381246967504 | validation: 0.26846717254244085]
	TIME [epoch: 7.62 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20151586797911666		[learning rate: 0.0080139]
	Learning Rate: 0.00801394
	LOSS [training: 0.20151586797911666 | validation: 0.0892460157443111]
	TIME [epoch: 7.6 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12311214907259173		[learning rate: 0.007995]
	Learning Rate: 0.00799504
	LOSS [training: 0.12311214907259173 | validation: 0.11050925618452384]
	TIME [epoch: 7.6 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12474624068534415		[learning rate: 0.0079762]
	Learning Rate: 0.00797618
	LOSS [training: 0.12474624068534415 | validation: 0.058127978505952936]
	TIME [epoch: 7.62 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0914472747560661		[learning rate: 0.0079574]
	Learning Rate: 0.00795736
	LOSS [training: 0.0914472747560661 | validation: 0.10559454709012907]
	TIME [epoch: 7.63 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07664227716553854		[learning rate: 0.0079386]
	Learning Rate: 0.00793859
	LOSS [training: 0.07664227716553854 | validation: 0.11876075106034809]
	TIME [epoch: 7.61 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13590272663374248		[learning rate: 0.0079199]
	Learning Rate: 0.00791987
	LOSS [training: 0.13590272663374248 | validation: 0.10476518254197628]
	TIME [epoch: 7.6 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07580133279772017		[learning rate: 0.0079012]
	Learning Rate: 0.00790119
	LOSS [training: 0.07580133279772017 | validation: 0.11797107274148647]
	TIME [epoch: 7.61 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15101123804527156		[learning rate: 0.0078825]
	Learning Rate: 0.00788255
	LOSS [training: 0.15101123804527156 | validation: 0.08182481933990232]
	TIME [epoch: 7.64 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.097513442779586		[learning rate: 0.007864]
	Learning Rate: 0.00786396
	LOSS [training: 0.097513442779586 | validation: 0.06553817270074234]
	TIME [epoch: 7.63 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08470960194738049		[learning rate: 0.0078454]
	Learning Rate: 0.00784541
	LOSS [training: 0.08470960194738049 | validation: 0.11098167348294992]
	TIME [epoch: 7.61 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09498181650824364		[learning rate: 0.0078269]
	Learning Rate: 0.0078269
	LOSS [training: 0.09498181650824364 | validation: 0.09251208233211049]
	TIME [epoch: 7.61 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07764566356648492		[learning rate: 0.0078084]
	Learning Rate: 0.00780844
	LOSS [training: 0.07764566356648492 | validation: 0.05316887336883136]
	TIME [epoch: 7.62 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0723805249147398		[learning rate: 0.00779]
	Learning Rate: 0.00779002
	LOSS [training: 0.0723805249147398 | validation: 0.15667072498926637]
	TIME [epoch: 7.68 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18093732294054482		[learning rate: 0.0077716]
	Learning Rate: 0.00777164
	LOSS [training: 0.18093732294054482 | validation: 0.1489237891768856]
	TIME [epoch: 7.61 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15433811154944613		[learning rate: 0.0077533]
	Learning Rate: 0.00775331
	LOSS [training: 0.15433811154944613 | validation: 0.07508777445588333]
	TIME [epoch: 7.61 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10510069123516347		[learning rate: 0.007735]
	Learning Rate: 0.00773502
	LOSS [training: 0.10510069123516347 | validation: 0.05121356561111741]
	TIME [epoch: 7.63 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1286036729106776		[learning rate: 0.0077168]
	Learning Rate: 0.00771678
	LOSS [training: 0.1286036729106776 | validation: 0.1005250195130729]
	TIME [epoch: 7.65 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07179444051615644		[learning rate: 0.0076986]
	Learning Rate: 0.00769857
	LOSS [training: 0.07179444051615644 | validation: 0.09108954609643646]
	TIME [epoch: 7.66 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07581861963904696		[learning rate: 0.0076804]
	Learning Rate: 0.00768041
	LOSS [training: 0.07581861963904696 | validation: 0.059220757282555964]
	TIME [epoch: 7.64 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08996076194785246		[learning rate: 0.0076623]
	Learning Rate: 0.0076623
	LOSS [training: 0.08996076194785246 | validation: 0.158491579459239]
	TIME [epoch: 7.63 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1284921946173084		[learning rate: 0.0076442]
	Learning Rate: 0.00764422
	LOSS [training: 0.1284921946173084 | validation: 0.07161416885869147]
	TIME [epoch: 7.64 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08078767502710697		[learning rate: 0.0076262]
	Learning Rate: 0.00762619
	LOSS [training: 0.08078767502710697 | validation: 0.082023177313983]
	TIME [epoch: 7.69 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07315752546980846		[learning rate: 0.0076082]
	Learning Rate: 0.0076082
	LOSS [training: 0.07315752546980846 | validation: 0.07062137563513853]
	TIME [epoch: 7.64 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10444408120544539		[learning rate: 0.0075903]
	Learning Rate: 0.00759025
	LOSS [training: 0.10444408120544539 | validation: 0.042860229381478834]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047396533080370104		[learning rate: 0.0075724]
	Learning Rate: 0.00757235
	LOSS [training: 0.047396533080370104 | validation: 0.06049392285286691]
	TIME [epoch: 7.64 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12568535228010125		[learning rate: 0.0075545]
	Learning Rate: 0.00755449
	LOSS [training: 0.12568535228010125 | validation: 0.08021417343471952]
	TIME [epoch: 7.68 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.175891011718039		[learning rate: 0.0075367]
	Learning Rate: 0.00753667
	LOSS [training: 0.175891011718039 | validation: 0.09128388960107023]
	TIME [epoch: 7.66 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0924592516930402		[learning rate: 0.0075189]
	Learning Rate: 0.00751889
	LOSS [training: 0.0924592516930402 | validation: 0.05055502939386089]
	TIME [epoch: 7.65 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05398406055247841		[learning rate: 0.0075012]
	Learning Rate: 0.00750116
	LOSS [training: 0.05398406055247841 | validation: 0.07147214883687687]
	TIME [epoch: 7.64 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10465622026557538		[learning rate: 0.0074835]
	Learning Rate: 0.00748346
	LOSS [training: 0.10465622026557538 | validation: 0.06972512687696154]
	TIME [epoch: 7.65 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07349000217694857		[learning rate: 0.0074658]
	Learning Rate: 0.00746581
	LOSS [training: 0.07349000217694857 | validation: 0.04098976989550492]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062127329279632576		[learning rate: 0.0074482]
	Learning Rate: 0.0074482
	LOSS [training: 0.062127329279632576 | validation: 0.1278146223248416]
	TIME [epoch: 7.65 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.148173221220347		[learning rate: 0.0074306]
	Learning Rate: 0.00743063
	LOSS [training: 0.148173221220347 | validation: 0.08398048145558154]
	TIME [epoch: 7.65 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07041870328674896		[learning rate: 0.0074131]
	Learning Rate: 0.0074131
	LOSS [training: 0.07041870328674896 | validation: 0.04160638026140781]
	TIME [epoch: 7.64 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053055600563475094		[learning rate: 0.0073956]
	Learning Rate: 0.00739562
	LOSS [training: 0.053055600563475094 | validation: 0.16952149658866705]
	TIME [epoch: 7.68 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11435687557088224		[learning rate: 0.0073782]
	Learning Rate: 0.00737817
	LOSS [training: 0.11435687557088224 | validation: 0.08062280942538622]
	TIME [epoch: 7.65 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06901226964361863		[learning rate: 0.0073608]
	Learning Rate: 0.00736077
	LOSS [training: 0.06901226964361863 | validation: 0.03924794847965225]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07627181539312511		[learning rate: 0.0073434]
	Learning Rate: 0.0073434
	LOSS [training: 0.07627181539312511 | validation: 0.052468657114785544]
	TIME [epoch: 7.66 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08970622918029417		[learning rate: 0.0073261]
	Learning Rate: 0.00732608
	LOSS [training: 0.08970622918029417 | validation: 0.047781725329126314]
	TIME [epoch: 7.65 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06784943960015162		[learning rate: 0.0073088]
	Learning Rate: 0.0073088
	LOSS [training: 0.06784943960015162 | validation: 0.070530270731994]
	TIME [epoch: 7.71 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058690029256972746		[learning rate: 0.0072916]
	Learning Rate: 0.00729156
	LOSS [training: 0.058690029256972746 | validation: 0.04757395440171777]
	TIME [epoch: 7.65 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06275075786455174		[learning rate: 0.0072744]
	Learning Rate: 0.00727436
	LOSS [training: 0.06275075786455174 | validation: 0.055902875190905046]
	TIME [epoch: 7.64 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08773256598116866		[learning rate: 0.0072572]
	Learning Rate: 0.0072572
	LOSS [training: 0.08773256598116866 | validation: 0.06255754368675757]
	TIME [epoch: 7.64 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06409161678179365		[learning rate: 0.0072401]
	Learning Rate: 0.00724008
	LOSS [training: 0.06409161678179365 | validation: 0.05269853921343187]
	TIME [epoch: 7.68 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04384621349098911		[learning rate: 0.007223]
	Learning Rate: 0.007223
	LOSS [training: 0.04384621349098911 | validation: 0.06928678105490604]
	TIME [epoch: 7.66 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07537190863171689		[learning rate: 0.007206]
	Learning Rate: 0.00720597
	LOSS [training: 0.07537190863171689 | validation: 0.05101211626830632]
	TIME [epoch: 7.65 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043833463285015785		[learning rate: 0.007189]
	Learning Rate: 0.00718897
	LOSS [training: 0.043833463285015785 | validation: 0.18127759414099673]
	TIME [epoch: 7.65 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.129055278753444		[learning rate: 0.007172]
	Learning Rate: 0.00717201
	LOSS [training: 0.129055278753444 | validation: 0.062093296172015804]
	TIME [epoch: 7.64 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06390893631918987		[learning rate: 0.0071551]
	Learning Rate: 0.00715509
	LOSS [training: 0.06390893631918987 | validation: 0.05454616178420581]
	TIME [epoch: 7.7 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061490214007245414		[learning rate: 0.0071382]
	Learning Rate: 0.00713822
	LOSS [training: 0.061490214007245414 | validation: 0.0389593004387076]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06807918120660231		[learning rate: 0.0071214]
	Learning Rate: 0.00712138
	LOSS [training: 0.06807918120660231 | validation: 0.05163508358980502]
	TIME [epoch: 7.54 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06869197121953709		[learning rate: 0.0071046]
	Learning Rate: 0.00710458
	LOSS [training: 0.06869197121953709 | validation: 0.0847614268864192]
	TIME [epoch: 7.55 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0658534058310065		[learning rate: 0.0070878]
	Learning Rate: 0.00708782
	LOSS [training: 0.0658534058310065 | validation: 0.040601290816746634]
	TIME [epoch: 7.59 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07152305742101994		[learning rate: 0.0070711]
	Learning Rate: 0.0070711
	LOSS [training: 0.07152305742101994 | validation: 0.05032366856474388]
	TIME [epoch: 7.56 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07031105864970637		[learning rate: 0.0070544]
	Learning Rate: 0.00705442
	LOSS [training: 0.07031105864970637 | validation: 0.079854747959967]
	TIME [epoch: 7.55 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07798144204392803		[learning rate: 0.0070378]
	Learning Rate: 0.00703778
	LOSS [training: 0.07798144204392803 | validation: 0.03968823065960411]
	TIME [epoch: 7.54 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04895747964787056		[learning rate: 0.0070212]
	Learning Rate: 0.00702118
	LOSS [training: 0.04895747964787056 | validation: 0.0439433526923891]
	TIME [epoch: 7.54 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06330281519080577		[learning rate: 0.0070046]
	Learning Rate: 0.00700462
	LOSS [training: 0.06330281519080577 | validation: 0.048523363414475364]
	TIME [epoch: 135 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04770107565852537		[learning rate: 0.0069881]
	Learning Rate: 0.0069881
	LOSS [training: 0.04770107565852537 | validation: 0.04177497014059078]
	TIME [epoch: 15 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08090102342900872		[learning rate: 0.0069716]
	Learning Rate: 0.00697161
	LOSS [training: 0.08090102342900872 | validation: 0.032624214610919564]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07839988989263243		[learning rate: 0.0069552]
	Learning Rate: 0.00695517
	LOSS [training: 0.07839988989263243 | validation: 0.04565565181835721]
	TIME [epoch: 14.9 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046624476562878506		[learning rate: 0.0069388]
	Learning Rate: 0.00693876
	LOSS [training: 0.046624476562878506 | validation: 0.05096061590577235]
	TIME [epoch: 14.9 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06888910600558612		[learning rate: 0.0069224]
	Learning Rate: 0.00692239
	LOSS [training: 0.06888910600558612 | validation: 0.06350874596305206]
	TIME [epoch: 14.9 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05717472423134075		[learning rate: 0.0069061]
	Learning Rate: 0.00690607
	LOSS [training: 0.05717472423134075 | validation: 0.07513772350818139]
	TIME [epoch: 14.9 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07176322360210949		[learning rate: 0.0068898]
	Learning Rate: 0.00688978
	LOSS [training: 0.07176322360210949 | validation: 0.03271439981401637]
	TIME [epoch: 14.9 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05454246300730223		[learning rate: 0.0068735]
	Learning Rate: 0.00687352
	LOSS [training: 0.05454246300730223 | validation: 0.03550306753515158]
	TIME [epoch: 14.9 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046512581346619467		[learning rate: 0.0068573]
	Learning Rate: 0.00685731
	LOSS [training: 0.046512581346619467 | validation: 0.036072669818375154]
	TIME [epoch: 14.9 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0327021878724133		[learning rate: 0.0068411]
	Learning Rate: 0.00684114
	LOSS [training: 0.0327021878724133 | validation: 0.046824655043405376]
	TIME [epoch: 14.9 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10165927775350642		[learning rate: 0.006825]
	Learning Rate: 0.006825
	LOSS [training: 0.10165927775350642 | validation: 0.03306682030022248]
	TIME [epoch: 14.9 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03572809592228904		[learning rate: 0.0068089]
	Learning Rate: 0.0068089
	LOSS [training: 0.03572809592228904 | validation: 0.04448224401038436]
	TIME [epoch: 14.9 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058965500325608364		[learning rate: 0.0067928]
	Learning Rate: 0.00679284
	LOSS [training: 0.058965500325608364 | validation: 0.06599994546445827]
	TIME [epoch: 14.9 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054845314177049534		[learning rate: 0.0067768]
	Learning Rate: 0.00677681
	LOSS [training: 0.054845314177049534 | validation: 0.04999946244237802]
	TIME [epoch: 14.9 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04837383209260076		[learning rate: 0.0067608]
	Learning Rate: 0.00676083
	LOSS [training: 0.04837383209260076 | validation: 0.08253023388428654]
	TIME [epoch: 14.9 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05812667642642852		[learning rate: 0.0067449]
	Learning Rate: 0.00674488
	LOSS [training: 0.05812667642642852 | validation: 0.05210006637094148]
	TIME [epoch: 14.9 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06249639246184927		[learning rate: 0.006729]
	Learning Rate: 0.00672897
	LOSS [training: 0.06249639246184927 | validation: 0.055072810569334996]
	TIME [epoch: 14.9 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07464679390903059		[learning rate: 0.0067131]
	Learning Rate: 0.0067131
	LOSS [training: 0.07464679390903059 | validation: 0.06360434789819502]
	TIME [epoch: 14.9 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06260085433835996		[learning rate: 0.0066973]
	Learning Rate: 0.00669726
	LOSS [training: 0.06260085433835996 | validation: 0.031692519065478784]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029813510567835037		[learning rate: 0.0066815]
	Learning Rate: 0.00668147
	LOSS [training: 0.029813510567835037 | validation: 0.030884257184735708]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07899633841312505		[learning rate: 0.0066657]
	Learning Rate: 0.00666571
	LOSS [training: 0.07899633841312505 | validation: 0.02547701330968126]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04056665052814572		[learning rate: 0.00665]
	Learning Rate: 0.00664998
	LOSS [training: 0.04056665052814572 | validation: 0.08148983286752555]
	TIME [epoch: 14.9 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05077940730320124		[learning rate: 0.0066343]
	Learning Rate: 0.0066343
	LOSS [training: 0.05077940730320124 | validation: 0.04916551690799439]
	TIME [epoch: 14.9 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06585525503787476		[learning rate: 0.0066186]
	Learning Rate: 0.00661865
	LOSS [training: 0.06585525503787476 | validation: 0.027423268679446174]
	TIME [epoch: 14.8 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04182780004850352		[learning rate: 0.006603]
	Learning Rate: 0.00660304
	LOSS [training: 0.04182780004850352 | validation: 0.04269098177866937]
	TIME [epoch: 14.9 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06470524087642898		[learning rate: 0.0065875]
	Learning Rate: 0.00658746
	LOSS [training: 0.06470524087642898 | validation: 0.022368001307599056]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046085664115670895		[learning rate: 0.0065719]
	Learning Rate: 0.00657192
	LOSS [training: 0.046085664115670895 | validation: 0.061274318549192616]
	TIME [epoch: 14.9 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0633479687835164		[learning rate: 0.0065564]
	Learning Rate: 0.00655642
	LOSS [training: 0.0633479687835164 | validation: 0.054645017245554046]
	TIME [epoch: 14.9 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048699065850834306		[learning rate: 0.006541]
	Learning Rate: 0.00654095
	LOSS [training: 0.048699065850834306 | validation: 0.03107173274852243]
	TIME [epoch: 14.9 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03599125747196679		[learning rate: 0.0065255]
	Learning Rate: 0.00652552
	LOSS [training: 0.03599125747196679 | validation: 0.09575135108055613]
	TIME [epoch: 14.9 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06225725007411518		[learning rate: 0.0065101]
	Learning Rate: 0.00651013
	LOSS [training: 0.06225725007411518 | validation: 0.026228045082506632]
	TIME [epoch: 14.9 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04708802117436159		[learning rate: 0.0064948]
	Learning Rate: 0.00649477
	LOSS [training: 0.04708802117436159 | validation: 0.03371188565066934]
	TIME [epoch: 14.9 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06564652488822498		[learning rate: 0.0064795]
	Learning Rate: 0.00647945
	LOSS [training: 0.06564652488822498 | validation: 0.030140203542169876]
	TIME [epoch: 14.9 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056767718310336224		[learning rate: 0.0064642]
	Learning Rate: 0.00646417
	LOSS [training: 0.056767718310336224 | validation: 0.030615559503315817]
	TIME [epoch: 14.9 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03532709210895985		[learning rate: 0.0064489]
	Learning Rate: 0.00644892
	LOSS [training: 0.03532709210895985 | validation: 0.029533203602024143]
	TIME [epoch: 14.9 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023199446166574076		[learning rate: 0.0064337]
	Learning Rate: 0.00643371
	LOSS [training: 0.023199446166574076 | validation: 0.019886836131175627]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09134911836178339		[learning rate: 0.0064185]
	Learning Rate: 0.00641853
	LOSS [training: 0.09134911836178339 | validation: 0.07960664256504124]
	TIME [epoch: 14.9 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12672843276024687		[learning rate: 0.0064034]
	Learning Rate: 0.00640339
	LOSS [training: 0.12672843276024687 | validation: 0.030677234504844858]
	TIME [epoch: 14.9 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04309668858767474		[learning rate: 0.0063883]
	Learning Rate: 0.00638829
	LOSS [training: 0.04309668858767474 | validation: 0.04133636960124449]
	TIME [epoch: 15 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030174760903397546		[learning rate: 0.0063732]
	Learning Rate: 0.00637322
	LOSS [training: 0.030174760903397546 | validation: 0.043739209564721195]
	TIME [epoch: 14.9 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034804093571133635		[learning rate: 0.0063582]
	Learning Rate: 0.00635819
	LOSS [training: 0.034804093571133635 | validation: 0.030549289133690656]
	TIME [epoch: 14.9 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03496496564901386		[learning rate: 0.0063432]
	Learning Rate: 0.00634319
	LOSS [training: 0.03496496564901386 | validation: 0.073130208854594]
	TIME [epoch: 14.9 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06056322660221919		[learning rate: 0.0063282]
	Learning Rate: 0.00632823
	LOSS [training: 0.06056322660221919 | validation: 0.040586061984140824]
	TIME [epoch: 14.9 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04304076375634931		[learning rate: 0.0063133]
	Learning Rate: 0.0063133
	LOSS [training: 0.04304076375634931 | validation: 0.033352921314290636]
	TIME [epoch: 14.9 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062065053694767516		[learning rate: 0.0062984]
	Learning Rate: 0.00629841
	LOSS [training: 0.062065053694767516 | validation: 0.023897426444102407]
	TIME [epoch: 14.9 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03528903586351253		[learning rate: 0.0062836]
	Learning Rate: 0.00628355
	LOSS [training: 0.03528903586351253 | validation: 0.04777242501937694]
	TIME [epoch: 14.9 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032706459182912324		[learning rate: 0.0062687]
	Learning Rate: 0.00626873
	LOSS [training: 0.032706459182912324 | validation: 0.058341083410327066]
	TIME [epoch: 14.9 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06298840069048998		[learning rate: 0.0062539]
	Learning Rate: 0.00625394
	LOSS [training: 0.06298840069048998 | validation: 0.031107873558751744]
	TIME [epoch: 14.9 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03806930321195459		[learning rate: 0.0062392]
	Learning Rate: 0.00623919
	LOSS [training: 0.03806930321195459 | validation: 0.03597084301379826]
	TIME [epoch: 14.9 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03175196682206967		[learning rate: 0.0062245]
	Learning Rate: 0.00622447
	LOSS [training: 0.03175196682206967 | validation: 0.07854371894047103]
	TIME [epoch: 14.9 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05355862004101753		[learning rate: 0.0062098]
	Learning Rate: 0.00620979
	LOSS [training: 0.05355862004101753 | validation: 0.025513915126495486]
	TIME [epoch: 14.9 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03073300192533225		[learning rate: 0.0061951]
	Learning Rate: 0.00619514
	LOSS [training: 0.03073300192533225 | validation: 0.05926661551836433]
	TIME [epoch: 14.9 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04688045475378452		[learning rate: 0.0061805]
	Learning Rate: 0.00618053
	LOSS [training: 0.04688045475378452 | validation: 0.036422098388119065]
	TIME [epoch: 14.9 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06316028447117267		[learning rate: 0.0061659]
	Learning Rate: 0.00616595
	LOSS [training: 0.06316028447117267 | validation: 0.025307842017222727]
	TIME [epoch: 14.9 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029483578294977233		[learning rate: 0.0061514]
	Learning Rate: 0.00615141
	LOSS [training: 0.029483578294977233 | validation: 0.0650844422252591]
	TIME [epoch: 14.9 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04495745428361802		[learning rate: 0.0061369]
	Learning Rate: 0.0061369
	LOSS [training: 0.04495745428361802 | validation: 0.06038006685915192]
	TIME [epoch: 14.9 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037529512011355584		[learning rate: 0.0061224]
	Learning Rate: 0.00612242
	LOSS [training: 0.037529512011355584 | validation: 0.023150456421547776]
	TIME [epoch: 14.9 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0645792922838247		[learning rate: 0.006108]
	Learning Rate: 0.00610798
	LOSS [training: 0.0645792922838247 | validation: 0.026283001034654606]
	TIME [epoch: 14.9 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05560230508962813		[learning rate: 0.0060936]
	Learning Rate: 0.00609357
	LOSS [training: 0.05560230508962813 | validation: 0.039474368190301645]
	TIME [epoch: 14.9 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03740011851679852		[learning rate: 0.0060792]
	Learning Rate: 0.0060792
	LOSS [training: 0.03740011851679852 | validation: 0.024284776130421418]
	TIME [epoch: 14.9 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03217066534476		[learning rate: 0.0060649]
	Learning Rate: 0.00606486
	LOSS [training: 0.03217066534476 | validation: 0.054362450085595135]
	TIME [epoch: 14.9 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041614407413376374		[learning rate: 0.0060505]
	Learning Rate: 0.00605055
	LOSS [training: 0.041614407413376374 | validation: 0.03510090029576807]
	TIME [epoch: 14.9 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033510033424708374		[learning rate: 0.0060363]
	Learning Rate: 0.00603628
	LOSS [training: 0.033510033424708374 | validation: 0.02727415085765729]
	TIME [epoch: 14.9 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0283861366797355		[learning rate: 0.006022]
	Learning Rate: 0.00602204
	LOSS [training: 0.0283861366797355 | validation: 0.040921470611258295]
	TIME [epoch: 14.9 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05502527037689048		[learning rate: 0.0060078]
	Learning Rate: 0.00600783
	LOSS [training: 0.05502527037689048 | validation: 0.031057144795039925]
	TIME [epoch: 15 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028317402295211628		[learning rate: 0.0059937]
	Learning Rate: 0.00599366
	LOSS [training: 0.028317402295211628 | validation: 0.018538194512146042]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05027098070684632		[learning rate: 0.0059795]
	Learning Rate: 0.00597952
	LOSS [training: 0.05027098070684632 | validation: 0.04434566244288493]
	TIME [epoch: 14.9 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03365872561142718		[learning rate: 0.0059654]
	Learning Rate: 0.00596542
	LOSS [training: 0.03365872561142718 | validation: 0.028041778651257596]
	TIME [epoch: 14.9 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039814677795906715		[learning rate: 0.0059513]
	Learning Rate: 0.00595135
	LOSS [training: 0.039814677795906715 | validation: 0.032096159761198395]
	TIME [epoch: 14.9 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023120639478325024		[learning rate: 0.0059373]
	Learning Rate: 0.00593731
	LOSS [training: 0.023120639478325024 | validation: 0.019362029248382585]
	TIME [epoch: 14.9 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04583717208409306		[learning rate: 0.0059233]
	Learning Rate: 0.00592331
	LOSS [training: 0.04583717208409306 | validation: 0.044020072400191124]
	TIME [epoch: 14.9 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026554821265710245		[learning rate: 0.0059093]
	Learning Rate: 0.00590933
	LOSS [training: 0.026554821265710245 | validation: 0.018369475457512056]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024745297632365583		[learning rate: 0.0058954]
	Learning Rate: 0.00589539
	LOSS [training: 0.024745297632365583 | validation: 0.08009581235214704]
	TIME [epoch: 14.9 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05677827181135356		[learning rate: 0.0058815]
	Learning Rate: 0.00588149
	LOSS [training: 0.05677827181135356 | validation: 0.0518461086765645]
	TIME [epoch: 14.9 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05211439561275468		[learning rate: 0.0058676]
	Learning Rate: 0.00586761
	LOSS [training: 0.05211439561275468 | validation: 0.02443482015031389]
	TIME [epoch: 14.9 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03304507511488575		[learning rate: 0.0058538]
	Learning Rate: 0.00585377
	LOSS [training: 0.03304507511488575 | validation: 0.026434725168175917]
	TIME [epoch: 14.9 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027360391636121112		[learning rate: 0.00584]
	Learning Rate: 0.00583996
	LOSS [training: 0.027360391636121112 | validation: 0.02315475866920838]
	TIME [epoch: 14.9 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05330364538248148		[learning rate: 0.0058262]
	Learning Rate: 0.00582619
	LOSS [training: 0.05330364538248148 | validation: 0.04081432232594187]
	TIME [epoch: 14.9 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036822147036890046		[learning rate: 0.0058124]
	Learning Rate: 0.00581245
	LOSS [training: 0.036822147036890046 | validation: 0.025011134587850686]
	TIME [epoch: 14.9 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02484815835945401		[learning rate: 0.0057987]
	Learning Rate: 0.00579874
	LOSS [training: 0.02484815835945401 | validation: 0.023419734863217398]
	TIME [epoch: 14.9 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03582777072888377		[learning rate: 0.0057851]
	Learning Rate: 0.00578506
	LOSS [training: 0.03582777072888377 | validation: 0.028905830679349306]
	TIME [epoch: 15 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03397460877907932		[learning rate: 0.0057714]
	Learning Rate: 0.00577141
	LOSS [training: 0.03397460877907932 | validation: 0.024325824771225465]
	TIME [epoch: 14.9 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04017637245185786		[learning rate: 0.0057578]
	Learning Rate: 0.0057578
	LOSS [training: 0.04017637245185786 | validation: 0.030438417459567872]
	TIME [epoch: 14.9 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034955829514970976		[learning rate: 0.0057442]
	Learning Rate: 0.00574422
	LOSS [training: 0.034955829514970976 | validation: 0.021695471875584134]
	TIME [epoch: 14.9 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02764916271691245		[learning rate: 0.0057307]
	Learning Rate: 0.00573067
	LOSS [training: 0.02764916271691245 | validation: 0.029307167791521527]
	TIME [epoch: 14.9 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02842969806489199		[learning rate: 0.0057171]
	Learning Rate: 0.00571715
	LOSS [training: 0.02842969806489199 | validation: 0.014436386695526559]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024538046378270312		[learning rate: 0.0057037]
	Learning Rate: 0.00570366
	LOSS [training: 0.024538046378270312 | validation: 0.057262223507058715]
	TIME [epoch: 14.9 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05615557295938586		[learning rate: 0.0056902]
	Learning Rate: 0.00569021
	LOSS [training: 0.05615557295938586 | validation: 0.019933187040644652]
	TIME [epoch: 14.9 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0360716011542485		[learning rate: 0.0056768]
	Learning Rate: 0.00567679
	LOSS [training: 0.0360716011542485 | validation: 0.032406840158825445]
	TIME [epoch: 15 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03323455832741093		[learning rate: 0.0056634]
	Learning Rate: 0.0056634
	LOSS [training: 0.03323455832741093 | validation: 0.024490097510819486]
	TIME [epoch: 14.9 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024732983484968288		[learning rate: 0.00565]
	Learning Rate: 0.00565004
	LOSS [training: 0.024732983484968288 | validation: 0.02606777473548197]
	TIME [epoch: 14.9 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03374385578035521		[learning rate: 0.0056367]
	Learning Rate: 0.00563671
	LOSS [training: 0.03374385578035521 | validation: 0.020597502069392006]
	TIME [epoch: 15 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020837400281179936		[learning rate: 0.0056234]
	Learning Rate: 0.00562341
	LOSS [training: 0.020837400281179936 | validation: 0.04512645557357392]
	TIME [epoch: 14.9 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03927249034006683		[learning rate: 0.0056101]
	Learning Rate: 0.00561015
	LOSS [training: 0.03927249034006683 | validation: 0.01680633587576849]
	TIME [epoch: 15 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0323921608494432		[learning rate: 0.0055969]
	Learning Rate: 0.00559691
	LOSS [training: 0.0323921608494432 | validation: 0.03567584068445999]
	TIME [epoch: 14.9 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034065426957554566		[learning rate: 0.0055837]
	Learning Rate: 0.00558371
	LOSS [training: 0.034065426957554566 | validation: 0.028879097618286144]
	TIME [epoch: 14.9 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022663258675062886		[learning rate: 0.0055705]
	Learning Rate: 0.00557054
	LOSS [training: 0.022663258675062886 | validation: 0.018206500866521556]
	TIME [epoch: 15 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030333166952918628		[learning rate: 0.0055574]
	Learning Rate: 0.0055574
	LOSS [training: 0.030333166952918628 | validation: 0.025382586955711023]
	TIME [epoch: 14.9 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04935518491937623		[learning rate: 0.0055443]
	Learning Rate: 0.00554429
	LOSS [training: 0.04935518491937623 | validation: 0.01806353681614021]
	TIME [epoch: 15 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035168228908552986		[learning rate: 0.0055312]
	Learning Rate: 0.00553121
	LOSS [training: 0.035168228908552986 | validation: 0.023974176600970912]
	TIME [epoch: 15 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020962936794801497		[learning rate: 0.0055182]
	Learning Rate: 0.00551817
	LOSS [training: 0.020962936794801497 | validation: 0.024829612569528228]
	TIME [epoch: 14.9 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02582531531938232		[learning rate: 0.0055052]
	Learning Rate: 0.00550515
	LOSS [training: 0.02582531531938232 | validation: 0.021699355591436646]
	TIME [epoch: 15 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01671666157911915		[learning rate: 0.0054922]
	Learning Rate: 0.00549216
	LOSS [training: 0.01671666157911915 | validation: 0.01404756784075366]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0387597658073479		[learning rate: 0.0054792]
	Learning Rate: 0.00547921
	LOSS [training: 0.0387597658073479 | validation: 0.03837021027089642]
	TIME [epoch: 14.9 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02952018121845112		[learning rate: 0.0054663]
	Learning Rate: 0.00546629
	LOSS [training: 0.02952018121845112 | validation: 0.03360077174463843]
	TIME [epoch: 15 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027292200351972586		[learning rate: 0.0054534]
	Learning Rate: 0.00545339
	LOSS [training: 0.027292200351972586 | validation: 0.015593623423139616]
	TIME [epoch: 14.9 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03336157074699868		[learning rate: 0.0054405]
	Learning Rate: 0.00544053
	LOSS [training: 0.03336157074699868 | validation: 0.058231410357725666]
	TIME [epoch: 15 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044134864844161785		[learning rate: 0.0054277]
	Learning Rate: 0.00542769
	LOSS [training: 0.044134864844161785 | validation: 0.05772624757761409]
	TIME [epoch: 15 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02883601125035266		[learning rate: 0.0054149]
	Learning Rate: 0.00541489
	LOSS [training: 0.02883601125035266 | validation: 0.014926559380199456]
	TIME [epoch: 14.9 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014599128042608399		[learning rate: 0.0054021]
	Learning Rate: 0.00540212
	LOSS [training: 0.014599128042608399 | validation: 0.02575908571457846]
	TIME [epoch: 15 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0462245222785231		[learning rate: 0.0053894]
	Learning Rate: 0.00538938
	LOSS [training: 0.0462245222785231 | validation: 0.02065691933247496]
	TIME [epoch: 15 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02441294173407292		[learning rate: 0.0053767]
	Learning Rate: 0.00537666
	LOSS [training: 0.02441294173407292 | validation: 0.029356367084218384]
	TIME [epoch: 15 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028662063609932075		[learning rate: 0.005364]
	Learning Rate: 0.00536398
	LOSS [training: 0.028662063609932075 | validation: 0.03343733049544975]
	TIME [epoch: 15 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023018080296732624		[learning rate: 0.0053513]
	Learning Rate: 0.00535133
	LOSS [training: 0.023018080296732624 | validation: 0.01874966506754034]
	TIME [epoch: 14.9 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03808788894120139		[learning rate: 0.0053387]
	Learning Rate: 0.00533871
	LOSS [training: 0.03808788894120139 | validation: 0.012557052225122666]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01626649821204481		[learning rate: 0.0053261]
	Learning Rate: 0.00532611
	LOSS [training: 0.01626649821204481 | validation: 0.030365391707366577]
	TIME [epoch: 14.9 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04647332198569968		[learning rate: 0.0053135]
	Learning Rate: 0.00531355
	LOSS [training: 0.04647332198569968 | validation: 0.02098463004892591]
	TIME [epoch: 14.9 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02785774306119667		[learning rate: 0.005301]
	Learning Rate: 0.00530101
	LOSS [training: 0.02785774306119667 | validation: 0.01704159790941874]
	TIME [epoch: 15 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01783877461707532		[learning rate: 0.0052885]
	Learning Rate: 0.00528851
	LOSS [training: 0.01783877461707532 | validation: 0.027067884170725236]
	TIME [epoch: 14.9 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027042616865092052		[learning rate: 0.005276]
	Learning Rate: 0.00527604
	LOSS [training: 0.027042616865092052 | validation: 0.016967566022795266]
	TIME [epoch: 15 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020008975653914927		[learning rate: 0.0052636]
	Learning Rate: 0.00526359
	LOSS [training: 0.020008975653914927 | validation: 0.03534127896362395]
	TIME [epoch: 15 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02539583024213873		[learning rate: 0.0052512]
	Learning Rate: 0.00525117
	LOSS [training: 0.02539583024213873 | validation: 0.029463592024017077]
	TIME [epoch: 14.9 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051535871141231496		[learning rate: 0.0052388]
	Learning Rate: 0.00523879
	LOSS [training: 0.051535871141231496 | validation: 0.051397751986023234]
	TIME [epoch: 15 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03811063523189971		[learning rate: 0.0052264]
	Learning Rate: 0.00522643
	LOSS [training: 0.03811063523189971 | validation: 0.015137925355458822]
	TIME [epoch: 14.9 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01671808915903465		[learning rate: 0.0052141]
	Learning Rate: 0.0052141
	LOSS [training: 0.01671808915903465 | validation: 0.017165736472396824]
	TIME [epoch: 14.9 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01570833014455236		[learning rate: 0.0052018]
	Learning Rate: 0.0052018
	LOSS [training: 0.01570833014455236 | validation: 0.015449605190585013]
	TIME [epoch: 15 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04060987049794667		[learning rate: 0.0051895]
	Learning Rate: 0.00518953
	LOSS [training: 0.04060987049794667 | validation: 0.024306530863738768]
	TIME [epoch: 14.9 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025368599636942282		[learning rate: 0.0051773]
	Learning Rate: 0.00517729
	LOSS [training: 0.025368599636942282 | validation: 0.0181095070785092]
	TIME [epoch: 15 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01749853688613339		[learning rate: 0.0051651]
	Learning Rate: 0.00516508
	LOSS [training: 0.01749853688613339 | validation: 0.012594408052019179]
	TIME [epoch: 14.9 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022266631402564487		[learning rate: 0.0051529]
	Learning Rate: 0.00515289
	LOSS [training: 0.022266631402564487 | validation: 0.0159928427347444]
	TIME [epoch: 14.9 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016002497958081948		[learning rate: 0.0051407]
	Learning Rate: 0.00514074
	LOSS [training: 0.016002497958081948 | validation: 0.021428475587070776]
	TIME [epoch: 15 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05102290582841829		[learning rate: 0.0051286]
	Learning Rate: 0.00512861
	LOSS [training: 0.05102290582841829 | validation: 0.015305843602428986]
	TIME [epoch: 14.9 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016049830901589728		[learning rate: 0.0051165]
	Learning Rate: 0.00511652
	LOSS [training: 0.016049830901589728 | validation: 0.014010860665228612]
	TIME [epoch: 14.9 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027955048580204353		[learning rate: 0.0051044]
	Learning Rate: 0.00510445
	LOSS [training: 0.027955048580204353 | validation: 0.02381871809136979]
	TIME [epoch: 15 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016893808728991034		[learning rate: 0.0050924]
	Learning Rate: 0.00509241
	LOSS [training: 0.016893808728991034 | validation: 0.01643567842763043]
	TIME [epoch: 14.9 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015687192144107274		[learning rate: 0.0050804]
	Learning Rate: 0.00508039
	LOSS [training: 0.015687192144107274 | validation: 0.019796108570789086]
	TIME [epoch: 15 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029005099739345713		[learning rate: 0.0050684]
	Learning Rate: 0.00506841
	LOSS [training: 0.029005099739345713 | validation: 0.047127047831039576]
	TIME [epoch: 15 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04006823008713456		[learning rate: 0.0050565]
	Learning Rate: 0.00505645
	LOSS [training: 0.04006823008713456 | validation: 0.05626458838437577]
	TIME [epoch: 15 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025967162316298868		[learning rate: 0.0050445]
	Learning Rate: 0.00504453
	LOSS [training: 0.025967162316298868 | validation: 0.017564173844637994]
	TIME [epoch: 15 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015493925154460162		[learning rate: 0.0050326]
	Learning Rate: 0.00503263
	LOSS [training: 0.015493925154460162 | validation: 0.017867850064698362]
	TIME [epoch: 14.9 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017759092459608047		[learning rate: 0.0050208]
	Learning Rate: 0.00502076
	LOSS [training: 0.017759092459608047 | validation: 0.027776417457420738]
	TIME [epoch: 14.9 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05095482858067278		[learning rate: 0.0050089]
	Learning Rate: 0.00500891
	LOSS [training: 0.05095482858067278 | validation: 0.05531117769952418]
	TIME [epoch: 15 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023372675600354817		[learning rate: 0.0049971]
	Learning Rate: 0.0049971
	LOSS [training: 0.023372675600354817 | validation: 0.011029089724727794]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_344.pth
	Model improved!!!
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01446486594382243		[learning rate: 0.0049853]
	Learning Rate: 0.00498531
	LOSS [training: 0.01446486594382243 | validation: 0.025478664876308604]
	TIME [epoch: 15 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022883847992037912		[learning rate: 0.0049736]
	Learning Rate: 0.00497355
	LOSS [training: 0.022883847992037912 | validation: 0.03743216315326477]
	TIME [epoch: 15 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033953652688049084		[learning rate: 0.0049618]
	Learning Rate: 0.00496182
	LOSS [training: 0.033953652688049084 | validation: 0.01362309516827746]
	TIME [epoch: 14.9 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018764953157998882		[learning rate: 0.0049501]
	Learning Rate: 0.00495012
	LOSS [training: 0.018764953157998882 | validation: 0.02245647545870767]
	TIME [epoch: 15 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019876305861587184		[learning rate: 0.0049384]
	Learning Rate: 0.00493844
	LOSS [training: 0.019876305861587184 | validation: 0.013266887575191231]
	TIME [epoch: 14.9 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021654475879697974		[learning rate: 0.0049268]
	Learning Rate: 0.00492679
	LOSS [training: 0.021654475879697974 | validation: 0.0278374902619712]
	TIME [epoch: 15 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026650321412791927		[learning rate: 0.0049152]
	Learning Rate: 0.00491517
	LOSS [training: 0.026650321412791927 | validation: 0.03132885619647049]
	TIME [epoch: 15 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025792554249763983		[learning rate: 0.0049036]
	Learning Rate: 0.00490357
	LOSS [training: 0.025792554249763983 | validation: 0.027748124175490295]
	TIME [epoch: 14.9 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028798460677218536		[learning rate: 0.004892]
	Learning Rate: 0.00489201
	LOSS [training: 0.028798460677218536 | validation: 0.03380367125476125]
	TIME [epoch: 15.1 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030116403556306525		[learning rate: 0.0048805]
	Learning Rate: 0.00488047
	LOSS [training: 0.030116403556306525 | validation: 0.01468971463497406]
	TIME [epoch: 15 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013607414675488445		[learning rate: 0.004869]
	Learning Rate: 0.00486896
	LOSS [training: 0.013607414675488445 | validation: 0.010503979653936426]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013001965557456537		[learning rate: 0.0048575]
	Learning Rate: 0.00485747
	LOSS [training: 0.013001965557456537 | validation: 0.050885744067244304]
	TIME [epoch: 15 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03805857788870311		[learning rate: 0.004846]
	Learning Rate: 0.00484601
	LOSS [training: 0.03805857788870311 | validation: 0.018582284983570296]
	TIME [epoch: 15 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02641691025743783		[learning rate: 0.0048346]
	Learning Rate: 0.00483458
	LOSS [training: 0.02641691025743783 | validation: 0.01628486019591876]
	TIME [epoch: 15 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027060947341014274		[learning rate: 0.0048232]
	Learning Rate: 0.00482318
	LOSS [training: 0.027060947341014274 | validation: 0.03132444664416098]
	TIME [epoch: 15 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02487558239098911		[learning rate: 0.0048118]
	Learning Rate: 0.0048118
	LOSS [training: 0.02487558239098911 | validation: 0.02296549397584706]
	TIME [epoch: 15 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015420424910594395		[learning rate: 0.0048005]
	Learning Rate: 0.00480045
	LOSS [training: 0.015420424910594395 | validation: 0.013484819793529642]
	TIME [epoch: 15 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013466775353930378		[learning rate: 0.0047891]
	Learning Rate: 0.00478913
	LOSS [training: 0.013466775353930378 | validation: 0.012712803484391015]
	TIME [epoch: 14.9 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07351378217286705		[learning rate: 0.0047778]
	Learning Rate: 0.00477783
	LOSS [training: 0.07351378217286705 | validation: 0.03631102305427259]
	TIME [epoch: 15 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029247763364215876		[learning rate: 0.0047666]
	Learning Rate: 0.00476656
	LOSS [training: 0.029247763364215876 | validation: 0.013868671576656442]
	TIME [epoch: 15 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015478375229376305		[learning rate: 0.0047553]
	Learning Rate: 0.00475532
	LOSS [training: 0.015478375229376305 | validation: 0.01062364082763841]
	TIME [epoch: 15 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01278223655801488		[learning rate: 0.0047441]
	Learning Rate: 0.0047441
	LOSS [training: 0.01278223655801488 | validation: 0.012116744554475756]
	TIME [epoch: 15 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01581100560123345		[learning rate: 0.0047329]
	Learning Rate: 0.00473291
	LOSS [training: 0.01581100560123345 | validation: 0.028218748162440792]
	TIME [epoch: 14.9 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027823072852581173		[learning rate: 0.0047217]
	Learning Rate: 0.00472175
	LOSS [training: 0.027823072852581173 | validation: 0.011182399003715203]
	TIME [epoch: 14.9 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012368715546542038		[learning rate: 0.0047106]
	Learning Rate: 0.00471061
	LOSS [training: 0.012368715546542038 | validation: 0.020923297503744512]
	TIME [epoch: 15 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03239156736540097		[learning rate: 0.0046995]
	Learning Rate: 0.0046995
	LOSS [training: 0.03239156736540097 | validation: 0.015746024178969503]
	TIME [epoch: 15 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01607045596093778		[learning rate: 0.0046884]
	Learning Rate: 0.00468841
	LOSS [training: 0.01607045596093778 | validation: 0.011996610539326606]
	TIME [epoch: 15 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013469051576906596		[learning rate: 0.0046774]
	Learning Rate: 0.00467735
	LOSS [training: 0.013469051576906596 | validation: 0.027181353932386337]
	TIME [epoch: 15 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022585565625630268		[learning rate: 0.0046663]
	Learning Rate: 0.00466632
	LOSS [training: 0.022585565625630268 | validation: 0.011647463282110523]
	TIME [epoch: 15 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017320675846741428		[learning rate: 0.0046553]
	Learning Rate: 0.00465531
	LOSS [training: 0.017320675846741428 | validation: 0.03323368837238769]
	TIME [epoch: 15 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019234405295551097		[learning rate: 0.0046443]
	Learning Rate: 0.00464433
	LOSS [training: 0.019234405295551097 | validation: 0.01427056131592765]
	TIME [epoch: 15 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02071207574015212		[learning rate: 0.0046334]
	Learning Rate: 0.00463338
	LOSS [training: 0.02071207574015212 | validation: 0.057388500408185106]
	TIME [epoch: 15 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03668333949754149		[learning rate: 0.0046224]
	Learning Rate: 0.00462245
	LOSS [training: 0.03668333949754149 | validation: 0.013009525356061303]
	TIME [epoch: 15 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01581534120389916		[learning rate: 0.0046115]
	Learning Rate: 0.00461154
	LOSS [training: 0.01581534120389916 | validation: 0.029831420874559353]
	TIME [epoch: 15 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027448506559015186		[learning rate: 0.0046007]
	Learning Rate: 0.00460066
	LOSS [training: 0.027448506559015186 | validation: 0.014224492330603139]
	TIME [epoch: 15 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019242170023033252		[learning rate: 0.0045898]
	Learning Rate: 0.00458981
	LOSS [training: 0.019242170023033252 | validation: 0.01353052800093984]
	TIME [epoch: 15 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013303750853074173		[learning rate: 0.004579]
	Learning Rate: 0.00457899
	LOSS [training: 0.013303750853074173 | validation: 0.011953910076896423]
	TIME [epoch: 15 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020860047590010294		[learning rate: 0.0045682]
	Learning Rate: 0.00456818
	LOSS [training: 0.020860047590010294 | validation: 0.012697772799431907]
	TIME [epoch: 15 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031684150833471834		[learning rate: 0.0045574]
	Learning Rate: 0.00455741
	LOSS [training: 0.031684150833471834 | validation: 0.015804991159294974]
	TIME [epoch: 14.9 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11109686526882927		[learning rate: 0.0045467]
	Learning Rate: 0.00454666
	LOSS [training: 0.11109686526882927 | validation: 0.05042184739616592]
	TIME [epoch: 15 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039244576316826214		[learning rate: 0.0045359]
	Learning Rate: 0.00453593
	LOSS [training: 0.039244576316826214 | validation: 0.020194120866914224]
	TIME [epoch: 15 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01504271786570284		[learning rate: 0.0045252]
	Learning Rate: 0.00452523
	LOSS [training: 0.01504271786570284 | validation: 0.010806231348174464]
	TIME [epoch: 14.9 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0120219981752267		[learning rate: 0.0045146]
	Learning Rate: 0.00451456
	LOSS [training: 0.0120219981752267 | validation: 0.008531253000496203]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010092907366337353		[learning rate: 0.0045039]
	Learning Rate: 0.00450391
	LOSS [training: 0.010092907366337353 | validation: 0.011276492503420353]
	TIME [epoch: 15 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011306104392049023		[learning rate: 0.0044933]
	Learning Rate: 0.00449329
	LOSS [training: 0.011306104392049023 | validation: 0.011269129256379129]
	TIME [epoch: 15 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03345621268413459		[learning rate: 0.0044827]
	Learning Rate: 0.00448269
	LOSS [training: 0.03345621268413459 | validation: 0.01500777135398067]
	TIME [epoch: 15 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015309350140248278		[learning rate: 0.0044721]
	Learning Rate: 0.00447211
	LOSS [training: 0.015309350140248278 | validation: 0.013356113345083864]
	TIME [epoch: 15 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014044724598545893		[learning rate: 0.0044616]
	Learning Rate: 0.00446156
	LOSS [training: 0.014044724598545893 | validation: 0.013128441688135872]
	TIME [epoch: 15 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018198260336221735		[learning rate: 0.004451]
	Learning Rate: 0.00445104
	LOSS [training: 0.018198260336221735 | validation: 0.029365815012003967]
	TIME [epoch: 15 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024362146837126016		[learning rate: 0.0044405]
	Learning Rate: 0.00444054
	LOSS [training: 0.024362146837126016 | validation: 0.012245317022199428]
	TIME [epoch: 15 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020975063483575487		[learning rate: 0.0044301]
	Learning Rate: 0.00443007
	LOSS [training: 0.020975063483575487 | validation: 0.02777178264674257]
	TIME [epoch: 15 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02558792912033278		[learning rate: 0.0044196]
	Learning Rate: 0.00441962
	LOSS [training: 0.02558792912033278 | validation: 0.013502066441541762]
	TIME [epoch: 15 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0329274885019286		[learning rate: 0.0044092]
	Learning Rate: 0.00440919
	LOSS [training: 0.0329274885019286 | validation: 0.015708649499150025]
	TIME [epoch: 14.9 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016260047810720317		[learning rate: 0.0043988]
	Learning Rate: 0.00439879
	LOSS [training: 0.016260047810720317 | validation: 0.009784843798492163]
	TIME [epoch: 15 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009527644393784323		[learning rate: 0.0043884]
	Learning Rate: 0.00438841
	LOSS [training: 0.009527644393784323 | validation: 0.012255621956679218]
	TIME [epoch: 14.9 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01928843053952964		[learning rate: 0.0043781]
	Learning Rate: 0.00437806
	LOSS [training: 0.01928843053952964 | validation: 0.033083335667201286]
	TIME [epoch: 15 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03297192779980058		[learning rate: 0.0043677]
	Learning Rate: 0.00436774
	LOSS [training: 0.03297192779980058 | validation: 0.028963100863403486]
	TIME [epoch: 14.9 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03647477145129106		[learning rate: 0.0043574]
	Learning Rate: 0.00435743
	LOSS [training: 0.03647477145129106 | validation: 0.023267510875559973]
	TIME [epoch: 14.8 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0174424270178687		[learning rate: 0.0043472]
	Learning Rate: 0.00434715
	LOSS [training: 0.0174424270178687 | validation: 0.01072510306205167]
	TIME [epoch: 14.9 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012062466052277891		[learning rate: 0.0043369]
	Learning Rate: 0.0043369
	LOSS [training: 0.012062466052277891 | validation: 0.010987667617442205]
	TIME [epoch: 14.8 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013044078280407148		[learning rate: 0.0043267]
	Learning Rate: 0.00432667
	LOSS [training: 0.013044078280407148 | validation: 0.023198150581537257]
	TIME [epoch: 14.9 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03054544365246196		[learning rate: 0.0043165]
	Learning Rate: 0.00431646
	LOSS [training: 0.03054544365246196 | validation: 0.013807450372185535]
	TIME [epoch: 14.9 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01307160251848386		[learning rate: 0.0043063]
	Learning Rate: 0.00430628
	LOSS [training: 0.01307160251848386 | validation: 0.011386709954133774]
	TIME [epoch: 14.8 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017799194765632746		[learning rate: 0.0042961]
	Learning Rate: 0.00429612
	LOSS [training: 0.017799194765632746 | validation: 0.01241782779114671]
	TIME [epoch: 14.9 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013336235893284958		[learning rate: 0.004286]
	Learning Rate: 0.00428599
	LOSS [training: 0.013336235893284958 | validation: 0.01053140318253059]
	TIME [epoch: 14.9 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03161088333967496		[learning rate: 0.0042759]
	Learning Rate: 0.00427588
	LOSS [training: 0.03161088333967496 | validation: 0.03216095278897898]
	TIME [epoch: 14.9 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020841674432381704		[learning rate: 0.0042658]
	Learning Rate: 0.0042658
	LOSS [training: 0.020841674432381704 | validation: 0.020075339616567506]
	TIME [epoch: 14.9 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025578819358596197		[learning rate: 0.0042557]
	Learning Rate: 0.00425573
	LOSS [training: 0.025578819358596197 | validation: 0.06609096856794275]
	TIME [epoch: 14.8 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04289171043766783		[learning rate: 0.0042457]
	Learning Rate: 0.00424569
	LOSS [training: 0.04289171043766783 | validation: 0.018067155179698324]
	TIME [epoch: 14.9 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014063801152326534		[learning rate: 0.0042357]
	Learning Rate: 0.00423568
	LOSS [training: 0.014063801152326534 | validation: 0.00958512248730853]
	TIME [epoch: 14.9 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014933392518718992		[learning rate: 0.0042257]
	Learning Rate: 0.00422569
	LOSS [training: 0.014933392518718992 | validation: 0.012719458851433352]
	TIME [epoch: 14.9 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012238414852935805		[learning rate: 0.0042157]
	Learning Rate: 0.00421572
	LOSS [training: 0.012238414852935805 | validation: 0.0155067666444079]
	TIME [epoch: 14.9 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012122596716039649		[learning rate: 0.0042058]
	Learning Rate: 0.00420578
	LOSS [training: 0.012122596716039649 | validation: 0.01513062924324415]
	TIME [epoch: 14.9 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020166949277965283		[learning rate: 0.0041959]
	Learning Rate: 0.00419585
	LOSS [training: 0.020166949277965283 | validation: 0.016886723323493908]
	TIME [epoch: 14.9 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016981616858635036		[learning rate: 0.004186]
	Learning Rate: 0.00418596
	LOSS [training: 0.016981616858635036 | validation: 0.020671498635676362]
	TIME [epoch: 14.9 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03693645900084097		[learning rate: 0.0041761]
	Learning Rate: 0.00417608
	LOSS [training: 0.03693645900084097 | validation: 0.06606730482365696]
	TIME [epoch: 14.8 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03853692431250197		[learning rate: 0.0041662]
	Learning Rate: 0.00416623
	LOSS [training: 0.03853692431250197 | validation: 0.024763241561692788]
	TIME [epoch: 14.9 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01987447755034002		[learning rate: 0.0041564]
	Learning Rate: 0.00415641
	LOSS [training: 0.01987447755034002 | validation: 0.016695470533214764]
	TIME [epoch: 14.9 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033400491175647626		[learning rate: 0.0041466]
	Learning Rate: 0.0041466
	LOSS [training: 0.033400491175647626 | validation: 0.018199973883671866]
	TIME [epoch: 14.9 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019033111186852317		[learning rate: 0.0041368]
	Learning Rate: 0.00413682
	LOSS [training: 0.019033111186852317 | validation: 0.014261278224396382]
	TIME [epoch: 14.9 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016209442743197987		[learning rate: 0.0041271]
	Learning Rate: 0.00412706
	LOSS [training: 0.016209442743197987 | validation: 0.011014919953878507]
	TIME [epoch: 14.9 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011532285629851455		[learning rate: 0.0041173]
	Learning Rate: 0.00411733
	LOSS [training: 0.011532285629851455 | validation: 0.016312337306923337]
	TIME [epoch: 14.9 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01875328770762127		[learning rate: 0.0041076]
	Learning Rate: 0.00410762
	LOSS [training: 0.01875328770762127 | validation: 0.019933487334719096]
	TIME [epoch: 14.9 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02014868274307799		[learning rate: 0.0040979]
	Learning Rate: 0.00409793
	LOSS [training: 0.02014868274307799 | validation: 0.01507699455145441]
	TIME [epoch: 14.9 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012433234712700759		[learning rate: 0.0040883]
	Learning Rate: 0.00408826
	LOSS [training: 0.012433234712700759 | validation: 0.010411412107468992]
	TIME [epoch: 14.9 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013629893577479835		[learning rate: 0.0040786]
	Learning Rate: 0.00407862
	LOSS [training: 0.013629893577479835 | validation: 0.061954618746370343]
	TIME [epoch: 14.9 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03549112195878714		[learning rate: 0.004069]
	Learning Rate: 0.004069
	LOSS [training: 0.03549112195878714 | validation: 0.01778085391494122]
	TIME [epoch: 14.9 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013477277897943618		[learning rate: 0.0040594]
	Learning Rate: 0.0040594
	LOSS [training: 0.013477277897943618 | validation: 0.009019139254080731]
	TIME [epoch: 14.9 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012115162558552653		[learning rate: 0.0040498]
	Learning Rate: 0.00404982
	LOSS [training: 0.012115162558552653 | validation: 0.016921444731724068]
	TIME [epoch: 14.9 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025730387916440875		[learning rate: 0.0040403]
	Learning Rate: 0.00404027
	LOSS [training: 0.025730387916440875 | validation: 0.031517314226408286]
	TIME [epoch: 14.9 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024067037662135395		[learning rate: 0.0040307]
	Learning Rate: 0.00403074
	LOSS [training: 0.024067037662135395 | validation: 0.013282422874719509]
	TIME [epoch: 14.9 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01787231978346058		[learning rate: 0.0040212]
	Learning Rate: 0.00402123
	LOSS [training: 0.01787231978346058 | validation: 0.012864640687269974]
	TIME [epoch: 14.9 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015614485424837532		[learning rate: 0.0040117]
	Learning Rate: 0.00401175
	LOSS [training: 0.015614485424837532 | validation: 0.014315223329311146]
	TIME [epoch: 14.9 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01571360621267691		[learning rate: 0.0040023]
	Learning Rate: 0.00400228
	LOSS [training: 0.01571360621267691 | validation: 0.012557486452328825]
	TIME [epoch: 15 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015505944843000576		[learning rate: 0.0039928]
	Learning Rate: 0.00399284
	LOSS [training: 0.015505944843000576 | validation: 0.01827343131939773]
	TIME [epoch: 14.9 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024225363871748194		[learning rate: 0.0039834]
	Learning Rate: 0.00398342
	LOSS [training: 0.024225363871748194 | validation: 0.013495089839804617]
	TIME [epoch: 15 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027869385489809367		[learning rate: 0.003974]
	Learning Rate: 0.00397403
	LOSS [training: 0.027869385489809367 | validation: 0.026853168120035402]
	TIME [epoch: 14.9 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02125722906662128		[learning rate: 0.0039647]
	Learning Rate: 0.00396465
	LOSS [training: 0.02125722906662128 | validation: 0.01102306897613257]
	TIME [epoch: 14.9 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01037237325289421		[learning rate: 0.0039553]
	Learning Rate: 0.0039553
	LOSS [training: 0.01037237325289421 | validation: 0.012367304071163418]
	TIME [epoch: 15 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012762357757665734		[learning rate: 0.003946]
	Learning Rate: 0.00394597
	LOSS [training: 0.012762357757665734 | validation: 0.011455878466865216]
	TIME [epoch: 14.9 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01755085998435752		[learning rate: 0.0039367]
	Learning Rate: 0.00393666
	LOSS [training: 0.01755085998435752 | validation: 0.011668752755689897]
	TIME [epoch: 14.9 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017632794543426415		[learning rate: 0.0039274]
	Learning Rate: 0.00392738
	LOSS [training: 0.017632794543426415 | validation: 0.012306732055121254]
	TIME [epoch: 15 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009598104515027283		[learning rate: 0.0039181]
	Learning Rate: 0.00391811
	LOSS [training: 0.009598104515027283 | validation: 0.008866387939049527]
	TIME [epoch: 14.9 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027762956371201367		[learning rate: 0.0039089]
	Learning Rate: 0.00390887
	LOSS [training: 0.027762956371201367 | validation: 0.017972503714556262]
	TIME [epoch: 15 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012130911573354887		[learning rate: 0.0038996]
	Learning Rate: 0.00389965
	LOSS [training: 0.012130911573354887 | validation: 0.009509887495038992]
	TIME [epoch: 14.9 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017731590191334773		[learning rate: 0.0038905]
	Learning Rate: 0.00389045
	LOSS [training: 0.017731590191334773 | validation: 0.020485876493475275]
	TIME [epoch: 14.9 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01455385773054924		[learning rate: 0.0038813]
	Learning Rate: 0.00388127
	LOSS [training: 0.01455385773054924 | validation: 0.0113785034465306]
	TIME [epoch: 15 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035442786098744856		[learning rate: 0.0038721]
	Learning Rate: 0.00387212
	LOSS [training: 0.035442786098744856 | validation: 0.030771078325893046]
	TIME [epoch: 14.9 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028899666339479938		[learning rate: 0.003863]
	Learning Rate: 0.00386299
	LOSS [training: 0.028899666339479938 | validation: 0.020623839522150453]
	TIME [epoch: 15 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012480270138332132		[learning rate: 0.0038539]
	Learning Rate: 0.00385387
	LOSS [training: 0.012480270138332132 | validation: 0.011932088201169921]
	TIME [epoch: 14.9 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011351105977609539		[learning rate: 0.0038448]
	Learning Rate: 0.00384478
	LOSS [training: 0.011351105977609539 | validation: 0.009194361083302944]
	TIME [epoch: 14.9 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010034765544669949		[learning rate: 0.0038357]
	Learning Rate: 0.00383571
	LOSS [training: 0.010034765544669949 | validation: 0.01130755990275191]
	TIME [epoch: 15 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015851280028270937		[learning rate: 0.0038267]
	Learning Rate: 0.00382667
	LOSS [training: 0.015851280028270937 | validation: 0.01320191996971921]
	TIME [epoch: 14.9 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021401468470594594		[learning rate: 0.0038176]
	Learning Rate: 0.00381764
	LOSS [training: 0.021401468470594594 | validation: 0.0176903882078299]
	TIME [epoch: 14.9 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018671543371656073		[learning rate: 0.0038086]
	Learning Rate: 0.00380863
	LOSS [training: 0.018671543371656073 | validation: 0.011903805390689678]
	TIME [epoch: 15 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01547437480731582		[learning rate: 0.0037996]
	Learning Rate: 0.00379965
	LOSS [training: 0.01547437480731582 | validation: 0.007961672687708851]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015045928400446166		[learning rate: 0.0037907]
	Learning Rate: 0.00379069
	LOSS [training: 0.015045928400446166 | validation: 0.06492011003392874]
	TIME [epoch: 15 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041523725092082416		[learning rate: 0.0037817]
	Learning Rate: 0.00378175
	LOSS [training: 0.041523725092082416 | validation: 0.02423124840147185]
	TIME [epoch: 14.9 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019412594084170856		[learning rate: 0.0037728]
	Learning Rate: 0.00377283
	LOSS [training: 0.019412594084170856 | validation: 0.011379950959630118]
	TIME [epoch: 14.9 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010807989529549368		[learning rate: 0.0037639]
	Learning Rate: 0.00376393
	LOSS [training: 0.010807989529549368 | validation: 0.024461586760079777]
	TIME [epoch: 15 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02076523468448132		[learning rate: 0.003755]
	Learning Rate: 0.00375505
	LOSS [training: 0.02076523468448132 | validation: 0.008897891646636258]
	TIME [epoch: 14.9 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011170164541316464		[learning rate: 0.0037462]
	Learning Rate: 0.00374619
	LOSS [training: 0.011170164541316464 | validation: 0.012660033468800253]
	TIME [epoch: 14.9 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010559655856604695		[learning rate: 0.0037374]
	Learning Rate: 0.00373735
	LOSS [training: 0.010559655856604695 | validation: 0.019342076096658816]
	TIME [epoch: 14.9 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02511362984741544		[learning rate: 0.0037285]
	Learning Rate: 0.00372854
	LOSS [training: 0.02511362984741544 | validation: 0.024186798464749675]
	TIME [epoch: 14.9 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01857067881368877		[learning rate: 0.0037197]
	Learning Rate: 0.00371974
	LOSS [training: 0.01857067881368877 | validation: 0.010579237748368994]
	TIME [epoch: 14.9 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009188633361152321		[learning rate: 0.003711]
	Learning Rate: 0.00371097
	LOSS [training: 0.009188633361152321 | validation: 0.011521509721728329]
	TIME [epoch: 14.9 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04886876459233724		[learning rate: 0.0037022]
	Learning Rate: 0.00370221
	LOSS [training: 0.04886876459233724 | validation: 0.05040291141007924]
	TIME [epoch: 14.9 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028892728426670813		[learning rate: 0.0036935]
	Learning Rate: 0.00369348
	LOSS [training: 0.028892728426670813 | validation: 0.015093164909882283]
	TIME [epoch: 15 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013779540093909278		[learning rate: 0.0036848]
	Learning Rate: 0.00368477
	LOSS [training: 0.013779540093909278 | validation: 0.011827531962819205]
	TIME [epoch: 14.9 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01026170211027689		[learning rate: 0.0036761]
	Learning Rate: 0.00367608
	LOSS [training: 0.01026170211027689 | validation: 0.012964039131939092]
	TIME [epoch: 14.9 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011986520060741763		[learning rate: 0.0036674]
	Learning Rate: 0.00366741
	LOSS [training: 0.011986520060741763 | validation: 0.015365530136362465]
	TIME [epoch: 14.9 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010211530948888003		[learning rate: 0.0036588]
	Learning Rate: 0.00365875
	LOSS [training: 0.010211530948888003 | validation: 0.0064010930621976846]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015585864309967577		[learning rate: 0.0036501]
	Learning Rate: 0.00365012
	LOSS [training: 0.015585864309967577 | validation: 0.011354162567277913]
	TIME [epoch: 15 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02039352021715446		[learning rate: 0.0036415]
	Learning Rate: 0.00364151
	LOSS [training: 0.02039352021715446 | validation: 0.023518558722495635]
	TIME [epoch: 14.9 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023649961274487746		[learning rate: 0.0036329]
	Learning Rate: 0.00363292
	LOSS [training: 0.023649961274487746 | validation: 0.011909622960670841]
	TIME [epoch: 14.9 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01017741414378587		[learning rate: 0.0036244]
	Learning Rate: 0.00362436
	LOSS [training: 0.01017741414378587 | validation: 0.010778136788565712]
	TIME [epoch: 14.9 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010669109605899944		[learning rate: 0.0036158]
	Learning Rate: 0.00361581
	LOSS [training: 0.010669109605899944 | validation: 0.011567150386516659]
	TIME [epoch: 14.9 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015562681242153032		[learning rate: 0.0036073]
	Learning Rate: 0.00360728
	LOSS [training: 0.015562681242153032 | validation: 0.013267583074743855]
	TIME [epoch: 14.9 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01915306721737005		[learning rate: 0.0035988]
	Learning Rate: 0.00359877
	LOSS [training: 0.01915306721737005 | validation: 0.018961679666566957]
	TIME [epoch: 14.9 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017538738431645093		[learning rate: 0.0035903]
	Learning Rate: 0.00359028
	LOSS [training: 0.017538738431645093 | validation: 0.028718109295631386]
	TIME [epoch: 14.9 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014826042871083925		[learning rate: 0.0035818]
	Learning Rate: 0.00358181
	LOSS [training: 0.014826042871083925 | validation: 0.00885406042259344]
	TIME [epoch: 14.9 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010757389956395781		[learning rate: 0.0035734]
	Learning Rate: 0.00357336
	LOSS [training: 0.010757389956395781 | validation: 0.014667064215248216]
	TIME [epoch: 14.9 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015702082907233114		[learning rate: 0.0035649]
	Learning Rate: 0.00356493
	LOSS [training: 0.015702082907233114 | validation: 0.010918886446225996]
	TIME [epoch: 14.9 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014587653440330736		[learning rate: 0.0035565]
	Learning Rate: 0.00355652
	LOSS [training: 0.014587653440330736 | validation: 0.019621023192881046]
	TIME [epoch: 15 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03443502689348825		[learning rate: 0.0035481]
	Learning Rate: 0.00354813
	LOSS [training: 0.03443502689348825 | validation: 0.023846232824165918]
	TIME [epoch: 14.9 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014971731899041803		[learning rate: 0.0035398]
	Learning Rate: 0.00353976
	LOSS [training: 0.014971731899041803 | validation: 0.015369834468223856]
	TIME [epoch: 14.9 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012363886744231323		[learning rate: 0.0035314]
	Learning Rate: 0.00353141
	LOSS [training: 0.012363886744231323 | validation: 0.008963479592512064]
	TIME [epoch: 14.9 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022987102417812617		[learning rate: 0.0035231]
	Learning Rate: 0.00352308
	LOSS [training: 0.022987102417812617 | validation: 0.008983488740629375]
	TIME [epoch: 14.9 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011162771144324376		[learning rate: 0.0035148]
	Learning Rate: 0.00351477
	LOSS [training: 0.011162771144324376 | validation: 0.013104043358964017]
	TIME [epoch: 14.9 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016272817587096478		[learning rate: 0.0035065]
	Learning Rate: 0.00350648
	LOSS [training: 0.016272817587096478 | validation: 0.010283731325925546]
	TIME [epoch: 14.9 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009476652107008873		[learning rate: 0.0034982]
	Learning Rate: 0.00349821
	LOSS [training: 0.009476652107008873 | validation: 0.012073919397000688]
	TIME [epoch: 14.9 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012131759141157432		[learning rate: 0.00349]
	Learning Rate: 0.00348996
	LOSS [training: 0.012131759141157432 | validation: 0.013449938973356867]
	TIME [epoch: 14.9 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013558905517984183		[learning rate: 0.0034817]
	Learning Rate: 0.00348173
	LOSS [training: 0.013558905517984183 | validation: 0.011200112362058819]
	TIME [epoch: 14.9 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01014777338503747		[learning rate: 0.0034735]
	Learning Rate: 0.00347352
	LOSS [training: 0.01014777338503747 | validation: 0.012104510648680736]
	TIME [epoch: 14.9 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017548245708116965		[learning rate: 0.0034653]
	Learning Rate: 0.00346532
	LOSS [training: 0.017548245708116965 | validation: 0.012132613472947215]
	TIME [epoch: 14.9 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011589961172096624		[learning rate: 0.0034571]
	Learning Rate: 0.00345715
	LOSS [training: 0.011589961172096624 | validation: 0.02423633959978685]
	TIME [epoch: 14.9 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0204484192038493		[learning rate: 0.003449]
	Learning Rate: 0.00344899
	LOSS [training: 0.0204484192038493 | validation: 0.014544313506493083]
	TIME [epoch: 149 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014219428914266845		[learning rate: 0.0034409]
	Learning Rate: 0.00344086
	LOSS [training: 0.014219428914266845 | validation: 0.053493975455021804]
	TIME [epoch: 32.4 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026307548604314265		[learning rate: 0.0034327]
	Learning Rate: 0.00343274
	LOSS [training: 0.026307548604314265 | validation: 0.015297857079274464]
	TIME [epoch: 32.4 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01810035953247028		[learning rate: 0.0034246]
	Learning Rate: 0.00342464
	LOSS [training: 0.01810035953247028 | validation: 0.009249220352455614]
	TIME [epoch: 32.4 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008949815858733978		[learning rate: 0.0034166]
	Learning Rate: 0.00341657
	LOSS [training: 0.008949815858733978 | validation: 0.009249895089750658]
	TIME [epoch: 32.4 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009650032198432525		[learning rate: 0.0034085]
	Learning Rate: 0.00340851
	LOSS [training: 0.009650032198432525 | validation: 0.009712615750858794]
	TIME [epoch: 32.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013009784710168138		[learning rate: 0.0034005]
	Learning Rate: 0.00340047
	LOSS [training: 0.013009784710168138 | validation: 0.018158305498381523]
	TIME [epoch: 32.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029488930588079998		[learning rate: 0.0033924]
	Learning Rate: 0.00339244
	LOSS [training: 0.029488930588079998 | validation: 0.01918470281995495]
	TIME [epoch: 32.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012464146272845877		[learning rate: 0.0033844]
	Learning Rate: 0.00338444
	LOSS [training: 0.012464146272845877 | validation: 0.008098176669200215]
	TIME [epoch: 32.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010501657751225205		[learning rate: 0.0033765]
	Learning Rate: 0.00337646
	LOSS [training: 0.010501657751225205 | validation: 0.013204387947633501]
	TIME [epoch: 32.4 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020072853890085528		[learning rate: 0.0033685]
	Learning Rate: 0.0033685
	LOSS [training: 0.020072853890085528 | validation: 0.011971438184999877]
	TIME [epoch: 32.4 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010564413859772033		[learning rate: 0.0033605]
	Learning Rate: 0.00336055
	LOSS [training: 0.010564413859772033 | validation: 0.007639309948189563]
	TIME [epoch: 32.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007836418651561325		[learning rate: 0.0033526]
	Learning Rate: 0.00335262
	LOSS [training: 0.007836418651561325 | validation: 0.00751864636839351]
	TIME [epoch: 32.4 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013821801483598836		[learning rate: 0.0033447]
	Learning Rate: 0.00334471
	LOSS [training: 0.013821801483598836 | validation: 0.009087823324378491]
	TIME [epoch: 32.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009303216045555592		[learning rate: 0.0033368]
	Learning Rate: 0.00333682
	LOSS [training: 0.009303216045555592 | validation: 0.007134040099222473]
	TIME [epoch: 32.4 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016006683006988816		[learning rate: 0.003329]
	Learning Rate: 0.00332895
	LOSS [training: 0.016006683006988816 | validation: 0.013166782620107433]
	TIME [epoch: 32.4 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022495856260457518		[learning rate: 0.0033211]
	Learning Rate: 0.0033211
	LOSS [training: 0.022495856260457518 | validation: 0.017157275131241646]
	TIME [epoch: 32.4 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013088589621048618		[learning rate: 0.0033133]
	Learning Rate: 0.00331327
	LOSS [training: 0.013088589621048618 | validation: 0.008183723443334397]
	TIME [epoch: 32.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008693027940079267		[learning rate: 0.0033055]
	Learning Rate: 0.00330545
	LOSS [training: 0.008693027940079267 | validation: 0.008422840288216845]
	TIME [epoch: 32.5 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010148141201311314		[learning rate: 0.0032977]
	Learning Rate: 0.00329765
	LOSS [training: 0.010148141201311314 | validation: 0.014360846912029285]
	TIME [epoch: 32.4 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02319328266958427		[learning rate: 0.0032899]
	Learning Rate: 0.00328988
	LOSS [training: 0.02319328266958427 | validation: 0.01599897880975285]
	TIME [epoch: 32.4 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013681908576175886		[learning rate: 0.0032821]
	Learning Rate: 0.00328212
	LOSS [training: 0.013681908576175886 | validation: 0.009076853085816952]
	TIME [epoch: 32.5 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010129596641537343		[learning rate: 0.0032744]
	Learning Rate: 0.00327437
	LOSS [training: 0.010129596641537343 | validation: 0.007986678096074163]
	TIME [epoch: 32.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014017306437306809		[learning rate: 0.0032666]
	Learning Rate: 0.00326665
	LOSS [training: 0.014017306437306809 | validation: 0.011617729241139112]
	TIME [epoch: 32.5 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015089204433027813		[learning rate: 0.0032589]
	Learning Rate: 0.00325894
	LOSS [training: 0.015089204433027813 | validation: 0.01820944018821061]
	TIME [epoch: 32.4 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01495947414769597		[learning rate: 0.0032513]
	Learning Rate: 0.00325126
	LOSS [training: 0.01495947414769597 | validation: 0.0071879418104321465]
	TIME [epoch: 32.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01025988710620972		[learning rate: 0.0032436]
	Learning Rate: 0.00324359
	LOSS [training: 0.01025988710620972 | validation: 0.01050528084169235]
	TIME [epoch: 32.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015291411012301232		[learning rate: 0.0032359]
	Learning Rate: 0.00323594
	LOSS [training: 0.015291411012301232 | validation: 0.018027306194720198]
	TIME [epoch: 32.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017684721053068297		[learning rate: 0.0032283]
	Learning Rate: 0.0032283
	LOSS [training: 0.017684721053068297 | validation: 0.01174477083632416]
	TIME [epoch: 32.4 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02215478268931347		[learning rate: 0.0032207]
	Learning Rate: 0.00322069
	LOSS [training: 0.02215478268931347 | validation: 0.009438788804494811]
	TIME [epoch: 32.4 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014653062223213982		[learning rate: 0.0032131]
	Learning Rate: 0.00321309
	LOSS [training: 0.014653062223213982 | validation: 0.008554035748370452]
	TIME [epoch: 32.4 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009153653260945787		[learning rate: 0.0032055]
	Learning Rate: 0.00320551
	LOSS [training: 0.009153653260945787 | validation: 0.011051095204556587]
	TIME [epoch: 32.5 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015492710657601785		[learning rate: 0.003198]
	Learning Rate: 0.00319795
	LOSS [training: 0.015492710657601785 | validation: 0.010980745999203168]
	TIME [epoch: 32.5 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010289253537330546		[learning rate: 0.0031904]
	Learning Rate: 0.00319041
	LOSS [training: 0.010289253537330546 | validation: 0.011395975962408284]
	TIME [epoch: 32.4 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0109627280041363		[learning rate: 0.0031829]
	Learning Rate: 0.00318288
	LOSS [training: 0.0109627280041363 | validation: 0.02940762760728095]
	TIME [epoch: 32.4 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02115077953235013		[learning rate: 0.0031754]
	Learning Rate: 0.00317537
	LOSS [training: 0.02115077953235013 | validation: 0.010832387792967484]
	TIME [epoch: 32.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00910919225920374		[learning rate: 0.0031679]
	Learning Rate: 0.00316788
	LOSS [training: 0.00910919225920374 | validation: 0.011036568417543037]
	TIME [epoch: 32.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00941491887146308		[learning rate: 0.0031604]
	Learning Rate: 0.00316041
	LOSS [training: 0.00941491887146308 | validation: 0.009195125370093435]
	TIME [epoch: 32.4 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011376040857906857		[learning rate: 0.003153]
	Learning Rate: 0.00315296
	LOSS [training: 0.011376040857906857 | validation: 0.00794732074455144]
	TIME [epoch: 32.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012985490198600416		[learning rate: 0.0031455]
	Learning Rate: 0.00314552
	LOSS [training: 0.012985490198600416 | validation: 0.030236296522919433]
	TIME [epoch: 32.5 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019389466556774686		[learning rate: 0.0031381]
	Learning Rate: 0.0031381
	LOSS [training: 0.019389466556774686 | validation: 0.009762158564157533]
	TIME [epoch: 32.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009411594627580448		[learning rate: 0.0031307]
	Learning Rate: 0.0031307
	LOSS [training: 0.009411594627580448 | validation: 0.010031750423361675]
	TIME [epoch: 32.4 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009905102249561532		[learning rate: 0.0031233]
	Learning Rate: 0.00312331
	LOSS [training: 0.009905102249561532 | validation: 0.01749636252804239]
	TIME [epoch: 32.4 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011341608467424734		[learning rate: 0.0031159]
	Learning Rate: 0.00311594
	LOSS [training: 0.011341608467424734 | validation: 0.02018354418101824]
	TIME [epoch: 32.7 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020468932600214658		[learning rate: 0.0031086]
	Learning Rate: 0.0031086
	LOSS [training: 0.020468932600214658 | validation: 0.009716364387452568]
	TIME [epoch: 33 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008937841551178714		[learning rate: 0.0031013]
	Learning Rate: 0.00310126
	LOSS [training: 0.008937841551178714 | validation: 0.010219732972924252]
	TIME [epoch: 32.9 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009300251197818894		[learning rate: 0.0030939]
	Learning Rate: 0.00309395
	LOSS [training: 0.009300251197818894 | validation: 0.007141652115021278]
	TIME [epoch: 33 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071126952362503115		[learning rate: 0.0030866]
	Learning Rate: 0.00308665
	LOSS [training: 0.0071126952362503115 | validation: 0.007202584813193503]
	TIME [epoch: 33 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02079278265902454		[learning rate: 0.0030794]
	Learning Rate: 0.00307937
	LOSS [training: 0.02079278265902454 | validation: 0.010830561006702561]
	TIME [epoch: 32.9 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014025959143792635		[learning rate: 0.0030721]
	Learning Rate: 0.0030721
	LOSS [training: 0.014025959143792635 | validation: 0.00857378052809528]
	TIME [epoch: 33 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00790081146333661		[learning rate: 0.0030649]
	Learning Rate: 0.00306486
	LOSS [training: 0.00790081146333661 | validation: 0.007324413864165205]
	TIME [epoch: 33 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008816781698512095		[learning rate: 0.0030576]
	Learning Rate: 0.00305763
	LOSS [training: 0.008816781698512095 | validation: 0.007661423006762555]
	TIME [epoch: 33 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009452454909624167		[learning rate: 0.0030504]
	Learning Rate: 0.00305042
	LOSS [training: 0.009452454909624167 | validation: 0.006893152001234176]
	TIME [epoch: 33 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012529113836653592		[learning rate: 0.0030432]
	Learning Rate: 0.00304322
	LOSS [training: 0.012529113836653592 | validation: 0.00988689502436248]
	TIME [epoch: 33 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010074374360008262		[learning rate: 0.003036]
	Learning Rate: 0.00303604
	LOSS [training: 0.010074374360008262 | validation: 0.007225807698519257]
	TIME [epoch: 33 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01969815643834949		[learning rate: 0.0030289]
	Learning Rate: 0.00302888
	LOSS [training: 0.01969815643834949 | validation: 0.03129742258340123]
	TIME [epoch: 33 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01731200949601636		[learning rate: 0.0030217]
	Learning Rate: 0.00302174
	LOSS [training: 0.01731200949601636 | validation: 0.011509725413455798]
	TIME [epoch: 33 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017771779836444266		[learning rate: 0.0030146]
	Learning Rate: 0.00301461
	LOSS [training: 0.017771779836444266 | validation: 0.012455041648837121]
	TIME [epoch: 33 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010724840522559618		[learning rate: 0.0030075]
	Learning Rate: 0.0030075
	LOSS [training: 0.010724840522559618 | validation: 0.008881763794234597]
	TIME [epoch: 33 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009003987019978576		[learning rate: 0.0030004]
	Learning Rate: 0.0030004
	LOSS [training: 0.009003987019978576 | validation: 0.010022237716436263]
	TIME [epoch: 33 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017127405348339328		[learning rate: 0.0029933]
	Learning Rate: 0.00299332
	LOSS [training: 0.017127405348339328 | validation: 0.011586195694172765]
	TIME [epoch: 33 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012404841776469363		[learning rate: 0.0029863]
	Learning Rate: 0.00298626
	LOSS [training: 0.012404841776469363 | validation: 0.007078261252710487]
	TIME [epoch: 33 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01308371052616751		[learning rate: 0.0029792]
	Learning Rate: 0.00297922
	LOSS [training: 0.01308371052616751 | validation: 0.014172241930668888]
	TIME [epoch: 33 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024010803416002514		[learning rate: 0.0029722]
	Learning Rate: 0.00297219
	LOSS [training: 0.024010803416002514 | validation: 0.016786856592239698]
	TIME [epoch: 33 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015346734225586293		[learning rate: 0.0029652]
	Learning Rate: 0.00296518
	LOSS [training: 0.015346734225586293 | validation: 0.00917100267724285]
	TIME [epoch: 33.1 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008042371479664736		[learning rate: 0.0029582]
	Learning Rate: 0.00295819
	LOSS [training: 0.008042371479664736 | validation: 0.008903623218243534]
	TIME [epoch: 33.1 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00967381218251152		[learning rate: 0.0029512]
	Learning Rate: 0.00295121
	LOSS [training: 0.00967381218251152 | validation: 0.022014875155354607]
	TIME [epoch: 33.1 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020955120951418174		[learning rate: 0.0029442]
	Learning Rate: 0.00294425
	LOSS [training: 0.020955120951418174 | validation: 0.011376745090089645]
	TIME [epoch: 33 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009971749638721165		[learning rate: 0.0029373]
	Learning Rate: 0.0029373
	LOSS [training: 0.009971749638721165 | validation: 0.007422344335490022]
	TIME [epoch: 33 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00951526210914878		[learning rate: 0.0029304]
	Learning Rate: 0.00293037
	LOSS [training: 0.00951526210914878 | validation: 0.008554307439133992]
	TIME [epoch: 33 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008707086447947406		[learning rate: 0.0029235]
	Learning Rate: 0.00292346
	LOSS [training: 0.008707086447947406 | validation: 0.013005315020994975]
	TIME [epoch: 33.1 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017261317658752644		[learning rate: 0.0029166]
	Learning Rate: 0.00291657
	LOSS [training: 0.017261317658752644 | validation: 0.012403338188269207]
	TIME [epoch: 33.1 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012061435745082706		[learning rate: 0.0029097]
	Learning Rate: 0.00290969
	LOSS [training: 0.012061435745082706 | validation: 0.019266493985461337]
	TIME [epoch: 33 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013583015634754735		[learning rate: 0.0029028]
	Learning Rate: 0.00290282
	LOSS [training: 0.013583015634754735 | validation: 0.007134291633339544]
	TIME [epoch: 33 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008281542308063395		[learning rate: 0.002896]
	Learning Rate: 0.00289598
	LOSS [training: 0.008281542308063395 | validation: 0.007099114057919445]
	TIME [epoch: 33.1 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009956763508127037		[learning rate: 0.0028891]
	Learning Rate: 0.00288914
	LOSS [training: 0.009956763508127037 | validation: 0.014073463167543765]
	TIME [epoch: 33.1 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016719734537593567		[learning rate: 0.0028823]
	Learning Rate: 0.00288233
	LOSS [training: 0.016719734537593567 | validation: 0.011152113741130301]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd2_20240707_125342/states/model_phi2_1a_v_mmd2_577.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 8939.742 seconds.
