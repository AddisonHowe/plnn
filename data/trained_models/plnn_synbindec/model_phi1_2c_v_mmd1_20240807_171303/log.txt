Args:
Namespace(name='model_phi1_2c_v_mmd1', outdir='out/model_training/model_phi1_2c_v_mmd1', training_data='data/training_data/data_phi1_2c/training', validation_data='data/training_data/data_phi1_2c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2993930989

Training model...

Saving initial model state to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 2/2] avg loss: 6.052564909253934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.052564909253934 | validation: 5.7079800021910065]
	TIME [epoch: 94.4 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.7668608563826815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7668608563826815 | validation: 4.309675024997147]
	TIME [epoch: 7.01 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.481846272324671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.481846272324671 | validation: 5.610615725888195]
	TIME [epoch: 6.93 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.153929058224309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.153929058224309 | validation: 4.390432899479165]
	TIME [epoch: 6.94 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.4286943744717275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4286943744717275 | validation: 3.97114740447369]
	TIME [epoch: 6.93 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.004610814409431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.004610814409431 | validation: 3.824712800281846]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.130481073528032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.130481073528032 | validation: 3.8638752305232673]
	TIME [epoch: 6.94 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.9745171000884545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9745171000884545 | validation: 3.6569416405675614]
	TIME [epoch: 6.98 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.8258990898634786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8258990898634786 | validation: 3.5675233825805224]
	TIME [epoch: 6.97 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.672607851134181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.672607851134181 | validation: 3.5534337785412298]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.713772255917778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.713772255917778 | validation: 3.5388851174731784]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.7538351618793055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7538351618793055 | validation: 3.7407306937159337]
	TIME [epoch: 6.96 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.7970952591036493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7970952591036493 | validation: 3.682376010701759]
	TIME [epoch: 6.95 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.76681708947489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.76681708947489 | validation: 3.4699830571860506]
	TIME [epoch: 6.96 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.696498838757149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.696498838757149 | validation: 3.4379012356034644]
	TIME [epoch: 6.97 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.5963695838125282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5963695838125282 | validation: 3.363169612757956]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.5517339206980347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5517339206980347 | validation: 3.353996032370132]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.5417129914197076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5417129914197076 | validation: 3.340878659219015]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.579149745414989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.579149745414989 | validation: 3.447798440608153]
	TIME [epoch: 6.95 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.5950079038494653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5950079038494653 | validation: 3.2529452087893875]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.4667615569996695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4667615569996695 | validation: 3.290874227004778]
	TIME [epoch: 6.97 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.5002105494997737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5002105494997737 | validation: 3.1852014802201527]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.4032270973527536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4032270973527536 | validation: 3.159775922265376]
	TIME [epoch: 6.97 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.2598128353819122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2598128353819122 | validation: 3.1563809233781392]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.3310052901359115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3310052901359115 | validation: 3.172078191504603]
	TIME [epoch: 6.94 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.363680685771905		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.363680685771905 | validation: 3.2886268766550764]
	TIME [epoch: 6.95 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.318494368524231		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.318494368524231 | validation: 3.057822671148152]
	TIME [epoch: 6.98 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.368948841087495		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.368948841087495 | validation: 3.0408505050428536]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.3130465690612967		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.3130465690612967 | validation: 2.9518425237585486]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.1297487494678924		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.1297487494678924 | validation: 2.670771112541425]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.935034351023557		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.935034351023557 | validation: 2.9800953089935236]
	TIME [epoch: 6.95 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.0492949417998076		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 3.0492949417998076 | validation: 2.3167942635154226]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.5596867785721225		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.5596867785721225 | validation: 2.2483708478208193]
	TIME [epoch: 6.98 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.3156578461650783		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.3156578461650783 | validation: 1.9218797738295834]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.3930977459349174		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.3930977459349174 | validation: 1.6673330761927767]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.291658476944536		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.291658476944536 | validation: 1.7703590248789027]
	TIME [epoch: 6.96 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7733954124573503		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.7733954124573503 | validation: 1.6774708877690259]
	TIME [epoch: 6.95 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8362395528716164		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.8362395528716164 | validation: 1.4854473336989216]
	TIME [epoch: 6.96 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5022072041236139		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.5022072041236139 | validation: 1.4575806391513364]
	TIME [epoch: 6.98 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.449350815532508		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.449350815532508 | validation: 1.6356519843229762]
	TIME [epoch: 6.98 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6648590124006728		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.6648590124006728 | validation: 1.4126519890401448]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4010559023488816		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.4010559023488816 | validation: 1.369103087386271]
	TIME [epoch: 6.97 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.50172239902094		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.50172239902094 | validation: 1.4360775231549257]
	TIME [epoch: 6.95 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4692677516100665		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.4692677516100665 | validation: 1.6070344115596953]
	TIME [epoch: 6.95 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.616988325322883		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.616988325322883 | validation: 1.3577620138471844]
	TIME [epoch: 6.96 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3503967846698115		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.3503967846698115 | validation: 1.3439215029893807]
	TIME [epoch: 6.98 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3614316776482445		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.3614316776482445 | validation: 1.693271035279703]
	TIME [epoch: 6.97 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5431548081131938		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.5431548081131938 | validation: 1.3208928630593375]
	TIME [epoch: 6.97 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3599548333998683		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.3599548333998683 | validation: 1.2882203667469565]
	TIME [epoch: 6.97 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.364041493205369		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.364041493205369 | validation: 1.336408663528213]
	TIME [epoch: 6.96 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2880623774059106		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.2880623774059106 | validation: 1.2486671105239404]
	TIME [epoch: 6.96 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2756120981119912		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.2756120981119912 | validation: 1.8961222846478603]
	TIME [epoch: 6.98 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.689216841849404		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.689216841849404 | validation: 1.4363003769108706]
	TIME [epoch: 6.96 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.38768067749184		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.38768067749184 | validation: 1.8396610535136837]
	TIME [epoch: 6.96 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5968818204570132		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.5968818204570132 | validation: 1.5020491376397718]
	TIME [epoch: 6.94 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.444519793274405		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.444519793274405 | validation: 1.5062594815615242]
	TIME [epoch: 6.95 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4381921161568445		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.4381921161568445 | validation: 1.4435226777665342]
	TIME [epoch: 6.95 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.412908159867475		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.412908159867475 | validation: 1.352393344807794]
	TIME [epoch: 6.96 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3080126745743583		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.3080126745743583 | validation: 1.2765060265069221]
	TIME [epoch: 6.98 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3409576339315368		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.3409576339315368 | validation: 1.3422863181609348]
	TIME [epoch: 6.95 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3629964767149558		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.3629964767149558 | validation: 1.2195445965144434]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2727312713461467		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.2727312713461467 | validation: 1.2203797534254726]
	TIME [epoch: 6.95 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5043970093104888		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.5043970093104888 | validation: 1.5103788133925304]
	TIME [epoch: 6.95 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5706472533560332		[learning rate: 0.0076127]
	Learning Rate: 0.00761269
	LOSS [training: 1.5706472533560332 | validation: 1.3814777800056988]
	TIME [epoch: 6.95 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3896383998138786		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.3896383998138786 | validation: 1.257318770654273]
	TIME [epoch: 6.98 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2671388320837271		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.2671388320837271 | validation: 1.225111892584106]
	TIME [epoch: 6.96 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2934731058203046		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.2934731058203046 | validation: 1.2900789847834429]
	TIME [epoch: 6.96 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.394425024671004		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.394425024671004 | validation: 1.216136513405319]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2799538531421257		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.2799538531421257 | validation: 1.2765615019916656]
	TIME [epoch: 6.94 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.256032644476519		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.256032644476519 | validation: 1.2724550067494955]
	TIME [epoch: 6.97 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2333899130568058		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.2333899130568058 | validation: 1.323654536653504]
	TIME [epoch: 6.95 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3121445728303076		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.3121445728303076 | validation: 1.4178141170802372]
	TIME [epoch: 7 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3392926598177657		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.3392926598177657 | validation: 1.606749604593504]
	TIME [epoch: 6.95 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6173610796839686		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.6173610796839686 | validation: 1.3101334267637004]
	TIME [epoch: 6.97 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3096317540160414		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.3096317540160414 | validation: 1.2866290264822982]
	TIME [epoch: 6.94 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2344865451192437		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 1.2344865451192437 | validation: 1.2116133242440208]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2340731759166668		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.2340731759166668 | validation: 1.20890188453801]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.205047300194146		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.205047300194146 | validation: 1.200067514332103]
	TIME [epoch: 7 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.229074486806332		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.229074486806332 | validation: 1.1818823897762847]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2213084598115245		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.2213084598115245 | validation: 1.3559477765195784]
	TIME [epoch: 6.97 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.373685840453563		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 1.373685840453563 | validation: 1.1740580601227633]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1784238084799317		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.1784238084799317 | validation: 1.226687095914883]
	TIME [epoch: 7.04 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.234371812279881		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.234371812279881 | validation: 1.146603231555903]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.193656589119171		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.193656589119171 | validation: 1.2376642229215016]
	TIME [epoch: 6.96 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2383727154773587		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.2383727154773587 | validation: 1.394231703049717]
	TIME [epoch: 6.98 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3282494816030792		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.3282494816030792 | validation: 1.3042733725263878]
	TIME [epoch: 6.96 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3325483099254876		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.3325483099254876 | validation: 1.1323656794593135]
	TIME [epoch: 6.96 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1653963707154236		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.1653963707154236 | validation: 1.1800945907236076]
	TIME [epoch: 6.96 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1708384757551302		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.1708384757551302 | validation: 1.1381447583222066]
	TIME [epoch: 6.95 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1491250511008708		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.1491250511008708 | validation: 1.1576252993321419]
	TIME [epoch: 6.95 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.296623814395078		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.296623814395078 | validation: 1.395325283272301]
	TIME [epoch: 6.98 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3609889277596756		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.3609889277596756 | validation: 1.5376771705346575]
	TIME [epoch: 6.97 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3337892913860312		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.3337892913860312 | validation: 1.2607735319466977]
	TIME [epoch: 6.95 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2319235902565793		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.2319235902565793 | validation: 1.1839865736686936]
	TIME [epoch: 6.95 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.200787219282182		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.200787219282182 | validation: 1.1149157097671019]
	TIME [epoch: 6.96 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2070475889083023		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.2070475889083023 | validation: 1.1619604141538036]
	TIME [epoch: 6.95 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1848451808213478		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.1848451808213478 | validation: 1.1303720053022974]
	TIME [epoch: 6.96 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1500492080265317		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.1500492080265317 | validation: 1.115403784325643]
	TIME [epoch: 6.97 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1485289691428053		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.1485289691428053 | validation: 1.354039066245762]
	TIME [epoch: 6.95 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2564679430154109		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.2564679430154109 | validation: 1.1315058752786455]
	TIME [epoch: 6.94 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1367052647668028		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.1367052647668028 | validation: 1.2072428701557918]
	TIME [epoch: 6.94 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2904533410031613		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.2904533410031613 | validation: 1.1252549237926583]
	TIME [epoch: 6.94 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2180089253291855		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.2180089253291855 | validation: 1.131071731910741]
	TIME [epoch: 6.95 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1596238385281468		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.1596238385281468 | validation: 1.091812015090986]
	TIME [epoch: 6.99 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1550625283012974		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.1550625283012974 | validation: 1.2622159467904406]
	TIME [epoch: 6.95 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.227511431095735		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.227511431095735 | validation: 1.2715133092449014]
	TIME [epoch: 6.95 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1558564506615019		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.1558564506615019 | validation: 1.0699295834005358]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1520441737227578		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.1520441737227578 | validation: 1.0828630556397378]
	TIME [epoch: 6.95 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1181303826794622		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.1181303826794622 | validation: 1.137561606066862]
	TIME [epoch: 6.95 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0935678162696523		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.0935678162696523 | validation: 1.0409473713312245]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0840571625612354		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.0840571625612354 | validation: 1.0858115832783255]
	TIME [epoch: 6.96 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0955855030384563		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.0955855030384563 | validation: 1.1196764721145769]
	TIME [epoch: 6.94 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1520304094767826		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.1520304094767826 | validation: 1.1215864916273313]
	TIME [epoch: 6.94 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1995209982148203		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.1995209982148203 | validation: 1.2737933753470596]
	TIME [epoch: 6.95 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2045380819549503		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.2045380819549503 | validation: 1.1386789517454623]
	TIME [epoch: 6.95 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.114234625400056		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.114234625400056 | validation: 1.0745968112615887]
	TIME [epoch: 6.94 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1404644735527716		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.1404644735527716 | validation: 1.025868293267833]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0489185187027223		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.0489185187027223 | validation: 1.0552235669848156]
	TIME [epoch: 6.96 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0819867883452745		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.0819867883452745 | validation: 1.2020340423934168]
	TIME [epoch: 6.95 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1172419913591922		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.1172419913591922 | validation: 1.0311736367914268]
	TIME [epoch: 6.94 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0474037402739662		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 1.0474037402739662 | validation: 0.9874868210857894]
	TIME [epoch: 6.96 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0746609094961972		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.0746609094961972 | validation: 1.0459322622481853]
	TIME [epoch: 6.96 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0612290050623976		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.0612290050623976 | validation: 0.989097325238885]
	TIME [epoch: 6.98 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0460749063971502		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.0460749063971502 | validation: 0.9825310960985544]
	TIME [epoch: 6.99 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1098366605548682		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.1098366605548682 | validation: 1.0586631505284714]
	TIME [epoch: 6.95 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0820193540250629		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.0820193540250629 | validation: 1.0567552619086624]
	TIME [epoch: 6.96 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.047833390952761		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.047833390952761 | validation: 0.9610214183004516]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0444968987975045		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.0444968987975045 | validation: 1.0494188975447283]
	TIME [epoch: 6.96 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0044628182145916		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.0044628182145916 | validation: 0.9399157592026754]
	TIME [epoch: 6.96 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0371807599028022		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.0371807599028022 | validation: 0.9282326036454717]
	TIME [epoch: 6.98 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9939523516717105		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.9939523516717105 | validation: 0.9499805417896905]
	TIME [epoch: 6.96 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.012468758889148		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.012468758889148 | validation: 1.176583242661132]
	TIME [epoch: 6.95 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.080372222214209		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.080372222214209 | validation: 1.0200379207925019]
	TIME [epoch: 6.96 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.070976262250455		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.070976262250455 | validation: 1.1080455648838878]
	TIME [epoch: 6.95 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.150079197609514		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.150079197609514 | validation: 1.0667062526159565]
	TIME [epoch: 6.96 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.039282002112892		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.039282002112892 | validation: 0.9558028425546511]
	TIME [epoch: 6.96 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0093273539288083		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.0093273539288083 | validation: 0.9684638989027499]
	TIME [epoch: 6.99 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9594818697454902		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.9594818697454902 | validation: 0.9335537687922205]
	TIME [epoch: 6.95 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9803417466350721		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.9803417466350721 | validation: 0.9736068883927982]
	TIME [epoch: 6.95 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0035282581860294		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.0035282581860294 | validation: 0.925955497334077]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9777452323408871		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.9777452323408871 | validation: 0.9678076460559679]
	TIME [epoch: 6.97 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.988621214088499		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.988621214088499 | validation: 0.952757518739884]
	TIME [epoch: 6.96 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0405042836449714		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.0405042836449714 | validation: 0.9833999075458536]
	TIME [epoch: 6.97 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0289425171053739		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.0289425171053739 | validation: 0.9643179804834523]
	TIME [epoch: 6.95 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9899050415079065		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.9899050415079065 | validation: 0.8949181510075941]
	TIME [epoch: 6.97 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9293883543381978		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.9293883543381978 | validation: 0.8986733405280337]
	TIME [epoch: 6.94 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9509078328163341		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.9509078328163341 | validation: 0.9659320039560598]
	TIME [epoch: 6.94 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9715248195653587		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.9715248195653587 | validation: 0.9485588418002684]
	TIME [epoch: 6.95 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.994114268735699		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.994114268735699 | validation: 1.051430174299935]
	TIME [epoch: 6.96 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9765411252931543		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.9765411252931543 | validation: 0.9020732311020989]
	TIME [epoch: 7 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.926499031721739		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.926499031721739 | validation: 0.901510476062541]
	TIME [epoch: 6.94 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9270725734105253		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.9270725734105253 | validation: 0.9274835789372411]
	TIME [epoch: 6.96 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9214039655640538		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.9214039655640538 | validation: 0.9291619168944973]
	TIME [epoch: 6.95 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9199409547614713		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.9199409547614713 | validation: 0.88471482234445]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9350743003577376		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.9350743003577376 | validation: 0.9019066843732887]
	TIME [epoch: 6.94 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9264909542025543		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.9264909542025543 | validation: 0.9896936905127177]
	TIME [epoch: 6.97 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9604350121652172		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.9604350121652172 | validation: 0.8771210143418536]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9492316598538071		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.9492316598538071 | validation: 0.8858642215770293]
	TIME [epoch: 6.96 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9085219707843402		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.9085219707843402 | validation: 1.0984264387048055]
	TIME [epoch: 6.95 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.00715713999086		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.00715713999086 | validation: 0.8888877230165871]
	TIME [epoch: 6.97 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8973133619261158		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.8973133619261158 | validation: 0.9375538606622039]
	TIME [epoch: 6.96 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9316564416043782		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.9316564416043782 | validation: 0.8671033994686159]
	TIME [epoch: 6.96 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.893322695469544		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.893322695469544 | validation: 0.8463061604304625]
	TIME [epoch: 6.97 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8615824920975723		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.8615824920975723 | validation: 0.8865161838363739]
	TIME [epoch: 6.95 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8929450149952718		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.8929450149952718 | validation: 0.8493788130929251]
	TIME [epoch: 6.95 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8798512514171528		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.8798512514171528 | validation: 0.8846982781576943]
	TIME [epoch: 6.95 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8860954494477706		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.8860954494477706 | validation: 0.9056284791028084]
	TIME [epoch: 6.94 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8706363522901024		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.8706363522901024 | validation: 0.8989485658839151]
	TIME [epoch: 6.95 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9009119893547624		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.9009119893547624 | validation: 0.9361234707986056]
	TIME [epoch: 6.97 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9130183261049889		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.9130183261049889 | validation: 0.8212781985921591]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8359238642450165		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.8359238642450165 | validation: 0.8956185081976286]
	TIME [epoch: 6.97 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8686869321183632		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.8686869321183632 | validation: 0.8305850435768374]
	TIME [epoch: 6.95 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8206024655371753		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.8206024655371753 | validation: 0.831002271224085]
	TIME [epoch: 6.96 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8200096597187325		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.8200096597187325 | validation: 0.8132647507376366]
	TIME [epoch: 6.97 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8283460707329486		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.8283460707329486 | validation: 0.8618886499744058]
	TIME [epoch: 6.96 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9093420905910627		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.9093420905910627 | validation: 0.8195807606394752]
	TIME [epoch: 6.98 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7796007447340387		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.7796007447340387 | validation: 0.8264994793630097]
	TIME [epoch: 6.95 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7915871672315982		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.7915871672315982 | validation: 0.7986924202561604]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8167698153634647		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.8167698153634647 | validation: 0.792348889333629]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7944492325833494		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.7944492325833494 | validation: 0.8578611511033476]
	TIME [epoch: 6.94 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8269683113903592		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.8269683113903592 | validation: 0.7953622034832223]
	TIME [epoch: 6.95 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8067629126935845		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.8067629126935845 | validation: 0.7894236544266788]
	TIME [epoch: 6.96 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7620130684902598		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.7620130684902598 | validation: 0.7791420908366669]
	TIME [epoch: 6.94 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7429715851395319		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.7429715851395319 | validation: 0.7448493608655932]
	TIME [epoch: 6.93 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7476148519646899		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.7476148519646899 | validation: 0.7366452863034755]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7043501617221712		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.7043501617221712 | validation: 0.7338052382935888]
	TIME [epoch: 6.96 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7329104995075219		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.7329104995075219 | validation: 0.8096379204727295]
	TIME [epoch: 6.95 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7676657576498002		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.7676657576498002 | validation: 0.7098933400944781]
	TIME [epoch: 6.98 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7024763234732195		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.7024763234732195 | validation: 0.6844104635386231]
	TIME [epoch: 6.96 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7229994309572911		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.7229994309572911 | validation: 0.7280787734129162]
	TIME [epoch: 6.94 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8375649253439417		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.8375649253439417 | validation: 0.702924538754131]
	TIME [epoch: 6.97 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7527302021321465		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.7527302021321465 | validation: 0.7547770928033476]
	TIME [epoch: 6.95 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6865257524195874		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.6865257524195874 | validation: 0.6603513231562163]
	TIME [epoch: 6.95 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6687256048404728		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.6687256048404728 | validation: 0.668363834563601]
	TIME [epoch: 6.98 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6568276479900991		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.6568276479900991 | validation: 0.643758309799184]
	TIME [epoch: 6.98 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7370820976642073		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.7370820976642073 | validation: 0.6703868224889281]
	TIME [epoch: 6.94 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6745532926292914		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.6745532926292914 | validation: 0.652310201274755]
	TIME [epoch: 6.94 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6191305948464259		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.6191305948464259 | validation: 0.7535574292434237]
	TIME [epoch: 6.96 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7176941235707381		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.7176941235707381 | validation: 0.7490046949635759]
	TIME [epoch: 6.96 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.677058428999011		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.677058428999011 | validation: 0.6028996637203371]
	TIME [epoch: 6.96 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6065400230434087		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.6065400230434087 | validation: 0.6421450150062141]
	TIME [epoch: 101 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6480163230190621		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.6480163230190621 | validation: 0.6345859802626327]
	TIME [epoch: 15.2 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6142323326529993		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.6142323326529993 | validation: 0.6038585923487361]
	TIME [epoch: 15.2 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6234896636947218		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.6234896636947218 | validation: 0.5805695595573142]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5824268227871476		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.5824268227871476 | validation: 0.613191427645212]
	TIME [epoch: 15.2 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.61276754439565		[learning rate: 0.0027837]
	Learning Rate: 0.00278366
	LOSS [training: 0.61276754439565 | validation: 0.6613754072091923]
	TIME [epoch: 15.2 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6338143358191224		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.6338143358191224 | validation: 0.5646028623965583]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5396276908484537		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.5396276908484537 | validation: 0.5692322933028777]
	TIME [epoch: 15.2 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5521150116076419		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.5521150116076419 | validation: 0.5629547070763001]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5469310274516301		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.5469310274516301 | validation: 0.7121861828804145]
	TIME [epoch: 15.2 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6197961847493929		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.6197961847493929 | validation: 0.5076266728806725]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5279890196499375		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.5279890196499375 | validation: 0.5088115467787583]
	TIME [epoch: 15.2 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.51150638774547		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.51150638774547 | validation: 0.8457987799062242]
	TIME [epoch: 15.2 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7219978212734077		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.7219978212734077 | validation: 0.7593740775248173]
	TIME [epoch: 15.2 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5889438503421027		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.5889438503421027 | validation: 0.5214131425204868]
	TIME [epoch: 15.2 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9873901369785458		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.9873901369785458 | validation: 0.659876727118308]
	TIME [epoch: 15.2 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6653624607776116		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.6653624607776116 | validation: 0.5498694126421766]
	TIME [epoch: 15.2 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6101026887197736		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.6101026887197736 | validation: 0.5388672947731289]
	TIME [epoch: 15.2 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5392339415266749		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.5392339415266749 | validation: 0.4685622018801259]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4891057759491301		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.4891057759491301 | validation: 0.4868135925636605]
	TIME [epoch: 15.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4663137537561347		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.4663137537561347 | validation: 0.49771617052997874]
	TIME [epoch: 15.2 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4512900990073653		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.4512900990073653 | validation: 0.48907376605070585]
	TIME [epoch: 15.2 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4562127827051946		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.4562127827051946 | validation: 0.44125563674391666]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.47128678268661256		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.47128678268661256 | validation: 0.5269032330387928]
	TIME [epoch: 15.2 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45318768341154947		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.45318768341154947 | validation: 0.47304260709234075]
	TIME [epoch: 15.2 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45612952970733994		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.45612952970733994 | validation: 0.4733501552613545]
	TIME [epoch: 15.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4881688646837759		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.4881688646837759 | validation: 0.4479254207363701]
	TIME [epoch: 15.2 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4242344113194559		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.4242344113194559 | validation: 0.968746708220968]
	TIME [epoch: 15.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6965854053302165		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.6965854053302165 | validation: 0.6372458967316101]
	TIME [epoch: 15.2 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5469558590994019		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.5469558590994019 | validation: 0.6553151408961573]
	TIME [epoch: 15.2 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.502011181159455		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.502011181159455 | validation: 0.4169189253952639]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7592742757386287		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.7592742757386287 | validation: 1.437803516790716]
	TIME [epoch: 15.2 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2876964656610828		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 1.2876964656610828 | validation: 1.4557853911235048]
	TIME [epoch: 15.2 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1876418417646661		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 1.1876418417646661 | validation: 0.9700187336977941]
	TIME [epoch: 15.2 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6976694620100665		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.6976694620100665 | validation: 0.43460437131817903]
	TIME [epoch: 15.2 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45276847402222853		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.45276847402222853 | validation: 0.6619185544301731]
	TIME [epoch: 15.2 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5038715319434516		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.5038715319434516 | validation: 0.450322830825733]
	TIME [epoch: 15.2 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4513218284724472		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.4513218284724472 | validation: 0.5644785227799182]
	TIME [epoch: 15.2 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.48001816396236663		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.48001816396236663 | validation: 0.45540502447081654]
	TIME [epoch: 15.2 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.47155640661656795		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.47155640661656795 | validation: 0.6966739040814877]
	TIME [epoch: 15.1 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5527795106850621		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.5527795106850621 | validation: 0.5294100282270907]
	TIME [epoch: 15.2 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5140967279124607		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.5140967279124607 | validation: 0.49219911498027846]
	TIME [epoch: 15.2 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4541201513367556		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.4541201513367556 | validation: 0.44758446627954895]
	TIME [epoch: 15.2 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45893871548674925		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.45893871548674925 | validation: 0.5779257109046725]
	TIME [epoch: 15.2 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4818342407133577		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.4818342407133577 | validation: 0.4680858295245958]
	TIME [epoch: 15.2 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3964592626973583		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.3964592626973583 | validation: 0.4046585171440253]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6953181408439517		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.6953181408439517 | validation: 0.4881622109820001]
	TIME [epoch: 15.1 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5417710623474015		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.5417710623474015 | validation: 0.8071406301800043]
	TIME [epoch: 15.2 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.519059754128797		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.519059754128797 | validation: 0.45120257044517953]
	TIME [epoch: 15.2 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4109256571158597		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.4109256571158597 | validation: 0.490865813323611]
	TIME [epoch: 15.2 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6402923285554458		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.6402923285554458 | validation: 0.6652676337000356]
	TIME [epoch: 15.1 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6547047740530059		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.6547047740530059 | validation: 0.726472915105024]
	TIME [epoch: 15.2 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5265400579939519		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.5265400579939519 | validation: 0.4678793678422724]
	TIME [epoch: 15.2 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.42468880368075823		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.42468880368075823 | validation: 0.5055072368425552]
	TIME [epoch: 15.2 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.46552588019234525		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.46552588019234525 | validation: 0.5687110704625233]
	TIME [epoch: 15.2 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.44167975386509606		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.44167975386509606 | validation: 0.45521979940780616]
	TIME [epoch: 15.2 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4052867035828954		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.4052867035828954 | validation: 0.7337688875175932]
	TIME [epoch: 15.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6364853997527417		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.6364853997527417 | validation: 0.6315354889225889]
	TIME [epoch: 15.2 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45130995355578163		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.45130995355578163 | validation: 0.5175659743952886]
	TIME [epoch: 15.2 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4450578772026224		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.4450578772026224 | validation: 0.5268244756344036]
	TIME [epoch: 15.2 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7151776458103674		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.7151776458103674 | validation: 1.2531017465304364]
	TIME [epoch: 15.2 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.092139613712302		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 1.092139613712302 | validation: 1.0879393198134]
	TIME [epoch: 15.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8969096113274809		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.8969096113274809 | validation: 0.8696669824686927]
	TIME [epoch: 15.2 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.693784671900803		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.693784671900803 | validation: 0.591279485001935]
	TIME [epoch: 15.1 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5079518940172929		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.5079518940172929 | validation: 0.6158320728383235]
	TIME [epoch: 15.2 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5021503174825189		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.5021503174825189 | validation: 0.5388659337399674]
	TIME [epoch: 15.2 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4463567768441006		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.4463567768441006 | validation: 0.5814290660942139]
	TIME [epoch: 15.2 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.43695440915624273		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.43695440915624273 | validation: 0.5551705360420252]
	TIME [epoch: 15.2 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5214235725502292		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.5214235725502292 | validation: 0.6016559997552882]
	TIME [epoch: 15.2 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.43746296378602534		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.43746296378602534 | validation: 0.5386763993566636]
	TIME [epoch: 15.1 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.42502852241968275		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.42502852241968275 | validation: 0.7572509087423889]
	TIME [epoch: 15.2 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.49659157453202674		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.49659157453202674 | validation: 0.5512691842525106]
	TIME [epoch: 15.2 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5357521459626705		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.5357521459626705 | validation: 0.6820953815964227]
	TIME [epoch: 15.1 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4929598974717706		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.4929598974717706 | validation: 0.5099909139306645]
	TIME [epoch: 15.2 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.41272480661742805		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.41272480661742805 | validation: 0.4955170060475096]
	TIME [epoch: 15.2 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5176105946800107		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.5176105946800107 | validation: 0.5744894028467]
	TIME [epoch: 15.2 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4389401699698396		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.4389401699698396 | validation: 0.5401777667892572]
	TIME [epoch: 15.1 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3918572382456251		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.3918572382456251 | validation: 0.3988473881408996]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3816414598349852		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.3816414598349852 | validation: 0.684731316917881]
	TIME [epoch: 15.2 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5430474565046206		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.5430474565046206 | validation: 0.5310968839860564]
	TIME [epoch: 15.2 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4346537933458079		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.4346537933458079 | validation: 0.36323926364913084]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45293788786809386		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.45293788786809386 | validation: 0.6444510834549111]
	TIME [epoch: 15.2 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4612350862368389		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.4612350862368389 | validation: 0.42530893981203155]
	TIME [epoch: 15.2 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45143790337635176		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.45143790337635176 | validation: 0.5614540118433565]
	TIME [epoch: 15.2 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4855274355043327		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.4855274355043327 | validation: 0.4908869367995716]
	TIME [epoch: 15.2 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.39072614890906404		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.39072614890906404 | validation: 0.3620139279916849]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4178006412141445		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.4178006412141445 | validation: 0.5728091065734404]
	TIME [epoch: 15.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.401021302078458		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.401021302078458 | validation: 0.4214375528050296]
	TIME [epoch: 15.2 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.34738473316017404		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.34738473316017404 | validation: 0.38544743297502265]
	TIME [epoch: 15.2 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3733164660559387		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.3733164660559387 | validation: 0.5425461447036037]
	TIME [epoch: 15.2 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.42526394660003847		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.42526394660003847 | validation: 0.46124179153394795]
	TIME [epoch: 15.2 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.46984367178093744		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.46984367178093744 | validation: 0.7074475177710262]
	TIME [epoch: 15.2 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7250188363094661		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.7250188363094661 | validation: 0.7850697924954345]
	TIME [epoch: 15.2 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5193372649045411		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.5193372649045411 | validation: 0.40359299980506347]
	TIME [epoch: 15.2 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3742443083454594		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.3742443083454594 | validation: 0.5392415655680879]
	TIME [epoch: 15.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4283125556055932		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.4283125556055932 | validation: 0.3526508541900004]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3463785841126279		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.3463785841126279 | validation: 0.5292351623350706]
	TIME [epoch: 15.2 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.41628340043393186		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.41628340043393186 | validation: 0.3706767688153293]
	TIME [epoch: 15.2 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.34937448260311965		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.34937448260311965 | validation: 0.5309831466379018]
	TIME [epoch: 15.2 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4556301599538845		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.4556301599538845 | validation: 0.39065180127791715]
	TIME [epoch: 15.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.37195238476386777		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.37195238476386777 | validation: 0.3814526531647969]
	TIME [epoch: 15.2 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35752518236509107		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.35752518236509107 | validation: 0.3151814133261245]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.34932356709367196		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.34932356709367196 | validation: 0.5392201115853273]
	TIME [epoch: 15.2 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4043768392693576		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.4043768392693576 | validation: 0.33956038354393714]
	TIME [epoch: 15.2 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3683230932803624		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.3683230932803624 | validation: 0.3350060193893678]
	TIME [epoch: 15.2 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3330336936814037		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.3330336936814037 | validation: 0.4689775330722869]
	TIME [epoch: 15.2 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5309759174693751		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.5309759174693751 | validation: 0.3524670483596173]
	TIME [epoch: 15.2 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.41009918979026116		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.41009918979026116 | validation: 0.39515957454577616]
	TIME [epoch: 15.2 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.36187582954848696		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.36187582954848696 | validation: 0.37409629633638763]
	TIME [epoch: 15.2 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.359530492758656		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.359530492758656 | validation: 0.29704989918512376]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33074129784083445		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.33074129784083445 | validation: 0.40778936723460285]
	TIME [epoch: 15.2 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.34065068051578207		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.34065068051578207 | validation: 0.4289467420711981]
	TIME [epoch: 15.2 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.344121561424009		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.344121561424009 | validation: 0.45806237028320745]
	TIME [epoch: 15.2 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4576750323277668		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.4576750323277668 | validation: 0.33705115747437686]
	TIME [epoch: 15.2 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3350452068697256		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.3350452068697256 | validation: 0.3640972769999784]
	TIME [epoch: 15.2 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32917739938426327		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.32917739938426327 | validation: 0.3644637752763931]
	TIME [epoch: 15.2 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3727495610254349		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.3727495610254349 | validation: 0.5852858488289828]
	TIME [epoch: 15.2 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4934091262908896		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.4934091262908896 | validation: 0.35447565697623246]
	TIME [epoch: 15.2 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32856649201286336		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.32856649201286336 | validation: 0.47374353304157285]
	TIME [epoch: 15.2 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.37198101612091383		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.37198101612091383 | validation: 0.3129630422105268]
	TIME [epoch: 15.2 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3219494758879795		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.3219494758879795 | validation: 0.6331075320967924]
	TIME [epoch: 15.2 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.691160308787091		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.691160308787091 | validation: 0.5063367316903065]
	TIME [epoch: 15.2 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.42460967424911733		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.42460967424911733 | validation: 0.507730560712451]
	TIME [epoch: 15.2 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3714137937741506		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.3714137937741506 | validation: 0.3707210975704162]
	TIME [epoch: 15.2 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3340463414042687		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.3340463414042687 | validation: 0.29598416488591756]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3282399622693005		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.3282399622693005 | validation: 0.5060332555120589]
	TIME [epoch: 15.2 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5225540136211642		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.5225540136211642 | validation: 0.45786565097120957]
	TIME [epoch: 15.2 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3736277940518041		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.3736277940518041 | validation: 0.33173888478514013]
	TIME [epoch: 15.2 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2894349020387922		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.2894349020387922 | validation: 0.41453753000825283]
	TIME [epoch: 15.2 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3471604162150628		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.3471604162150628 | validation: 0.3676500083642442]
	TIME [epoch: 15.2 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31210106530783643		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.31210106530783643 | validation: 0.27713520937177694]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35112391411588983		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.35112391411588983 | validation: 0.44150589665340567]
	TIME [epoch: 15.2 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.34626687442313764		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.34626687442313764 | validation: 0.3489012625182114]
	TIME [epoch: 15.2 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35779538150086876		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.35779538150086876 | validation: 0.41765396900151225]
	TIME [epoch: 15.2 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3123134645825135		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.3123134645825135 | validation: 0.28477209915285046]
	TIME [epoch: 15.2 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35844638194209666		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.35844638194209666 | validation: 0.3138248029080593]
	TIME [epoch: 15.2 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32081932656384027		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.32081932656384027 | validation: 0.5766478526354557]
	TIME [epoch: 15.1 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.40485309478084475		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.40485309478084475 | validation: 0.3599722988948969]
	TIME [epoch: 15.2 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3595541213674738		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.3595541213674738 | validation: 0.5682529475527351]
	TIME [epoch: 15.2 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.39296914944135153		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.39296914944135153 | validation: 0.40682464119063627]
	TIME [epoch: 15.2 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3352987692522962		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.3352987692522962 | validation: 0.31846260174466184]
	TIME [epoch: 15.2 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3234377663324063		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.3234377663324063 | validation: 0.34592552528964027]
	TIME [epoch: 15.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3770813260824757		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.3770813260824757 | validation: 0.4649306531245736]
	TIME [epoch: 15.2 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4319398525608448		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.4319398525608448 | validation: 0.4063791202011299]
	TIME [epoch: 15.2 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38453871524199035		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.38453871524199035 | validation: 0.43019280357548634]
	TIME [epoch: 15.2 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3911774552924967		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.3911774552924967 | validation: 0.5442697299203205]
	TIME [epoch: 15.1 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3665924784929181		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.3665924784929181 | validation: 0.3262807674919409]
	TIME [epoch: 15.2 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3192112872741696		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.3192112872741696 | validation: 0.3645423674018928]
	TIME [epoch: 15.2 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30387752318645844		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.30387752318645844 | validation: 0.3351252105180198]
	TIME [epoch: 15.2 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3186254039253476		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.3186254039253476 | validation: 0.5048163142273255]
	TIME [epoch: 15.2 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.37996869264742694		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.37996869264742694 | validation: 0.3045208774172343]
	TIME [epoch: 15.2 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2932072161715201		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.2932072161715201 | validation: 0.28100124891840234]
	TIME [epoch: 15.1 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3197931560990578		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.3197931560990578 | validation: 0.28579725755529084]
	TIME [epoch: 15.2 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27939631301477674		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.27939631301477674 | validation: 0.3067124554009312]
	TIME [epoch: 15.2 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2992561512511095		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.2992561512511095 | validation: 0.37834662889800463]
	TIME [epoch: 15.2 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3822748655988926		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.3822748655988926 | validation: 0.2626967331543548]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2802487704101368		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.2802487704101368 | validation: 0.2761192862224138]
	TIME [epoch: 15.2 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32331358121705545		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.32331358121705545 | validation: 0.44170868966402044]
	TIME [epoch: 15.1 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3305015240607112		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.3305015240607112 | validation: 0.26117909080441354]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3304402056273109		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.3304402056273109 | validation: 0.3471704415097312]
	TIME [epoch: 15.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31201829086832894		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.31201829086832894 | validation: 0.2647291399062341]
	TIME [epoch: 15.2 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2805180696947673		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.2805180696947673 | validation: 0.2910148973531503]
	TIME [epoch: 15.1 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2694395297277483		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.2694395297277483 | validation: 0.25586365604331623]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32379178254478935		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.32379178254478935 | validation: 0.3007247213285331]
	TIME [epoch: 15.2 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.377761230357633		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.377761230357633 | validation: 0.3954597282332388]
	TIME [epoch: 15.2 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3930735353283494		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.3930735353283494 | validation: 0.5251284193408217]
	TIME [epoch: 15.2 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3460951408066327		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.3460951408066327 | validation: 0.2778678519805488]
	TIME [epoch: 15.1 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3024058572123214		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.3024058572123214 | validation: 0.3145746579356025]
	TIME [epoch: 15.1 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2981131516483969		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.2981131516483969 | validation: 0.2998661788637671]
	TIME [epoch: 15.2 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27652715785930837		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.27652715785930837 | validation: 0.30589158699674485]
	TIME [epoch: 15.2 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29186788167160443		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.29186788167160443 | validation: 0.47836283971786975]
	TIME [epoch: 15.1 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3707190681265085		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.3707190681265085 | validation: 0.2872353080182022]
	TIME [epoch: 15.1 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2865474540599066		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.2865474540599066 | validation: 0.3110002149896544]
	TIME [epoch: 15.2 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2883247669156662		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.2883247669156662 | validation: 0.42508940375152726]
	TIME [epoch: 15.1 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3231221946151315		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.3231221946151315 | validation: 0.32076313920866983]
	TIME [epoch: 15.2 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26950456271333917		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.26950456271333917 | validation: 0.27673732712772914]
	TIME [epoch: 15.2 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6828335710205915		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.6828335710205915 | validation: 0.48238754348306423]
	TIME [epoch: 15.2 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45192023908749035		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.45192023908749035 | validation: 0.6563371426757565]
	TIME [epoch: 15.2 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5487394825561076		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.5487394825561076 | validation: 0.8186212313997608]
	TIME [epoch: 15.2 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5693527269054193		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.5693527269054193 | validation: 0.307319515363377]
	TIME [epoch: 15.1 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28333952697960096		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.28333952697960096 | validation: 0.2699842851156376]
	TIME [epoch: 15.1 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27203103296770553		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.27203103296770553 | validation: 0.4394275954731505]
	TIME [epoch: 15.2 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32074154661729026		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.32074154661729026 | validation: 0.28488331429340447]
	TIME [epoch: 15.1 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2784147793599962		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.2784147793599962 | validation: 0.29735896545159585]
	TIME [epoch: 15.2 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26187560836364876		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.26187560836364876 | validation: 0.36997864313244716]
	TIME [epoch: 15.2 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3295559549239998		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.3295559549239998 | validation: 0.2694077321099661]
	TIME [epoch: 15.1 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28817471098080927		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.28817471098080927 | validation: 0.5606789479314288]
	TIME [epoch: 15.2 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38343971665058973		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.38343971665058973 | validation: 0.34020415235318685]
	TIME [epoch: 15.2 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3248166165738698		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.3248166165738698 | validation: 0.37446741243407333]
	TIME [epoch: 15.2 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30035824738492445		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.30035824738492445 | validation: 0.3287209657775175]
	TIME [epoch: 15.1 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2874946686317735		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.2874946686317735 | validation: 0.3597309280711407]
	TIME [epoch: 15.2 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2876823934853582		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.2876823934853582 | validation: 0.33000711543160377]
	TIME [epoch: 15.1 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27694631367280925		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.27694631367280925 | validation: 0.45936426323036034]
	TIME [epoch: 15.2 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31843881293637455		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.31843881293637455 | validation: 0.2595123726869315]
	TIME [epoch: 15.2 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25379452362182586		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.25379452362182586 | validation: 0.4322390577317107]
	TIME [epoch: 15.2 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.34675131419251476		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.34675131419251476 | validation: 0.24899285449676056]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24880310621238605		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.24880310621238605 | validation: 0.23278271263369268]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24970816117877126		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.24970816117877126 | validation: 0.29571307496951]
	TIME [epoch: 15.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2780886352661438		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.2780886352661438 | validation: 0.2637141354125054]
	TIME [epoch: 15.2 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2788054604599303		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.2788054604599303 | validation: 0.2717071326724385]
	TIME [epoch: 15.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2534053184224397		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.2534053184224397 | validation: 0.4711553655929532]
	TIME [epoch: 15.2 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30209302946525435		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.30209302946525435 | validation: 0.2828956783799602]
	TIME [epoch: 15.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24132184917986083		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.24132184917986083 | validation: 0.2632886127021364]
	TIME [epoch: 15.2 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24995407340657766		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.24995407340657766 | validation: 0.345280995065939]
	TIME [epoch: 15.2 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2992854318667745		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.2992854318667745 | validation: 0.25232243453412817]
	TIME [epoch: 15.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2807031228903971		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.2807031228903971 | validation: 0.28611533443958564]
	TIME [epoch: 15.2 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45576074323157556		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.45576074323157556 | validation: 0.5013768492715037]
	TIME [epoch: 15.2 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.44848569126410975		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.44848569126410975 | validation: 0.4355015250305847]
	TIME [epoch: 15.1 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3418352914015059		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.3418352914015059 | validation: 0.35393605387723676]
	TIME [epoch: 15.2 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.287854080407305		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.287854080407305 | validation: 0.2520666330286532]
	TIME [epoch: 15.2 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2500032620759808		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.2500032620759808 | validation: 0.24146779870229926]
	TIME [epoch: 15.2 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2941139448601344		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.2941139448601344 | validation: 0.23507851681473416]
	TIME [epoch: 15.2 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29095125061297017		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.29095125061297017 | validation: 0.3493189120195883]
	TIME [epoch: 15.2 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30529946932168106		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.30529946932168106 | validation: 0.30556940171067004]
	TIME [epoch: 15.2 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27393997347656684		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.27393997347656684 | validation: 0.2824508467503861]
	TIME [epoch: 15.2 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28609803303244036		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.28609803303244036 | validation: 0.5628326811359293]
	TIME [epoch: 15.2 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4438778608354706		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.4438778608354706 | validation: 0.38987523971625826]
	TIME [epoch: 15.1 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2920004131199108		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.2920004131199108 | validation: 0.2527574525359961]
	TIME [epoch: 15.2 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2617800807670807		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.2617800807670807 | validation: 0.3229278863155369]
	TIME [epoch: 15.2 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2801187992735436		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.2801187992735436 | validation: 0.22819603137240724]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29010293570022183		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.29010293570022183 | validation: 0.25530895316889296]
	TIME [epoch: 15.2 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2891791620022455		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.2891791620022455 | validation: 0.2631374690327892]
	TIME [epoch: 15.2 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2436164259574292		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.2436164259574292 | validation: 0.28581341251566067]
	TIME [epoch: 15.2 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24916326404062872		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.24916326404062872 | validation: 0.35622582284282195]
	TIME [epoch: 15.2 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29295312884724317		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.29295312884724317 | validation: 0.24823888117584297]
	TIME [epoch: 15.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2784046538237005		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.2784046538237005 | validation: 0.3170148544524251]
	TIME [epoch: 15.2 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2761113740732994		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.2761113740732994 | validation: 0.24842095915756196]
	TIME [epoch: 15.2 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24325067407597808		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.24325067407597808 | validation: 0.2793991215237154]
	TIME [epoch: 15.2 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25369985746437285		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.25369985746437285 | validation: 0.2886288042244333]
	TIME [epoch: 15.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26036603795721924		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.26036603795721924 | validation: 0.2950166305923233]
	TIME [epoch: 15.2 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.267464215977571		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.267464215977571 | validation: 0.32103087510059475]
	TIME [epoch: 15.2 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26429118419762676		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.26429118419762676 | validation: 0.28667694535919525]
	TIME [epoch: 15.2 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2534444042804217		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.2534444042804217 | validation: 0.2738412801976287]
	TIME [epoch: 15.2 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2697951259714257		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.2697951259714257 | validation: 0.40971364805525756]
	TIME [epoch: 15.2 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28179174330082885		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.28179174330082885 | validation: 0.2495811796067293]
	TIME [epoch: 15.2 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2435455988273345		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.2435455988273345 | validation: 0.2810576315443852]
	TIME [epoch: 15.2 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32741866208692827		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.32741866208692827 | validation: 0.26847082897892954]
	TIME [epoch: 15.2 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24693059163098002		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.24693059163098002 | validation: 0.35848336743658193]
	TIME [epoch: 15.2 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29335247545468673		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.29335247545468673 | validation: 0.2624548671546445]
	TIME [epoch: 15.2 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24193966716112686		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.24193966716112686 | validation: 0.35517295001596677]
	TIME [epoch: 15.2 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2875129549007297		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.2875129549007297 | validation: 0.2273216533640608]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23329384555862404		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.23329384555862404 | validation: 0.3444322636080296]
	TIME [epoch: 15.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26490336825587985		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.26490336825587985 | validation: 0.3269052883989243]
	TIME [epoch: 15.2 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25919219318301995		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.25919219318301995 | validation: 0.2581461602206771]
	TIME [epoch: 15.2 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23812203596876927		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.23812203596876927 | validation: 0.3320953784864366]
	TIME [epoch: 15.2 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30126703315246317		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.30126703315246317 | validation: 0.23889718192301085]
	TIME [epoch: 15.2 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23570439301245072		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.23570439301245072 | validation: 0.35645754205572877]
	TIME [epoch: 15.2 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27898707430657715		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.27898707430657715 | validation: 0.21917902990349827]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2356942413153866		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.2356942413153866 | validation: 0.28043985151512335]
	TIME [epoch: 15.2 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2459885358221287		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.2459885358221287 | validation: 0.32500237192568804]
	TIME [epoch: 15.2 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.269480073378021		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.269480073378021 | validation: 0.24281334260721002]
	TIME [epoch: 15.2 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2741432800113551		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.2741432800113551 | validation: 0.2631811722909884]
	TIME [epoch: 15.2 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28321014085937063		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.28321014085937063 | validation: 0.3222678632353477]
	TIME [epoch: 15.2 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27031987299475213		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.27031987299475213 | validation: 0.22022991866936192]
	TIME [epoch: 15.2 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22517456640520817		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.22517456640520817 | validation: 0.3125884943602641]
	TIME [epoch: 15.2 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24132938852724936		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.24132938852724936 | validation: 0.22441813854371664]
	TIME [epoch: 15.2 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2350967040777574		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.2350967040777574 | validation: 0.32166226259374303]
	TIME [epoch: 15.2 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28061238645862396		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.28061238645862396 | validation: 0.21934362289348847]
	TIME [epoch: 15.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22277852587605762		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.22277852587605762 | validation: 0.3098592889846711]
	TIME [epoch: 15.2 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2793403016604617		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.2793403016604617 | validation: 0.255909182729582]
	TIME [epoch: 15.2 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23382697201192446		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.23382697201192446 | validation: 0.2419066817666313]
	TIME [epoch: 15.2 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23975729302146354		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.23975729302146354 | validation: 0.2479987840674446]
	TIME [epoch: 15.2 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24343197451407145		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.24343197451407145 | validation: 0.22660711863585928]
	TIME [epoch: 15.2 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24644227176780348		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.24644227176780348 | validation: 0.21745055533321345]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23452813390494065		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.23452813390494065 | validation: 0.212010678037894]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_465.pth
	Model improved!!!
EPOCH 466/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24576176687037093		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.24576176687037093 | validation: 0.7037240631226113]
	TIME [epoch: 15.2 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.590703226844639		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.590703226844639 | validation: 0.7836123121682592]
	TIME [epoch: 15.2 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5389457221488909		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.5389457221488909 | validation: 0.44728305794816026]
	TIME [epoch: 15.2 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30323504012782343		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.30323504012782343 | validation: 0.26067387633304157]
	TIME [epoch: 15.2 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2270551449266744		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.2270551449266744 | validation: 0.25214361551337866]
	TIME [epoch: 15.2 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2260820096863618		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.2260820096863618 | validation: 0.3099757231566838]
	TIME [epoch: 15.2 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2515122054168524		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.2515122054168524 | validation: 0.23333113741348643]
	TIME [epoch: 15.2 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22849416900538122		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.22849416900538122 | validation: 0.2440638415943467]
	TIME [epoch: 15.2 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22612973032154443		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.22612973032154443 | validation: 0.27872741092775677]
	TIME [epoch: 15.2 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23921895132772836		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.23921895132772836 | validation: 0.23461649237868235]
	TIME [epoch: 15.2 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2233635897185422		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.2233635897185422 | validation: 0.2523762308875632]
	TIME [epoch: 15.2 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24238070397818404		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.24238070397818404 | validation: 0.23891534723060134]
	TIME [epoch: 15.2 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2666796046898561		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.2666796046898561 | validation: 0.21788765723038242]
	TIME [epoch: 15.2 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25780371894957077		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.25780371894957077 | validation: 0.3263228702885439]
	TIME [epoch: 15.2 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23694068415989483		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.23694068415989483 | validation: 0.2543043271268042]
	TIME [epoch: 15.2 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22141845014024975		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.22141845014024975 | validation: 0.3080173054648384]
	TIME [epoch: 15.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2406240863293058		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.2406240863293058 | validation: 0.2994197777417631]
	TIME [epoch: 15.2 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2380117266996623		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.2380117266996623 | validation: 0.23426617981947057]
	TIME [epoch: 15.2 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26816078581323316		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.26816078581323316 | validation: 0.35296105058449423]
	TIME [epoch: 15.2 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28086080285931064		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.28086080285931064 | validation: 0.3918914260023598]
	TIME [epoch: 15.2 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28176731252788473		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.28176731252788473 | validation: 0.25421684391305654]
	TIME [epoch: 15.2 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2309499086669558		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.2309499086669558 | validation: 0.2255957884554115]
	TIME [epoch: 15.2 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.226915043729151		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.226915043729151 | validation: 0.31619916560193356]
	TIME [epoch: 15.2 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25164251370232		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.25164251370232 | validation: 0.21203245193784373]
	TIME [epoch: 15.2 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2278256408631996		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.2278256408631996 | validation: 0.26573882766740065]
	TIME [epoch: 15.2 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23600428202685128		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.23600428202685128 | validation: 0.34381280938712594]
	TIME [epoch: 15.2 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25228014551383726		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.25228014551383726 | validation: 0.2780173956343347]
	TIME [epoch: 15.2 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23830464902914889		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.23830464902914889 | validation: 0.2651154911976179]
	TIME [epoch: 15.2 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22622797056080823		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.22622797056080823 | validation: 0.2879368590192102]
	TIME [epoch: 15.2 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3454112174142838		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.3454112174142838 | validation: 0.20516521804100885]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24860068923124376		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.24860068923124376 | validation: 0.319636910185227]
	TIME [epoch: 15.2 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.249673329810431		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.249673329810431 | validation: 0.22131901106997015]
	TIME [epoch: 15.2 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22359494600997487		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.22359494600997487 | validation: 0.25640189521526263]
	TIME [epoch: 15.2 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23215328856816772		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.23215328856816772 | validation: 0.3707249445390315]
	TIME [epoch: 15.2 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2834811590501217		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.2834811590501217 | validation: 0.2528399837150144]
	TIME [epoch: 15.2 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22004794732364782		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.22004794732364782 | validation: 0.25869080420745744]
	TIME [epoch: 117 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23209005478938524		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.23209005478938524 | validation: 0.22971480562080038]
	TIME [epoch: 32.7 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.247351229960719		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.247351229960719 | validation: 0.21934835591855598]
	TIME [epoch: 32.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.227781677780493		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.227781677780493 | validation: 0.2749144914206613]
	TIME [epoch: 32.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2340051672657306		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.2340051672657306 | validation: 0.2721004302506396]
	TIME [epoch: 32.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22853243487336383		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.22853243487336383 | validation: 0.2581766185558489]
	TIME [epoch: 32.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2357812526103314		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.2357812526103314 | validation: 0.2528539278581957]
	TIME [epoch: 32.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22583835952950815		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.22583835952950815 | validation: 0.21567400370249834]
	TIME [epoch: 32.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22663238065055738		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.22663238065055738 | validation: 0.24335116865232154]
	TIME [epoch: 32.6 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2203049147732869		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.2203049147732869 | validation: 0.2652905556999325]
	TIME [epoch: 32.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2711684516115015		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.2711684516115015 | validation: 0.2689241749267183]
	TIME [epoch: 32.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24061790682371345		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.24061790682371345 | validation: 0.24265904205811442]
	TIME [epoch: 32.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.225331602055945		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.225331602055945 | validation: 0.22824315955076738]
	TIME [epoch: 32.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2308266707967403		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.2308266707967403 | validation: 0.28777482867371657]
	TIME [epoch: 32.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23964520784105414		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.23964520784105414 | validation: 0.2666420949778684]
	TIME [epoch: 32.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23050816954999223		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.23050816954999223 | validation: 0.23584737907692396]
	TIME [epoch: 32.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22099091254493197		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.22099091254493197 | validation: 0.23977593563928412]
	TIME [epoch: 32.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2431712580745079		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.2431712580745079 | validation: 0.2690910935571563]
	TIME [epoch: 32.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23548758434659223		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.23548758434659223 | validation: 0.21866420860290203]
	TIME [epoch: 32.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21828170219561477		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.21828170219561477 | validation: 0.2337387770682713]
	TIME [epoch: 32.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2594986264594957		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.2594986264594957 | validation: 0.2037296183747067]
	TIME [epoch: 32.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_521.pth
	Model improved!!!
EPOCH 522/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22066926782435725		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.22066926782435725 | validation: 0.2452458752649327]
	TIME [epoch: 32.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23738828547027865		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.23738828547027865 | validation: 0.21639488284167196]
	TIME [epoch: 32.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21330273987722012		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.21330273987722012 | validation: 0.24304719418923876]
	TIME [epoch: 32.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22532097166953347		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.22532097166953347 | validation: 0.21032704948208042]
	TIME [epoch: 32.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2239015219651499		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.2239015219651499 | validation: 0.2136604148541606]
	TIME [epoch: 32.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22278397598916092		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.22278397598916092 | validation: 0.22369159995407484]
	TIME [epoch: 32.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2118920771603962		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.2118920771603962 | validation: 0.2205195520528263]
	TIME [epoch: 32.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21711027758682228		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.21711027758682228 | validation: 0.24187796977683457]
	TIME [epoch: 32.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2176088759178521		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.2176088759178521 | validation: 0.21935594853732276]
	TIME [epoch: 32.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2304287572230942		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.2304287572230942 | validation: 0.1980604877751807]
	TIME [epoch: 32.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2321095690520873		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.2321095690520873 | validation: 0.3000108780136163]
	TIME [epoch: 32.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24594370019702483		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.24594370019702483 | validation: 0.25741583891353276]
	TIME [epoch: 32.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22082179190876244		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.22082179190876244 | validation: 0.20789591936263832]
	TIME [epoch: 32.7 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21104471142473058		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.21104471142473058 | validation: 0.22177084061440422]
	TIME [epoch: 32.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23069756332205632		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.23069756332205632 | validation: 0.2101663923046711]
	TIME [epoch: 32.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2814340640251318		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.2814340640251318 | validation: 0.2949303308959578]
	TIME [epoch: 32.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3271727400642526		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.3271727400642526 | validation: 0.21656686110238096]
	TIME [epoch: 32.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24208951706517534		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.24208951706517534 | validation: 0.26594145450311385]
	TIME [epoch: 32.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24320289621577018		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.24320289621577018 | validation: 0.2827923091983558]
	TIME [epoch: 32.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2462902907841531		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.2462902907841531 | validation: 0.25084950873504985]
	TIME [epoch: 32.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23390177367494192		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.23390177367494192 | validation: 0.25632593702156464]
	TIME [epoch: 32.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23288196827926944		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.23288196827926944 | validation: 0.2623358443343358]
	TIME [epoch: 32.5 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24384528121109636		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.24384528121109636 | validation: 0.28691216659375673]
	TIME [epoch: 32.5 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23227121513142224		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.23227121513142224 | validation: 0.20837394200565307]
	TIME [epoch: 32.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21740142047868882		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.21740142047868882 | validation: 0.2384348064226167]
	TIME [epoch: 32.7 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24998752235445104		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.24998752235445104 | validation: 0.35457461701672766]
	TIME [epoch: 32.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2879106675315748		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.2879106675315748 | validation: 0.24134944305580214]
	TIME [epoch: 32.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2323608663019624		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.2323608663019624 | validation: 0.21566068773395008]
	TIME [epoch: 32.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22900750217064553		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.22900750217064553 | validation: 0.22877648884211418]
	TIME [epoch: 32.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2151348500018567		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.2151348500018567 | validation: 0.22938624103913718]
	TIME [epoch: 32.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22288225113242904		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.22288225113242904 | validation: 0.2834774361887867]
	TIME [epoch: 32.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2626328099948001		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.2626328099948001 | validation: 0.27519661035760307]
	TIME [epoch: 32.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24301661741637529		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.24301661741637529 | validation: 0.2487808340063955]
	TIME [epoch: 32.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26336458026949033		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.26336458026949033 | validation: 0.3322451408841525]
	TIME [epoch: 32.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2788050678340132		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.2788050678340132 | validation: 0.22878180074892876]
	TIME [epoch: 32.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23685147670488532		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.23685147670488532 | validation: 0.24707853458906062]
	TIME [epoch: 32.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2467135350145736		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.2467135350145736 | validation: 0.261721390157384]
	TIME [epoch: 32.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23639934065856516		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.23639934065856516 | validation: 0.2487971169846006]
	TIME [epoch: 32.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23039586741935952		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.23039586741935952 | validation: 0.2583069128960589]
	TIME [epoch: 32.5 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22989505799795978		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.22989505799795978 | validation: 0.2008073296561418]
	TIME [epoch: 32.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21829744880925994		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.21829744880925994 | validation: 0.24578275657646753]
	TIME [epoch: 32.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24578405170091963		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.24578405170091963 | validation: 0.2431381263745528]
	TIME [epoch: 32.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23102930320485945		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.23102930320485945 | validation: 0.23723369409126527]
	TIME [epoch: 32.5 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23770436448209215		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.23770436448209215 | validation: 0.2567256586399704]
	TIME [epoch: 32.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24690448082232758		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.24690448082232758 | validation: 0.22039274645143722]
	TIME [epoch: 32.5 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22243535297446185		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.22243535297446185 | validation: 0.24045192613325816]
	TIME [epoch: 32.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2281607696980767		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.2281607696980767 | validation: 0.23397036349786038]
	TIME [epoch: 32.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2278828284225705		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.2278828284225705 | validation: 0.20967514711208804]
	TIME [epoch: 32.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23501940071513322		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.23501940071513322 | validation: 0.28540660826823594]
	TIME [epoch: 32.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24255918455720735		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.24255918455720735 | validation: 0.24250727066888378]
	TIME [epoch: 32.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22547564716301194		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.22547564716301194 | validation: 0.22029950039194263]
	TIME [epoch: 32.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.216985993953226		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.216985993953226 | validation: 0.2684560742420358]
	TIME [epoch: 32.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22419746558888776		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.22419746558888776 | validation: 0.22310573833073885]
	TIME [epoch: 32.6 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2111399475887939		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.2111399475887939 | validation: 0.266194845461219]
	TIME [epoch: 32.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24372522147688044		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.24372522147688044 | validation: 0.24702679765707347]
	TIME [epoch: 32.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22265513198238418		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.22265513198238418 | validation: 0.20358968754122167]
	TIME [epoch: 32.6 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21078085162334406		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.21078085162334406 | validation: 0.20931470589227566]
	TIME [epoch: 32.6 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21265617095558753		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.21265617095558753 | validation: 0.2212478657749994]
	TIME [epoch: 32.6 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22589128561961402		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.22589128561961402 | validation: 0.2530679650384852]
	TIME [epoch: 32.6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22555690352701444		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.22555690352701444 | validation: 0.22501609391713587]
	TIME [epoch: 32.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2091973725114234		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.2091973725114234 | validation: 0.2290172779520214]
	TIME [epoch: 32.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2302546568261168		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.2302546568261168 | validation: 0.24117807385076012]
	TIME [epoch: 32.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2267238734203894		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.2267238734203894 | validation: 0.2592921632805447]
	TIME [epoch: 32.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22182935349015181		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.22182935349015181 | validation: 0.20945615328691658]
	TIME [epoch: 32.6 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20135838669310718		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.20135838669310718 | validation: 0.22607496606239796]
	TIME [epoch: 32.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2050984420523594		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.2050984420523594 | validation: 0.3597480542390823]
	TIME [epoch: 32.6 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2764101644568819		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.2764101644568819 | validation: 0.29756054681361105]
	TIME [epoch: 32.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2382143470037295		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.2382143470037295 | validation: 0.20652087333737637]
	TIME [epoch: 32.6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20474356453713938		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.20474356453713938 | validation: 0.2222664144140686]
	TIME [epoch: 32.6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21436231778975184		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.21436231778975184 | validation: 0.2582620453669628]
	TIME [epoch: 32.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24003017496394063		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.24003017496394063 | validation: 0.2476432862422312]
	TIME [epoch: 32.6 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21138182829358862		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.21138182829358862 | validation: 0.19337464274995805]
	TIME [epoch: 32.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2036840202348188		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.2036840202348188 | validation: 0.26442554349010544]
	TIME [epoch: 32.6 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23701282488234643		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.23701282488234643 | validation: 0.23330164515794674]
	TIME [epoch: 32.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20911850782149768		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.20911850782149768 | validation: 0.23904008651188696]
	TIME [epoch: 32.6 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2196575931193152		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.2196575931193152 | validation: 0.23283694658561205]
	TIME [epoch: 32.6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2089873060985773		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.2089873060985773 | validation: 0.2295938806388298]
	TIME [epoch: 32.6 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20577469667704554		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.20577469667704554 | validation: 0.1890632101569571]
	TIME [epoch: 32.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_599.pth
	Model improved!!!
EPOCH 600/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20314441089216365		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.20314441089216365 | validation: 0.25630297568323096]
	TIME [epoch: 32.6 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23336621657608853		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.23336621657608853 | validation: 0.25376711602608676]
	TIME [epoch: 32.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22742127027135822		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.22742127027135822 | validation: 0.2177715271997311]
	TIME [epoch: 32.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2062491390645015		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.2062491390645015 | validation: 0.22344916949670762]
	TIME [epoch: 32.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2293880829362846		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.2293880829362846 | validation: 0.2541500492154377]
	TIME [epoch: 32.6 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22258027022579915		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.22258027022579915 | validation: 0.19385506912132128]
	TIME [epoch: 32.6 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21002958893822193		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.21002958893822193 | validation: 0.22460034939948026]
	TIME [epoch: 32.6 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2066639501162113		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.2066639501162113 | validation: 0.2833272544006569]
	TIME [epoch: 32.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21566386100850082		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.21566386100850082 | validation: 0.24081683912895302]
	TIME [epoch: 32.6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2215342061524882		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.2215342061524882 | validation: 0.2797677238530673]
	TIME [epoch: 32.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22643848100786892		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.22643848100786892 | validation: 0.20060628064322056]
	TIME [epoch: 32.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19819762805187824		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.19819762805187824 | validation: 0.20961938887163561]
	TIME [epoch: 32.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20364831236189235		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.20364831236189235 | validation: 0.2566672925891306]
	TIME [epoch: 32.6 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22315274873836555		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.22315274873836555 | validation: 0.22504210413689063]
	TIME [epoch: 32.6 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21117495310920326		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.21117495310920326 | validation: 0.28166854377438794]
	TIME [epoch: 32.6 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2227377209041938		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.2227377209041938 | validation: 0.23652559255389427]
	TIME [epoch: 32.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20228311826010614		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.20228311826010614 | validation: 0.23780976677963017]
	TIME [epoch: 32.6 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21525014476660742		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.21525014476660742 | validation: 0.24391337454130968]
	TIME [epoch: 32.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20818057631756035		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.20818057631756035 | validation: 0.23666027405647005]
	TIME [epoch: 32.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2069847567420813		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.2069847567420813 | validation: 0.24927433266299223]
	TIME [epoch: 32.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21601675987831184		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.21601675987831184 | validation: 0.23898795607324905]
	TIME [epoch: 32.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2072409735900273		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.2072409735900273 | validation: 0.21474407112717664]
	TIME [epoch: 32.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2077981625091414		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.2077981625091414 | validation: 0.2485292825347567]
	TIME [epoch: 32.6 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21455130222300156		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.21455130222300156 | validation: 0.20407093384805253]
	TIME [epoch: 32.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21169960484541736		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.21169960484541736 | validation: 0.24264292125160758]
	TIME [epoch: 32.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2016473844046207		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.2016473844046207 | validation: 0.24210983666326513]
	TIME [epoch: 32.6 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2467905317558796		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.2467905317558796 | validation: 0.3828380843233622]
	TIME [epoch: 32.6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27680304686041224		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.27680304686041224 | validation: 0.3046433715201576]
	TIME [epoch: 32.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24335455726751842		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.24335455726751842 | validation: 0.23388672783468412]
	TIME [epoch: 32.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20839029817394483		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.20839029817394483 | validation: 0.20020597941958665]
	TIME [epoch: 32.6 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20041834163672015		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.20041834163672015 | validation: 0.20119486644111706]
	TIME [epoch: 32.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.194564630331201		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.194564630331201 | validation: 0.20607968023454967]
	TIME [epoch: 32.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20640861995771154		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.20640861995771154 | validation: 0.24408382914338056]
	TIME [epoch: 32.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20698135328684672		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.20698135328684672 | validation: 0.25259252380617875]
	TIME [epoch: 32.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21044071247557583		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.21044071247557583 | validation: 0.33251189381534335]
	TIME [epoch: 32.6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26815552888293515		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.26815552888293515 | validation: 0.30378104311776694]
	TIME [epoch: 32.6 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23359958392923935		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.23359958392923935 | validation: 0.2467804326829528]
	TIME [epoch: 32.6 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21151199117691055		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.21151199117691055 | validation: 0.22807809627603298]
	TIME [epoch: 32.6 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20828467399114003		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.20828467399114003 | validation: 0.21675512636967192]
	TIME [epoch: 32.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20885701750618701		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.20885701750618701 | validation: 0.21851648730625156]
	TIME [epoch: 32.7 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1958687307688493		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.1958687307688493 | validation: 0.23359330800451455]
	TIME [epoch: 32.8 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21145761999131218		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.21145761999131218 | validation: 0.25280146625763794]
	TIME [epoch: 32.8 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2046831259423539		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.2046831259423539 | validation: 0.23135784157973882]
	TIME [epoch: 32.8 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21641707918983388		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.21641707918983388 | validation: 0.25955620216183933]
	TIME [epoch: 32.7 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22238280936515084		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.22238280936515084 | validation: 0.26374324731872595]
	TIME [epoch: 32.6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20960575839147655		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.20960575839147655 | validation: 0.22302554617978576]
	TIME [epoch: 32.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20284761611343105		[learning rate: 0.00012324]
	Learning Rate: 0.000123245
	LOSS [training: 0.20284761611343105 | validation: 0.2318013493848161]
	TIME [epoch: 32.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20609897015084566		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.20609897015084566 | validation: 0.2513103595019446]
	TIME [epoch: 32.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22099388999027414		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.22099388999027414 | validation: 0.2516174679196894]
	TIME [epoch: 32.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20763407049365967		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.20763407049365967 | validation: 0.2502581717904404]
	TIME [epoch: 32.6 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2079306787180552		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.2079306787180552 | validation: 0.2141460241457115]
	TIME [epoch: 32.5 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2007304391102781		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.2007304391102781 | validation: 0.2075659825540168]
	TIME [epoch: 32.6 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20583411110422095		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.20583411110422095 | validation: 0.21573395128239353]
	TIME [epoch: 32.6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19191388925400887		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.19191388925400887 | validation: 0.25053966157086954]
	TIME [epoch: 32.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20933631222767335		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.20933631222767335 | validation: 0.2561136082870605]
	TIME [epoch: 32.6 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21705719280087427		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.21705719280087427 | validation: 0.24176184615711604]
	TIME [epoch: 32.6 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2057307184713676		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.2057307184713676 | validation: 0.21945154193592684]
	TIME [epoch: 32.6 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19994330234813934		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.19994330234813934 | validation: 0.18984239640397615]
	TIME [epoch: 32.5 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20379860393015625		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.20379860393015625 | validation: 0.21289089487130544]
	TIME [epoch: 32.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20907914762837126		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.20907914762837126 | validation: 0.22809070799237718]
	TIME [epoch: 32.6 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.196818821539989		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.196818821539989 | validation: 0.21145694873999465]
	TIME [epoch: 32.6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20758982921916497		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.20758982921916497 | validation: 0.2240017972182709]
	TIME [epoch: 32.5 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19856328558947012		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.19856328558947012 | validation: 0.2341461837741898]
	TIME [epoch: 32.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21355224427067465		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.21355224427067465 | validation: 0.23529758929798217]
	TIME [epoch: 32.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2019859658583498		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.2019859658583498 | validation: 0.2672829093479905]
	TIME [epoch: 32.6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22280778226949122		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.22280778226949122 | validation: 0.21368192377418566]
	TIME [epoch: 32.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19873493871539794		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.19873493871539794 | validation: 0.20822809432811923]
	TIME [epoch: 32.6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2015446602011214		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.2015446602011214 | validation: 0.25438767127815437]
	TIME [epoch: 32.5 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21740901938951498		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.21740901938951498 | validation: 0.21120467136230708]
	TIME [epoch: 32.6 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20138409650676403		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.20138409650676403 | validation: 0.22287017123019615]
	TIME [epoch: 32.5 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2087343539674108		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.2087343539674108 | validation: 0.23541534671367703]
	TIME [epoch: 32.5 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21782276426734012		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.21782276426734012 | validation: 0.3044101825614156]
	TIME [epoch: 32.5 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2293624247868467		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.2293624247868467 | validation: 0.23815782601504512]
	TIME [epoch: 32.6 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20721614605956395		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.20721614605956395 | validation: 0.20082739131429683]
	TIME [epoch: 32.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19991156265782664		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.19991156265782664 | validation: 0.1858644836504201]
	TIME [epoch: 32.6 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_674.pth
	Model improved!!!
EPOCH 675/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21219543638763522		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.21219543638763522 | validation: 0.19701013332405504]
	TIME [epoch: 32.5 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1870694849695882		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.1870694849695882 | validation: 0.24630770464091245]
	TIME [epoch: 32.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2202646483702388		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.2202646483702388 | validation: 0.24153216829599]
	TIME [epoch: 32.5 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20330556220683516		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.20330556220683516 | validation: 0.22176657551669576]
	TIME [epoch: 32.5 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19767213576871984		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.19767213576871984 | validation: 0.21802430947292556]
	TIME [epoch: 32.5 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19551658480661144		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.19551658480661144 | validation: 0.2052312512140791]
	TIME [epoch: 32.6 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20337830061689743		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.20337830061689743 | validation: 0.21580435808424833]
	TIME [epoch: 32.6 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19861656815981732		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.19861656815981732 | validation: 0.23274815471426097]
	TIME [epoch: 32.6 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21256532322901844		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.21256532322901844 | validation: 0.20846591284190988]
	TIME [epoch: 32.5 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20270802222973128		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.20270802222973128 | validation: 0.22447173147067387]
	TIME [epoch: 32.5 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19177230445057716		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.19177230445057716 | validation: 0.23734897841992098]
	TIME [epoch: 32.5 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20349243991959387		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.20349243991959387 | validation: 0.24675310944847295]
	TIME [epoch: 32.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21700310134933246		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.21700310134933246 | validation: 0.23780467512730247]
	TIME [epoch: 32.6 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2052971011708644		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.2052971011708644 | validation: 0.202973120079231]
	TIME [epoch: 32.6 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19882448540340303		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.19882448540340303 | validation: 0.22964886157660205]
	TIME [epoch: 32.5 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2020285119077992		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.2020285119077992 | validation: 0.21612232259852823]
	TIME [epoch: 32.5 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19655690932266756		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.19655690932266756 | validation: 0.1964889688300421]
	TIME [epoch: 32.5 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1924948701121985		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.1924948701121985 | validation: 0.2110273588930491]
	TIME [epoch: 32.5 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20038003861553255		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.20038003861553255 | validation: 0.2711361418521667]
	TIME [epoch: 32.6 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21646862780369402		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.21646862780369402 | validation: 0.21171792983130888]
	TIME [epoch: 32.5 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20146705600176545		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.20146705600176545 | validation: 0.19240672743594917]
	TIME [epoch: 32.5 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1943329841457297		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.1943329841457297 | validation: 0.19385772243182478]
	TIME [epoch: 32.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1994481814053163		[learning rate: 8.5871e-05]
	Learning Rate: 8.58709e-05
	LOSS [training: 0.1994481814053163 | validation: 0.1792256280045224]
	TIME [epoch: 32.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_697.pth
	Model improved!!!
EPOCH 698/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19242468432064957		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.19242468432064957 | validation: 0.2006385049640641]
	TIME [epoch: 32.5 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18907755003641696		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.18907755003641696 | validation: 0.18745502937267167]
	TIME [epoch: 32.6 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1874377981678526		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.1874377981678526 | validation: 0.20065156008963553]
	TIME [epoch: 32.5 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.193223684730128		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.193223684730128 | validation: 0.21365480889392074]
	TIME [epoch: 32.5 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1963332270081658		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.1963332270081658 | validation: 0.23145690532491225]
	TIME [epoch: 32.5 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.203435244539579		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.203435244539579 | validation: 0.20835316321205577]
	TIME [epoch: 32.5 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19880463114135283		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.19880463114135283 | validation: 0.20602013762067026]
	TIME [epoch: 32.5 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20323980099104394		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.20323980099104394 | validation: 0.23654901633381692]
	TIME [epoch: 32.5 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2079665199489002		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.2079665199489002 | validation: 0.23814825596618605]
	TIME [epoch: 32.5 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2189144765677629		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.2189144765677629 | validation: 0.224533624784632]
	TIME [epoch: 32.5 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20033753721111225		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.20033753721111225 | validation: 0.21489763599342604]
	TIME [epoch: 32.5 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19844993335605743		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.19844993335605743 | validation: 0.19901082498818756]
	TIME [epoch: 32.5 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19010779523876364		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.19010779523876364 | validation: 0.2115232468592895]
	TIME [epoch: 32.5 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19912344676714294		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.19912344676714294 | validation: 0.21273242022039646]
	TIME [epoch: 32.5 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20181569986790432		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.20181569986790432 | validation: 0.1966056257697505]
	TIME [epoch: 32.5 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19672202019717266		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.19672202019717266 | validation: 0.21994969440072054]
	TIME [epoch: 32.5 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20251748834821542		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.20251748834821542 | validation: 0.2131975669300511]
	TIME [epoch: 32.5 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20184526323597554		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.20184526323597554 | validation: 0.213596238644775]
	TIME [epoch: 32.5 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21599134559929362		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.21599134559929362 | validation: 0.2520045397048969]
	TIME [epoch: 32.5 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23703465650565952		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.23703465650565952 | validation: 0.262698588672597]
	TIME [epoch: 32.5 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21027162507476743		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.21027162507476743 | validation: 0.21612034557676593]
	TIME [epoch: 32.5 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1962417329160099		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.1962417329160099 | validation: 0.1891877595615181]
	TIME [epoch: 32.5 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19915444587494846		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.19915444587494846 | validation: 0.2001814787531897]
	TIME [epoch: 32.5 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18908649965544677		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.18908649965544677 | validation: 0.20134006218547135]
	TIME [epoch: 32.6 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18896179336665053		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.18896179336665053 | validation: 0.20595079843169045]
	TIME [epoch: 32.5 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21176197144551917		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.21176197144551917 | validation: 0.2346079950277405]
	TIME [epoch: 32.5 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21477701833896345		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.21477701833896345 | validation: 0.2348163404729572]
	TIME [epoch: 32.5 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21433757103873968		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.21433757103873968 | validation: 0.23240484568380274]
	TIME [epoch: 32.5 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2048354864986858		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.2048354864986858 | validation: 0.22029402013675733]
	TIME [epoch: 32.5 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19787723362389176		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.19787723362389176 | validation: 0.20390330637158507]
	TIME [epoch: 32.5 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1987703853593531		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.1987703853593531 | validation: 0.22977008910182387]
	TIME [epoch: 32.5 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2041447438976773		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.2041447438976773 | validation: 0.2237037326181051]
	TIME [epoch: 32.5 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2027477284807908		[learning rate: 6.7968e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.2027477284807908 | validation: 0.2004916699156631]
	TIME [epoch: 32.5 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1948640766584642		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.1948640766584642 | validation: 0.20798419408802027]
	TIME [epoch: 32.5 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1948281469373436		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.1948281469373436 | validation: 0.19349401230685043]
	TIME [epoch: 32.5 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18527862464021422		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.18527862464021422 | validation: 0.20065285264947358]
	TIME [epoch: 32.5 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1980656726353266		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.1980656726353266 | validation: 0.22466620837006604]
	TIME [epoch: 32.5 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20951481768714691		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.20951481768714691 | validation: 0.19984734230163337]
	TIME [epoch: 32.5 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18889061939039276		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.18889061939039276 | validation: 0.18752944567746577]
	TIME [epoch: 32.6 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19680066121696288		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.19680066121696288 | validation: 0.19934207907872648]
	TIME [epoch: 32.6 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20265210488524243		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.20265210488524243 | validation: 0.21909989730943613]
	TIME [epoch: 32.6 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20196376097070434		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.20196376097070434 | validation: 0.2162970218605661]
	TIME [epoch: 32.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19454772864571215		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.19454772864571215 | validation: 0.22283830451130157]
	TIME [epoch: 32.5 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19803444646797752		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.19803444646797752 | validation: 0.2099262132408539]
	TIME [epoch: 32.5 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19968217113371572		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.19968217113371572 | validation: 0.2025440830295751]
	TIME [epoch: 32.5 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19588355140965616		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.19588355140965616 | validation: 0.23342191523503208]
	TIME [epoch: 32.5 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20864991740453592		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.20864991740453592 | validation: 0.23446217086539792]
	TIME [epoch: 32.5 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20876253274147438		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.20876253274147438 | validation: 0.22279635096131414]
	TIME [epoch: 32.5 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2084665842796361		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.2084665842796361 | validation: 0.22995177096554364]
	TIME [epoch: 32.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20744554471089194		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.20744554471089194 | validation: 0.22302112104440655]
	TIME [epoch: 32.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20420558562606533		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.20420558562606533 | validation: 0.23235529968264867]
	TIME [epoch: 32.5 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22045835044228568		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.22045835044228568 | validation: 0.2280131539830312]
	TIME [epoch: 32.5 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2012373550511689		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.2012373550511689 | validation: 0.21895803258644408]
	TIME [epoch: 32.5 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21983277235731474		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.21983277235731474 | validation: 0.21181909645375463]
	TIME [epoch: 32.5 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19809770633963236		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.19809770633963236 | validation: 0.21209607600188482]
	TIME [epoch: 32.5 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2048605325376705		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.2048605325376705 | validation: 0.24038253290532108]
	TIME [epoch: 32.5 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21725871957924325		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.21725871957924325 | validation: 0.2219151452486131]
	TIME [epoch: 32.6 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20705845237211393		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.20705845237211393 | validation: 0.2077023338710655]
	TIME [epoch: 32.5 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19364926499964913		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.19364926499964913 | validation: 0.2074174995883194]
	TIME [epoch: 32.5 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19121098603696335		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.19121098603696335 | validation: 0.18565058927853173]
	TIME [epoch: 32.5 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1860196737524469		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.1860196737524469 | validation: 0.19651823828094347]
	TIME [epoch: 32.5 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1848352587585494		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.1848352587585494 | validation: 0.20946841159449378]
	TIME [epoch: 32.5 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19642413898785854		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.19642413898785854 | validation: 0.1935065403094823]
	TIME [epoch: 32.5 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19343399309680037		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.19343399309680037 | validation: 0.19366782776649305]
	TIME [epoch: 32.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18575561636932741		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.18575561636932741 | validation: 0.2064946180203341]
	TIME [epoch: 32.5 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19202540901445414		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.19202540901445414 | validation: 0.2199008172667746]
	TIME [epoch: 32.5 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19817578537344493		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.19817578537344493 | validation: 0.22445985483342998]
	TIME [epoch: 32.5 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21411346356067973		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.21411346356067973 | validation: 0.24561975562193716]
	TIME [epoch: 32.5 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21285285428987283		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.21285285428987283 | validation: 0.2289078416675322]
	TIME [epoch: 32.5 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20730310609591396		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.20730310609591396 | validation: 0.21618547498258464]
	TIME [epoch: 32.5 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2048525094516603		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.2048525094516603 | validation: 0.21771634720265198]
	TIME [epoch: 32.5 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20006606492639328		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.20006606492639328 | validation: 0.20840848587794675]
	TIME [epoch: 32.5 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2010362063772239		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.2010362063772239 | validation: 0.24688838549088407]
	TIME [epoch: 32.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21447231936794137		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.21447231936794137 | validation: 0.2091957890289911]
	TIME [epoch: 32.5 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19263951136759974		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.19263951136759974 | validation: 0.19338267455089386]
	TIME [epoch: 32.5 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18735728777737132		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.18735728777737132 | validation: 0.19331007090295205]
	TIME [epoch: 32.5 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19254401809084104		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.19254401809084104 | validation: 0.2083567414779867]
	TIME [epoch: 32.5 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20835915300986135		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.20835915300986135 | validation: 0.22278342572589188]
	TIME [epoch: 32.5 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19626804573793427		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.19626804573793427 | validation: 0.21016972004938542]
	TIME [epoch: 32.5 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.203796579921861		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.203796579921861 | validation: 0.2400849067611858]
	TIME [epoch: 32.5 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21343062531014811		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.21343062531014811 | validation: 0.2296598185557505]
	TIME [epoch: 32.5 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21739860378401193		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.21739860378401193 | validation: 0.2327198467394859]
	TIME [epoch: 32.5 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20658063310744443		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.20658063310744443 | validation: 0.22342676430744846]
	TIME [epoch: 32.5 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20737576766892984		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.20737576766892984 | validation: 0.23010882626089568]
	TIME [epoch: 32.5 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2000146287873319		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.2000146287873319 | validation: 0.22697160907431418]
	TIME [epoch: 32.5 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21104282186038048		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.21104282186038048 | validation: 0.24810867949125806]
	TIME [epoch: 32.5 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21109432794112168		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.21109432794112168 | validation: 0.22973013453630423]
	TIME [epoch: 32.5 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21068087225438428		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.21068087225438428 | validation: 0.24378169222775617]
	TIME [epoch: 32.5 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19673403096809122		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.19673403096809122 | validation: 0.2138791214655311]
	TIME [epoch: 32.5 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2011643355917234		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.2011643355917234 | validation: 0.23439976543864904]
	TIME [epoch: 32.5 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.201735626620291		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.201735626620291 | validation: 0.22624531482671115]
	TIME [epoch: 32.5 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19870817595188595		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.19870817595188595 | validation: 0.2155758967751416]
	TIME [epoch: 32.5 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21567048637294678		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.21567048637294678 | validation: 0.21362305695200678]
	TIME [epoch: 32.5 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2023467730948927		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.2023467730948927 | validation: 0.22864833337087626]
	TIME [epoch: 32.5 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20964403806069398		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.20964403806069398 | validation: 0.23041074916674803]
	TIME [epoch: 32.5 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20663643573466456		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.20663643573466456 | validation: 0.22655083844777213]
	TIME [epoch: 32.5 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19852212383646695		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.19852212383646695 | validation: 0.22600961357536847]
	TIME [epoch: 32.5 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20607814780141182		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.20607814780141182 | validation: 0.21665144434339872]
	TIME [epoch: 32.5 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1968497881744536		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.1968497881744536 | validation: 0.22963185231574545]
	TIME [epoch: 32.5 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20006042675166708		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.20006042675166708 | validation: 0.23674252993418926]
	TIME [epoch: 32.5 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21023863537607232		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.21023863537607232 | validation: 0.22762237832007923]
	TIME [epoch: 32.5 sec]
	Saving model to: out/model_training/model_phi1_2c_v_mmd1_20240807_171303/states/model_phi1_2c_v_mmd1_798.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 16000.085 seconds.
