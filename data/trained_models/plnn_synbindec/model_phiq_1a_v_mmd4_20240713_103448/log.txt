Args:
Namespace(name='model_phiq_1a_v_mmd4', outdir='out/model_training/model_phiq_1a_v_mmd4', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.1, weight_decay=0.9, clip=1.0, lr_schedule='warmup_cosine_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1030268432

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 4.767073465223445		[learning rate: 0.01015]
	Learning Rate: 0.01015
	LOSS [training: 4.767073465223445 | validation: 4.607369953502834]
	TIME [epoch: 170 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.523583070113876		[learning rate: 0.01035]
	Learning Rate: 0.01035
	LOSS [training: 4.523583070113876 | validation: 4.362077056005495]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5658753127905936		[learning rate: 0.01055]
	Learning Rate: 0.01055
	LOSS [training: 4.5658753127905936 | validation: 4.584107014015533]
	TIME [epoch: 7.66 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.379017228716341		[learning rate: 0.01075]
	Learning Rate: 0.01075
	LOSS [training: 4.379017228716341 | validation: 4.303767170076409]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158341028765505		[learning rate: 0.01095]
	Learning Rate: 0.01095
	LOSS [training: 4.158341028765505 | validation: 4.272790031315255]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.091403754434056		[learning rate: 0.01115]
	Learning Rate: 0.01115
	LOSS [training: 4.091403754434056 | validation: 3.9100956772615665]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9857466764711806		[learning rate: 0.01135]
	Learning Rate: 0.01135
	LOSS [training: 3.9857466764711806 | validation: 3.9345036388533225]
	TIME [epoch: 7.65 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8942206178238035		[learning rate: 0.01155]
	Learning Rate: 0.01155
	LOSS [training: 3.8942206178238035 | validation: 3.9442084949817975]
	TIME [epoch: 7.66 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8376731703644253		[learning rate: 0.01175]
	Learning Rate: 0.01175
	LOSS [training: 3.8376731703644253 | validation: 3.7065459477533995]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.752035605285034		[learning rate: 0.01195]
	Learning Rate: 0.01195
	LOSS [training: 3.752035605285034 | validation: 3.695893657114352]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6822906985724364		[learning rate: 0.01215]
	Learning Rate: 0.01215
	LOSS [training: 3.6822906985724364 | validation: 3.5670522543171623]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5987913836296794		[learning rate: 0.01235]
	Learning Rate: 0.01235
	LOSS [training: 3.5987913836296794 | validation: 3.5989209515767713]
	TIME [epoch: 7.67 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5706647489537255		[learning rate: 0.01255]
	Learning Rate: 0.01255
	LOSS [training: 3.5706647489537255 | validation: 3.553125998068616]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.481953828373377		[learning rate: 0.01275]
	Learning Rate: 0.01275
	LOSS [training: 3.481953828373377 | validation: 3.427279498074964]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3864403691840574		[learning rate: 0.01295]
	Learning Rate: 0.01295
	LOSS [training: 3.3864403691840574 | validation: 3.3785998391636616]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.337340591818968		[learning rate: 0.01315]
	Learning Rate: 0.01315
	LOSS [training: 3.337340591818968 | validation: 3.2582739677053176]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.220089658802298		[learning rate: 0.01335]
	Learning Rate: 0.01335
	LOSS [training: 3.220089658802298 | validation: 3.29133537474375]
	TIME [epoch: 7.67 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2222305299869216		[learning rate: 0.01355]
	Learning Rate: 0.01355
	LOSS [training: 3.2222305299869216 | validation: 3.130040416088359]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.11750096461096		[learning rate: 0.01375]
	Learning Rate: 0.01375
	LOSS [training: 3.11750096461096 | validation: 3.067328187268882]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.026850159992378		[learning rate: 0.01395]
	Learning Rate: 0.01395
	LOSS [training: 3.026850159992378 | validation: 3.009333452232871]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9618404443113224		[learning rate: 0.01415]
	Learning Rate: 0.01415
	LOSS [training: 2.9618404443113224 | validation: 2.912745794255641]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9232886180180424		[learning rate: 0.01435]
	Learning Rate: 0.01435
	LOSS [training: 2.9232886180180424 | validation: 2.89503592598483]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.828422274741957		[learning rate: 0.01455]
	Learning Rate: 0.01455
	LOSS [training: 2.828422274741957 | validation: 2.8227747811382624]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7517031850378917		[learning rate: 0.01475]
	Learning Rate: 0.01475
	LOSS [training: 2.7517031850378917 | validation: 2.7352426547444058]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.671929354720144		[learning rate: 0.01495]
	Learning Rate: 0.01495
	LOSS [training: 2.671929354720144 | validation: 2.6615018246602338]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5944059408394438		[learning rate: 0.01515]
	Learning Rate: 0.01515
	LOSS [training: 2.5944059408394438 | validation: 2.59767000630296]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.530031103350061		[learning rate: 0.01535]
	Learning Rate: 0.01535
	LOSS [training: 2.530031103350061 | validation: 2.5192023508837718]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.457193476963581		[learning rate: 0.01555]
	Learning Rate: 0.01555
	LOSS [training: 2.457193476963581 | validation: 2.4439911353757773]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3735043468762944		[learning rate: 0.01575]
	Learning Rate: 0.01575
	LOSS [training: 2.3735043468762944 | validation: 2.351075294917946]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2930667479235454		[learning rate: 0.01595]
	Learning Rate: 0.01595
	LOSS [training: 2.2930667479235454 | validation: 2.3193333198257458]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.282747234809472		[learning rate: 0.01615]
	Learning Rate: 0.01615
	LOSS [training: 2.282747234809472 | validation: 2.1996868328488492]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.167727690384015		[learning rate: 0.01635]
	Learning Rate: 0.01635
	LOSS [training: 2.167727690384015 | validation: 2.188148008699725]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1095209775838257		[learning rate: 0.01655]
	Learning Rate: 0.01655
	LOSS [training: 2.1095209775838257 | validation: 2.067846924854257]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.03848906604481		[learning rate: 0.01675]
	Learning Rate: 0.01675
	LOSS [training: 2.03848906604481 | validation: 2.0415552905515915]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9856799079599605		[learning rate: 0.01695]
	Learning Rate: 0.01695
	LOSS [training: 1.9856799079599605 | validation: 1.948507171574756]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.922480101787622		[learning rate: 0.01715]
	Learning Rate: 0.01715
	LOSS [training: 1.922480101787622 | validation: 1.9415037383597513]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9092164679051664		[learning rate: 0.01735]
	Learning Rate: 0.01735
	LOSS [training: 1.9092164679051664 | validation: 1.8614794434134283]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8370126079259936		[learning rate: 0.01755]
	Learning Rate: 0.01755
	LOSS [training: 1.8370126079259936 | validation: 1.8028599713002154]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7826650948791123		[learning rate: 0.01775]
	Learning Rate: 0.01775
	LOSS [training: 1.7826650948791123 | validation: 1.7420020031692305]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.752578586119732		[learning rate: 0.01795]
	Learning Rate: 0.01795
	LOSS [training: 1.752578586119732 | validation: 1.6965515500442563]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6942698867488255		[learning rate: 0.01815]
	Learning Rate: 0.01815
	LOSS [training: 1.6942698867488255 | validation: 1.6969971699426902]
	TIME [epoch: 7.67 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6465983492166507		[learning rate: 0.01835]
	Learning Rate: 0.01835
	LOSS [training: 1.6465983492166507 | validation: 1.613415061402662]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6228475561506635		[learning rate: 0.01855]
	Learning Rate: 0.01855
	LOSS [training: 1.6228475561506635 | validation: 1.5994189714082045]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5921944338264615		[learning rate: 0.01875]
	Learning Rate: 0.01875
	LOSS [training: 1.5921944338264615 | validation: 1.5706706349127453]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.52928651110946		[learning rate: 0.01895]
	Learning Rate: 0.01895
	LOSS [training: 1.52928651110946 | validation: 1.5476997004290096]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5319519504540369		[learning rate: 0.01915]
	Learning Rate: 0.01915
	LOSS [training: 1.5319519504540369 | validation: 1.49976010441187]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4784873953932318		[learning rate: 0.01935]
	Learning Rate: 0.01935
	LOSS [training: 1.4784873953932318 | validation: 1.4902774291714116]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5308580904847569		[learning rate: 0.01955]
	Learning Rate: 0.01955
	LOSS [training: 1.5308580904847569 | validation: 1.5986053035276475]
	TIME [epoch: 7.71 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.465701827693945		[learning rate: 0.01975]
	Learning Rate: 0.01975
	LOSS [training: 1.465701827693945 | validation: 1.4377095534318127]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.397664121022295		[learning rate: 0.01995]
	Learning Rate: 0.01995
	LOSS [training: 1.397664121022295 | validation: 1.4234308989941051]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.386602575738999		[learning rate: 0.02]
	Learning Rate: 0.02
	LOSS [training: 1.386602575738999 | validation: 1.3811670707689359]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3801071637707205		[learning rate: 0.02]
	Learning Rate: 0.02
	LOSS [training: 1.3801071637707205 | validation: 1.3750753775370623]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3706279358917275		[learning rate: 0.02]
	Learning Rate: 0.0199999
	LOSS [training: 1.3706279358917275 | validation: 1.3748148635319437]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3497247765620113		[learning rate: 0.02]
	Learning Rate: 0.0199998
	LOSS [training: 1.3497247765620113 | validation: 1.4210411766108089]
	TIME [epoch: 7.66 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3702602873568672		[learning rate: 0.02]
	Learning Rate: 0.0199997
	LOSS [training: 1.3702602873568672 | validation: 1.371191174134382]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3251246625407127		[learning rate: 0.02]
	Learning Rate: 0.0199995
	LOSS [training: 1.3251246625407127 | validation: 1.338449988723801]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3172852432145123		[learning rate: 0.019999]
	Learning Rate: 0.0199994
	LOSS [training: 1.3172852432145123 | validation: 1.3652572283191486]
	TIME [epoch: 7.66 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.304834801663329		[learning rate: 0.019999]
	Learning Rate: 0.0199992
	LOSS [training: 1.304834801663329 | validation: 1.3025343090160266]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3104300889420324		[learning rate: 0.019999]
	Learning Rate: 0.019999
	LOSS [training: 1.3104300889420324 | validation: 1.3242795597749217]
	TIME [epoch: 7.67 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3159760730190142		[learning rate: 0.019999]
	Learning Rate: 0.0199987
	LOSS [training: 1.3159760730190142 | validation: 1.3111536664131043]
	TIME [epoch: 7.66 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2859532787568442		[learning rate: 0.019998]
	Learning Rate: 0.0199984
	LOSS [training: 1.2859532787568442 | validation: 1.2925710117951548]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2769744522982394		[learning rate: 0.019998]
	Learning Rate: 0.0199981
	LOSS [training: 1.2769744522982394 | validation: 1.3463680537326592]
	TIME [epoch: 7.68 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2900773912766426		[learning rate: 0.019998]
	Learning Rate: 0.0199978
	LOSS [training: 1.2900773912766426 | validation: 1.299114313410323]
	TIME [epoch: 7.69 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2777505992850784		[learning rate: 0.019997]
	Learning Rate: 0.0199974
	LOSS [training: 1.2777505992850784 | validation: 1.2938292300637633]
	TIME [epoch: 7.66 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2762322921491422		[learning rate: 0.019997]
	Learning Rate: 0.019997
	LOSS [training: 1.2762322921491422 | validation: 1.2738485462112066]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.259520775846547		[learning rate: 0.019997]
	Learning Rate: 0.0199966
	LOSS [training: 1.259520775846547 | validation: 1.2651659667562747]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.256149408305934		[learning rate: 0.019996]
	Learning Rate: 0.0199962
	LOSS [training: 1.256149408305934 | validation: 1.2776343740094376]
	TIME [epoch: 7.68 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2640007716544992		[learning rate: 0.019996]
	Learning Rate: 0.0199957
	LOSS [training: 1.2640007716544992 | validation: 1.2560445456469345]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2534975699619184		[learning rate: 0.019995]
	Learning Rate: 0.0199952
	LOSS [training: 1.2534975699619184 | validation: 1.338292780893459]
	TIME [epoch: 7.63 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2756169702852098		[learning rate: 0.019995]
	Learning Rate: 0.0199947
	LOSS [training: 1.2756169702852098 | validation: 1.254052940989792]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2511775111511796		[learning rate: 0.019994]
	Learning Rate: 0.0199941
	LOSS [training: 1.2511775111511796 | validation: 1.2495879234363578]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2426645429889263		[learning rate: 0.019994]
	Learning Rate: 0.0199935
	LOSS [training: 1.2426645429889263 | validation: 1.2338320885474445]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2247519938461615		[learning rate: 0.019993]
	Learning Rate: 0.0199929
	LOSS [training: 1.2247519938461615 | validation: 1.2195088039973545]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2728345578711475		[learning rate: 0.019992]
	Learning Rate: 0.0199923
	LOSS [training: 1.2728345578711475 | validation: 1.2218893104003923]
	TIME [epoch: 7.66 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2363977055833315		[learning rate: 0.019992]
	Learning Rate: 0.0199916
	LOSS [training: 1.2363977055833315 | validation: 1.2274048872337655]
	TIME [epoch: 7.66 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2203120616399719		[learning rate: 0.019991]
	Learning Rate: 0.0199909
	LOSS [training: 1.2203120616399719 | validation: 1.2094270783225496]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2135163103966424		[learning rate: 0.01999]
	Learning Rate: 0.0199902
	LOSS [training: 1.2135163103966424 | validation: 1.2035571755736423]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1992194321873066		[learning rate: 0.019989]
	Learning Rate: 0.0199895
	LOSS [training: 1.1992194321873066 | validation: 1.21346295748469]
	TIME [epoch: 7.66 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2409043149566732		[learning rate: 0.019989]
	Learning Rate: 0.0199887
	LOSS [training: 1.2409043149566732 | validation: 1.218635628933442]
	TIME [epoch: 7.66 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.18920921730553		[learning rate: 0.019988]
	Learning Rate: 0.0199879
	LOSS [training: 1.18920921730553 | validation: 1.1709921715921756]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.222872289586769		[learning rate: 0.019987]
	Learning Rate: 0.0199871
	LOSS [training: 1.222872289586769 | validation: 1.223545502050631]
	TIME [epoch: 7.67 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1805692158534673		[learning rate: 0.019986]
	Learning Rate: 0.0199862
	LOSS [training: 1.1805692158534673 | validation: 1.1610258312226454]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1843595045366593		[learning rate: 0.019985]
	Learning Rate: 0.0199853
	LOSS [training: 1.1843595045366593 | validation: 1.1530516255378598]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1699996804917452		[learning rate: 0.019984]
	Learning Rate: 0.0199844
	LOSS [training: 1.1699996804917452 | validation: 1.1733064846730334]
	TIME [epoch: 7.66 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1605181905750237		[learning rate: 0.019984]
	Learning Rate: 0.0199835
	LOSS [training: 1.1605181905750237 | validation: 1.1474531638238576]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1692043848350642		[learning rate: 0.019983]
	Learning Rate: 0.0199825
	LOSS [training: 1.1692043848350642 | validation: 1.152565308429458]
	TIME [epoch: 7.68 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1648077240258594		[learning rate: 0.019982]
	Learning Rate: 0.0199816
	LOSS [training: 1.1648077240258594 | validation: 1.124530891749247]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1248991048349049		[learning rate: 0.019981]
	Learning Rate: 0.0199805
	LOSS [training: 1.1248991048349049 | validation: 1.170749282125592]
	TIME [epoch: 7.67 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1903860335942247		[learning rate: 0.019979]
	Learning Rate: 0.0199795
	LOSS [training: 1.1903860335942247 | validation: 1.1083754351013388]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1526901744912175		[learning rate: 0.019978]
	Learning Rate: 0.0199784
	LOSS [training: 1.1526901744912175 | validation: 1.1973704502113387]
	TIME [epoch: 7.67 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1352760777569317		[learning rate: 0.019977]
	Learning Rate: 0.0199773
	LOSS [training: 1.1352760777569317 | validation: 1.1475863898357352]
	TIME [epoch: 7.69 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1503170360035988		[learning rate: 0.019976]
	Learning Rate: 0.0199762
	LOSS [training: 1.1503170360035988 | validation: 1.2181813603418652]
	TIME [epoch: 7.7 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1258583240062094		[learning rate: 0.019975]
	Learning Rate: 0.019975
	LOSS [training: 1.1258583240062094 | validation: 1.1214396841211958]
	TIME [epoch: 7.67 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1571863353293157		[learning rate: 0.019974]
	Learning Rate: 0.0199739
	LOSS [training: 1.1571863353293157 | validation: 1.180819014027056]
	TIME [epoch: 7.67 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1188748381261644		[learning rate: 0.019973]
	Learning Rate: 0.0199727
	LOSS [training: 1.1188748381261644 | validation: 1.0709272652933437]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1130652048784504		[learning rate: 0.019971]
	Learning Rate: 0.0199714
	LOSS [training: 1.1130652048784504 | validation: 1.1713349591320998]
	TIME [epoch: 7.71 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1181876097006234		[learning rate: 0.01997]
	Learning Rate: 0.0199702
	LOSS [training: 1.1181876097006234 | validation: 1.0367010902869531]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0959106013438842		[learning rate: 0.019969]
	Learning Rate: 0.0199689
	LOSS [training: 1.0959106013438842 | validation: 1.136878844215759]
	TIME [epoch: 7.66 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1075323037201301		[learning rate: 0.019968]
	Learning Rate: 0.0199675
	LOSS [training: 1.1075323037201301 | validation: 1.109286226706483]
	TIME [epoch: 7.67 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0724784537162178		[learning rate: 0.019966]
	Learning Rate: 0.0199662
	LOSS [training: 1.0724784537162178 | validation: 1.0607973051541326]
	TIME [epoch: 7.67 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1135689180476556		[learning rate: 0.019965]
	Learning Rate: 0.0199648
	LOSS [training: 1.1135689180476556 | validation: 1.0456027306387399]
	TIME [epoch: 7.72 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0815772398860999		[learning rate: 0.019963]
	Learning Rate: 0.0199634
	LOSS [training: 1.0815772398860999 | validation: 1.1542264057031044]
	TIME [epoch: 7.67 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1025882621935712		[learning rate: 0.019962]
	Learning Rate: 0.019962
	LOSS [training: 1.1025882621935712 | validation: 1.0279639235195757]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.037233478876563		[learning rate: 0.019961]
	Learning Rate: 0.0199606
	LOSS [training: 1.037233478876563 | validation: 1.1266385270192552]
	TIME [epoch: 7.67 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1046272877651626		[learning rate: 0.019959]
	Learning Rate: 0.0199591
	LOSS [training: 1.1046272877651626 | validation: 1.0094786371119118]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0336484684393308		[learning rate: 0.019958]
	Learning Rate: 0.0199576
	LOSS [training: 1.0336484684393308 | validation: 1.1092635360415515]
	TIME [epoch: 7.72 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1222055956119943		[learning rate: 0.019956]
	Learning Rate: 0.019956
	LOSS [training: 1.1222055956119943 | validation: 1.0533629964393234]
	TIME [epoch: 7.67 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0503900186916788		[learning rate: 0.019954]
	Learning Rate: 0.0199545
	LOSS [training: 1.0503900186916788 | validation: 1.0426072974346823]
	TIME [epoch: 7.66 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0930655321736258		[learning rate: 0.019953]
	Learning Rate: 0.0199529
	LOSS [training: 1.0930655321736258 | validation: 1.0839508038630479]
	TIME [epoch: 7.67 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0399854331622607		[learning rate: 0.019951]
	Learning Rate: 0.0199513
	LOSS [training: 1.0399854331622607 | validation: 0.9718287509082664]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0617238412623098		[learning rate: 0.01995]
	Learning Rate: 0.0199496
	LOSS [training: 1.0617238412623098 | validation: 1.0352516421950455]
	TIME [epoch: 7.72 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0427632340894197		[learning rate: 0.019948]
	Learning Rate: 0.0199479
	LOSS [training: 1.0427632340894197 | validation: 1.0944880471669083]
	TIME [epoch: 7.67 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0714120734509895		[learning rate: 0.019946]
	Learning Rate: 0.0199462
	LOSS [training: 1.0714120734509895 | validation: 1.0172798994633634]
	TIME [epoch: 7.66 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0048769719565624		[learning rate: 0.019945]
	Learning Rate: 0.0199445
	LOSS [training: 1.0048769719565624 | validation: 1.1148900337580585]
	TIME [epoch: 7.66 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.046505784405675		[learning rate: 0.019943]
	Learning Rate: 0.0199428
	LOSS [training: 1.046505784405675 | validation: 0.9395804818686158]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0501726907348463		[learning rate: 0.019941]
	Learning Rate: 0.019941
	LOSS [training: 1.0501726907348463 | validation: 0.987765438694582]
	TIME [epoch: 7.72 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9924699717472566		[learning rate: 0.019939]
	Learning Rate: 0.0199392
	LOSS [training: 0.9924699717472566 | validation: 1.1660210204423254]
	TIME [epoch: 7.68 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0584551730344776		[learning rate: 0.019937]
	Learning Rate: 0.0199374
	LOSS [training: 1.0584551730344776 | validation: 0.9418109391952244]
	TIME [epoch: 7.66 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9761916553246824		[learning rate: 0.019935]
	Learning Rate: 0.0199355
	LOSS [training: 0.9761916553246824 | validation: 0.9409956416203447]
	TIME [epoch: 7.67 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0668561980218343		[learning rate: 0.019934]
	Learning Rate: 0.0199336
	LOSS [training: 1.0668561980218343 | validation: 1.023373502594009]
	TIME [epoch: 7.67 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9866571563127726		[learning rate: 0.019932]
	Learning Rate: 0.0199317
	LOSS [training: 0.9866571563127726 | validation: 0.9573868831474608]
	TIME [epoch: 7.71 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0663451813503328		[learning rate: 0.01993]
	Learning Rate: 0.0199297
	LOSS [training: 1.0663451813503328 | validation: 0.9720077722699596]
	TIME [epoch: 7.67 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9775883505545939		[learning rate: 0.019928]
	Learning Rate: 0.0199278
	LOSS [training: 0.9775883505545939 | validation: 1.136251963367159]
	TIME [epoch: 7.67 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0251466529434143		[learning rate: 0.019926]
	Learning Rate: 0.0199258
	LOSS [training: 1.0251466529434143 | validation: 0.8956416478891052]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9741235920759999		[learning rate: 0.019924]
	Learning Rate: 0.0199238
	LOSS [training: 0.9741235920759999 | validation: 0.9603678445886166]
	TIME [epoch: 7.67 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0074263812159758		[learning rate: 0.019922]
	Learning Rate: 0.0199217
	LOSS [training: 1.0074263812159758 | validation: 0.8799765961930843]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9681048536388396		[learning rate: 0.01992]
	Learning Rate: 0.0199196
	LOSS [training: 0.9681048536388396 | validation: 1.0521726747774498]
	TIME [epoch: 7.66 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0051809847621955		[learning rate: 0.019918]
	Learning Rate: 0.0199175
	LOSS [training: 1.0051809847621955 | validation: 0.8883231285084032]
	TIME [epoch: 7.66 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0209867334577982		[learning rate: 0.019915]
	Learning Rate: 0.0199154
	LOSS [training: 1.0209867334577982 | validation: 1.044410896403436]
	TIME [epoch: 7.66 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9914955060970148		[learning rate: 0.019913]
	Learning Rate: 0.0199132
	LOSS [training: 0.9914955060970148 | validation: 0.9940470901278398]
	TIME [epoch: 7.67 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9864357671233351		[learning rate: 0.019911]
	Learning Rate: 0.019911
	LOSS [training: 0.9864357671233351 | validation: 0.9071034657108722]
	TIME [epoch: 7.73 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9448154956634334		[learning rate: 0.019909]
	Learning Rate: 0.0199088
	LOSS [training: 0.9448154956634334 | validation: 0.8850021448388712]
	TIME [epoch: 7.67 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9343346550727428		[learning rate: 0.019907]
	Learning Rate: 0.0199066
	LOSS [training: 0.9343346550727428 | validation: 0.9611568196506206]
	TIME [epoch: 7.67 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9913443586327421		[learning rate: 0.019904]
	Learning Rate: 0.0199043
	LOSS [training: 0.9913443586327421 | validation: 0.9560247464552489]
	TIME [epoch: 7.66 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9129928592215171		[learning rate: 0.019902]
	Learning Rate: 0.019902
	LOSS [training: 0.9129928592215171 | validation: 0.9387652342430963]
	TIME [epoch: 7.67 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9944993100697347		[learning rate: 0.0199]
	Learning Rate: 0.0198997
	LOSS [training: 0.9944993100697347 | validation: 1.0063143179506644]
	TIME [epoch: 7.71 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9633612995590246		[learning rate: 0.019897]
	Learning Rate: 0.0198974
	LOSS [training: 0.9633612995590246 | validation: 0.9236331538076178]
	TIME [epoch: 7.68 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9484273332818272		[learning rate: 0.019895]
	Learning Rate: 0.019895
	LOSS [training: 0.9484273332818272 | validation: 0.8592842804085563]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9429287860823781		[learning rate: 0.019893]
	Learning Rate: 0.0198926
	LOSS [training: 0.9429287860823781 | validation: 0.8936270498916047]
	TIME [epoch: 7.65 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8974123669749638		[learning rate: 0.01989]
	Learning Rate: 0.0198901
	LOSS [training: 0.8974123669749638 | validation: 0.8362349482051272]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0276523804239788		[learning rate: 0.019888]
	Learning Rate: 0.0198877
	LOSS [training: 1.0276523804239788 | validation: 0.8774414148020729]
	TIME [epoch: 7.71 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9343144188900014		[learning rate: 0.019885]
	Learning Rate: 0.0198852
	LOSS [training: 0.9343144188900014 | validation: 0.9081156904883254]
	TIME [epoch: 7.66 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9078809773505729		[learning rate: 0.019883]
	Learning Rate: 0.0198827
	LOSS [training: 0.9078809773505729 | validation: 0.8396136590330938]
	TIME [epoch: 7.67 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9853424231077308		[learning rate: 0.01988]
	Learning Rate: 0.0198802
	LOSS [training: 0.9853424231077308 | validation: 0.9305986499137797]
	TIME [epoch: 7.68 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9677896624444071		[learning rate: 0.019878]
	Learning Rate: 0.0198776
	LOSS [training: 0.9677896624444071 | validation: 0.9058419115262322]
	TIME [epoch: 7.66 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8627658323673116		[learning rate: 0.019875]
	Learning Rate: 0.019875
	LOSS [training: 0.8627658323673116 | validation: 0.9444750265927647]
	TIME [epoch: 7.69 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9641768092520596		[learning rate: 0.019872]
	Learning Rate: 0.0198724
	LOSS [training: 0.9641768092520596 | validation: 0.9610132117675518]
	TIME [epoch: 7.67 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8739588505489897		[learning rate: 0.01987]
	Learning Rate: 0.0198697
	LOSS [training: 0.8739588505489897 | validation: 0.914380021862264]
	TIME [epoch: 7.67 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9820823009739466		[learning rate: 0.019867]
	Learning Rate: 0.0198671
	LOSS [training: 0.9820823009739466 | validation: 0.8926417017848771]
	TIME [epoch: 7.65 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8793331566383598		[learning rate: 0.019864]
	Learning Rate: 0.0198644
	LOSS [training: 0.8793331566383598 | validation: 0.9075805727343969]
	TIME [epoch: 7.64 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8906286309479543		[learning rate: 0.019862]
	Learning Rate: 0.0198616
	LOSS [training: 0.8906286309479543 | validation: 0.9344793586346776]
	TIME [epoch: 7.68 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9230168115948612		[learning rate: 0.019859]
	Learning Rate: 0.0198589
	LOSS [training: 0.9230168115948612 | validation: 0.8693970854626258]
	TIME [epoch: 7.65 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9039825249364342		[learning rate: 0.019856]
	Learning Rate: 0.0198561
	LOSS [training: 0.9039825249364342 | validation: 0.9025443159726769]
	TIME [epoch: 7.66 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9223950147659131		[learning rate: 0.019853]
	Learning Rate: 0.0198533
	LOSS [training: 0.9223950147659131 | validation: 0.834977036155446]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.870112887199634		[learning rate: 0.01985]
	Learning Rate: 0.0198505
	LOSS [training: 0.870112887199634 | validation: 0.944341138685229]
	TIME [epoch: 7.69 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9753734950670081		[learning rate: 0.019848]
	Learning Rate: 0.0198476
	LOSS [training: 0.9753734950670081 | validation: 0.8915092445111782]
	TIME [epoch: 7.7 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8561121106556837		[learning rate: 0.019845]
	Learning Rate: 0.0198447
	LOSS [training: 0.8561121106556837 | validation: 0.8504692748718479]
	TIME [epoch: 7.67 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8667527778477102		[learning rate: 0.019842]
	Learning Rate: 0.0198418
	LOSS [training: 0.8667527778477102 | validation: 0.9506035381804168]
	TIME [epoch: 7.67 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8832028966166547		[learning rate: 0.019839]
	Learning Rate: 0.0198388
	LOSS [training: 0.8832028966166547 | validation: 0.9389111697838719]
	TIME [epoch: 7.67 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8861261389922757		[learning rate: 0.019836]
	Learning Rate: 0.0198359
	LOSS [training: 0.8861261389922757 | validation: 0.9553648579569871]
	TIME [epoch: 7.67 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9149209145164264		[learning rate: 0.019833]
	Learning Rate: 0.0198329
	LOSS [training: 0.9149209145164264 | validation: 0.7788175096954546]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8757721324026304		[learning rate: 0.01983]
	Learning Rate: 0.0198299
	LOSS [training: 0.8757721324026304 | validation: 0.8463054366535483]
	TIME [epoch: 7.66 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8621996372604669		[learning rate: 0.019827]
	Learning Rate: 0.0198268
	LOSS [training: 0.8621996372604669 | validation: 0.8798244001037864]
	TIME [epoch: 7.66 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8803125689700252		[learning rate: 0.019824]
	Learning Rate: 0.0198237
	LOSS [training: 0.8803125689700252 | validation: 0.9758556835549335]
	TIME [epoch: 7.66 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.91444121076207		[learning rate: 0.019821]
	Learning Rate: 0.0198206
	LOSS [training: 0.91444121076207 | validation: 0.7644811767802318]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8509907427058182		[learning rate: 0.019818]
	Learning Rate: 0.0198175
	LOSS [training: 0.8509907427058182 | validation: 0.9614625446637612]
	TIME [epoch: 7.7 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8758441422429228		[learning rate: 0.019814]
	Learning Rate: 0.0198143
	LOSS [training: 0.8758441422429228 | validation: 0.802438900460688]
	TIME [epoch: 7.67 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9000473222506336		[learning rate: 0.019811]
	Learning Rate: 0.0198112
	LOSS [training: 0.9000473222506336 | validation: 0.8090535412020303]
	TIME [epoch: 7.64 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8556405133575937		[learning rate: 0.019808]
	Learning Rate: 0.0198079
	LOSS [training: 0.8556405133575937 | validation: 1.1164683074161512]
	TIME [epoch: 7.66 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9981886677742102		[learning rate: 0.019805]
	Learning Rate: 0.0198047
	LOSS [training: 0.9981886677742102 | validation: 0.8565473293032669]
	TIME [epoch: 7.69 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8444202393606409		[learning rate: 0.019801]
	Learning Rate: 0.0198014
	LOSS [training: 0.8444202393606409 | validation: 0.880595260899081]
	TIME [epoch: 7.7 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7934441394460459		[learning rate: 0.019798]
	Learning Rate: 0.0197982
	LOSS [training: 0.7934441394460459 | validation: 1.0869775652997107]
	TIME [epoch: 7.66 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.882586018687856		[learning rate: 0.019795]
	Learning Rate: 0.0197948
	LOSS [training: 0.882586018687856 | validation: 0.7249609040668878]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8795399866021273		[learning rate: 0.019791]
	Learning Rate: 0.0197915
	LOSS [training: 0.8795399866021273 | validation: 0.8170382101396715]
	TIME [epoch: 7.66 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8373763401289147		[learning rate: 0.019788]
	Learning Rate: 0.0197881
	LOSS [training: 0.8373763401289147 | validation: 1.0485822915747398]
	TIME [epoch: 7.69 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8657603834740868		[learning rate: 0.019785]
	Learning Rate: 0.0197847
	LOSS [training: 0.8657603834740868 | validation: 0.7196292027827154]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7859932734998244		[learning rate: 0.019781]
	Learning Rate: 0.0197813
	LOSS [training: 0.7859932734998244 | validation: 0.7799400920003008]
	TIME [epoch: 7.66 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8221751435094714		[learning rate: 0.019778]
	Learning Rate: 0.0197778
	LOSS [training: 0.8221751435094714 | validation: 0.9434515288234188]
	TIME [epoch: 7.66 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8750560038072666		[learning rate: 0.019774]
	Learning Rate: 0.0197744
	LOSS [training: 0.8750560038072666 | validation: 0.8010187393453454]
	TIME [epoch: 7.67 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8271819906110771		[learning rate: 0.019771]
	Learning Rate: 0.0197709
	LOSS [training: 0.8271819906110771 | validation: 0.8050726119188255]
	TIME [epoch: 7.68 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7664141903312551		[learning rate: 0.019767]
	Learning Rate: 0.0197673
	LOSS [training: 0.7664141903312551 | validation: 0.9722092410524894]
	TIME [epoch: 7.7 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8829257865135131		[learning rate: 0.019764]
	Learning Rate: 0.0197638
	LOSS [training: 0.8829257865135131 | validation: 0.7764979127424209]
	TIME [epoch: 7.66 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7665326454164854		[learning rate: 0.01976]
	Learning Rate: 0.0197602
	LOSS [training: 0.7665326454164854 | validation: 0.9611517332497141]
	TIME [epoch: 7.66 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8792212808278335		[learning rate: 0.019757]
	Learning Rate: 0.0197566
	LOSS [training: 0.8792212808278335 | validation: 0.8506711856007256]
	TIME [epoch: 7.66 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8758835237770463		[learning rate: 0.019753]
	Learning Rate: 0.0197529
	LOSS [training: 0.8758835237770463 | validation: 0.7253137784505513]
	TIME [epoch: 7.69 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.771969602117405		[learning rate: 0.019749]
	Learning Rate: 0.0197493
	LOSS [training: 0.771969602117405 | validation: 0.884704508589416]
	TIME [epoch: 7.7 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8308711412604541		[learning rate: 0.019746]
	Learning Rate: 0.0197456
	LOSS [training: 0.8308711412604541 | validation: 0.9208299099868491]
	TIME [epoch: 7.66 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7964697945380387		[learning rate: 0.019742]
	Learning Rate: 0.0197419
	LOSS [training: 0.7964697945380387 | validation: 0.8534575746867652]
	TIME [epoch: 7.67 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8306540564246773		[learning rate: 0.019738]
	Learning Rate: 0.0197381
	LOSS [training: 0.8306540564246773 | validation: 0.7544610826031384]
	TIME [epoch: 7.67 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7804739080818961		[learning rate: 0.019734]
	Learning Rate: 0.0197343
	LOSS [training: 0.7804739080818961 | validation: 0.8561866424671142]
	TIME [epoch: 7.69 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8663915873051653		[learning rate: 0.019731]
	Learning Rate: 0.0197305
	LOSS [training: 0.8663915873051653 | validation: 0.7631536823917615]
	TIME [epoch: 7.7 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7887029206953801		[learning rate: 0.019727]
	Learning Rate: 0.0197267
	LOSS [training: 0.7887029206953801 | validation: 0.8614680368964612]
	TIME [epoch: 7.67 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8376744539739378		[learning rate: 0.019723]
	Learning Rate: 0.0197229
	LOSS [training: 0.8376744539739378 | validation: 0.8694884204879099]
	TIME [epoch: 7.66 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.831353654502338		[learning rate: 0.019719]
	Learning Rate: 0.019719
	LOSS [training: 0.831353654502338 | validation: 0.7888439761494821]
	TIME [epoch: 7.66 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7909921979740099		[learning rate: 0.019715]
	Learning Rate: 0.0197151
	LOSS [training: 0.7909921979740099 | validation: 0.7208568177283736]
	TIME [epoch: 7.68 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8615774452791949		[learning rate: 0.019711]
	Learning Rate: 0.0197112
	LOSS [training: 0.8615774452791949 | validation: 0.8679081755774282]
	TIME [epoch: 7.71 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8712904670682582		[learning rate: 0.019707]
	Learning Rate: 0.0197072
	LOSS [training: 0.8712904670682582 | validation: 0.7704152313908612]
	TIME [epoch: 7.67 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8177118290597788		[learning rate: 0.019703]
	Learning Rate: 0.0197032
	LOSS [training: 0.8177118290597788 | validation: 0.8152315532843112]
	TIME [epoch: 7.68 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7182850925404648		[learning rate: 0.019699]
	Learning Rate: 0.0196992
	LOSS [training: 0.7182850925404648 | validation: 0.8883547301878295]
	TIME [epoch: 7.67 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7785653184614516		[learning rate: 0.019695]
	Learning Rate: 0.0196952
	LOSS [training: 0.7785653184614516 | validation: 0.8683633180047865]
	TIME [epoch: 7.69 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.784072535766696		[learning rate: 0.019691]
	Learning Rate: 0.0196911
	LOSS [training: 0.784072535766696 | validation: 0.848086581226339]
	TIME [epoch: 121 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7641187255969064		[learning rate: 0.019687]
	Learning Rate: 0.019687
	LOSS [training: 0.7641187255969064 | validation: 0.6642443225667756]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7694417181811553		[learning rate: 0.019683]
	Learning Rate: 0.0196829
	LOSS [training: 0.7694417181811553 | validation: 0.8960354869730138]
	TIME [epoch: 15.1 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8071270295440764		[learning rate: 0.019679]
	Learning Rate: 0.0196788
	LOSS [training: 0.8071270295440764 | validation: 1.0005562340202236]
	TIME [epoch: 15.1 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.802608792385925		[learning rate: 0.019675]
	Learning Rate: 0.0196746
	LOSS [training: 0.802608792385925 | validation: 0.6537074359727824]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6826904805188239		[learning rate: 0.01967]
	Learning Rate: 0.0196704
	LOSS [training: 0.6826904805188239 | validation: 0.8878528904147938]
	TIME [epoch: 15.1 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8258262052169283		[learning rate: 0.019666]
	Learning Rate: 0.0196662
	LOSS [training: 0.8258262052169283 | validation: 0.6929098991898643]
	TIME [epoch: 15.1 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7515763875414023		[learning rate: 0.019662]
	Learning Rate: 0.0196619
	LOSS [training: 0.7515763875414023 | validation: 0.6957420006918563]
	TIME [epoch: 15.1 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7550745343979106		[learning rate: 0.019658]
	Learning Rate: 0.0196576
	LOSS [training: 0.7550745343979106 | validation: 0.7391313823515403]
	TIME [epoch: 15.2 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6760149209975395		[learning rate: 0.019653]
	Learning Rate: 0.0196533
	LOSS [training: 0.6760149209975395 | validation: 0.9047989732735688]
	TIME [epoch: 15.1 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8042455220524962		[learning rate: 0.019649]
	Learning Rate: 0.019649
	LOSS [training: 0.8042455220524962 | validation: 0.7080336727023857]
	TIME [epoch: 15.1 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7783942377716342		[learning rate: 0.019645]
	Learning Rate: 0.0196447
	LOSS [training: 0.7783942377716342 | validation: 0.7495341106271562]
	TIME [epoch: 15.1 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6953199459043096		[learning rate: 0.01964]
	Learning Rate: 0.0196403
	LOSS [training: 0.6953199459043096 | validation: 0.8863392083337008]
	TIME [epoch: 15 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7372762290623077		[learning rate: 0.019636]
	Learning Rate: 0.0196359
	LOSS [training: 0.7372762290623077 | validation: 0.7933272824072063]
	TIME [epoch: 15.2 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7290022231450961		[learning rate: 0.019631]
	Learning Rate: 0.0196314
	LOSS [training: 0.7290022231450961 | validation: 0.7488720417357156]
	TIME [epoch: 15.1 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7386375100809893		[learning rate: 0.019627]
	Learning Rate: 0.019627
	LOSS [training: 0.7386375100809893 | validation: 0.6656895009927506]
	TIME [epoch: 15.2 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7398541384565471		[learning rate: 0.019622]
	Learning Rate: 0.0196225
	LOSS [training: 0.7398541384565471 | validation: 0.7055606713283813]
	TIME [epoch: 15.1 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7489101457188074		[learning rate: 0.019618]
	Learning Rate: 0.019618
	LOSS [training: 0.7489101457188074 | validation: 0.7136975364952083]
	TIME [epoch: 15.1 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6984477622131794		[learning rate: 0.019613]
	Learning Rate: 0.0196134
	LOSS [training: 0.6984477622131794 | validation: 0.7714579896245392]
	TIME [epoch: 15.1 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7519476997651178		[learning rate: 0.019609]
	Learning Rate: 0.0196089
	LOSS [training: 0.7519476997651178 | validation: 0.7845568307926359]
	TIME [epoch: 15 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6971898630664177		[learning rate: 0.019604]
	Learning Rate: 0.0196043
	LOSS [training: 0.6971898630664177 | validation: 0.8943358052857899]
	TIME [epoch: 15.1 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7726436298372172		[learning rate: 0.0196]
	Learning Rate: 0.0195997
	LOSS [training: 0.7726436298372172 | validation: 0.7758126385499537]
	TIME [epoch: 15.2 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.679173031929962		[learning rate: 0.019595]
	Learning Rate: 0.019595
	LOSS [training: 0.679173031929962 | validation: 0.6898968963267222]
	TIME [epoch: 15.1 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7608908643731005		[learning rate: 0.01959]
	Learning Rate: 0.0195904
	LOSS [training: 0.7608908643731005 | validation: 0.7769113779495238]
	TIME [epoch: 15.1 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6955735986590419		[learning rate: 0.019586]
	Learning Rate: 0.0195857
	LOSS [training: 0.6955735986590419 | validation: 0.9747451331897459]
	TIME [epoch: 15.1 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7470298918916294		[learning rate: 0.019581]
	Learning Rate: 0.0195809
	LOSS [training: 0.7470298918916294 | validation: 0.7114132233608155]
	TIME [epoch: 15.1 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.734147360422779		[learning rate: 0.019576]
	Learning Rate: 0.0195762
	LOSS [training: 0.734147360422779 | validation: 0.7063496190784515]
	TIME [epoch: 15 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6862706949525449		[learning rate: 0.019571]
	Learning Rate: 0.0195714
	LOSS [training: 0.6862706949525449 | validation: 0.807467635730881]
	TIME [epoch: 15.1 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6840226594945781		[learning rate: 0.019567]
	Learning Rate: 0.0195666
	LOSS [training: 0.6840226594945781 | validation: 0.9253033908996204]
	TIME [epoch: 15.2 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7741491453417533		[learning rate: 0.019562]
	Learning Rate: 0.0195618
	LOSS [training: 0.7741491453417533 | validation: 0.7384121900382825]
	TIME [epoch: 15.1 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6824992098797107		[learning rate: 0.019557]
	Learning Rate: 0.0195569
	LOSS [training: 0.6824992098797107 | validation: 0.7095920333854836]
	TIME [epoch: 15.1 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6696252921260617		[learning rate: 0.019552]
	Learning Rate: 0.0195521
	LOSS [training: 0.6696252921260617 | validation: 1.1051376872785732]
	TIME [epoch: 15.2 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.826858667654609		[learning rate: 0.019547]
	Learning Rate: 0.0195472
	LOSS [training: 0.826858667654609 | validation: 0.620805740152959]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6970156059396941		[learning rate: 0.019542]
	Learning Rate: 0.0195422
	LOSS [training: 0.6970156059396941 | validation: 0.6542882857257967]
	TIME [epoch: 15.1 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6847826605292427		[learning rate: 0.019537]
	Learning Rate: 0.0195373
	LOSS [training: 0.6847826605292427 | validation: 0.7195208817571002]
	TIME [epoch: 15.1 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6643583047921806		[learning rate: 0.019532]
	Learning Rate: 0.0195323
	LOSS [training: 0.6643583047921806 | validation: 0.7012528672475847]
	TIME [epoch: 15 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6540500953076596		[learning rate: 0.019527]
	Learning Rate: 0.0195273
	LOSS [training: 0.6540500953076596 | validation: 0.6639027291986049]
	TIME [epoch: 15.1 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7292975065262535		[learning rate: 0.019522]
	Learning Rate: 0.0195222
	LOSS [training: 0.7292975065262535 | validation: 0.6033637899149578]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6452508929852708		[learning rate: 0.019517]
	Learning Rate: 0.0195172
	LOSS [training: 0.6452508929852708 | validation: 0.7992336782282097]
	TIME [epoch: 15.1 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.662382185315127		[learning rate: 0.019512]
	Learning Rate: 0.0195121
	LOSS [training: 0.662382185315127 | validation: 0.8648949396416259]
	TIME [epoch: 15.1 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7216202470990146		[learning rate: 0.019507]
	Learning Rate: 0.019507
	LOSS [training: 0.7216202470990146 | validation: 0.934947403176814]
	TIME [epoch: 15.1 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6375165026739312		[learning rate: 0.019502]
	Learning Rate: 0.0195018
	LOSS [training: 0.6375165026739312 | validation: 0.717455428686085]
	TIME [epoch: 15.1 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.680462933013698		[learning rate: 0.019497]
	Learning Rate: 0.0194967
	LOSS [training: 0.680462933013698 | validation: 0.7188931228661521]
	TIME [epoch: 15.1 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6304799279784276		[learning rate: 0.019491]
	Learning Rate: 0.0194915
	LOSS [training: 0.6304799279784276 | validation: 0.7838123029210118]
	TIME [epoch: 15.1 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6997423725622873		[learning rate: 0.019486]
	Learning Rate: 0.0194863
	LOSS [training: 0.6997423725622873 | validation: 0.6910785592895397]
	TIME [epoch: 15.1 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6610058652236495		[learning rate: 0.019481]
	Learning Rate: 0.019481
	LOSS [training: 0.6610058652236495 | validation: 0.8080333477177439]
	TIME [epoch: 15.1 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6625839656524298		[learning rate: 0.019476]
	Learning Rate: 0.0194757
	LOSS [training: 0.6625839656524298 | validation: 0.6016584570026658]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7256964669302164		[learning rate: 0.01947]
	Learning Rate: 0.0194705
	LOSS [training: 0.7256964669302164 | validation: 0.633104103755012]
	TIME [epoch: 15.1 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.652261346271266		[learning rate: 0.019465]
	Learning Rate: 0.0194651
	LOSS [training: 0.652261346271266 | validation: 0.8604425270371817]
	TIME [epoch: 15.1 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7566584028439305		[learning rate: 0.01946]
	Learning Rate: 0.0194598
	LOSS [training: 0.7566584028439305 | validation: 0.7242512794708174]
	TIME [epoch: 15.2 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6145014347662154		[learning rate: 0.019454]
	Learning Rate: 0.0194544
	LOSS [training: 0.6145014347662154 | validation: 1.027482392140569]
	TIME [epoch: 15.1 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7420317351207404		[learning rate: 0.019449]
	Learning Rate: 0.019449
	LOSS [training: 0.7420317351207404 | validation: 0.5999531752011267]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6918384096970182		[learning rate: 0.019444]
	Learning Rate: 0.0194436
	LOSS [training: 0.6918384096970182 | validation: 0.6978415237114053]
	TIME [epoch: 15.2 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5958663310232215		[learning rate: 0.019438]
	Learning Rate: 0.0194381
	LOSS [training: 0.5958663310232215 | validation: 0.6690588352884763]
	TIME [epoch: 15.1 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7504205670048626		[learning rate: 0.019433]
	Learning Rate: 0.0194327
	LOSS [training: 0.7504205670048626 | validation: 0.6717470508573828]
	TIME [epoch: 15.1 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6446306877120623		[learning rate: 0.019427]
	Learning Rate: 0.0194272
	LOSS [training: 0.6446306877120623 | validation: 0.5765310232221434]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.690312727956592		[learning rate: 0.019422]
	Learning Rate: 0.0194216
	LOSS [training: 0.690312727956592 | validation: 0.6053163699255226]
	TIME [epoch: 15.1 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6755901752815192		[learning rate: 0.019416]
	Learning Rate: 0.0194161
	LOSS [training: 0.6755901752815192 | validation: 0.5579083774602754]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6750532834779923		[learning rate: 0.019411]
	Learning Rate: 0.0194105
	LOSS [training: 0.6750532834779923 | validation: 0.7883895686232785]
	TIME [epoch: 15.2 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5960041652611333		[learning rate: 0.019405]
	Learning Rate: 0.0194049
	LOSS [training: 0.5960041652611333 | validation: 0.6256914942479289]
	TIME [epoch: 15.1 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6195396379734056		[learning rate: 0.019399]
	Learning Rate: 0.0193993
	LOSS [training: 0.6195396379734056 | validation: 0.6686942964624208]
	TIME [epoch: 15.1 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6274234665718494		[learning rate: 0.019394]
	Learning Rate: 0.0193936
	LOSS [training: 0.6274234665718494 | validation: 0.5905708631751385]
	TIME [epoch: 15.1 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6691330914455486		[learning rate: 0.019388]
	Learning Rate: 0.0193879
	LOSS [training: 0.6691330914455486 | validation: 0.8293753267814133]
	TIME [epoch: 15 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6426016794560933		[learning rate: 0.019382]
	Learning Rate: 0.0193822
	LOSS [training: 0.6426016794560933 | validation: 0.8486699315604853]
	TIME [epoch: 15.2 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6283902484627019		[learning rate: 0.019376]
	Learning Rate: 0.0193765
	LOSS [training: 0.6283902484627019 | validation: 0.6145860009081494]
	TIME [epoch: 15.1 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6413637667107464		[learning rate: 0.019371]
	Learning Rate: 0.0193707
	LOSS [training: 0.6413637667107464 | validation: 0.7106755031410166]
	TIME [epoch: 15.1 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6945798813187153		[learning rate: 0.019365]
	Learning Rate: 0.0193649
	LOSS [training: 0.6945798813187153 | validation: 0.6353408774200169]
	TIME [epoch: 15.2 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5903085492375916		[learning rate: 0.019359]
	Learning Rate: 0.0193591
	LOSS [training: 0.5903085492375916 | validation: 0.5557604043430822]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6643166871773409		[learning rate: 0.019353]
	Learning Rate: 0.0193533
	LOSS [training: 0.6643166871773409 | validation: 0.6674437580262833]
	TIME [epoch: 15.1 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6587862794549849		[learning rate: 0.019347]
	Learning Rate: 0.0193474
	LOSS [training: 0.6587862794549849 | validation: 0.6464150191601366]
	TIME [epoch: 15 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.628307721509834		[learning rate: 0.019342]
	Learning Rate: 0.0193416
	LOSS [training: 0.628307721509834 | validation: 0.7678234097840895]
	TIME [epoch: 15.1 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6696397698544967		[learning rate: 0.019336]
	Learning Rate: 0.0193356
	LOSS [training: 0.6696397698544967 | validation: 0.6358493456252995]
	TIME [epoch: 15.2 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6012374276102131		[learning rate: 0.01933]
	Learning Rate: 0.0193297
	LOSS [training: 0.6012374276102131 | validation: 0.6959223417265754]
	TIME [epoch: 15.1 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.609884313834815		[learning rate: 0.019324]
	Learning Rate: 0.0193237
	LOSS [training: 0.609884313834815 | validation: 0.7706226702982923]
	TIME [epoch: 15.1 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6292105483416609		[learning rate: 0.019318]
	Learning Rate: 0.0193178
	LOSS [training: 0.6292105483416609 | validation: 0.6297692505610748]
	TIME [epoch: 15.1 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7117904538365247		[learning rate: 0.019312]
	Learning Rate: 0.0193117
	LOSS [training: 0.7117904538365247 | validation: 0.5864488959607443]
	TIME [epoch: 15 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5687561448482179		[learning rate: 0.019306]
	Learning Rate: 0.0193057
	LOSS [training: 0.5687561448482179 | validation: 0.7889013654300929]
	TIME [epoch: 15.1 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6827901755480542		[learning rate: 0.0193]
	Learning Rate: 0.0192996
	LOSS [training: 0.6827901755480542 | validation: 0.5789460767276853]
	TIME [epoch: 15.2 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5379639436088232		[learning rate: 0.019294]
	Learning Rate: 0.0192935
	LOSS [training: 0.5379639436088232 | validation: 1.1065376029873726]
	TIME [epoch: 15 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7199181436799158		[learning rate: 0.019287]
	Learning Rate: 0.0192874
	LOSS [training: 0.7199181436799158 | validation: 0.559146968288555]
	TIME [epoch: 15 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5805523687332004		[learning rate: 0.019281]
	Learning Rate: 0.0192813
	LOSS [training: 0.5805523687332004 | validation: 0.905958114981029]
	TIME [epoch: 15.1 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.628255152266718		[learning rate: 0.019275]
	Learning Rate: 0.0192751
	LOSS [training: 0.628255152266718 | validation: 0.6247178560144606]
	TIME [epoch: 15.1 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6323786160019258		[learning rate: 0.019269]
	Learning Rate: 0.0192689
	LOSS [training: 0.6323786160019258 | validation: 0.5655263428774098]
	TIME [epoch: 15.1 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6281783072349882		[learning rate: 0.019263]
	Learning Rate: 0.0192627
	LOSS [training: 0.6281783072349882 | validation: 0.662495349257568]
	TIME [epoch: 15.1 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6160409819214167		[learning rate: 0.019256]
	Learning Rate: 0.0192565
	LOSS [training: 0.6160409819214167 | validation: 0.580958851464271]
	TIME [epoch: 15.1 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5780117509441112		[learning rate: 0.01925]
	Learning Rate: 0.0192502
	LOSS [training: 0.5780117509441112 | validation: 0.5561701066401303]
	TIME [epoch: 15 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.622596249540651		[learning rate: 0.019244]
	Learning Rate: 0.0192439
	LOSS [training: 0.622596249540651 | validation: 0.7197255018048438]
	TIME [epoch: 15.1 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.649156564242036		[learning rate: 0.019238]
	Learning Rate: 0.0192376
	LOSS [training: 0.649156564242036 | validation: 0.6494749612598345]
	TIME [epoch: 15.1 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.562372936783367		[learning rate: 0.019231]
	Learning Rate: 0.0192313
	LOSS [training: 0.562372936783367 | validation: 0.6239865924166267]
	TIME [epoch: 15.2 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6744516014542851		[learning rate: 0.019225]
	Learning Rate: 0.0192249
	LOSS [training: 0.6744516014542851 | validation: 0.7716043794333822]
	TIME [epoch: 15.1 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6304160326077164		[learning rate: 0.019218]
	Learning Rate: 0.0192185
	LOSS [training: 0.6304160326077164 | validation: 0.7453363517120375]
	TIME [epoch: 15.1 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5925112078733744		[learning rate: 0.019212]
	Learning Rate: 0.0192121
	LOSS [training: 0.5925112078733744 | validation: 0.8451769563086025]
	TIME [epoch: 15.1 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6101418543069581		[learning rate: 0.019206]
	Learning Rate: 0.0192056
	LOSS [training: 0.6101418543069581 | validation: 0.6255694008511068]
	TIME [epoch: 15.1 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6247457255073925		[learning rate: 0.019199]
	Learning Rate: 0.0191992
	LOSS [training: 0.6247457255073925 | validation: 0.6400509721662896]
	TIME [epoch: 15.1 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.594579886877499		[learning rate: 0.019193]
	Learning Rate: 0.0191927
	LOSS [training: 0.594579886877499 | validation: 0.7040762870911624]
	TIME [epoch: 15.1 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6006378476496155		[learning rate: 0.019186]
	Learning Rate: 0.0191861
	LOSS [training: 0.6006378476496155 | validation: 0.586790672755271]
	TIME [epoch: 15.1 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6000128423766419		[learning rate: 0.01918]
	Learning Rate: 0.0191796
	LOSS [training: 0.6000128423766419 | validation: 0.7067692921021606]
	TIME [epoch: 15 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.629081464537921		[learning rate: 0.019173]
	Learning Rate: 0.019173
	LOSS [training: 0.629081464537921 | validation: 0.6007008523627203]
	TIME [epoch: 15.1 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5907739453256952		[learning rate: 0.019166]
	Learning Rate: 0.0191664
	LOSS [training: 0.5907739453256952 | validation: 0.7218515003837129]
	TIME [epoch: 15.1 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5824271401760739		[learning rate: 0.01916]
	Learning Rate: 0.0191598
	LOSS [training: 0.5824271401760739 | validation: 0.7296779502383672]
	TIME [epoch: 15.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5682039967313879		[learning rate: 0.019153]
	Learning Rate: 0.0191532
	LOSS [training: 0.5682039967313879 | validation: 0.8062256863041728]
	TIME [epoch: 15.1 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.60407459280537		[learning rate: 0.019146]
	Learning Rate: 0.0191465
	LOSS [training: 0.60407459280537 | validation: 0.8614304784655953]
	TIME [epoch: 15.1 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5753632823283473		[learning rate: 0.01914]
	Learning Rate: 0.0191398
	LOSS [training: 0.5753632823283473 | validation: 0.9333215468617212]
	TIME [epoch: 15.2 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6218386737982668		[learning rate: 0.019133]
	Learning Rate: 0.0191331
	LOSS [training: 0.6218386737982668 | validation: 0.7468883034074998]
	TIME [epoch: 15.1 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5895311871349617		[learning rate: 0.019126]
	Learning Rate: 0.0191263
	LOSS [training: 0.5895311871349617 | validation: 0.6587379299370004]
	TIME [epoch: 15 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5969105441767542		[learning rate: 0.01912]
	Learning Rate: 0.0191196
	LOSS [training: 0.5969105441767542 | validation: 0.608392324988495]
	TIME [epoch: 15.2 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5934245102440089		[learning rate: 0.019113]
	Learning Rate: 0.0191128
	LOSS [training: 0.5934245102440089 | validation: 0.7021802283798328]
	TIME [epoch: 15.1 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6168712398358761		[learning rate: 0.019106]
	Learning Rate: 0.019106
	LOSS [training: 0.6168712398358761 | validation: 0.6663672726398546]
	TIME [epoch: 15.1 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5642672881531363		[learning rate: 0.019099]
	Learning Rate: 0.0190991
	LOSS [training: 0.5642672881531363 | validation: 0.6525044953987096]
	TIME [epoch: 15.1 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5241730102732403		[learning rate: 0.019092]
	Learning Rate: 0.0190922
	LOSS [training: 0.5241730102732403 | validation: 0.6213594925360362]
	TIME [epoch: 15.1 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7661649192542453		[learning rate: 0.019085]
	Learning Rate: 0.0190853
	LOSS [training: 0.7661649192542453 | validation: 0.6592205895479499]
	TIME [epoch: 15.1 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5773884556449725		[learning rate: 0.019078]
	Learning Rate: 0.0190784
	LOSS [training: 0.5773884556449725 | validation: 0.6081997042957864]
	TIME [epoch: 15 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5492122560531101		[learning rate: 0.019071]
	Learning Rate: 0.0190715
	LOSS [training: 0.5492122560531101 | validation: 0.5883207765047944]
	TIME [epoch: 15.1 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5548496001362624		[learning rate: 0.019065]
	Learning Rate: 0.0190645
	LOSS [training: 0.5548496001362624 | validation: 0.794894468730719]
	TIME [epoch: 15.2 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6865573846231564		[learning rate: 0.019058]
	Learning Rate: 0.0190575
	LOSS [training: 0.6865573846231564 | validation: 0.6022005474879775]
	TIME [epoch: 15.2 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5051978862789303		[learning rate: 0.019051]
	Learning Rate: 0.0190505
	LOSS [training: 0.5051978862789303 | validation: 0.8754357876730062]
	TIME [epoch: 15.1 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5445487995379957		[learning rate: 0.019043]
	Learning Rate: 0.0190435
	LOSS [training: 0.5445487995379957 | validation: 0.6502014980713597]
	TIME [epoch: 15.1 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6647908182625166		[learning rate: 0.019036]
	Learning Rate: 0.0190364
	LOSS [training: 0.6647908182625166 | validation: 0.657276142694093]
	TIME [epoch: 15.1 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.557171494923508		[learning rate: 0.019029]
	Learning Rate: 0.0190293
	LOSS [training: 0.557171494923508 | validation: 0.6667318825533337]
	TIME [epoch: 15 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6123272646621608		[learning rate: 0.019022]
	Learning Rate: 0.0190222
	LOSS [training: 0.6123272646621608 | validation: 0.5357037978926322]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5352549313943078		[learning rate: 0.019015]
	Learning Rate: 0.019015
	LOSS [training: 0.5352549313943078 | validation: 0.5522507211026215]
	TIME [epoch: 15.1 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5846903214334177		[learning rate: 0.019008]
	Learning Rate: 0.0190079
	LOSS [training: 0.5846903214334177 | validation: 0.7089892878058317]
	TIME [epoch: 15.1 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5300929948341405		[learning rate: 0.019001]
	Learning Rate: 0.0190007
	LOSS [training: 0.5300929948341405 | validation: 0.5636889789013302]
	TIME [epoch: 15.1 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5677430468750773		[learning rate: 0.018993]
	Learning Rate: 0.0189935
	LOSS [training: 0.5677430468750773 | validation: 0.7528119542190133]
	TIME [epoch: 15.1 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5619661973667092		[learning rate: 0.018986]
	Learning Rate: 0.0189862
	LOSS [training: 0.5619661973667092 | validation: 0.6940997180084718]
	TIME [epoch: 15.2 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6371705640604878		[learning rate: 0.018979]
	Learning Rate: 0.018979
	LOSS [training: 0.6371705640604878 | validation: 0.5503046640221769]
	TIME [epoch: 15.1 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6071624329744016		[learning rate: 0.018972]
	Learning Rate: 0.0189717
	LOSS [training: 0.6071624329744016 | validation: 0.5168312434889444]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5356411439646878		[learning rate: 0.018964]
	Learning Rate: 0.0189644
	LOSS [training: 0.5356411439646878 | validation: 0.5894365387241607]
	TIME [epoch: 15.2 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5748654616528476		[learning rate: 0.018957]
	Learning Rate: 0.018957
	LOSS [training: 0.5748654616528476 | validation: 0.7798361675381176]
	TIME [epoch: 15.1 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6127952429930839		[learning rate: 0.01895]
	Learning Rate: 0.0189497
	LOSS [training: 0.6127952429930839 | validation: 0.6049190688990038]
	TIME [epoch: 15.1 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5384804713222192		[learning rate: 0.018942]
	Learning Rate: 0.0189423
	LOSS [training: 0.5384804713222192 | validation: 0.643772936663606]
	TIME [epoch: 15.1 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5682576702481538		[learning rate: 0.018935]
	Learning Rate: 0.0189349
	LOSS [training: 0.5682576702481538 | validation: 0.7578107167313397]
	TIME [epoch: 15.1 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5705205556545747		[learning rate: 0.018927]
	Learning Rate: 0.0189274
	LOSS [training: 0.5705205556545747 | validation: 0.7384675881233143]
	TIME [epoch: 15.1 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5731690108094573		[learning rate: 0.01892]
	Learning Rate: 0.01892
	LOSS [training: 0.5731690108094573 | validation: 0.6695919514672082]
	TIME [epoch: 15.1 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5651723249690701		[learning rate: 0.018912]
	Learning Rate: 0.0189125
	LOSS [training: 0.5651723249690701 | validation: 0.5797016740514773]
	TIME [epoch: 15.1 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6070707924494514		[learning rate: 0.018905]
	Learning Rate: 0.018905
	LOSS [training: 0.6070707924494514 | validation: 0.52725512145171]
	TIME [epoch: 15.1 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5551234883481106		[learning rate: 0.018897]
	Learning Rate: 0.0188974
	LOSS [training: 0.5551234883481106 | validation: 0.977371262222536]
	TIME [epoch: 15 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7185985531251167		[learning rate: 0.01889]
	Learning Rate: 0.0188899
	LOSS [training: 0.7185985531251167 | validation: 1.1426254261936717]
	TIME [epoch: 15 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7070209800308356		[learning rate: 0.018882]
	Learning Rate: 0.0188823
	LOSS [training: 0.7070209800308356 | validation: 0.5699470655086033]
	TIME [epoch: 15.2 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5256006840226676		[learning rate: 0.018875]
	Learning Rate: 0.0188747
	LOSS [training: 0.5256006840226676 | validation: 0.7660782605992891]
	TIME [epoch: 15.1 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5169138714950405		[learning rate: 0.018867]
	Learning Rate: 0.0188671
	LOSS [training: 0.5169138714950405 | validation: 0.8143172310328552]
	TIME [epoch: 15.1 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.591959830545848		[learning rate: 0.018859]
	Learning Rate: 0.0188594
	LOSS [training: 0.591959830545848 | validation: 0.6145265049023325]
	TIME [epoch: 15.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5894963301648756		[learning rate: 0.018852]
	Learning Rate: 0.0188517
	LOSS [training: 0.5894963301648756 | validation: 0.5853745106222671]
	TIME [epoch: 15.1 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.536740016512671		[learning rate: 0.018844]
	Learning Rate: 0.018844
	LOSS [training: 0.536740016512671 | validation: 0.6119228410923305]
	TIME [epoch: 15.1 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5538382801730597		[learning rate: 0.018836]
	Learning Rate: 0.0188363
	LOSS [training: 0.5538382801730597 | validation: 0.7076601526851436]
	TIME [epoch: 15.1 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6168091948307954		[learning rate: 0.018829]
	Learning Rate: 0.0188286
	LOSS [training: 0.6168091948307954 | validation: 0.5322600449645465]
	TIME [epoch: 15.1 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49509935029320923		[learning rate: 0.018821]
	Learning Rate: 0.0188208
	LOSS [training: 0.49509935029320923 | validation: 0.6839194522742099]
	TIME [epoch: 15.1 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5813111249921699		[learning rate: 0.018813]
	Learning Rate: 0.018813
	LOSS [training: 0.5813111249921699 | validation: 0.5197927025734472]
	TIME [epoch: 15 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5936598733346322		[learning rate: 0.018805]
	Learning Rate: 0.0188052
	LOSS [training: 0.5936598733346322 | validation: 0.5179167212524564]
	TIME [epoch: 15.1 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5478902291002457		[learning rate: 0.018797]
	Learning Rate: 0.0187973
	LOSS [training: 0.5478902291002457 | validation: 0.5212585368647317]
	TIME [epoch: 15.2 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5550454877374614		[learning rate: 0.018789]
	Learning Rate: 0.0187894
	LOSS [training: 0.5550454877374614 | validation: 0.5411329614484944]
	TIME [epoch: 15.1 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5438910985596547		[learning rate: 0.018782]
	Learning Rate: 0.0187815
	LOSS [training: 0.5438910985596547 | validation: 0.6050643162383802]
	TIME [epoch: 15.1 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5470977022494222		[learning rate: 0.018774]
	Learning Rate: 0.0187736
	LOSS [training: 0.5470977022494222 | validation: 0.6027904157085777]
	TIME [epoch: 15.2 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5257200708943394		[learning rate: 0.018766]
	Learning Rate: 0.0187657
	LOSS [training: 0.5257200708943394 | validation: 0.6873852492448351]
	TIME [epoch: 15.1 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5387483001056845		[learning rate: 0.018758]
	Learning Rate: 0.0187577
	LOSS [training: 0.5387483001056845 | validation: 0.4890767618444698]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6040648983499672		[learning rate: 0.01875]
	Learning Rate: 0.0187497
	LOSS [training: 0.6040648983499672 | validation: 0.518154464989576]
	TIME [epoch: 15 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5462371464735741		[learning rate: 0.018742]
	Learning Rate: 0.0187417
	LOSS [training: 0.5462371464735741 | validation: 0.6889440635321314]
	TIME [epoch: 15.1 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5137216469567347		[learning rate: 0.018734]
	Learning Rate: 0.0187337
	LOSS [training: 0.5137216469567347 | validation: 0.6685372995010466]
	TIME [epoch: 15.1 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5663043568880887		[learning rate: 0.018726]
	Learning Rate: 0.0187256
	LOSS [training: 0.5663043568880887 | validation: 0.5003132498651461]
	TIME [epoch: 15.1 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5470947042349876		[learning rate: 0.018718]
	Learning Rate: 0.0187175
	LOSS [training: 0.5470947042349876 | validation: 0.5174655445001815]
	TIME [epoch: 15.1 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5294067595914798		[learning rate: 0.018709]
	Learning Rate: 0.0187094
	LOSS [training: 0.5294067595914798 | validation: 0.6112250059771864]
	TIME [epoch: 15.1 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5789566164758269		[learning rate: 0.018701]
	Learning Rate: 0.0187013
	LOSS [training: 0.5789566164758269 | validation: 0.5200275276301736]
	TIME [epoch: 15.1 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49582471893099533		[learning rate: 0.018693]
	Learning Rate: 0.0186931
	LOSS [training: 0.49582471893099533 | validation: 0.5496252913797842]
	TIME [epoch: 15.1 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49301602930598215		[learning rate: 0.018685]
	Learning Rate: 0.0186849
	LOSS [training: 0.49301602930598215 | validation: 0.9444108197799503]
	TIME [epoch: 15 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5947728780620252		[learning rate: 0.018677]
	Learning Rate: 0.0186767
	LOSS [training: 0.5947728780620252 | validation: 0.7370839752552341]
	TIME [epoch: 15.1 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5887759737585344		[learning rate: 0.018668]
	Learning Rate: 0.0186685
	LOSS [training: 0.5887759737585344 | validation: 0.6571191823595308]
	TIME [epoch: 15.1 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5393932388811464		[learning rate: 0.01866]
	Learning Rate: 0.0186602
	LOSS [training: 0.5393932388811464 | validation: 0.6641883134681548]
	TIME [epoch: 15.2 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5713684179081213		[learning rate: 0.018652]
	Learning Rate: 0.018652
	LOSS [training: 0.5713684179081213 | validation: 0.5566636882773282]
	TIME [epoch: 15.1 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5042110536631292		[learning rate: 0.018644]
	Learning Rate: 0.0186437
	LOSS [training: 0.5042110536631292 | validation: 0.5945587203583795]
	TIME [epoch: 15.1 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5554259980416163		[learning rate: 0.018635]
	Learning Rate: 0.0186353
	LOSS [training: 0.5554259980416163 | validation: 0.6186867042122401]
	TIME [epoch: 15.2 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5201674556558481		[learning rate: 0.018627]
	Learning Rate: 0.018627
	LOSS [training: 0.5201674556558481 | validation: 0.5858700979725476]
	TIME [epoch: 15 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5790386690642167		[learning rate: 0.018619]
	Learning Rate: 0.0186186
	LOSS [training: 0.5790386690642167 | validation: 0.5403912327165828]
	TIME [epoch: 15.1 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5287136716595597		[learning rate: 0.01861]
	Learning Rate: 0.0186102
	LOSS [training: 0.5287136716595597 | validation: 0.6721593363492144]
	TIME [epoch: 15.2 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5573298352288101		[learning rate: 0.018602]
	Learning Rate: 0.0186018
	LOSS [training: 0.5573298352288101 | validation: 0.5440298536552168]
	TIME [epoch: 15.1 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4987717623456175		[learning rate: 0.018593]
	Learning Rate: 0.0185934
	LOSS [training: 0.4987717623456175 | validation: 0.6122554089749663]
	TIME [epoch: 15.1 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5867544697420246		[learning rate: 0.018585]
	Learning Rate: 0.0185849
	LOSS [training: 0.5867544697420246 | validation: 0.5100973300909587]
	TIME [epoch: 15.1 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4947674455231433		[learning rate: 0.018576]
	Learning Rate: 0.0185764
	LOSS [training: 0.4947674455231433 | validation: 0.5599495415400371]
	TIME [epoch: 15.1 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49552680144641237		[learning rate: 0.018568]
	Learning Rate: 0.0185679
	LOSS [training: 0.49552680144641237 | validation: 0.5771356484494297]
	TIME [epoch: 15.2 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.601920457939089		[learning rate: 0.018559]
	Learning Rate: 0.0185594
	LOSS [training: 0.601920457939089 | validation: 0.5499692479650937]
	TIME [epoch: 15 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4575621980314535		[learning rate: 0.018551]
	Learning Rate: 0.0185508
	LOSS [training: 0.4575621980314535 | validation: 0.7921945179198573]
	TIME [epoch: 15.1 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5775028935926043		[learning rate: 0.018542]
	Learning Rate: 0.0185422
	LOSS [training: 0.5775028935926043 | validation: 0.5562523571798872]
	TIME [epoch: 15.2 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5298459219248124		[learning rate: 0.018534]
	Learning Rate: 0.0185336
	LOSS [training: 0.5298459219248124 | validation: 0.5998032941147178]
	TIME [epoch: 15.1 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5528996163042179		[learning rate: 0.018525]
	Learning Rate: 0.018525
	LOSS [training: 0.5528996163042179 | validation: 0.6375768297340174]
	TIME [epoch: 15.1 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5571664421343633		[learning rate: 0.018516]
	Learning Rate: 0.0185163
	LOSS [training: 0.5571664421343633 | validation: 0.5188623387475579]
	TIME [epoch: 15.1 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5483600831518636		[learning rate: 0.018508]
	Learning Rate: 0.0185077
	LOSS [training: 0.5483600831518636 | validation: 0.5753474154313343]
	TIME [epoch: 15.1 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4689857563557244		[learning rate: 0.018499]
	Learning Rate: 0.018499
	LOSS [training: 0.4689857563557244 | validation: 0.8130727806632808]
	TIME [epoch: 15.1 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6125529645812178		[learning rate: 0.01849]
	Learning Rate: 0.0184902
	LOSS [training: 0.6125529645812178 | validation: 0.49987843817501193]
	TIME [epoch: 15.1 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4163636943744324		[learning rate: 0.018481]
	Learning Rate: 0.0184815
	LOSS [training: 0.4163636943744324 | validation: 0.7710674730987701]
	TIME [epoch: 15 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6434397404935989		[learning rate: 0.018473]
	Learning Rate: 0.0184727
	LOSS [training: 0.6434397404935989 | validation: 0.5550173725577834]
	TIME [epoch: 15.1 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5407196970773139		[learning rate: 0.018464]
	Learning Rate: 0.0184639
	LOSS [training: 0.5407196970773139 | validation: 0.549071411138963]
	TIME [epoch: 15.1 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4998002118089623		[learning rate: 0.018455]
	Learning Rate: 0.0184551
	LOSS [training: 0.4998002118089623 | validation: 0.5056006607138691]
	TIME [epoch: 15.1 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5298605094863732		[learning rate: 0.018446]
	Learning Rate: 0.0184463
	LOSS [training: 0.5298605094863732 | validation: 0.5451970751498236]
	TIME [epoch: 15.2 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4923594219507462		[learning rate: 0.018437]
	Learning Rate: 0.0184374
	LOSS [training: 0.4923594219507462 | validation: 0.5720636836709677]
	TIME [epoch: 15.1 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5372263929588386		[learning rate: 0.018429]
	Learning Rate: 0.0184285
	LOSS [training: 0.5372263929588386 | validation: 0.5192765291172968]
	TIME [epoch: 15.1 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4521174773194799		[learning rate: 0.01842]
	Learning Rate: 0.0184196
	LOSS [training: 0.4521174773194799 | validation: 0.5498041533889968]
	TIME [epoch: 15.1 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6032956889110639		[learning rate: 0.018411]
	Learning Rate: 0.0184107
	LOSS [training: 0.6032956889110639 | validation: 0.49073220556893404]
	TIME [epoch: 15.1 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4860485760317188		[learning rate: 0.018402]
	Learning Rate: 0.0184017
	LOSS [training: 0.4860485760317188 | validation: 0.5544640364963276]
	TIME [epoch: 15.1 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.493579866051211		[learning rate: 0.018393]
	Learning Rate: 0.0183928
	LOSS [training: 0.493579866051211 | validation: 0.5312176594399665]
	TIME [epoch: 15 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5456510245427855		[learning rate: 0.018384]
	Learning Rate: 0.0183838
	LOSS [training: 0.5456510245427855 | validation: 0.5906421877931887]
	TIME [epoch: 15.1 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5325376757176356		[learning rate: 0.018375]
	Learning Rate: 0.0183747
	LOSS [training: 0.5325376757176356 | validation: 0.4966247102956918]
	TIME [epoch: 15.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5044314974415288		[learning rate: 0.018366]
	Learning Rate: 0.0183657
	LOSS [training: 0.5044314974415288 | validation: 0.5279199984005031]
	TIME [epoch: 15.1 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5495841383771726		[learning rate: 0.018357]
	Learning Rate: 0.0183566
	LOSS [training: 0.5495841383771726 | validation: 0.5378805761510475]
	TIME [epoch: 15.1 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5122345066416247		[learning rate: 0.018348]
	Learning Rate: 0.0183475
	LOSS [training: 0.5122345066416247 | validation: 0.5514447622984155]
	TIME [epoch: 15.1 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5098940208672997		[learning rate: 0.018338]
	Learning Rate: 0.0183384
	LOSS [training: 0.5098940208672997 | validation: 0.5134074868471004]
	TIME [epoch: 15.1 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.499603606960522		[learning rate: 0.018329]
	Learning Rate: 0.0183293
	LOSS [training: 0.499603606960522 | validation: 0.5795489429225951]
	TIME [epoch: 15 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.53659831438344		[learning rate: 0.01832]
	Learning Rate: 0.0183201
	LOSS [training: 0.53659831438344 | validation: 0.46521428452607005]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4858732792002152		[learning rate: 0.018311]
	Learning Rate: 0.0183109
	LOSS [training: 0.4858732792002152 | validation: 0.6319188165950483]
	TIME [epoch: 15.1 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5416400175809973		[learning rate: 0.018302]
	Learning Rate: 0.0183017
	LOSS [training: 0.5416400175809973 | validation: 0.47569879156499484]
	TIME [epoch: 15.1 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4868142865154822		[learning rate: 0.018293]
	Learning Rate: 0.0182925
	LOSS [training: 0.4868142865154822 | validation: 0.615623215993159]
	TIME [epoch: 15.2 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5182718583864163		[learning rate: 0.018283]
	Learning Rate: 0.0182833
	LOSS [training: 0.5182718583864163 | validation: 0.49015272867464965]
	TIME [epoch: 15.1 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5206100581699703		[learning rate: 0.018274]
	Learning Rate: 0.018274
	LOSS [training: 0.5206100581699703 | validation: 0.48543754559863816]
	TIME [epoch: 15.2 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5077561262507028		[learning rate: 0.018265]
	Learning Rate: 0.0182647
	LOSS [training: 0.5077561262507028 | validation: 0.6653797610434188]
	TIME [epoch: 15.1 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46415880793797126		[learning rate: 0.018255]
	Learning Rate: 0.0182554
	LOSS [training: 0.46415880793797126 | validation: 0.6261935743929467]
	TIME [epoch: 15.1 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5234721181392133		[learning rate: 0.018246]
	Learning Rate: 0.018246
	LOSS [training: 0.5234721181392133 | validation: 0.49700268590312113]
	TIME [epoch: 15.2 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5550668610980903		[learning rate: 0.018237]
	Learning Rate: 0.0182367
	LOSS [training: 0.5550668610980903 | validation: 0.4808642591574499]
	TIME [epoch: 15.1 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4675481345803653		[learning rate: 0.018227]
	Learning Rate: 0.0182273
	LOSS [training: 0.4675481345803653 | validation: 0.4884080875199375]
	TIME [epoch: 15.1 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4660933360352025		[learning rate: 0.018218]
	Learning Rate: 0.0182179
	LOSS [training: 0.4660933360352025 | validation: 0.638752373818128]
	TIME [epoch: 15.2 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5038029932843456		[learning rate: 0.018208]
	Learning Rate: 0.0182085
	LOSS [training: 0.5038029932843456 | validation: 0.4952391915859069]
	TIME [epoch: 15.1 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5133439308709915		[learning rate: 0.018199]
	Learning Rate: 0.018199
	LOSS [training: 0.5133439308709915 | validation: 0.48853657695682773]
	TIME [epoch: 15.1 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5132993643740125		[learning rate: 0.01819]
	Learning Rate: 0.0181895
	LOSS [training: 0.5132993643740125 | validation: 0.507117515965914]
	TIME [epoch: 15.2 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46974478578085743		[learning rate: 0.01818]
	Learning Rate: 0.01818
	LOSS [training: 0.46974478578085743 | validation: 0.49703933524992505]
	TIME [epoch: 15.1 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5340998008354431		[learning rate: 0.018171]
	Learning Rate: 0.0181705
	LOSS [training: 0.5340998008354431 | validation: 0.6354651029462247]
	TIME [epoch: 15.2 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5472784963683559		[learning rate: 0.018161]
	Learning Rate: 0.018161
	LOSS [training: 0.5472784963683559 | validation: 0.5691274733911338]
	TIME [epoch: 15.1 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5243943202250269		[learning rate: 0.018151]
	Learning Rate: 0.0181514
	LOSS [training: 0.5243943202250269 | validation: 0.5865202023431737]
	TIME [epoch: 15.1 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4271336920187272		[learning rate: 0.018142]
	Learning Rate: 0.0181418
	LOSS [training: 0.4271336920187272 | validation: 0.527479056726891]
	TIME [epoch: 15.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6467512104658169		[learning rate: 0.018132]
	Learning Rate: 0.0181322
	LOSS [training: 0.6467512104658169 | validation: 0.5049298088181169]
	TIME [epoch: 15.2 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49513304464722396		[learning rate: 0.018123]
	Learning Rate: 0.0181226
	LOSS [training: 0.49513304464722396 | validation: 0.44796866347210873]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4911298016859732		[learning rate: 0.018113]
	Learning Rate: 0.0181129
	LOSS [training: 0.4911298016859732 | validation: 0.44929720301652987]
	TIME [epoch: 15.2 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47054319971851527		[learning rate: 0.018103]
	Learning Rate: 0.0181032
	LOSS [training: 0.47054319971851527 | validation: 0.6812581679468639]
	TIME [epoch: 15 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.507239864395551		[learning rate: 0.018094]
	Learning Rate: 0.0180936
	LOSS [training: 0.507239864395551 | validation: 0.5608438740518003]
	TIME [epoch: 15 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5310788288817314		[learning rate: 0.018084]
	Learning Rate: 0.0180838
	LOSS [training: 0.5310788288817314 | validation: 0.42958928144889813]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.551677659106415		[learning rate: 0.018074]
	Learning Rate: 0.0180741
	LOSS [training: 0.551677659106415 | validation: 0.4477022052259627]
	TIME [epoch: 15 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47778486953063226		[learning rate: 0.018064]
	Learning Rate: 0.0180643
	LOSS [training: 0.47778486953063226 | validation: 0.5303921935907909]
	TIME [epoch: 15.1 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4800065841812724		[learning rate: 0.018055]
	Learning Rate: 0.0180545
	LOSS [training: 0.4800065841812724 | validation: 0.540657065037703]
	TIME [epoch: 15.1 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5346185072968508		[learning rate: 0.018045]
	Learning Rate: 0.0180447
	LOSS [training: 0.5346185072968508 | validation: 0.4545448125457084]
	TIME [epoch: 15.1 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49258866378107796		[learning rate: 0.018035]
	Learning Rate: 0.0180349
	LOSS [training: 0.49258866378107796 | validation: 0.5834374399680485]
	TIME [epoch: 15.1 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4763341885150105		[learning rate: 0.018025]
	Learning Rate: 0.0180251
	LOSS [training: 0.4763341885150105 | validation: 0.6874396412873837]
	TIME [epoch: 15.1 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48328591288906375		[learning rate: 0.018015]
	Learning Rate: 0.0180152
	LOSS [training: 0.48328591288906375 | validation: 0.6833052611286802]
	TIME [epoch: 15.1 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48675002368328346		[learning rate: 0.018005]
	Learning Rate: 0.0180053
	LOSS [training: 0.48675002368328346 | validation: 0.554891805325785]
	TIME [epoch: 15.1 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47772410927521725		[learning rate: 0.017995]
	Learning Rate: 0.0179954
	LOSS [training: 0.47772410927521725 | validation: 0.5106284301386648]
	TIME [epoch: 15.1 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5482234591821047		[learning rate: 0.017985]
	Learning Rate: 0.0179854
	LOSS [training: 0.5482234591821047 | validation: 0.5406230032676713]
	TIME [epoch: 15.1 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4446351645148753		[learning rate: 0.017975]
	Learning Rate: 0.0179755
	LOSS [training: 0.4446351645148753 | validation: 0.4857258089049541]
	TIME [epoch: 15.1 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5205501145752703		[learning rate: 0.017965]
	Learning Rate: 0.0179655
	LOSS [training: 0.5205501145752703 | validation: 0.43024715833547056]
	TIME [epoch: 15.1 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5232930194754142		[learning rate: 0.017955]
	Learning Rate: 0.0179555
	LOSS [training: 0.5232930194754142 | validation: 0.4817306578750121]
	TIME [epoch: 15.1 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4877499356061507		[learning rate: 0.017945]
	Learning Rate: 0.0179455
	LOSS [training: 0.4877499356061507 | validation: 0.47362592953621874]
	TIME [epoch: 15.2 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4454907324376054		[learning rate: 0.017935]
	Learning Rate: 0.0179354
	LOSS [training: 0.4454907324376054 | validation: 0.5230177683887807]
	TIME [epoch: 15.1 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5425217594470721		[learning rate: 0.017925]
	Learning Rate: 0.0179253
	LOSS [training: 0.5425217594470721 | validation: 0.5323847481887296]
	TIME [epoch: 15.1 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46778851248029896		[learning rate: 0.017915]
	Learning Rate: 0.0179152
	LOSS [training: 0.46778851248029896 | validation: 0.6202461796464112]
	TIME [epoch: 15.1 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5161654396830874		[learning rate: 0.017905]
	Learning Rate: 0.0179051
	LOSS [training: 0.5161654396830874 | validation: 0.46080898686805494]
	TIME [epoch: 15.1 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4455554069166779		[learning rate: 0.017895]
	Learning Rate: 0.017895
	LOSS [training: 0.4455554069166779 | validation: 0.44896573498048864]
	TIME [epoch: 15.1 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.50198772718663		[learning rate: 0.017885]
	Learning Rate: 0.0178848
	LOSS [training: 0.50198772718663 | validation: 0.637936773261597]
	TIME [epoch: 15.1 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4647678485455178		[learning rate: 0.017875]
	Learning Rate: 0.0178747
	LOSS [training: 0.4647678485455178 | validation: 0.6446137927354185]
	TIME [epoch: 15.1 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5410342919504856		[learning rate: 0.017864]
	Learning Rate: 0.0178645
	LOSS [training: 0.5410342919504856 | validation: 0.4950060918146112]
	TIME [epoch: 15.1 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45874473223333717		[learning rate: 0.017854]
	Learning Rate: 0.0178542
	LOSS [training: 0.45874473223333717 | validation: 0.5268208759830155]
	TIME [epoch: 15.1 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43800322572015987		[learning rate: 0.017844]
	Learning Rate: 0.017844
	LOSS [training: 0.43800322572015987 | validation: 0.5780692471106033]
	TIME [epoch: 15.1 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5404699083257021		[learning rate: 0.017834]
	Learning Rate: 0.0178337
	LOSS [training: 0.5404699083257021 | validation: 0.47991682878935]
	TIME [epoch: 15.2 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43201938613070096		[learning rate: 0.017823]
	Learning Rate: 0.0178235
	LOSS [training: 0.43201938613070096 | validation: 0.5456572742590798]
	TIME [epoch: 15.1 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4781116325283753		[learning rate: 0.017813]
	Learning Rate: 0.0178131
	LOSS [training: 0.4781116325283753 | validation: 0.4930102871983171]
	TIME [epoch: 15.1 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49986733445616505		[learning rate: 0.017803]
	Learning Rate: 0.0178028
	LOSS [training: 0.49986733445616505 | validation: 0.5063321315508292]
	TIME [epoch: 15.1 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48454157858278973		[learning rate: 0.017792]
	Learning Rate: 0.0177925
	LOSS [training: 0.48454157858278973 | validation: 0.4462436574799562]
	TIME [epoch: 15.1 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5244687278891051		[learning rate: 0.017782]
	Learning Rate: 0.0177821
	LOSS [training: 0.5244687278891051 | validation: 0.5076329905255548]
	TIME [epoch: 15.1 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41117930703689215		[learning rate: 0.017772]
	Learning Rate: 0.0177717
	LOSS [training: 0.41117930703689215 | validation: 0.7818984298541743]
	TIME [epoch: 15.1 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5593096986449783		[learning rate: 0.017761]
	Learning Rate: 0.0177613
	LOSS [training: 0.5593096986449783 | validation: 0.49029842543188784]
	TIME [epoch: 15.1 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45090244467389773		[learning rate: 0.017751]
	Learning Rate: 0.0177509
	LOSS [training: 0.45090244467389773 | validation: 0.46434224612823694]
	TIME [epoch: 15.2 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4804622249867758		[learning rate: 0.01774]
	Learning Rate: 0.0177404
	LOSS [training: 0.4804622249867758 | validation: 0.4321010636346564]
	TIME [epoch: 15.1 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4796692996516194		[learning rate: 0.01773]
	Learning Rate: 0.0177299
	LOSS [training: 0.4796692996516194 | validation: 0.495614052220365]
	TIME [epoch: 15.1 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5126844586647185		[learning rate: 0.017719]
	Learning Rate: 0.0177194
	LOSS [training: 0.5126844586647185 | validation: 0.4476405132889849]
	TIME [epoch: 15.2 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4048138074727102		[learning rate: 0.017709]
	Learning Rate: 0.0177089
	LOSS [training: 0.4048138074727102 | validation: 0.5607294689337365]
	TIME [epoch: 15.1 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5813733248457531		[learning rate: 0.017698]
	Learning Rate: 0.0176984
	LOSS [training: 0.5813733248457531 | validation: 0.5171996830698163]
	TIME [epoch: 15.1 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43583280862494844		[learning rate: 0.017688]
	Learning Rate: 0.0176878
	LOSS [training: 0.43583280862494844 | validation: 0.45069297086257964]
	TIME [epoch: 15.2 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5047560363321482		[learning rate: 0.017677]
	Learning Rate: 0.0176772
	LOSS [training: 0.5047560363321482 | validation: 0.46897208467011664]
	TIME [epoch: 15.1 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42538238457922695		[learning rate: 0.017667]
	Learning Rate: 0.0176666
	LOSS [training: 0.42538238457922695 | validation: 0.48366599101303515]
	TIME [epoch: 15.1 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5120525728031083		[learning rate: 0.017656]
	Learning Rate: 0.017656
	LOSS [training: 0.5120525728031083 | validation: 0.483431846458087]
	TIME [epoch: 15.1 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4414577285008211		[learning rate: 0.017645]
	Learning Rate: 0.0176454
	LOSS [training: 0.4414577285008211 | validation: 0.5154302137381989]
	TIME [epoch: 15.1 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4834370705739678		[learning rate: 0.017635]
	Learning Rate: 0.0176347
	LOSS [training: 0.4834370705739678 | validation: 0.4054800490935877]
	TIME [epoch: 15.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47201380857637265		[learning rate: 0.017624]
	Learning Rate: 0.017624
	LOSS [training: 0.47201380857637265 | validation: 0.46138998322525837]
	TIME [epoch: 15.1 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4787417881333904		[learning rate: 0.017613]
	Learning Rate: 0.0176133
	LOSS [training: 0.4787417881333904 | validation: 0.45127316574464904]
	TIME [epoch: 15.1 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47331963458999765		[learning rate: 0.017603]
	Learning Rate: 0.0176026
	LOSS [training: 0.47331963458999765 | validation: 0.5101643810932275]
	TIME [epoch: 15.2 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46952148678543276		[learning rate: 0.017592]
	Learning Rate: 0.0175918
	LOSS [training: 0.46952148678543276 | validation: 0.49419228537679993]
	TIME [epoch: 15.1 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44061555177894696		[learning rate: 0.017581]
	Learning Rate: 0.0175811
	LOSS [training: 0.44061555177894696 | validation: 0.46805087441292287]
	TIME [epoch: 15.1 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5112402709284176		[learning rate: 0.01757]
	Learning Rate: 0.0175703
	LOSS [training: 0.5112402709284176 | validation: 0.4907821620377462]
	TIME [epoch: 15.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4402948849499605		[learning rate: 0.017559]
	Learning Rate: 0.0175595
	LOSS [training: 0.4402948849499605 | validation: 0.5507986017000315]
	TIME [epoch: 15.1 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48525445927359373		[learning rate: 0.017549]
	Learning Rate: 0.0175486
	LOSS [training: 0.48525445927359373 | validation: 0.4797858389341823]
	TIME [epoch: 15.1 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5067763461974688		[learning rate: 0.017538]
	Learning Rate: 0.0175378
	LOSS [training: 0.5067763461974688 | validation: 0.5276270975893878]
	TIME [epoch: 15.2 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42396557860234885		[learning rate: 0.017527]
	Learning Rate: 0.0175269
	LOSS [training: 0.42396557860234885 | validation: 0.5422543375662275]
	TIME [epoch: 15.1 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5082980376308099		[learning rate: 0.017516]
	Learning Rate: 0.017516
	LOSS [training: 0.5082980376308099 | validation: 0.5144327516648187]
	TIME [epoch: 15.2 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44094468626080574		[learning rate: 0.017505]
	Learning Rate: 0.0175051
	LOSS [training: 0.44094468626080574 | validation: 0.5152579797968332]
	TIME [epoch: 15.1 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45282493995156126		[learning rate: 0.017494]
	Learning Rate: 0.0174942
	LOSS [training: 0.45282493995156126 | validation: 0.5554512451163784]
	TIME [epoch: 15.1 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48335416901455075		[learning rate: 0.017483]
	Learning Rate: 0.0174832
	LOSS [training: 0.48335416901455075 | validation: 0.44762341944087913]
	TIME [epoch: 15.2 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4699746805278558		[learning rate: 0.017472]
	Learning Rate: 0.0174722
	LOSS [training: 0.4699746805278558 | validation: 0.44320406152422553]
	TIME [epoch: 15.1 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5001866311763086		[learning rate: 0.017461]
	Learning Rate: 0.0174612
	LOSS [training: 0.5001866311763086 | validation: 0.5211309971552132]
	TIME [epoch: 15.1 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4179901935075856		[learning rate: 0.01745]
	Learning Rate: 0.0174502
	LOSS [training: 0.4179901935075856 | validation: 0.5755230173633219]
	TIME [epoch: 15.2 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4866240061015413		[learning rate: 0.017439]
	Learning Rate: 0.0174392
	LOSS [training: 0.4866240061015413 | validation: 0.6303113885305522]
	TIME [epoch: 15.1 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4641643015087594		[learning rate: 0.017428]
	Learning Rate: 0.0174281
	LOSS [training: 0.4641643015087594 | validation: 0.44289909189304505]
	TIME [epoch: 15.1 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.475665481668721		[learning rate: 0.017417]
	Learning Rate: 0.017417
	LOSS [training: 0.475665481668721 | validation: 0.5613564702757275]
	TIME [epoch: 15.1 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44713059163970115		[learning rate: 0.017406]
	Learning Rate: 0.0174059
	LOSS [training: 0.44713059163970115 | validation: 0.48147599661915313]
	TIME [epoch: 15.1 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4370987008980125		[learning rate: 0.017395]
	Learning Rate: 0.0173948
	LOSS [training: 0.4370987008980125 | validation: 0.9003115210605588]
	TIME [epoch: 15.2 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5218785797574298		[learning rate: 0.017384]
	Learning Rate: 0.0173837
	LOSS [training: 0.5218785797574298 | validation: 0.4436422665438827]
	TIME [epoch: 15.1 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4415507623051421		[learning rate: 0.017373]
	Learning Rate: 0.0173725
	LOSS [training: 0.4415507623051421 | validation: 0.5120323324361271]
	TIME [epoch: 15.1 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46085074084190064		[learning rate: 0.017361]
	Learning Rate: 0.0173614
	LOSS [training: 0.46085074084190064 | validation: 0.500272324201803]
	TIME [epoch: 15.2 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4087773773842617		[learning rate: 0.01735]
	Learning Rate: 0.0173502
	LOSS [training: 0.4087773773842617 | validation: 0.6103710913793043]
	TIME [epoch: 138 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4824545201239835		[learning rate: 0.017339]
	Learning Rate: 0.0173389
	LOSS [training: 0.4824545201239835 | validation: 0.4331983976129779]
	TIME [epoch: 32.9 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42905489494507276		[learning rate: 0.017328]
	Learning Rate: 0.0173277
	LOSS [training: 0.42905489494507276 | validation: 0.6723258459299901]
	TIME [epoch: 33 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48007706981871645		[learning rate: 0.017316]
	Learning Rate: 0.0173164
	LOSS [training: 0.48007706981871645 | validation: 0.5316856987142422]
	TIME [epoch: 32.9 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41235743919301704		[learning rate: 0.017305]
	Learning Rate: 0.0173052
	LOSS [training: 0.41235743919301704 | validation: 0.47154403736813255]
	TIME [epoch: 33 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.516087848433658		[learning rate: 0.017294]
	Learning Rate: 0.0172939
	LOSS [training: 0.516087848433658 | validation: 0.47336313750485415]
	TIME [epoch: 33 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3956432116156443		[learning rate: 0.017283]
	Learning Rate: 0.0172826
	LOSS [training: 0.3956432116156443 | validation: 0.5085717855875784]
	TIME [epoch: 33 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4692192777867441		[learning rate: 0.017271]
	Learning Rate: 0.0172712
	LOSS [training: 0.4692192777867441 | validation: 0.5231992324506417]
	TIME [epoch: 33 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43801905106709116		[learning rate: 0.01726]
	Learning Rate: 0.0172599
	LOSS [training: 0.43801905106709116 | validation: 0.7140400937121667]
	TIME [epoch: 33 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44866189057639017		[learning rate: 0.017248]
	Learning Rate: 0.0172485
	LOSS [training: 0.44866189057639017 | validation: 0.3919272827688481]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5128216315583637		[learning rate: 0.017237]
	Learning Rate: 0.0172371
	LOSS [training: 0.5128216315583637 | validation: 0.43823874796792106]
	TIME [epoch: 32.9 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4306338695062155		[learning rate: 0.017226]
	Learning Rate: 0.0172257
	LOSS [training: 0.4306338695062155 | validation: 0.45646414739666297]
	TIME [epoch: 33 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42176364446509707		[learning rate: 0.017214]
	Learning Rate: 0.0172142
	LOSS [training: 0.42176364446509707 | validation: 0.44029573109967196]
	TIME [epoch: 33 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46752969900212515		[learning rate: 0.017203]
	Learning Rate: 0.0172028
	LOSS [training: 0.46752969900212515 | validation: 0.40447174339381653]
	TIME [epoch: 33 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4197983752638212		[learning rate: 0.017191]
	Learning Rate: 0.0171913
	LOSS [training: 0.4197983752638212 | validation: 0.40175554195887353]
	TIME [epoch: 33 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45082415434546863		[learning rate: 0.01718]
	Learning Rate: 0.0171798
	LOSS [training: 0.45082415434546863 | validation: 0.600727597034209]
	TIME [epoch: 32.9 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48673639444344674		[learning rate: 0.017168]
	Learning Rate: 0.0171683
	LOSS [training: 0.48673639444344674 | validation: 0.4184349145658133]
	TIME [epoch: 33 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44263944382868076		[learning rate: 0.017157]
	Learning Rate: 0.0171567
	LOSS [training: 0.44263944382868076 | validation: 0.4671086559095756]
	TIME [epoch: 32.8 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4495570163375383		[learning rate: 0.017145]
	Learning Rate: 0.0171452
	LOSS [training: 0.4495570163375383 | validation: 0.6269947847128461]
	TIME [epoch: 32.9 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4941631439891949		[learning rate: 0.017134]
	Learning Rate: 0.0171336
	LOSS [training: 0.4941631439891949 | validation: 0.45792302572486554]
	TIME [epoch: 32.9 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.423225315815394		[learning rate: 0.017122]
	Learning Rate: 0.017122
	LOSS [training: 0.423225315815394 | validation: 0.5985489988190804]
	TIME [epoch: 32.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42050275978680474		[learning rate: 0.01711]
	Learning Rate: 0.0171104
	LOSS [training: 0.42050275978680474 | validation: 0.644697001451607]
	TIME [epoch: 33 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5114796748295138		[learning rate: 0.017099]
	Learning Rate: 0.0170988
	LOSS [training: 0.5114796748295138 | validation: 0.5177733521794174]
	TIME [epoch: 33 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4200016513682362		[learning rate: 0.017087]
	Learning Rate: 0.0170871
	LOSS [training: 0.4200016513682362 | validation: 0.53154887533716]
	TIME [epoch: 32.9 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4331979502672306		[learning rate: 0.017075]
	Learning Rate: 0.0170755
	LOSS [training: 0.4331979502672306 | validation: 0.4673141078858154]
	TIME [epoch: 32.8 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4535974565159881		[learning rate: 0.017064]
	Learning Rate: 0.0170638
	LOSS [training: 0.4535974565159881 | validation: 0.5289820923334795]
	TIME [epoch: 32.8 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42525640964890277		[learning rate: 0.017052]
	Learning Rate: 0.0170521
	LOSS [training: 0.42525640964890277 | validation: 0.44658498498769605]
	TIME [epoch: 33 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.530993505183593		[learning rate: 0.01704]
	Learning Rate: 0.0170403
	LOSS [training: 0.530993505183593 | validation: 0.5015232517489401]
	TIME [epoch: 32.9 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4218922733890615		[learning rate: 0.017029]
	Learning Rate: 0.0170286
	LOSS [training: 0.4218922733890615 | validation: 0.38697590283240957]
	TIME [epoch: 32.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4009129578399511		[learning rate: 0.017017]
	Learning Rate: 0.0170168
	LOSS [training: 0.4009129578399511 | validation: 0.5758841461588272]
	TIME [epoch: 33 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45001154834325574		[learning rate: 0.017005]
	Learning Rate: 0.017005
	LOSS [training: 0.45001154834325574 | validation: 0.4600206320965119]
	TIME [epoch: 33 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4574911583873118		[learning rate: 0.016993]
	Learning Rate: 0.0169932
	LOSS [training: 0.4574911583873118 | validation: 0.4577564943128849]
	TIME [epoch: 32.8 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4591795216072343		[learning rate: 0.016981]
	Learning Rate: 0.0169814
	LOSS [training: 0.4591795216072343 | validation: 0.5236236943200431]
	TIME [epoch: 33 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42129752812661386		[learning rate: 0.01697]
	Learning Rate: 0.0169695
	LOSS [training: 0.42129752812661386 | validation: 0.46294798640232937]
	TIME [epoch: 33 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44043325394697064		[learning rate: 0.016958]
	Learning Rate: 0.0169577
	LOSS [training: 0.44043325394697064 | validation: 0.7847204941250019]
	TIME [epoch: 32.8 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46738570289592773		[learning rate: 0.016946]
	Learning Rate: 0.0169458
	LOSS [training: 0.46738570289592773 | validation: 0.4910573128004274]
	TIME [epoch: 32.9 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4088554089077956		[learning rate: 0.016934]
	Learning Rate: 0.0169339
	LOSS [training: 0.4088554089077956 | validation: 0.4308902521788279]
	TIME [epoch: 33 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4616222406779903		[learning rate: 0.016922]
	Learning Rate: 0.016922
	LOSS [training: 0.4616222406779903 | validation: 0.466628718653321]
	TIME [epoch: 32.9 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42162443620672574		[learning rate: 0.01691]
	Learning Rate: 0.01691
	LOSS [training: 0.42162443620672574 | validation: 0.4106598503762814]
	TIME [epoch: 33 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46276912304442813		[learning rate: 0.016898]
	Learning Rate: 0.0168981
	LOSS [training: 0.46276912304442813 | validation: 0.4233072977925055]
	TIME [epoch: 32.8 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4158933777616768		[learning rate: 0.016886]
	Learning Rate: 0.0168861
	LOSS [training: 0.4158933777616768 | validation: 0.5008826624035654]
	TIME [epoch: 32.9 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5144350692027662		[learning rate: 0.016874]
	Learning Rate: 0.0168741
	LOSS [training: 0.5144350692027662 | validation: 0.4435349706376083]
	TIME [epoch: 32.9 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45885013505904815		[learning rate: 0.016862]
	Learning Rate: 0.0168621
	LOSS [training: 0.45885013505904815 | validation: 0.4241439211220169]
	TIME [epoch: 32.9 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40179320673291613		[learning rate: 0.01685]
	Learning Rate: 0.0168501
	LOSS [training: 0.40179320673291613 | validation: 0.4267185008889428]
	TIME [epoch: 32.8 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4299463958205831		[learning rate: 0.016838]
	Learning Rate: 0.016838
	LOSS [training: 0.4299463958205831 | validation: 0.6185033596240603]
	TIME [epoch: 32.9 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4823508757479109		[learning rate: 0.016826]
	Learning Rate: 0.0168259
	LOSS [training: 0.4823508757479109 | validation: 0.38996757705441343]
	TIME [epoch: 32.9 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4442706920152285		[learning rate: 0.016814]
	Learning Rate: 0.0168138
	LOSS [training: 0.4442706920152285 | validation: 0.5285879740285925]
	TIME [epoch: 32.9 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46083736054502095		[learning rate: 0.016802]
	Learning Rate: 0.0168017
	LOSS [training: 0.46083736054502095 | validation: 0.4773901125002561]
	TIME [epoch: 33 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39321100720199265		[learning rate: 0.01679]
	Learning Rate: 0.0167896
	LOSS [training: 0.39321100720199265 | validation: 0.583516924791331]
	TIME [epoch: 32.9 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43372904672297385		[learning rate: 0.016777]
	Learning Rate: 0.0167775
	LOSS [training: 0.43372904672297385 | validation: 0.494655899420592]
	TIME [epoch: 32.9 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41437610751437826		[learning rate: 0.016765]
	Learning Rate: 0.0167653
	LOSS [training: 0.41437610751437826 | validation: 0.5459710015327285]
	TIME [epoch: 32.9 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4368243957964617		[learning rate: 0.016753]
	Learning Rate: 0.0167531
	LOSS [training: 0.4368243957964617 | validation: 0.44678159008561363]
	TIME [epoch: 32.9 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47805703286020573		[learning rate: 0.016741]
	Learning Rate: 0.0167409
	LOSS [training: 0.47805703286020573 | validation: 0.6222907106523003]
	TIME [epoch: 32.8 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4906994486254714		[learning rate: 0.016729]
	Learning Rate: 0.0167287
	LOSS [training: 0.4906994486254714 | validation: 0.4579617379946201]
	TIME [epoch: 33 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43697009354696864		[learning rate: 0.016716]
	Learning Rate: 0.0167165
	LOSS [training: 0.43697009354696864 | validation: 0.4716464725952141]
	TIME [epoch: 33 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44319818046542625		[learning rate: 0.016704]
	Learning Rate: 0.0167042
	LOSS [training: 0.44319818046542625 | validation: 0.4303129108794614]
	TIME [epoch: 32.9 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4246824961542314		[learning rate: 0.016692]
	Learning Rate: 0.0166919
	LOSS [training: 0.4246824961542314 | validation: 0.4170392627776875]
	TIME [epoch: 33 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43075852894621086		[learning rate: 0.01668]
	Learning Rate: 0.0166796
	LOSS [training: 0.43075852894621086 | validation: 0.5331178249356086]
	TIME [epoch: 33 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4557313193834389		[learning rate: 0.016667]
	Learning Rate: 0.0166673
	LOSS [training: 0.4557313193834389 | validation: 0.4479306624241276]
	TIME [epoch: 32.9 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37426323473130746		[learning rate: 0.016655]
	Learning Rate: 0.016655
	LOSS [training: 0.37426323473130746 | validation: 0.5231689680858478]
	TIME [epoch: 32.8 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5443932794271037		[learning rate: 0.016643]
	Learning Rate: 0.0166427
	LOSS [training: 0.5443932794271037 | validation: 0.42391852678048786]
	TIME [epoch: 33 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36946953529375226		[learning rate: 0.01663]
	Learning Rate: 0.0166303
	LOSS [training: 0.36946953529375226 | validation: 0.44549413461253795]
	TIME [epoch: 32.9 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46506646733956747		[learning rate: 0.016618]
	Learning Rate: 0.0166179
	LOSS [training: 0.46506646733956747 | validation: 0.5331829424370401]
	TIME [epoch: 33 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45717880189823845		[learning rate: 0.016606]
	Learning Rate: 0.0166055
	LOSS [training: 0.45717880189823845 | validation: 0.6946699084008128]
	TIME [epoch: 33 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47365318031477527		[learning rate: 0.016593]
	Learning Rate: 0.0165931
	LOSS [training: 0.47365318031477527 | validation: 0.48538818033187]
	TIME [epoch: 32.7 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41409094878780084		[learning rate: 0.016581]
	Learning Rate: 0.0165807
	LOSS [training: 0.41409094878780084 | validation: 0.47143797086134914]
	TIME [epoch: 32.8 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42107132446185924		[learning rate: 0.016568]
	Learning Rate: 0.0165682
	LOSS [training: 0.42107132446185924 | validation: 0.6233967970933235]
	TIME [epoch: 32.8 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3884616649913638		[learning rate: 0.016556]
	Learning Rate: 0.0165557
	LOSS [training: 0.3884616649913638 | validation: 0.3842374995126435]
	TIME [epoch: 32.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47563785800440606		[learning rate: 0.016543]
	Learning Rate: 0.0165432
	LOSS [training: 0.47563785800440606 | validation: 0.46162179322145025]
	TIME [epoch: 32.9 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4067175020222288		[learning rate: 0.016531]
	Learning Rate: 0.0165307
	LOSS [training: 0.4067175020222288 | validation: 0.47348145698031235]
	TIME [epoch: 32.9 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4356915163849515		[learning rate: 0.016518]
	Learning Rate: 0.0165182
	LOSS [training: 0.4356915163849515 | validation: 0.5045432482146437]
	TIME [epoch: 32.9 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4377004201854249		[learning rate: 0.016506]
	Learning Rate: 0.0165057
	LOSS [training: 0.4377004201854249 | validation: 0.41532550769370463]
	TIME [epoch: 32.9 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40908990977687465		[learning rate: 0.016493]
	Learning Rate: 0.0164931
	LOSS [training: 0.40908990977687465 | validation: 0.4056924468513412]
	TIME [epoch: 33 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4080257182784273		[learning rate: 0.016481]
	Learning Rate: 0.0164805
	LOSS [training: 0.4080257182784273 | validation: 0.37520102543192196]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_574.pth
	Model improved!!!
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4385890376381103		[learning rate: 0.016468]
	Learning Rate: 0.0164679
	LOSS [training: 0.4385890376381103 | validation: 0.4071466832407771]
	TIME [epoch: 33 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38155398001104457		[learning rate: 0.016455]
	Learning Rate: 0.0164553
	LOSS [training: 0.38155398001104457 | validation: 0.6656568996346774]
	TIME [epoch: 32.9 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4496470953877726		[learning rate: 0.016443]
	Learning Rate: 0.0164427
	LOSS [training: 0.4496470953877726 | validation: 0.38827844139987777]
	TIME [epoch: 33 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43328454967023877		[learning rate: 0.01643]
	Learning Rate: 0.01643
	LOSS [training: 0.43328454967023877 | validation: 0.3776987308203096]
	TIME [epoch: 33 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4213464057906103		[learning rate: 0.016417]
	Learning Rate: 0.0164173
	LOSS [training: 0.4213464057906103 | validation: 0.46221828189835057]
	TIME [epoch: 32.9 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4268195011659329		[learning rate: 0.016405]
	Learning Rate: 0.0164047
	LOSS [training: 0.4268195011659329 | validation: 0.5185925038867697]
	TIME [epoch: 32.9 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4334058577891034		[learning rate: 0.016392]
	Learning Rate: 0.016392
	LOSS [training: 0.4334058577891034 | validation: 0.46292861885484826]
	TIME [epoch: 32.9 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44397991274748894		[learning rate: 0.016379]
	Learning Rate: 0.0163792
	LOSS [training: 0.44397991274748894 | validation: 0.4293804174295946]
	TIME [epoch: 32.9 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39635948546650557		[learning rate: 0.016366]
	Learning Rate: 0.0163665
	LOSS [training: 0.39635948546650557 | validation: 0.5146876190057131]
	TIME [epoch: 32.9 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4399145432112719		[learning rate: 0.016354]
	Learning Rate: 0.0163537
	LOSS [training: 0.4399145432112719 | validation: 0.48177857909934496]
	TIME [epoch: 32.9 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4169689818267785		[learning rate: 0.016341]
	Learning Rate: 0.016341
	LOSS [training: 0.4169689818267785 | validation: 0.3891395179773733]
	TIME [epoch: 32.9 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.417388142809642		[learning rate: 0.016328]
	Learning Rate: 0.0163282
	LOSS [training: 0.417388142809642 | validation: 0.4188214838983024]
	TIME [epoch: 32.9 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4182695086217467		[learning rate: 0.016315]
	Learning Rate: 0.0163154
	LOSS [training: 0.4182695086217467 | validation: 0.6054795249798586]
	TIME [epoch: 32.9 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4560957836395643		[learning rate: 0.016303]
	Learning Rate: 0.0163025
	LOSS [training: 0.4560957836395643 | validation: 0.38612222780066296]
	TIME [epoch: 33 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42970082358825734		[learning rate: 0.01629]
	Learning Rate: 0.0162897
	LOSS [training: 0.42970082358825734 | validation: 0.45088178374568305]
	TIME [epoch: 33 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3877496623921877		[learning rate: 0.016277]
	Learning Rate: 0.0162768
	LOSS [training: 0.3877496623921877 | validation: 0.7973032798072768]
	TIME [epoch: 32.9 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44005465806813915		[learning rate: 0.016264]
	Learning Rate: 0.016264
	LOSS [training: 0.44005465806813915 | validation: 0.3871472199543767]
	TIME [epoch: 32.9 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3495970514579705		[learning rate: 0.016251]
	Learning Rate: 0.0162511
	LOSS [training: 0.3495970514579705 | validation: 0.8411204517596982]
	TIME [epoch: 32.9 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5196433744919982		[learning rate: 0.016238]
	Learning Rate: 0.0162382
	LOSS [training: 0.5196433744919982 | validation: 0.5468950447342662]
	TIME [epoch: 32.9 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.393299527469453		[learning rate: 0.016225]
	Learning Rate: 0.0162252
	LOSS [training: 0.393299527469453 | validation: 0.4178255943961646]
	TIME [epoch: 33 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4148285586380815		[learning rate: 0.016212]
	Learning Rate: 0.0162123
	LOSS [training: 0.4148285586380815 | validation: 0.4711675712744735]
	TIME [epoch: 33 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41125861441669076		[learning rate: 0.016199]
	Learning Rate: 0.0161993
	LOSS [training: 0.41125861441669076 | validation: 0.42987606181372134]
	TIME [epoch: 32.9 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45558163964175963		[learning rate: 0.016186]
	Learning Rate: 0.0161864
	LOSS [training: 0.45558163964175963 | validation: 0.418671426758965]
	TIME [epoch: 32.8 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4173974644077968		[learning rate: 0.016173]
	Learning Rate: 0.0161734
	LOSS [training: 0.4173974644077968 | validation: 0.4763528709317428]
	TIME [epoch: 33 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3969096881347415		[learning rate: 0.01616]
	Learning Rate: 0.0161603
	LOSS [training: 0.3969096881347415 | validation: 0.4827773521896621]
	TIME [epoch: 33 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4032912786474466		[learning rate: 0.016147]
	Learning Rate: 0.0161473
	LOSS [training: 0.4032912786474466 | validation: 0.37259832776978763]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_600.pth
	Model improved!!!
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3860621865860514		[learning rate: 0.016134]
	Learning Rate: 0.0161343
	LOSS [training: 0.3860621865860514 | validation: 0.40249403872338196]
	TIME [epoch: 33 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4346284395019799		[learning rate: 0.016121]
	Learning Rate: 0.0161212
	LOSS [training: 0.4346284395019799 | validation: 0.4518178060709679]
	TIME [epoch: 33 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42906355795659235		[learning rate: 0.016108]
	Learning Rate: 0.0161081
	LOSS [training: 0.42906355795659235 | validation: 0.4352879394551422]
	TIME [epoch: 33 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47500095887290594		[learning rate: 0.016095]
	Learning Rate: 0.016095
	LOSS [training: 0.47500095887290594 | validation: 0.45313572120003087]
	TIME [epoch: 33 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3849359606461478		[learning rate: 0.016082]
	Learning Rate: 0.0160819
	LOSS [training: 0.3849359606461478 | validation: 0.44540731783017806]
	TIME [epoch: 33 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4786023916337219		[learning rate: 0.016069]
	Learning Rate: 0.0160688
	LOSS [training: 0.4786023916337219 | validation: 0.4775698506346634]
	TIME [epoch: 33 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37167777860061896		[learning rate: 0.016056]
	Learning Rate: 0.0160556
	LOSS [training: 0.37167777860061896 | validation: 0.43910332796869167]
	TIME [epoch: 33 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43082165219358287		[learning rate: 0.016042]
	Learning Rate: 0.0160425
	LOSS [training: 0.43082165219358287 | validation: 0.45725918391062015]
	TIME [epoch: 33 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4422411923585959		[learning rate: 0.016029]
	Learning Rate: 0.0160293
	LOSS [training: 0.4422411923585959 | validation: 0.422691757308741]
	TIME [epoch: 33 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40860400366907684		[learning rate: 0.016016]
	Learning Rate: 0.0160161
	LOSS [training: 0.40860400366907684 | validation: 0.5229879714263603]
	TIME [epoch: 33 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43791555453084696		[learning rate: 0.016003]
	Learning Rate: 0.0160029
	LOSS [training: 0.43791555453084696 | validation: 0.4294862726080117]
	TIME [epoch: 33 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4126385615828119		[learning rate: 0.01599]
	Learning Rate: 0.0159897
	LOSS [training: 0.4126385615828119 | validation: 0.41980619067461555]
	TIME [epoch: 33 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4113544487514046		[learning rate: 0.015976]
	Learning Rate: 0.0159764
	LOSS [training: 0.4113544487514046 | validation: 0.5054970446151548]
	TIME [epoch: 33 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42007730764693385		[learning rate: 0.015963]
	Learning Rate: 0.0159632
	LOSS [training: 0.42007730764693385 | validation: 0.4177433970838169]
	TIME [epoch: 33 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3958914065527464		[learning rate: 0.01595]
	Learning Rate: 0.0159499
	LOSS [training: 0.3958914065527464 | validation: 0.3992639345689961]
	TIME [epoch: 33 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36809567490651934		[learning rate: 0.015937]
	Learning Rate: 0.0159366
	LOSS [training: 0.36809567490651934 | validation: 0.6514806422567607]
	TIME [epoch: 33 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4224585254810723		[learning rate: 0.015923]
	Learning Rate: 0.0159233
	LOSS [training: 0.4224585254810723 | validation: 0.39223758880689685]
	TIME [epoch: 33 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4408616075405361		[learning rate: 0.01591]
	Learning Rate: 0.01591
	LOSS [training: 0.4408616075405361 | validation: 0.38688576994299595]
	TIME [epoch: 33 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39990221906008233		[learning rate: 0.015897]
	Learning Rate: 0.0158966
	LOSS [training: 0.39990221906008233 | validation: 0.5440655828999662]
	TIME [epoch: 33 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43065988259652177		[learning rate: 0.015883]
	Learning Rate: 0.0158833
	LOSS [training: 0.43065988259652177 | validation: 0.40692029875731806]
	TIME [epoch: 33 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4003648587975075		[learning rate: 0.01587]
	Learning Rate: 0.0158699
	LOSS [training: 0.4003648587975075 | validation: 0.380554172075552]
	TIME [epoch: 33 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41870088972412156		[learning rate: 0.015856]
	Learning Rate: 0.0158565
	LOSS [training: 0.41870088972412156 | validation: 0.3929210115632892]
	TIME [epoch: 33 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41990831486306923		[learning rate: 0.015843]
	Learning Rate: 0.0158431
	LOSS [training: 0.41990831486306923 | validation: 0.4401390795487238]
	TIME [epoch: 33 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41874736625908904		[learning rate: 0.01583]
	Learning Rate: 0.0158297
	LOSS [training: 0.41874736625908904 | validation: 0.3975459367630716]
	TIME [epoch: 33 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35634713857860256		[learning rate: 0.015816]
	Learning Rate: 0.0158162
	LOSS [training: 0.35634713857860256 | validation: 0.5323894301288785]
	TIME [epoch: 32.9 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4643967738084908		[learning rate: 0.015803]
	Learning Rate: 0.0158028
	LOSS [training: 0.4643967738084908 | validation: 0.392114893330417]
	TIME [epoch: 32.9 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4003676290477251		[learning rate: 0.015789]
	Learning Rate: 0.0157893
	LOSS [training: 0.4003676290477251 | validation: 0.3916575744026256]
	TIME [epoch: 33 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3895904541935473		[learning rate: 0.015776]
	Learning Rate: 0.0157758
	LOSS [training: 0.3895904541935473 | validation: 0.4958295639127386]
	TIME [epoch: 33 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44454048290703996		[learning rate: 0.015762]
	Learning Rate: 0.0157623
	LOSS [training: 0.44454048290703996 | validation: 0.4082589754343557]
	TIME [epoch: 33 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3524593928294545		[learning rate: 0.015749]
	Learning Rate: 0.0157488
	LOSS [training: 0.3524593928294545 | validation: 0.43093520794806883]
	TIME [epoch: 33 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45591424831495075		[learning rate: 0.015735]
	Learning Rate: 0.0157353
	LOSS [training: 0.45591424831495075 | validation: 0.43408082944646875]
	TIME [epoch: 33 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38358002264920105		[learning rate: 0.015722]
	Learning Rate: 0.0157217
	LOSS [training: 0.38358002264920105 | validation: 0.44403624371671474]
	TIME [epoch: 33 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40407106013181915		[learning rate: 0.015708]
	Learning Rate: 0.0157082
	LOSS [training: 0.40407106013181915 | validation: 0.3844571670336837]
	TIME [epoch: 33 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41856842149350226		[learning rate: 0.015695]
	Learning Rate: 0.0156946
	LOSS [training: 0.41856842149350226 | validation: 0.3673193416447001]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_634.pth
	Model improved!!!
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36083419718333865		[learning rate: 0.015681]
	Learning Rate: 0.015681
	LOSS [training: 0.36083419718333865 | validation: 0.5102846648534853]
	TIME [epoch: 33 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4892406123217732		[learning rate: 0.015667]
	Learning Rate: 0.0156674
	LOSS [training: 0.4892406123217732 | validation: 0.422434999628881]
	TIME [epoch: 33 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3351952471117751		[learning rate: 0.015654]
	Learning Rate: 0.0156537
	LOSS [training: 0.3351952471117751 | validation: 0.5136740144478082]
	TIME [epoch: 33 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44892947251276216		[learning rate: 0.01564]
	Learning Rate: 0.0156401
	LOSS [training: 0.44892947251276216 | validation: 0.5536074978522618]
	TIME [epoch: 33 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43434383582001984		[learning rate: 0.015626]
	Learning Rate: 0.0156264
	LOSS [training: 0.43434383582001984 | validation: 0.3945112471459387]
	TIME [epoch: 33 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39591691186870875		[learning rate: 0.015613]
	Learning Rate: 0.0156128
	LOSS [training: 0.39591691186870875 | validation: 0.4309886488950928]
	TIME [epoch: 33 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3700217730890768		[learning rate: 0.015599]
	Learning Rate: 0.0155991
	LOSS [training: 0.3700217730890768 | validation: 0.4564255043336163]
	TIME [epoch: 33 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44502484758944866		[learning rate: 0.015585]
	Learning Rate: 0.0155854
	LOSS [training: 0.44502484758944866 | validation: 0.41867122357940456]
	TIME [epoch: 33 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38493286389819287		[learning rate: 0.015572]
	Learning Rate: 0.0155717
	LOSS [training: 0.38493286389819287 | validation: 0.504029869314921]
	TIME [epoch: 33 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4498458965153611		[learning rate: 0.015558]
	Learning Rate: 0.0155579
	LOSS [training: 0.4498458965153611 | validation: 0.46195842969896234]
	TIME [epoch: 33 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40176528709981063		[learning rate: 0.015544]
	Learning Rate: 0.0155442
	LOSS [training: 0.40176528709981063 | validation: 0.41641865728700667]
	TIME [epoch: 33 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3660625987469765		[learning rate: 0.01553]
	Learning Rate: 0.0155304
	LOSS [training: 0.3660625987469765 | validation: 0.5101072463322598]
	TIME [epoch: 33 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4513809937099455		[learning rate: 0.015517]
	Learning Rate: 0.0155166
	LOSS [training: 0.4513809937099455 | validation: 0.36009991933490526]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_647.pth
	Model improved!!!
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36676136117368335		[learning rate: 0.015503]
	Learning Rate: 0.0155028
	LOSS [training: 0.36676136117368335 | validation: 0.4176478131747565]
	TIME [epoch: 33 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3592512801088599		[learning rate: 0.015489]
	Learning Rate: 0.015489
	LOSS [training: 0.3592512801088599 | validation: 0.515669087956789]
	TIME [epoch: 33 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44648937191997434		[learning rate: 0.015475]
	Learning Rate: 0.0154752
	LOSS [training: 0.44648937191997434 | validation: 0.5990991311487444]
	TIME [epoch: 33 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4213772003363977		[learning rate: 0.015461]
	Learning Rate: 0.0154614
	LOSS [training: 0.4213772003363977 | validation: 0.40351405523250405]
	TIME [epoch: 33 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4004439868734958		[learning rate: 0.015448]
	Learning Rate: 0.0154475
	LOSS [training: 0.4004439868734958 | validation: 0.45186959708349694]
	TIME [epoch: 32.9 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3852617009838782		[learning rate: 0.015434]
	Learning Rate: 0.0154336
	LOSS [training: 0.3852617009838782 | validation: 0.40001256011165315]
	TIME [epoch: 33 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4327781355241106		[learning rate: 0.01542]
	Learning Rate: 0.0154198
	LOSS [training: 0.4327781355241106 | validation: 0.40254553454631903]
	TIME [epoch: 32.9 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3812942033156804		[learning rate: 0.015406]
	Learning Rate: 0.0154059
	LOSS [training: 0.3812942033156804 | validation: 0.41196236590267366]
	TIME [epoch: 32.9 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4049605989941443		[learning rate: 0.015392]
	Learning Rate: 0.0153919
	LOSS [training: 0.4049605989941443 | validation: 0.4074059705761153]
	TIME [epoch: 33 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36698415460160133		[learning rate: 0.015378]
	Learning Rate: 0.015378
	LOSS [training: 0.36698415460160133 | validation: 0.5140352052580666]
	TIME [epoch: 33 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41175234157105023		[learning rate: 0.015364]
	Learning Rate: 0.0153641
	LOSS [training: 0.41175234157105023 | validation: 0.5024262165355159]
	TIME [epoch: 32.9 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42386402804856216		[learning rate: 0.01535]
	Learning Rate: 0.0153501
	LOSS [training: 0.42386402804856216 | validation: 0.40003017407690256]
	TIME [epoch: 32.9 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38982327022417385		[learning rate: 0.015336]
	Learning Rate: 0.0153361
	LOSS [training: 0.38982327022417385 | validation: 0.4336073083482802]
	TIME [epoch: 32.8 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3857396888008395		[learning rate: 0.015322]
	Learning Rate: 0.0153222
	LOSS [training: 0.3857396888008395 | validation: 0.5460811833913151]
	TIME [epoch: 33 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.418555074644878		[learning rate: 0.015308]
	Learning Rate: 0.0153082
	LOSS [training: 0.418555074644878 | validation: 0.37728131805150955]
	TIME [epoch: 32.9 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43867705378222566		[learning rate: 0.015294]
	Learning Rate: 0.0152941
	LOSS [training: 0.43867705378222566 | validation: 0.5848647942511247]
	TIME [epoch: 32.9 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.381532829875315		[learning rate: 0.01528]
	Learning Rate: 0.0152801
	LOSS [training: 0.381532829875315 | validation: 0.35640143345503217]
	TIME [epoch: 32.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_664.pth
	Model improved!!!
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3986133861544816		[learning rate: 0.015266]
	Learning Rate: 0.0152661
	LOSS [training: 0.3986133861544816 | validation: 0.36608952653980475]
	TIME [epoch: 33 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3986215060920387		[learning rate: 0.015252]
	Learning Rate: 0.015252
	LOSS [training: 0.3986215060920387 | validation: 0.3930291297763037]
	TIME [epoch: 32.9 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4336839238223842		[learning rate: 0.015238]
	Learning Rate: 0.0152379
	LOSS [training: 0.4336839238223842 | validation: 0.4471725037693285]
	TIME [epoch: 32.9 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38727539701015823		[learning rate: 0.015224]
	Learning Rate: 0.0152238
	LOSS [training: 0.38727539701015823 | validation: 0.46142636661305486]
	TIME [epoch: 33 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39528191411894903		[learning rate: 0.01521]
	Learning Rate: 0.0152097
	LOSS [training: 0.39528191411894903 | validation: 0.34898010810761104]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38648748166621993		[learning rate: 0.015196]
	Learning Rate: 0.0151956
	LOSS [training: 0.38648748166621993 | validation: 0.3867602801412834]
	TIME [epoch: 33 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3534176569301163		[learning rate: 0.015182]
	Learning Rate: 0.0151815
	LOSS [training: 0.3534176569301163 | validation: 0.4648763651223152]
	TIME [epoch: 32.9 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44076055187854507		[learning rate: 0.015167]
	Learning Rate: 0.0151674
	LOSS [training: 0.44076055187854507 | validation: 0.3856131878779461]
	TIME [epoch: 33 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3732115894400662		[learning rate: 0.015153]
	Learning Rate: 0.0151532
	LOSS [training: 0.3732115894400662 | validation: 0.4388534183404067]
	TIME [epoch: 33 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39881833391520743		[learning rate: 0.015139]
	Learning Rate: 0.015139
	LOSS [training: 0.39881833391520743 | validation: 0.34986962833304724]
	TIME [epoch: 32.9 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.378868868773927		[learning rate: 0.015125]
	Learning Rate: 0.0151248
	LOSS [training: 0.378868868773927 | validation: 0.389909616789942]
	TIME [epoch: 33 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40433093322053404		[learning rate: 0.015111]
	Learning Rate: 0.0151106
	LOSS [training: 0.40433093322053404 | validation: 0.5168030402559193]
	TIME [epoch: 32.9 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3571486951958167		[learning rate: 0.015096]
	Learning Rate: 0.0150964
	LOSS [training: 0.3571486951958167 | validation: 0.5102083394609155]
	TIME [epoch: 32.9 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4977427105395671		[learning rate: 0.015082]
	Learning Rate: 0.0150822
	LOSS [training: 0.4977427105395671 | validation: 0.49231382303756377]
	TIME [epoch: 32.9 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36042834284650566		[learning rate: 0.015068]
	Learning Rate: 0.015068
	LOSS [training: 0.36042834284650566 | validation: 0.4532262714853541]
	TIME [epoch: 33 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38703263572208935		[learning rate: 0.015054]
	Learning Rate: 0.0150537
	LOSS [training: 0.38703263572208935 | validation: 0.4415325138927937]
	TIME [epoch: 33 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3991602264905975		[learning rate: 0.015039]
	Learning Rate: 0.0150394
	LOSS [training: 0.3991602264905975 | validation: 0.3514229663867276]
	TIME [epoch: 33 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4170790294703896		[learning rate: 0.015025]
	Learning Rate: 0.0150251
	LOSS [training: 0.4170790294703896 | validation: 0.4427891703775341]
	TIME [epoch: 32.9 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3702939626002437		[learning rate: 0.015011]
	Learning Rate: 0.0150108
	LOSS [training: 0.3702939626002437 | validation: 0.4032287980776501]
	TIME [epoch: 32.9 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4071502835342534		[learning rate: 0.014997]
	Learning Rate: 0.0149965
	LOSS [training: 0.4071502835342534 | validation: 0.4349563166348501]
	TIME [epoch: 33 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3708814291981365		[learning rate: 0.014982]
	Learning Rate: 0.0149822
	LOSS [training: 0.3708814291981365 | validation: 0.4482426367564605]
	TIME [epoch: 33 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3834823151700301		[learning rate: 0.014968]
	Learning Rate: 0.0149679
	LOSS [training: 0.3834823151700301 | validation: 0.4013024797121908]
	TIME [epoch: 32.9 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38328778254405443		[learning rate: 0.014954]
	Learning Rate: 0.0149535
	LOSS [training: 0.38328778254405443 | validation: 0.45936812547563743]
	TIME [epoch: 33 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41467769146226247		[learning rate: 0.014939]
	Learning Rate: 0.0149392
	LOSS [training: 0.41467769146226247 | validation: 0.4177058578161279]
	TIME [epoch: 33 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3863959353524965		[learning rate: 0.014925]
	Learning Rate: 0.0149248
	LOSS [training: 0.3863959353524965 | validation: 0.45000466682874124]
	TIME [epoch: 32.9 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43298802112365936		[learning rate: 0.01491]
	Learning Rate: 0.0149104
	LOSS [training: 0.43298802112365936 | validation: 0.38803768570151065]
	TIME [epoch: 33 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3373468492625535		[learning rate: 0.014896]
	Learning Rate: 0.014896
	LOSS [training: 0.3373468492625535 | validation: 0.359229647001455]
	TIME [epoch: 33 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4228180132324727		[learning rate: 0.014882]
	Learning Rate: 0.0148816
	LOSS [training: 0.4228180132324727 | validation: 0.44385284414845627]
	TIME [epoch: 32.9 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38322583129596494		[learning rate: 0.014867]
	Learning Rate: 0.0148671
	LOSS [training: 0.38322583129596494 | validation: 0.4671330160274507]
	TIME [epoch: 33 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3626439376232208		[learning rate: 0.014853]
	Learning Rate: 0.0148527
	LOSS [training: 0.3626439376232208 | validation: 0.33631893195640283]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4264507087610988		[learning rate: 0.014838]
	Learning Rate: 0.0148382
	LOSS [training: 0.4264507087610988 | validation: 0.4700118136589709]
	TIME [epoch: 33 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3830866147755893		[learning rate: 0.014824]
	Learning Rate: 0.0148237
	LOSS [training: 0.3830866147755893 | validation: 0.3818963901803266]
	TIME [epoch: 33 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3749867859352168		[learning rate: 0.014809]
	Learning Rate: 0.0148093
	LOSS [training: 0.3749867859352168 | validation: 0.43118706670468]
	TIME [epoch: 33 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39696253874903115		[learning rate: 0.014795]
	Learning Rate: 0.0147948
	LOSS [training: 0.39696253874903115 | validation: 0.34440090739609863]
	TIME [epoch: 33 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40946519200260506		[learning rate: 0.01478]
	Learning Rate: 0.0147803
	LOSS [training: 0.40946519200260506 | validation: 0.3769528318814416]
	TIME [epoch: 32.9 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4102772691773512		[learning rate: 0.014766]
	Learning Rate: 0.0147657
	LOSS [training: 0.4102772691773512 | validation: 0.42185499814306926]
	TIME [epoch: 32.9 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.360306794873306		[learning rate: 0.014751]
	Learning Rate: 0.0147512
	LOSS [training: 0.360306794873306 | validation: 0.3808062724919945]
	TIME [epoch: 32.9 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38010785393285224		[learning rate: 0.014737]
	Learning Rate: 0.0147366
	LOSS [training: 0.38010785393285224 | validation: 0.3799828093150346]
	TIME [epoch: 32.9 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3866709576625712		[learning rate: 0.014722]
	Learning Rate: 0.0147221
	LOSS [training: 0.3866709576625712 | validation: 0.42553387643884955]
	TIME [epoch: 32.9 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42292622535348207		[learning rate: 0.014707]
	Learning Rate: 0.0147075
	LOSS [training: 0.42292622535348207 | validation: 0.3927896650541368]
	TIME [epoch: 33 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3771795545504408		[learning rate: 0.014693]
	Learning Rate: 0.0146929
	LOSS [training: 0.3771795545504408 | validation: 0.43622420069972917]
	TIME [epoch: 33 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36149026078402774		[learning rate: 0.014678]
	Learning Rate: 0.0146783
	LOSS [training: 0.36149026078402774 | validation: 0.37312417283753296]
	TIME [epoch: 33 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4182444864776787		[learning rate: 0.014664]
	Learning Rate: 0.0146637
	LOSS [training: 0.4182444864776787 | validation: 0.3909129437100821]
	TIME [epoch: 32.9 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3225640734361388		[learning rate: 0.014649]
	Learning Rate: 0.0146491
	LOSS [training: 0.3225640734361388 | validation: 0.4407530278595796]
	TIME [epoch: 33 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4847654899089333		[learning rate: 0.014634]
	Learning Rate: 0.0146344
	LOSS [training: 0.4847654899089333 | validation: 0.4244637461088634]
	TIME [epoch: 33 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3674938031785802		[learning rate: 0.01462]
	Learning Rate: 0.0146198
	LOSS [training: 0.3674938031785802 | validation: 0.38077660595756135]
	TIME [epoch: 33 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3634539594704245		[learning rate: 0.014605]
	Learning Rate: 0.0146051
	LOSS [training: 0.3634539594704245 | validation: 0.3832869298722504]
	TIME [epoch: 33 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.403256698172969		[learning rate: 0.01459]
	Learning Rate: 0.0145904
	LOSS [training: 0.403256698172969 | validation: 0.40264599395878714]
	TIME [epoch: 32.9 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37749952177057605		[learning rate: 0.014576]
	Learning Rate: 0.0145757
	LOSS [training: 0.37749952177057605 | validation: 0.5147673605569736]
	TIME [epoch: 33 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4038477380671138		[learning rate: 0.014561]
	Learning Rate: 0.014561
	LOSS [training: 0.4038477380671138 | validation: 0.37103232098346]
	TIME [epoch: 33 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36003556284524213		[learning rate: 0.014546]
	Learning Rate: 0.0145463
	LOSS [training: 0.36003556284524213 | validation: 0.5027819586778086]
	TIME [epoch: 33 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4076380870654117		[learning rate: 0.014532]
	Learning Rate: 0.0145316
	LOSS [training: 0.4076380870654117 | validation: 0.3612579964892243]
	TIME [epoch: 32.9 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38309071340293316		[learning rate: 0.014517]
	Learning Rate: 0.0145168
	LOSS [training: 0.38309071340293316 | validation: 0.4800378025008491]
	TIME [epoch: 33 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3540149294145829		[learning rate: 0.014502]
	Learning Rate: 0.0145021
	LOSS [training: 0.3540149294145829 | validation: 0.4038753487600697]
	TIME [epoch: 33 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3747458256292824		[learning rate: 0.014487]
	Learning Rate: 0.0144873
	LOSS [training: 0.3747458256292824 | validation: 0.38809327158179713]
	TIME [epoch: 32.9 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4509850166306011		[learning rate: 0.014473]
	Learning Rate: 0.0144726
	LOSS [training: 0.4509850166306011 | validation: 0.43508623716015693]
	TIME [epoch: 33 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3386401507125519		[learning rate: 0.014458]
	Learning Rate: 0.0144578
	LOSS [training: 0.3386401507125519 | validation: 0.3531465676505151]
	TIME [epoch: 32.9 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41751546623641955		[learning rate: 0.014443]
	Learning Rate: 0.014443
	LOSS [training: 0.41751546623641955 | validation: 0.4670170102620512]
	TIME [epoch: 33 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3796297995073298		[learning rate: 0.014428]
	Learning Rate: 0.0144281
	LOSS [training: 0.3796297995073298 | validation: 0.47164587999733176]
	TIME [epoch: 33 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37000994024621203		[learning rate: 0.014413]
	Learning Rate: 0.0144133
	LOSS [training: 0.37000994024621203 | validation: 0.5041816084649695]
	TIME [epoch: 32.9 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4064259720642966		[learning rate: 0.014398]
	Learning Rate: 0.0143985
	LOSS [training: 0.4064259720642966 | validation: 0.37365121179500843]
	TIME [epoch: 33 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37610130318087465		[learning rate: 0.014384]
	Learning Rate: 0.0143836
	LOSS [training: 0.37610130318087465 | validation: 0.3598139323046767]
	TIME [epoch: 33 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37769874618741545		[learning rate: 0.014369]
	Learning Rate: 0.0143688
	LOSS [training: 0.37769874618741545 | validation: 0.503525232569399]
	TIME [epoch: 33 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39216116624324293		[learning rate: 0.014354]
	Learning Rate: 0.0143539
	LOSS [training: 0.39216116624324293 | validation: 0.40148977403031855]
	TIME [epoch: 33 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36900062676962453		[learning rate: 0.014339]
	Learning Rate: 0.014339
	LOSS [training: 0.36900062676962453 | validation: 0.33802112129451634]
	TIME [epoch: 33 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40530323620940434		[learning rate: 0.014324]
	Learning Rate: 0.0143241
	LOSS [training: 0.40530323620940434 | validation: 0.4096151783067632]
	TIME [epoch: 33 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3674361849539119		[learning rate: 0.014309]
	Learning Rate: 0.0143092
	LOSS [training: 0.3674361849539119 | validation: 0.4324354915890656]
	TIME [epoch: 33 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3618045129810541		[learning rate: 0.014294]
	Learning Rate: 0.0142943
	LOSS [training: 0.3618045129810541 | validation: 0.4232014274094614]
	TIME [epoch: 33 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4644024285071353		[learning rate: 0.014279]
	Learning Rate: 0.0142793
	LOSS [training: 0.4644024285071353 | validation: 0.4214007226382761]
	TIME [epoch: 32.9 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3415543085863103		[learning rate: 0.014264]
	Learning Rate: 0.0142644
	LOSS [training: 0.3415543085863103 | validation: 0.4487995745224177]
	TIME [epoch: 33 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3612530286612174		[learning rate: 0.014249]
	Learning Rate: 0.0142494
	LOSS [training: 0.3612530286612174 | validation: 0.45468736892467804]
	TIME [epoch: 33 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3957566126397805		[learning rate: 0.014234]
	Learning Rate: 0.0142345
	LOSS [training: 0.3957566126397805 | validation: 0.46068061245027736]
	TIME [epoch: 33 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35830830887023785		[learning rate: 0.014219]
	Learning Rate: 0.0142195
	LOSS [training: 0.35830830887023785 | validation: 0.35513363017475874]
	TIME [epoch: 32.9 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3671636272478875		[learning rate: 0.014205]
	Learning Rate: 0.0142045
	LOSS [training: 0.3671636272478875 | validation: 0.39377586982915885]
	TIME [epoch: 33 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38315602989610353		[learning rate: 0.01419]
	Learning Rate: 0.0141895
	LOSS [training: 0.38315602989610353 | validation: 0.3519284116215006]
	TIME [epoch: 33 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3981006706701542		[learning rate: 0.014174]
	Learning Rate: 0.0141745
	LOSS [training: 0.3981006706701542 | validation: 0.41774971112951886]
	TIME [epoch: 33 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37087908567174166		[learning rate: 0.014159]
	Learning Rate: 0.0141595
	LOSS [training: 0.37087908567174166 | validation: 0.49694827323856433]
	TIME [epoch: 33 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4040075339325413		[learning rate: 0.014144]
	Learning Rate: 0.0141444
	LOSS [training: 0.4040075339325413 | validation: 0.4111745014368769]
	TIME [epoch: 33 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3412970707941627		[learning rate: 0.014129]
	Learning Rate: 0.0141294
	LOSS [training: 0.3412970707941627 | validation: 0.41029084518751113]
	TIME [epoch: 32.9 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39813068084040215		[learning rate: 0.014114]
	Learning Rate: 0.0141143
	LOSS [training: 0.39813068084040215 | validation: 0.35031608226028876]
	TIME [epoch: 33 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36761878092592276		[learning rate: 0.014099]
	Learning Rate: 0.0140992
	LOSS [training: 0.36761878092592276 | validation: 0.3852157956993213]
	TIME [epoch: 33 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.321815596068927		[learning rate: 0.014084]
	Learning Rate: 0.0140842
	LOSS [training: 0.321815596068927 | validation: 0.44849927587856714]
	TIME [epoch: 33 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5089572506639249		[learning rate: 0.014069]
	Learning Rate: 0.0140691
	LOSS [training: 0.5089572506639249 | validation: 0.3825147905986625]
	TIME [epoch: 32.9 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3298180733799563		[learning rate: 0.014054]
	Learning Rate: 0.014054
	LOSS [training: 0.3298180733799563 | validation: 0.3507489133795605]
	TIME [epoch: 33 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37352474194930135		[learning rate: 0.014039]
	Learning Rate: 0.0140389
	LOSS [training: 0.37352474194930135 | validation: 0.44349407041989]
	TIME [epoch: 32.9 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4050833708441596		[learning rate: 0.014024]
	Learning Rate: 0.0140237
	LOSS [training: 0.4050833708441596 | validation: 0.46824566102531395]
	TIME [epoch: 32.9 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40154915257309354		[learning rate: 0.014009]
	Learning Rate: 0.0140086
	LOSS [training: 0.40154915257309354 | validation: 0.36144756666810474]
	TIME [epoch: 32.9 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37329662316237966		[learning rate: 0.013993]
	Learning Rate: 0.0139934
	LOSS [training: 0.37329662316237966 | validation: 0.483290216497679]
	TIME [epoch: 32.9 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3555811871969302		[learning rate: 0.013978]
	Learning Rate: 0.0139783
	LOSS [training: 0.3555811871969302 | validation: 0.4193005324386929]
	TIME [epoch: 33 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.367059516530343		[learning rate: 0.013963]
	Learning Rate: 0.0139631
	LOSS [training: 0.367059516530343 | validation: 0.36798655804747815]
	TIME [epoch: 33 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40426466300909897		[learning rate: 0.013948]
	Learning Rate: 0.0139479
	LOSS [training: 0.40426466300909897 | validation: 0.3455653335716445]
	TIME [epoch: 33 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34456476248871115		[learning rate: 0.013933]
	Learning Rate: 0.0139327
	LOSS [training: 0.34456476248871115 | validation: 0.3440945787981397]
	TIME [epoch: 32.9 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37698382197752667		[learning rate: 0.013918]
	Learning Rate: 0.0139175
	LOSS [training: 0.37698382197752667 | validation: 0.46489725776589286]
	TIME [epoch: 33 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3870978415302674		[learning rate: 0.013902]
	Learning Rate: 0.0139023
	LOSS [training: 0.3870978415302674 | validation: 0.3750494150344595]
	TIME [epoch: 33 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3467859188432791		[learning rate: 0.013887]
	Learning Rate: 0.0138871
	LOSS [training: 0.3467859188432791 | validation: 0.38802040653812186]
	TIME [epoch: 33 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4558870750533113		[learning rate: 0.013872]
	Learning Rate: 0.0138719
	LOSS [training: 0.4558870750533113 | validation: 0.40794901200908557]
	TIME [epoch: 32.9 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34435599994956967		[learning rate: 0.013857]
	Learning Rate: 0.0138566
	LOSS [training: 0.34435599994956967 | validation: 0.3585347821916428]
	TIME [epoch: 32.9 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3755594903556984		[learning rate: 0.013841]
	Learning Rate: 0.0138414
	LOSS [training: 0.3755594903556984 | validation: 0.3616102530807997]
	TIME [epoch: 33 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3658419287211139		[learning rate: 0.013826]
	Learning Rate: 0.0138261
	LOSS [training: 0.3658419287211139 | validation: 0.3346725894331125]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_763.pth
	Model improved!!!
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35658819556199206		[learning rate: 0.013811]
	Learning Rate: 0.0138108
	LOSS [training: 0.35658819556199206 | validation: 0.36334532879281994]
	TIME [epoch: 33 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40137662791499507		[learning rate: 0.013796]
	Learning Rate: 0.0137955
	LOSS [training: 0.40137662791499507 | validation: 0.43497656942834695]
	TIME [epoch: 33 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3474895025873995		[learning rate: 0.01378]
	Learning Rate: 0.0137802
	LOSS [training: 0.3474895025873995 | validation: 0.3968988443530631]
	TIME [epoch: 33 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.387321179624099		[learning rate: 0.013765]
	Learning Rate: 0.0137649
	LOSS [training: 0.387321179624099 | validation: 0.4005966121825454]
	TIME [epoch: 33 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3630781136383133		[learning rate: 0.01375]
	Learning Rate: 0.0137496
	LOSS [training: 0.3630781136383133 | validation: 0.36036193956479834]
	TIME [epoch: 33 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38415164614007086		[learning rate: 0.013734]
	Learning Rate: 0.0137343
	LOSS [training: 0.38415164614007086 | validation: 0.4407134806520943]
	TIME [epoch: 33 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3448110398713026		[learning rate: 0.013719]
	Learning Rate: 0.013719
	LOSS [training: 0.3448110398713026 | validation: 0.39634939744149444]
	TIME [epoch: 33 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39713641060730887		[learning rate: 0.013704]
	Learning Rate: 0.0137036
	LOSS [training: 0.39713641060730887 | validation: 0.3587293204056774]
	TIME [epoch: 33 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3171885019676676		[learning rate: 0.013688]
	Learning Rate: 0.0136882
	LOSS [training: 0.3171885019676676 | validation: 0.4697221548644871]
	TIME [epoch: 32.9 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38270453791328196		[learning rate: 0.013673]
	Learning Rate: 0.0136729
	LOSS [training: 0.38270453791328196 | validation: 0.3792340789123062]
	TIME [epoch: 33 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40211742157539576		[learning rate: 0.013657]
	Learning Rate: 0.0136575
	LOSS [training: 0.40211742157539576 | validation: 0.39329846251095757]
	TIME [epoch: 32.9 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37051239396637503		[learning rate: 0.013642]
	Learning Rate: 0.0136421
	LOSS [training: 0.37051239396637503 | validation: 0.43308082129469694]
	TIME [epoch: 33 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36902138970789966		[learning rate: 0.013627]
	Learning Rate: 0.0136267
	LOSS [training: 0.36902138970789966 | validation: 0.4054119616162337]
	TIME [epoch: 33 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34977896464344077		[learning rate: 0.013611]
	Learning Rate: 0.0136113
	LOSS [training: 0.34977896464344077 | validation: 0.3827630262258599]
	TIME [epoch: 32.9 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3942770811252091		[learning rate: 0.013596]
	Learning Rate: 0.0135959
	LOSS [training: 0.3942770811252091 | validation: 0.3726621296054108]
	TIME [epoch: 33 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33689331032676784		[learning rate: 0.01358]
	Learning Rate: 0.0135805
	LOSS [training: 0.33689331032676784 | validation: 0.39600337800629787]
	TIME [epoch: 33 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40359379019257835		[learning rate: 0.013565]
	Learning Rate: 0.013565
	LOSS [training: 0.40359379019257835 | validation: 0.4745105358952022]
	TIME [epoch: 33 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3777700681816941		[learning rate: 0.01355]
	Learning Rate: 0.0135496
	LOSS [training: 0.3777700681816941 | validation: 0.3601663300953296]
	TIME [epoch: 33 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3550313860475258		[learning rate: 0.013534]
	Learning Rate: 0.0135341
	LOSS [training: 0.3550313860475258 | validation: 0.3810944531511952]
	TIME [epoch: 32.9 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36629958048877037		[learning rate: 0.013519]
	Learning Rate: 0.0135186
	LOSS [training: 0.36629958048877037 | validation: 0.44201362015072476]
	TIME [epoch: 33 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3540289859682483		[learning rate: 0.013503]
	Learning Rate: 0.0135032
	LOSS [training: 0.3540289859682483 | validation: 0.36125098863286287]
	TIME [epoch: 33 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3796607955116532		[learning rate: 0.013488]
	Learning Rate: 0.0134877
	LOSS [training: 0.3796607955116532 | validation: 0.36317159318154435]
	TIME [epoch: 33 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3435727048659513		[learning rate: 0.013472]
	Learning Rate: 0.0134722
	LOSS [training: 0.3435727048659513 | validation: 0.406731007958985]
	TIME [epoch: 33 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39770777965805504		[learning rate: 0.013457]
	Learning Rate: 0.0134567
	LOSS [training: 0.39770777965805504 | validation: 0.39723003712925464]
	TIME [epoch: 33 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.347426883052374		[learning rate: 0.013441]
	Learning Rate: 0.0134412
	LOSS [training: 0.347426883052374 | validation: 0.4372063526610034]
	TIME [epoch: 33 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37654046858328305		[learning rate: 0.013426]
	Learning Rate: 0.0134256
	LOSS [training: 0.37654046858328305 | validation: 0.3403050214898952]
	TIME [epoch: 33 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3629956532284187		[learning rate: 0.01341]
	Learning Rate: 0.0134101
	LOSS [training: 0.3629956532284187 | validation: 0.4258468605814577]
	TIME [epoch: 33 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3655245481097122		[learning rate: 0.013395]
	Learning Rate: 0.0133946
	LOSS [training: 0.3655245481097122 | validation: 0.4332920362213196]
	TIME [epoch: 33 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37403898924197254		[learning rate: 0.013379]
	Learning Rate: 0.013379
	LOSS [training: 0.37403898924197254 | validation: 0.35263967308608646]
	TIME [epoch: 33 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3317430866345061		[learning rate: 0.013363]
	Learning Rate: 0.0133635
	LOSS [training: 0.3317430866345061 | validation: 0.4028758753616299]
	TIME [epoch: 33 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37324374792434906		[learning rate: 0.013348]
	Learning Rate: 0.0133479
	LOSS [training: 0.37324374792434906 | validation: 0.3763485813459425]
	TIME [epoch: 33 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3970074937328851		[learning rate: 0.013332]
	Learning Rate: 0.0133323
	LOSS [training: 0.3970074937328851 | validation: 0.5253985459364299]
	TIME [epoch: 33 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38490479909902886		[learning rate: 0.013317]
	Learning Rate: 0.0133167
	LOSS [training: 0.38490479909902886 | validation: 0.3307034383095363]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_796.pth
	Model improved!!!
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3611996334073354		[learning rate: 0.013301]
	Learning Rate: 0.0133011
	LOSS [training: 0.3611996334073354 | validation: 0.5054448691499143]
	TIME [epoch: 33 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.395116752760701		[learning rate: 0.013286]
	Learning Rate: 0.0132855
	LOSS [training: 0.395116752760701 | validation: 0.395710633573983]
	TIME [epoch: 33 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3220665283500458		[learning rate: 0.01327]
	Learning Rate: 0.0132699
	LOSS [training: 0.3220665283500458 | validation: 0.36289794954745747]
	TIME [epoch: 33 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3377449507424464		[learning rate: 0.013254]
	Learning Rate: 0.0132543
	LOSS [training: 0.3377449507424464 | validation: 0.4558879404205323]
	TIME [epoch: 33 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48310272338474813		[learning rate: 0.013239]
	Learning Rate: 0.0132386
	LOSS [training: 0.48310272338474813 | validation: 0.34324708131662773]
	TIME [epoch: 33 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32818798492587686		[learning rate: 0.013223]
	Learning Rate: 0.013223
	LOSS [training: 0.32818798492587686 | validation: 0.45544621439484434]
	TIME [epoch: 33 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34566999059637554		[learning rate: 0.013207]
	Learning Rate: 0.0132074
	LOSS [training: 0.34566999059637554 | validation: 0.35046675121362547]
	TIME [epoch: 33 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3589895171920369		[learning rate: 0.013192]
	Learning Rate: 0.0131917
	LOSS [training: 0.3589895171920369 | validation: 0.42222259616946944]
	TIME [epoch: 33 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3652179644514604		[learning rate: 0.013176]
	Learning Rate: 0.013176
	LOSS [training: 0.3652179644514604 | validation: 0.3379469968865958]
	TIME [epoch: 33 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4275305095834503		[learning rate: 0.01316]
	Learning Rate: 0.0131603
	LOSS [training: 0.4275305095834503 | validation: 0.38389757911200595]
	TIME [epoch: 33 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3213374873418402		[learning rate: 0.013145]
	Learning Rate: 0.0131447
	LOSS [training: 0.3213374873418402 | validation: 0.3687932353310126]
	TIME [epoch: 33 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4044028894197612		[learning rate: 0.013129]
	Learning Rate: 0.013129
	LOSS [training: 0.4044028894197612 | validation: 0.3605454151864528]
	TIME [epoch: 33 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3439762550278735		[learning rate: 0.013113]
	Learning Rate: 0.0131133
	LOSS [training: 0.3439762550278735 | validation: 0.40297698594811976]
	TIME [epoch: 33 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3834380569028704		[learning rate: 0.013098]
	Learning Rate: 0.0130976
	LOSS [training: 0.3834380569028704 | validation: 0.3249693513567267]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_810.pth
	Model improved!!!
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34400140402650436		[learning rate: 0.013082]
	Learning Rate: 0.0130818
	LOSS [training: 0.34400140402650436 | validation: 0.3980281473685683]
	TIME [epoch: 33 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42618920345870487		[learning rate: 0.013066]
	Learning Rate: 0.0130661
	LOSS [training: 0.42618920345870487 | validation: 0.38119902964062446]
	TIME [epoch: 33 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34267127791757424		[learning rate: 0.01305]
	Learning Rate: 0.0130504
	LOSS [training: 0.34267127791757424 | validation: 0.3436274178876807]
	TIME [epoch: 33 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36088610749941685		[learning rate: 0.013035]
	Learning Rate: 0.0130346
	LOSS [training: 0.36088610749941685 | validation: 0.3367385536852797]
	TIME [epoch: 33 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34851023584151136		[learning rate: 0.013019]
	Learning Rate: 0.0130189
	LOSS [training: 0.34851023584151136 | validation: 0.3529673478392976]
	TIME [epoch: 33 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3333510098213439		[learning rate: 0.013003]
	Learning Rate: 0.0130031
	LOSS [training: 0.3333510098213439 | validation: 0.35978009419071433]
	TIME [epoch: 33 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36695190739684397		[learning rate: 0.012987]
	Learning Rate: 0.0129873
	LOSS [training: 0.36695190739684397 | validation: 0.4249043394516704]
	TIME [epoch: 33 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39038824631787084		[learning rate: 0.012972]
	Learning Rate: 0.0129716
	LOSS [training: 0.39038824631787084 | validation: 0.3477066898428151]
	TIME [epoch: 33 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3268589266752252		[learning rate: 0.012956]
	Learning Rate: 0.0129558
	LOSS [training: 0.3268589266752252 | validation: 0.3427717513777927]
	TIME [epoch: 33 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35418936103199966		[learning rate: 0.01294]
	Learning Rate: 0.01294
	LOSS [training: 0.35418936103199966 | validation: 0.44802924811511635]
	TIME [epoch: 33 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42766662243966935		[learning rate: 0.012924]
	Learning Rate: 0.0129242
	LOSS [training: 0.42766662243966935 | validation: 0.38364948186669057]
	TIME [epoch: 33 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3112838778015129		[learning rate: 0.012908]
	Learning Rate: 0.0129084
	LOSS [training: 0.3112838778015129 | validation: 0.40804958791844376]
	TIME [epoch: 33 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40215277028179197		[learning rate: 0.012893]
	Learning Rate: 0.0128926
	LOSS [training: 0.40215277028179197 | validation: 0.3761261059182511]
	TIME [epoch: 32.9 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3486441884156088		[learning rate: 0.012877]
	Learning Rate: 0.0128767
	LOSS [training: 0.3486441884156088 | validation: 0.4148203252984105]
	TIME [epoch: 33 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3561172698557935		[learning rate: 0.012861]
	Learning Rate: 0.0128609
	LOSS [training: 0.3561172698557935 | validation: 0.37260971387900876]
	TIME [epoch: 33 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39107951973985416		[learning rate: 0.012845]
	Learning Rate: 0.0128451
	LOSS [training: 0.39107951973985416 | validation: 0.3441956461713635]
	TIME [epoch: 33 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3271618395501025		[learning rate: 0.012829]
	Learning Rate: 0.0128292
	LOSS [training: 0.3271618395501025 | validation: 0.4073206420105916]
	TIME [epoch: 33 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37929707061266726		[learning rate: 0.012813]
	Learning Rate: 0.0128133
	LOSS [training: 0.37929707061266726 | validation: 0.37105231296383495]
	TIME [epoch: 33 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33740736491451534		[learning rate: 0.012797]
	Learning Rate: 0.0127975
	LOSS [training: 0.33740736491451534 | validation: 0.38011266606017735]
	TIME [epoch: 33 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3303680854707087		[learning rate: 0.012782]
	Learning Rate: 0.0127816
	LOSS [training: 0.3303680854707087 | validation: 0.36386744507405105]
	TIME [epoch: 33 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4571503406489775		[learning rate: 0.012766]
	Learning Rate: 0.0127657
	LOSS [training: 0.4571503406489775 | validation: 0.3559348601455502]
	TIME [epoch: 33 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3159263019711891		[learning rate: 0.01275]
	Learning Rate: 0.0127498
	LOSS [training: 0.3159263019711891 | validation: 0.3558883662835859]
	TIME [epoch: 33 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39412540944842617		[learning rate: 0.012734]
	Learning Rate: 0.0127339
	LOSS [training: 0.39412540944842617 | validation: 0.3537184123181428]
	TIME [epoch: 33 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33325594709387113		[learning rate: 0.012718]
	Learning Rate: 0.012718
	LOSS [training: 0.33325594709387113 | validation: 0.361691522789383]
	TIME [epoch: 33 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37102461659514663		[learning rate: 0.012702]
	Learning Rate: 0.0127021
	LOSS [training: 0.37102461659514663 | validation: 0.33815085795537175]
	TIME [epoch: 33 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34710552794304456		[learning rate: 0.012686]
	Learning Rate: 0.0126862
	LOSS [training: 0.34710552794304456 | validation: 0.35413664924593785]
	TIME [epoch: 33 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3574284538817325		[learning rate: 0.01267]
	Learning Rate: 0.0126703
	LOSS [training: 0.3574284538817325 | validation: 0.32663029200082155]
	TIME [epoch: 33 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31645352651939895		[learning rate: 0.012654]
	Learning Rate: 0.0126544
	LOSS [training: 0.31645352651939895 | validation: 0.4517106894876781]
	TIME [epoch: 33 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4575348280566779		[learning rate: 0.012638]
	Learning Rate: 0.0126384
	LOSS [training: 0.4575348280566779 | validation: 0.35823857905591805]
	TIME [epoch: 33 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3046931125254969		[learning rate: 0.012622]
	Learning Rate: 0.0126225
	LOSS [training: 0.3046931125254969 | validation: 0.33443234277165323]
	TIME [epoch: 33 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3698370960300967		[learning rate: 0.012607]
	Learning Rate: 0.0126065
	LOSS [training: 0.3698370960300967 | validation: 0.3737976439269828]
	TIME [epoch: 33 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33175335087045577		[learning rate: 0.012591]
	Learning Rate: 0.0125906
	LOSS [training: 0.33175335087045577 | validation: 0.3951525648511138]
	TIME [epoch: 33 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3545673906439118		[learning rate: 0.012575]
	Learning Rate: 0.0125746
	LOSS [training: 0.3545673906439118 | validation: 0.3182284553954927]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_843.pth
	Model improved!!!
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3460344475627961		[learning rate: 0.012559]
	Learning Rate: 0.0125586
	LOSS [training: 0.3460344475627961 | validation: 0.3985887246835383]
	TIME [epoch: 33 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38352243132815594		[learning rate: 0.012543]
	Learning Rate: 0.0125426
	LOSS [training: 0.38352243132815594 | validation: 0.3423807271552856]
	TIME [epoch: 33 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3626046034933361		[learning rate: 0.012527]
	Learning Rate: 0.0125267
	LOSS [training: 0.3626046034933361 | validation: 0.38370575313650734]
	TIME [epoch: 33 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3463857717084235		[learning rate: 0.012511]
	Learning Rate: 0.0125107
	LOSS [training: 0.3463857717084235 | validation: 0.3585885978996737]
	TIME [epoch: 33 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34721851306655616		[learning rate: 0.012495]
	Learning Rate: 0.0124947
	LOSS [training: 0.34721851306655616 | validation: 0.3529676723821017]
	TIME [epoch: 32.9 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39266404010923556		[learning rate: 0.012479]
	Learning Rate: 0.0124786
	LOSS [training: 0.39266404010923556 | validation: 0.3600067737751058]
	TIME [epoch: 33 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3131676036978078		[learning rate: 0.012463]
	Learning Rate: 0.0124626
	LOSS [training: 0.3131676036978078 | validation: 0.40308251856219424]
	TIME [epoch: 33 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3744334977541566		[learning rate: 0.012447]
	Learning Rate: 0.0124466
	LOSS [training: 0.3744334977541566 | validation: 0.3821399343649089]
	TIME [epoch: 33 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3371671971073363		[learning rate: 0.012431]
	Learning Rate: 0.0124306
	LOSS [training: 0.3371671971073363 | validation: 0.397771574921441]
	TIME [epoch: 33 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3494829902451103		[learning rate: 0.012415]
	Learning Rate: 0.0124145
	LOSS [training: 0.3494829902451103 | validation: 0.3726973514927959]
	TIME [epoch: 33 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38741667361219534		[learning rate: 0.012399]
	Learning Rate: 0.0123985
	LOSS [training: 0.38741667361219534 | validation: 0.38715690375945405]
	TIME [epoch: 33 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33416116826443043		[learning rate: 0.012382]
	Learning Rate: 0.0123825
	LOSS [training: 0.33416116826443043 | validation: 0.42317031302195746]
	TIME [epoch: 33 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3674411491802534		[learning rate: 0.012366]
	Learning Rate: 0.0123664
	LOSS [training: 0.3674411491802534 | validation: 0.35280546470037427]
	TIME [epoch: 33 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3444584539361212		[learning rate: 0.01235]
	Learning Rate: 0.0123503
	LOSS [training: 0.3444584539361212 | validation: 0.501972518480569]
	TIME [epoch: 33 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3887390622046849		[learning rate: 0.012334]
	Learning Rate: 0.0123343
	LOSS [training: 0.3887390622046849 | validation: 0.3538678731974532]
	TIME [epoch: 33 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31993963234464123		[learning rate: 0.012318]
	Learning Rate: 0.0123182
	LOSS [training: 0.31993963234464123 | validation: 0.4273892559149922]
	TIME [epoch: 33 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3492349243468561		[learning rate: 0.012302]
	Learning Rate: 0.0123021
	LOSS [training: 0.3492349243468561 | validation: 0.3843021813703056]
	TIME [epoch: 33 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34214019233078924		[learning rate: 0.012286]
	Learning Rate: 0.012286
	LOSS [training: 0.34214019233078924 | validation: 0.4138994697466957]
	TIME [epoch: 33 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3859046756491383		[learning rate: 0.01227]
	Learning Rate: 0.0122699
	LOSS [training: 0.3859046756491383 | validation: 0.39193963683642913]
	TIME [epoch: 33 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2975505305883244		[learning rate: 0.012254]
	Learning Rate: 0.0122538
	LOSS [training: 0.2975505305883244 | validation: 0.379317184665269]
	TIME [epoch: 33 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38280726068811255		[learning rate: 0.012238]
	Learning Rate: 0.0122377
	LOSS [training: 0.38280726068811255 | validation: 0.3469054454010827]
	TIME [epoch: 33 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38266443836261255		[learning rate: 0.012222]
	Learning Rate: 0.0122216
	LOSS [training: 0.38266443836261255 | validation: 0.4033959064528663]
	TIME [epoch: 33 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32336129431152394		[learning rate: 0.012205]
	Learning Rate: 0.0122055
	LOSS [training: 0.32336129431152394 | validation: 0.33389214929497696]
	TIME [epoch: 33 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35015597625136063		[learning rate: 0.012189]
	Learning Rate: 0.0121894
	LOSS [training: 0.35015597625136063 | validation: 0.42116015835381343]
	TIME [epoch: 33 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3564217889544202		[learning rate: 0.012173]
	Learning Rate: 0.0121732
	LOSS [training: 0.3564217889544202 | validation: 0.3915992609473795]
	TIME [epoch: 33 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.361474421582056		[learning rate: 0.012157]
	Learning Rate: 0.0121571
	LOSS [training: 0.361474421582056 | validation: 0.38918087634678844]
	TIME [epoch: 33 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33554123136112896		[learning rate: 0.012141]
	Learning Rate: 0.012141
	LOSS [training: 0.33554123136112896 | validation: 0.3158920448468454]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_870.pth
	Model improved!!!
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3735962262406324		[learning rate: 0.012125]
	Learning Rate: 0.0121248
	LOSS [training: 0.3735962262406324 | validation: 0.33512132528321337]
	TIME [epoch: 33 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3358949426113509		[learning rate: 0.012109]
	Learning Rate: 0.0121087
	LOSS [training: 0.3358949426113509 | validation: 0.38810100217017435]
	TIME [epoch: 33 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35656915952910595		[learning rate: 0.012093]
	Learning Rate: 0.0120925
	LOSS [training: 0.35656915952910595 | validation: 0.35173093130051447]
	TIME [epoch: 33 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33175518920640834		[learning rate: 0.012076]
	Learning Rate: 0.0120763
	LOSS [training: 0.33175518920640834 | validation: 0.36635326122843676]
	TIME [epoch: 33 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36244581964249006		[learning rate: 0.01206]
	Learning Rate: 0.0120602
	LOSS [training: 0.36244581964249006 | validation: 0.35944980321891373]
	TIME [epoch: 33 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31918771643224353		[learning rate: 0.012044]
	Learning Rate: 0.012044
	LOSS [training: 0.31918771643224353 | validation: 0.36751855388329413]
	TIME [epoch: 33 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36746375241434426		[learning rate: 0.012028]
	Learning Rate: 0.0120278
	LOSS [training: 0.36746375241434426 | validation: 0.43538545914166327]
	TIME [epoch: 33 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3745383559473006		[learning rate: 0.012012]
	Learning Rate: 0.0120116
	LOSS [training: 0.3745383559473006 | validation: 0.33068028543112293]
	TIME [epoch: 33 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34172966937612353		[learning rate: 0.011995]
	Learning Rate: 0.0119954
	LOSS [training: 0.34172966937612353 | validation: 0.465693793604657]
	TIME [epoch: 33 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37389178346077967		[learning rate: 0.011979]
	Learning Rate: 0.0119792
	LOSS [training: 0.37389178346077967 | validation: 0.32735655617439374]
	TIME [epoch: 33 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31632815546002013		[learning rate: 0.011963]
	Learning Rate: 0.011963
	LOSS [training: 0.31632815546002013 | validation: 0.39439183630407393]
	TIME [epoch: 33 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3537965594348421		[learning rate: 0.011947]
	Learning Rate: 0.0119468
	LOSS [training: 0.3537965594348421 | validation: 0.46476838549775557]
	TIME [epoch: 33 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38729606785029597		[learning rate: 0.011931]
	Learning Rate: 0.0119306
	LOSS [training: 0.38729606785029597 | validation: 0.342076099470274]
	TIME [epoch: 33 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33865852427601606		[learning rate: 0.011914]
	Learning Rate: 0.0119144
	LOSS [training: 0.33865852427601606 | validation: 0.3636346875441658]
	TIME [epoch: 33 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34531279343758176		[learning rate: 0.011898]
	Learning Rate: 0.0118982
	LOSS [training: 0.34531279343758176 | validation: 0.3635817206594287]
	TIME [epoch: 33 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3346482296395019		[learning rate: 0.011882]
	Learning Rate: 0.0118819
	LOSS [training: 0.3346482296395019 | validation: 0.339779724952777]
	TIME [epoch: 33 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3193193778067248		[learning rate: 0.011866]
	Learning Rate: 0.0118657
	LOSS [training: 0.3193193778067248 | validation: 0.4211303713295672]
	TIME [epoch: 33 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3934541572716011		[learning rate: 0.011849]
	Learning Rate: 0.0118495
	LOSS [training: 0.3934541572716011 | validation: 0.32391557098832513]
	TIME [epoch: 33 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3291399472132916		[learning rate: 0.011833]
	Learning Rate: 0.0118332
	LOSS [training: 0.3291399472132916 | validation: 0.3560688412700846]
	TIME [epoch: 33 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3377371523257079		[learning rate: 0.011817]
	Learning Rate: 0.011817
	LOSS [training: 0.3377371523257079 | validation: 0.3948153826730606]
	TIME [epoch: 33 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34056545935653293		[learning rate: 0.011801]
	Learning Rate: 0.0118007
	LOSS [training: 0.34056545935653293 | validation: 0.3133065282384109]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_891.pth
	Model improved!!!
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3615713341734734		[learning rate: 0.011784]
	Learning Rate: 0.0117844
	LOSS [training: 0.3615713341734734 | validation: 0.47325816492665557]
	TIME [epoch: 33 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38844176498374133		[learning rate: 0.011768]
	Learning Rate: 0.0117682
	LOSS [training: 0.38844176498374133 | validation: 0.36217441870269584]
	TIME [epoch: 33 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31783164360640653		[learning rate: 0.011752]
	Learning Rate: 0.0117519
	LOSS [training: 0.31783164360640653 | validation: 0.37592107295344146]
	TIME [epoch: 33 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34134385173304205		[learning rate: 0.011736]
	Learning Rate: 0.0117356
	LOSS [training: 0.34134385173304205 | validation: 0.3380101297864195]
	TIME [epoch: 33 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32491765035800096		[learning rate: 0.011719]
	Learning Rate: 0.0117194
	LOSS [training: 0.32491765035800096 | validation: 0.37438373893550225]
	TIME [epoch: 33 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34525700002844195		[learning rate: 0.011703]
	Learning Rate: 0.0117031
	LOSS [training: 0.34525700002844195 | validation: 0.3669540764940209]
	TIME [epoch: 33 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.319715311004207		[learning rate: 0.011687]
	Learning Rate: 0.0116868
	LOSS [training: 0.319715311004207 | validation: 0.46616959702779465]
	TIME [epoch: 33 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4370852368216729		[learning rate: 0.01167]
	Learning Rate: 0.0116705
	LOSS [training: 0.4370852368216729 | validation: 0.3942294768691561]
	TIME [epoch: 33 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3245631097866601		[learning rate: 0.011654]
	Learning Rate: 0.0116542
	LOSS [training: 0.3245631097866601 | validation: 0.3319230064831562]
	TIME [epoch: 33 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31880711352544955		[learning rate: 0.011638]
	Learning Rate: 0.0116379
	LOSS [training: 0.31880711352544955 | validation: 0.3835874487658475]
	TIME [epoch: 33 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3201198929720747		[learning rate: 0.011622]
	Learning Rate: 0.0116216
	LOSS [training: 0.3201198929720747 | validation: 0.467888476047273]
	TIME [epoch: 32.9 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39435069706181164		[learning rate: 0.011605]
	Learning Rate: 0.0116053
	LOSS [training: 0.39435069706181164 | validation: 0.3723401863235416]
	TIME [epoch: 33 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31043460340833506		[learning rate: 0.011589]
	Learning Rate: 0.011589
	LOSS [training: 0.31043460340833506 | validation: 0.3073001646407644]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_904.pth
	Model improved!!!
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34983408489622675		[learning rate: 0.011573]
	Learning Rate: 0.0115726
	LOSS [training: 0.34983408489622675 | validation: 0.4264805584197858]
	TIME [epoch: 33 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3353642820860472		[learning rate: 0.011556]
	Learning Rate: 0.0115563
	LOSS [training: 0.3353642820860472 | validation: 0.29672249361379904]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_906.pth
	Model improved!!!
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3495782856472859		[learning rate: 0.01154]
	Learning Rate: 0.01154
	LOSS [training: 0.3495782856472859 | validation: 0.39222032750730645]
	TIME [epoch: 33 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.338427293595639		[learning rate: 0.011524]
	Learning Rate: 0.0115237
	LOSS [training: 0.338427293595639 | validation: 0.32266804953568423]
	TIME [epoch: 33 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3763336926626608		[learning rate: 0.011507]
	Learning Rate: 0.0115073
	LOSS [training: 0.3763336926626608 | validation: 0.39357346306465385]
	TIME [epoch: 33 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3146625827678672		[learning rate: 0.011491]
	Learning Rate: 0.011491
	LOSS [training: 0.3146625827678672 | validation: 0.407950323386579]
	TIME [epoch: 32.9 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38542389733657456		[learning rate: 0.011475]
	Learning Rate: 0.0114746
	LOSS [training: 0.38542389733657456 | validation: 0.3504080866987678]
	TIME [epoch: 33 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30827056908878486		[learning rate: 0.011458]
	Learning Rate: 0.0114583
	LOSS [training: 0.30827056908878486 | validation: 0.3446626348288712]
	TIME [epoch: 33 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36641290834666457		[learning rate: 0.011442]
	Learning Rate: 0.0114419
	LOSS [training: 0.36641290834666457 | validation: 0.39332059855050316]
	TIME [epoch: 33 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3157961855797545		[learning rate: 0.011426]
	Learning Rate: 0.0114256
	LOSS [training: 0.3157961855797545 | validation: 0.39023783231827835]
	TIME [epoch: 33 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3422153674013908		[learning rate: 0.011409]
	Learning Rate: 0.0114092
	LOSS [training: 0.3422153674013908 | validation: 0.4743853216355538]
	TIME [epoch: 33.1 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38981099035774913		[learning rate: 0.011393]
	Learning Rate: 0.0113929
	LOSS [training: 0.38981099035774913 | validation: 0.34478966898325825]
	TIME [epoch: 33 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3202940913715623		[learning rate: 0.011376]
	Learning Rate: 0.0113765
	LOSS [training: 0.3202940913715623 | validation: 0.3335709074788813]
	TIME [epoch: 33 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3591444019462058		[learning rate: 0.01136]
	Learning Rate: 0.0113601
	LOSS [training: 0.3591444019462058 | validation: 0.38068400865849883]
	TIME [epoch: 33 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3159656860112723		[learning rate: 0.011344]
	Learning Rate: 0.0113437
	LOSS [training: 0.3159656860112723 | validation: 0.2902794139052953]
	TIME [epoch: 33 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3683506298099197		[learning rate: 0.011327]
	Learning Rate: 0.0113274
	LOSS [training: 0.3683506298099197 | validation: 0.36235367912593475]
	TIME [epoch: 33 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32992823478299244		[learning rate: 0.011311]
	Learning Rate: 0.011311
	LOSS [training: 0.32992823478299244 | validation: 0.3221550566932303]
	TIME [epoch: 33 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.345792028543907		[learning rate: 0.011295]
	Learning Rate: 0.0112946
	LOSS [training: 0.345792028543907 | validation: 0.3647487993660146]
	TIME [epoch: 32.9 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34343767927308294		[learning rate: 0.011278]
	Learning Rate: 0.0112782
	LOSS [training: 0.34343767927308294 | validation: 0.3407256267428893]
	TIME [epoch: 33 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3345761258121399		[learning rate: 0.011262]
	Learning Rate: 0.0112618
	LOSS [training: 0.3345761258121399 | validation: 0.3523636870552783]
	TIME [epoch: 33 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3361473429597837		[learning rate: 0.011245]
	Learning Rate: 0.0112454
	LOSS [training: 0.3361473429597837 | validation: 0.3335548636921796]
	TIME [epoch: 33 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3308330612517676		[learning rate: 0.011229]
	Learning Rate: 0.011229
	LOSS [training: 0.3308330612517676 | validation: 0.3203812001432341]
	TIME [epoch: 32.9 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3532545757653317		[learning rate: 0.011213]
	Learning Rate: 0.0112126
	LOSS [training: 0.3532545757653317 | validation: 0.34577160836554505]
	TIME [epoch: 33 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32633972067702566		[learning rate: 0.011196]
	Learning Rate: 0.0111962
	LOSS [training: 0.32633972067702566 | validation: 0.5132598259095966]
	TIME [epoch: 33 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40788784905867		[learning rate: 0.01118]
	Learning Rate: 0.0111798
	LOSS [training: 0.40788784905867 | validation: 0.3433369129686797]
	TIME [epoch: 33 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31625894362089335		[learning rate: 0.011163]
	Learning Rate: 0.0111634
	LOSS [training: 0.31625894362089335 | validation: 0.3335714079297627]
	TIME [epoch: 33 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2934283299887326		[learning rate: 0.011147]
	Learning Rate: 0.011147
	LOSS [training: 0.2934283299887326 | validation: 0.3661774918124291]
	TIME [epoch: 33 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.344261839137402		[learning rate: 0.011131]
	Learning Rate: 0.0111305
	LOSS [training: 0.344261839137402 | validation: 0.3663818055842715]
	TIME [epoch: 33 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3410438999689739		[learning rate: 0.011114]
	Learning Rate: 0.0111141
	LOSS [training: 0.3410438999689739 | validation: 0.38106225788501147]
	TIME [epoch: 33 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.362771529333667		[learning rate: 0.011098]
	Learning Rate: 0.0110977
	LOSS [training: 0.362771529333667 | validation: 0.3604647116513026]
	TIME [epoch: 33 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3374684531740905		[learning rate: 0.011081]
	Learning Rate: 0.0110813
	LOSS [training: 0.3374684531740905 | validation: 0.3633493350643382]
	TIME [epoch: 33 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3300985000278494		[learning rate: 0.011065]
	Learning Rate: 0.0110648
	LOSS [training: 0.3300985000278494 | validation: 0.35477645129427887]
	TIME [epoch: 33 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3460215814152927		[learning rate: 0.011048]
	Learning Rate: 0.0110484
	LOSS [training: 0.3460215814152927 | validation: 0.40235636117402007]
	TIME [epoch: 33 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39443566540512054		[learning rate: 0.011032]
	Learning Rate: 0.011032
	LOSS [training: 0.39443566540512054 | validation: 0.3332326279261818]
	TIME [epoch: 33 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3035567500687596		[learning rate: 0.011016]
	Learning Rate: 0.0110155
	LOSS [training: 0.3035567500687596 | validation: 0.37570362446704214]
	TIME [epoch: 33 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3893573220978848		[learning rate: 0.010999]
	Learning Rate: 0.0109991
	LOSS [training: 0.3893573220978848 | validation: 0.32009411143227495]
	TIME [epoch: 33 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2977800961590128		[learning rate: 0.010983]
	Learning Rate: 0.0109826
	LOSS [training: 0.2977800961590128 | validation: 0.3905022568464348]
	TIME [epoch: 33 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36603776616346434		[learning rate: 0.010966]
	Learning Rate: 0.0109662
	LOSS [training: 0.36603776616346434 | validation: 0.3339618422641179]
	TIME [epoch: 33 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32400753392905735		[learning rate: 0.01095]
	Learning Rate: 0.0109497
	LOSS [training: 0.32400753392905735 | validation: 0.3771839978804622]
	TIME [epoch: 33 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3237556048369036		[learning rate: 0.010933]
	Learning Rate: 0.0109333
	LOSS [training: 0.3237556048369036 | validation: 0.40560499780600234]
	TIME [epoch: 33 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33374150253804125		[learning rate: 0.010917]
	Learning Rate: 0.0109168
	LOSS [training: 0.33374150253804125 | validation: 0.2994477206035448]
	TIME [epoch: 33 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37146698330596384		[learning rate: 0.0109]
	Learning Rate: 0.0109004
	LOSS [training: 0.37146698330596384 | validation: 0.3944259065476003]
	TIME [epoch: 33 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3299139877048543		[learning rate: 0.010884]
	Learning Rate: 0.0108839
	LOSS [training: 0.3299139877048543 | validation: 0.3413471542147444]
	TIME [epoch: 33 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3219674894171064		[learning rate: 0.010867]
	Learning Rate: 0.0108674
	LOSS [training: 0.3219674894171064 | validation: 0.3659218546227485]
	TIME [epoch: 33 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33371908795381444		[learning rate: 0.010851]
	Learning Rate: 0.010851
	LOSS [training: 0.33371908795381444 | validation: 0.3553092872873906]
	TIME [epoch: 33 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3566376422546048		[learning rate: 0.010834]
	Learning Rate: 0.0108345
	LOSS [training: 0.3566376422546048 | validation: 0.3375026605112678]
	TIME [epoch: 33 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2961933179154055		[learning rate: 0.010818]
	Learning Rate: 0.010818
	LOSS [training: 0.2961933179154055 | validation: 0.3608034484278536]
	TIME [epoch: 33 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3440868315141876		[learning rate: 0.010802]
	Learning Rate: 0.0108016
	LOSS [training: 0.3440868315141876 | validation: 0.3674962244372961]
	TIME [epoch: 33 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3356043741483785		[learning rate: 0.010785]
	Learning Rate: 0.0107851
	LOSS [training: 0.3356043741483785 | validation: 0.31194894535656625]
	TIME [epoch: 33 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3542210269983551		[learning rate: 0.010769]
	Learning Rate: 0.0107686
	LOSS [training: 0.3542210269983551 | validation: 0.40891481773087923]
	TIME [epoch: 33 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3488352943303819		[learning rate: 0.010752]
	Learning Rate: 0.0107521
	LOSS [training: 0.3488352943303819 | validation: 0.31130547990049495]
	TIME [epoch: 33 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3016268271682452		[learning rate: 0.010736]
	Learning Rate: 0.0107356
	LOSS [training: 0.3016268271682452 | validation: 0.3150898927814141]
	TIME [epoch: 33 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36727782429990474		[learning rate: 0.010719]
	Learning Rate: 0.0107192
	LOSS [training: 0.36727782429990474 | validation: 0.4003741537396975]
	TIME [epoch: 33 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31953210263070714		[learning rate: 0.010703]
	Learning Rate: 0.0107027
	LOSS [training: 0.31953210263070714 | validation: 0.3039092207413199]
	TIME [epoch: 33 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32734000057989776		[learning rate: 0.010686]
	Learning Rate: 0.0106862
	LOSS [training: 0.32734000057989776 | validation: 0.31296990123646684]
	TIME [epoch: 33 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3249673790814902		[learning rate: 0.01067]
	Learning Rate: 0.0106697
	LOSS [training: 0.3249673790814902 | validation: 0.4377787287915369]
	TIME [epoch: 33 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.356405705613691		[learning rate: 0.010653]
	Learning Rate: 0.0106532
	LOSS [training: 0.356405705613691 | validation: 0.30918126606795093]
	TIME [epoch: 33 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3264841883971366		[learning rate: 0.010637]
	Learning Rate: 0.0106367
	LOSS [training: 0.3264841883971366 | validation: 0.30597274543246683]
	TIME [epoch: 33 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33472302286398947		[learning rate: 0.01062]
	Learning Rate: 0.0106202
	LOSS [training: 0.33472302286398947 | validation: 0.3872288592978873]
	TIME [epoch: 33 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3346591038087994		[learning rate: 0.010604]
	Learning Rate: 0.0106037
	LOSS [training: 0.3346591038087994 | validation: 0.31325729155673887]
	TIME [epoch: 33 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3260054589419496		[learning rate: 0.010587]
	Learning Rate: 0.0105872
	LOSS [training: 0.3260054589419496 | validation: 0.32152765916054715]
	TIME [epoch: 33 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31181873963114515		[learning rate: 0.010571]
	Learning Rate: 0.0105707
	LOSS [training: 0.31181873963114515 | validation: 0.3598207394783677]
	TIME [epoch: 33 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4005180314884783		[learning rate: 0.010554]
	Learning Rate: 0.0105542
	LOSS [training: 0.4005180314884783 | validation: 0.3335566353856958]
	TIME [epoch: 33 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3004027488773616		[learning rate: 0.010538]
	Learning Rate: 0.0105377
	LOSS [training: 0.3004027488773616 | validation: 0.31757047215637524]
	TIME [epoch: 33 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3479607473211833		[learning rate: 0.010521]
	Learning Rate: 0.0105212
	LOSS [training: 0.3479607473211833 | validation: 0.3678148569036612]
	TIME [epoch: 33 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3177723081929953		[learning rate: 0.010505]
	Learning Rate: 0.0105047
	LOSS [training: 0.3177723081929953 | validation: 0.3643083007875084]
	TIME [epoch: 33 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33435949977422036		[learning rate: 0.010488]
	Learning Rate: 0.0104882
	LOSS [training: 0.33435949977422036 | validation: 0.33587809330354657]
	TIME [epoch: 33 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3260660213019294		[learning rate: 0.010472]
	Learning Rate: 0.0104717
	LOSS [training: 0.3260660213019294 | validation: 0.31225093006867355]
	TIME [epoch: 33 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3236843476334313		[learning rate: 0.010455]
	Learning Rate: 0.0104552
	LOSS [training: 0.3236843476334313 | validation: 0.30635346449939227]
	TIME [epoch: 32.9 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31217932225125156		[learning rate: 0.010439]
	Learning Rate: 0.0104387
	LOSS [training: 0.31217932225125156 | validation: 0.32293750630157325]
	TIME [epoch: 33 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3255398502499628		[learning rate: 0.010422]
	Learning Rate: 0.0104222
	LOSS [training: 0.3255398502499628 | validation: 0.34383766728617027]
	TIME [epoch: 33 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3343206153646231		[learning rate: 0.010406]
	Learning Rate: 0.0104057
	LOSS [training: 0.3343206153646231 | validation: 0.3725920208850391]
	TIME [epoch: 33 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32197088527473683		[learning rate: 0.010389]
	Learning Rate: 0.0103891
	LOSS [training: 0.32197088527473683 | validation: 0.30317004372626116]
	TIME [epoch: 33 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3208948411351532		[learning rate: 0.010373]
	Learning Rate: 0.0103726
	LOSS [training: 0.3208948411351532 | validation: 0.40388405645701064]
	TIME [epoch: 33 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3286987099247759		[learning rate: 0.010356]
	Learning Rate: 0.0103561
	LOSS [training: 0.3286987099247759 | validation: 0.35922653282019756]
	TIME [epoch: 33 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33748868620087674		[learning rate: 0.01034]
	Learning Rate: 0.0103396
	LOSS [training: 0.33748868620087674 | validation: 0.30245997195135]
	TIME [epoch: 33 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3286903758002856		[learning rate: 0.010323]
	Learning Rate: 0.0103231
	LOSS [training: 0.3286903758002856 | validation: 0.3608910023192226]
	TIME [epoch: 33 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29845358536518757		[learning rate: 0.010307]
	Learning Rate: 0.0103066
	LOSS [training: 0.29845358536518757 | validation: 0.30317169851511805]
	TIME [epoch: 32.9 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3935497752782233		[learning rate: 0.01029]
	Learning Rate: 0.01029
	LOSS [training: 0.3935497752782233 | validation: 0.41966442584469765]
	TIME [epoch: 33 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3137625006306094		[learning rate: 0.010274]
	Learning Rate: 0.0102735
	LOSS [training: 0.3137625006306094 | validation: 0.29648178594206187]
	TIME [epoch: 33 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3403393025954159		[learning rate: 0.010257]
	Learning Rate: 0.010257
	LOSS [training: 0.3403393025954159 | validation: 0.35163770736675576]
	TIME [epoch: 33 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31528016214042887		[learning rate: 0.01024]
	Learning Rate: 0.0102405
	LOSS [training: 0.31528016214042887 | validation: 0.34278093023722983]
	TIME [epoch: 32.9 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33276051752867974		[learning rate: 0.010224]
	Learning Rate: 0.010224
	LOSS [training: 0.33276051752867974 | validation: 0.352699615419817]
	TIME [epoch: 33 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3141958730059229		[learning rate: 0.010207]
	Learning Rate: 0.0102074
	LOSS [training: 0.3141958730059229 | validation: 0.312116961040827]
	TIME [epoch: 33 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3014392259682178		[learning rate: 0.010191]
	Learning Rate: 0.0101909
	LOSS [training: 0.3014392259682178 | validation: 0.35218539336355503]
	TIME [epoch: 33 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35514843484348857		[learning rate: 0.010174]
	Learning Rate: 0.0101744
	LOSS [training: 0.35514843484348857 | validation: 0.3520143423424017]
	TIME [epoch: 33 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3033660728561848		[learning rate: 0.010158]
	Learning Rate: 0.0101579
	LOSS [training: 0.3033660728561848 | validation: 0.3249679450089268]
	TIME [epoch: 33 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3498524462237102		[learning rate: 0.010141]
	Learning Rate: 0.0101413
	LOSS [training: 0.3498524462237102 | validation: 0.33620778299595644]
	TIME [epoch: 33 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30491249307815016		[learning rate: 0.010125]
	Learning Rate: 0.0101248
	LOSS [training: 0.30491249307815016 | validation: 0.36380721662142124]
	TIME [epoch: 33 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3426439880009889		[learning rate: 0.010108]
	Learning Rate: 0.0101083
	LOSS [training: 0.3426439880009889 | validation: 0.34502512410306463]
	TIME [epoch: 33 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3157841858455185		[learning rate: 0.010092]
	Learning Rate: 0.0100918
	LOSS [training: 0.3157841858455185 | validation: 0.33406838231480895]
	TIME [epoch: 33 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3115160773895693		[learning rate: 0.010075]
	Learning Rate: 0.0100752
	LOSS [training: 0.3115160773895693 | validation: 0.3614383513725091]
	TIME [epoch: 33 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3138024760038587		[learning rate: 0.010059]
	Learning Rate: 0.0100587
	LOSS [training: 0.3138024760038587 | validation: 0.3022712524616823]
	TIME [epoch: 32.9 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34017595440403037		[learning rate: 0.010042]
	Learning Rate: 0.0100422
	LOSS [training: 0.34017595440403037 | validation: 0.34664846674682104]
	TIME [epoch: 33 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3357969062415116		[learning rate: 0.010026]
	Learning Rate: 0.0100257
	LOSS [training: 0.3357969062415116 | validation: 0.31657631502927375]
	TIME [epoch: 33 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33862410936928433		[learning rate: 0.010009]
	Learning Rate: 0.0100091
	LOSS [training: 0.33862410936928433 | validation: 0.3956631399627346]
	TIME [epoch: 33 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33345726312554513		[learning rate: 0.0099926]
	Learning Rate: 0.00999261
	LOSS [training: 0.33345726312554513 | validation: 0.2994122433015757]
	TIME [epoch: 175 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3070667520952598		[learning rate: 0.0099761]
	Learning Rate: 0.00997608
	LOSS [training: 0.3070667520952598 | validation: 0.2898652110785479]
	TIME [epoch: 70.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_1002.pth
	Model improved!!!
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3272658909032735		[learning rate: 0.0099596]
	Learning Rate: 0.00995955
	LOSS [training: 0.3272658909032735 | validation: 0.3046590932524189]
	TIME [epoch: 70.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31257508899001096		[learning rate: 0.009943]
	Learning Rate: 0.00994303
	LOSS [training: 0.31257508899001096 | validation: 0.2963552964940643]
	TIME [epoch: 70.8 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34480468919509966		[learning rate: 0.0099265]
	Learning Rate: 0.0099265
	LOSS [training: 0.34480468919509966 | validation: 0.38256249120296737]
	TIME [epoch: 70.6 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3245604403620254		[learning rate: 0.00991]
	Learning Rate: 0.00990997
	LOSS [training: 0.3245604403620254 | validation: 0.3201818072906639]
	TIME [epoch: 70.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31377996553767223		[learning rate: 0.0098934]
	Learning Rate: 0.00989345
	LOSS [training: 0.31377996553767223 | validation: 0.3013098631387187]
	TIME [epoch: 70.7 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3166201343628189		[learning rate: 0.0098769]
	Learning Rate: 0.00987692
	LOSS [training: 0.3166201343628189 | validation: 0.32173214818648466]
	TIME [epoch: 70.8 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31949687363516405		[learning rate: 0.0098604]
	Learning Rate: 0.0098604
	LOSS [training: 0.31949687363516405 | validation: 0.3287198206902538]
	TIME [epoch: 70.6 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3355460445107923		[learning rate: 0.0098439]
	Learning Rate: 0.00984387
	LOSS [training: 0.3355460445107923 | validation: 0.3551331606017757]
	TIME [epoch: 70.7 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3023196983920876		[learning rate: 0.0098274]
	Learning Rate: 0.00982735
	LOSS [training: 0.3023196983920876 | validation: 0.3684160504655153]
	TIME [epoch: 70.6 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34996961491407713		[learning rate: 0.0098108]
	Learning Rate: 0.00981083
	LOSS [training: 0.34996961491407713 | validation: 0.30941549553004444]
	TIME [epoch: 70.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3132808627714602		[learning rate: 0.0097943]
	Learning Rate: 0.0097943
	LOSS [training: 0.3132808627714602 | validation: 0.37230006099281754]
	TIME [epoch: 70.6 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3087302460465454		[learning rate: 0.0097778]
	Learning Rate: 0.00977778
	LOSS [training: 0.3087302460465454 | validation: 0.31223294558669157]
	TIME [epoch: 70.8 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33391387974893505		[learning rate: 0.0097613]
	Learning Rate: 0.00976126
	LOSS [training: 0.33391387974893505 | validation: 0.3244789732279474]
	TIME [epoch: 70.6 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.317004678904552		[learning rate: 0.0097447]
	Learning Rate: 0.00974474
	LOSS [training: 0.317004678904552 | validation: 0.4171858792654784]
	TIME [epoch: 70.6 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.336538727891614		[learning rate: 0.0097282]
	Learning Rate: 0.00972822
	LOSS [training: 0.336538727891614 | validation: 0.3222760940927422]
	TIME [epoch: 70.7 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31588768957403773		[learning rate: 0.0097117]
	Learning Rate: 0.0097117
	LOSS [training: 0.31588768957403773 | validation: 0.3071823520567454]
	TIME [epoch: 70.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3420725118766986		[learning rate: 0.0096952]
	Learning Rate: 0.00969518
	LOSS [training: 0.3420725118766986 | validation: 0.3702433337605348]
	TIME [epoch: 70.6 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.311311802165135		[learning rate: 0.0096787]
	Learning Rate: 0.00967866
	LOSS [training: 0.311311802165135 | validation: 0.3097761611932708]
	TIME [epoch: 70.5 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2926870466177007		[learning rate: 0.0096621]
	Learning Rate: 0.00966214
	LOSS [training: 0.2926870466177007 | validation: 0.3334006361474846]
	TIME [epoch: 70.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.361800633433887		[learning rate: 0.0096456]
	Learning Rate: 0.00964563
	LOSS [training: 0.361800633433887 | validation: 0.39129896736964886]
	TIME [epoch: 70.2 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3240585524510353		[learning rate: 0.0096291]
	Learning Rate: 0.00962911
	LOSS [training: 0.3240585524510353 | validation: 0.3022661748031198]
	TIME [epoch: 70.6 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31314800718833147		[learning rate: 0.0096126]
	Learning Rate: 0.0096126
	LOSS [training: 0.31314800718833147 | validation: 0.3645394387273033]
	TIME [epoch: 70.5 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31862135069345643		[learning rate: 0.0095961]
	Learning Rate: 0.00959609
	LOSS [training: 0.31862135069345643 | validation: 0.2944146966935063]
	TIME [epoch: 70.4 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3149866959896076		[learning rate: 0.0095796]
	Learning Rate: 0.00957957
	LOSS [training: 0.3149866959896076 | validation: 0.33356575526308296]
	TIME [epoch: 70.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29616231544356697		[learning rate: 0.0095631]
	Learning Rate: 0.00956306
	LOSS [training: 0.29616231544356697 | validation: 0.30181954270123024]
	TIME [epoch: 70.6 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33286330191937163		[learning rate: 0.0095466]
	Learning Rate: 0.00954655
	LOSS [training: 0.33286330191937163 | validation: 0.4029245569317963]
	TIME [epoch: 70.5 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3245337772142164		[learning rate: 0.00953]
	Learning Rate: 0.00953004
	LOSS [training: 0.3245337772142164 | validation: 0.3319463389425823]
	TIME [epoch: 70.1 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3057408893122685		[learning rate: 0.0095135]
	Learning Rate: 0.00951354
	LOSS [training: 0.3057408893122685 | validation: 0.3482720477500709]
	TIME [epoch: 70.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3426405604442605		[learning rate: 0.009497]
	Learning Rate: 0.00949703
	LOSS [training: 0.3426405604442605 | validation: 0.32286587982738374]
	TIME [epoch: 70.5 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27942887828961216		[learning rate: 0.0094805]
	Learning Rate: 0.00948053
	LOSS [training: 0.27942887828961216 | validation: 0.4121979637569131]
	TIME [epoch: 70.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3767072380987403		[learning rate: 0.009464]
	Learning Rate: 0.00946402
	LOSS [training: 0.3767072380987403 | validation: 0.33739607293844776]
	TIME [epoch: 70.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28731453146091085		[learning rate: 0.0094475]
	Learning Rate: 0.00944752
	LOSS [training: 0.28731453146091085 | validation: 0.33931215466016773]
	TIME [epoch: 70.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35270040317001444		[learning rate: 0.009431]
	Learning Rate: 0.00943102
	LOSS [training: 0.35270040317001444 | validation: 0.29610154372793146]
	TIME [epoch: 70.5 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3014542568372342		[learning rate: 0.0094145]
	Learning Rate: 0.00941452
	LOSS [training: 0.3014542568372342 | validation: 0.3668950681980039]
	TIME [epoch: 70.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.324324750386693		[learning rate: 0.009398]
	Learning Rate: 0.00939803
	LOSS [training: 0.324324750386693 | validation: 0.2997753926188882]
	TIME [epoch: 70.5 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31369372052468025		[learning rate: 0.0093815]
	Learning Rate: 0.00938153
	LOSS [training: 0.31369372052468025 | validation: 0.3164449324659502]
	TIME [epoch: 70.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3164770674059801		[learning rate: 0.009365]
	Learning Rate: 0.00936504
	LOSS [training: 0.3164770674059801 | validation: 0.3751951433688372]
	TIME [epoch: 70.3 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32047811488651956		[learning rate: 0.0093485]
	Learning Rate: 0.00934855
	LOSS [training: 0.32047811488651956 | validation: 0.2822353698784962]
	TIME [epoch: 70.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_1040.pth
	Model improved!!!
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.311908983511382		[learning rate: 0.0093321]
	Learning Rate: 0.00933206
	LOSS [training: 0.311908983511382 | validation: 0.321135014452714]
	TIME [epoch: 70.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29966884044177033		[learning rate: 0.0093156]
	Learning Rate: 0.00931557
	LOSS [training: 0.29966884044177033 | validation: 0.34748995429910856]
	TIME [epoch: 70.7 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32290682981766344		[learning rate: 0.0092991]
	Learning Rate: 0.00929908
	LOSS [training: 0.32290682981766344 | validation: 0.29121437583684995]
	TIME [epoch: 70.5 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28622934140165296		[learning rate: 0.0092826]
	Learning Rate: 0.0092826
	LOSS [training: 0.28622934140165296 | validation: 0.3223892294594518]
	TIME [epoch: 70.5 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3205626074520855		[learning rate: 0.0092661]
	Learning Rate: 0.00926612
	LOSS [training: 0.3205626074520855 | validation: 0.3344854835463239]
	TIME [epoch: 70.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32685231131287246		[learning rate: 0.0092496]
	Learning Rate: 0.00924964
	LOSS [training: 0.32685231131287246 | validation: 0.30209907413146675]
	TIME [epoch: 70.6 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30548681412265966		[learning rate: 0.0092332]
	Learning Rate: 0.00923316
	LOSS [training: 0.30548681412265966 | validation: 0.3101759672047958]
	TIME [epoch: 70.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30937972301433025		[learning rate: 0.0092167]
	Learning Rate: 0.00921668
	LOSS [training: 0.30937972301433025 | validation: 0.35750225941582336]
	TIME [epoch: 70.7 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31763548725887625		[learning rate: 0.0092002]
	Learning Rate: 0.00920021
	LOSS [training: 0.31763548725887625 | validation: 0.294495958551114]
	TIME [epoch: 70.5 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32473249191987336		[learning rate: 0.0091837]
	Learning Rate: 0.00918374
	LOSS [training: 0.32473249191987336 | validation: 0.3438514723929558]
	TIME [epoch: 70.7 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30782153400987966		[learning rate: 0.0091673]
	Learning Rate: 0.00916727
	LOSS [training: 0.30782153400987966 | validation: 0.31384886311108656]
	TIME [epoch: 70.6 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30246128559373553		[learning rate: 0.0091508]
	Learning Rate: 0.0091508
	LOSS [training: 0.30246128559373553 | validation: 0.29500055933125086]
	TIME [epoch: 70.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3260497918655343		[learning rate: 0.0091343]
	Learning Rate: 0.00913434
	LOSS [training: 0.3260497918655343 | validation: 0.3614408472214773]
	TIME [epoch: 70.5 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2980262778636478		[learning rate: 0.0091179]
	Learning Rate: 0.00911787
	LOSS [training: 0.2980262778636478 | validation: 0.34816043707085975]
	TIME [epoch: 70.6 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34150814384551514		[learning rate: 0.0091014]
	Learning Rate: 0.00910141
	LOSS [training: 0.34150814384551514 | validation: 0.31001674951677816]
	TIME [epoch: 70.5 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2882803987089004		[learning rate: 0.009085]
	Learning Rate: 0.00908496
	LOSS [training: 0.2882803987089004 | validation: 0.318606902727815]
	TIME [epoch: 70.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3094719641643762		[learning rate: 0.0090685]
	Learning Rate: 0.0090685
	LOSS [training: 0.3094719641643762 | validation: 0.40694655690413867]
	TIME [epoch: 70.6 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3476695824258474		[learning rate: 0.009052]
	Learning Rate: 0.00905205
	LOSS [training: 0.3476695824258474 | validation: 0.2999779752912426]
	TIME [epoch: 70.5 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28872297668734076		[learning rate: 0.0090356]
	Learning Rate: 0.0090356
	LOSS [training: 0.28872297668734076 | validation: 0.3624411403263579]
	TIME [epoch: 70.6 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32847830532684374		[learning rate: 0.0090192]
	Learning Rate: 0.00901915
	LOSS [training: 0.32847830532684374 | validation: 0.28351555780199195]
	TIME [epoch: 70.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3169221342378266		[learning rate: 0.0090027]
	Learning Rate: 0.00900271
	LOSS [training: 0.3169221342378266 | validation: 0.36135372421382184]
	TIME [epoch: 70.5 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29969681514212326		[learning rate: 0.0089863]
	Learning Rate: 0.00898627
	LOSS [training: 0.29969681514212326 | validation: 0.3011982640995189]
	TIME [epoch: 70.6 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32162401800145474		[learning rate: 0.0089698]
	Learning Rate: 0.00896983
	LOSS [training: 0.32162401800145474 | validation: 0.2977588458675524]
	TIME [epoch: 70.6 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.294639019590314		[learning rate: 0.0089534]
	Learning Rate: 0.00895339
	LOSS [training: 0.294639019590314 | validation: 0.31761960883166696]
	TIME [epoch: 70.5 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30654158183104135		[learning rate: 0.008937]
	Learning Rate: 0.00893696
	LOSS [training: 0.30654158183104135 | validation: 0.28463749329520177]
	TIME [epoch: 70.5 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3091695724699933		[learning rate: 0.0089205]
	Learning Rate: 0.00892053
	LOSS [training: 0.3091695724699933 | validation: 0.3192566023288429]
	TIME [epoch: 70.6 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32270541307297057		[learning rate: 0.0089041]
	Learning Rate: 0.0089041
	LOSS [training: 0.32270541307297057 | validation: 0.30446401087557773]
	TIME [epoch: 70.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28380228059547286		[learning rate: 0.0088877]
	Learning Rate: 0.00888767
	LOSS [training: 0.28380228059547286 | validation: 0.3063529016823141]
	TIME [epoch: 70.5 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2989435556494149		[learning rate: 0.0088713]
	Learning Rate: 0.00887125
	LOSS [training: 0.2989435556494149 | validation: 0.3342607207660414]
	TIME [epoch: 70.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30059479124718336		[learning rate: 0.0088548]
	Learning Rate: 0.00885484
	LOSS [training: 0.30059479124718336 | validation: 0.3085100433295843]
	TIME [epoch: 70.5 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3150029903335614		[learning rate: 0.0088384]
	Learning Rate: 0.00883842
	LOSS [training: 0.3150029903335614 | validation: 0.2923453201649444]
	TIME [epoch: 70.5 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32906532699317653		[learning rate: 0.008822]
	Learning Rate: 0.00882201
	LOSS [training: 0.32906532699317653 | validation: 0.3172849841450499]
	TIME [epoch: 70.7 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2944393930833489		[learning rate: 0.0088056]
	Learning Rate: 0.0088056
	LOSS [training: 0.2944393930833489 | validation: 0.30759243125950997]
	TIME [epoch: 70.6 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3205033958826038		[learning rate: 0.0087892]
	Learning Rate: 0.00878919
	LOSS [training: 0.3205033958826038 | validation: 0.3814189829798038]
	TIME [epoch: 70.5 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34961499253733663		[learning rate: 0.0087728]
	Learning Rate: 0.00877279
	LOSS [training: 0.34961499253733663 | validation: 0.29903835948302526]
	TIME [epoch: 70.5 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28735231254719584		[learning rate: 0.0087564]
	Learning Rate: 0.00875639
	LOSS [training: 0.28735231254719584 | validation: 0.2788626826823838]
	TIME [epoch: 70.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_1076.pth
	Model improved!!!
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29460389417843463		[learning rate: 0.00874]
	Learning Rate: 0.00874
	LOSS [training: 0.29460389417843463 | validation: 0.3302630395727807]
	TIME [epoch: 70.8 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32711132300448686		[learning rate: 0.0087236]
	Learning Rate: 0.00872361
	LOSS [training: 0.32711132300448686 | validation: 0.31352352550205353]
	TIME [epoch: 70.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3101884983140131		[learning rate: 0.0087072]
	Learning Rate: 0.00870722
	LOSS [training: 0.3101884983140131 | validation: 0.28052692582234967]
	TIME [epoch: 70.6 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28276851246998813		[learning rate: 0.0086908]
	Learning Rate: 0.00869083
	LOSS [training: 0.28276851246998813 | validation: 0.33846262524282267]
	TIME [epoch: 70.5 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.327581218506246		[learning rate: 0.0086745]
	Learning Rate: 0.00867445
	LOSS [training: 0.327581218506246 | validation: 0.2965484658619679]
	TIME [epoch: 70.8 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31912589555005766		[learning rate: 0.0086581]
	Learning Rate: 0.00865807
	LOSS [training: 0.31912589555005766 | validation: 0.3086552904826322]
	TIME [epoch: 70.7 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2909287545100796		[learning rate: 0.0086417]
	Learning Rate: 0.0086417
	LOSS [training: 0.2909287545100796 | validation: 0.3207157497245763]
	TIME [epoch: 70.7 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2739270165154022		[learning rate: 0.0086253]
	Learning Rate: 0.00862533
	LOSS [training: 0.2739270165154022 | validation: 0.34658618274208497]
	TIME [epoch: 70.6 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.355806580642718		[learning rate: 0.008609]
	Learning Rate: 0.00860896
	LOSS [training: 0.355806580642718 | validation: 0.34620788762139515]
	TIME [epoch: 70.6 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.301838926773076		[learning rate: 0.0085926]
	Learning Rate: 0.0085926
	LOSS [training: 0.301838926773076 | validation: 0.2806643358606844]
	TIME [epoch: 70.6 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3318168666582808		[learning rate: 0.0085762]
	Learning Rate: 0.00857624
	LOSS [training: 0.3318168666582808 | validation: 0.380542951456906]
	TIME [epoch: 70.6 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3019737473505609		[learning rate: 0.0085599]
	Learning Rate: 0.00855989
	LOSS [training: 0.3019737473505609 | validation: 0.29525096021674124]
	TIME [epoch: 70.7 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28863274077788154		[learning rate: 0.0085435]
	Learning Rate: 0.00854354
	LOSS [training: 0.28863274077788154 | validation: 0.3128838734249114]
	TIME [epoch: 70.5 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3070980307371714		[learning rate: 0.0085272]
	Learning Rate: 0.00852719
	LOSS [training: 0.3070980307371714 | validation: 0.3120514646422824]
	TIME [epoch: 70.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2992506472033624		[learning rate: 0.0085108]
	Learning Rate: 0.00851085
	LOSS [training: 0.2992506472033624 | validation: 0.27695066105703464]
	TIME [epoch: 70.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_1091.pth
	Model improved!!!
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3137401644502955		[learning rate: 0.0084945]
	Learning Rate: 0.00849451
	LOSS [training: 0.3137401644502955 | validation: 0.4540720804778683]
	TIME [epoch: 70.7 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35179414542050635		[learning rate: 0.0084782]
	Learning Rate: 0.00847817
	LOSS [training: 0.35179414542050635 | validation: 0.29813249403131575]
	TIME [epoch: 70.8 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2902520926503841		[learning rate: 0.0084618]
	Learning Rate: 0.00846184
	LOSS [training: 0.2902520926503841 | validation: 0.33186904548576407]
	TIME [epoch: 70.7 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3140349074360295		[learning rate: 0.0084455]
	Learning Rate: 0.00844552
	LOSS [training: 0.3140349074360295 | validation: 0.3283696124167751]
	TIME [epoch: 70.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3191127532468402		[learning rate: 0.0084292]
	Learning Rate: 0.0084292
	LOSS [training: 0.3191127532468402 | validation: 0.31568601211690317]
	TIME [epoch: 70.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2777387379498926		[learning rate: 0.0084129]
	Learning Rate: 0.00841288
	LOSS [training: 0.2777387379498926 | validation: 0.2831405748932734]
	TIME [epoch: 70.6 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32675940488858346		[learning rate: 0.0083966]
	Learning Rate: 0.00839657
	LOSS [training: 0.32675940488858346 | validation: 0.3352062517180717]
	TIME [epoch: 70.9 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2942534247946004		[learning rate: 0.0083803]
	Learning Rate: 0.00838026
	LOSS [training: 0.2942534247946004 | validation: 0.28373430762254553]
	TIME [epoch: 70.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3326464108729824		[learning rate: 0.008364]
	Learning Rate: 0.00836395
	LOSS [training: 0.3326464108729824 | validation: 0.32901791066758235]
	TIME [epoch: 70.8 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2955172518671604		[learning rate: 0.0083477]
	Learning Rate: 0.00834765
	LOSS [training: 0.2955172518671604 | validation: 0.28318054511019813]
	TIME [epoch: 70.8 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.292122662377984		[learning rate: 0.0083314]
	Learning Rate: 0.00833136
	LOSS [training: 0.292122662377984 | validation: 0.3052582743266026]
	TIME [epoch: 70.8 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29572315444129293		[learning rate: 0.0083151]
	Learning Rate: 0.00831507
	LOSS [training: 0.29572315444129293 | validation: 0.2823035570181944]
	TIME [epoch: 70.7 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2963935884748532		[learning rate: 0.0082988]
	Learning Rate: 0.00829878
	LOSS [training: 0.2963935884748532 | validation: 0.2751726956072311]
	TIME [epoch: 70.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_1104.pth
	Model improved!!!
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3395914327590911		[learning rate: 0.0082825]
	Learning Rate: 0.0082825
	LOSS [training: 0.3395914327590911 | validation: 0.3626302348131259]
	TIME [epoch: 70.8 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.314319692868363		[learning rate: 0.0082662]
	Learning Rate: 0.00826622
	LOSS [training: 0.314319692868363 | validation: 0.2894783648408886]
	TIME [epoch: 70.8 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27824311451767786		[learning rate: 0.0082499]
	Learning Rate: 0.00824995
	LOSS [training: 0.27824311451767786 | validation: 0.32093190378463454]
	TIME [epoch: 70.7 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3043634870275501		[learning rate: 0.0082337]
	Learning Rate: 0.00823368
	LOSS [training: 0.3043634870275501 | validation: 0.2807781453259021]
	TIME [epoch: 70.8 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30727466683481325		[learning rate: 0.0082174]
	Learning Rate: 0.00821742
	LOSS [training: 0.30727466683481325 | validation: 0.3693260489251826]
	TIME [epoch: 70.8 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29685836830659773		[learning rate: 0.0082012]
	Learning Rate: 0.00820116
	LOSS [training: 0.29685836830659773 | validation: 0.2849216458684282]
	TIME [epoch: 70.8 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3022781385804296		[learning rate: 0.0081849]
	Learning Rate: 0.00818491
	LOSS [training: 0.3022781385804296 | validation: 0.3051846035037261]
	TIME [epoch: 70.7 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3038551869117306		[learning rate: 0.0081687]
	Learning Rate: 0.00816866
	LOSS [training: 0.3038551869117306 | validation: 0.30585061432691457]
	TIME [epoch: 70.8 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30891835162937736		[learning rate: 0.0081524]
	Learning Rate: 0.00815242
	LOSS [training: 0.30891835162937736 | validation: 0.2803560585252293]
	TIME [epoch: 70.8 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28919221461783073		[learning rate: 0.0081362]
	Learning Rate: 0.00813618
	LOSS [training: 0.28919221461783073 | validation: 0.29504015636462066]
	TIME [epoch: 70.8 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2725930579801445		[learning rate: 0.00812]
	Learning Rate: 0.00811995
	LOSS [training: 0.2725930579801445 | validation: 0.31345205381567776]
	TIME [epoch: 70.8 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37954846935700753		[learning rate: 0.0081037]
	Learning Rate: 0.00810372
	LOSS [training: 0.37954846935700753 | validation: 0.33938921138787836]
	TIME [epoch: 70.8 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2974962102968483		[learning rate: 0.0080875]
	Learning Rate: 0.0080875
	LOSS [training: 0.2974962102968483 | validation: 0.2818113191417765]
	TIME [epoch: 70.8 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2791654358229069		[learning rate: 0.0080713]
	Learning Rate: 0.00807128
	LOSS [training: 0.2791654358229069 | validation: 0.2813666569103332]
	TIME [epoch: 70.7 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29378714508804465		[learning rate: 0.0080551]
	Learning Rate: 0.00805507
	LOSS [training: 0.29378714508804465 | validation: 0.34426854786435085]
	TIME [epoch: 70.8 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31976559509212055		[learning rate: 0.0080389]
	Learning Rate: 0.00803887
	LOSS [training: 0.31976559509212055 | validation: 0.3083578025411278]
	TIME [epoch: 70.7 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3041272326251195		[learning rate: 0.0080227]
	Learning Rate: 0.00802267
	LOSS [training: 0.3041272326251195 | validation: 0.31855300865383274]
	TIME [epoch: 70.8 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2806520522569067		[learning rate: 0.0080065]
	Learning Rate: 0.00800647
	LOSS [training: 0.2806520522569067 | validation: 0.2773050389629529]
	TIME [epoch: 70.9 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3133729227352288		[learning rate: 0.0079903]
	Learning Rate: 0.00799028
	LOSS [training: 0.3133729227352288 | validation: 0.29575265969184056]
	TIME [epoch: 70.8 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2713105875929882		[learning rate: 0.0079741]
	Learning Rate: 0.0079741
	LOSS [training: 0.2713105875929882 | validation: 0.33806164595598875]
	TIME [epoch: 70.8 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3112018778429304		[learning rate: 0.0079579]
	Learning Rate: 0.00795792
	LOSS [training: 0.3112018778429304 | validation: 0.2731701246269923]
	TIME [epoch: 70.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_1125.pth
	Model improved!!!
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27823334406410294		[learning rate: 0.0079417]
	Learning Rate: 0.00794174
	LOSS [training: 0.27823334406410294 | validation: 0.32499663072074747]
	TIME [epoch: 70.7 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31166653491762547		[learning rate: 0.0079256]
	Learning Rate: 0.00792558
	LOSS [training: 0.31166653491762547 | validation: 0.27995823681540855]
	TIME [epoch: 70.8 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32838538674458706		[learning rate: 0.0079094]
	Learning Rate: 0.00790941
	LOSS [training: 0.32838538674458706 | validation: 0.3321732406970403]
	TIME [epoch: 70.8 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27755847599543565		[learning rate: 0.0078933]
	Learning Rate: 0.00789326
	LOSS [training: 0.27755847599543565 | validation: 0.30703639655526627]
	TIME [epoch: 70.8 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29403698608898315		[learning rate: 0.0078771]
	Learning Rate: 0.00787711
	LOSS [training: 0.29403698608898315 | validation: 0.3043360388347195]
	TIME [epoch: 70.8 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26817576475805327		[learning rate: 0.007861]
	Learning Rate: 0.00786096
	LOSS [training: 0.26817576475805327 | validation: 0.3760794247047951]
	TIME [epoch: 70.7 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3500686583478373		[learning rate: 0.0078448]
	Learning Rate: 0.00784482
	LOSS [training: 0.3500686583478373 | validation: 0.3150104373749095]
	TIME [epoch: 70.8 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2910196734003895		[learning rate: 0.0078287]
	Learning Rate: 0.00782869
	LOSS [training: 0.2910196734003895 | validation: 0.2822751057812698]
	TIME [epoch: 70.7 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2727645619709692		[learning rate: 0.0078126]
	Learning Rate: 0.00781256
	LOSS [training: 0.2727645619709692 | validation: 0.32854592444223696]
	TIME [epoch: 70.8 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34596199584779697		[learning rate: 0.0077964]
	Learning Rate: 0.00779644
	LOSS [training: 0.34596199584779697 | validation: 0.3120756970068371]
	TIME [epoch: 70.8 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29452000642610127		[learning rate: 0.0077803]
	Learning Rate: 0.00778033
	LOSS [training: 0.29452000642610127 | validation: 0.2794633217035712]
	TIME [epoch: 70.7 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31489265883049017		[learning rate: 0.0077642]
	Learning Rate: 0.00776422
	LOSS [training: 0.31489265883049017 | validation: 0.30704362853195977]
	TIME [epoch: 70.8 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.272821273713177		[learning rate: 0.0077481]
	Learning Rate: 0.00774812
	LOSS [training: 0.272821273713177 | validation: 0.262390070800872]
	TIME [epoch: 70.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_1138.pth
	Model improved!!!
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29097732870334597		[learning rate: 0.007732]
	Learning Rate: 0.00773202
	LOSS [training: 0.29097732870334597 | validation: 0.34132732924476006]
	TIME [epoch: 70.8 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28121692823151		[learning rate: 0.0077159]
	Learning Rate: 0.00771593
	LOSS [training: 0.28121692823151 | validation: 0.28169021787674825]
	TIME [epoch: 70.8 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3098935264090732		[learning rate: 0.0076998]
	Learning Rate: 0.00769985
	LOSS [training: 0.3098935264090732 | validation: 0.3524241064513083]
	TIME [epoch: 70.7 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.319594040133339		[learning rate: 0.0076838]
	Learning Rate: 0.00768377
	LOSS [training: 0.319594040133339 | validation: 0.3234059888607785]
	TIME [epoch: 70.7 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28469708283639017		[learning rate: 0.0076677]
	Learning Rate: 0.0076677
	LOSS [training: 0.28469708283639017 | validation: 0.27618360828307864]
	TIME [epoch: 70.8 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3068038691381635		[learning rate: 0.0076516]
	Learning Rate: 0.00765163
	LOSS [training: 0.3068038691381635 | validation: 0.32008071639666624]
	TIME [epoch: 70.8 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27993937536935964		[learning rate: 0.0076356]
	Learning Rate: 0.00763557
	LOSS [training: 0.27993937536935964 | validation: 0.296067903018455]
	TIME [epoch: 70.8 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28961529507196326		[learning rate: 0.0076195]
	Learning Rate: 0.00761952
	LOSS [training: 0.28961529507196326 | validation: 0.29868568194688044]
	TIME [epoch: 70.8 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2992272858288608		[learning rate: 0.0076035]
	Learning Rate: 0.00760347
	LOSS [training: 0.2992272858288608 | validation: 0.2961160335766719]
	TIME [epoch: 70.8 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2915110250951926		[learning rate: 0.0075874]
	Learning Rate: 0.00758744
	LOSS [training: 0.2915110250951926 | validation: 0.30385429546514153]
	TIME [epoch: 70.9 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29568862334203283		[learning rate: 0.0075714]
	Learning Rate: 0.0075714
	LOSS [training: 0.29568862334203283 | validation: 0.2936730323410913]
	TIME [epoch: 70.8 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30324198049131734		[learning rate: 0.0075554]
	Learning Rate: 0.00755538
	LOSS [training: 0.30324198049131734 | validation: 0.33289059352613787]
	TIME [epoch: 70.8 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2974254924563013		[learning rate: 0.0075394]
	Learning Rate: 0.00753936
	LOSS [training: 0.2974254924563013 | validation: 0.27695921524973016]
	TIME [epoch: 70.8 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26521831500344123		[learning rate: 0.0075233]
	Learning Rate: 0.00752335
	LOSS [training: 0.26521831500344123 | validation: 0.2943677972863177]
	TIME [epoch: 70.8 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3324220138320019		[learning rate: 0.0075073]
	Learning Rate: 0.00750734
	LOSS [training: 0.3324220138320019 | validation: 0.3545166633994345]
	TIME [epoch: 70.8 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28687877009728463		[learning rate: 0.0074913]
	Learning Rate: 0.00749134
	LOSS [training: 0.28687877009728463 | validation: 0.28615072925074503]
	TIME [epoch: 70.9 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26100228112040386		[learning rate: 0.0074754]
	Learning Rate: 0.00747535
	LOSS [training: 0.26100228112040386 | validation: 0.30189504854779536]
	TIME [epoch: 70.8 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.335320832432217		[learning rate: 0.0074594]
	Learning Rate: 0.00745937
	LOSS [training: 0.335320832432217 | validation: 0.2930998845839959]
	TIME [epoch: 70.8 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27870216358636785		[learning rate: 0.0074434]
	Learning Rate: 0.00744339
	LOSS [training: 0.27870216358636785 | validation: 0.29235021533591576]
	TIME [epoch: 70.9 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2890915105816686		[learning rate: 0.0074274]
	Learning Rate: 0.00742742
	LOSS [training: 0.2890915105816686 | validation: 0.29370764037536173]
	TIME [epoch: 70.9 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2982214979777238		[learning rate: 0.0074115]
	Learning Rate: 0.00741145
	LOSS [training: 0.2982214979777238 | validation: 0.2850722951058175]
	TIME [epoch: 70.8 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27725477320932357		[learning rate: 0.0073955]
	Learning Rate: 0.0073955
	LOSS [training: 0.27725477320932357 | validation: 0.27536655183772235]
	TIME [epoch: 70.8 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29254043880850245		[learning rate: 0.0073795]
	Learning Rate: 0.00737955
	LOSS [training: 0.29254043880850245 | validation: 0.29381653441114575]
	TIME [epoch: 70.8 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27869387358844155		[learning rate: 0.0073636]
	Learning Rate: 0.0073636
	LOSS [training: 0.27869387358844155 | validation: 0.3168557742930614]
	TIME [epoch: 70.8 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29627450027595825		[learning rate: 0.0073477]
	Learning Rate: 0.00734767
	LOSS [training: 0.29627450027595825 | validation: 0.26640362432622583]
	TIME [epoch: 70.9 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30199691612464274		[learning rate: 0.0073317]
	Learning Rate: 0.00733174
	LOSS [training: 0.30199691612464274 | validation: 0.32064554998091876]
	TIME [epoch: 70.9 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26862000129184765		[learning rate: 0.0073158]
	Learning Rate: 0.00731582
	LOSS [training: 0.26862000129184765 | validation: 0.32877703282232207]
	TIME [epoch: 70.8 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3008133832651818		[learning rate: 0.0072999]
	Learning Rate: 0.00729991
	LOSS [training: 0.3008133832651818 | validation: 0.2787017062984574]
	TIME [epoch: 70.8 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2951077616941696		[learning rate: 0.007284]
	Learning Rate: 0.007284
	LOSS [training: 0.2951077616941696 | validation: 0.3301595329606537]
	TIME [epoch: 70.8 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28753834670524653		[learning rate: 0.0072681]
	Learning Rate: 0.0072681
	LOSS [training: 0.28753834670524653 | validation: 0.27224133521164817]
	TIME [epoch: 70.8 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3150960602244083		[learning rate: 0.0072522]
	Learning Rate: 0.00725221
	LOSS [training: 0.3150960602244083 | validation: 0.3017669645625448]
	TIME [epoch: 70.8 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2637322559853008		[learning rate: 0.0072363]
	Learning Rate: 0.00723633
	LOSS [training: 0.2637322559853008 | validation: 0.28187307384617327]
	TIME [epoch: 70.8 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3258975098805549		[learning rate: 0.0072205]
	Learning Rate: 0.00722045
	LOSS [training: 0.3258975098805549 | validation: 0.32938018871991204]
	TIME [epoch: 70.8 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28302670105872746		[learning rate: 0.0072046]
	Learning Rate: 0.00720458
	LOSS [training: 0.28302670105872746 | validation: 0.27941118542705334]
	TIME [epoch: 70.8 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.278612458084386		[learning rate: 0.0071887]
	Learning Rate: 0.00718872
	LOSS [training: 0.278612458084386 | validation: 0.3159847342202463]
	TIME [epoch: 70.8 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2808180409697669		[learning rate: 0.0071729]
	Learning Rate: 0.00717287
	LOSS [training: 0.2808180409697669 | validation: 0.3773153040630045]
	TIME [epoch: 70.8 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34502769851485277		[learning rate: 0.007157]
	Learning Rate: 0.00715702
	LOSS [training: 0.34502769851485277 | validation: 0.29640989341942797]
	TIME [epoch: 70.8 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2657237903172812		[learning rate: 0.0071412]
	Learning Rate: 0.00714119
	LOSS [training: 0.2657237903172812 | validation: 0.2629014347826817]
	TIME [epoch: 70.8 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3016195045259312		[learning rate: 0.0071254]
	Learning Rate: 0.00712536
	LOSS [training: 0.3016195045259312 | validation: 0.349844000691494]
	TIME [epoch: 70.8 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2796496343115822		[learning rate: 0.0071095]
	Learning Rate: 0.00710954
	LOSS [training: 0.2796496343115822 | validation: 0.27930345272515605]
	TIME [epoch: 70.8 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.285950773926878		[learning rate: 0.0070937]
	Learning Rate: 0.00709372
	LOSS [training: 0.285950773926878 | validation: 0.29619139162241925]
	TIME [epoch: 70.8 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27466314069432257		[learning rate: 0.0070779]
	Learning Rate: 0.00707792
	LOSS [training: 0.27466314069432257 | validation: 0.2995466908243345]
	TIME [epoch: 70.8 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3010211636014417		[learning rate: 0.0070621]
	Learning Rate: 0.00706212
	LOSS [training: 0.3010211636014417 | validation: 0.2975485197963089]
	TIME [epoch: 70.8 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33092934199497237		[learning rate: 0.0070463]
	Learning Rate: 0.00704633
	LOSS [training: 0.33092934199497237 | validation: 0.29937329370464516]
	TIME [epoch: 70.9 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2609541357142765		[learning rate: 0.0070305]
	Learning Rate: 0.00703055
	LOSS [training: 0.2609541357142765 | validation: 0.2655523170348139]
	TIME [epoch: 70.8 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.314363762478736		[learning rate: 0.0070148]
	Learning Rate: 0.00701477
	LOSS [training: 0.314363762478736 | validation: 0.31040199920203077]
	TIME [epoch: 70.8 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2811974834117885		[learning rate: 0.006999]
	Learning Rate: 0.00699901
	LOSS [training: 0.2811974834117885 | validation: 0.303275837485528]
	TIME [epoch: 70.8 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2784530635145965		[learning rate: 0.0069833]
	Learning Rate: 0.00698325
	LOSS [training: 0.2784530635145965 | validation: 0.26368563282750845]
	TIME [epoch: 70.8 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2676681843545463		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.2676681843545463 | validation: 0.3120327528096719]
	TIME [epoch: 70.8 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2810110429754414		[learning rate: 0.0069518]
	Learning Rate: 0.00695176
	LOSS [training: 0.2810110429754414 | validation: 0.33538296813306634]
	TIME [epoch: 70.9 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30305545206924783		[learning rate: 0.006936]
	Learning Rate: 0.00693603
	LOSS [training: 0.30305545206924783 | validation: 0.3043556914791354]
	TIME [epoch: 70.8 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.286489284759363		[learning rate: 0.0069203]
	Learning Rate: 0.0069203
	LOSS [training: 0.286489284759363 | validation: 0.31107069557628675]
	TIME [epoch: 70.8 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29316908848707757		[learning rate: 0.0069046]
	Learning Rate: 0.00690459
	LOSS [training: 0.29316908848707757 | validation: 0.35640104381020443]
	TIME [epoch: 70.8 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28514394633012496		[learning rate: 0.0068889]
	Learning Rate: 0.00688888
	LOSS [training: 0.28514394633012496 | validation: 0.29118614308961355]
	TIME [epoch: 70.7 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.281303435932309		[learning rate: 0.0068732]
	Learning Rate: 0.00687318
	LOSS [training: 0.281303435932309 | validation: 0.33535257670457386]
	TIME [epoch: 70.8 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3153669298661371		[learning rate: 0.0068575]
	Learning Rate: 0.00685749
	LOSS [training: 0.3153669298661371 | validation: 0.31050415313370144]
	TIME [epoch: 70.9 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26734369047229356		[learning rate: 0.0068418]
	Learning Rate: 0.00684181
	LOSS [training: 0.26734369047229356 | validation: 0.2816981091617175]
	TIME [epoch: 70.8 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2883631227579062		[learning rate: 0.0068261]
	Learning Rate: 0.00682614
	LOSS [training: 0.2883631227579062 | validation: 0.36649691402440565]
	TIME [epoch: 70.8 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30978538154174134		[learning rate: 0.0068105]
	Learning Rate: 0.00681048
	LOSS [training: 0.30978538154174134 | validation: 0.2886234970039927]
	TIME [epoch: 70.8 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2701843945355373		[learning rate: 0.0067948]
	Learning Rate: 0.00679482
	LOSS [training: 0.2701843945355373 | validation: 0.29483460289884833]
	TIME [epoch: 70.8 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28222079992414545		[learning rate: 0.0067792]
	Learning Rate: 0.00677917
	LOSS [training: 0.28222079992414545 | validation: 0.3117383653899702]
	TIME [epoch: 70.8 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28607159972116464		[learning rate: 0.0067635]
	Learning Rate: 0.00676354
	LOSS [training: 0.28607159972116464 | validation: 0.2570235099250097]
	TIME [epoch: 70.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_1200.pth
	Model improved!!!
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2880880865441146		[learning rate: 0.0067479]
	Learning Rate: 0.00674791
	LOSS [training: 0.2880880865441146 | validation: 0.26781974397735503]
	TIME [epoch: 70.8 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28550894831267815		[learning rate: 0.0067323]
	Learning Rate: 0.00673229
	LOSS [training: 0.28550894831267815 | validation: 0.30560163603829194]
	TIME [epoch: 70.8 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2869358930169087		[learning rate: 0.0067167]
	Learning Rate: 0.00671668
	LOSS [training: 0.2869358930169087 | validation: 0.3060901434722928]
	TIME [epoch: 70.9 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3020840797543812		[learning rate: 0.0067011]
	Learning Rate: 0.00670108
	LOSS [training: 0.3020840797543812 | validation: 0.30606265895687845]
	TIME [epoch: 70.8 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2747931986401871		[learning rate: 0.0066855]
	Learning Rate: 0.00668548
	LOSS [training: 0.2747931986401871 | validation: 0.2598280183515093]
	TIME [epoch: 70.8 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27877435159565733		[learning rate: 0.0066699]
	Learning Rate: 0.0066699
	LOSS [training: 0.27877435159565733 | validation: 0.3509115481747952]
	TIME [epoch: 70.8 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30324680537413556		[learning rate: 0.0066543]
	Learning Rate: 0.00665432
	LOSS [training: 0.30324680537413556 | validation: 0.28113858675094117]
	TIME [epoch: 70.8 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27987466662618776		[learning rate: 0.0066388]
	Learning Rate: 0.00663876
	LOSS [training: 0.27987466662618776 | validation: 0.26420944621360265]
	TIME [epoch: 70.8 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2836704120893596		[learning rate: 0.0066232]
	Learning Rate: 0.0066232
	LOSS [training: 0.2836704120893596 | validation: 0.29259992336304935]
	TIME [epoch: 70.8 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2781290636387544		[learning rate: 0.0066077]
	Learning Rate: 0.00660765
	LOSS [training: 0.2781290636387544 | validation: 0.26530359533941605]
	TIME [epoch: 70.8 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28414563486044925		[learning rate: 0.0065921]
	Learning Rate: 0.00659212
	LOSS [training: 0.28414563486044925 | validation: 0.33562272248332037]
	TIME [epoch: 70.8 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28491496146599066		[learning rate: 0.0065766]
	Learning Rate: 0.00657659
	LOSS [training: 0.28491496146599066 | validation: 0.2701057479645598]
	TIME [epoch: 70.9 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27455277375781656		[learning rate: 0.0065611]
	Learning Rate: 0.00656107
	LOSS [training: 0.27455277375781656 | validation: 0.26916653647263244]
	TIME [epoch: 70.8 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29432244906315036		[learning rate: 0.0065456]
	Learning Rate: 0.00654556
	LOSS [training: 0.29432244906315036 | validation: 0.28382243916360733]
	TIME [epoch: 70.8 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2755324289373066		[learning rate: 0.0065301]
	Learning Rate: 0.00653006
	LOSS [training: 0.2755324289373066 | validation: 0.2605351379321942]
	TIME [epoch: 70.8 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26669757869135013		[learning rate: 0.0065146]
	Learning Rate: 0.00651457
	LOSS [training: 0.26669757869135013 | validation: 0.3075929861917388]
	TIME [epoch: 70.8 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2853700679190946		[learning rate: 0.0064991]
	Learning Rate: 0.00649909
	LOSS [training: 0.2853700679190946 | validation: 0.28450626944130897]
	TIME [epoch: 70.8 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2782352989220052		[learning rate: 0.0064836]
	Learning Rate: 0.00648362
	LOSS [training: 0.2782352989220052 | validation: 0.2787558789539634]
	TIME [epoch: 70.8 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.286472981189506		[learning rate: 0.0064682]
	Learning Rate: 0.00646815
	LOSS [training: 0.286472981189506 | validation: 0.309863051186392]
	TIME [epoch: 70.8 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2831805595651612		[learning rate: 0.0064527]
	Learning Rate: 0.0064527
	LOSS [training: 0.2831805595651612 | validation: 0.2998777100178389]
	TIME [epoch: 70.8 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29178944670739315		[learning rate: 0.0064373]
	Learning Rate: 0.00643726
	LOSS [training: 0.29178944670739315 | validation: 0.2690172263602363]
	TIME [epoch: 70.8 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2630227675732106		[learning rate: 0.0064218]
	Learning Rate: 0.00642183
	LOSS [training: 0.2630227675732106 | validation: 0.2995756955790302]
	TIME [epoch: 70.8 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28452325922180655		[learning rate: 0.0064064]
	Learning Rate: 0.0064064
	LOSS [training: 0.28452325922180655 | validation: 0.3083455612171741]
	TIME [epoch: 70.8 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.305641649585384		[learning rate: 0.006391]
	Learning Rate: 0.00639099
	LOSS [training: 0.305641649585384 | validation: 0.29101218469509754]
	TIME [epoch: 70.9 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26259283347221707		[learning rate: 0.0063756]
	Learning Rate: 0.00637559
	LOSS [training: 0.26259283347221707 | validation: 0.26093717037532504]
	TIME [epoch: 70.8 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2722440096912177		[learning rate: 0.0063602]
	Learning Rate: 0.00636019
	LOSS [training: 0.2722440096912177 | validation: 0.34453612494675]
	TIME [epoch: 70.9 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3093441658530167		[learning rate: 0.0063448]
	Learning Rate: 0.00634481
	LOSS [training: 0.3093441658530167 | validation: 0.27596176853431886]
	TIME [epoch: 70.8 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2646470599002428		[learning rate: 0.0063294]
	Learning Rate: 0.00632944
	LOSS [training: 0.2646470599002428 | validation: 0.2971165184732816]
	TIME [epoch: 70.9 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29089912495085524		[learning rate: 0.0063141]
	Learning Rate: 0.00631407
	LOSS [training: 0.29089912495085524 | validation: 0.2925841227294613]
	TIME [epoch: 70.8 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2709225227173791		[learning rate: 0.0062987]
	Learning Rate: 0.00629872
	LOSS [training: 0.2709225227173791 | validation: 0.2846130834618471]
	TIME [epoch: 70.8 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2906627607207078		[learning rate: 0.0062834]
	Learning Rate: 0.00628338
	LOSS [training: 0.2906627607207078 | validation: 0.27858458171492423]
	TIME [epoch: 70.8 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2927971389324934		[learning rate: 0.006268]
	Learning Rate: 0.00626804
	LOSS [training: 0.2927971389324934 | validation: 0.28100096866601393]
	TIME [epoch: 70.8 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28146032274316013		[learning rate: 0.0062527]
	Learning Rate: 0.00625272
	LOSS [training: 0.28146032274316013 | validation: 0.2896622482843312]
	TIME [epoch: 70.8 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26576437103217676		[learning rate: 0.0062374]
	Learning Rate: 0.00623741
	LOSS [training: 0.26576437103217676 | validation: 0.2705577932351756]
	TIME [epoch: 70.8 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26867011256562673		[learning rate: 0.0062221]
	Learning Rate: 0.00622211
	LOSS [training: 0.26867011256562673 | validation: 0.2540662819666155]
	TIME [epoch: 70.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_1235.pth
	Model improved!!!
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2899346283197698		[learning rate: 0.0062068]
	Learning Rate: 0.00620681
	LOSS [training: 0.2899346283197698 | validation: 0.3082955078206917]
	TIME [epoch: 70.8 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2760056147754183		[learning rate: 0.0061915]
	Learning Rate: 0.00619153
	LOSS [training: 0.2760056147754183 | validation: 0.2949761349179173]
	TIME [epoch: 70.8 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27868431286293166		[learning rate: 0.0061763]
	Learning Rate: 0.00617626
	LOSS [training: 0.27868431286293166 | validation: 0.2734190348540111]
	TIME [epoch: 70.9 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2682233356109341		[learning rate: 0.006161]
	Learning Rate: 0.006161
	LOSS [training: 0.2682233356109341 | validation: 0.32947103025378366]
	TIME [epoch: 70.9 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30484046259213415		[learning rate: 0.0061458]
	Learning Rate: 0.00614575
	LOSS [training: 0.30484046259213415 | validation: 0.262173114710992]
	TIME [epoch: 70.8 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25627450507466043		[learning rate: 0.0061305]
	Learning Rate: 0.00613051
	LOSS [training: 0.25627450507466043 | validation: 0.3352363633538449]
	TIME [epoch: 70.8 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3088316215823018		[learning rate: 0.0061153]
	Learning Rate: 0.00611528
	LOSS [training: 0.3088316215823018 | validation: 0.3066796354053929]
	TIME [epoch: 70.8 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26450167622467746		[learning rate: 0.0061001]
	Learning Rate: 0.00610007
	LOSS [training: 0.26450167622467746 | validation: 0.264541267273904]
	TIME [epoch: 70.7 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3011728140987338		[learning rate: 0.0060849]
	Learning Rate: 0.00608486
	LOSS [training: 0.3011728140987338 | validation: 0.2934166686985429]
	TIME [epoch: 70.8 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26578635716605087		[learning rate: 0.0060697]
	Learning Rate: 0.00606966
	LOSS [training: 0.26578635716605087 | validation: 0.26454416730528846]
	TIME [epoch: 70.8 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2651255308432628		[learning rate: 0.0060545]
	Learning Rate: 0.00605447
	LOSS [training: 0.2651255308432628 | validation: 0.3001005545343536]
	TIME [epoch: 70.8 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28929898923499686		[learning rate: 0.0060393]
	Learning Rate: 0.0060393
	LOSS [training: 0.28929898923499686 | validation: 0.2964598692256414]
	TIME [epoch: 70.8 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2773416229798049		[learning rate: 0.0060241]
	Learning Rate: 0.00602413
	LOSS [training: 0.2773416229798049 | validation: 0.2700538798889308]
	TIME [epoch: 70.9 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27626982950430784		[learning rate: 0.006009]
	Learning Rate: 0.00600898
	LOSS [training: 0.27626982950430784 | validation: 0.27706617597321487]
	TIME [epoch: 70.8 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2657600390278699		[learning rate: 0.0059938]
	Learning Rate: 0.00599384
	LOSS [training: 0.2657600390278699 | validation: 0.2537636263670062]
	TIME [epoch: 70.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_1250.pth
	Model improved!!!
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2697484213128619		[learning rate: 0.0059787]
	Learning Rate: 0.00597871
	LOSS [training: 0.2697484213128619 | validation: 0.2833430991564751]
	TIME [epoch: 70.8 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2734289705078431		[learning rate: 0.0059636]
	Learning Rate: 0.00596359
	LOSS [training: 0.2734289705078431 | validation: 0.31265206904123555]
	TIME [epoch: 70.8 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2898223690764054		[learning rate: 0.0059485]
	Learning Rate: 0.00594848
	LOSS [training: 0.2898223690764054 | validation: 0.28368407156088216]
	TIME [epoch: 70.8 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27211120533697436		[learning rate: 0.0059334]
	Learning Rate: 0.00593338
	LOSS [training: 0.27211120533697436 | validation: 0.27217711036119563]
	TIME [epoch: 70.9 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2694990522853702		[learning rate: 0.0059183]
	Learning Rate: 0.00591829
	LOSS [training: 0.2694990522853702 | validation: 0.26795330564332204]
	TIME [epoch: 70.8 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2681344847548561		[learning rate: 0.0059032]
	Learning Rate: 0.00590321
	LOSS [training: 0.2681344847548561 | validation: 0.3153851844802833]
	TIME [epoch: 70.8 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2717208544023476		[learning rate: 0.0058881]
	Learning Rate: 0.00588815
	LOSS [training: 0.2717208544023476 | validation: 0.2700007446126278]
	TIME [epoch: 70.9 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28744144180229325		[learning rate: 0.0058731]
	Learning Rate: 0.00587309
	LOSS [training: 0.28744144180229325 | validation: 0.3215536599604494]
	TIME [epoch: 70.8 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2953276518176361		[learning rate: 0.0058581]
	Learning Rate: 0.00585805
	LOSS [training: 0.2953276518176361 | validation: 0.2680376593782221]
	TIME [epoch: 70.8 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2500251610187771		[learning rate: 0.005843]
	Learning Rate: 0.00584302
	LOSS [training: 0.2500251610187771 | validation: 0.26281335837136]
	TIME [epoch: 70.8 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30118995476728655		[learning rate: 0.005828]
	Learning Rate: 0.005828
	LOSS [training: 0.30118995476728655 | validation: 0.2707243837540308]
	TIME [epoch: 70.8 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26247811819701866		[learning rate: 0.005813]
	Learning Rate: 0.00581299
	LOSS [training: 0.26247811819701866 | validation: 0.29626195938556]
	TIME [epoch: 70.8 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29226113200351334		[learning rate: 0.005798]
	Learning Rate: 0.005798
	LOSS [training: 0.29226113200351334 | validation: 0.2987683061619373]
	TIME [epoch: 70.9 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2668177043615284		[learning rate: 0.005783]
	Learning Rate: 0.00578301
	LOSS [training: 0.2668177043615284 | validation: 0.2696139919945324]
	TIME [epoch: 70.8 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2726436895034274		[learning rate: 0.005768]
	Learning Rate: 0.00576804
	LOSS [training: 0.2726436895034274 | validation: 0.2686557183016127]
	TIME [epoch: 70.8 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.280298175065008		[learning rate: 0.0057531]
	Learning Rate: 0.00575307
	LOSS [training: 0.280298175065008 | validation: 0.2685953162420695]
	TIME [epoch: 70.9 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26439461193759234		[learning rate: 0.0057381]
	Learning Rate: 0.00573812
	LOSS [training: 0.26439461193759234 | validation: 0.3117760624626872]
	TIME [epoch: 70.8 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2807087699486633		[learning rate: 0.0057232]
	Learning Rate: 0.00572318
	LOSS [training: 0.2807087699486633 | validation: 0.2579311426825895]
	TIME [epoch: 70.8 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26700327494610476		[learning rate: 0.0057083]
	Learning Rate: 0.00570826
	LOSS [training: 0.26700327494610476 | validation: 0.3161973504382678]
	TIME [epoch: 70.9 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27634464021630367		[learning rate: 0.0056933]
	Learning Rate: 0.00569334
	LOSS [training: 0.27634464021630367 | validation: 0.25303521825058695]
	TIME [epoch: 70.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_1270.pth
	Model improved!!!
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2754909883889979		[learning rate: 0.0056784]
	Learning Rate: 0.00567844
	LOSS [training: 0.2754909883889979 | validation: 0.3213682341786309]
	TIME [epoch: 70.8 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28376157882155933		[learning rate: 0.0056635]
	Learning Rate: 0.00566355
	LOSS [training: 0.28376157882155933 | validation: 0.25991880103586323]
	TIME [epoch: 70.9 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2668912901142324		[learning rate: 0.0056487]
	Learning Rate: 0.00564867
	LOSS [training: 0.2668912901142324 | validation: 0.3199868365772829]
	TIME [epoch: 70.9 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2698465125286793		[learning rate: 0.0056338]
	Learning Rate: 0.0056338
	LOSS [training: 0.2698465125286793 | validation: 0.25118329639155157]
	TIME [epoch: 70.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd4_20240713_103448/states/model_phiq_1a_v_mmd4_1274.pth
	Model improved!!!
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25864342372347604		[learning rate: 0.0056189]
	Learning Rate: 0.00561894
	LOSS [training: 0.25864342372347604 | validation: 0.3546706792623219]
	TIME [epoch: 70.8 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2840322926696581		[learning rate: 0.0056041]
	Learning Rate: 0.0056041
	LOSS [training: 0.2840322926696581 | validation: 0.27227411438598187]
	TIME [epoch: 70.9 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2759090808778503		[learning rate: 0.0055893]
	Learning Rate: 0.00558927
	LOSS [training: 0.2759090808778503 | validation: 0.28669752874583404]
	TIME [epoch: 70.8 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2583865731340468		[learning rate: 0.0055744]
	Learning Rate: 0.00557445
	LOSS [training: 0.2583865731340468 | validation: 0.2621421614457043]
	TIME [epoch: 70.8 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2615242002266923		[learning rate: 0.0055596]
	Learning Rate: 0.00555964
	LOSS [training: 0.2615242002266923 | validation: 0.25611126796949873]
	TIME [epoch: 70.8 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2794083249687491		[learning rate: 0.0055448]
	Learning Rate: 0.00554484
	LOSS [training: 0.2794083249687491 | validation: 0.2721061249953226]
	TIME [epoch: 70.8 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2573803655221204		[learning rate: 0.0055301]
	Learning Rate: 0.00553006
	LOSS [training: 0.2573803655221204 | validation: 0.25220428649322635]
	TIME [epoch: 70.8 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27950063349804877		[learning rate: 0.0055153]
	Learning Rate: 0.00551529
	LOSS [training: 0.27950063349804877 | validation: 0.2771455267588968]
	TIME [epoch: 70.8 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.272791298805217		[learning rate: 0.0055005]
	Learning Rate: 0.00550053
	LOSS [training: 0.272791298805217 | validation: 0.2989150932186077]
	TIME [epoch: 70.8 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2709011896663267		[learning rate: 0.0054858]
	Learning Rate: 0.00548578
	LOSS [training: 0.2709011896663267 | validation: 0.31066870991594386]
	TIME [epoch: 70.8 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26235992760007615		[learning rate: 0.005471]
	Learning Rate: 0.00547105
	LOSS [training: 0.26235992760007615 | validation: 0.2641182965481683]
	TIME [epoch: 70.8 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29957589539135654		[learning rate: 0.0054563]
	Learning Rate: 0.00545632
	LOSS [training: 0.29957589539135654 | validation: 0.3111296221177152]
	TIME [epoch: 70.9 sec]
EPOCH 1287/2000:
	Training over batches...
