Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1484755978

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.820373696787891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.820373696787891 | validation: 5.669047498418107]
	TIME [epoch: 44.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.6065879949796384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.6065879949796384 | validation: 5.4302199701944724]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.815457599174133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.815457599174133 | validation: 5.934476417309367]
	TIME [epoch: 1.83 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.918576703411076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.918576703411076 | validation: 6.842385932475046]
	TIME [epoch: 1.82 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.84116808409181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.84116808409181 | validation: 5.443978223447805]
	TIME [epoch: 1.82 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.245359514767677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.245359514767677 | validation: 4.52590604897935]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.706172487700187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.706172487700187 | validation: 4.634745176907984]
	TIME [epoch: 1.83 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.67949180907892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.67949180907892 | validation: 4.419780085226073]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.540154873843647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.540154873843647 | validation: 5.0184486646678765]
	TIME [epoch: 1.83 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.686277790306792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.686277790306792 | validation: 4.333516416697157]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.429447103055619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.429447103055619 | validation: 4.412585769772346]
	TIME [epoch: 1.83 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.353518460812352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.353518460812352 | validation: 4.431581186049295]
	TIME [epoch: 1.83 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.281535933432596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.281535933432596 | validation: 4.407304352051868]
	TIME [epoch: 1.83 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.253816596852149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.253816596852149 | validation: 4.384917774111407]
	TIME [epoch: 1.84 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.222169782709736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.222169782709736 | validation: 4.236982025440009]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.2349027906407475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2349027906407475 | validation: 4.589163691651039]
	TIME [epoch: 1.84 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.318396755629777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.318396755629777 | validation: 4.190672973397409]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.307011343831783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.307011343831783 | validation: 4.2809226702578185]
	TIME [epoch: 1.82 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.097274959638195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.097274959638195 | validation: 4.281027596281605]
	TIME [epoch: 1.82 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.070414443584284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.070414443584284 | validation: 4.11817402251438]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0602975149924685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0602975149924685 | validation: 4.304187399376254]
	TIME [epoch: 1.83 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0630163955791065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0630163955791065 | validation: 4.110358730329497]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.120989675437391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.120989675437391 | validation: 4.296583734920007]
	TIME [epoch: 1.84 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.083257555740375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.083257555740375 | validation: 4.0870694702033274]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0736372257156965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0736372257156965 | validation: 4.152211936371711]
	TIME [epoch: 1.83 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9684464683458924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9684464683458924 | validation: 4.035142742511706]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9333386023644112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9333386023644112 | validation: 4.106478383845842]
	TIME [epoch: 1.82 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.914287010913841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.914287010913841 | validation: 3.951381532287243]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9275918110809758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9275918110809758 | validation: 4.236103710336518]
	TIME [epoch: 1.82 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9765894013146488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9765894013146488 | validation: 3.9369350812371153]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.08145888612489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.08145888612489 | validation: 4.015021020481531]
	TIME [epoch: 1.83 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.84147250625731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.84147250625731 | validation: 4.015327557373427]
	TIME [epoch: 1.83 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8289799820890615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8289799820890615 | validation: 3.863632042826893]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.866422763602962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.866422763602962 | validation: 4.047016940812028]
	TIME [epoch: 1.83 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8571829238741895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8571829238741895 | validation: 3.9856440160733464]
	TIME [epoch: 1.83 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.929576146123803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.929576146123803 | validation: 3.909255289936607]
	TIME [epoch: 1.83 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8710566966713333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8710566966713333 | validation: 3.887758316979429]
	TIME [epoch: 1.83 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7458550929613468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7458550929613468 | validation: 3.8167380943435774]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.743282924222932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.743282924222932 | validation: 3.88514119165931]
	TIME [epoch: 1.84 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7418130501914972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7418130501914972 | validation: 3.7741278088004075]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7420759685685288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7420759685685288 | validation: 3.9070631843284573]
	TIME [epoch: 1.83 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.729718902205932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.729718902205932 | validation: 3.7039179209307194]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.75427950690824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.75427950690824 | validation: 3.856719501862604]
	TIME [epoch: 1.83 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.685515315386259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.685515315386259 | validation: 3.67418172551419]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.652080555127274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.652080555127274 | validation: 3.745348484887743]
	TIME [epoch: 1.83 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.620089162163164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.620089162163164 | validation: 3.66773477093972]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6023076076720804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6023076076720804 | validation: 3.7433047640157016]
	TIME [epoch: 1.82 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.625418673943768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.625418673943768 | validation: 3.8259762244617166]
	TIME [epoch: 1.82 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.788932481040198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.788932481040198 | validation: 3.7225306145801107]
	TIME [epoch: 1.82 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.688175683149848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.688175683149848 | validation: 3.674792325357922]
	TIME [epoch: 1.82 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5665394108255613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5665394108255613 | validation: 3.5943414916220124]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5542658062407075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5542658062407075 | validation: 3.651343095076838]
	TIME [epoch: 1.83 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5323066175662614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5323066175662614 | validation: 3.5779702495484034]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.510479755977258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.510479755977258 | validation: 3.5966205279364107]
	TIME [epoch: 1.83 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.493667742630399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.493667742630399 | validation: 3.5484090244251902]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.482692247532988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.482692247532988 | validation: 3.571184573052891]
	TIME [epoch: 1.83 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.484730255427063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.484730255427063 | validation: 3.5276707840341817]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.49966424475815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.49966424475815 | validation: 3.7101216804577635]
	TIME [epoch: 1.83 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5900338009379418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5900338009379418 | validation: 3.719073416558804]
	TIME [epoch: 1.83 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7223267218022364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7223267218022364 | validation: 3.494392187942849]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.527098594710352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.527098594710352 | validation: 3.635511626349514]
	TIME [epoch: 1.85 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.507651960995148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.507651960995148 | validation: 3.4779349247724505]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.412706592980072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.412706592980072 | validation: 3.463738038317347]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4301947667777775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4301947667777775 | validation: 3.470784871665792]
	TIME [epoch: 1.82 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3961979389695456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3961979389695456 | validation: 3.4570851591168665]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3917306393180557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3917306393180557 | validation: 3.421734785143666]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3752162112471376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3752162112471376 | validation: 3.415508044512862]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.374290390926152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.374290390926152 | validation: 3.456904429963477]
	TIME [epoch: 1.83 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.378349905604846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.378349905604846 | validation: 3.4566939154205514]
	TIME [epoch: 1.83 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4149725919686897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4149725919686897 | validation: 3.430694179757882]
	TIME [epoch: 1.83 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.382273375025862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.382273375025862 | validation: 3.4105568953732206]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.358239017415771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.358239017415771 | validation: 3.3701177147858568]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3132699928290545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3132699928290545 | validation: 3.3478956783725944]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.294835444343357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.294835444343357 | validation: 3.3397353108025984]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.287499297004854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.287499297004854 | validation: 3.3395374393016057]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2862187848087774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2862187848087774 | validation: 3.344037456921109]
	TIME [epoch: 1.83 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2863442194737047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2863442194737047 | validation: 3.3579525690299405]
	TIME [epoch: 1.83 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.335744581859674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.335744581859674 | validation: 3.5238323040165387]
	TIME [epoch: 1.83 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.461522333369585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.461522333369585 | validation: 3.3199168928181466]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.304275778671488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.304275778671488 | validation: 3.270550383079094]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2497357557408146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2497357557408146 | validation: 3.337896477823128]
	TIME [epoch: 1.84 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.283066351915263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.283066351915263 | validation: 3.2764722925074397]
	TIME [epoch: 1.83 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.245711656598111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.245711656598111 | validation: 3.263880721816856]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.224011864384254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.224011864384254 | validation: 3.2579154149447143]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2159273144436606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2159273144436606 | validation: 3.246874761030175]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.212082779185414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.212082779185414 | validation: 3.241140832897856]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2167895945412055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2167895945412055 | validation: 3.254742930171238]
	TIME [epoch: 1.82 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2175920977895784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2175920977895784 | validation: 3.2569656200690975]
	TIME [epoch: 1.82 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.248797838473007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.248797838473007 | validation: 3.2298299993430533]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2238479116995755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2238479116995755 | validation: 3.215573205034046]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.209535620110572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.209535620110572 | validation: 3.181858844375297]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.164839784290911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.164839784290911 | validation: 3.1684350355873416]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1518518838883036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1518518838883036 | validation: 3.168197629938348]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.137994189451826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.137994189451826 | validation: 3.149646281868189]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1258990824221717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1258990824221717 | validation: 3.1331847927294056]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.125079748069617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.125079748069617 | validation: 3.1414901879946875]
	TIME [epoch: 1.82 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.139284237389765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.139284237389765 | validation: 3.1225284758439082]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1140615737784167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1140615737784167 | validation: 2.9773979702315283]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0076242496160064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0076242496160064 | validation: 2.5416275544908826]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6313975909357863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6313975909357863 | validation: 1.9095306341648346]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9746779139374275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9746779139374275 | validation: 3.436878898284309]
	TIME [epoch: 1.84 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.671802680904806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.671802680904806 | validation: 2.1645894003113946]
	TIME [epoch: 1.82 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3159910773657777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3159910773657777 | validation: 2.613276847980561]
	TIME [epoch: 1.82 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.522961727690421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.522961727690421 | validation: 1.5487030281116627]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.554741701666316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.554741701666316 | validation: 1.5134149322859742]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5043541767717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5043541767717 | validation: 1.249482920699145]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2254554938634532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2254554938634532 | validation: 1.116546302527438]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1542551859942387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1542551859942387 | validation: 1.080279286536818]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.07345162638336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.07345162638336 | validation: 1.064850697963448]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0453815243970246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0453815243970246 | validation: 1.0341955106459373]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.017834307936039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.017834307936039 | validation: 1.0191248907061379]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9871971885937264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9871971885937264 | validation: 0.9950583155767533]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.970545629388247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.970545629388247 | validation: 0.9539058264424061]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9498120305805527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9498120305805527 | validation: 1.0142652016874565]
	TIME [epoch: 1.82 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9364097248627568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9364097248627568 | validation: 0.9266432968667998]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9155006515846733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9155006515846733 | validation: 0.9783291273539352]
	TIME [epoch: 1.82 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9082023115854603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9082023115854603 | validation: 0.9208420474516921]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9255520317038547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9255520317038547 | validation: 1.0468980056611124]
	TIME [epoch: 1.84 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9290292750064858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9290292750064858 | validation: 0.9419260646137471]
	TIME [epoch: 1.83 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9489851819879457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9489851819879457 | validation: 1.0292063100108877]
	TIME [epoch: 1.83 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8975770975289242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8975770975289242 | validation: 0.9011391679649411]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604570360791436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8604570360791436 | validation: 0.9010734473724957]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8400090940370569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8400090940370569 | validation: 0.8812954783586143]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8277619391723141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8277619391723141 | validation: 0.8586691505917816]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8143702219332377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8143702219332377 | validation: 0.926684077737108]
	TIME [epoch: 1.83 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8269955176279234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8269955176279234 | validation: 0.8587500747925335]
	TIME [epoch: 1.83 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8798079735109852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8798079735109852 | validation: 1.1645160703226654]
	TIME [epoch: 1.82 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9820243397690913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9820243397690913 | validation: 0.9411021293023327]
	TIME [epoch: 1.83 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.951013545259064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.951013545259064 | validation: 0.8679590010937268]
	TIME [epoch: 1.82 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8098517079309423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8098517079309423 | validation: 0.9861540862594591]
	TIME [epoch: 1.83 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8474657069636864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8474657069636864 | validation: 0.8313506356807983]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7946983694006678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7946983694006678 | validation: 0.872126433419674]
	TIME [epoch: 1.82 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7702876293872402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7702876293872402 | validation: 0.8129401246324659]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619446555073859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7619446555073859 | validation: 0.8573594921111568]
	TIME [epoch: 1.83 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7591508149134489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7591508149134489 | validation: 0.7913135156287504]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7634026478130824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7634026478130824 | validation: 0.9196199227675868]
	TIME [epoch: 1.82 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7870987034732798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7870987034732798 | validation: 0.8246188232989069]
	TIME [epoch: 1.83 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8191405264717085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8191405264717085 | validation: 1.0953391633433618]
	TIME [epoch: 1.82 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.895038416710639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.895038416710639 | validation: 0.7933152103725507]
	TIME [epoch: 1.82 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7544486627807812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7544486627807812 | validation: 0.858841282044309]
	TIME [epoch: 1.83 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7967378242903673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7967378242903673 | validation: 1.0652357069797902]
	TIME [epoch: 1.82 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9828411455535527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9828411455535527 | validation: 1.0521469130600105]
	TIME [epoch: 1.82 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8605045579529546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8605045579529546 | validation: 0.8187368514159632]
	TIME [epoch: 1.82 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7978634977055822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7978634977055822 | validation: 0.9206687499045935]
	TIME [epoch: 1.83 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7596337232052055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7596337232052055 | validation: 0.887785587868736]
	TIME [epoch: 1.84 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.755838694599615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.755838694599615 | validation: 0.8168791881517882]
	TIME [epoch: 1.83 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7669266483944651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7669266483944651 | validation: 0.8762194634606525]
	TIME [epoch: 1.83 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7375917693770668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7375917693770668 | validation: 0.8106366190327927]
	TIME [epoch: 1.83 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7135845797063207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7135845797063207 | validation: 0.8085416671031963]
	TIME [epoch: 1.82 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7069212642585945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7069212642585945 | validation: 0.8053037437851613]
	TIME [epoch: 1.83 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7049455728077177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7049455728077177 | validation: 0.802113627831038]
	TIME [epoch: 1.82 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7104763235236308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7104763235236308 | validation: 0.8000164920872023]
	TIME [epoch: 1.83 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7541722243056381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7541722243056381 | validation: 1.124906513947991]
	TIME [epoch: 1.83 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0338453596857557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0338453596857557 | validation: 1.0596727221490811]
	TIME [epoch: 1.83 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8380768824659075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8380768824659075 | validation: 0.8505478996397822]
	TIME [epoch: 1.83 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8585109000468447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8585109000468447 | validation: 1.0740139088292409]
	TIME [epoch: 1.83 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8016212216244222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8016212216244222 | validation: 0.8363339295461171]
	TIME [epoch: 1.83 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7149695588112809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7149695588112809 | validation: 0.7779466650122441]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7247327321967169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7247327321967169 | validation: 0.8800669967350712]
	TIME [epoch: 1.83 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7056186480959094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7056186480959094 | validation: 0.7628775359763182]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6987245246979658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6987245246979658 | validation: 0.7814636521945485]
	TIME [epoch: 1.82 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6850199405090891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6850199405090891 | validation: 0.7662621071073952]
	TIME [epoch: 1.82 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969221977185228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6969221977185228 | validation: 0.8585341877201828]
	TIME [epoch: 1.82 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7522895951767208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7522895951767208 | validation: 1.0888765042955575]
	TIME [epoch: 1.82 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9014114209303219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9014114209303219 | validation: 0.9498530541078289]
	TIME [epoch: 1.82 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8076765399021159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8076765399021159 | validation: 0.7502398461294485]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6879967863848873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6879967863848873 | validation: 0.8883140324668828]
	TIME [epoch: 1.82 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7009179632848768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7009179632848768 | validation: 0.7418204610595913]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7027827396963615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7027827396963615 | validation: 0.9369790819839928]
	TIME [epoch: 1.84 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7139342222407108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7139342222407108 | validation: 0.7331869603042761]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7127004089809102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7127004089809102 | validation: 1.0048888165433456]
	TIME [epoch: 1.83 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7400745005499733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7400745005499733 | validation: 0.7199149343746551]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7394632386270135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7394632386270135 | validation: 0.9521479561278536]
	TIME [epoch: 1.83 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7614239551996315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7614239551996315 | validation: 0.7714088393206087]
	TIME [epoch: 1.82 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7016934550505047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7016934550505047 | validation: 0.839958091891333]
	TIME [epoch: 1.83 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7891870196100947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7891870196100947 | validation: 1.1188377974946626]
	TIME [epoch: 1.83 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7788050203178781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7788050203178781 | validation: 0.8119265472602848]
	TIME [epoch: 1.83 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6733101831892777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6733101831892777 | validation: 0.7307234718925926]
	TIME [epoch: 1.83 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6759272205545529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6759272205545529 | validation: 0.8878267752606753]
	TIME [epoch: 1.83 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6960965420578596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6960965420578596 | validation: 0.7010829379436595]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6958464112996056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6958464112996056 | validation: 0.8727719573741294]
	TIME [epoch: 1.82 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7383725371418018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7383725371418018 | validation: 0.7932727405463843]
	TIME [epoch: 1.82 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7542091059144269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7542091059144269 | validation: 0.878481279024319]
	TIME [epoch: 1.82 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668420406581461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7668420406581461 | validation: 0.8988682972860547]
	TIME [epoch: 1.82 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6998133018628502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6998133018628502 | validation: 0.73954230645736]
	TIME [epoch: 1.82 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6315105089587363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6315105089587363 | validation: 0.7408189402833392]
	TIME [epoch: 1.83 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6198893452497909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6198893452497909 | validation: 0.7123605024004268]
	TIME [epoch: 1.83 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6262515623617885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6262515623617885 | validation: 0.7442800417539642]
	TIME [epoch: 1.82 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6430931101049483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6430931101049483 | validation: 0.7423135143972365]
	TIME [epoch: 1.83 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7819065876987377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7819065876987377 | validation: 1.0369750568824239]
	TIME [epoch: 1.82 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8879025289287259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8879025289287259 | validation: 0.8113347486103173]
	TIME [epoch: 1.83 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.662019144014366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.662019144014366 | validation: 0.6992193467631598]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429438128290331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6429438128290331 | validation: 0.8961759383409712]
	TIME [epoch: 1.83 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6606769314852402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6606769314852402 | validation: 0.6691457867587247]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6203684574631773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6203684574631773 | validation: 0.7194332602963984]
	TIME [epoch: 1.83 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5878251136415501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5878251136415501 | validation: 0.6420691142546506]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5801321575325377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5801321575325377 | validation: 0.722421891933645]
	TIME [epoch: 1.83 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5860050324168106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5860050324168106 | validation: 0.6835511493452233]
	TIME [epoch: 1.83 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6548431738912366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6548431738912366 | validation: 1.3252511447217088]
	TIME [epoch: 1.83 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8885393358247512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8885393358247512 | validation: 0.680245394021428]
	TIME [epoch: 1.83 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6352060838428342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6352060838428342 | validation: 0.7101651647113266]
	TIME [epoch: 43.7 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6029091766273549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6029091766273549 | validation: 0.794255797639001]
	TIME [epoch: 3.64 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5970130115876174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5970130115876174 | validation: 0.7627554277002191]
	TIME [epoch: 3.62 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6377337926969122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6377337926969122 | validation: 0.7127692603079849]
	TIME [epoch: 3.63 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880383875916399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7880383875916399 | validation: 0.7716795099296698]
	TIME [epoch: 3.62 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.587502057097847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.587502057097847 | validation: 0.6547392335935869]
	TIME [epoch: 3.62 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5159073208869027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5159073208869027 | validation: 0.5887717647923282]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49276588120138487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49276588120138487 | validation: 0.6255667339923842]
	TIME [epoch: 3.62 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5113414191013522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5113414191013522 | validation: 0.6518179008027356]
	TIME [epoch: 3.62 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6567775368828024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6567775368828024 | validation: 0.8192116221729482]
	TIME [epoch: 3.62 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6731803327126165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6731803327126165 | validation: 0.866572532991631]
	TIME [epoch: 3.62 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.555117559668888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.555117559668888 | validation: 0.5980084498499612]
	TIME [epoch: 3.63 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4484902482800653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4484902482800653 | validation: 0.5963417314381394]
	TIME [epoch: 3.64 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47854155111790325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47854155111790325 | validation: 0.7499043307575857]
	TIME [epoch: 3.64 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7202016760163134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7202016760163134 | validation: 0.7270240312529466]
	TIME [epoch: 3.63 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5546385533726381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5546385533726381 | validation: 0.6733007871785535]
	TIME [epoch: 3.62 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4311615243468107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4311615243468107 | validation: 0.504414690019417]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45748757812382324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45748757812382324 | validation: 1.0715337213291158]
	TIME [epoch: 3.6 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9671243174640448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9671243174640448 | validation: 0.757483125075985]
	TIME [epoch: 3.61 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6001080129882295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6001080129882295 | validation: 0.5002451195867664]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5030128147601753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5030128147601753 | validation: 0.8602702205445173]
	TIME [epoch: 3.62 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284845944192852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6284845944192852 | validation: 0.5093369889701829]
	TIME [epoch: 3.61 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41504827590655907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41504827590655907 | validation: 0.5216956388140246]
	TIME [epoch: 3.62 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3666335141918443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3666335141918443 | validation: 0.6051599125984204]
	TIME [epoch: 3.62 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37855610820655344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37855610820655344 | validation: 0.45174413236548894]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40492146093215015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40492146093215015 | validation: 0.7662785148679756]
	TIME [epoch: 3.62 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46110847633083907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46110847633083907 | validation: 0.43949411828192164]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3774511117350244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3774511117350244 | validation: 0.6499227263985774]
	TIME [epoch: 3.6 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38892003440272577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38892003440272577 | validation: 0.47738582470994706]
	TIME [epoch: 3.6 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4472039101106134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4472039101106134 | validation: 0.7272400220588955]
	TIME [epoch: 3.6 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48778465546496547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48778465546496547 | validation: 0.47900228142722306]
	TIME [epoch: 3.6 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31543797141466856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31543797141466856 | validation: 0.471623406159402]
	TIME [epoch: 3.6 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27977356699354344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27977356699354344 | validation: 0.47614679026927337]
	TIME [epoch: 3.6 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2671856004046231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2671856004046231 | validation: 0.468437297892175]
	TIME [epoch: 3.6 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27851180019390115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27851180019390115 | validation: 0.6127477361610779]
	TIME [epoch: 3.6 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3499537934400654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3499537934400654 | validation: 0.5197643076738693]
	TIME [epoch: 3.6 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3832139073249867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3832139073249867 | validation: 0.8485445444471673]
	TIME [epoch: 3.6 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5071082854037137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5071082854037137 | validation: 0.6748429773250084]
	TIME [epoch: 3.61 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8184187856864037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8184187856864037 | validation: 0.5526915082795727]
	TIME [epoch: 3.6 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39480949702522855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39480949702522855 | validation: 0.8743613170544571]
	TIME [epoch: 3.6 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.534784846737155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.534784846737155 | validation: 0.45002542181247734]
	TIME [epoch: 3.6 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34065787534176767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34065787534176767 | validation: 0.44254205078724224]
	TIME [epoch: 3.6 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2644125026890321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2644125026890321 | validation: 0.5984084305968359]
	TIME [epoch: 3.6 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2784945610347806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2784945610347806 | validation: 0.41328538781704877]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3149866650181967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3149866650181967 | validation: 0.7128374387478096]
	TIME [epoch: 3.6 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4815597482034971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4815597482034971 | validation: 0.5412842756657602]
	TIME [epoch: 3.59 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3052838531189769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3052838531189769 | validation: 0.4642929570693579]
	TIME [epoch: 3.59 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25812270798655723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25812270798655723 | validation: 0.5213068133112072]
	TIME [epoch: 3.6 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.250302833699588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.250302833699588 | validation: 0.37694500590796987]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_249.pth
	Model improved!!!
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29159627414026573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29159627414026573 | validation: 0.8850707783543273]
	TIME [epoch: 3.61 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5746957744446407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5746957744446407 | validation: 0.5125336784038871]
	TIME [epoch: 3.61 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3796719927190162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3796719927190162 | validation: 0.5265021544801284]
	TIME [epoch: 3.61 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33592215496982486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33592215496982486 | validation: 0.6415117771523928]
	TIME [epoch: 3.59 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3488299591089287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3488299591089287 | validation: 0.4471243529767387]
	TIME [epoch: 3.61 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3699371181787213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3699371181787213 | validation: 0.6061158791138074]
	TIME [epoch: 3.6 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.308097179769552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.308097179769552 | validation: 0.4418260673413388]
	TIME [epoch: 3.6 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2598641085777267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2598641085777267 | validation: 0.5556034754449901]
	TIME [epoch: 3.6 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27554536877196306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27554536877196306 | validation: 0.5060716082438473]
	TIME [epoch: 3.6 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2691468810264273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2691468810264273 | validation: 0.5146369661091525]
	TIME [epoch: 3.6 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2647641684753566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2647641684753566 | validation: 0.46673780237892254]
	TIME [epoch: 3.59 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2479205654838825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2479205654838825 | validation: 0.4981794709104702]
	TIME [epoch: 3.59 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23317293665595615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23317293665595615 | validation: 0.4244755739679362]
	TIME [epoch: 3.59 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26431292634046266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26431292634046266 | validation: 0.8961302512323779]
	TIME [epoch: 3.61 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5721878553051167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5721878553051167 | validation: 0.5182125010767297]
	TIME [epoch: 3.61 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44351161652593035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44351161652593035 | validation: 0.5264632466332552]
	TIME [epoch: 3.6 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41868145320875316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41868145320875316 | validation: 0.7591826339511646]
	TIME [epoch: 3.6 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.378984955997839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.378984955997839 | validation: 0.4427162128858557]
	TIME [epoch: 3.59 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24365497114352905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24365497114352905 | validation: 0.4260936742683781]
	TIME [epoch: 3.59 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24730768204563916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24730768204563916 | validation: 0.5295502302380095]
	TIME [epoch: 3.6 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2667234921461092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2667234921461092 | validation: 0.45210276495815]
	TIME [epoch: 3.6 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2791685905157392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2791685905157392 | validation: 0.6088810672769225]
	TIME [epoch: 3.6 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2886974959920572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2886974959920572 | validation: 0.38098641438435177]
	TIME [epoch: 3.6 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24251057326530245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24251057326530245 | validation: 0.587251734510475]
	TIME [epoch: 3.6 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2666855042383787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2666855042383787 | validation: 0.399660249308227]
	TIME [epoch: 3.6 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22857279716043047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22857279716043047 | validation: 0.5508862824182567]
	TIME [epoch: 3.61 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2312377122680775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2312377122680775 | validation: 0.3678782813742757]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24761644871011065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24761644871011065 | validation: 0.7541491584941344]
	TIME [epoch: 3.62 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3625918734156571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3625918734156571 | validation: 0.40067575987909676]
	TIME [epoch: 3.61 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.327041766061687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.327041766061687 | validation: 0.5636733317287325]
	TIME [epoch: 3.62 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2209052219347664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2209052219347664 | validation: 0.4122886696546891]
	TIME [epoch: 3.61 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.180620287521865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.180620287521865 | validation: 0.41658525570039145]
	TIME [epoch: 3.62 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17144609981633852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17144609981633852 | validation: 0.4919131980722542]
	TIME [epoch: 3.61 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19090550474501286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19090550474501286 | validation: 0.353910876863404]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25755055428559265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25755055428559265 | validation: 0.6869367581652539]
	TIME [epoch: 3.61 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4807016164278754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4807016164278754 | validation: 0.7367759519820847]
	TIME [epoch: 3.61 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35995415947003734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35995415947003734 | validation: 0.43745636693760304]
	TIME [epoch: 3.61 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5122891419123475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5122891419123475 | validation: 0.6272313438910926]
	TIME [epoch: 3.61 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3750265824130697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3750265824130697 | validation: 0.5833603575945635]
	TIME [epoch: 3.62 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3371786437349922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3371786437349922 | validation: 0.3697513887513382]
	TIME [epoch: 3.63 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23516183823603104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23516183823603104 | validation: 0.6578079736075051]
	TIME [epoch: 3.61 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2921069541554918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2921069541554918 | validation: 0.4113319398920616]
	TIME [epoch: 3.61 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2784938696175205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2784938696175205 | validation: 0.48210398072708327]
	TIME [epoch: 3.61 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2260918646792297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2260918646792297 | validation: 0.4419079961613703]
	TIME [epoch: 3.61 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21833036564291017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21833036564291017 | validation: 0.312541782486083]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2743451493117988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2743451493117988 | validation: 0.6487208488051416]
	TIME [epoch: 3.61 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3547197110094845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3547197110094845 | validation: 0.41365630063435543]
	TIME [epoch: 3.61 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16276707788642739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16276707788642739 | validation: 0.30093972383755085]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20195899729416752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20195899729416752 | validation: 0.6964088251822773]
	TIME [epoch: 3.61 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2991135077411641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2991135077411641 | validation: 0.3470263444864129]
	TIME [epoch: 3.61 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18283469299796237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18283469299796237 | validation: 0.499952837984256]
	TIME [epoch: 3.62 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2331918084540191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2331918084540191 | validation: 0.5355124825318798]
	TIME [epoch: 3.63 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28777614317152156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28777614317152156 | validation: 0.38918910812304125]
	TIME [epoch: 3.62 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18397620952555685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18397620952555685 | validation: 0.39785405773133453]
	TIME [epoch: 3.63 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20687891536858363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20687891536858363 | validation: 0.5505777547614317]
	TIME [epoch: 3.64 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.226194143282943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.226194143282943 | validation: 0.35233169539573145]
	TIME [epoch: 3.63 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16639216978283577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16639216978283577 | validation: 0.45435277765365023]
	TIME [epoch: 3.62 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16417254303206177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16417254303206177 | validation: 0.3185274491597865]
	TIME [epoch: 3.62 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24999330182978796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24999330182978796 | validation: 0.8953806165797572]
	TIME [epoch: 3.63 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5168233029075977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5168233029075977 | validation: 0.5285699126659882]
	TIME [epoch: 3.62 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4365253127428419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4365253127428419 | validation: 0.5049714602827943]
	TIME [epoch: 3.63 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5689477412849197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5689477412849197 | validation: 0.6898147368643812]
	TIME [epoch: 3.63 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39158293742660905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39158293742660905 | validation: 0.49223436034459367]
	TIME [epoch: 3.62 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3222936046505634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3222936046505634 | validation: 0.3664664178995577]
	TIME [epoch: 3.64 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3098995143719001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3098995143719001 | validation: 0.5970571243403944]
	TIME [epoch: 3.64 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37354954968305637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37354954968305637 | validation: 0.4679783755042218]
	TIME [epoch: 3.63 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21163525667217292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21163525667217292 | validation: 0.3131018059077655]
	TIME [epoch: 3.63 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19757414244542365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19757414244542365 | validation: 0.540274079202824]
	TIME [epoch: 3.63 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2368712449975494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2368712449975494 | validation: 0.3192941901842523]
	TIME [epoch: 3.62 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20125215487523676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20125215487523676 | validation: 0.6115138666965776]
	TIME [epoch: 3.62 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23697883401347983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23697883401347983 | validation: 0.3357432951979137]
	TIME [epoch: 3.63 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1850916600509612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1850916600509612 | validation: 0.45323988532653303]
	TIME [epoch: 3.62 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17595584041181017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17595584041181017 | validation: 0.3657993593086635]
	TIME [epoch: 3.62 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16338770438800423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16338770438800423 | validation: 0.4680949059097122]
	TIME [epoch: 3.62 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1686438398822809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1686438398822809 | validation: 0.34726091256921543]
	TIME [epoch: 3.63 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19802881015503984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19802881015503984 | validation: 0.5815796677762425]
	TIME [epoch: 3.64 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21603345553258288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21603345553258288 | validation: 0.34845453142279914]
	TIME [epoch: 3.64 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2896735274440396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2896735274440396 | validation: 0.6610852134178193]
	TIME [epoch: 3.64 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44080215046840737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44080215046840737 | validation: 0.4148657965309953]
	TIME [epoch: 3.63 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17599855773224124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17599855773224124 | validation: 0.28688449103915287]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21651125315289613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21651125315289613 | validation: 0.611873025505008]
	TIME [epoch: 3.6 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31634387615988047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31634387615988047 | validation: 0.3906127692293503]
	TIME [epoch: 3.6 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15298691571246603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15298691571246603 | validation: 0.3178090145176715]
	TIME [epoch: 3.6 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1827817200142357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1827817200142357 | validation: 0.577919353005213]
	TIME [epoch: 3.6 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21208002138100093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21208002138100093 | validation: 0.30492108232579024]
	TIME [epoch: 3.6 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16874757442929536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16874757442929536 | validation: 0.5410908970207565]
	TIME [epoch: 3.6 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18227961492637065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18227961492637065 | validation: 0.3226065773927196]
	TIME [epoch: 3.62 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1544413867741847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1544413867741847 | validation: 0.41474514486829267]
	TIME [epoch: 3.63 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17844019712509585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17844019712509585 | validation: 0.45845368974849415]
	TIME [epoch: 3.63 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2645823400029225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2645823400029225 | validation: 0.3765697471553233]
	TIME [epoch: 3.61 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22023342944005717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22023342944005717 | validation: 0.6274691551312102]
	TIME [epoch: 3.61 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30474753124020804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30474753124020804 | validation: 0.4017438516558727]
	TIME [epoch: 3.61 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2871732812982284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2871732812982284 | validation: 0.6788610131879816]
	TIME [epoch: 3.61 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3083695095195057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3083695095195057 | validation: 0.5461950757245252]
	TIME [epoch: 3.61 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22705133498646782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22705133498646782 | validation: 0.2864952194793435]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_344.pth
	Model improved!!!
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.209625779303186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.209625779303186 | validation: 0.43306118403464783]
	TIME [epoch: 3.62 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19588663901419318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19588663901419318 | validation: 0.3200708594806575]
	TIME [epoch: 3.62 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16797503412817605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16797503412817605 | validation: 0.43621565245397903]
	TIME [epoch: 3.62 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15334842541665344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15334842541665344 | validation: 0.29053880808684235]
	TIME [epoch: 3.62 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15458357212838908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15458357212838908 | validation: 0.49275073557157134]
	TIME [epoch: 3.62 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17119338554122984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17119338554122984 | validation: 1.0587363751475458]
	TIME [epoch: 3.63 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.38567250468471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.38567250468471 | validation: 1.3422275996184607]
	TIME [epoch: 3.62 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.489827789880784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.489827789880784 | validation: 0.9543916593700623]
	TIME [epoch: 3.61 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0034505057472733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0034505057472733 | validation: 0.5658216747520819]
	TIME [epoch: 3.61 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4172560102342648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4172560102342648 | validation: 0.5262990573004105]
	TIME [epoch: 3.62 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3043513254323876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3043513254323876 | validation: 0.4829126833093235]
	TIME [epoch: 3.61 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27402663482261896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27402663482261896 | validation: 0.5512591739928504]
	TIME [epoch: 3.61 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2716007014791915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2716007014791915 | validation: 0.4110822471424011]
	TIME [epoch: 3.61 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27862133243307197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27862133243307197 | validation: 0.5513622733999667]
	TIME [epoch: 3.62 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3914131864668029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3914131864668029 | validation: 0.3560063436627322]
	TIME [epoch: 3.61 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34471512361784745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34471512361784745 | validation: 0.4406514747318264]
	TIME [epoch: 3.61 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20596716333638568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20596716333638568 | validation: 0.42131419731890785]
	TIME [epoch: 3.62 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19205620001407725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19205620001407725 | validation: 0.33126329958315204]
	TIME [epoch: 3.63 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22545652516899067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22545652516899067 | validation: 0.5177134048641294]
	TIME [epoch: 3.63 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28914301629110933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28914301629110933 | validation: 0.3634658831789819]
	TIME [epoch: 3.62 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14459787864192428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14459787864192428 | validation: 0.32121146482698537]
	TIME [epoch: 3.62 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1634740045966284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1634740045966284 | validation: 0.4539222762718719]
	TIME [epoch: 3.62 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18477677185942654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18477677185942654 | validation: 0.3090493360862884]
	TIME [epoch: 3.61 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1447242563539415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1447242563539415 | validation: 0.3330393346522213]
	TIME [epoch: 3.62 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12987645457572333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12987645457572333 | validation: 0.34105455885364844]
	TIME [epoch: 3.61 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12385777254446684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12385777254446684 | validation: 0.3258382356782165]
	TIME [epoch: 3.62 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12287268842698965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12287268842698965 | validation: 0.34735262268667494]
	TIME [epoch: 3.61 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13616003887727135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13616003887727135 | validation: 0.4188982253865239]
	TIME [epoch: 3.62 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1877605586269034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1877605586269034 | validation: 0.3187365039959776]
	TIME [epoch: 3.61 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2865843644384579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2865843644384579 | validation: 0.7674292530148397]
	TIME [epoch: 3.63 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4941873537505646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4941873537505646 | validation: 0.44415050188599525]
	TIME [epoch: 3.62 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2330388225529942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2330388225529942 | validation: 0.2864300092578546]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2793902958776934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2793902958776934 | validation: 0.48141521985944236]
	TIME [epoch: 3.61 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19198579524899004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19198579524899004 | validation: 0.3307565578130439]
	TIME [epoch: 3.62 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14608158385574324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14608158385574324 | validation: 0.27709629980216643]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14995043089699336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14995043089699336 | validation: 0.3146913945710711]
	TIME [epoch: 3.62 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11785488239737672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11785488239737672 | validation: 0.2427587874864845]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12131414517939132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12131414517939132 | validation: 0.4312172919622582]
	TIME [epoch: 3.62 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15299907640934718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15299907640934718 | validation: 0.3269601860582402]
	TIME [epoch: 3.62 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24802943016489637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24802943016489637 | validation: 0.7221293857742842]
	TIME [epoch: 3.62 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32748883052364974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32748883052364974 | validation: 0.3146208495582231]
	TIME [epoch: 3.63 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28238575931763654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28238575931763654 | validation: 0.38104256784222423]
	TIME [epoch: 3.63 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18465639743075676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18465639743075676 | validation: 0.3181921819577218]
	TIME [epoch: 3.62 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13268365289890735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13268365289890735 | validation: 0.2974430502407965]
	TIME [epoch: 3.61 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14383724404197287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14383724404197287 | validation: 0.3254448533417722]
	TIME [epoch: 3.62 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13456953363001473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13456953363001473 | validation: 0.30932676508889856]
	TIME [epoch: 3.62 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12586821836380968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12586821836380968 | validation: 0.3056541666179471]
	TIME [epoch: 3.61 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.126171134082658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.126171134082658 | validation: 0.37508953430267755]
	TIME [epoch: 3.61 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16694672442837166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16694672442837166 | validation: 0.2890403012933695]
	TIME [epoch: 3.61 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1908448507603007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1908448507603007 | validation: 0.5099267182758948]
	TIME [epoch: 3.62 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28822127274910064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28822127274910064 | validation: 0.32497790331802395]
	TIME [epoch: 3.61 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17278846593944855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17278846593944855 | validation: 0.25866597394713503]
	TIME [epoch: 3.61 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12138635974294434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12138635974294434 | validation: 0.3079828062854266]
	TIME [epoch: 3.62 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11802677756283361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11802677756283361 | validation: 0.22348179776080387]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11422373169142525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11422373169142525 | validation: 0.4445671705922483]
	TIME [epoch: 3.63 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13598553255959064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13598553255959064 | validation: 0.25061419523135325]
	TIME [epoch: 3.62 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19171803682992966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19171803682992966 | validation: 0.7309357039885322]
	TIME [epoch: 3.62 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33300949014109676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33300949014109676 | validation: 0.37377739245272495]
	TIME [epoch: 3.61 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2387904307506743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2387904307506743 | validation: 0.25110407530394696]
	TIME [epoch: 3.61 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15185732257612222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15185732257612222 | validation: 0.3924649926299137]
	TIME [epoch: 3.62 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17189862433938366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17189862433938366 | validation: 0.35419931482414024]
	TIME [epoch: 3.62 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16004512151500117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16004512151500117 | validation: 0.3053499061833931]
	TIME [epoch: 3.61 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.159858956286532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.159858956286532 | validation: 0.3913544914730869]
	TIME [epoch: 3.62 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19347659589478802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19347659589478802 | validation: 0.3887455897232669]
	TIME [epoch: 3.61 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2571484435487276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2571484435487276 | validation: 0.3027874711295498]
	TIME [epoch: 3.62 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2155319225224089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2155319225224089 | validation: 0.631912866700804]
	TIME [epoch: 3.62 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29350528114304913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29350528114304913 | validation: 0.27807805756396187]
	TIME [epoch: 3.63 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.165639799251352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.165639799251352 | validation: 0.42521936626645473]
	TIME [epoch: 3.62 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23560968259291762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23560968259291762 | validation: 0.2563874741687348]
	TIME [epoch: 3.62 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15809021793304387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15809021793304387 | validation: 0.4688713136851731]
	TIME [epoch: 3.62 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1572975156419522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1572975156419522 | validation: 0.205219653716241]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15676708001139328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15676708001139328 | validation: 0.5946507495329266]
	TIME [epoch: 3.61 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21924196269185176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21924196269185176 | validation: 0.24903117359920054]
	TIME [epoch: 3.62 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1327726453193848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1327726453193848 | validation: 0.24667073510285303]
	TIME [epoch: 3.62 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12746941459815858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12746941459815858 | validation: 0.33813406615470365]
	TIME [epoch: 3.62 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1306573818748491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1306573818748491 | validation: 0.22939950257243977]
	TIME [epoch: 3.62 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10087434080592889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10087434080592889 | validation: 0.29549846673680885]
	TIME [epoch: 3.62 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08651826820695341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08651826820695341 | validation: 0.1987111130814086]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10258695039694908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10258695039694908 | validation: 0.5583868913608075]
	TIME [epoch: 3.63 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20175588136189354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20175588136189354 | validation: 0.29286163880904925]
	TIME [epoch: 3.63 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23917978017411223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23917978017411223 | validation: 0.634644278558996]
	TIME [epoch: 3.62 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2581735169792259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2581735169792259 | validation: 0.3774595873621956]
	TIME [epoch: 3.61 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30415603803948515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30415603803948515 | validation: 0.3586401218766172]
	TIME [epoch: 3.62 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27944056207081214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27944056207081214 | validation: 0.6924606021523537]
	TIME [epoch: 3.61 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3574975799240657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3574975799240657 | validation: 0.31107641996474955]
	TIME [epoch: 3.62 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14543318633462696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14543318633462696 | validation: 0.23204375858251353]
	TIME [epoch: 3.61 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21735396623882725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21735396623882725 | validation: 0.38581022368191775]
	TIME [epoch: 3.61 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17402754974841628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17402754974841628 | validation: 0.31373045657327275]
	TIME [epoch: 3.62 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11258424505877336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11258424505877336 | validation: 0.22516125001658002]
	TIME [epoch: 3.61 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1266494068814355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1266494068814355 | validation: 0.29043166757654526]
	TIME [epoch: 3.62 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16564894112723913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16564894112723913 | validation: 0.5380729827506289]
	TIME [epoch: 3.62 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21689814772685181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21689814772685181 | validation: 0.2899558746396877]
	TIME [epoch: 3.62 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1226913454333744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1226913454333744 | validation: 0.23823138214909967]
	TIME [epoch: 3.64 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09990446482333976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09990446482333976 | validation: 0.28038497093727566]
	TIME [epoch: 3.62 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08649798836443719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08649798836443719 | validation: 0.24605555114668168]
	TIME [epoch: 3.62 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0850634042365029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0850634042365029 | validation: 0.2188769780775525]
	TIME [epoch: 3.61 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1028049730308696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1028049730308696 | validation: 0.48112594851897017]
	TIME [epoch: 3.62 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16480013053397904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16480013053397904 | validation: 0.2915951031172046]
	TIME [epoch: 3.62 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21381129438654475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21381129438654475 | validation: 0.6540373604092672]
	TIME [epoch: 3.62 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26918451505146657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26918451505146657 | validation: 0.24806720712507452]
	TIME [epoch: 3.62 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1915027293510299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1915027293510299 | validation: 0.30739398175289384]
	TIME [epoch: 3.61 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15824370803352947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15824370803352947 | validation: 0.3745995034431245]
	TIME [epoch: 3.62 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12358578926314635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12358578926314635 | validation: 0.21787544862204486]
	TIME [epoch: 3.62 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08947585800779076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08947585800779076 | validation: 0.23145844736897578]
	TIME [epoch: 3.63 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08952528114571444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08952528114571444 | validation: 0.2732317191362061]
	TIME [epoch: 3.62 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09348555559793688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09348555559793688 | validation: 0.24011159325971831]
	TIME [epoch: 3.62 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10026971692067844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10026971692067844 | validation: 0.2597463734185111]
	TIME [epoch: 3.61 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15447315119266644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15447315119266644 | validation: 0.5296146604199463]
	TIME [epoch: 3.62 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2571938794051886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2571938794051886 | validation: 0.37114477172514465]
	TIME [epoch: 3.61 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23674116118900831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23674116118900831 | validation: 0.21778725162773638]
	TIME [epoch: 3.61 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17142465287018616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17142465287018616 | validation: 0.4649452042431249]
	TIME [epoch: 3.62 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2085452210511701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2085452210511701 | validation: 0.2941106305273084]
	TIME [epoch: 3.61 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09386247841503373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09386247841503373 | validation: 0.2338935920238984]
	TIME [epoch: 3.61 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19535517295138555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19535517295138555 | validation: 0.542725023839485]
	TIME [epoch: 3.62 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19297108547763223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19297108547763223 | validation: 0.25024615664768535]
	TIME [epoch: 3.62 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10768385151386184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10768385151386184 | validation: 0.24995515292394305]
	TIME [epoch: 3.63 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17126777944696966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17126777944696966 | validation: 0.27915934131173525]
	TIME [epoch: 3.62 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19338967309043875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19338967309043875 | validation: 0.5096951054312167]
	TIME [epoch: 3.62 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17107709801489052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17107709801489052 | validation: 0.1844636820133755]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10896075031283381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10896075031283381 | validation: 0.21438653936589375]
	TIME [epoch: 3.61 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08398838696781354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08398838696781354 | validation: 0.32486718106916]
	TIME [epoch: 3.62 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08819567244150182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08819567244150182 | validation: 0.17903223562821702]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06549235845996793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06549235845996793 | validation: 0.19325012614887205]
	TIME [epoch: 3.61 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06495506831793002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06495506831793002 | validation: 0.2332331753296058]
	TIME [epoch: 3.61 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08114004165267456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08114004165267456 | validation: 0.22383914094856322]
	TIME [epoch: 3.61 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14539763895130942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14539763895130942 | validation: 0.5138661914565305]
	TIME [epoch: 3.61 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3871495180728502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3871495180728502 | validation: 0.6816933346979203]
	TIME [epoch: 3.62 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26078101198481846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26078101198481846 | validation: 0.2149292115181377]
	TIME [epoch: 3.63 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31879901257226595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31879901257226595 | validation: 0.3606317939540722]
	TIME [epoch: 3.61 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2179329121694028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2179329121694028 | validation: 0.24472072725532956]
	TIME [epoch: 3.61 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13438875322633173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13438875322633173 | validation: 0.2689866680215351]
	TIME [epoch: 3.62 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.128629263987514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.128629263987514 | validation: 0.20753522765625787]
	TIME [epoch: 3.61 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1345804055601182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1345804055601182 | validation: 0.34315304213513304]
	TIME [epoch: 3.61 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16007637940460576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16007637940460576 | validation: 0.2802834841202252]
	TIME [epoch: 3.61 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16148387110651138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16148387110651138 | validation: 0.5375051786845854]
	TIME [epoch: 3.61 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15360011290928044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15360011290928044 | validation: 0.17666324690146037]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08644222206901969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08644222206901969 | validation: 0.21430934966073645]
	TIME [epoch: 3.61 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09175052055729058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09175052055729058 | validation: 0.4909431069264753]
	TIME [epoch: 3.61 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12090838017709804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12090838017709804 | validation: 0.22178459359891978]
	TIME [epoch: 3.62 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07579787676347897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07579787676347897 | validation: 0.20962273304675963]
	TIME [epoch: 3.62 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11218464145639724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11218464145639724 | validation: 0.3094362940705466]
	TIME [epoch: 3.62 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11852191857295065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11852191857295065 | validation: 0.32135929779006384]
	TIME [epoch: 3.61 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13989725429670208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13989725429670208 | validation: 0.24176109893722295]
	TIME [epoch: 3.62 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17434534337324398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17434534337324398 | validation: 0.6338786840393746]
	TIME [epoch: 3.61 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2527161489741966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2527161489741966 | validation: 0.1626069501759173]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11135021507911981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11135021507911981 | validation: 0.24800330420750427]
	TIME [epoch: 3.61 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13429457389061283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13429457389061283 | validation: 0.18802277344524795]
	TIME [epoch: 3.61 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11351708153322745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11351708153322745 | validation: 0.2362905887161748]
	TIME [epoch: 3.61 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07736866871914036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07736866871914036 | validation: 0.13617918070279603]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0785713483739816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0785713483739816 | validation: 0.3153159755280195]
	TIME [epoch: 3.61 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10298217700963612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10298217700963612 | validation: 0.2143112342653145]
	TIME [epoch: 3.61 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12807407044075786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12807407044075786 | validation: 0.40350332437332814]
	TIME [epoch: 3.62 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16952505485534453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16952505485534453 | validation: 0.35085518838055535]
	TIME [epoch: 3.62 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20927860743737867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20927860743737867 | validation: 0.22501002652745294]
	TIME [epoch: 3.61 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16253009918643518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16253009918643518 | validation: 0.37219388182605945]
	TIME [epoch: 3.62 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10643688514264737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10643688514264737 | validation: 0.1575343948378212]
	TIME [epoch: 3.61 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07733841285687472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07733841285687472 | validation: 0.5083506138698651]
	TIME [epoch: 48.1 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16223474269506788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16223474269506788 | validation: 0.23842833898493945]
	TIME [epoch: 7.86 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16900112564380737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16900112564380737 | validation: 0.2921558819343784]
	TIME [epoch: 7.82 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11862225546186043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11862225546186043 | validation: 0.19923475751666153]
	TIME [epoch: 7.83 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08615771800476313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08615771800476313 | validation: 0.145008079507559]
	TIME [epoch: 7.83 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09357195948144567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09357195948144567 | validation: 0.27408084196625637]
	TIME [epoch: 7.84 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18162366025210258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18162366025210258 | validation: 0.33176655638953884]
	TIME [epoch: 7.85 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11403097080031063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11403097080031063 | validation: 0.1488048235791656]
	TIME [epoch: 7.87 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0821873869350245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0821873869350245 | validation: 0.15279258745500904]
	TIME [epoch: 7.83 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06417394474175865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06417394474175865 | validation: 0.19551747361764935]
	TIME [epoch: 7.83 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06368299591015701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06368299591015701 | validation: 0.1318364168029788]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07547923821140178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07547923821140178 | validation: 0.18355697200955073]
	TIME [epoch: 7.84 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.103554096092494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.103554096092494 | validation: 0.2611128977574664]
	TIME [epoch: 7.86 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16066214063882428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16066214063882428 | validation: 0.3133208090364019]
	TIME [epoch: 7.85 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2219826346412753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2219826346412753 | validation: 0.20403161705229855]
	TIME [epoch: 7.85 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10697081122793835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10697081122793835 | validation: 0.1717366664397538]
	TIME [epoch: 7.84 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10049005516206584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10049005516206584 | validation: 0.21191223576221124]
	TIME [epoch: 7.85 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13987274333707075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13987274333707075 | validation: 0.2532281135855177]
	TIME [epoch: 7.85 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13189930404951292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13189930404951292 | validation: 1.4354630026796311]
	TIME [epoch: 7.86 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.053955664864623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.053955664864623 | validation: 1.412513597965627]
	TIME [epoch: 7.85 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9660613729854532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9660613729854532 | validation: 0.9755341075746806]
	TIME [epoch: 7.86 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7030530212650349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7030530212650349 | validation: 1.0450287028038998]
	TIME [epoch: 7.84 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7087505491870401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7087505491870401 | validation: 0.8864068097188484]
	TIME [epoch: 7.84 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5271820649273145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5271820649273145 | validation: 0.769032310337145]
	TIME [epoch: 7.85 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3249723749579052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3249723749579052 | validation: 0.3965669094490536]
	TIME [epoch: 7.85 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2119636489352593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2119636489352593 | validation: 0.35964744866152654]
	TIME [epoch: 7.86 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1526518191278167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1526518191278167 | validation: 0.25245568334545315]
	TIME [epoch: 7.85 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1589200295573152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1589200295573152 | validation: 0.3431100831160948]
	TIME [epoch: 7.84 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16383604804879587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16383604804879587 | validation: 0.1990625866946597]
	TIME [epoch: 7.86 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11094953661662744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11094953661662744 | validation: 0.22158080140317954]
	TIME [epoch: 7.83 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.078731155765925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.078731155765925 | validation: 0.17149799367818713]
	TIME [epoch: 7.85 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06502546237757727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06502546237757727 | validation: 0.1919449955052421]
	TIME [epoch: 7.86 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06624024851188001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06624024851188001 | validation: 0.18721571267962597]
	TIME [epoch: 7.85 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07813734819981828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07813734819981828 | validation: 0.19271183209449094]
	TIME [epoch: 7.84 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09555233799264594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09555233799264594 | validation: 0.24048150058149625]
	TIME [epoch: 7.84 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.159641740592792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.159641740592792 | validation: 0.3506116767033771]
	TIME [epoch: 7.84 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13492205614376584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13492205614376584 | validation: 0.23156396130621007]
	TIME [epoch: 7.85 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15108188492698518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15108188492698518 | validation: 0.49096294805747953]
	TIME [epoch: 7.85 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16485360629562468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16485360629562468 | validation: 0.2655398077197629]
	TIME [epoch: 7.83 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19144792181313597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19144792181313597 | validation: 0.3625680907098261]
	TIME [epoch: 7.83 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16743634592725648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16743634592725648 | validation: 0.32976560368694763]
	TIME [epoch: 7.84 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08786510678544991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08786510678544991 | validation: 0.18153116544372008]
	TIME [epoch: 7.83 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05839526887862071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05839526887862071 | validation: 0.16191311214411824]
	TIME [epoch: 7.84 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07056307038406313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07056307038406313 | validation: 0.22290119519102533]
	TIME [epoch: 7.85 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06261946356686242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06261946356686242 | validation: 0.20617414843447746]
	TIME [epoch: 7.84 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07547003722711418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07547003722711418 | validation: 0.20506378968731567]
	TIME [epoch: 7.85 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11501383839610643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11501383839610643 | validation: 0.22270784248792125]
	TIME [epoch: 7.84 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16302198624810038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16302198624810038 | validation: 0.3307838422447147]
	TIME [epoch: 7.85 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12635506075497419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12635506075497419 | validation: 0.15804492285789606]
	TIME [epoch: 7.84 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06836202216431765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06836202216431765 | validation: 0.13653372910255973]
	TIME [epoch: 7.85 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05048201139091684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05048201139091684 | validation: 0.16097197186700984]
	TIME [epoch: 7.82 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04484244462294786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04484244462294786 | validation: 0.15395267524362688]
	TIME [epoch: 7.83 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04780175119294816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04780175119294816 | validation: 0.19613813240017902]
	TIME [epoch: 7.83 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09728259790088963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09728259790088963 | validation: 0.5158088405064337]
	TIME [epoch: 7.84 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17904616631357528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17904616631357528 | validation: 0.4573335901882278]
	TIME [epoch: 7.83 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19134507368529882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19134507368529882 | validation: 0.27991424218405675]
	TIME [epoch: 7.85 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1861189620530424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1861189620530424 | validation: 0.2585048699026346]
	TIME [epoch: 7.83 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07625753966402324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07625753966402324 | validation: 0.2810359672642381]
	TIME [epoch: 7.83 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1269937421086843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1269937421086843 | validation: 0.3241368726011827]
	TIME [epoch: 7.83 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20259850251368433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20259850251368433 | validation: 0.19946487307583896]
	TIME [epoch: 7.84 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.134742355449529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.134742355449529 | validation: 0.285785991877516]
	TIME [epoch: 7.84 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10395288771712295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10395288771712295 | validation: 0.20661629858491937]
	TIME [epoch: 7.85 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09596274480847646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09596274480847646 | validation: 0.18264549351049308]
	TIME [epoch: 7.83 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06433909864807533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06433909864807533 | validation: 0.16398911891054146]
	TIME [epoch: 7.85 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08849433786039733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08849433786039733 | validation: 0.20378520713316509]
	TIME [epoch: 7.85 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1274463554891104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1274463554891104 | validation: 0.12026752816636482]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09167949900942957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09167949900942957 | validation: 0.3201031011809519]
	TIME [epoch: 7.87 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11120180440568224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11120180440568224 | validation: 0.21271697624760597]
	TIME [epoch: 7.83 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10095952388465083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10095952388465083 | validation: 0.1831331861322902]
	TIME [epoch: 7.84 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09859008516032577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09859008516032577 | validation: 0.14499590586768743]
	TIME [epoch: 7.84 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0745731355486989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0745731355486989 | validation: 0.223038704558427]
	TIME [epoch: 7.85 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05819666846452386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05819666846452386 | validation: 0.11501375164487654]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_572.pth
	Model improved!!!
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06421395631228485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06421395631228485 | validation: 0.3175211118015005]
	TIME [epoch: 7.86 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15777214402601447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15777214402601447 | validation: 0.16535249088809675]
	TIME [epoch: 7.85 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09829050328375924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09829050328375924 | validation: 0.13249701319244786]
	TIME [epoch: 7.84 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08313659153030149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08313659153030149 | validation: 0.1202625649158732]
	TIME [epoch: 7.84 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07076510855854622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07076510855854622 | validation: 0.23787023794943335]
	TIME [epoch: 7.84 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09066535236254156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09066535236254156 | validation: 0.24677680383560505]
	TIME [epoch: 7.84 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17600931436591866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17600931436591866 | validation: 0.2511415458932392]
	TIME [epoch: 7.85 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17600738518814438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17600738518814438 | validation: 0.1821290357613098]
	TIME [epoch: 7.87 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11151945548703135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11151945548703135 | validation: 0.18445689080127603]
	TIME [epoch: 7.88 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05866618158537279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05866618158537279 | validation: 0.12786610386503938]
	TIME [epoch: 7.86 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09014456826652766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09014456826652766 | validation: 0.20689076051139188]
	TIME [epoch: 7.86 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1145272236064078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1145272236064078 | validation: 0.13947809830123553]
	TIME [epoch: 7.86 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08880440891365872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08880440891365872 | validation: 0.3730809696125881]
	TIME [epoch: 7.84 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15018757781663808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15018757781663808 | validation: 0.16805967258812068]
	TIME [epoch: 7.86 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10706693248479957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10706693248479957 | validation: 0.17040894930964484]
	TIME [epoch: 7.83 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09236300857522586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09236300857522586 | validation: 0.12975858726976403]
	TIME [epoch: 7.83 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0586949974208669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0586949974208669 | validation: 0.15381935469116662]
	TIME [epoch: 7.84 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04714343783738925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04714343783738925 | validation: 0.15060631869734686]
	TIME [epoch: 7.85 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06518577801022758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06518577801022758 | validation: 0.3747587566106896]
	TIME [epoch: 7.85 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08454241650018794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08454241650018794 | validation: 0.14922415297354194]
	TIME [epoch: 7.86 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06105812171372115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06105812171372115 | validation: 0.19218486398222412]
	TIME [epoch: 7.85 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10711493613693264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10711493613693264 | validation: 0.33042062043275805]
	TIME [epoch: 7.84 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13073508204553883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13073508204553883 | validation: 0.22961454987634947]
	TIME [epoch: 7.84 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11819359113234659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11819359113234659 | validation: 0.183356002409904]
	TIME [epoch: 7.84 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312317355462545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1312317355462545 | validation: 0.15925336611771068]
	TIME [epoch: 7.86 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07352072364957266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07352072364957266 | validation: 0.14023224655136135]
	TIME [epoch: 7.86 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0639830120645733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0639830120645733 | validation: 0.11954479651685923]
	TIME [epoch: 7.84 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07536721167888202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07536721167888202 | validation: 0.5157524289118793]
	TIME [epoch: 7.84 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21246125777042682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21246125777042682 | validation: 0.2674189281823666]
	TIME [epoch: 7.85 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17610405976990323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17610405976990323 | validation: 0.2084641297262384]
	TIME [epoch: 7.85 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21045765410799605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21045765410799605 | validation: 0.22198245302293587]
	TIME [epoch: 7.86 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08599933184372688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08599933184372688 | validation: 0.2302585893695357]
	TIME [epoch: 7.86 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07984642483471902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07984642483471902 | validation: 0.20866844105237697]
	TIME [epoch: 7.85 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051756061654597785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051756061654597785 | validation: 0.15442547076527846]
	TIME [epoch: 7.83 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052511818950213014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052511818950213014 | validation: 0.1386668643442924]
	TIME [epoch: 7.84 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05793167623076273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05793167623076273 | validation: 0.19699481679746947]
	TIME [epoch: 7.85 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10172526692514508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10172526692514508 | validation: 0.20887083260346115]
	TIME [epoch: 7.84 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13506369861764328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13506369861764328 | validation: 0.18534526932039583]
	TIME [epoch: 7.87 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10031202073581678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10031202073581678 | validation: 0.13729724401061313]
	TIME [epoch: 7.86 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07259362993713789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07259362993713789 | validation: 0.18255182580604232]
	TIME [epoch: 7.84 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06425130851348688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06425130851348688 | validation: 0.15756750140015935]
	TIME [epoch: 7.84 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09000768761499686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09000768761499686 | validation: 0.20845727669949765]
	TIME [epoch: 7.84 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10458365298022708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10458365298022708 | validation: 0.3547930706345622]
	TIME [epoch: 7.85 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.155360923241509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.155360923241509 | validation: 0.18908129693116474]
	TIME [epoch: 7.85 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10058199624846866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10058199624846866 | validation: 0.15482651293475516]
	TIME [epoch: 7.85 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05247352059208315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05247352059208315 | validation: 0.23503201802283533]
	TIME [epoch: 7.84 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.103931904930128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.103931904930128 | validation: 0.24965195617747682]
	TIME [epoch: 7.84 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11734907504433242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11734907504433242 | validation: 0.2284556170551061]
	TIME [epoch: 7.83 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1528132376370925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1528132376370925 | validation: 0.1653260388338465]
	TIME [epoch: 7.84 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08889490398612977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08889490398612977 | validation: 0.12372857175964132]
	TIME [epoch: 7.86 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05213335259717797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05213335259717797 | validation: 0.15501997939213627]
	TIME [epoch: 7.84 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040833562304237174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040833562304237174 | validation: 0.13577310792471514]
	TIME [epoch: 7.84 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03999631376747283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03999631376747283 | validation: 0.10502787249314077]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047349136070125954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047349136070125954 | validation: 0.127201154062805]
	TIME [epoch: 7.81 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07320977293822922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07320977293822922 | validation: 0.17501251386412994]
	TIME [epoch: 7.81 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13127762226952558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13127762226952558 | validation: 0.19405518769085547]
	TIME [epoch: 7.86 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1299829507713875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1299829507713875 | validation: 0.1835509677521321]
	TIME [epoch: 7.86 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11490434394680771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11490434394680771 | validation: 0.09247860397354613]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_630.pth
	Model improved!!!
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04267305801508004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04267305801508004 | validation: 0.10355040517520711]
	TIME [epoch: 7.79 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03618084815393036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03618084815393036 | validation: 0.1654304790811676]
	TIME [epoch: 7.8 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06575161328962524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06575161328962524 | validation: 0.3621263398309253]
	TIME [epoch: 7.78 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12094496876685973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12094496876685973 | validation: 0.17728499987188429]
	TIME [epoch: 7.83 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11717231477440576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11717231477440576 | validation: 0.18223775516772536]
	TIME [epoch: 7.8 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11268359744231245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11268359744231245 | validation: 0.18841368048733015]
	TIME [epoch: 7.8 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07777135281669445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07777135281669445 | validation: 0.17561897410581848]
	TIME [epoch: 7.79 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055868195811198104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055868195811198104 | validation: 0.1066187367680763]
	TIME [epoch: 7.79 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05668691649232053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05668691649232053 | validation: 0.18429672929485225]
	TIME [epoch: 7.81 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07248632039356505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07248632039356505 | validation: 0.11115636866838621]
	TIME [epoch: 7.82 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0770979236675488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0770979236675488 | validation: 0.27901805332294954]
	TIME [epoch: 7.81 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10644129135183601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10644129135183601 | validation: 0.12822377108319505]
	TIME [epoch: 7.81 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07437894779585197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07437894779585197 | validation: 0.11190799985989748]
	TIME [epoch: 7.79 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061804148402008624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061804148402008624 | validation: 0.16979148089811535]
	TIME [epoch: 7.8 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05125116882217583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05125116882217583 | validation: 0.14531096541656688]
	TIME [epoch: 7.79 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057826596254255014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057826596254255014 | validation: 0.271223836841127]
	TIME [epoch: 7.82 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07546201743001853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07546201743001853 | validation: 0.13393525374068826]
	TIME [epoch: 7.8 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08648612332698033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08648612332698033 | validation: 0.16403368182965805]
	TIME [epoch: 7.79 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10372591228623243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10372591228623243 | validation: 0.32163378183415303]
	TIME [epoch: 7.79 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1309795628525441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1309795628525441 | validation: 0.11282220395256354]
	TIME [epoch: 7.79 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06580511084770145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06580511084770145 | validation: 0.13928191338351306]
	TIME [epoch: 7.81 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08308099608806951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08308099608806951 | validation: 0.2140783307166951]
	TIME [epoch: 7.79 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10753003859462719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10753003859462719 | validation: 0.1559246143579033]
	TIME [epoch: 7.8 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12284839740731249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12284839740731249 | validation: 0.15778128911733214]
	TIME [epoch: 7.84 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07874563596074197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07874563596074197 | validation: 0.17229297632472693]
	TIME [epoch: 7.81 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04246510242634983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04246510242634983 | validation: 0.10016999173671604]
	TIME [epoch: 7.88 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028798364351002272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028798364351002272 | validation: 0.1014528174584954]
	TIME [epoch: 7.85 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02790673541944717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02790673541944717 | validation: 0.09158881770350397]
	TIME [epoch: 7.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_658.pth
	Model improved!!!
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029661215117253725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029661215117253725 | validation: 0.13000177032447488]
	TIME [epoch: 7.85 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05606525618306473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05606525618306473 | validation: 0.19032010178582745]
	TIME [epoch: 7.83 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13738125388747555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13738125388747555 | validation: 0.3188969297733826]
	TIME [epoch: 7.83 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15351232862579958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15351232862579958 | validation: 0.1524125031114284]
	TIME [epoch: 7.83 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1154009888303041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1154009888303041 | validation: 0.16188676680407033]
	TIME [epoch: 7.84 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0655310918539149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0655310918539149 | validation: 0.10605695401784493]
	TIME [epoch: 7.85 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0495067478018366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0495067478018366 | validation: 0.12603521355145947]
	TIME [epoch: 7.88 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04963370986474391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04963370986474391 | validation: 0.13232664703563]
	TIME [epoch: 7.85 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11257923722279845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11257923722279845 | validation: 0.19018202322001207]
	TIME [epoch: 7.85 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15316050480355714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15316050480355714 | validation: 0.171249900250111]
	TIME [epoch: 7.87 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036446888253232994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036446888253232994 | validation: 0.09661674647657148]
	TIME [epoch: 7.84 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04489765791798357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04489765791798357 | validation: 0.13026567661826235]
	TIME [epoch: 7.88 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049237028913098405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049237028913098405 | validation: 0.13700900498887197]
	TIME [epoch: 7.9 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06426279201180526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06426279201180526 | validation: 0.26202817265211803]
	TIME [epoch: 7.85 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10685957803780252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10685957803780252 | validation: 0.16884914597659792]
	TIME [epoch: 7.85 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10633251838218871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10633251838218871 | validation: 0.4045640112542559]
	TIME [epoch: 7.85 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11775961026725161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11775961026725161 | validation: 0.08442637081355792]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036939683297042995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036939683297042995 | validation: 0.1038374350469195]
	TIME [epoch: 7.82 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049975106554930315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049975106554930315 | validation: 0.09575713879224972]
	TIME [epoch: 7.81 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04378145653102794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04378145653102794 | validation: 0.20630302979140677]
	TIME [epoch: 7.83 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06848851812639219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06848851812639219 | validation: 0.1262693947081305]
	TIME [epoch: 7.82 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06524282030382299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06524282030382299 | validation: 0.10545280092827602]
	TIME [epoch: 7.82 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06861366525813016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06861366525813016 | validation: 0.14065392428900256]
	TIME [epoch: 7.81 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08205624203895787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08205624203895787 | validation: 0.21623077336869445]
	TIME [epoch: 7.83 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08777291778290247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08777291778290247 | validation: 0.18519564488045082]
	TIME [epoch: 7.83 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11626933620505021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11626933620505021 | validation: 0.1631805203235726]
	TIME [epoch: 7.81 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08853680540370334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08853680540370334 | validation: 0.14133066958974172]
	TIME [epoch: 7.8 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06428880036782576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06428880036782576 | validation: 0.10972955027594278]
	TIME [epoch: 7.82 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03849669227447957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03849669227447957 | validation: 0.09454149685997704]
	TIME [epoch: 7.8 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03675816008067652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03675816008067652 | validation: 0.09610690333677176]
	TIME [epoch: 7.82 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04579960435334925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04579960435334925 | validation: 0.18987030625150136]
	TIME [epoch: 7.83 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08039666339992364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08039666339992364 | validation: 0.13390953572866002]
	TIME [epoch: 7.82 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0874594990926627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0874594990926627 | validation: 0.1910996108863009]
	TIME [epoch: 7.81 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06330761604653833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06330761604653833 | validation: 0.12890612232667226]
	TIME [epoch: 7.79 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06212313911777904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06212313911777904 | validation: 0.15091278448878698]
	TIME [epoch: 7.79 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09111884553934617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09111884553934617 | validation: 0.15974634633466614]
	TIME [epoch: 7.81 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.099359658343961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.099359658343961 | validation: 0.1753495141008793]
	TIME [epoch: 7.81 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08450292418120603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08450292418120603 | validation: 0.11005838928385743]
	TIME [epoch: 7.79 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05570524679542205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05570524679542205 | validation: 0.12901155958612862]
	TIME [epoch: 7.79 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044391671978716137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044391671978716137 | validation: 0.09244428383287812]
	TIME [epoch: 7.8 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03504518284978439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03504518284978439 | validation: 0.5646814073065283]
	TIME [epoch: 7.79 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2313427597602081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2313427597602081 | validation: 0.20450910222354546]
	TIME [epoch: 7.82 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07813060394173178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07813060394173178 | validation: 0.13833783175067146]
	TIME [epoch: 7.79 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055208073275450145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055208073275450145 | validation: 0.14020141437717543]
	TIME [epoch: 7.79 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06542680663746689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06542680663746689 | validation: 0.1841722525700968]
	TIME [epoch: 7.8 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07908273641380742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07908273641380742 | validation: 0.19863899431331747]
	TIME [epoch: 7.8 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12046019700552765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12046019700552765 | validation: 0.20189778679106998]
	TIME [epoch: 7.8 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10737332318767347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10737332318767347 | validation: 0.14106460556110087]
	TIME [epoch: 7.82 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08609989389957849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08609989389957849 | validation: 0.1363431419476671]
	TIME [epoch: 7.8 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07014372942835995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07014372942835995 | validation: 0.10804186721606374]
	TIME [epoch: 7.79 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04432992510663663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04432992510663663 | validation: 0.11190772447707512]
	TIME [epoch: 7.8 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052444743387868115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052444743387868115 | validation: 0.13614140236789526]
	TIME [epoch: 7.79 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07571095703754492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07571095703754492 | validation: 0.1476846087348984]
	TIME [epoch: 7.79 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038056913234023305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038056913234023305 | validation: 0.12575802131754707]
	TIME [epoch: 7.8 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045489147990680795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045489147990680795 | validation: 0.12494521489090185]
	TIME [epoch: 7.82 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05377455366709044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05377455366709044 | validation: 0.20852506451896177]
	TIME [epoch: 7.81 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05485865055344706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05485865055344706 | validation: 0.09577112946760818]
	TIME [epoch: 7.83 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05091648243601888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05091648243601888 | validation: 0.12250229800039714]
	TIME [epoch: 7.81 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05478657672269532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05478657672269532 | validation: 0.11176962841757221]
	TIME [epoch: 7.81 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07016407537558403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07016407537558403 | validation: 0.15285949675549218]
	TIME [epoch: 7.81 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08726359542757535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08726359542757535 | validation: 0.12047056641847909]
	TIME [epoch: 7.83 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0712671443640211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0712671443640211 | validation: 0.1813614190397254]
	TIME [epoch: 7.79 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07835647889426348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07835647889426348 | validation: 0.1966598765034264]
	TIME [epoch: 7.81 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1282150034329328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1282150034329328 | validation: 0.16601748114453785]
	TIME [epoch: 7.8 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09084370023415234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09084370023415234 | validation: 0.09888069384020241]
	TIME [epoch: 7.81 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05790793650876184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05790793650876184 | validation: 0.13274424092535417]
	TIME [epoch: 7.8 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03817035592410672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03817035592410672 | validation: 0.0853204715551158]
	TIME [epoch: 7.81 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03166221549441082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03166221549441082 | validation: 0.12219784776129439]
	TIME [epoch: 7.79 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030284488525000274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030284488525000274 | validation: 0.09480495332430343]
	TIME [epoch: 7.81 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03650974067610841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03650974067610841 | validation: 1.0352471382111275]
	TIME [epoch: 7.8 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44375618293300917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44375618293300917 | validation: 1.1350672713160774]
	TIME [epoch: 7.81 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48791122661081865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48791122661081865 | validation: 0.37199445766462363]
	TIME [epoch: 7.81 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2593990314868895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2593990314868895 | validation: 0.3147786829355235]
	TIME [epoch: 7.82 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12752737772337763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12752737772337763 | validation: 0.2697443279658723]
	TIME [epoch: 7.83 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09472966144583221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09472966144583221 | validation: 0.1418366965390407]
	TIME [epoch: 7.81 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06412015909130062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06412015909130062 | validation: 0.11689243320139045]
	TIME [epoch: 7.79 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0526597433364445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0526597433364445 | validation: 0.12446379179989818]
	TIME [epoch: 7.8 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04723253550073892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04723253550073892 | validation: 0.12993130545283454]
	TIME [epoch: 7.82 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045832087007029634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045832087007029634 | validation: 0.12691015540316924]
	TIME [epoch: 7.81 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056217303735590266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056217303735590266 | validation: 0.19323370578597193]
	TIME [epoch: 7.79 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14086717568928778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14086717568928778 | validation: 0.19030744301536637]
	TIME [epoch: 7.8 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13058540398462035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13058540398462035 | validation: 0.1338757689197887]
	TIME [epoch: 7.8 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10505496496704715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10505496496704715 | validation: 0.13013136375733003]
	TIME [epoch: 7.8 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06337372709674588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06337372709674588 | validation: 0.08384778436671608]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_742.pth
	Model improved!!!
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0591197930197373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0591197930197373 | validation: 0.13811016698570186]
	TIME [epoch: 7.84 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1077408180148775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1077408180148775 | validation: 0.11240774699820832]
	TIME [epoch: 7.83 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05101727030191039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05101727030191039 | validation: 0.08926901623932218]
	TIME [epoch: 7.83 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03355006740857381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03355006740857381 | validation: 0.07334113160671855]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_746.pth
	Model improved!!!
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023184007021193134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023184007021193134 | validation: 0.07688584862362687]
	TIME [epoch: 7.85 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03155482695773336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03155482695773336 | validation: 0.11139968664635602]
	TIME [epoch: 7.86 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06258505953604356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06258505953604356 | validation: 0.12179836227696408]
	TIME [epoch: 7.85 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09908426244009157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09908426244009157 | validation: 0.15128872915422803]
	TIME [epoch: 7.84 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0940293970844944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0940293970844944 | validation: 0.07510230340384916]
	TIME [epoch: 7.84 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03525964213754389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03525964213754389 | validation: 0.0748027546599133]
	TIME [epoch: 7.84 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034677838974692315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034677838974692315 | validation: 0.10276631047035907]
	TIME [epoch: 7.82 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059294147289139246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059294147289139246 | validation: 0.13154577111828325]
	TIME [epoch: 7.84 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09416987045205048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09416987045205048 | validation: 0.1578172795877808]
	TIME [epoch: 7.84 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11534797571981682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11534797571981682 | validation: 0.17080363209872373]
	TIME [epoch: 7.83 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1099542690984124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1099542690984124 | validation: 0.08293317593292526]
	TIME [epoch: 7.82 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051181319220611636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051181319220611636 | validation: 0.08296521643558281]
	TIME [epoch: 7.83 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03915443582928764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03915443582928764 | validation: 0.08884204977791224]
	TIME [epoch: 7.83 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039075479972574455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039075479972574455 | validation: 0.24123139189237752]
	TIME [epoch: 7.86 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3054390629655246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3054390629655246 | validation: 0.3236959521357201]
	TIME [epoch: 7.84 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28139916896009687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28139916896009687 | validation: 0.17931982650677183]
	TIME [epoch: 7.83 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12496784330236584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12496784330236584 | validation: 0.43752689320439847]
	TIME [epoch: 7.83 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07138523162760546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07138523162760546 | validation: 0.3825322202920234]
	TIME [epoch: 7.83 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07585482850666041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07585482850666041 | validation: 0.11769590119835338]
	TIME [epoch: 7.83 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03270761325604116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03270761325604116 | validation: 0.1597403953005353]
	TIME [epoch: 7.84 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04767599319952697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04767599319952697 | validation: 0.14939717245647535]
	TIME [epoch: 7.83 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06015986014086746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06015986014086746 | validation: 0.17526975679104037]
	TIME [epoch: 7.83 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09153548530332555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09153548530332555 | validation: 0.20091479290307446]
	TIME [epoch: 7.83 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09125969432093178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09125969432093178 | validation: 0.1300231185058894]
	TIME [epoch: 7.83 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04460155364860375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04460155364860375 | validation: 0.10939992520618012]
	TIME [epoch: 7.84 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025613976406273026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025613976406273026 | validation: 0.09206602552649971]
	TIME [epoch: 7.85 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019883201331838136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019883201331838136 | validation: 0.09525269576329337]
	TIME [epoch: 7.83 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019945685509286392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019945685509286392 | validation: 0.09294938012854541]
	TIME [epoch: 7.82 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022893270882475402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022893270882475402 | validation: 0.0999652071682828]
	TIME [epoch: 7.83 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03240975841328736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03240975841328736 | validation: 0.11675853411452267]
	TIME [epoch: 7.82 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045701274879863106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045701274879863106 | validation: 0.11056235864676503]
	TIME [epoch: 7.83 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05746111846836357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05746111846836357 | validation: 0.12931432607865587]
	TIME [epoch: 7.85 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05618693635110114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05618693635110114 | validation: 0.1309496847267119]
	TIME [epoch: 7.84 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0964063580397896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0964063580397896 | validation: 0.1804711021516362]
	TIME [epoch: 7.82 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07682433228472414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07682433228472414 | validation: 0.1432829979238682]
	TIME [epoch: 7.83 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1163661985927628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1163661985927628 | validation: 0.12629487039902187]
	TIME [epoch: 7.83 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08927704594842757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08927704594842757 | validation: 0.07749659023540184]
	TIME [epoch: 7.84 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03541735140094962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03541735140094962 | validation: 0.07207231625135746]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019453155463264475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019453155463264475 | validation: 0.06456356855557142]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020670395291742884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020670395291742884 | validation: 0.07874644504140005]
	TIME [epoch: 7.83 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027718417898751166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027718417898751166 | validation: 0.0892516380373648]
	TIME [epoch: 7.85 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03905445364212641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03905445364212641 | validation: 0.09523711084137956]
	TIME [epoch: 7.82 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060534626714791706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060534626714791706 | validation: 0.11518585099096433]
	TIME [epoch: 7.83 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07091357091770027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07091357091770027 | validation: 0.0967060290286436]
	TIME [epoch: 7.84 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05575123545437759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05575123545437759 | validation: 0.1279281543670363]
	TIME [epoch: 7.84 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0501092417686864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0501092417686864 | validation: 0.11251756013200254]
	TIME [epoch: 7.84 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0689692426080471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0689692426080471 | validation: 0.13350843427986772]
	TIME [epoch: 7.84 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08104758970303497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08104758970303497 | validation: 0.13421147919465354]
	TIME [epoch: 7.86 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07879520722234877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07879520722234877 | validation: 0.1317280055409659]
	TIME [epoch: 7.86 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06060568376718733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06060568376718733 | validation: 0.07173149009468145]
	TIME [epoch: 7.88 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02537016800699793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02537016800699793 | validation: 0.0814086545798472]
	TIME [epoch: 7.88 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02007601810967346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02007601810967346 | validation: 0.12153998644601033]
	TIME [epoch: 7.86 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03584818919620002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03584818919620002 | validation: 0.1395753772739964]
	TIME [epoch: 7.86 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05965346081229371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05965346081229371 | validation: 0.10139527951299998]
	TIME [epoch: 7.86 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042222543630846564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042222543630846564 | validation: 0.09861587308941568]
	TIME [epoch: 7.82 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04374554314535585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04374554314535585 | validation: 0.10529640971310311]
	TIME [epoch: 7.87 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04559412150287933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04559412150287933 | validation: 0.09571954515277428]
	TIME [epoch: 7.87 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06677333520512295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06677333520512295 | validation: 0.16540774195318786]
	TIME [epoch: 7.83 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08216717757707025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08216717757707025 | validation: 0.12141852636372903]
	TIME [epoch: 7.82 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08206471218776057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08206471218776057 | validation: 0.11795338815973162]
	TIME [epoch: 7.84 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06024412402969336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06024412402969336 | validation: 0.11019194678178851]
	TIME [epoch: 7.82 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05141247436601699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05141247436601699 | validation: 0.1320495730907078]
	TIME [epoch: 7.85 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0487446751194865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0487446751194865 | validation: 0.12736269320771126]
	TIME [epoch: 7.82 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03593401414796948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03593401414796948 | validation: 0.08750438905423351]
	TIME [epoch: 7.83 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028976093540589602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028976093540589602 | validation: 0.08627853003377628]
	TIME [epoch: 7.84 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04120731410344038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04120731410344038 | validation: 0.11678777084125058]
	TIME [epoch: 7.81 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052954635517983406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052954635517983406 | validation: 0.21843070813384965]
	TIME [epoch: 7.86 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04664476185108633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04664476185108633 | validation: 0.12599831580538276]
	TIME [epoch: 7.86 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054858110835584395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054858110835584395 | validation: 0.17924439748021864]
	TIME [epoch: 7.84 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09978627344194232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09978627344194232 | validation: 0.14523406764197966]
	TIME [epoch: 7.84 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08840784826672522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08840784826672522 | validation: 0.1229409544198755]
	TIME [epoch: 7.84 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0520329201806121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0520329201806121 | validation: 0.0760983444174356]
	TIME [epoch: 7.83 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02593715276757278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02593715276757278 | validation: 0.07950776466055504]
	TIME [epoch: 7.83 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023446877780433174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023446877780433174 | validation: 0.09504018480484307]
	TIME [epoch: 7.85 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04149677733375848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04149677733375848 | validation: 0.12968714338295925]
	TIME [epoch: 7.84 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07878910774885009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07878910774885009 | validation: 0.11781000420877936]
	TIME [epoch: 7.84 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07254726361114246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07254726361114246 | validation: 0.10470564199580884]
	TIME [epoch: 7.83 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04693880640678323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04693880640678323 | validation: 0.08067391073559799]
	TIME [epoch: 7.84 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04249733132667081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04249733132667081 | validation: 0.1788706052842732]
	TIME [epoch: 7.84 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06940176274867904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06940176274867904 | validation: 0.11338416764447912]
	TIME [epoch: 7.88 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07806014339821085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07806014339821085 | validation: 0.09334989213023724]
	TIME [epoch: 7.85 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047978745326495105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047978745326495105 | validation: 0.09815646455261146]
	TIME [epoch: 7.85 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04089067002526819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04089067002526819 | validation: 0.079145993156422]
	TIME [epoch: 7.83 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03625284328478207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03625284328478207 | validation: 0.08001573121362127]
	TIME [epoch: 7.85 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03094850834082916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03094850834082916 | validation: 0.08232944681075433]
	TIME [epoch: 7.83 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03607896431093786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03607896431093786 | validation: 0.09580691834869885]
	TIME [epoch: 7.87 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040792933413828385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040792933413828385 | validation: 0.07995630922553873]
	TIME [epoch: 7.82 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03717514593217355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03717514593217355 | validation: 0.09260322913127769]
	TIME [epoch: 7.86 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04370844079246568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04370844079246568 | validation: 0.1214612186244374]
	TIME [epoch: 7.83 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08697249228252066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08697249228252066 | validation: 0.18196113593823537]
	TIME [epoch: 7.86 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13106592685511606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13106592685511606 | validation: 0.10530895502409804]
	TIME [epoch: 7.83 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06606141399431807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06606141399431807 | validation: 0.09407726615254908]
	TIME [epoch: 7.87 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021496870465955564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021496870465955564 | validation: 0.06659686028159115]
	TIME [epoch: 7.84 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015740328021846245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015740328021846245 | validation: 0.07874443597491582]
	TIME [epoch: 7.85 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028409092975544753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028409092975544753 | validation: 0.11591461987677901]
	TIME [epoch: 7.84 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05200077140288158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05200077140288158 | validation: 0.2004620694841703]
	TIME [epoch: 7.85 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0600358027561769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0600358027561769 | validation: 0.1946833138422533]
	TIME [epoch: 7.84 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0868407273428888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0868407273428888 | validation: 0.13443855356415924]
	TIME [epoch: 7.86 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062370637046236355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062370637046236355 | validation: 0.07206605061848712]
	TIME [epoch: 7.83 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02339026161154415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02339026161154415 | validation: 0.10416116441654995]
	TIME [epoch: 7.83 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024483986312185547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024483986312185547 | validation: 0.215017920945469]
	TIME [epoch: 7.85 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07707370275735716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07707370275735716 | validation: 0.12163197426956943]
	TIME [epoch: 7.85 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08762088097019213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08762088097019213 | validation: 0.1485214029890861]
	TIME [epoch: 7.85 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11287055456495723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11287055456495723 | validation: 0.1336266911892679]
	TIME [epoch: 7.87 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.068346539705365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.068346539705365 | validation: 0.08312047817385301]
	TIME [epoch: 7.84 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0469128383451215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0469128383451215 | validation: 0.09097200985388215]
	TIME [epoch: 7.84 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031246580209367308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031246580209367308 | validation: 0.07605471443418235]
	TIME [epoch: 7.84 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0229684737821535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0229684737821535 | validation: 0.06645773919831294]
	TIME [epoch: 7.85 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027250810000745373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027250810000745373 | validation: 0.08587181550926318]
	TIME [epoch: 7.84 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03464039037998422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03464039037998422 | validation: 0.07550446148325547]
	TIME [epoch: 7.84 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0401985675735242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0401985675735242 | validation: 0.09689784960897668]
	TIME [epoch: 7.85 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05036352969152853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05036352969152853 | validation: 0.07440577252106916]
	TIME [epoch: 7.85 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04289288782364076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04289288782364076 | validation: 0.08515725851407843]
	TIME [epoch: 7.84 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035215660182354384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035215660182354384 | validation: 0.08421964259167308]
	TIME [epoch: 7.86 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0434055181810079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0434055181810079 | validation: 0.12125060361625957]
	TIME [epoch: 7.86 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07779457581600893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07779457581600893 | validation: 0.11140661136956992]
	TIME [epoch: 7.88 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05482820549287252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05482820549287252 | validation: 0.06945465836238456]
	TIME [epoch: 7.83 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027687882292590442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027687882292590442 | validation: 0.1820062756244164]
	TIME [epoch: 7.84 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06649918898209892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06649918898209892 | validation: 0.1598963499326539]
	TIME [epoch: 7.83 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11489221438886514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11489221438886514 | validation: 0.1589750847210754]
	TIME [epoch: 7.85 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12251979918855974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12251979918855974 | validation: 0.09650158592393294]
	TIME [epoch: 7.85 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042897241222563144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042897241222563144 | validation: 0.12378248155767212]
	TIME [epoch: 7.85 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04915388609537289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04915388609537289 | validation: 0.10801269745659932]
	TIME [epoch: 7.83 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03406472920066695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03406472920066695 | validation: 0.13784198655100482]
	TIME [epoch: 7.84 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022464067861067166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022464067861067166 | validation: 0.11133839218990804]
	TIME [epoch: 7.84 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0204313301705074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0204313301705074 | validation: 0.1091473648829851]
	TIME [epoch: 7.84 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03010124794744032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03010124794744032 | validation: 0.16109302954031535]
	TIME [epoch: 7.87 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0334685816879934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0334685816879934 | validation: 0.11951519280876946]
	TIME [epoch: 7.84 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04265330246488601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04265330246488601 | validation: 0.13288723113576648]
	TIME [epoch: 7.82 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06073414552284018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06073414552284018 | validation: 0.11161662179523946]
	TIME [epoch: 7.84 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060456581208743136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060456581208743136 | validation: 0.14647711619568154]
	TIME [epoch: 7.83 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06629474012165401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06629474012165401 | validation: 0.13905222807033016]
	TIME [epoch: 7.86 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08556980990440557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08556980990440557 | validation: 0.15093818277703652]
	TIME [epoch: 7.87 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09587609552871444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09587609552871444 | validation: 0.1009597753021177]
	TIME [epoch: 7.85 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0493492204792797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0493492204792797 | validation: 0.07099894346391555]
	TIME [epoch: 7.83 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017539110821712594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017539110821712594 | validation: 0.07084511342104859]
	TIME [epoch: 7.84 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013456238478756304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013456238478756304 | validation: 0.07256403657571121]
	TIME [epoch: 7.84 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018786583568843482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018786583568843482 | validation: 0.08234258162150888]
	TIME [epoch: 7.84 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02439858205356602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02439858205356602 | validation: 0.08808522130705693]
	TIME [epoch: 7.85 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032015566781068976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032015566781068976 | validation: 1.2077808992722177]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_132805/states/model_phi1_4b_v_mmd1_886.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4694.845 seconds.
