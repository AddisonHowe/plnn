Args:
Namespace(name='model_phi1_1a_v_mmd1', outdir='out/model_training/model_phi1_1a_v_mmd1', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1099225060

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.154069170747823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.154069170747823 | validation: 4.703388993035331]
	TIME [epoch: 115 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.686367822457136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.686367822457136 | validation: 4.371768087088692]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.016035908475352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.016035908475352 | validation: 3.758518278409676]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6578708092012264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6578708092012264 | validation: 3.4341103835852245]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2136917381576593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2136917381576593 | validation: 3.623362400312611]
	TIME [epoch: 7.68 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1934272981952034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1934272981952034 | validation: 3.2067140189775363]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8008456011738563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8008456011738563 | validation: 3.3097122241022756]
	TIME [epoch: 7.74 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7467696146859617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7467696146859617 | validation: 2.863859841112868]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.612075671499993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.612075671499993 | validation: 2.8001412163048944]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4913251858232734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4913251858232734 | validation: 2.727323957964499]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.473443202052362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.473443202052362 | validation: 2.7093827106821524]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.439673908146149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.439673908146149 | validation: 2.6725172775201607]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3923769180453713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3923769180453713 | validation: 2.6247574329710943]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3862907823217063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3862907823217063 | validation: 2.5582410557653215]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3458740262259115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3458740262259115 | validation: 2.526397046553222]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.328365566480061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.328365566480061 | validation: 2.5334224624852864]
	TIME [epoch: 7.72 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3087739133607146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3087739133607146 | validation: 2.4169716539825705]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.286526948296399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.286526948296399 | validation: 2.599767753688175]
	TIME [epoch: 7.67 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.32163598323963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.32163598323963 | validation: 2.39991569717857]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2177287850670306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2177287850670306 | validation: 2.348722656896288]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.732367389006192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.732367389006192 | validation: 2.2937327423771947]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1810182469624646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1810182469624646 | validation: 2.2719624455010026]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1440918942446423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1440918942446423 | validation: 2.2136288671042736]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.016045155294847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.016045155294847 | validation: 2.079841523938106]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7235973250681123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7235973250681123 | validation: 1.568574299845737]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.862819194850093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.862819194850093 | validation: 1.4279974548190228]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.418382731802299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.418382731802299 | validation: 1.3063550129853965]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1922541177617427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1922541177617427 | validation: 1.2130541694502597]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.323006326697878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.323006326697878 | validation: 1.2210446155426382]
	TIME [epoch: 7.73 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1308948516927204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1308948516927204 | validation: 1.4344027450551757]
	TIME [epoch: 7.67 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4056513090311629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4056513090311629 | validation: 1.2232120531021007]
	TIME [epoch: 7.67 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.067060493980438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.067060493980438 | validation: 1.079185542633939]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.012671080378945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.012671080378945 | validation: 1.7316226431471253]
	TIME [epoch: 7.67 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1728680526989526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1728680526989526 | validation: 0.8967776590202363]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8649397911410284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8649397911410284 | validation: 0.856786396524476]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.173252724326799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.173252724326799 | validation: 1.1772030082056206]
	TIME [epoch: 7.67 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.020392757197672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.020392757197672 | validation: 0.8320450657244953]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7824462037517078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7824462037517078 | validation: 0.9139122456124458]
	TIME [epoch: 7.71 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8046964722739542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8046964722739542 | validation: 0.7344728935776861]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7079881860146731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7079881860146731 | validation: 0.66271459275504]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8714951352926326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8714951352926326 | validation: 1.59647056279726]
	TIME [epoch: 7.68 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9622039608131698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9622039608131698 | validation: 0.7831159265279055]
	TIME [epoch: 7.67 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9066726635189293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9066726635189293 | validation: 1.441254967905904]
	TIME [epoch: 7.73 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1812410889263583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1812410889263583 | validation: 0.7302135121179871]
	TIME [epoch: 7.68 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.723210027211109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.723210027211109 | validation: 0.6637597285413204]
	TIME [epoch: 7.68 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6544815605253994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6544815605253994 | validation: 0.7480902536951629]
	TIME [epoch: 7.67 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6558424107499368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6558424107499368 | validation: 0.500321561434699]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0436986347513137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0436986347513137 | validation: 0.8327913045672126]
	TIME [epoch: 7.73 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.737691810603479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.737691810603479 | validation: 1.031130325025004]
	TIME [epoch: 7.68 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9647313644995421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9647313644995421 | validation: 0.9255825057445559]
	TIME [epoch: 7.67 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7628772619106221		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.7628772619106221 | validation: 0.6779330252119259]
	TIME [epoch: 7.66 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6857193741950774		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.6857193741950774 | validation: 0.6309939554259822]
	TIME [epoch: 7.67 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6145970446091525		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.6145970446091525 | validation: 0.5851182929034321]
	TIME [epoch: 7.72 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5823723337437249		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.5823723337437249 | validation: 0.7316879880098502]
	TIME [epoch: 7.68 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8336358381175		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.8336358381175 | validation: 0.7890297896454243]
	TIME [epoch: 7.67 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6919431895623295		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.6919431895623295 | validation: 0.48563527533273143]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0747823772556067		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.0747823772556067 | validation: 0.7637367164621108]
	TIME [epoch: 7.7 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6929910870480576		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.6929910870480576 | validation: 0.629142564241451]
	TIME [epoch: 7.72 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6567742680199611		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.6567742680199611 | validation: 0.6600893603732436]
	TIME [epoch: 7.67 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6476038129892582		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.6476038129892582 | validation: 0.6080382933362549]
	TIME [epoch: 7.68 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5655767802225213		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.5655767802225213 | validation: 0.6443167682968649]
	TIME [epoch: 7.67 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5922347060526734		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.5922347060526734 | validation: 0.7661946143014104]
	TIME [epoch: 7.73 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6108486664525532		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.6108486664525532 | validation: 0.5349315707786384]
	TIME [epoch: 7.71 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48604594975809207		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.48604594975809207 | validation: 1.0874201657175662]
	TIME [epoch: 7.67 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9053616222339623		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.9053616222339623 | validation: 0.503245873437222]
	TIME [epoch: 7.67 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5677551160200321		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.5677551160200321 | validation: 0.5847320587699222]
	TIME [epoch: 7.67 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5499813543940405		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.5499813543940405 | validation: 0.3898751879617933]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8616327475085012		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.8616327475085012 | validation: 0.7663137691238813]
	TIME [epoch: 7.68 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.622120544295539		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.622120544295539 | validation: 0.4444263453534497]
	TIME [epoch: 7.68 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46119241129660116		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.46119241129660116 | validation: 0.3401438510592542]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.630674919431815		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.630674919431815 | validation: 0.6599755337893864]
	TIME [epoch: 7.67 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5696580811536326		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.5696580811536326 | validation: 0.6055811326975135]
	TIME [epoch: 7.75 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5530809896887403		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.5530809896887403 | validation: 0.40275588976391175]
	TIME [epoch: 7.67 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4080456664066126		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.4080456664066126 | validation: 0.3519980901356313]
	TIME [epoch: 7.67 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.782549175293828		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.782549175293828 | validation: 0.5281239792092918]
	TIME [epoch: 7.7 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39688227073297333		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.39688227073297333 | validation: 0.444295999773786]
	TIME [epoch: 7.67 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.428542138389819		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.428542138389819 | validation: 0.4726436149878708]
	TIME [epoch: 7.76 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.560257837427261		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.560257837427261 | validation: 0.6559111791342024]
	TIME [epoch: 7.7 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.552146598930758		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.552146598930758 | validation: 0.43171835662731783]
	TIME [epoch: 7.68 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4381450405200221		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.4381450405200221 | validation: 0.36669070708166696]
	TIME [epoch: 7.67 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36389935815032504		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.36389935815032504 | validation: 0.40185553645816674]
	TIME [epoch: 7.97 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.448850647489339		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.448850647489339 | validation: 1.2509982445394403]
	TIME [epoch: 7.72 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7214103501704319		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.7214103501704319 | validation: 0.46258967181477484]
	TIME [epoch: 7.68 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39256159453417916		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.39256159453417916 | validation: 0.5440038503019516]
	TIME [epoch: 7.67 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39730958246158055		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.39730958246158055 | validation: 0.34284301950201157]
	TIME [epoch: 7.67 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46547689348263105		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.46547689348263105 | validation: 0.39933610898937644]
	TIME [epoch: 7.69 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3493166418074803		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.3493166418074803 | validation: 0.23068560232735114]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3518375471403961		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.3518375471403961 | validation: 0.9124819581403244]
	TIME [epoch: 7.67 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5559224113916243		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.5559224113916243 | validation: 0.3128572091492348]
	TIME [epoch: 7.68 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31654807973708243		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.31654807973708243 | validation: 0.3107216980820467]
	TIME [epoch: 7.68 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31082523232210774		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.31082523232210774 | validation: 0.465211185801802]
	TIME [epoch: 7.72 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4382210562754842		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.4382210562754842 | validation: 0.3850688755138445]
	TIME [epoch: 7.7 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3340425525856331		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.3340425525856331 | validation: 0.36312017997286083]
	TIME [epoch: 7.67 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3445123775731983		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.3445123775731983 | validation: 0.4955023805742247]
	TIME [epoch: 7.68 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4046250137491298		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.4046250137491298 | validation: 0.34609580653834565]
	TIME [epoch: 7.68 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3680446135285427		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.3680446135285427 | validation: 0.3206265496745848]
	TIME [epoch: 7.73 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26867023398144396		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.26867023398144396 | validation: 0.2160894934664429]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3072293946681511		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.3072293946681511 | validation: 0.18257034889849097]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3982610069944821		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.3982610069944821 | validation: 0.45250286361480463]
	TIME [epoch: 7.68 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32895551123096517		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.32895551123096517 | validation: 0.348173468182976]
	TIME [epoch: 7.67 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2792412846572255		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.2792412846572255 | validation: 0.18542252513314933]
	TIME [epoch: 7.75 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2349686126051545		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.2349686126051545 | validation: 0.7677977189331491]
	TIME [epoch: 7.68 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6000142531120947		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.6000142531120947 | validation: 0.5276233925516574]
	TIME [epoch: 7.67 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3536089306469504		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.3536089306469504 | validation: 0.1818464667214868]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2381053662462729		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.2381053662462729 | validation: 0.26609466336050214]
	TIME [epoch: 7.71 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2428927819283771		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.2428927819283771 | validation: 0.35978640325467304]
	TIME [epoch: 7.75 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32213367274720633		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.32213367274720633 | validation: 0.5582643900595525]
	TIME [epoch: 7.69 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4203767776490586		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.4203767776490586 | validation: 0.20342672872314072]
	TIME [epoch: 7.7 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22577792460190604		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.22577792460190604 | validation: 0.2430348137611934]
	TIME [epoch: 7.67 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2312332090623254		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.2312332090623254 | validation: 0.21354866528711972]
	TIME [epoch: 7.72 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25112685655843164		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.25112685655843164 | validation: 0.2953907806564027]
	TIME [epoch: 7.72 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2700869706626319		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.2700869706626319 | validation: 0.17182021298341837]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2367446334049073		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.2367446334049073 | validation: 0.1473373677058352]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24043236006556723		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.24043236006556723 | validation: 0.18621014581061546]
	TIME [epoch: 7.69 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16075754151248323		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.16075754151248323 | validation: 0.1495084085263496]
	TIME [epoch: 7.75 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.303907591512731		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.303907591512731 | validation: 0.2824929188445383]
	TIME [epoch: 7.69 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34695420131809457		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.34695420131809457 | validation: 0.23467838152993117]
	TIME [epoch: 7.69 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25275474209106846		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.25275474209106846 | validation: 0.16772679643169802]
	TIME [epoch: 7.7 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18368902679506377		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.18368902679506377 | validation: 0.2788974698416801]
	TIME [epoch: 7.69 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2718029129878967		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.2718029129878967 | validation: 0.1751260109039764]
	TIME [epoch: 7.75 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15557684254060852		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.15557684254060852 | validation: 0.39468521012334734]
	TIME [epoch: 7.69 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3553838917710474		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.3553838917710474 | validation: 0.17581944676202516]
	TIME [epoch: 7.67 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1705770705036277		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.1705770705036277 | validation: 0.9419280551479273]
	TIME [epoch: 7.69 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39054952017800293		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.39054952017800293 | validation: 0.1421776411169316]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1692388337368322		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.1692388337368322 | validation: 0.292707227975711]
	TIME [epoch: 7.74 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2150160695956979		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.2150160695956979 | validation: 0.29661965705071114]
	TIME [epoch: 7.67 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23186883750360304		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.23186883750360304 | validation: 0.16784475105769062]
	TIME [epoch: 7.66 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2418438404071529		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.2418438404071529 | validation: 0.2561791466960293]
	TIME [epoch: 7.66 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2684831253013582		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.2684831253013582 | validation: 0.28541847250054486]
	TIME [epoch: 7.68 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24755535990383737		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.24755535990383737 | validation: 0.281816444153642]
	TIME [epoch: 7.71 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24312532334683162		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.24312532334683162 | validation: 0.37919268523348215]
	TIME [epoch: 7.66 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2945545714083636		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.2945545714083636 | validation: 0.22982202556311665]
	TIME [epoch: 7.67 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22789428052816968		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.22789428052816968 | validation: 0.35233118924636864]
	TIME [epoch: 7.66 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2601094819399106		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.2601094819399106 | validation: 0.13350434904538405]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1539449265641502		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.1539449265641502 | validation: 0.12830819809806066]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20130557678694427		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.20130557678694427 | validation: 0.24271345004412653]
	TIME [epoch: 7.66 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17713951239571388		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.17713951239571388 | validation: 0.17844522366758664]
	TIME [epoch: 7.66 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28138162616205625		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.28138162616205625 | validation: 0.542745377374733]
	TIME [epoch: 7.66 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3008790851561591		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.3008790851561591 | validation: 0.1918633072218432]
	TIME [epoch: 7.7 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19607906991933116		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.19607906991933116 | validation: 0.19868098463728406]
	TIME [epoch: 7.67 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20523785837580366		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.20523785837580366 | validation: 0.12572402232986407]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12701978204133152		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.12701978204133152 | validation: 0.2488153829722018]
	TIME [epoch: 7.68 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19749521797651667		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.19749521797651667 | validation: 0.2781580256418894]
	TIME [epoch: 7.68 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.299007370474699		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.299007370474699 | validation: 0.19543379222226853]
	TIME [epoch: 7.73 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15148534331634228		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.15148534331634228 | validation: 1.3452064466737095]
	TIME [epoch: 7.7 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9608564717182553		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.9608564717182553 | validation: 1.2814589515519146]
	TIME [epoch: 7.67 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7396389743733878		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.7396389743733878 | validation: 0.36580989261318836]
	TIME [epoch: 7.67 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24570981990132593		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.24570981990132593 | validation: 0.23199733935083483]
	TIME [epoch: 7.69 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20259818220148282		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.20259818220148282 | validation: 0.1328860382973197]
	TIME [epoch: 7.73 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12342755157214651		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.12342755157214651 | validation: 0.21171587718246243]
	TIME [epoch: 7.69 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.141707740018746		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.141707740018746 | validation: 0.10634511699439286]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10168950800586483		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.10168950800586483 | validation: 0.12224506563938575]
	TIME [epoch: 7.68 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15810663492950516		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.15810663492950516 | validation: 0.28764611320774847]
	TIME [epoch: 7.69 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19258396339020176		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.19258396339020176 | validation: 0.16078462035770508]
	TIME [epoch: 7.72 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1619516143845472		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.1619516143845472 | validation: 0.17338445822334272]
	TIME [epoch: 7.69 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14990663154042938		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.14990663154042938 | validation: 0.1374668476856925]
	TIME [epoch: 7.67 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21294236651682946		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.21294236651682946 | validation: 0.2397529627725555]
	TIME [epoch: 7.69 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20910298146396816		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.20910298146396816 | validation: 0.11647696081424587]
	TIME [epoch: 7.7 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11861148916394372		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.11861148916394372 | validation: 0.20071485002033151]
	TIME [epoch: 7.73 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2464748327646003		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.2464748327646003 | validation: 0.16399348371062872]
	TIME [epoch: 7.68 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11570647564100717		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.11570647564100717 | validation: 0.12263445613215224]
	TIME [epoch: 7.68 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10558323014691252		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.10558323014691252 | validation: 0.08763786761566411]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12271575629317091		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.12271575629317091 | validation: 0.27993810249560397]
	TIME [epoch: 7.76 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21336464203584912		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.21336464203584912 | validation: 0.14806806899392136]
	TIME [epoch: 7.69 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10864037661283137		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.10864037661283137 | validation: 0.11934810685606925]
	TIME [epoch: 7.7 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15244451124597777		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.15244451124597777 | validation: 0.0805639831965767]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1495692653198148		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.1495692653198148 | validation: 0.19294239684023615]
	TIME [epoch: 7.7 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16151652550038842		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.16151652550038842 | validation: 0.1321008852933785]
	TIME [epoch: 7.74 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10777219423874188		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.10777219423874188 | validation: 0.07385883289173797]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11619390352828218		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.11619390352828218 | validation: 0.3583071624891707]
	TIME [epoch: 7.66 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2630827531923243		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.2630827531923243 | validation: 0.3014678251511194]
	TIME [epoch: 7.66 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15556020947058327		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.15556020947058327 | validation: 0.07519245055593188]
	TIME [epoch: 7.67 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09091162255482381		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.09091162255482381 | validation: 0.1948080190712777]
	TIME [epoch: 7.73 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1627754512429742		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.1627754512429742 | validation: 0.11237701992958134]
	TIME [epoch: 7.66 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09718795017945393		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.09718795017945393 | validation: 0.07563477576400537]
	TIME [epoch: 7.66 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11086308428600405		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.11086308428600405 | validation: 0.06345566181122046]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10542651378712256		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.10542651378712256 | validation: 1.2922726937672162]
	TIME [epoch: 7.68 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6005991090182388		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.6005991090182388 | validation: 0.1507326582570305]
	TIME [epoch: 7.7 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17757611896551534		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.17757611896551534 | validation: 0.09385526260369983]
	TIME [epoch: 7.65 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08655162579886207		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.08655162579886207 | validation: 0.061514249115165565]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08078945652867534		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.08078945652867534 | validation: 0.1293140023106153]
	TIME [epoch: 7.67 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1281966510264538		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.1281966510264538 | validation: 0.23189613290626587]
	TIME [epoch: 7.7 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20182501529775485		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.20182501529775485 | validation: 0.09934738200714667]
	TIME [epoch: 7.7 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1229119395822018		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.1229119395822018 | validation: 0.16796260142047398]
	TIME [epoch: 7.67 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11142524824899895		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.11142524824899895 | validation: 0.2159948813609904]
	TIME [epoch: 7.67 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17502545283279847		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.17502545283279847 | validation: 0.06395216195945957]
	TIME [epoch: 7.66 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12124049486232226		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.12124049486232226 | validation: 0.0758645976190434]
	TIME [epoch: 7.73 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07030285306970083		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.07030285306970083 | validation: 0.053402224158725226]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10165050212188555		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.10165050212188555 | validation: 0.09298784477139913]
	TIME [epoch: 7.66 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09502541414779811		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.09502541414779811 | validation: 0.08851794393488213]
	TIME [epoch: 7.66 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0776092040601097		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.0776092040601097 | validation: 0.0859778679391452]
	TIME [epoch: 7.66 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08947213890368705		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.08947213890368705 | validation: 0.12512478363133991]
	TIME [epoch: 7.73 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12595912087917602		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.12595912087917602 | validation: 0.077127645168099]
	TIME [epoch: 7.67 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10316702915257182		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.10316702915257182 | validation: 0.08992497491408957]
	TIME [epoch: 7.68 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07506848188403313		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.07506848188403313 | validation: 0.0494756620495648]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09995920428486013		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.09995920428486013 | validation: 0.06401244121943112]
	TIME [epoch: 7.7 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0566366050764779		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.0566366050764779 | validation: 0.09734049214175462]
	TIME [epoch: 7.74 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09517708247265927		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.09517708247265927 | validation: 0.14253336766956984]
	TIME [epoch: 7.7 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1345341916695674		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.1345341916695674 | validation: 0.20739505478904102]
	TIME [epoch: 7.69 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12075657312916797		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.12075657312916797 | validation: 0.14724768046749437]
	TIME [epoch: 7.69 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11159702395792008		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.11159702395792008 | validation: 0.13884023328111203]
	TIME [epoch: 7.71 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08740093802534105		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.08740093802534105 | validation: 0.07044116982594048]
	TIME [epoch: 7.75 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07412905780923655		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.07412905780923655 | validation: 0.09508345754893155]
	TIME [epoch: 7.69 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08169187863845073		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.08169187863845073 | validation: 0.09618159164448345]
	TIME [epoch: 7.67 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07074195922706258		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.07074195922706258 | validation: 0.2031520268982558]
	TIME [epoch: 7.69 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13001944770511328		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.13001944770511328 | validation: 0.10283458433892484]
	TIME [epoch: 7.71 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14301139262124668		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.14301139262124668 | validation: 0.07808065230822023]
	TIME [epoch: 7.73 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22031662383577671		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.22031662383577671 | validation: 0.12151214366485348]
	TIME [epoch: 7.69 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0948501137263816		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.0948501137263816 | validation: 0.0669694437822906]
	TIME [epoch: 7.69 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06817266967051071		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.06817266967051071 | validation: 0.06562710743799843]
	TIME [epoch: 7.69 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05636854850048406		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.05636854850048406 | validation: 0.05869725149714887]
	TIME [epoch: 7.73 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08337961949241753		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.08337961949241753 | validation: 0.11972946815122804]
	TIME [epoch: 7.71 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08932655862196359		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.08932655862196359 | validation: 0.04397476081067627]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05039891129396373		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.05039891129396373 | validation: 0.059650444057880656]
	TIME [epoch: 7.68 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12619836624246572		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.12619836624246572 | validation: 0.14453173511816136]
	TIME [epoch: 7.69 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09051974236919046		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.09051974236919046 | validation: 0.09155867714720645]
	TIME [epoch: 7.74 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07538450565316154		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.07538450565316154 | validation: 0.06657355017866344]
	TIME [epoch: 7.7 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18920281363295322		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.18920281363295322 | validation: 0.17176917928528462]
	TIME [epoch: 7.7 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12638023327137282		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.12638023327137282 | validation: 0.0720664412498684]
	TIME [epoch: 7.7 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06698061816294032		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.06698061816294032 | validation: 0.08309563929898528]
	TIME [epoch: 7.7 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07656915309636478		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.07656915309636478 | validation: 0.09411904909028773]
	TIME [epoch: 7.76 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059118768931848385		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.059118768931848385 | validation: 0.09823025793928464]
	TIME [epoch: 7.7 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09829951540735149		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.09829951540735149 | validation: 0.06296728962900448]
	TIME [epoch: 7.69 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07794270150861861		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.07794270150861861 | validation: 0.06614385221007175]
	TIME [epoch: 7.69 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06973096048670151		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.06973096048670151 | validation: 0.0440674783484961]
	TIME [epoch: 7.7 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05679555297788484		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.05679555297788484 | validation: 0.07825400891617376]
	TIME [epoch: 7.76 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06468488951748977		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.06468488951748977 | validation: 0.15867599662955814]
	TIME [epoch: 7.7 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11490493629229011		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.11490493629229011 | validation: 0.04590588713048534]
	TIME [epoch: 7.67 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04991651110857943		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.04991651110857943 | validation: 0.056712239338802525]
	TIME [epoch: 7.7 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13707609768737342		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.13707609768737342 | validation: 0.10064216405202218]
	TIME [epoch: 7.67 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08450081135884352		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.08450081135884352 | validation: 0.10103160753282635]
	TIME [epoch: 7.74 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06217625285090936		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.06217625285090936 | validation: 0.06348920037490974]
	TIME [epoch: 7.67 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051783807720907606		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.051783807720907606 | validation: 0.07310617741847233]
	TIME [epoch: 7.68 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07020564382841765		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.07020564382841765 | validation: 0.07268475642666447]
	TIME [epoch: 7.7 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06182111053727471		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.06182111053727471 | validation: 0.10050626036946807]
	TIME [epoch: 7.69 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08190231903851956		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.08190231903851956 | validation: 0.057258008109334985]
	TIME [epoch: 7.73 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06609527659535158		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.06609527659535158 | validation: 0.0793946763528954]
	TIME [epoch: 7.7 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09933380790159893		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.09933380790159893 | validation: 0.04645203657101192]
	TIME [epoch: 7.68 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04243312807787756		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.04243312807787756 | validation: 0.061385689310954034]
	TIME [epoch: 7.69 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06903959264679194		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.06903959264679194 | validation: 0.06792629376837458]
	TIME [epoch: 7.71 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06256371349720921		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.06256371349720921 | validation: 0.0476219297157407]
	TIME [epoch: 7.73 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08852470545760202		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.08852470545760202 | validation: 0.07873629989906941]
	TIME [epoch: 7.68 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05202251222223499		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.05202251222223499 | validation: 0.03950914752064257]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04004538442057125		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.04004538442057125 | validation: 0.10057155979378057]
	TIME [epoch: 7.68 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12879903163751777		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.12879903163751777 | validation: 0.05736876748255485]
	TIME [epoch: 7.73 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046574166765655706		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.046574166765655706 | validation: 0.03858653778358087]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03210784280916615		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.03210784280916615 | validation: 0.06417814673143388]
	TIME [epoch: 7.66 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046673099481612856		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.046673099481612856 | validation: 0.13800518357990518]
	TIME [epoch: 7.65 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09725539494710392		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.09725539494710392 | validation: 0.08087714128715731]
	TIME [epoch: 7.66 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0750359671609152		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.0750359671609152 | validation: 0.03806528860852988]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03189026928088652		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.03189026928088652 | validation: 0.05513015899592645]
	TIME [epoch: 7.68 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05328596986547493		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.05328596986547493 | validation: 0.03825882785927333]
	TIME [epoch: 7.67 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043530778808599346		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.043530778808599346 | validation: 0.06976563762482296]
	TIME [epoch: 7.67 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07816924645823596		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.07816924645823596 | validation: 0.03875470161574483]
	TIME [epoch: 7.68 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04651233595380355		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.04651233595380355 | validation: 0.04117686638332167]
	TIME [epoch: 7.73 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035948551894854984		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.035948551894854984 | validation: 0.052109729813043626]
	TIME [epoch: 7.67 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031009942416476213		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.031009942416476213 | validation: 0.09448001394185476]
	TIME [epoch: 7.67 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09731863739065824		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.09731863739065824 | validation: 0.15427303669052322]
	TIME [epoch: 7.67 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12234798643011371		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.12234798643011371 | validation: 0.09786566372125993]
	TIME [epoch: 7.69 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06356686562682623		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.06356686562682623 | validation: 0.045183551507204586]
	TIME [epoch: 7.72 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030098914880329522		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.030098914880329522 | validation: 0.04036405092754247]
	TIME [epoch: 7.67 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03821798564278528		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.03821798564278528 | validation: 0.03841175513746369]
	TIME [epoch: 7.68 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04897742745558187		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.04897742745558187 | validation: 0.07889257130220279]
	TIME [epoch: 7.67 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05030483899885381		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.05030483899885381 | validation: 0.07050058961569707]
	TIME [epoch: 7.67 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04956313423121618		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.04956313423121618 | validation: 0.06491209722335933]
	TIME [epoch: 7.71 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08289717943041038		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.08289717943041038 | validation: 0.06709136057664905]
	TIME [epoch: 7.68 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05577948332730877		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.05577948332730877 | validation: 0.031684248471559914]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03756072207460301		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.03756072207460301 | validation: 0.04806418980085483]
	TIME [epoch: 7.7 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04817048750021412		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.04817048750021412 | validation: 0.07759595624692583]
	TIME [epoch: 7.72 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05629926179826425		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.05629926179826425 | validation: 0.03548491676914087]
	TIME [epoch: 7.7 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03090517992659461		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.03090517992659461 | validation: 0.03535246392536542]
	TIME [epoch: 7.68 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05678282675627258		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.05678282675627258 | validation: 0.06709614059681468]
	TIME [epoch: 7.7 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05262258235650845		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.05262258235650845 | validation: 0.05523344630516914]
	TIME [epoch: 7.69 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03534394732596338		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.03534394732596338 | validation: 0.036386575299449025]
	TIME [epoch: 7.76 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03111430155123829		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.03111430155123829 | validation: 0.041133431978028365]
	TIME [epoch: 7.68 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05924970392009672		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.05924970392009672 | validation: 0.0318875856737122]
	TIME [epoch: 7.7 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03226759864187233		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.03226759864187233 | validation: 0.03305569117589925]
	TIME [epoch: 7.68 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045425550802497675		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.045425550802497675 | validation: 0.08399220886640313]
	TIME [epoch: 7.69 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07859245787102213		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.07859245787102213 | validation: 0.07641191925443469]
	TIME [epoch: 7.74 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04877209060338232		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.04877209060338232 | validation: 0.03453227660537467]
	TIME [epoch: 7.69 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032456334881768675		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.032456334881768675 | validation: 0.06193838338200916]
	TIME [epoch: 7.69 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05049789153552852		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.05049789153552852 | validation: 0.04415904610629343]
	TIME [epoch: 7.69 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03358404170048657		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.03358404170048657 | validation: 0.038663379067169386]
	TIME [epoch: 7.69 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04804325271559074		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.04804325271559074 | validation: 0.07134093169364195]
	TIME [epoch: 7.75 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0712277288029699		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.0712277288029699 | validation: 0.045518445546518224]
	TIME [epoch: 7.7 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037373361161324906		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.037373361161324906 | validation: 0.035872917422873275]
	TIME [epoch: 7.69 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030523820539037176		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.030523820539037176 | validation: 0.039340630683252005]
	TIME [epoch: 7.68 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03693876404991166		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.03693876404991166 | validation: 0.047186399974903315]
	TIME [epoch: 7.69 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03896202431787913		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.03896202431787913 | validation: 0.04311267965175482]
	TIME [epoch: 7.76 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053862077577182005		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.053862077577182005 | validation: 0.03126312516006283]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03655409550185338		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.03655409550185338 | validation: 0.03680102479978422]
	TIME [epoch: 7.69 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03873542561035858		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.03873542561035858 | validation: 0.03689114872484024]
	TIME [epoch: 7.69 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02763657846991628		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.02763657846991628 | validation: 0.08105328710532093]
	TIME [epoch: 7.7 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051439205960096		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.051439205960096 | validation: 0.03439529854574502]
	TIME [epoch: 7.8 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03360765303269032		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.03360765303269032 | validation: 0.04609756482838054]
	TIME [epoch: 7.68 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04191356189310369		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.04191356189310369 | validation: 0.17580084712353836]
	TIME [epoch: 7.69 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09989271532267933		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.09989271532267933 | validation: 0.06111940773512864]
	TIME [epoch: 7.68 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035642345325101944		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.035642345325101944 | validation: 0.04665788580215326]
	TIME [epoch: 7.72 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03867967439991998		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.03867967439991998 | validation: 0.05097169609801211]
	TIME [epoch: 7.71 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0315366264826339		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.0315366264826339 | validation: 0.029956726925419928]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043194634884203637		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.043194634884203637 | validation: 0.030911651202180375]
	TIME [epoch: 7.69 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03236505961548831		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.03236505961548831 | validation: 0.04721333049177481]
	TIME [epoch: 7.7 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06992959670253628		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.06992959670253628 | validation: 0.06945023262489382]
	TIME [epoch: 7.76 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04041469915694364		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.04041469915694364 | validation: 0.026318644625393052]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02377832300640247		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.02377832300640247 | validation: 0.03134283106763754]
	TIME [epoch: 7.69 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03125937607815479		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.03125937607815479 | validation: 0.04041566240342689]
	TIME [epoch: 7.69 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024441329997404223		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.024441329997404223 | validation: 0.02462618181076137]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047910282340087824		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.047910282340087824 | validation: 0.048391929457194297]
	TIME [epoch: 7.75 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03643075627882637		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.03643075627882637 | validation: 0.03167204042620081]
	TIME [epoch: 7.69 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02566756772758985		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.02566756772758985 | validation: 0.043700271345982894]
	TIME [epoch: 7.71 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043729360157203294		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.043729360157203294 | validation: 0.04500812452769523]
	TIME [epoch: 7.69 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059285018174432436		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.059285018174432436 | validation: 0.032872595678799535]
	TIME [epoch: 7.7 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027614790822634128		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.027614790822634128 | validation: 0.030181389667923775]
	TIME [epoch: 7.75 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03556031469086596		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.03556031469086596 | validation: 0.034139947439705456]
	TIME [epoch: 7.7 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02261408512739342		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.02261408512739342 | validation: 0.023856377408813002]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0265121446841454		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.0265121446841454 | validation: 0.04614589883627147]
	TIME [epoch: 7.68 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05880017356298635		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.05880017356298635 | validation: 0.02577547533013233]
	TIME [epoch: 7.69 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02619787007484597		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.02619787007484597 | validation: 0.027954607506885977]
	TIME [epoch: 7.71 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03997702623567125		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.03997702623567125 | validation: 0.05390494969410449]
	TIME [epoch: 7.67 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030321892749614752		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.030321892749614752 | validation: 0.04171499728087236]
	TIME [epoch: 7.67 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02702996103481249		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.02702996103481249 | validation: 0.026241979043189333]
	TIME [epoch: 7.67 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02650259506465893		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.02650259506465893 | validation: 0.06371077346127613]
	TIME [epoch: 7.71 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03738188520750008		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.03738188520750008 | validation: 0.028461724463655295]
	TIME [epoch: 7.69 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03201203669083113		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.03201203669083113 | validation: 0.06308007013780452]
	TIME [epoch: 7.67 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03539298892454847		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.03539298892454847 | validation: 0.0206914034617239]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020100708831969072		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.020100708831969072 | validation: 0.02046455422875983]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01868818375272377		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.01868818375272377 | validation: 0.04544381297874972]
	TIME [epoch: 7.74 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03589852207420107		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.03589852207420107 | validation: 0.06228124971019747]
	TIME [epoch: 7.69 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06140168604540751		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.06140168604540751 | validation: 0.0575405298446826]
	TIME [epoch: 7.68 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03556723430135088		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.03556723430135088 | validation: 0.03664504912794503]
	TIME [epoch: 7.68 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0422139040800642		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.0422139040800642 | validation: 0.031451511662017366]
	TIME [epoch: 7.68 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022887652345523556		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.022887652345523556 | validation: 0.020566794392710037]
	TIME [epoch: 7.73 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017873092280110966		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.017873092280110966 | validation: 0.02807371320499056]
	TIME [epoch: 7.68 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037677056712950505		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.037677056712950505 | validation: 0.0453231559392646]
	TIME [epoch: 7.67 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02903743876812797		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.02903743876812797 | validation: 0.04802479165425416]
	TIME [epoch: 7.68 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04184534767486015		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.04184534767486015 | validation: 0.0332709157750346]
	TIME [epoch: 7.68 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021431226308540118		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.021431226308540118 | validation: 0.043533920534978635]
	TIME [epoch: 7.72 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03138666353593625		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.03138666353593625 | validation: 0.03796998653230779]
	TIME [epoch: 7.68 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03741895659295018		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.03741895659295018 | validation: 0.027516936663478597]
	TIME [epoch: 7.67 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027644005221381537		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.027644005221381537 | validation: 0.022926026415088167]
	TIME [epoch: 7.68 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021486037321271606		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.021486037321271606 | validation: 0.029257302428297514]
	TIME [epoch: 7.68 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04562608010638716		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.04562608010638716 | validation: 0.04303675163382583]
	TIME [epoch: 7.72 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029292483627055756		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.029292483627055756 | validation: 0.060591184185488006]
	TIME [epoch: 7.68 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031921042577080075		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.031921042577080075 | validation: 0.02447569549935999]
	TIME [epoch: 7.68 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030782928579974903		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.030782928579974903 | validation: 0.029370534942202972]
	TIME [epoch: 7.68 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03251008118581644		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.03251008118581644 | validation: 0.031760303573565934]
	TIME [epoch: 7.7 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024374269844956664		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.024374269844956664 | validation: 0.01873375123870633]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019665996855856502		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.019665996855856502 | validation: 0.06069032789818304]
	TIME [epoch: 7.69 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03807913874678594		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.03807913874678594 | validation: 0.02873929209709987]
	TIME [epoch: 7.68 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024676322643171607		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.024676322643171607 | validation: 0.025640525729823744]
	TIME [epoch: 7.67 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023762059151349643		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.023762059151349643 | validation: 0.025140170198347293]
	TIME [epoch: 7.75 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03051823374260942		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.03051823374260942 | validation: 0.050326065125435276]
	TIME [epoch: 7.7 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025102973122384423		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.025102973122384423 | validation: 0.02651596698792607]
	TIME [epoch: 7.68 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01868826559014468		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.01868826559014468 | validation: 0.016943915085079614]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01974827017413428		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.01974827017413428 | validation: 0.039123664742091335]
	TIME [epoch: 7.69 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04128480355393507		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.04128480355393507 | validation: 0.0421877141003068]
	TIME [epoch: 7.75 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025392754790617075		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.025392754790617075 | validation: 0.016458629584879023]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015395180159239026		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.015395180159239026 | validation: 0.033399449446736396]
	TIME [epoch: 7.7 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031105636231211903		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.031105636231211903 | validation: 0.024876221891878273]
	TIME [epoch: 7.7 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024408063998722154		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.024408063998722154 | validation: 0.024835045208266567]
	TIME [epoch: 7.69 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029499554649020186		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.029499554649020186 | validation: 0.02094911838058347]
	TIME [epoch: 7.74 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015158923860064795		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.015158923860064795 | validation: 0.016448101255305127]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032048045506360104		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.032048045506360104 | validation: 0.0847309354841045]
	TIME [epoch: 7.7 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05123076173952079		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.05123076173952079 | validation: 0.02651063097777922]
	TIME [epoch: 7.7 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016025950490867434		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.016025950490867434 | validation: 0.01797640210925966]
	TIME [epoch: 7.69 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016843173291008852		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.016843173291008852 | validation: 0.07073862377172559]
	TIME [epoch: 7.74 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031142846319002836		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.031142846319002836 | validation: 0.016089748334298097]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016797365752971062		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.016797365752971062 | validation: 0.04033217511192985]
	TIME [epoch: 7.67 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03451436138183183		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.03451436138183183 | validation: 0.018618488533772196]
	TIME [epoch: 7.67 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015169421806305063		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.015169421806305063 | validation: 0.015807731620564987]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02107973557959717		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.02107973557959717 | validation: 0.03300676265941997]
	TIME [epoch: 7.69 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02787348983532119		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.02787348983532119 | validation: 0.04762848640823221]
	TIME [epoch: 7.67 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02761632308161787		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.02761632308161787 | validation: 0.015192827634480727]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021433349067710786		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.021433349067710786 | validation: 0.03252793191454608]
	TIME [epoch: 7.68 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03091971560940005		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.03091971560940005 | validation: 0.01679790011816583]
	TIME [epoch: 7.74 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015524650213890424		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.015524650213890424 | validation: 0.014404469606871192]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016341006249321947		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.016341006249321947 | validation: 0.03451881817699273]
	TIME [epoch: 7.68 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033883105405892996		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.033883105405892996 | validation: 0.021324359894241182]
	TIME [epoch: 7.69 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013615730063028419		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.013615730063028419 | validation: 0.015264018698419342]
	TIME [epoch: 7.68 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016503439006739465		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.016503439006739465 | validation: 0.029541660698668946]
	TIME [epoch: 7.73 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028163429658168412		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.028163429658168412 | validation: 0.027399924531351042]
	TIME [epoch: 7.69 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01613440800362836		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.01613440800362836 | validation: 0.021959219968013987]
	TIME [epoch: 7.69 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031568015485722994		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.031568015485722994 | validation: 0.034367357343991614]
	TIME [epoch: 7.68 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01683127991396611		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.01683127991396611 | validation: 0.029293101710311263]
	TIME [epoch: 7.69 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025155131553120977		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.025155131553120977 | validation: 0.015527887673208554]
	TIME [epoch: 7.72 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015018768159610153		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.015018768159610153 | validation: 0.022018635445912205]
	TIME [epoch: 7.68 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016893494019358864		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.016893494019358864 | validation: 0.0203609609113343]
	TIME [epoch: 7.68 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021515046897122754		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.021515046897122754 | validation: 0.02069467641201087]
	TIME [epoch: 7.68 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014239285437619582		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.014239285437619582 | validation: 0.01758553054674649]
	TIME [epoch: 7.71 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029030382685709306		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.029030382685709306 | validation: 0.028455805995671286]
	TIME [epoch: 7.72 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03232211959803487		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.03232211959803487 | validation: 0.023753008016822074]
	TIME [epoch: 7.68 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014052637312031066		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.014052637312031066 | validation: 0.013160669720790897]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015796473053720644		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.015796473053720644 | validation: 0.014064020813469585]
	TIME [epoch: 7.69 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012567469924450302		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.012567469924450302 | validation: 0.018431521703164325]
	TIME [epoch: 7.71 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061396316675838114		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.061396316675838114 | validation: 0.018976590288035614]
	TIME [epoch: 7.71 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0191712050819436		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.0191712050819436 | validation: 0.019930847028550712]
	TIME [epoch: 7.69 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015377693270442076		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.015377693270442076 | validation: 0.013724776171932768]
	TIME [epoch: 7.69 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015368365720003975		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.015368365720003975 | validation: 0.028575614226653147]
	TIME [epoch: 7.68 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018135818576290815		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.018135818576290815 | validation: 0.015353198432173143]
	TIME [epoch: 7.76 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011903327494622272		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.011903327494622272 | validation: 0.01665373278867298]
	TIME [epoch: 7.68 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02724192781834401		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.02724192781834401 | validation: 0.019817416052954017]
	TIME [epoch: 7.69 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02436350574100375		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.02436350574100375 | validation: 0.0156266652356468]
	TIME [epoch: 7.68 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01474530982087556		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.01474530982087556 | validation: 0.013979515366405494]
	TIME [epoch: 7.68 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011430028098099225		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.011430028098099225 | validation: 0.03705433671951985]
	TIME [epoch: 7.74 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027473746858417362		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.027473746858417362 | validation: 0.016384670434715764]
	TIME [epoch: 7.7 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022291721255962557		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.022291721255962557 | validation: 0.01783381357547871]
	TIME [epoch: 7.68 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01313512469755665		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.01313512469755665 | validation: 0.016384377626036284]
	TIME [epoch: 7.7 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017254237091728134		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.017254237091728134 | validation: 0.029836381185090223]
	TIME [epoch: 7.69 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018433091003295356		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.018433091003295356 | validation: 0.015205525418114128]
	TIME [epoch: 7.75 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018005321503369302		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.018005321503369302 | validation: 0.01950962175852816]
	TIME [epoch: 7.69 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012805797287562678		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.012805797287562678 | validation: 0.029365604798833204]
	TIME [epoch: 7.67 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02368424760622267		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.02368424760622267 | validation: 0.014157788544027935]
	TIME [epoch: 7.69 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012079443041681317		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.012079443041681317 | validation: 0.01535514339414727]
	TIME [epoch: 7.72 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012886463187326658		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.012886463187326658 | validation: 0.029037568668229374]
	TIME [epoch: 7.74 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023408786083310172		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.023408786083310172 | validation: 0.019054181260584425]
	TIME [epoch: 7.68 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014550212323412531		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.014550212323412531 | validation: 0.01684173196154714]
	TIME [epoch: 7.69 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012242998312744632		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.012242998312744632 | validation: 0.020251717493950327]
	TIME [epoch: 7.7 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02007485841478167		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.02007485841478167 | validation: 0.014875177150845957]
	TIME [epoch: 7.71 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022647997616314874		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.022647997616314874 | validation: 0.01677748128237381]
	TIME [epoch: 7.72 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0178507349299519		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.0178507349299519 | validation: 0.016445739061424245]
	TIME [epoch: 7.68 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01376451060070685		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.01376451060070685 | validation: 0.020200178480968764]
	TIME [epoch: 7.68 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018346468048237106		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.018346468048237106 | validation: 0.017738959509705988]
	TIME [epoch: 7.71 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01068824702416026		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.01068824702416026 | validation: 0.017316031156133324]
	TIME [epoch: 7.73 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013123266297126393		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.013123266297126393 | validation: 0.029233808188119322]
	TIME [epoch: 7.68 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01956263666286047		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.01956263666286047 | validation: 0.014297553840650006]
	TIME [epoch: 7.67 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011997947388256625		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.011997947388256625 | validation: 0.012290045638804706]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_426.pth
	Model improved!!!
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020020547311722092		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.020020547311722092 | validation: 0.022418208358620892]
	TIME [epoch: 7.68 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014528408930121656		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.014528408930121656 | validation: 0.013003779829217306]
	TIME [epoch: 7.73 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01035823892979201		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.01035823892979201 | validation: 0.01343400730435429]
	TIME [epoch: 7.7 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018962738709084628		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.018962738709084628 | validation: 0.0169662036073878]
	TIME [epoch: 7.68 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02025924288578535		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.02025924288578535 | validation: 0.02314706609063755]
	TIME [epoch: 7.68 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011309580027105175		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.011309580027105175 | validation: 0.014641927299263714]
	TIME [epoch: 7.67 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01968686502467802		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.01968686502467802 | validation: 0.0186101335307238]
	TIME [epoch: 7.75 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017761755001819625		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.017761755001819625 | validation: 0.02100609279572714]
	TIME [epoch: 7.69 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01524793381014948		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.01524793381014948 | validation: 0.012459905433536086]
	TIME [epoch: 7.68 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009747172203955667		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.009747172203955667 | validation: 0.015199287436103822]
	TIME [epoch: 7.67 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020043433984670598		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.020043433984670598 | validation: 0.017003504765087316]
	TIME [epoch: 7.69 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012172048779466047		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.012172048779466047 | validation: 0.014411225957294927]
	TIME [epoch: 7.74 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012968688450579056		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.012968688450579056 | validation: 0.015081586687619653]
	TIME [epoch: 7.68 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010771543694863975		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.010771543694863975 | validation: 0.01809468177597259]
	TIME [epoch: 7.7 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019645188462751064		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.019645188462751064 | validation: 0.0119804358594287]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010979454457734976		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.010979454457734976 | validation: 0.013946996315189836]
	TIME [epoch: 7.7 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020162128046789442		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.020162128046789442 | validation: 0.02083001719376839]
	TIME [epoch: 7.72 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011586002213599544		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.011586002213599544 | validation: 0.01875834605382152]
	TIME [epoch: 7.67 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023287610400061453		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.023287610400061453 | validation: 0.023761162248878094]
	TIME [epoch: 7.68 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013754258644597938		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.013754258644597938 | validation: 0.010986332550483844]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011279225688470472		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.011279225688470472 | validation: 0.02611510039515305]
	TIME [epoch: 7.72 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016402840974811896		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.016402840974811896 | validation: 0.015172752277217778]
	TIME [epoch: 7.68 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013227106669515466		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.013227106669515466 | validation: 0.013912545349121867]
	TIME [epoch: 7.67 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010025065995267963		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.010025065995267963 | validation: 0.010796557150911198]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_450.pth
	Model improved!!!
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018418894080329644		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.018418894080329644 | validation: 0.04261894559839763]
	TIME [epoch: 7.69 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01861705773018165		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.01861705773018165 | validation: 0.06269912048400479]
	TIME [epoch: 7.73 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03799383949174369		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.03799383949174369 | validation: 0.013281826923198327]
	TIME [epoch: 7.69 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00892974942267366		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.00892974942267366 | validation: 0.011188652718105322]
	TIME [epoch: 7.68 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01632019048392826		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.01632019048392826 | validation: 0.018651147771857314]
	TIME [epoch: 7.68 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011671559858203878		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.011671559858203878 | validation: 0.010555441334480997]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010741214169785403		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.010741214169785403 | validation: 0.009138595450542138]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007799597492998063		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.007799597492998063 | validation: 0.012464148165660813]
	TIME [epoch: 7.68 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01764837292859759		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.01764837292859759 | validation: 0.013282546986776191]
	TIME [epoch: 7.68 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009067143768111153		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.009067143768111153 | validation: 0.011695475391486323]
	TIME [epoch: 7.69 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011923639234011149		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.011923639234011149 | validation: 0.015861869230899064]
	TIME [epoch: 7.7 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02011576977054451		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.02011576977054451 | validation: 0.023322178603285754]
	TIME [epoch: 7.72 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016189026894118167		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.016189026894118167 | validation: 0.010673521544444526]
	TIME [epoch: 7.68 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008573709667300568		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.008573709667300568 | validation: 0.014634917633722083]
	TIME [epoch: 7.67 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00909250813337131		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.00909250813337131 | validation: 0.009718242671685372]
	TIME [epoch: 7.67 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008988198116581556		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.008988198116581556 | validation: 0.017834641785322847]
	TIME [epoch: 7.69 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0180118503260181		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.0180118503260181 | validation: 0.009272588739737058]
	TIME [epoch: 7.71 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021231716486378328		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.021231716486378328 | validation: 0.013876886002503218]
	TIME [epoch: 7.7 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0095311389086348		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.0095311389086348 | validation: 0.010306305241476469]
	TIME [epoch: 7.65 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008200155852903544		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.008200155852903544 | validation: 0.013412377546887181]
	TIME [epoch: 7.67 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016308129849204316		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.016308129849204316 | validation: 0.012941289325855466]
	TIME [epoch: 7.71 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010113469917043592		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.010113469917043592 | validation: 0.010123818779386163]
	TIME [epoch: 7.69 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00820979083233541		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.00820979083233541 | validation: 0.012600187813709699]
	TIME [epoch: 7.68 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01568249379410837		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.01568249379410837 | validation: 0.02308138710319775]
	TIME [epoch: 7.68 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014779127162088235		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.014779127162088235 | validation: 0.012302604663365118]
	TIME [epoch: 7.67 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007611862159306039		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.007611862159306039 | validation: 0.010173971755393377]
	TIME [epoch: 7.73 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00708029562468224		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.00708029562468224 | validation: 0.011043360734624228]
	TIME [epoch: 7.71 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017215977796041258		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.017215977796041258 | validation: 0.011451722897700444]
	TIME [epoch: 7.7 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0113679278117939		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.0113679278117939 | validation: 0.014925839135859903]
	TIME [epoch: 7.69 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05912776755736352		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.05912776755736352 | validation: 0.037284082089473765]
	TIME [epoch: 7.71 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018040408251951082		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.018040408251951082 | validation: 0.015621444467117003]
	TIME [epoch: 7.74 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008977748684718557		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.008977748684718557 | validation: 0.011578288245676586]
	TIME [epoch: 7.7 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007536962248998167		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.007536962248998167 | validation: 0.01039346615157247]
	TIME [epoch: 7.69 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008059324678977334		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.008059324678977334 | validation: 0.021100056793099484]
	TIME [epoch: 7.68 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016316998938383603		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.016316998938383603 | validation: 0.011218663691615572]
	TIME [epoch: 7.68 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007701805408193069		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.007701805408193069 | validation: 0.011456478749152834]
	TIME [epoch: 7.75 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00932261401417035		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.00932261401417035 | validation: 0.016155655026929828]
	TIME [epoch: 7.69 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01425643914335811		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.01425643914335811 | validation: 0.009811140535021579]
	TIME [epoch: 7.71 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009183381053996152		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.009183381053996152 | validation: 0.009457881174001582]
	TIME [epoch: 7.69 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007690124560520781		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.007690124560520781 | validation: 0.012673039530295815]
	TIME [epoch: 7.69 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012846783851855458		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.012846783851855458 | validation: 0.018482937934606015]
	TIME [epoch: 7.73 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01456357509863111		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.01456357509863111 | validation: 0.011412637716358455]
	TIME [epoch: 7.68 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008412272500633079		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.008412272500633079 | validation: 0.010784074483561515]
	TIME [epoch: 7.67 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008965277341237957		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.008965277341237957 | validation: 0.02008830885352615]
	TIME [epoch: 7.68 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012689738555212523		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.012689738555212523 | validation: 0.023398317677519317]
	TIME [epoch: 7.68 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015586043410427658		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.015586043410427658 | validation: 0.014332505841436562]
	TIME [epoch: 7.73 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009218308477394875		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.009218308477394875 | validation: 0.010540346838665133]
	TIME [epoch: 7.67 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007367577472668845		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.007367577472668845 | validation: 0.009645113311979907]
	TIME [epoch: 7.68 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013075653304989138		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.013075653304989138 | validation: 0.0192035591337108]
	TIME [epoch: 7.67 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011168761317358403		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.011168761317358403 | validation: 0.011037223260415396]
	TIME [epoch: 7.68 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008452778837663568		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.008452778837663568 | validation: 0.009058684063014443]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008142576126658266		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.008142576126658266 | validation: 0.01247783843907674]
	TIME [epoch: 7.69 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009284786766542943		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.009284786766542943 | validation: 0.025687128624562627]
	TIME [epoch: 7.68 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012973955008789465		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.012973955008789465 | validation: 0.00995994241920471]
	TIME [epoch: 7.68 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009111051302158639		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.009111051302158639 | validation: 0.012419542373285991]
	TIME [epoch: 7.73 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009733638137105514		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.009733638137105514 | validation: 0.009309487954352193]
	TIME [epoch: 7.69 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0062813547370422496		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.0062813547370422496 | validation: 0.00933150713612973]
	TIME [epoch: 7.67 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01665853373397462		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.01665853373397462 | validation: 0.02885390706309008]
	TIME [epoch: 7.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01588992168494144		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.01588992168494144 | validation: 0.009712237424244204]
	TIME [epoch: 7.68 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007160353603130815		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.007160353603130815 | validation: 0.009017489846549636]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010417425910433247		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.010417425910433247 | validation: 0.00906532498053942]
	TIME [epoch: 7.67 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008821326877991049		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.008821326877991049 | validation: 0.010821913523807292]
	TIME [epoch: 7.67 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012899740206669872		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.012899740206669872 | validation: 0.012409454174868089]
	TIME [epoch: 7.67 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007194751325754695		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.007194751325754695 | validation: 0.008371599980418186]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008225738788583268		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.008225738788583268 | validation: 0.010989228778576816]
	TIME [epoch: 7.73 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013883043745044846		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.013883043745044846 | validation: 0.015064486635682484]
	TIME [epoch: 7.67 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008386127510743026		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.008386127510743026 | validation: 0.008552476021152552]
	TIME [epoch: 7.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0100145469175199		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.0100145469175199 | validation: 0.019566152245465224]
	TIME [epoch: 7.67 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010765258624553789		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.010765258624553789 | validation: 0.012252486651348076]
	TIME [epoch: 7.69 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012271774407691356		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.012271774407691356 | validation: 0.013241230937605914]
	TIME [epoch: 7.72 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076765494591034145		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.0076765494591034145 | validation: 0.009825385591808955]
	TIME [epoch: 7.67 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008942479111665704		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.008942479111665704 | validation: 0.019949493044160875]
	TIME [epoch: 7.67 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009508085106171271		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.009508085106171271 | validation: 0.013935015788489206]
	TIME [epoch: 7.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009576252500186665		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.009576252500186665 | validation: 0.010504997973495989]
	TIME [epoch: 7.68 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010709339746699638		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.010709339746699638 | validation: 0.011456837291058428]
	TIME [epoch: 7.71 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007847318852523174		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.007847318852523174 | validation: 0.00869731271103693]
	TIME [epoch: 7.67 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006828913485652618		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.006828913485652618 | validation: 0.013950860941355822]
	TIME [epoch: 7.67 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012798840525004948		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.012798840525004948 | validation: 0.011289247692638248]
	TIME [epoch: 7.66 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00819788191600797		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.00819788191600797 | validation: 0.008469304903705411]
	TIME [epoch: 7.7 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006222984949776213		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.006222984949776213 | validation: 0.011217466101618875]
	TIME [epoch: 7.69 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011776452586024054		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.011776452586024054 | validation: 0.011797288071211514]
	TIME [epoch: 7.67 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008259787248869646		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.008259787248869646 | validation: 0.013150586160367085]
	TIME [epoch: 7.67 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007249929285922919		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.007249929285922919 | validation: 0.007482918658595867]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_533.pth
	Model improved!!!
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00877890593030099		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.00877890593030099 | validation: 0.01698673361387163]
	TIME [epoch: 7.72 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009715749717939813		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.009715749717939813 | validation: 0.007842709002788854]
	TIME [epoch: 7.68 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007499668792900061		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.007499668792900061 | validation: 0.008866144540192068]
	TIME [epoch: 7.67 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007562655326837549		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.007562655326837549 | validation: 0.01669416529389855]
	TIME [epoch: 7.67 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016302565519710793		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.016302565519710793 | validation: 0.008880851936570965]
	TIME [epoch: 7.69 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007070039155770287		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.007070039155770287 | validation: 0.008148309808220323]
	TIME [epoch: 7.74 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006097482144151968		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.006097482144151968 | validation: 0.012856931061264072]
	TIME [epoch: 7.68 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009249144915204234		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.009249144915204234 | validation: 0.007156194436362952]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006522815002386123		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.006522815002386123 | validation: 0.01358338894718079]
	TIME [epoch: 7.69 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010249449778855375		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.010249449778855375 | validation: 0.008515752807294712]
	TIME [epoch: 7.7 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005386528493465208		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.005386528493465208 | validation: 0.007887075435811589]
	TIME [epoch: 7.74 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0073258845396826176		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.0073258845396826176 | validation: 0.012458264378987616]
	TIME [epoch: 7.71 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015289913767919924		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.015289913767919924 | validation: 0.01803951927673257]
	TIME [epoch: 7.67 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008234900490034553		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.008234900490034553 | validation: 0.008670809009638205]
	TIME [epoch: 7.67 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008837046960884254		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.008837046960884254 | validation: 0.014122711935802711]
	TIME [epoch: 7.7 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008315473632798224		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.008315473632798224 | validation: 0.012223570599511487]
	TIME [epoch: 7.73 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007740514103417126		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.007740514103417126 | validation: 0.008382930100792368]
	TIME [epoch: 7.69 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008216162996607403		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.008216162996607403 | validation: 0.009467248081168155]
	TIME [epoch: 7.68 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010009656530459071		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.010009656530459071 | validation: 0.010300037092983664]
	TIME [epoch: 7.68 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007799434499775026		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.007799434499775026 | validation: 0.010539533047489228]
	TIME [epoch: 7.71 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007316811060644737		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.007316811060644737 | validation: 0.008998854285695697]
	TIME [epoch: 7.73 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0067974414598326455		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.0067974414598326455 | validation: 0.007870257737342548]
	TIME [epoch: 7.69 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00936726693397235		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.00936726693397235 | validation: 0.013791883790987232]
	TIME [epoch: 7.71 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010161662815061031		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.010161662815061031 | validation: 0.008351025955469597]
	TIME [epoch: 7.7 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00579677908885387		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.00579677908885387 | validation: 0.00796046934793969]
	TIME [epoch: 7.74 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009633296182728002		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.009633296182728002 | validation: 0.012543102036928978]
	TIME [epoch: 7.72 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072605238758546645		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.0072605238758546645 | validation: 0.017040210439316375]
	TIME [epoch: 7.71 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009668014635704904		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.009668014635704904 | validation: 0.007601851453963487]
	TIME [epoch: 7.71 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007818809256942625		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.007818809256942625 | validation: 0.011395964887251381]
	TIME [epoch: 7.71 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009384772970060466		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.009384772970060466 | validation: 0.008457879668661348]
	TIME [epoch: 7.75 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00713693159177385		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.00713693159177385 | validation: 0.01141586342165541]
	TIME [epoch: 7.72 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013109851710416811		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.013109851710416811 | validation: 0.011740884007613684]
	TIME [epoch: 7.71 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007362300496878888		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.007362300496878888 | validation: 0.010461835821147894]
	TIME [epoch: 7.7 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005915477634569658		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.005915477634569658 | validation: 0.008974856744938352]
	TIME [epoch: 7.69 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007660549134755356		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.007660549134755356 | validation: 0.01632039603034683]
	TIME [epoch: 7.76 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008441109280542028		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.008441109280542028 | validation: 0.0076181119390960434]
	TIME [epoch: 7.73 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007409555823238946		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.007409555823238946 | validation: 0.00930229781669316]
	TIME [epoch: 7.69 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006077955949943232		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.006077955949943232 | validation: 0.009197440484065896]
	TIME [epoch: 7.7 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010086509803231218		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.010086509803231218 | validation: 0.007163541831225223]
	TIME [epoch: 7.69 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008360201041375146		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.008360201041375146 | validation: 0.007531312086509591]
	TIME [epoch: 7.76 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00585119247363301		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.00585119247363301 | validation: 0.009060803724531495]
	TIME [epoch: 7.69 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007084380185640182		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.007084380185640182 | validation: 0.008647894671009114]
	TIME [epoch: 7.7 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007305838328605552		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.007305838328605552 | validation: 0.010082960340754461]
	TIME [epoch: 7.69 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006639961596138595		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.006639961596138595 | validation: 0.007954971728986765]
	TIME [epoch: 7.7 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008245376058876326		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.008245376058876326 | validation: 0.012343748186774849]
	TIME [epoch: 7.78 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007228516924768887		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.007228516924768887 | validation: 0.006991078766335078]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005940060439100001		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.005940060439100001 | validation: 0.008744896185362444]
	TIME [epoch: 7.7 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008919562268243293		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.008919562268243293 | validation: 0.00858429382729577]
	TIME [epoch: 7.7 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006861465693523165		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.006861465693523165 | validation: 0.01176872557691798]
	TIME [epoch: 7.7 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0078123312651164276		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.0078123312651164276 | validation: 0.008765928474265241]
	TIME [epoch: 7.75 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008641365213184142		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.008641365213184142 | validation: 0.010419319898901498]
	TIME [epoch: 7.7 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0070196683144387925		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.0070196683144387925 | validation: 0.007382366232203518]
	TIME [epoch: 7.71 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005518115873922657		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.005518115873922657 | validation: 0.007540689171003097]
	TIME [epoch: 7.7 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009304892877643932		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.009304892877643932 | validation: 0.012193082769199271]
	TIME [epoch: 7.74 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012049556108304722		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.012049556108304722 | validation: 0.008513793007365687]
	TIME [epoch: 7.71 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005707565616811152		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.005707565616811152 | validation: 0.007167567913173769]
	TIME [epoch: 7.71 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005144562977380191		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.005144562977380191 | validation: 0.0070183433319114565]
	TIME [epoch: 7.7 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005505530895221793		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.005505530895221793 | validation: 0.013725867416108158]
	TIME [epoch: 7.7 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010328323689757373		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.010328323689757373 | validation: 0.009285676564081632]
	TIME [epoch: 7.77 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007193289205488777		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.007193289205488777 | validation: 0.006512330529174764]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006174061316528967		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.006174061316528967 | validation: 0.008782130887043667]
	TIME [epoch: 7.72 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008175068746427733		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.008175068746427733 | validation: 0.008175940214566526]
	TIME [epoch: 7.7 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005514633486580236		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.005514633486580236 | validation: 0.010111392133579613]
	TIME [epoch: 7.71 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007358054356667096		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.007358054356667096 | validation: 0.007886153025013328]
	TIME [epoch: 7.78 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009535502138717475		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.009535502138717475 | validation: 0.007761121108714323]
	TIME [epoch: 7.72 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005741258418439028		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.005741258418439028 | validation: 0.0066810162330483945]
	TIME [epoch: 7.7 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006550054102288405		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.006550054102288405 | validation: 0.010489874605865528]
	TIME [epoch: 7.72 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005796722249510222		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.005796722249510222 | validation: 0.008552702766066303]
	TIME [epoch: 7.72 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006246768374829661		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.006246768374829661 | validation: 0.009314486489054268]
	TIME [epoch: 7.73 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00887448047380695		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.00887448047380695 | validation: 0.00783521450716068]
	TIME [epoch: 7.69 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007711923848076197		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.007711923848076197 | validation: 0.008382659988141894]
	TIME [epoch: 7.69 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005777052678983887		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.005777052678983887 | validation: 0.008518744654090834]
	TIME [epoch: 7.69 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006795336575743056		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.006795336575743056 | validation: 0.007625903103579914]
	TIME [epoch: 7.7 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005737533655131813		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.005737533655131813 | validation: 0.008406086787793757]
	TIME [epoch: 7.72 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009745491181533242		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.009745491181533242 | validation: 0.011815767362059613]
	TIME [epoch: 7.68 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071142646237844715		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.0071142646237844715 | validation: 0.007110849687682503]
	TIME [epoch: 7.7 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005390346788510877		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.005390346788510877 | validation: 0.008763057902030453]
	TIME [epoch: 7.7 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008090756766851911		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.008090756766851911 | validation: 0.006934234492643035]
	TIME [epoch: 7.73 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006313956978303623		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.006313956978303623 | validation: 0.007423252391504764]
	TIME [epoch: 7.73 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006259665059669295		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.006259665059669295 | validation: 0.010280625716261503]
	TIME [epoch: 7.7 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006278250298037576		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.006278250298037576 | validation: 0.006729989316578158]
	TIME [epoch: 7.71 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006899485999356088		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.006899485999356088 | validation: 0.007044596265027282]
	TIME [epoch: 7.71 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005583251555901529		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.005583251555901529 | validation: 0.009288353090479186]
	TIME [epoch: 7.76 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0069595973387659524		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.0069595973387659524 | validation: 0.006542998110766533]
	TIME [epoch: 7.71 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005726273244280213		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.005726273244280213 | validation: 0.008399749486909497]
	TIME [epoch: 7.7 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009057894822267857		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.009057894822267857 | validation: 0.007857880881862872]
	TIME [epoch: 7.69 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005954184445833158		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.005954184445833158 | validation: 0.0072276257366557265]
	TIME [epoch: 7.69 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005577739602840056		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.005577739602840056 | validation: 0.010511390440878692]
	TIME [epoch: 7.74 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007241934910731575		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.007241934910731575 | validation: 0.008817649134295516]
	TIME [epoch: 7.7 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006528396609685853		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.006528396609685853 | validation: 0.006753361200035083]
	TIME [epoch: 7.68 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006186737981648302		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.006186737981648302 | validation: 0.008028439505756053]
	TIME [epoch: 7.68 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005519907422671181		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.005519907422671181 | validation: 0.006527568896063695]
	TIME [epoch: 7.68 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006224061463054854		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.006224061463054854 | validation: 0.009701645625637285]
	TIME [epoch: 7.74 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008054120922887266		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.008054120922887266 | validation: 0.00788476140479166]
	TIME [epoch: 7.69 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00603023920661362		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.00603023920661362 | validation: 0.007185787733169103]
	TIME [epoch: 7.68 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005134346328969258		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.005134346328969258 | validation: 0.007595085511207394]
	TIME [epoch: 7.69 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005873107680847432		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.005873107680847432 | validation: 0.009266216683853412]
	TIME [epoch: 7.69 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014505400870290825		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.014505400870290825 | validation: 0.015444366552077595]
	TIME [epoch: 7.75 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00657417339784057		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.00657417339784057 | validation: 0.008474124221176507]
	TIME [epoch: 7.69 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005850507392058421		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.005850507392058421 | validation: 0.007506964018868697]
	TIME [epoch: 7.69 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006259855494816761		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.006259855494816761 | validation: 0.007977409945007253]
	TIME [epoch: 7.68 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00508182746104713		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.00508182746104713 | validation: 0.009653715093340117]
	TIME [epoch: 7.69 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00747129486745665		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.00747129486745665 | validation: 0.00753834474416259]
	TIME [epoch: 7.75 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005225457876037787		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.005225457876037787 | validation: 0.0074291783843802375]
	TIME [epoch: 7.69 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00582164439785351		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.00582164439785351 | validation: 0.007627875708750472]
	TIME [epoch: 7.69 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005720716997491789		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.005720716997491789 | validation: 0.008824551482583091]
	TIME [epoch: 7.69 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006248041610148875		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.006248041610148875 | validation: 0.011165217484461028]
	TIME [epoch: 7.71 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007220629538907366		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.007220629538907366 | validation: 0.007367565254604231]
	TIME [epoch: 7.74 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007169011288981392		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.007169011288981392 | validation: 0.007530728792148587]
	TIME [epoch: 7.7 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00524210989010376		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.00524210989010376 | validation: 0.005868172662428444]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_643.pth
	Model improved!!!
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005335872574728634		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.005335872574728634 | validation: 0.010211772234377133]
	TIME [epoch: 7.68 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0068425852830971325		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.0068425852830971325 | validation: 0.009502253936373509]
	TIME [epoch: 7.72 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006696194518184266		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.006696194518184266 | validation: 0.006588666878019713]
	TIME [epoch: 7.71 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004943472518804568		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.004943472518804568 | validation: 0.006995481223920114]
	TIME [epoch: 7.69 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051473842509816466		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.0051473842509816466 | validation: 0.006219664835751504]
	TIME [epoch: 7.69 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005792177082518588		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.005792177082518588 | validation: 0.008106174600899994]
	TIME [epoch: 7.68 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00501220884604904		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.00501220884604904 | validation: 0.010841052568286297]
	TIME [epoch: 7.73 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00820769296089476		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.00820769296089476 | validation: 0.007577717341940479]
	TIME [epoch: 7.69 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006383414714552799		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.006383414714552799 | validation: 0.007749796692070592]
	TIME [epoch: 7.69 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00535565393456542		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.00535565393456542 | validation: 0.008716816551212123]
	TIME [epoch: 7.68 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00541532007261997		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.00541532007261997 | validation: 0.006382419019612938]
	TIME [epoch: 7.68 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004400608992107891		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.004400608992107891 | validation: 0.0062500306065872676]
	TIME [epoch: 7.74 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005777616239844354		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.005777616239844354 | validation: 0.012687689746736053]
	TIME [epoch: 7.69 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007906125747085185		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.007906125747085185 | validation: 0.009202184971872845]
	TIME [epoch: 7.68 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006796943192873529		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.006796943192873529 | validation: 0.0071822506736264075]
	TIME [epoch: 7.68 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005357870094461106		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.005357870094461106 | validation: 0.00937223462785877]
	TIME [epoch: 7.7 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00617653842447273		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.00617653842447273 | validation: 0.009915025757949928]
	TIME [epoch: 7.75 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006087187574803648		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.006087187574803648 | validation: 0.00863313535910196]
	TIME [epoch: 7.69 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005820126986639668		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.005820126986639668 | validation: 0.0058199939342063155]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005582182622901383		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.005582182622901383 | validation: 0.006835054324053967]
	TIME [epoch: 7.72 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010271556921430596		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.010271556921430596 | validation: 0.006390112597273746]
	TIME [epoch: 7.72 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004997643345307959		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.004997643345307959 | validation: 0.00595409267539608]
	TIME [epoch: 7.76 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047155916186784694		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.0047155916186784694 | validation: 0.007542549404672085]
	TIME [epoch: 7.71 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005797186228851377		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.005797186228851377 | validation: 0.006432646133478857]
	TIME [epoch: 7.7 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006677306473939528		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.006677306473939528 | validation: 0.008029504590248367]
	TIME [epoch: 7.7 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005199539494071409		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.005199539494071409 | validation: 0.006733945077752447]
	TIME [epoch: 7.72 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005822690580490834		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.005822690580490834 | validation: 0.005757572794960095]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_670.pth
	Model improved!!!
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004473996862446352		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.004473996862446352 | validation: 0.006659868110860352]
	TIME [epoch: 7.72 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006011342433233017		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.006011342433233017 | validation: 0.007233896518832974]
	TIME [epoch: 7.7 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005355060454236668		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.005355060454236668 | validation: 0.0067234497101137675]
	TIME [epoch: 7.72 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005330175261213465		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.005330175261213465 | validation: 0.007067986761915491]
	TIME [epoch: 7.76 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004819509297763188		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.004819509297763188 | validation: 0.008898090499626143]
	TIME [epoch: 7.73 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006681254536751961		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.006681254536751961 | validation: 0.006971995921198617]
	TIME [epoch: 7.72 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054506318591858935		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.0054506318591858935 | validation: 0.006185173626385465]
	TIME [epoch: 7.71 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041637133164327935		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.0041637133164327935 | validation: 0.006964712019728772]
	TIME [epoch: 7.71 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057359206269943595		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.0057359206269943595 | validation: 0.007451607443013622]
	TIME [epoch: 7.78 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004992894907988408		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.004992894907988408 | validation: 0.007916914631503054]
	TIME [epoch: 7.71 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00712405601013062		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.00712405601013062 | validation: 0.0064763457988960375]
	TIME [epoch: 7.72 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004749719062877985		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.004749719062877985 | validation: 0.006081495672560171]
	TIME [epoch: 7.72 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00531208427198983		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.00531208427198983 | validation: 0.005629052466389084]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_683.pth
	Model improved!!!
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005601951028593806		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.005601951028593806 | validation: 0.007148640489202574]
	TIME [epoch: 7.79 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005587034401544108		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.005587034401544108 | validation: 0.006361731466381309]
	TIME [epoch: 7.7 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004965603376854837		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.004965603376854837 | validation: 0.007227865625050939]
	TIME [epoch: 7.73 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006047376866409606		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.006047376866409606 | validation: 0.006618946947822163]
	TIME [epoch: 7.7 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004692984276419743		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.004692984276419743 | validation: 0.006354470719685992]
	TIME [epoch: 7.75 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004610305474779367		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.004610305474779367 | validation: 0.007389568374413215]
	TIME [epoch: 7.75 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006025923611989097		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.006025923611989097 | validation: 0.009434938819769013]
	TIME [epoch: 7.7 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006305863290327807		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.006305863290327807 | validation: 0.008328026661918871]
	TIME [epoch: 7.7 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005369049266432894		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.005369049266432894 | validation: 0.006267699397215941]
	TIME [epoch: 7.73 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005039938425777622		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.005039938425777622 | validation: 0.006393149380978021]
	TIME [epoch: 7.71 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004815453106138485		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.004815453106138485 | validation: 0.0064058678588020035]
	TIME [epoch: 7.75 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005439170571961472		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.005439170571961472 | validation: 0.008181954501622169]
	TIME [epoch: 7.7 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005934934803762696		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.005934934803762696 | validation: 0.005893164519551809]
	TIME [epoch: 7.7 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005229567083207308		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.005229567083207308 | validation: 0.005808386477292925]
	TIME [epoch: 7.69 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004562355241196801		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.004562355241196801 | validation: 0.007478713493911159]
	TIME [epoch: 7.74 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005469656364678613		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.005469656364678613 | validation: 0.007251590065314618]
	TIME [epoch: 7.75 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005854426926576801		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.005854426926576801 | validation: 0.006115080869168526]
	TIME [epoch: 7.71 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004289409693609576		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.004289409693609576 | validation: 0.006201159995265139]
	TIME [epoch: 7.74 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004473089052748099		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.004473089052748099 | validation: 0.006171805177400974]
	TIME [epoch: 7.73 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005419120892753756		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.005419120892753756 | validation: 0.005992383449125709]
	TIME [epoch: 7.79 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005017531703877679		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.005017531703877679 | validation: 0.006093263196683909]
	TIME [epoch: 7.75 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004577372309203333		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.004577372309203333 | validation: 0.005213938652003229]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_705.pth
	Model improved!!!
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005552884833230965		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.005552884833230965 | validation: 0.006852169787758393]
	TIME [epoch: 7.71 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005751156538397438		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.005751156538397438 | validation: 0.008242864436795243]
	TIME [epoch: 7.71 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005480418278303131		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.005480418278303131 | validation: 0.006757193009612708]
	TIME [epoch: 7.77 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047481505239471405		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.0047481505239471405 | validation: 0.006483201242901277]
	TIME [epoch: 7.72 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004602861647204708		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.004602861647204708 | validation: 0.005640578147700732]
	TIME [epoch: 7.72 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00587952507057985		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.00587952507057985 | validation: 0.0057550554560363565]
	TIME [epoch: 7.72 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004645680874170593		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.004645680874170593 | validation: 0.0057473811895979105]
	TIME [epoch: 7.73 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005240770038670126		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.005240770038670126 | validation: 0.0061469977980789575]
	TIME [epoch: 7.78 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004811840278045423		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.004811840278045423 | validation: 0.007451423950207037]
	TIME [epoch: 7.73 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005799740466745232		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.005799740466745232 | validation: 0.005946448276034317]
	TIME [epoch: 7.71 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004160071062103506		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.004160071062103506 | validation: 0.007981421249509991]
	TIME [epoch: 7.72 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007991724149915395		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.007991724149915395 | validation: 0.010841003769263403]
	TIME [epoch: 7.73 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0067779001587625		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.0067779001587625 | validation: 0.00670349159901912]
	TIME [epoch: 7.8 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005186923054478809		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.005186923054478809 | validation: 0.005536239976374761]
	TIME [epoch: 7.71 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004877501333430603		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.004877501333430603 | validation: 0.005181892996548412]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005085568346037116		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.005085568346037116 | validation: 0.007626193984219689]
	TIME [epoch: 7.69 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004893979772420116		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.004893979772420116 | validation: 0.00545104355784372]
	TIME [epoch: 7.74 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004439702450615309		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.004439702450615309 | validation: 0.00716059119519744]
	TIME [epoch: 7.7 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053309882134977655		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.0053309882134977655 | validation: 0.006713968153220918]
	TIME [epoch: 7.69 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005115627425637523		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.005115627425637523 | validation: 0.005808978777217557]
	TIME [epoch: 7.69 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004413407954675359		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.004413407954675359 | validation: 0.00704955458103842]
	TIME [epoch: 7.69 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005631753571326922		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.005631753571326922 | validation: 0.0071595381186966124]
	TIME [epoch: 7.74 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00483664342967689		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.00483664342967689 | validation: 0.005854002807684657]
	TIME [epoch: 7.7 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004602405216555357		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.004602405216555357 | validation: 0.007217469277426699]
	TIME [epoch: 7.69 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004943287594931832		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.004943287594931832 | validation: 0.005408812555804634]
	TIME [epoch: 7.68 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004499548046167987		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.004499548046167987 | validation: 0.00644330266277455]
	TIME [epoch: 7.69 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004470417354297507		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.004470417354297507 | validation: 0.007082016892527474]
	TIME [epoch: 7.75 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005159709039489412		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.005159709039489412 | validation: 0.0071794077468666185]
	TIME [epoch: 7.68 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005495314089009759		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.005495314089009759 | validation: 0.006079196046594445]
	TIME [epoch: 7.68 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003874037999186576		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.003874037999186576 | validation: 0.006224295111347399]
	TIME [epoch: 7.68 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049567846277519395		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.0049567846277519395 | validation: 0.005241609571063318]
	TIME [epoch: 7.68 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005126754879498764		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.005126754879498764 | validation: 0.008061707581226136]
	TIME [epoch: 7.75 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004861997362894864		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.004861997362894864 | validation: 0.0057071853400753585]
	TIME [epoch: 7.68 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005064547525279279		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.005064547525279279 | validation: 0.004969893040218824]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_739.pth
	Model improved!!!
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005233287245950207		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.005233287245950207 | validation: 0.006107957461773859]
	TIME [epoch: 7.68 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004228091673025502		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.004228091673025502 | validation: 0.006579140669014367]
	TIME [epoch: 7.7 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003994057869741161		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.003994057869741161 | validation: 0.007560372241508312]
	TIME [epoch: 7.73 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005367043142299813		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.005367043142299813 | validation: 0.00501388082759684]
	TIME [epoch: 7.68 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004755781275192477		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.004755781275192477 | validation: 0.00575732292584227]
	TIME [epoch: 7.68 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005283546570452225		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.005283546570452225 | validation: 0.007364515440643397]
	TIME [epoch: 7.68 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004740317903202004		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.004740317903202004 | validation: 0.006109351261551934]
	TIME [epoch: 7.71 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004532802411930956		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.004532802411930956 | validation: 0.005442594529880663]
	TIME [epoch: 7.71 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004014893118526106		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.004014893118526106 | validation: 0.006698429371313517]
	TIME [epoch: 7.68 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005370085302751494		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.005370085302751494 | validation: 0.008787402901130746]
	TIME [epoch: 7.68 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004898026819127583		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.004898026819127583 | validation: 0.007031287256521416]
	TIME [epoch: 7.67 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047363086655922235		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.0047363086655922235 | validation: 0.007538490005926827]
	TIME [epoch: 7.73 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004820149237992327		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.004820149237992327 | validation: 0.005330820344796018]
	TIME [epoch: 7.69 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005344700989633129		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.005344700989633129 | validation: 0.00542940643650777]
	TIME [epoch: 7.67 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004020822767304655		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.004020822767304655 | validation: 0.006463805578201417]
	TIME [epoch: 7.68 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00426822400712281		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.00426822400712281 | validation: 0.0054851035663498835]
	TIME [epoch: 7.69 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004582637156482137		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.004582637156482137 | validation: 0.006105226355021575]
	TIME [epoch: 7.74 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045397325464870825		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.0045397325464870825 | validation: 0.006585197800459934]
	TIME [epoch: 7.7 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045299866388298715		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.0045299866388298715 | validation: 0.00541849054542456]
	TIME [epoch: 7.69 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004766214151599333		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.004766214151599333 | validation: 0.006321285317723063]
	TIME [epoch: 7.69 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045607725824839095		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.0045607725824839095 | validation: 0.0073471934809341295]
	TIME [epoch: 7.69 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045132239354991645		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.0045132239354991645 | validation: 0.006356634655526594]
	TIME [epoch: 7.75 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005042702387709195		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.005042702387709195 | validation: 0.004926275697550726]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_762.pth
	Model improved!!!
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043237608620096775		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.0043237608620096775 | validation: 0.006128298015120897]
	TIME [epoch: 7.68 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004182425987268324		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.004182425987268324 | validation: 0.005875359419111378]
	TIME [epoch: 7.7 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004630545957659875		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.004630545957659875 | validation: 0.005964311066655698]
	TIME [epoch: 7.69 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004699167408616508		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.004699167408616508 | validation: 0.007084615072887016]
	TIME [epoch: 7.74 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004372779782718507		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.004372779782718507 | validation: 0.005399854867683598]
	TIME [epoch: 7.69 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004348277487792086		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.004348277487792086 | validation: 0.018465535323306644]
	TIME [epoch: 7.7 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00980240596041881		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.00980240596041881 | validation: 0.005580097795760451]
	TIME [epoch: 7.7 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004208102380447881		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.004208102380447881 | validation: 0.005146874981326323]
	TIME [epoch: 7.71 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038064731518055424		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.0038064731518055424 | validation: 0.005611999779784639]
	TIME [epoch: 7.75 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043227206203004815		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.0043227206203004815 | validation: 0.004754118979237012]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00383126221344623		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.00383126221344623 | validation: 0.005432230033432432]
	TIME [epoch: 7.7 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055347883328375645		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.0055347883328375645 | validation: 0.005640975845566704]
	TIME [epoch: 7.69 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00401824095453529		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.00401824095453529 | validation: 0.005535783958821686]
	TIME [epoch: 7.72 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004077734882554763		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.004077734882554763 | validation: 0.005154531766661915]
	TIME [epoch: 7.72 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004897467207870744		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.004897467207870744 | validation: 0.005270293585971336]
	TIME [epoch: 7.69 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004076786971457318		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.004076786971457318 | validation: 0.005431194209389473]
	TIME [epoch: 7.71 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047926027879948326		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.0047926027879948326 | validation: 0.0057996680076070305]
	TIME [epoch: 7.69 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004127274022168831		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.004127274022168831 | validation: 0.005566510221429972]
	TIME [epoch: 7.76 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004421897679686057		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.004421897679686057 | validation: 0.005808190801172339]
	TIME [epoch: 7.71 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004727082426967169		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.004727082426967169 | validation: 0.0059779262582041245]
	TIME [epoch: 7.69 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004282963542458296		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.004282963542458296 | validation: 0.004851875883459945]
	TIME [epoch: 7.71 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004896373376599313		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.004896373376599313 | validation: 0.0058343359513697065]
	TIME [epoch: 7.7 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037145087350877537		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.0037145087350877537 | validation: 0.004896835794711112]
	TIME [epoch: 7.75 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003939449127745828		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.003939449127745828 | validation: 0.005048841164897216]
	TIME [epoch: 7.71 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004128522576357106		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.004128522576357106 | validation: 0.00559016777800714]
	TIME [epoch: 7.71 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004342279975162936		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.004342279975162936 | validation: 0.006829541286302674]
	TIME [epoch: 7.7 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004780800780065438		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.004780800780065438 | validation: 0.009667256255292536]
	TIME [epoch: 7.7 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005241872655441453		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.005241872655441453 | validation: 0.005939420414378457]
	TIME [epoch: 7.76 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003908305076574912		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.003908305076574912 | validation: 0.006546874221569294]
	TIME [epoch: 7.71 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004318334364341292		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.004318334364341292 | validation: 0.005459059390810201]
	TIME [epoch: 7.71 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004010040477105145		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.004010040477105145 | validation: 0.005235284615903267]
	TIME [epoch: 7.7 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004401263490250714		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.004401263490250714 | validation: 0.005377725815220594]
	TIME [epoch: 7.71 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004282555854638062		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.004282555854638062 | validation: 0.007094701774492026]
	TIME [epoch: 7.76 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041891371864709315		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.0041891371864709315 | validation: 0.004474903641037731]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_796.pth
	Model improved!!!
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00422546059809379		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.00422546059809379 | validation: 0.0059171965952981905]
	TIME [epoch: 7.71 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047663423046657525		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.0047663423046657525 | validation: 0.004995907013449649]
	TIME [epoch: 7.71 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003959404624528457		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.003959404624528457 | validation: 0.00486458823456348]
	TIME [epoch: 7.72 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004556471417557918		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.004556471417557918 | validation: 0.0076613345162692414]
	TIME [epoch: 7.76 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004165358244551868		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.004165358244551868 | validation: 0.00497557599120052]
	TIME [epoch: 7.7 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004411525172470943		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.004411525172470943 | validation: 0.005077881194454135]
	TIME [epoch: 7.71 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004274911481042468		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.004274911481042468 | validation: 0.005962585547315832]
	TIME [epoch: 7.71 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003616864101345188		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.003616864101345188 | validation: 0.00589265359624859]
	TIME [epoch: 7.75 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003616496770868268		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.003616496770868268 | validation: 0.004788070408737947]
	TIME [epoch: 7.72 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004341479182961995		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.004341479182961995 | validation: 0.005509284550684904]
	TIME [epoch: 7.71 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037915790227426318		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.0037915790227426318 | validation: 0.0055145184772928965]
	TIME [epoch: 7.7 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004238078595646094		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.004238078595646094 | validation: 0.007677881246518099]
	TIME [epoch: 7.74 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004744217399420573		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.004744217399420573 | validation: 0.005331752299447091]
	TIME [epoch: 7.76 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004606501747151222		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.004606501747151222 | validation: 0.005532158874015836]
	TIME [epoch: 7.72 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004374167555345849		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.004374167555345849 | validation: 0.005040306023900046]
	TIME [epoch: 7.73 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038735991553048073		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.0038735991553048073 | validation: 0.005505119245725314]
	TIME [epoch: 7.71 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036458812128069926		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.0036458812128069926 | validation: 0.004447257701858311]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_813.pth
	Model improved!!!
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00393976377366046		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.00393976377366046 | validation: 0.005745526873228322]
	TIME [epoch: 7.76 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004435077428366668		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.004435077428366668 | validation: 0.005527783128339727]
	TIME [epoch: 7.71 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005498647679785879		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.005498647679785879 | validation: 0.011305146133449816]
	TIME [epoch: 7.74 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007286636985866305		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.007286636985866305 | validation: 0.006737992392059877]
	TIME [epoch: 7.73 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003854070849573033		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.003854070849573033 | validation: 0.005582316405428648]
	TIME [epoch: 7.74 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004370209194378886		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.004370209194378886 | validation: 0.006101544409919911]
	TIME [epoch: 7.76 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004017193925928787		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.004017193925928787 | validation: 0.004825242067673818]
	TIME [epoch: 7.72 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004046090261862891		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.004046090261862891 | validation: 0.006181917656869564]
	TIME [epoch: 7.71 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037579524864032763		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.0037579524864032763 | validation: 0.005632232517548267]
	TIME [epoch: 7.72 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035711564618432076		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.0035711564618432076 | validation: 0.005145639917600865]
	TIME [epoch: 7.73 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035394431039650824		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.0035394431039650824 | validation: 0.00665787471181048]
	TIME [epoch: 7.77 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004936866199061379		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.004936866199061379 | validation: 0.0056187489742561495]
	TIME [epoch: 7.72 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004210856294159333		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.004210856294159333 | validation: 0.0060118866423779635]
	TIME [epoch: 7.71 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004856087584177746		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.004856087584177746 | validation: 0.005837404000036391]
	TIME [epoch: 7.7 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003810777174695783		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.003810777174695783 | validation: 0.0050385963999291]
	TIME [epoch: 7.74 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036252438651047185		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.0036252438651047185 | validation: 0.00551570758625348]
	TIME [epoch: 7.74 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003773484224842509		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.003773484224842509 | validation: 0.005532890942070402]
	TIME [epoch: 7.7 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004623433180627473		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.004623433180627473 | validation: 0.005641105105942362]
	TIME [epoch: 7.7 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004150600694067571		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.004150600694067571 | validation: 0.004775969553486305]
	TIME [epoch: 7.71 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003532820234715417		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.003532820234715417 | validation: 0.00478609308292921]
	TIME [epoch: 7.78 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035419035316181643		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.0035419035316181643 | validation: 0.005678614172063085]
	TIME [epoch: 7.74 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004063638000847506		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.004063638000847506 | validation: 0.005805310827268119]
	TIME [epoch: 7.72 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037437884103090815		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.0037437884103090815 | validation: 0.00527585144429584]
	TIME [epoch: 7.72 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040373276639394055		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.0040373276639394055 | validation: 0.005841986016371405]
	TIME [epoch: 7.72 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038561020429518033		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.0038561020429518033 | validation: 0.004826636500991366]
	TIME [epoch: 7.8 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003828764536059813		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.003828764536059813 | validation: 0.004532083965734615]
	TIME [epoch: 7.72 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004482264871096684		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.004482264871096684 | validation: 0.005558767900786172]
	TIME [epoch: 7.71 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038553522873971784		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.0038553522873971784 | validation: 0.004878243795939859]
	TIME [epoch: 7.7 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003973793249277426		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.003973793249277426 | validation: 0.0057746951410050824]
	TIME [epoch: 7.7 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004635615987177641		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.004635615987177641 | validation: 0.005859997504581368]
	TIME [epoch: 7.76 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00391847020136529		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.00391847020136529 | validation: 0.005144466732208082]
	TIME [epoch: 7.7 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037766418549024343		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.0037766418549024343 | validation: 0.004492326898599703]
	TIME [epoch: 7.71 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004082799786874251		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.004082799786874251 | validation: 0.005543896101211953]
	TIME [epoch: 7.71 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042668044084595656		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.0042668044084595656 | validation: 0.004245883712384262]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_847.pth
	Model improved!!!
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003656837523382915		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.003656837523382915 | validation: 0.005279401952827569]
	TIME [epoch: 7.74 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004399983513341249		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.004399983513341249 | validation: 0.004639463529318292]
	TIME [epoch: 7.69 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037853304462288047		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.0037853304462288047 | validation: 0.005395615136222755]
	TIME [epoch: 7.69 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036205217362960517		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.0036205217362960517 | validation: 0.006399245418078072]
	TIME [epoch: 7.69 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004393107581509679		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.004393107581509679 | validation: 0.004946373697796319]
	TIME [epoch: 7.72 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004020532948188878		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.004020532948188878 | validation: 0.004759360168702539]
	TIME [epoch: 7.72 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037611146493377965		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.0037611146493377965 | validation: 0.004582713920391837]
	TIME [epoch: 7.69 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035257620084689512		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.0035257620084689512 | validation: 0.0048840464574287255]
	TIME [epoch: 7.68 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003675637093807071		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.003675637093807071 | validation: 0.004643233638840495]
	TIME [epoch: 7.69 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036783032133959614		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.0036783032133959614 | validation: 0.00629984153609777]
	TIME [epoch: 7.74 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004270733366266246		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.004270733366266246 | validation: 0.004866165472270267]
	TIME [epoch: 7.7 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038072186324813622		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.0038072186324813622 | validation: 0.005087208455510329]
	TIME [epoch: 7.69 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00369904345063173		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.00369904345063173 | validation: 0.00528951557259855]
	TIME [epoch: 7.7 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035942835105688773		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.0035942835105688773 | validation: 0.0050034857545868784]
	TIME [epoch: 7.7 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045469325192710455		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.0045469325192710455 | validation: 0.004896364930245646]
	TIME [epoch: 7.74 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004170672124011585		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.004170672124011585 | validation: 0.005018797129531373]
	TIME [epoch: 7.71 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038374987375999234		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.0038374987375999234 | validation: 0.005185539019400736]
	TIME [epoch: 7.69 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003928035486651879		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.003928035486651879 | validation: 0.0046771949852094825]
	TIME [epoch: 7.7 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037524967594182567		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.0037524967594182567 | validation: 0.004476025540855083]
	TIME [epoch: 7.7 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035065168891363005		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.0035065168891363005 | validation: 0.005451945570175644]
	TIME [epoch: 7.75 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003522603986730675		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.003522603986730675 | validation: 0.005343263836719085]
	TIME [epoch: 7.7 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004071707424112615		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.004071707424112615 | validation: 0.00542428539908204]
	TIME [epoch: 7.7 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003662000085113794		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.003662000085113794 | validation: 0.005084004897418182]
	TIME [epoch: 7.7 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003962036759208933		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.003962036759208933 | validation: 0.004965493331968942]
	TIME [epoch: 7.7 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005057181473958		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.005057181473958 | validation: 0.005000561337438286]
	TIME [epoch: 7.76 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036007691918686978		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.0036007691918686978 | validation: 0.004862657556306212]
	TIME [epoch: 7.7 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035125541904475407		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.0035125541904475407 | validation: 0.00524502683416255]
	TIME [epoch: 7.7 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035089197197890696		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.0035089197197890696 | validation: 0.004944880978776424]
	TIME [epoch: 7.7 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003460061353429266		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.003460061353429266 | validation: 0.004226435191790349]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_876.pth
	Model improved!!!
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034875470995983674		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.0034875470995983674 | validation: 0.004587271675282846]
	TIME [epoch: 7.76 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036605500916315247		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.0036605500916315247 | validation: 0.005031450258800406]
	TIME [epoch: 7.7 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003298098753329457		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.003298098753329457 | validation: 0.005114051472027951]
	TIME [epoch: 7.7 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004123465275542023		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.004123465275542023 | validation: 0.005472499473649514]
	TIME [epoch: 7.7 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038028764650681925		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.0038028764650681925 | validation: 0.005495977450286669]
	TIME [epoch: 7.74 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041566410818930175		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.0041566410818930175 | validation: 0.0074639570188864725]
	TIME [epoch: 7.72 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037691238571322325		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.0037691238571322325 | validation: 0.004688350075440749]
	TIME [epoch: 7.71 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003341756718349506		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.003341756718349506 | validation: 0.004215110995420862]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_884.pth
	Model improved!!!
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004111736301413949		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.004111736301413949 | validation: 0.004838576505583358]
	TIME [epoch: 7.73 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003375059711270274		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.003375059711270274 | validation: 0.004435925986732679]
	TIME [epoch: 7.81 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035796068785765196		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.0035796068785765196 | validation: 0.005037296379980541]
	TIME [epoch: 7.73 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003389155725921642		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.003389155725921642 | validation: 0.004418381297219425]
	TIME [epoch: 7.73 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003957268067417929		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.003957268067417929 | validation: 0.004504779641776778]
	TIME [epoch: 7.72 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036680324855152104		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.0036680324855152104 | validation: 0.00451854359677247]
	TIME [epoch: 7.72 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034300539436228983		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.0034300539436228983 | validation: 0.004576473226523311]
	TIME [epoch: 7.77 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036870309483053604		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.0036870309483053604 | validation: 0.004796032983893656]
	TIME [epoch: 7.72 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00325165497118614		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.00325165497118614 | validation: 0.005267178591552879]
	TIME [epoch: 7.72 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032369816551557785		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.0032369816551557785 | validation: 0.004686708692927677]
	TIME [epoch: 7.73 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035851824606367754		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.0035851824606367754 | validation: 0.005262467903334477]
	TIME [epoch: 7.73 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004020054349816296		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.004020054349816296 | validation: 0.004452432488914066]
	TIME [epoch: 7.76 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036144346182748655		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.0036144346182748655 | validation: 0.005463137014887144]
	TIME [epoch: 7.73 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003809262248628252		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.003809262248628252 | validation: 0.004593638362707789]
	TIME [epoch: 7.72 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037925645466016684		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.0037925645466016684 | validation: 0.004750573922382382]
	TIME [epoch: 7.71 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003429447481153293		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.003429447481153293 | validation: 0.005129730745820892]
	TIME [epoch: 7.73 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003480685289035442		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.003480685289035442 | validation: 0.005053196415739461]
	TIME [epoch: 7.78 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034120793675868166		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.0034120793675868166 | validation: 0.00483471258906199]
	TIME [epoch: 7.73 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003453101910281345		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.003453101910281345 | validation: 0.004719525038572555]
	TIME [epoch: 7.7 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035411033451908163		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.0035411033451908163 | validation: 0.005231494800190382]
	TIME [epoch: 7.7 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036541856345566085		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.0036541856345566085 | validation: 0.004247039254402143]
	TIME [epoch: 7.74 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003576601856158532		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.003576601856158532 | validation: 0.004493159864209475]
	TIME [epoch: 7.75 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003652184075505808		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.003652184075505808 | validation: 0.0047985233280342385]
	TIME [epoch: 7.72 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035749561617794963		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.0035749561617794963 | validation: 0.004414518991216626]
	TIME [epoch: 7.7 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033065119436114132		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.0033065119436114132 | validation: 0.004886546463374458]
	TIME [epoch: 7.71 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003769289552210373		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.003769289552210373 | validation: 0.005450819295850867]
	TIME [epoch: 7.77 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004184590181941734		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.004184590181941734 | validation: 0.005361130109044317]
	TIME [epoch: 7.73 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003355504733629376		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.003355504733629376 | validation: 0.003962119538501358]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_912.pth
	Model improved!!!
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032332548798415053		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.0032332548798415053 | validation: 0.0050845883481796025]
	TIME [epoch: 7.71 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035029495918250824		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.0035029495918250824 | validation: 0.004480921975265014]
	TIME [epoch: 7.71 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037483757518585975		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.0037483757518585975 | validation: 0.005046904464923439]
	TIME [epoch: 7.76 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030958809747861126		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.0030958809747861126 | validation: 0.0047427369973263266]
	TIME [epoch: 7.72 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00329139907960664		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.00329139907960664 | validation: 0.0046852530488299396]
	TIME [epoch: 7.71 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035692533237380985		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.0035692533237380985 | validation: 0.004200850110609682]
	TIME [epoch: 7.72 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003364324530523376		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.003364324530523376 | validation: 0.005320332440041714]
	TIME [epoch: 7.71 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036826030624243105		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.0036826030624243105 | validation: 0.00575092789334955]
	TIME [epoch: 7.76 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003584858547441283		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.003584858547441283 | validation: 0.004748604458824402]
	TIME [epoch: 7.72 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038103903999142296		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.0038103903999142296 | validation: 0.005440473038936482]
	TIME [epoch: 7.7 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00345277700817986		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.00345277700817986 | validation: 0.005116887720271312]
	TIME [epoch: 7.7 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032763873218005423		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.0032763873218005423 | validation: 0.004618300273232329]
	TIME [epoch: 7.71 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034204635993351743		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.0034204635993351743 | validation: 0.004550202251266783]
	TIME [epoch: 7.77 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035562476367643602		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.0035562476367643602 | validation: 0.004617637771025482]
	TIME [epoch: 7.7 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003257482154389717		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.003257482154389717 | validation: 0.004906844448546263]
	TIME [epoch: 7.71 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034858591174291246		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.0034858591174291246 | validation: 0.003915925365569505]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_928.pth
	Model improved!!!
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003297223071790909		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.003297223071790909 | validation: 0.003788356572686731]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_929.pth
	Model improved!!!
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031244386958648443		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.0031244386958648443 | validation: 0.0045598701645247364]
	TIME [epoch: 7.75 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032144782558506386		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.0032144782558506386 | validation: 0.0056399719117101385]
	TIME [epoch: 7.72 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040464130045725424		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.0040464130045725424 | validation: 0.004197663046944555]
	TIME [epoch: 7.72 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004146002268336211		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.004146002268336211 | validation: 0.005816729601679521]
	TIME [epoch: 7.74 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004150887296842191		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.004150887296842191 | validation: 0.005140375410166662]
	TIME [epoch: 7.76 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035159292169037164		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.0035159292169037164 | validation: 0.004615032662540687]
	TIME [epoch: 7.74 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003560962185432246		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.003560962185432246 | validation: 0.004654251618548728]
	TIME [epoch: 7.71 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031185717782658664		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.0031185717782658664 | validation: 0.004167910552580658]
	TIME [epoch: 7.72 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034169149978879295		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.0034169149978879295 | validation: 0.004193683819609631]
	TIME [epoch: 7.71 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003648613293241143		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.003648613293241143 | validation: 0.003865560088099483]
	TIME [epoch: 7.77 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033002668260662766		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.0033002668260662766 | validation: 0.0044304535981723875]
	TIME [epoch: 7.72 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034266368238549645		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.0034266368238549645 | validation: 0.0049915791632082155]
	TIME [epoch: 7.71 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034225754627576197		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.0034225754627576197 | validation: 0.004399654860027486]
	TIME [epoch: 7.71 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034411280313539675		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.0034411280313539675 | validation: 0.004555804668232111]
	TIME [epoch: 7.72 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031688906673607233		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.0031688906673607233 | validation: 0.00441029377234286]
	TIME [epoch: 7.77 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034870251740655098		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.0034870251740655098 | validation: 0.005153607830154522]
	TIME [epoch: 7.71 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036286829581843394		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.0036286829581843394 | validation: 0.004734812441077047]
	TIME [epoch: 7.7 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003382831228096435		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.003382831228096435 | validation: 0.003940460082362988]
	TIME [epoch: 7.7 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003384780357071164		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.003384780357071164 | validation: 0.00433664295284402]
	TIME [epoch: 7.73 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034830707494495333		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.0034830707494495333 | validation: 0.005036038598545795]
	TIME [epoch: 7.75 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003377086278419069		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.003377086278419069 | validation: 0.004580655101214495]
	TIME [epoch: 7.71 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003795930190228022		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.003795930190228022 | validation: 0.004798014677316938]
	TIME [epoch: 7.72 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030984725898697196		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.0030984725898697196 | validation: 0.004665987556866232]
	TIME [epoch: 7.7 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036458787962185884		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.0036458787962185884 | validation: 0.004792334532390479]
	TIME [epoch: 7.74 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003466768362438028		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.003466768362438028 | validation: 0.0049999102439852216]
	TIME [epoch: 7.76 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003598314895112484		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.003598314895112484 | validation: 0.004849428121922695]
	TIME [epoch: 7.72 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034564621831708864		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.0034564621831708864 | validation: 0.004248265918296897]
	TIME [epoch: 7.72 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003488025438421847		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.003488025438421847 | validation: 0.005939035017882377]
	TIME [epoch: 7.72 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033132052311465785		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.0033132052311465785 | validation: 0.004296114729983121]
	TIME [epoch: 7.75 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034163713012395967		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.0034163713012395967 | validation: 0.004480304690987265]
	TIME [epoch: 7.75 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003303582808033926		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.003303582808033926 | validation: 0.004233019764329662]
	TIME [epoch: 7.72 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035717470028644193		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.0035717470028644193 | validation: 0.004604737948137436]
	TIME [epoch: 7.69 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034922820927764065		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.0034922820927764065 | validation: 0.004826243334155038]
	TIME [epoch: 7.72 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003388350719848829		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.003388350719848829 | validation: 0.004297508886024974]
	TIME [epoch: 7.77 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003244170804648153		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.003244170804648153 | validation: 0.004350654073854515]
	TIME [epoch: 7.72 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031523945548756187		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.0031523945548756187 | validation: 0.004482003605898523]
	TIME [epoch: 7.7 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030830966145366434		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.0030830966145366434 | validation: 0.00535663128422032]
	TIME [epoch: 7.71 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029713604033892857		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.0029713604033892857 | validation: 0.0046077238342633045]
	TIME [epoch: 7.7 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003441144183755871		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.003441144183755871 | validation: 0.00487744974131972]
	TIME [epoch: 7.76 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003144204559298783		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.003144204559298783 | validation: 0.006133881141085826]
	TIME [epoch: 7.7 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003706746265715684		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.003706746265715684 | validation: 0.004393266966594919]
	TIME [epoch: 7.72 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033035042552227514		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.0033035042552227514 | validation: 0.004668213082193982]
	TIME [epoch: 7.72 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035663592753157466		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.0035663592753157466 | validation: 0.00490822696159424]
	TIME [epoch: 7.73 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033868967959305683		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.0033868967959305683 | validation: 0.004667247489354905]
	TIME [epoch: 7.8 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030125351396898146		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.0030125351396898146 | validation: 0.004912969397178139]
	TIME [epoch: 7.71 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034067895281526266		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.0034067895281526266 | validation: 0.004213035765188361]
	TIME [epoch: 7.73 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038521058188841573		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.0038521058188841573 | validation: 0.005134169456235085]
	TIME [epoch: 7.73 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003091734665203385		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.003091734665203385 | validation: 0.004230017848527693]
	TIME [epoch: 7.74 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00323427011443155		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.00323427011443155 | validation: 0.004562126223722476]
	TIME [epoch: 7.79 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00329469792164749		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.00329469792164749 | validation: 0.004645064895407184]
	TIME [epoch: 7.71 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003396384650263649		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.003396384650263649 | validation: 0.004975357337208747]
	TIME [epoch: 7.72 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033374831443049174		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.0033374831443049174 | validation: 0.004250560536232838]
	TIME [epoch: 7.73 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003313904129074566		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.003313904129074566 | validation: 0.0046994649401661676]
	TIME [epoch: 7.74 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035107857208678452		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.0035107857208678452 | validation: 0.004636210874755306]
	TIME [epoch: 7.75 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003030660442017146		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.003030660442017146 | validation: 0.004700862997889265]
	TIME [epoch: 7.72 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003360358724343343		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.003360358724343343 | validation: 0.005199675448273194]
	TIME [epoch: 7.7 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003472364451490603		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.003472364451490603 | validation: 0.004184579859845391]
	TIME [epoch: 7.72 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003230965171846881		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.003230965171846881 | validation: 0.004662571733084591]
	TIME [epoch: 7.76 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003112362238232886		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.003112362238232886 | validation: 0.00507891743275906]
	TIME [epoch: 7.76 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003483188903625585		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.003483188903625585 | validation: 0.005108537087645104]
	TIME [epoch: 7.69 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033252423807210263		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.0033252423807210263 | validation: 0.004075834289836327]
	TIME [epoch: 7.71 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031743553158962864		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.0031743553158962864 | validation: 0.004501755733372272]
	TIME [epoch: 7.73 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003422407533116175		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.003422407533116175 | validation: 0.004376222622375911]
	TIME [epoch: 7.78 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032106976252157084		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.0032106976252157084 | validation: 0.004501483912564207]
	TIME [epoch: 7.73 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003397021202182629		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.003397021202182629 | validation: 0.0043200305373796875]
	TIME [epoch: 7.74 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035092757085465292		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.0035092757085465292 | validation: 0.004205833460651446]
	TIME [epoch: 7.73 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034833108881776194		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.0034833108881776194 | validation: 0.004374843092251673]
	TIME [epoch: 7.73 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00320147220562415		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.00320147220562415 | validation: 0.004172703623673804]
	TIME [epoch: 7.77 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003287480367754496		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.003287480367754496 | validation: 0.003946694673381645]
	TIME [epoch: 7.73 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003209269388274636		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.003209269388274636 | validation: 0.004311497339527454]
	TIME [epoch: 7.71 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00334889630836013		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.00334889630836013 | validation: 0.004407002052909692]
	TIME [epoch: 7.7 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029062219348247017		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.0029062219348247017 | validation: 0.005410368915576223]
	TIME [epoch: 7.69 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003142520211581475		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.003142520211581475 | validation: 0.004264957533545729]
	TIME [epoch: 7.75 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002962129456316045		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.002962129456316045 | validation: 0.004853699619446291]
	TIME [epoch: 7.69 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037791592160751003		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.0037791592160751003 | validation: 0.005049721175749805]
	TIME [epoch: 7.68 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034672614825617828		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.0034672614825617828 | validation: 0.0049822610073699735]
	TIME [epoch: 7.68 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029633439670582813		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.0029633439670582813 | validation: 0.0037396545056753022]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1006.pth
	Model improved!!!
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032100172479693897		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.0032100172479693897 | validation: 0.004312336806143211]
	TIME [epoch: 7.73 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00326149328316645		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.00326149328316645 | validation: 0.00454498272948448]
	TIME [epoch: 7.69 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032980186708776306		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.0032980186708776306 | validation: 0.004139334541698716]
	TIME [epoch: 7.67 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00309046083127761		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.00309046083127761 | validation: 0.004419889327549556]
	TIME [epoch: 7.69 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003415505633465131		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.003415505633465131 | validation: 0.004213019489761267]
	TIME [epoch: 7.72 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031112654345875385		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.0031112654345875385 | validation: 0.00357405070520687]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1012.pth
	Model improved!!!
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003538834989605758		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.003538834989605758 | validation: 0.004607914357420397]
	TIME [epoch: 7.72 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032498142696710847		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.0032498142696710847 | validation: 0.003907313226754159]
	TIME [epoch: 7.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003171071256338397		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.003171071256338397 | validation: 0.004241327284142744]
	TIME [epoch: 7.73 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029268492984352618		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.0029268492984352618 | validation: 0.003879199187159732]
	TIME [epoch: 7.76 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003146342034216666		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.003146342034216666 | validation: 0.004023714506020173]
	TIME [epoch: 7.73 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030017915201765795		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.0030017915201765795 | validation: 0.004336517060171201]
	TIME [epoch: 7.73 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029641325656357747		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.0029641325656357747 | validation: 0.0038909886621004437]
	TIME [epoch: 7.73 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003143609093192168		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.003143609093192168 | validation: 0.0038581293851742988]
	TIME [epoch: 7.72 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030988418009925116		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.0030988418009925116 | validation: 0.004259288269199223]
	TIME [epoch: 7.76 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040913407644049965		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.0040913407644049965 | validation: 0.004690971812445387]
	TIME [epoch: 7.7 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030796623023206246		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.0030796623023206246 | validation: 0.003983927430181824]
	TIME [epoch: 7.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033254352952410866		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.0033254352952410866 | validation: 0.004420607306061459]
	TIME [epoch: 7.69 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033575641643211925		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.0033575641643211925 | validation: 0.003975348942037529]
	TIME [epoch: 7.71 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003254142962419204		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.003254142962419204 | validation: 0.0037874140522999094]
	TIME [epoch: 7.75 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031558892931838685		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.0031558892931838685 | validation: 0.005216917540087407]
	TIME [epoch: 7.7 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034670693109194483		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.0034670693109194483 | validation: 0.003844097479702243]
	TIME [epoch: 7.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00310097484832855		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.00310097484832855 | validation: 0.0042904006269029096]
	TIME [epoch: 7.73 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003014823663674658		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.003014823663674658 | validation: 0.004882935010847938]
	TIME [epoch: 7.72 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029856199077415552		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.0029856199077415552 | validation: 0.004082766467420205]
	TIME [epoch: 7.78 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031720660549987597		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.0031720660549987597 | validation: 0.0047596007365397035]
	TIME [epoch: 7.71 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031732864215849067		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.0031732864215849067 | validation: 0.004120490329478116]
	TIME [epoch: 7.72 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032160418791407266		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.0032160418791407266 | validation: 0.004108119043405916]
	TIME [epoch: 7.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002916969636944604		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.002916969636944604 | validation: 0.004977754576603639]
	TIME [epoch: 7.74 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003387644178069723		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.003387644178069723 | validation: 0.005416965196422199]
	TIME [epoch: 7.75 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035044608202434106		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.0035044608202434106 | validation: 0.004324412093817107]
	TIME [epoch: 7.73 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030834023096740527		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.0030834023096740527 | validation: 0.0038928357611734757]
	TIME [epoch: 7.73 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034501640934172396		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.0034501640934172396 | validation: 0.0042524356632652836]
	TIME [epoch: 7.73 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003101539114469083		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.003101539114469083 | validation: 0.004190375855415551]
	TIME [epoch: 7.75 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031605290767344448		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.0031605290767344448 | validation: 0.004079533283006224]
	TIME [epoch: 7.73 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003128567586999778		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.003128567586999778 | validation: 0.0040087304226578985]
	TIME [epoch: 7.71 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002845972109923828		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.002845972109923828 | validation: 0.0038305434253147093]
	TIME [epoch: 7.73 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027520488611800275		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.0027520488611800275 | validation: 0.004088772641462105]
	TIME [epoch: 7.71 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003037264490245276		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.003037264490245276 | validation: 0.004135986617122479]
	TIME [epoch: 7.76 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028137851565343645		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.0028137851565343645 | validation: 0.0036990144777023677]
	TIME [epoch: 7.71 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003103623313965722		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.003103623313965722 | validation: 0.004433169576677552]
	TIME [epoch: 7.71 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031331706094229863		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.0031331706094229863 | validation: 0.004560814334978976]
	TIME [epoch: 7.7 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00313340727350523		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.00313340727350523 | validation: 0.004484283537284251]
	TIME [epoch: 7.71 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002642234891613582		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.002642234891613582 | validation: 0.003975937253596376]
	TIME [epoch: 7.76 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033585086153905585		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.0033585086153905585 | validation: 0.0041229719848682456]
	TIME [epoch: 7.71 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002976040103258487		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.002976040103258487 | validation: 0.003806347862383162]
	TIME [epoch: 7.71 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030878413503558303		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.0030878413503558303 | validation: 0.0038989877819234726]
	TIME [epoch: 7.72 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029038772198624794		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.0029038772198624794 | validation: 0.004480372634358486]
	TIME [epoch: 7.73 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028942901570998547		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.0028942901570998547 | validation: 0.00403535510599956]
	TIME [epoch: 7.78 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003103267172192639		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.003103267172192639 | validation: 0.00407727191293809]
	TIME [epoch: 7.72 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030734587846219805		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.0030734587846219805 | validation: 0.004530961326980992]
	TIME [epoch: 7.73 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003237879874601097		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.003237879874601097 | validation: 0.0042111517242483755]
	TIME [epoch: 7.72 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002829437828716052		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.002829437828716052 | validation: 0.003561644557968374]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1059.pth
	Model improved!!!
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031132389313934406		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.0031132389313934406 | validation: 0.0037537120434380294]
	TIME [epoch: 7.76 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029117343275060544		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.0029117343275060544 | validation: 0.004613338334747737]
	TIME [epoch: 7.72 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029113050998180825		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.0029113050998180825 | validation: 0.003946356134149536]
	TIME [epoch: 7.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030405700533279617		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.0030405700533279617 | validation: 0.003988490750016018]
	TIME [epoch: 7.72 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031565246886487786		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.0031565246886487786 | validation: 0.0039588598350206025]
	TIME [epoch: 7.76 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003352736996494232		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.003352736996494232 | validation: 0.003266628788177482]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1065.pth
	Model improved!!!
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028101325977334833		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.0028101325977334833 | validation: 0.004536632993095124]
	TIME [epoch: 7.72 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032161173157577464		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.0032161173157577464 | validation: 0.0042516249797967766]
	TIME [epoch: 7.74 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031159347143071853		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.0031159347143071853 | validation: 0.004159560888164774]
	TIME [epoch: 7.71 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030823111449068273		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.0030823111449068273 | validation: 0.0040900639391598746]
	TIME [epoch: 7.79 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031984651240511983		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.0031984651240511983 | validation: 0.004394776728832496]
	TIME [epoch: 7.72 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002978342885699368		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.002978342885699368 | validation: 0.004000572877141303]
	TIME [epoch: 7.73 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029768075160507813		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.0029768075160507813 | validation: 0.004289754978781147]
	TIME [epoch: 7.72 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029662225263499134		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.0029662225263499134 | validation: 0.004251399727103409]
	TIME [epoch: 7.71 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029602201854602933		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.0029602201854602933 | validation: 0.0038317991514827876]
	TIME [epoch: 7.77 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031944226964439946		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.0031944226964439946 | validation: 0.0037427906165709143]
	TIME [epoch: 7.74 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030038383495230225		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.0030038383495230225 | validation: 0.0038345505526084693]
	TIME [epoch: 7.73 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002991417660802318		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.002991417660802318 | validation: 0.004199738629212784]
	TIME [epoch: 7.73 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030301626005213153		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.0030301626005213153 | validation: 0.004075882328222704]
	TIME [epoch: 7.72 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029271796156933286		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.0029271796156933286 | validation: 0.00391825273493134]
	TIME [epoch: 7.79 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002860062548880035		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.002860062548880035 | validation: 0.003415289869581622]
	TIME [epoch: 7.74 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002876278150311899		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.002876278150311899 | validation: 0.004490602584982645]
	TIME [epoch: 7.73 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003189167945477201		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.003189167945477201 | validation: 0.0037687315051956783]
	TIME [epoch: 7.72 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002888774272281274		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.002888774272281274 | validation: 0.0036981365293288267]
	TIME [epoch: 7.75 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030786318199598358		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.0030786318199598358 | validation: 0.005120757037335176]
	TIME [epoch: 7.77 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003004116398621348		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.003004116398621348 | validation: 0.0036296147884409312]
	TIME [epoch: 7.77 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00294101715453842		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.00294101715453842 | validation: 0.004089695629357372]
	TIME [epoch: 7.75 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00266237822136616		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.00266237822136616 | validation: 0.004736200242741381]
	TIME [epoch: 7.72 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002921763146364313		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.002921763146364313 | validation: 0.0046198050739468515]
	TIME [epoch: 7.77 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032769663109019475		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.0032769663109019475 | validation: 0.003760448560845796]
	TIME [epoch: 7.74 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027135300812275594		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.0027135300812275594 | validation: 0.0038834614673323173]
	TIME [epoch: 7.72 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029063802554226406		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.0029063802554226406 | validation: 0.004307932666401278]
	TIME [epoch: 7.73 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003142756543279997		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.003142756543279997 | validation: 0.0037932134599841212]
	TIME [epoch: 7.72 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028486515260985963		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.0028486515260985963 | validation: 0.004073769812399667]
	TIME [epoch: 7.79 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002999117871800655		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.002999117871800655 | validation: 0.003953164347904646]
	TIME [epoch: 7.75 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002990958891407397		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.002990958891407397 | validation: 0.004532314141631369]
	TIME [epoch: 7.74 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002802618489847473		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.002802618489847473 | validation: 0.003462718105267082]
	TIME [epoch: 7.72 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030137004008699195		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.0030137004008699195 | validation: 0.004603281149652528]
	TIME [epoch: 7.72 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028626104780186926		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.0028626104780186926 | validation: 0.004128330747498685]
	TIME [epoch: 7.78 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030861127885834653		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.0030861127885834653 | validation: 0.003806597582874991]
	TIME [epoch: 7.74 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031167507476314417		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.0031167507476314417 | validation: 0.0038446352038456954]
	TIME [epoch: 7.73 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030474488722828995		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.0030474488722828995 | validation: 0.003959690684165163]
	TIME [epoch: 7.73 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031132355264334517		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.0031132355264334517 | validation: 0.003987011320563312]
	TIME [epoch: 7.73 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002990537892140658		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.002990537892140658 | validation: 0.003954286866880053]
	TIME [epoch: 7.79 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002992879355517254		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.002992879355517254 | validation: 0.004131521477111419]
	TIME [epoch: 7.73 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029211282226759864		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.0029211282226759864 | validation: 0.0037499201647366222]
	TIME [epoch: 7.73 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029535311904866045		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.0029535311904866045 | validation: 0.0039292081604270284]
	TIME [epoch: 7.74 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028022613487278837		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.0028022613487278837 | validation: 0.0038043254854975474]
	TIME [epoch: 7.75 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002853429480262177		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.002853429480262177 | validation: 0.004213903853022765]
	TIME [epoch: 7.78 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003161137605210098		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.003161137605210098 | validation: 0.004693076930911944]
	TIME [epoch: 7.74 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030953567046701014		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.0030953567046701014 | validation: 0.00393783369219253]
	TIME [epoch: 7.73 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002888144875997078		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.002888144875997078 | validation: 0.003860550728743079]
	TIME [epoch: 7.73 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00302247165042003		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.00302247165042003 | validation: 0.004272176033281078]
	TIME [epoch: 7.76 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002828305668668713		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.002828305668668713 | validation: 0.003943977165471228]
	TIME [epoch: 7.78 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00270805470599477		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.00270805470599477 | validation: 0.0035748385487473526]
	TIME [epoch: 7.72 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003047097300220153		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.003047097300220153 | validation: 0.003789007631869506]
	TIME [epoch: 7.71 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002745258148436067		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.002745258148436067 | validation: 0.0038783415602291803]
	TIME [epoch: 7.72 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002903574406255119		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.002903574406255119 | validation: 0.003803138441452929]
	TIME [epoch: 7.78 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029511417613299314		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.0029511417613299314 | validation: 0.003739640949148605]
	TIME [epoch: 7.75 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030809696012946233		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.0030809696012946233 | validation: 0.003710478667306236]
	TIME [epoch: 7.72 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030188325808159716		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.0030188325808159716 | validation: 0.003955509403876926]
	TIME [epoch: 7.73 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002741039598094951		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.002741039598094951 | validation: 0.003629936047516627]
	TIME [epoch: 7.73 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002798697496854689		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.002798697496854689 | validation: 0.0035543107646571277]
	TIME [epoch: 7.76 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002911512932057642		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.002911512932057642 | validation: 0.004195908218278662]
	TIME [epoch: 7.74 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002739099767198413		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.002739099767198413 | validation: 0.0038083695820902105]
	TIME [epoch: 7.74 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027197003503661157		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.0027197003503661157 | validation: 0.003943678916301229]
	TIME [epoch: 7.73 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028752417383109807		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.0028752417383109807 | validation: 0.003831667855649492]
	TIME [epoch: 7.73 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029941856651104033		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.0029941856651104033 | validation: 0.003990333546081776]
	TIME [epoch: 7.78 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003073211537180584		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.003073211537180584 | validation: 0.003317153570398034]
	TIME [epoch: 7.74 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028445601116004224		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.0028445601116004224 | validation: 0.0037657419704724]
	TIME [epoch: 7.72 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00296241795964567		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.00296241795964567 | validation: 0.0037258569462218106]
	TIME [epoch: 7.74 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030087513069002184		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.0030087513069002184 | validation: 0.003796890988145714]
	TIME [epoch: 7.73 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027394079069809033		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.0027394079069809033 | validation: 0.0036649425916321548]
	TIME [epoch: 7.79 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027923855534516366		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.0027923855534516366 | validation: 0.003810282514176339]
	TIME [epoch: 7.73 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002812866083172015		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.002812866083172015 | validation: 0.004355076029649616]
	TIME [epoch: 7.73 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029462392495389605		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.0029462392495389605 | validation: 0.003949770848680353]
	TIME [epoch: 7.72 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029103896728985045		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.0029103896728985045 | validation: 0.0041635456676326595]
	TIME [epoch: 7.74 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002943598952348225		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.002943598952348225 | validation: 0.003505626152231068]
	TIME [epoch: 7.77 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026534825478757187		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.0026534825478757187 | validation: 0.003366745979663916]
	TIME [epoch: 7.73 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027990641516967654		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.0027990641516967654 | validation: 0.005333435780370256]
	TIME [epoch: 7.72 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026694376177683688		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.0026694376177683688 | validation: 0.0038008383424649192]
	TIME [epoch: 7.72 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026248480264799983		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.0026248480264799983 | validation: 0.0036307350975590645]
	TIME [epoch: 7.76 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002824090435863367		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.002824090435863367 | validation: 0.003718842562644964]
	TIME [epoch: 7.76 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002805063424687947		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.002805063424687947 | validation: 0.0038817810602397493]
	TIME [epoch: 7.74 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002621386118936034		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.002621386118936034 | validation: 0.004313758070683309]
	TIME [epoch: 7.71 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028953127199675397		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.0028953127199675397 | validation: 0.003980475759139242]
	TIME [epoch: 7.73 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002944393412832606		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.002944393412832606 | validation: 0.003786535299640073]
	TIME [epoch: 7.77 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029477242116312257		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.0029477242116312257 | validation: 0.004025193370382677]
	TIME [epoch: 7.73 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031891347945771724		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.0031891347945771724 | validation: 0.004067164646437062]
	TIME [epoch: 7.75 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027297632374588456		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.0027297632374588456 | validation: 0.004372038178195746]
	TIME [epoch: 7.73 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027534838794021385		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.0027534838794021385 | validation: 0.0036041902130954447]
	TIME [epoch: 7.73 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027880221831625427		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.0027880221831625427 | validation: 0.003704486444133179]
	TIME [epoch: 7.78 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002603090980925423		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.002603090980925423 | validation: 0.0041120689484363685]
	TIME [epoch: 7.74 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002587651556249141		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.002587651556249141 | validation: 0.003495943136274638]
	TIME [epoch: 7.74 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002747995421722669		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.002747995421722669 | validation: 0.004081369863983013]
	TIME [epoch: 7.72 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002707409931363736		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.002707409931363736 | validation: 0.0037979222115638606]
	TIME [epoch: 7.73 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026778444580321126		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.0026778444580321126 | validation: 0.00405537041876659]
	TIME [epoch: 7.78 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002961757479608579		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.002961757479608579 | validation: 0.0037532897070451598]
	TIME [epoch: 7.72 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028956615020624098		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.0028956615020624098 | validation: 0.00438311992459415]
	TIME [epoch: 7.72 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002923045495567625		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.002923045495567625 | validation: 0.0032525136887268363]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1159.pth
	Model improved!!!
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002818827901138508		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.002818827901138508 | validation: 0.004411769079268809]
	TIME [epoch: 7.74 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002613210791215991		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.002613210791215991 | validation: 0.0033573124491083827]
	TIME [epoch: 8.07 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032328776139546756		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.0032328776139546756 | validation: 0.004283089067231464]
	TIME [epoch: 7.69 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028954191828025944		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.0028954191828025944 | validation: 0.004236024920839916]
	TIME [epoch: 7.7 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027775181294300787		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.0027775181294300787 | validation: 0.0037667308480011574]
	TIME [epoch: 7.69 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028755104465160155		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.0028755104465160155 | validation: 0.0036637922370581742]
	TIME [epoch: 7.7 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025542202495831843		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.0025542202495831843 | validation: 0.0037523413593654248]
	TIME [epoch: 7.74 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028086603093839084		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.0028086603093839084 | validation: 0.003546581967036864]
	TIME [epoch: 7.69 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029096704467179955		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.0029096704467179955 | validation: 0.0037903634262627553]
	TIME [epoch: 7.68 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026280489884597654		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 0.0026280489884597654 | validation: 0.0035033610294185175]
	TIME [epoch: 7.68 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002584795359196243		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.002584795359196243 | validation: 0.0037395082831331175]
	TIME [epoch: 7.73 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028024816922040986		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.0028024816922040986 | validation: 0.0035328933373413593]
	TIME [epoch: 7.72 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002619155021717682		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.002619155021717682 | validation: 0.003864683291717323]
	TIME [epoch: 7.68 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029053285694594895		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.0029053285694594895 | validation: 0.003560385353836895]
	TIME [epoch: 7.69 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002685742390530348		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.002685742390530348 | validation: 0.0037159640961543713]
	TIME [epoch: 7.7 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025277626504716915		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.0025277626504716915 | validation: 0.0036556378670601947]
	TIME [epoch: 7.75 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002875482902202332		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.002875482902202332 | validation: 0.004426097052648204]
	TIME [epoch: 7.7 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030615159771420594		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.0030615159771420594 | validation: 0.004037220916053685]
	TIME [epoch: 7.69 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002608198017767702		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.002608198017767702 | validation: 0.0037825261335023654]
	TIME [epoch: 7.7 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00276315962930039		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.00276315962930039 | validation: 0.003768574623359928]
	TIME [epoch: 7.7 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002732745033371775		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.002732745033371775 | validation: 0.003974211918189414]
	TIME [epoch: 7.76 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002879869758468408		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.002879869758468408 | validation: 0.0035166486968487946]
	TIME [epoch: 7.7 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002667450184263499		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.002667450184263499 | validation: 0.003906394615700149]
	TIME [epoch: 7.7 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002721328108129371		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.002721328108129371 | validation: 0.0037079508724548963]
	TIME [epoch: 7.72 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002720885574862951		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.002720885574862951 | validation: 0.0036376883306015605]
	TIME [epoch: 7.7 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002609904220770629		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.002609904220770629 | validation: 0.004453237139701126]
	TIME [epoch: 7.76 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027275733269748696		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.0027275733269748696 | validation: 0.0036284876490264547]
	TIME [epoch: 7.71 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002795369988797762		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.002795369988797762 | validation: 0.0034060624508986913]
	TIME [epoch: 7.7 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029245743111557237		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.0029245743111557237 | validation: 0.004102108784773865]
	TIME [epoch: 7.71 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00275988072586145		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.00275988072586145 | validation: 0.003193007830136675]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1189.pth
	Model improved!!!
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027409648859401743		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.0027409648859401743 | validation: 0.004373594533876001]
	TIME [epoch: 7.76 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002707298377175319		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.002707298377175319 | validation: 0.0034588640732190323]
	TIME [epoch: 7.7 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002910042409041685		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.002910042409041685 | validation: 0.0039702749389657985]
	TIME [epoch: 7.7 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025855316304388585		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.0025855316304388585 | validation: 0.0035832699171347076]
	TIME [epoch: 7.69 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026512126809969424		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.0026512126809969424 | validation: 0.0037259161016214805]
	TIME [epoch: 7.71 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002722995096986371		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.002722995096986371 | validation: 0.003939464004345214]
	TIME [epoch: 7.72 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028915199981755634		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.0028915199981755634 | validation: 0.0032382900455772195]
	TIME [epoch: 7.7 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002835627729439126		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.002835627729439126 | validation: 0.004058336298398126]
	TIME [epoch: 7.69 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002869790696113583		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.002869790696113583 | validation: 0.0031855457777272226]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1198.pth
	Model improved!!!
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002789524037715138		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.002789524037715138 | validation: 0.003711579191516785]
	TIME [epoch: 7.75 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002865666741122467		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.002865666741122467 | validation: 0.003849285827558707]
	TIME [epoch: 7.7 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028777471616039936		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.0028777471616039936 | validation: 0.0032955321168324217]
	TIME [epoch: 7.71 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025337056876054594		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.0025337056876054594 | validation: 0.003326672937490535]
	TIME [epoch: 7.72 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026719895988698843		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.0026719895988698843 | validation: 0.003706138739694063]
	TIME [epoch: 7.71 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028017336771736182		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.0028017336771736182 | validation: 0.0036253650422121524]
	TIME [epoch: 7.77 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002717228730685315		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.002717228730685315 | validation: 0.003283514719239087]
	TIME [epoch: 7.72 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002754898994808234		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.002754898994808234 | validation: 0.003448793120285502]
	TIME [epoch: 7.7 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027145170219494944		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.0027145170219494944 | validation: 0.0036209455876305345]
	TIME [epoch: 7.71 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027692620729586853		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.0027692620729586853 | validation: 0.0032638828981161466]
	TIME [epoch: 7.71 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028609605441483835		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.0028609605441483835 | validation: 0.004603697947733003]
	TIME [epoch: 7.76 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026595793198039295		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.0026595793198039295 | validation: 0.003845952985073949]
	TIME [epoch: 7.71 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028070809359104824		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.0028070809359104824 | validation: 0.003533743777724056]
	TIME [epoch: 7.72 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002817247426555416		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.002817247426555416 | validation: 0.0034765858844420464]
	TIME [epoch: 7.7 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002844501497321428		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.002844501497321428 | validation: 0.0035225946041799096]
	TIME [epoch: 7.72 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002877275593452113		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.002877275593452113 | validation: 0.003905959915081965]
	TIME [epoch: 7.75 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002710736611202879		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.002710736611202879 | validation: 0.0036112396792043104]
	TIME [epoch: 7.71 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003010030994992048		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.003010030994992048 | validation: 0.003555211435265049]
	TIME [epoch: 7.71 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002550248855503548		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.002550248855503548 | validation: 0.003100278585160837]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1217.pth
	Model improved!!!
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002662378449604053		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.002662378449604053 | validation: 0.004101946339011022]
	TIME [epoch: 7.75 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002680565587437009		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.002680565587437009 | validation: 0.004269565234419552]
	TIME [epoch: 7.73 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027530833517827266		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.0027530833517827266 | validation: 0.004060198185711964]
	TIME [epoch: 7.72 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026202776326745836		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.0026202776326745836 | validation: 0.0042772796486651105]
	TIME [epoch: 7.72 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002632512720504734		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.002632512720504734 | validation: 0.0037794791819206522]
	TIME [epoch: 7.72 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002630298872616682		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.002630298872616682 | validation: 0.0034167696295781154]
	TIME [epoch: 7.78 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002640780800747673		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.002640780800747673 | validation: 0.004060520536379583]
	TIME [epoch: 7.71 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025164125773135555		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.0025164125773135555 | validation: 0.0035002949740493648]
	TIME [epoch: 7.73 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002809494244188541		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.002809494244188541 | validation: 0.004123304123757289]
	TIME [epoch: 7.74 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002477582425513467		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.002477582425513467 | validation: 0.003697143401703046]
	TIME [epoch: 7.71 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002597787741403568		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.002597787741403568 | validation: 0.0035603810492595266]
	TIME [epoch: 7.77 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026318744394389663		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.0026318744394389663 | validation: 0.00399927572766166]
	TIME [epoch: 7.72 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025747397177263236		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.0025747397177263236 | validation: 0.0035572318056707646]
	TIME [epoch: 7.75 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002723885386778949		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.002723885386778949 | validation: 0.0036941124952250214]
	TIME [epoch: 7.72 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026416030507412673		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.0026416030507412673 | validation: 0.004083377855254248]
	TIME [epoch: 7.72 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025214634854631513		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.0025214634854631513 | validation: 0.003953059202890211]
	TIME [epoch: 7.77 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002363858077815642		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.002363858077815642 | validation: 0.004069400762031786]
	TIME [epoch: 7.72 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002714450939843229		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.002714450939843229 | validation: 0.00470870509107013]
	TIME [epoch: 7.71 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026630495485803715		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.0026630495485803715 | validation: 0.0035145495883986974]
	TIME [epoch: 7.72 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027260518635058868		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.0027260518635058868 | validation: 0.003762013928620531]
	TIME [epoch: 7.73 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026600175662825085		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.0026600175662825085 | validation: 0.0040587648801375074]
	TIME [epoch: 7.75 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029066430073210314		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.0029066430073210314 | validation: 0.0032769563861849782]
	TIME [epoch: 7.71 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002734048578791435		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.002734048578791435 | validation: 0.0036243213605541775]
	TIME [epoch: 7.71 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002615574250787186		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.002615574250787186 | validation: 0.0037904055282535305]
	TIME [epoch: 7.71 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002860021836288011		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.002860021836288011 | validation: 0.0039402486746009015]
	TIME [epoch: 7.73 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025615910457344095		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.0025615910457344095 | validation: 0.0036979282560189956]
	TIME [epoch: 7.76 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024318851534452356		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.0024318851534452356 | validation: 0.0034671383031787545]
	TIME [epoch: 7.71 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00249738860735584		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.00249738860735584 | validation: 0.0035716142934758724]
	TIME [epoch: 7.72 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002626204861780071		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.002626204861780071 | validation: 0.0033501361222148577]
	TIME [epoch: 7.71 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002697146467812229		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.002697146467812229 | validation: 0.004233217625144637]
	TIME [epoch: 7.75 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027872221171748412		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.0027872221171748412 | validation: 0.003514588240720145]
	TIME [epoch: 7.74 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025296442660956084		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.0025296442660956084 | validation: 0.0038630299044253953]
	TIME [epoch: 7.72 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025402806042026035		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.0025402806042026035 | validation: 0.0032415748390723807]
	TIME [epoch: 7.72 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002757122040450848		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.002757122040450848 | validation: 0.0041515017828931714]
	TIME [epoch: 7.73 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002722646937625995		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.002722646937625995 | validation: 0.003536351556750115]
	TIME [epoch: 7.77 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002499179689104807		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.002499179689104807 | validation: 0.0033264298909001433]
	TIME [epoch: 7.73 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002808479240482437		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.002808479240482437 | validation: 0.003983349922124124]
	TIME [epoch: 7.71 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00241045308636721		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.00241045308636721 | validation: 0.0034642171898570714]
	TIME [epoch: 7.72 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027427910072591664		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.0027427910072591664 | validation: 0.003789140824415862]
	TIME [epoch: 7.72 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029121935392740516		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.0029121935392740516 | validation: 0.0035478905084179043]
	TIME [epoch: 7.77 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028076305473178275		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.0028076305473178275 | validation: 0.004393165802741199]
	TIME [epoch: 7.72 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002736987816104801		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.002736987816104801 | validation: 0.0039555190820600526]
	TIME [epoch: 7.72 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002715396054289251		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.002715396054289251 | validation: 0.0035935988090320947]
	TIME [epoch: 7.71 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002494471322309613		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.002494471322309613 | validation: 0.003526357841438463]
	TIME [epoch: 7.71 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025017084393824064		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.0025017084393824064 | validation: 0.0033963701414754625]
	TIME [epoch: 7.77 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025028232252101495		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.0025028232252101495 | validation: 0.0037945373781638064]
	TIME [epoch: 7.71 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025691918773695526		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.0025691918773695526 | validation: 0.0037923222978215767]
	TIME [epoch: 7.72 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026335849485368086		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.0026335849485368086 | validation: 0.0038069570750418613]
	TIME [epoch: 7.72 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026020442619898473		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.0026020442619898473 | validation: 0.004242416672134745]
	TIME [epoch: 7.73 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026951661967049003		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.0026951661967049003 | validation: 0.003694769655178917]
	TIME [epoch: 7.76 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025905588303952757		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.0025905588303952757 | validation: 0.0036602482490953]
	TIME [epoch: 7.72 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025512680821232806		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.0025512680821232806 | validation: 0.004321987696819293]
	TIME [epoch: 7.72 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026024360264145585		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.0026024360264145585 | validation: 0.003306450799099305]
	TIME [epoch: 7.71 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027398616170720338		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.0027398616170720338 | validation: 0.003461973202882238]
	TIME [epoch: 7.74 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002648022909953895		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.002648022909953895 | validation: 0.0037271779224190968]
	TIME [epoch: 7.76 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002915601226041007		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.002915601226041007 | validation: 0.0038917920278736555]
	TIME [epoch: 7.72 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025491557310440196		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.0025491557310440196 | validation: 0.0034038730607577226]
	TIME [epoch: 7.72 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00277734373525174		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.00277734373525174 | validation: 0.00371668958061995]
	TIME [epoch: 7.72 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026076929292778227		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.0026076929292778227 | validation: 0.0031994597710550184]
	TIME [epoch: 7.76 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026560830593230673		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.0026560830593230673 | validation: 0.003769358670720414]
	TIME [epoch: 7.74 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027219799105037386		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.0027219799105037386 | validation: 0.0033996136302542335]
	TIME [epoch: 7.72 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002743696221359757		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.002743696221359757 | validation: 0.0033197684482710182]
	TIME [epoch: 7.72 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025434035852243287		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.0025434035852243287 | validation: 0.003527557820079597]
	TIME [epoch: 7.72 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026846493644573114		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.0026846493644573114 | validation: 0.0032042744186411387]
	TIME [epoch: 7.77 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026631560780007563		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.0026631560780007563 | validation: 0.0032746156420284875]
	TIME [epoch: 7.73 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024511945256941504		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.0024511945256941504 | validation: 0.0036460660822544523]
	TIME [epoch: 7.71 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028482647854678545		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.0028482647854678545 | validation: 0.003378930538994945]
	TIME [epoch: 7.76 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026761915378671628		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.0026761915378671628 | validation: 0.0033860033650735862]
	TIME [epoch: 7.71 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002533944279877907		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.002533944279877907 | validation: 0.003815015602481618]
	TIME [epoch: 7.77 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002455289666701692		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.002455289666701692 | validation: 0.003924603739041304]
	TIME [epoch: 7.73 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025235806700518526		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.0025235806700518526 | validation: 0.003577526726468416]
	TIME [epoch: 7.72 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002687378216012101		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.002687378216012101 | validation: 0.003649887400568606]
	TIME [epoch: 7.71 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002750656028446494		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.002750656028446494 | validation: 0.003432706108577783]
	TIME [epoch: 7.74 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002531927642741183		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.002531927642741183 | validation: 0.003188958243083112]
	TIME [epoch: 7.82 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028574340457321965		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.0028574340457321965 | validation: 0.0036370778816671025]
	TIME [epoch: 7.73 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002725195612142529		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.002725195612142529 | validation: 0.003556372487971446]
	TIME [epoch: 7.72 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002635571403715872		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.002635571403715872 | validation: 0.0033375175099495426]
	TIME [epoch: 7.73 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027197512220907107		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.0027197512220907107 | validation: 0.0039939501673434685]
	TIME [epoch: 7.74 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002509502374725009		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.002509502374725009 | validation: 0.003354730064634705]
	TIME [epoch: 7.78 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002494520013090833		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.002494520013090833 | validation: 0.003200182515867467]
	TIME [epoch: 7.72 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002615453062951082		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.002615453062951082 | validation: 0.004051928786837218]
	TIME [epoch: 7.72 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030003181553733063		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.0030003181553733063 | validation: 0.003283495537951609]
	TIME [epoch: 7.7 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028330724734224183		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.0028330724734224183 | validation: 0.0032168085604437737]
	TIME [epoch: 7.74 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002554724550793037		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.002554724550793037 | validation: 0.0033741439265513656]
	TIME [epoch: 7.78 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027250070741934806		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.0027250070741934806 | validation: 0.003375003056571586]
	TIME [epoch: 7.72 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026045695186949084		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.0026045695186949084 | validation: 0.00334507549210467]
	TIME [epoch: 7.71 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026977527771985797		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.0026977527771985797 | validation: 0.0036138593130524144]
	TIME [epoch: 7.72 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002629022148427723		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.002629022148427723 | validation: 0.003525003010866535]
	TIME [epoch: 7.78 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028962388004495463		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.0028962388004495463 | validation: 0.0032900930566434746]
	TIME [epoch: 7.72 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023596513345318106		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.0023596513345318106 | validation: 0.00328176705731227]
	TIME [epoch: 7.72 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024515730485959878		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.0024515730485959878 | validation: 0.0036917754255932477]
	TIME [epoch: 7.72 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002974075296806208		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.002974075296806208 | validation: 0.003790616630035666]
	TIME [epoch: 7.71 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024942555511541127		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 0.0024942555511541127 | validation: 0.0033104187924408563]
	TIME [epoch: 7.78 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024045306008704456		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 0.0024045306008704456 | validation: 0.0032750352132238786]
	TIME [epoch: 7.73 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025267443095954296		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 0.0025267443095954296 | validation: 0.0029794320495540065]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1312.pth
	Model improved!!!
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002729492757363622		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 0.002729492757363622 | validation: 0.003779130663691773]
	TIME [epoch: 7.72 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024991768976907777		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 0.0024991768976907777 | validation: 0.0038746539057514674]
	TIME [epoch: 7.72 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002484972805722324		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 0.002484972805722324 | validation: 0.0031367472840286813]
	TIME [epoch: 7.8 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026704341382727597		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 0.0026704341382727597 | validation: 0.0031496488583353946]
	TIME [epoch: 7.71 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002517249251834269		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 0.002517249251834269 | validation: 0.0030676854577448395]
	TIME [epoch: 7.74 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026139596944551712		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 0.0026139596944551712 | validation: 0.003497705915364426]
	TIME [epoch: 7.75 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025248549902720566		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 0.0025248549902720566 | validation: 0.003153769331905386]
	TIME [epoch: 7.75 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027324381920121373		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 0.0027324381920121373 | validation: 0.0035987892416432356]
	TIME [epoch: 7.77 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002449248325674456		[learning rate: 0.00011092]
	Learning Rate: 0.000110918
	LOSS [training: 0.002449248325674456 | validation: 0.003544895466507808]
	TIME [epoch: 7.72 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023536909486526836		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 0.0023536909486526836 | validation: 0.003862249003641403]
	TIME [epoch: 7.71 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023307862384069128		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 0.0023307862384069128 | validation: 0.00377226113489599]
	TIME [epoch: 7.71 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023763159534271514		[learning rate: 0.00010975]
	Learning Rate: 0.000109745
	LOSS [training: 0.0023763159534271514 | validation: 0.0036749067641936037]
	TIME [epoch: 7.74 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027554395402933367		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 0.0027554395402933367 | validation: 0.0034959896122150327]
	TIME [epoch: 7.75 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002693736703413279		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 0.002693736703413279 | validation: 0.0036191560307747414]
	TIME [epoch: 7.69 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025301724803045546		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 0.0025301724803045546 | validation: 0.0033696779611963406]
	TIME [epoch: 7.74 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026901482679182994		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 0.0026901482679182994 | validation: 0.003887747474033313]
	TIME [epoch: 7.69 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024918087982586613		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 0.0024918087982586613 | validation: 0.00350725966586056]
	TIME [epoch: 7.77 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025866364143590946		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 0.0025866364143590946 | validation: 0.003265915274038222]
	TIME [epoch: 7.73 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026894547230738373		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 0.0026894547230738373 | validation: 0.0036920729504051576]
	TIME [epoch: 7.72 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002553183346288677		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 0.002553183346288677 | validation: 0.0039047642668327987]
	TIME [epoch: 7.73 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026317187215053146		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 0.0026317187215053146 | validation: 0.0033721900727510983]
	TIME [epoch: 7.72 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002336000210268604		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 0.002336000210268604 | validation: 0.003370741473196711]
	TIME [epoch: 7.78 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023189109080140355		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 0.0023189109080140355 | validation: 0.0032074586225046176]
	TIME [epoch: 7.74 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024540188036535466		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 0.0024540188036535466 | validation: 0.0036354179006798095]
	TIME [epoch: 7.74 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002624333480129905		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 0.002624333480129905 | validation: 0.0033127246340962516]
	TIME [epoch: 7.73 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025971755732616493		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 0.0025971755732616493 | validation: 0.0036575578205233616]
	TIME [epoch: 7.71 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00259543435134971		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 0.00259543435134971 | validation: 0.0031734846555700772]
	TIME [epoch: 7.8 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025078499823849316		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 0.0025078499823849316 | validation: 0.003574373071212958]
	TIME [epoch: 7.71 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023812918881461246		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 0.0023812918881461246 | validation: 0.0036452568912908228]
	TIME [epoch: 7.73 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00268702287328247		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 0.00268702287328247 | validation: 0.003261880661699501]
	TIME [epoch: 7.7 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027823393054356585		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 0.0027823393054356585 | validation: 0.00348721209126702]
	TIME [epoch: 7.73 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002652793049370292		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 0.002652793049370292 | validation: 0.0035022986797276275]
	TIME [epoch: 7.78 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025330712297600925		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 0.0025330712297600925 | validation: 0.004086137109060707]
	TIME [epoch: 7.73 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025329132445990827		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 0.0025329132445990827 | validation: 0.003522766192575337]
	TIME [epoch: 7.7 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002659263246471113		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.002659263246471113 | validation: 0.003404483731845336]
	TIME [epoch: 7.71 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025561876836949302		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 0.0025561876836949302 | validation: 0.0038748364706412543]
	TIME [epoch: 7.72 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002326126360955135		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 0.002326126360955135 | validation: 0.0034745951909421974]
	TIME [epoch: 7.77 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023864477441563245		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 0.0023864477441563245 | validation: 0.003278753380054072]
	TIME [epoch: 7.73 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025292156474316253		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 0.0025292156474316253 | validation: 0.003131467053698835]
	TIME [epoch: 7.75 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002598729049285164		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 0.002598729049285164 | validation: 0.003994581177119857]
	TIME [epoch: 7.73 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002707709735206283		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 0.002707709735206283 | validation: 0.0036264226565617115]
	TIME [epoch: 7.76 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00262123659075209		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 0.00262123659075209 | validation: 0.0031957751978728553]
	TIME [epoch: 7.75 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002429201733898205		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 0.002429201733898205 | validation: 0.002927997900854431]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1355.pth
	Model improved!!!
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002455097440133551		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 0.002455097440133551 | validation: 0.0031717006325276105]
	TIME [epoch: 7.73 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025495973300556814		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 0.0025495973300556814 | validation: 0.003600952990283549]
	TIME [epoch: 7.71 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025363699108958233		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 0.0025363699108958233 | validation: 0.003917798802916915]
	TIME [epoch: 7.77 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025283490594563904		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 0.0025283490594563904 | validation: 0.003485359823566003]
	TIME [epoch: 7.75 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023818817796018993		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 0.0023818817796018993 | validation: 0.003943027951266729]
	TIME [epoch: 7.73 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026442317887909552		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 0.0026442317887909552 | validation: 0.003551063986914814]
	TIME [epoch: 7.74 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025472669954714647		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 0.0025472669954714647 | validation: 0.0032569401156523387]
	TIME [epoch: 7.74 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027839790199607954		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 0.0027839790199607954 | validation: 0.0035911415350289824]
	TIME [epoch: 7.8 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002697245701378695		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 0.002697245701378695 | validation: 0.0037398168327415046]
	TIME [epoch: 7.74 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024631359305292903		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 0.0024631359305292903 | validation: 0.005140421959557301]
	TIME [epoch: 7.73 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025019680606200093		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 0.0025019680606200093 | validation: 0.0036766431599971083]
	TIME [epoch: 7.73 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026138100424199376		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 0.0026138100424199376 | validation: 0.003073270664231565]
	TIME [epoch: 7.75 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002584816331444379		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 0.002584816331444379 | validation: 0.0036460110970967284]
	TIME [epoch: 7.79 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027873747884094207		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 0.0027873747884094207 | validation: 0.0033632962119009716]
	TIME [epoch: 7.74 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025961588151404516		[learning rate: 9.3243e-05]
	Learning Rate: 9.32428e-05
	LOSS [training: 0.0025961588151404516 | validation: 0.003369458166980595]
	TIME [epoch: 7.73 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026360142014586214		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 0.0026360142014586214 | validation: 0.0037982476000359678]
	TIME [epoch: 7.73 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024900685743649183		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 0.0024900685743649183 | validation: 0.003412672490401632]
	TIME [epoch: 7.76 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026530194464101414		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 0.0026530194464101414 | validation: 0.004043459587074653]
	TIME [epoch: 7.78 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002461444058378684		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 0.002461444058378684 | validation: 0.003534751348599592]
	TIME [epoch: 7.74 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002668135961555659		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 0.002668135961555659 | validation: 0.002984526989339222]
	TIME [epoch: 7.73 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002618743781404913		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 0.002618743781404913 | validation: 0.0031799593140969984]
	TIME [epoch: 7.73 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025109506489321303		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 0.0025109506489321303 | validation: 0.0035722960038178393]
	TIME [epoch: 7.78 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026532417253970937		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 0.0026532417253970937 | validation: 0.0035184449423378136]
	TIME [epoch: 7.76 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023542510623045486		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 0.0023542510623045486 | validation: 0.003345269447588183]
	TIME [epoch: 7.74 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025381500392667		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 0.0025381500392667 | validation: 0.0037904334193289433]
	TIME [epoch: 7.73 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026114924076685647		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 0.0026114924076685647 | validation: 0.003168154043054207]
	TIME [epoch: 7.73 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00293165773385941		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 0.00293165773385941 | validation: 0.004233671404110869]
	TIME [epoch: 7.79 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002755620565228958		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 0.002755620565228958 | validation: 0.004011347994372278]
	TIME [epoch: 7.76 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027297339339589412		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 0.0027297339339589412 | validation: 0.0037249215304015473]
	TIME [epoch: 7.74 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025476098759340824		[learning rate: 8.8418e-05]
	Learning Rate: 8.84175e-05
	LOSS [training: 0.0025476098759340824 | validation: 0.0032738392529296735]
	TIME [epoch: 7.73 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027226794911640235		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.0027226794911640235 | validation: 0.003939322047496221]
	TIME [epoch: 7.74 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025792681864314675		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 0.0025792681864314675 | validation: 0.003638956325966525]
	TIME [epoch: 7.8 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00260929406025255		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 0.00260929406025255 | validation: 0.0034705721794717394]
	TIME [epoch: 7.73 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026690781523617513		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 0.0026690781523617513 | validation: 0.00393672711402924]
	TIME [epoch: 7.73 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002499348289845479		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 0.002499348289845479 | validation: 0.003598888282021195]
	TIME [epoch: 7.73 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002482264384632614		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 0.002482264384632614 | validation: 0.003912850060331745]
	TIME [epoch: 7.74 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027719161289942768		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 0.0027719161289942768 | validation: 0.0038369675351796785]
	TIME [epoch: 7.79 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002530885079204439		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 0.002530885079204439 | validation: 0.0033494869647133817]
	TIME [epoch: 7.73 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024868921708563474		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 0.0024868921708563474 | validation: 0.0031533332719126637]
	TIME [epoch: 7.73 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025749932659309225		[learning rate: 8.534e-05]
	Learning Rate: 8.53402e-05
	LOSS [training: 0.0025749932659309225 | validation: 0.0036155140478262007]
	TIME [epoch: 7.73 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026036457070663386		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 0.0026036457070663386 | validation: 0.0032476969344908905]
	TIME [epoch: 7.75 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002657756074539856		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 0.002657756074539856 | validation: 0.0038797604438779642]
	TIME [epoch: 7.78 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002644101562374273		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 0.002644101562374273 | validation: 0.0032203436508046156]
	TIME [epoch: 7.73 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026011598457690408		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 0.0026011598457690408 | validation: 0.003593303924388194]
	TIME [epoch: 7.74 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027917754555356355		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 0.0027917754555356355 | validation: 0.0029158279938979745]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1400.pth
	Model improved!!!
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002777602202543634		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 0.002777602202543634 | validation: 0.0033103713040859234]
	TIME [epoch: 7.77 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002542999114283969		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 0.002542999114283969 | validation: 0.0037112011251619376]
	TIME [epoch: 7.75 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024414702609737455		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 0.0024414702609737455 | validation: 0.004030624940114796]
	TIME [epoch: 7.73 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002415988944964927		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 0.002415988944964927 | validation: 0.003954456507771579]
	TIME [epoch: 7.72 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002502286547850503		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 0.002502286547850503 | validation: 0.003079468069770065]
	TIME [epoch: 7.72 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002497762037277478		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 0.002497762037277478 | validation: 0.0032708431240198663]
	TIME [epoch: 7.77 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002614156376376565		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 0.002614156376376565 | validation: 0.003925186473349573]
	TIME [epoch: 7.74 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025043776561189616		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 0.0025043776561189616 | validation: 0.003431833813971984]
	TIME [epoch: 7.73 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002406996477897253		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 0.002406996477897253 | validation: 0.0036184009928270804]
	TIME [epoch: 7.71 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002602294019030768		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 0.002602294019030768 | validation: 0.00371252273377228]
	TIME [epoch: 7.73 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026208458073880506		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 0.0026208458073880506 | validation: 0.003704027659175307]
	TIME [epoch: 7.76 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024287884412034725		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 0.0024287884412034725 | validation: 0.003542880528292163]
	TIME [epoch: 7.73 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00252524766950813		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 0.00252524766950813 | validation: 0.0038113102176126077]
	TIME [epoch: 7.72 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025798961008498458		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 0.0025798961008498458 | validation: 0.003109044780096073]
	TIME [epoch: 7.72 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024928060942481824		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 0.0024928060942481824 | validation: 0.0034780726952773136]
	TIME [epoch: 7.72 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025503841422938106		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 0.0025503841422938106 | validation: 0.003434586014816543]
	TIME [epoch: 7.78 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002445857055913354		[learning rate: 7.8942e-05]
	Learning Rate: 7.8942e-05
	LOSS [training: 0.002445857055913354 | validation: 0.003618929522058898]
	TIME [epoch: 7.73 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024153159124553785		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 0.0024153159124553785 | validation: 0.0035187516753479964]
	TIME [epoch: 7.71 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025151174185451737		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 0.0025151174185451737 | validation: 0.0034728235933578714]
	TIME [epoch: 7.73 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023908249429632905		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 0.0023908249429632905 | validation: 0.0034418132287682085]
	TIME [epoch: 7.74 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025577434529212616		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 0.0025577434529212616 | validation: 0.003564911980764609]
	TIME [epoch: 7.77 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002464944148975341		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 0.002464944148975341 | validation: 0.0034596023395733304]
	TIME [epoch: 7.7 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024962070699533446		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 0.0024962070699533446 | validation: 0.00338522400071477]
	TIME [epoch: 7.72 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024786863105363544		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 0.0024786863105363544 | validation: 0.003317486043200013]
	TIME [epoch: 7.72 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027178440906449066		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.0027178440906449066 | validation: 0.003078441891852874]
	TIME [epoch: 7.73 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002406272746237788		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 0.002406272746237788 | validation: 0.0032953041428227404]
	TIME [epoch: 7.76 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022857510181733647		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 0.0022857510181733647 | validation: 0.0032292618187845365]
	TIME [epoch: 7.69 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00237645886863001		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 0.00237645886863001 | validation: 0.0036957238432122706]
	TIME [epoch: 7.7 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002394040219565436		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 0.002394040219565436 | validation: 0.0035348952467891266]
	TIME [epoch: 7.7 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024373521110372795		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 0.0024373521110372795 | validation: 0.004060849155782069]
	TIME [epoch: 7.73 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025306053111271384		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 0.0025306053111271384 | validation: 0.0033971452942390583]
	TIME [epoch: 7.72 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027629314906610055		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 0.0027629314906610055 | validation: 0.003955217470945766]
	TIME [epoch: 7.71 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024737499534540314		[learning rate: 7.4592e-05]
	Learning Rate: 7.4592e-05
	LOSS [training: 0.0024737499534540314 | validation: 0.003133327257797887]
	TIME [epoch: 7.72 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002397051766977071		[learning rate: 7.4328e-05]
	Learning Rate: 7.43283e-05
	LOSS [training: 0.002397051766977071 | validation: 0.003516511241622798]
	TIME [epoch: 7.73 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026782583567282288		[learning rate: 7.4065e-05]
	Learning Rate: 7.40654e-05
	LOSS [training: 0.0026782583567282288 | validation: 0.00350416616404846]
	TIME [epoch: 7.78 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002488609858218099		[learning rate: 7.3803e-05]
	Learning Rate: 7.38035e-05
	LOSS [training: 0.002488609858218099 | validation: 0.00308035894182124]
	TIME [epoch: 7.71 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002238950145288467		[learning rate: 7.3543e-05]
	Learning Rate: 7.35425e-05
	LOSS [training: 0.002238950145288467 | validation: 0.0034379258197285998]
	TIME [epoch: 7.71 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026104015476294566		[learning rate: 7.3282e-05]
	Learning Rate: 7.32825e-05
	LOSS [training: 0.0026104015476294566 | validation: 0.0031880668051322224]
	TIME [epoch: 7.71 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026445591876080544		[learning rate: 7.3023e-05]
	Learning Rate: 7.30233e-05
	LOSS [training: 0.0026445591876080544 | validation: 0.0029498950971369926]
	TIME [epoch: 7.7 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002497843288657047		[learning rate: 7.2765e-05]
	Learning Rate: 7.27651e-05
	LOSS [training: 0.002497843288657047 | validation: 0.0033145927829927773]
	TIME [epoch: 7.77 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002297814348039541		[learning rate: 7.2508e-05]
	Learning Rate: 7.25078e-05
	LOSS [training: 0.002297814348039541 | validation: 0.0034290069834256834]
	TIME [epoch: 7.72 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027901987835030127		[learning rate: 7.2251e-05]
	Learning Rate: 7.22514e-05
	LOSS [training: 0.0027901987835030127 | validation: 0.003805320877791292]
	TIME [epoch: 7.71 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002563565256934024		[learning rate: 7.1996e-05]
	Learning Rate: 7.19959e-05
	LOSS [training: 0.002563565256934024 | validation: 0.0036031566091507755]
	TIME [epoch: 7.71 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002519770791630078		[learning rate: 7.1741e-05]
	Learning Rate: 7.17413e-05
	LOSS [training: 0.002519770791630078 | validation: 0.003329753340796746]
	TIME [epoch: 7.7 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024058713719390602		[learning rate: 7.1488e-05]
	Learning Rate: 7.14876e-05
	LOSS [training: 0.0024058713719390602 | validation: 0.003001851553638844]
	TIME [epoch: 7.76 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002273165492925522		[learning rate: 7.1235e-05]
	Learning Rate: 7.12348e-05
	LOSS [training: 0.002273165492925522 | validation: 0.0032626681543100515]
	TIME [epoch: 7.72 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025431782717993817		[learning rate: 7.0983e-05]
	Learning Rate: 7.09829e-05
	LOSS [training: 0.0025431782717993817 | validation: 0.0034356690383321445]
	TIME [epoch: 7.72 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002404011756197261		[learning rate: 7.0732e-05]
	Learning Rate: 7.07319e-05
	LOSS [training: 0.002404011756197261 | validation: 0.003476011606702781]
	TIME [epoch: 7.71 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026460161237486565		[learning rate: 7.0482e-05]
	Learning Rate: 7.04818e-05
	LOSS [training: 0.0026460161237486565 | validation: 0.0032909541104847303]
	TIME [epoch: 7.72 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002528887167809973		[learning rate: 7.0233e-05]
	Learning Rate: 7.02326e-05
	LOSS [training: 0.002528887167809973 | validation: 0.003443582875713041]
	TIME [epoch: 7.74 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023876504982115266		[learning rate: 6.9984e-05]
	Learning Rate: 6.99842e-05
	LOSS [training: 0.0023876504982115266 | validation: 0.003421685419992665]
	TIME [epoch: 7.71 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025794976365262996		[learning rate: 6.9737e-05]
	Learning Rate: 6.97367e-05
	LOSS [training: 0.0025794976365262996 | validation: 0.0033880987114556166]
	TIME [epoch: 7.71 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002260172197330219		[learning rate: 6.949e-05]
	Learning Rate: 6.94901e-05
	LOSS [training: 0.002260172197330219 | validation: 0.0033658959081102192]
	TIME [epoch: 7.71 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025245102923248334		[learning rate: 6.9244e-05]
	Learning Rate: 6.92444e-05
	LOSS [training: 0.0025245102923248334 | validation: 0.0034998771352872477]
	TIME [epoch: 7.75 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025403259764546185		[learning rate: 6.9e-05]
	Learning Rate: 6.89995e-05
	LOSS [training: 0.0025403259764546185 | validation: 0.0034242464285105826]
	TIME [epoch: 7.78 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026280328117209866		[learning rate: 6.8756e-05]
	Learning Rate: 6.87555e-05
	LOSS [training: 0.0026280328117209866 | validation: 0.004333408798637545]
	TIME [epoch: 7.7 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026305776944041143		[learning rate: 6.8512e-05]
	Learning Rate: 6.85124e-05
	LOSS [training: 0.0026305776944041143 | validation: 0.003756200279837276]
	TIME [epoch: 7.7 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024327668284610006		[learning rate: 6.827e-05]
	Learning Rate: 6.82702e-05
	LOSS [training: 0.0024327668284610006 | validation: 0.0032161212239228873]
	TIME [epoch: 7.72 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024775708408061106		[learning rate: 6.8029e-05]
	Learning Rate: 6.80287e-05
	LOSS [training: 0.0024775708408061106 | validation: 0.003506879483105295]
	TIME [epoch: 7.75 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024887713033339277		[learning rate: 6.7788e-05]
	Learning Rate: 6.77882e-05
	LOSS [training: 0.0024887713033339277 | validation: 0.0034793400439565936]
	TIME [epoch: 7.73 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024710596902795304		[learning rate: 6.7548e-05]
	Learning Rate: 6.75485e-05
	LOSS [training: 0.0024710596902795304 | validation: 0.0032213032104265365]
	TIME [epoch: 7.71 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002516588462116216		[learning rate: 6.731e-05]
	Learning Rate: 6.73096e-05
	LOSS [training: 0.002516588462116216 | validation: 0.003769743439052342]
	TIME [epoch: 7.71 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002410547552432568		[learning rate: 6.7072e-05]
	Learning Rate: 6.70716e-05
	LOSS [training: 0.002410547552432568 | validation: 0.003222291821046149]
	TIME [epoch: 7.7 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026855330331534686		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 0.0026855330331534686 | validation: 0.0028469656190168273]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1464.pth
	Model improved!!!
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002548825688346615		[learning rate: 6.6598e-05]
	Learning Rate: 6.65981e-05
	LOSS [training: 0.002548825688346615 | validation: 0.0036694449892266173]
	TIME [epoch: 7.74 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002391284448671159		[learning rate: 6.6363e-05]
	Learning Rate: 6.63626e-05
	LOSS [training: 0.002391284448671159 | validation: 0.0035538576139236937]
	TIME [epoch: 7.72 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023671825985324053		[learning rate: 6.6128e-05]
	Learning Rate: 6.61279e-05
	LOSS [training: 0.0023671825985324053 | validation: 0.003050794846278812]
	TIME [epoch: 7.73 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024087065713459003		[learning rate: 6.5894e-05]
	Learning Rate: 6.58941e-05
	LOSS [training: 0.0024087065713459003 | validation: 0.0032578041655734226]
	TIME [epoch: 7.73 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002133917947938651		[learning rate: 6.5661e-05]
	Learning Rate: 6.5661e-05
	LOSS [training: 0.002133917947938651 | validation: 0.003571830493921847]
	TIME [epoch: 7.78 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023840983206994005		[learning rate: 6.5429e-05]
	Learning Rate: 6.54289e-05
	LOSS [training: 0.0023840983206994005 | validation: 0.0027875723037120636]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1470.pth
	Model improved!!!
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025675480215242795		[learning rate: 6.5197e-05]
	Learning Rate: 6.51975e-05
	LOSS [training: 0.0025675480215242795 | validation: 0.0030771873815294585]
	TIME [epoch: 7.7 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024090880185789272		[learning rate: 6.4967e-05]
	Learning Rate: 6.49669e-05
	LOSS [training: 0.0024090880185789272 | validation: 0.003286110432378825]
	TIME [epoch: 7.74 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002451575104562538		[learning rate: 6.4737e-05]
	Learning Rate: 6.47372e-05
	LOSS [training: 0.002451575104562538 | validation: 0.0028837857609178546]
	TIME [epoch: 7.73 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002687043407761645		[learning rate: 6.4508e-05]
	Learning Rate: 6.45083e-05
	LOSS [training: 0.002687043407761645 | validation: 0.003070197208681848]
	TIME [epoch: 7.78 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024803711350344163		[learning rate: 6.428e-05]
	Learning Rate: 6.42802e-05
	LOSS [training: 0.0024803711350344163 | validation: 0.00316644250242212]
	TIME [epoch: 7.72 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025664394399346384		[learning rate: 6.4053e-05]
	Learning Rate: 6.40529e-05
	LOSS [training: 0.0025664394399346384 | validation: 0.003432811915621337]
	TIME [epoch: 7.72 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024867925115723663		[learning rate: 6.3826e-05]
	Learning Rate: 6.38264e-05
	LOSS [training: 0.0024867925115723663 | validation: 0.003758632441542735]
	TIME [epoch: 7.73 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025724892133148756		[learning rate: 6.3601e-05]
	Learning Rate: 6.36007e-05
	LOSS [training: 0.0025724892133148756 | validation: 0.003175894774448362]
	TIME [epoch: 7.76 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026979816179282753		[learning rate: 6.3376e-05]
	Learning Rate: 6.33758e-05
	LOSS [training: 0.0026979816179282753 | validation: 0.0034168900461207232]
	TIME [epoch: 7.74 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024929855572607394		[learning rate: 6.3152e-05]
	Learning Rate: 6.31517e-05
	LOSS [training: 0.0024929855572607394 | validation: 0.0036517289377260634]
	TIME [epoch: 7.73 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002608114474027256		[learning rate: 6.2928e-05]
	Learning Rate: 6.29283e-05
	LOSS [training: 0.002608114474027256 | validation: 0.002995925653648094]
	TIME [epoch: 7.72 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002590279704618833		[learning rate: 6.2706e-05]
	Learning Rate: 6.27058e-05
	LOSS [training: 0.002590279704618833 | validation: 0.0033206504196232178]
	TIME [epoch: 7.73 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023624751616852808		[learning rate: 6.2484e-05]
	Learning Rate: 6.24841e-05
	LOSS [training: 0.0023624751616852808 | validation: 0.003161100221418583]
	TIME [epoch: 7.77 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023741296146709986		[learning rate: 6.2263e-05]
	Learning Rate: 6.22631e-05
	LOSS [training: 0.0023741296146709986 | validation: 0.003163009394964756]
	TIME [epoch: 7.74 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025644129640366394		[learning rate: 6.2043e-05]
	Learning Rate: 6.20429e-05
	LOSS [training: 0.0025644129640366394 | validation: 0.00335096995129704]
	TIME [epoch: 7.73 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025778211023677767		[learning rate: 6.1824e-05]
	Learning Rate: 6.18235e-05
	LOSS [training: 0.0025778211023677767 | validation: 0.002981642850834727]
	TIME [epoch: 7.73 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002427717586336296		[learning rate: 6.1605e-05]
	Learning Rate: 6.16049e-05
	LOSS [training: 0.002427717586336296 | validation: 0.0034457878606981313]
	TIME [epoch: 7.71 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002615443483033246		[learning rate: 6.1387e-05]
	Learning Rate: 6.13871e-05
	LOSS [training: 0.002615443483033246 | validation: 0.003205872682897977]
	TIME [epoch: 7.78 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023874310675021156		[learning rate: 6.117e-05]
	Learning Rate: 6.117e-05
	LOSS [training: 0.0023874310675021156 | validation: 0.002937447206521975]
	TIME [epoch: 7.75 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002704285539441669		[learning rate: 6.0954e-05]
	Learning Rate: 6.09537e-05
	LOSS [training: 0.002704285539441669 | validation: 0.0037942695656893416]
	TIME [epoch: 7.71 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002347034595580585		[learning rate: 6.0738e-05]
	Learning Rate: 6.07382e-05
	LOSS [training: 0.002347034595580585 | validation: 0.0031963966734287466]
	TIME [epoch: 7.71 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024734972614680233		[learning rate: 6.0523e-05]
	Learning Rate: 6.05234e-05
	LOSS [training: 0.0024734972614680233 | validation: 0.003316755281367683]
	TIME [epoch: 7.71 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002336603945039605		[learning rate: 6.0309e-05]
	Learning Rate: 6.03094e-05
	LOSS [training: 0.002336603945039605 | validation: 0.003469430970811284]
	TIME [epoch: 7.78 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024157749226006646		[learning rate: 6.0096e-05]
	Learning Rate: 6.00961e-05
	LOSS [training: 0.0024157749226006646 | validation: 0.003618197985627558]
	TIME [epoch: 7.72 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025399900363123388		[learning rate: 5.9884e-05]
	Learning Rate: 5.98836e-05
	LOSS [training: 0.0025399900363123388 | validation: 0.003123479277523434]
	TIME [epoch: 7.71 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002406756062214007		[learning rate: 5.9672e-05]
	Learning Rate: 5.96718e-05
	LOSS [training: 0.002406756062214007 | validation: 0.003237708270855867]
	TIME [epoch: 7.72 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002504835583105489		[learning rate: 5.9461e-05]
	Learning Rate: 5.94608e-05
	LOSS [training: 0.002504835583105489 | validation: 0.003713556125336309]
	TIME [epoch: 7.73 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025146481694258453		[learning rate: 5.9251e-05]
	Learning Rate: 5.92505e-05
	LOSS [training: 0.0025146481694258453 | validation: 0.0035245361520661344]
	TIME [epoch: 7.78 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002536539055304308		[learning rate: 5.9041e-05]
	Learning Rate: 5.9041e-05
	LOSS [training: 0.002536539055304308 | validation: 0.003041464262012579]
	TIME [epoch: 7.71 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023037545410473674		[learning rate: 5.8832e-05]
	Learning Rate: 5.88323e-05
	LOSS [training: 0.0023037545410473674 | validation: 0.0033324695249732006]
	TIME [epoch: 7.71 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025709974347463394		[learning rate: 5.8624e-05]
	Learning Rate: 5.86242e-05
	LOSS [training: 0.0025709974347463394 | validation: 0.0033028401172564286]
	TIME [epoch: 7.73 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002675126628875539		[learning rate: 5.8417e-05]
	Learning Rate: 5.84169e-05
	LOSS [training: 0.002675126628875539 | validation: 0.00350949756324923]
	TIME [epoch: 7.72 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002559747079725624		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 0.002559747079725624 | validation: 0.0036201188699509705]
	TIME [epoch: 7.76 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002265320606512092		[learning rate: 5.8004e-05]
	Learning Rate: 5.80045e-05
	LOSS [training: 0.002265320606512092 | validation: 0.003401285512811798]
	TIME [epoch: 7.7 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024338872062206413		[learning rate: 5.7799e-05]
	Learning Rate: 5.77994e-05
	LOSS [training: 0.0024338872062206413 | validation: 0.0037785683911230068]
	TIME [epoch: 7.73 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026483344364308676		[learning rate: 5.7595e-05]
	Learning Rate: 5.7595e-05
	LOSS [training: 0.0026483344364308676 | validation: 0.003193948763294876]
	TIME [epoch: 7.74 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022090613781938486		[learning rate: 5.7391e-05]
	Learning Rate: 5.73913e-05
	LOSS [training: 0.0022090613781938486 | validation: 0.0031448920418737404]
	TIME [epoch: 7.78 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023539854558286446		[learning rate: 5.7188e-05]
	Learning Rate: 5.71884e-05
	LOSS [training: 0.0023539854558286446 | validation: 0.00344908874344374]
	TIME [epoch: 7.71 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002395770675953403		[learning rate: 5.6986e-05]
	Learning Rate: 5.69861e-05
	LOSS [training: 0.002395770675953403 | validation: 0.003364579782515226]
	TIME [epoch: 7.73 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002412132737912805		[learning rate: 5.6785e-05]
	Learning Rate: 5.67846e-05
	LOSS [training: 0.002412132737912805 | validation: 0.0032806893307854666]
	TIME [epoch: 7.71 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024623384797386673		[learning rate: 5.6584e-05]
	Learning Rate: 5.65838e-05
	LOSS [training: 0.0024623384797386673 | validation: 0.003491393399624893]
	TIME [epoch: 7.72 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024396466512069644		[learning rate: 5.6384e-05]
	Learning Rate: 5.63837e-05
	LOSS [training: 0.0024396466512069644 | validation: 0.0029830408209312836]
	TIME [epoch: 7.77 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002472189086964188		[learning rate: 5.6184e-05]
	Learning Rate: 5.61844e-05
	LOSS [training: 0.002472189086964188 | validation: 0.0036909192054049283]
	TIME [epoch: 7.74 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025342139084330766		[learning rate: 5.5986e-05]
	Learning Rate: 5.59857e-05
	LOSS [training: 0.0025342139084330766 | validation: 0.0039001806164835804]
	TIME [epoch: 7.7 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002236138053851934		[learning rate: 5.5788e-05]
	Learning Rate: 5.57877e-05
	LOSS [training: 0.002236138053851934 | validation: 0.0033565334572113023]
	TIME [epoch: 7.75 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002590731864939532		[learning rate: 5.559e-05]
	Learning Rate: 5.55904e-05
	LOSS [training: 0.002590731864939532 | validation: 0.003221719762114124]
	TIME [epoch: 7.74 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002515858978217975		[learning rate: 5.5394e-05]
	Learning Rate: 5.53939e-05
	LOSS [training: 0.002515858978217975 | validation: 0.003143735530983317]
	TIME [epoch: 7.79 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024578998632885504		[learning rate: 5.5198e-05]
	Learning Rate: 5.5198e-05
	LOSS [training: 0.0024578998632885504 | validation: 0.0037986951918191868]
	TIME [epoch: 7.75 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027326330491891553		[learning rate: 5.5003e-05]
	Learning Rate: 5.50028e-05
	LOSS [training: 0.0027326330491891553 | validation: 0.0035291347677208805]
	TIME [epoch: 7.75 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023673736933114706		[learning rate: 5.4808e-05]
	Learning Rate: 5.48083e-05
	LOSS [training: 0.0023673736933114706 | validation: 0.0036146557896393665]
	TIME [epoch: 7.75 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002466715438242958		[learning rate: 5.4614e-05]
	Learning Rate: 5.46145e-05
	LOSS [training: 0.002466715438242958 | validation: 0.003546604351989711]
	TIME [epoch: 7.75 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025898521365921986		[learning rate: 5.4421e-05]
	Learning Rate: 5.44213e-05
	LOSS [training: 0.0025898521365921986 | validation: 0.003027308483877514]
	TIME [epoch: 7.79 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024770114601965583		[learning rate: 5.4229e-05]
	Learning Rate: 5.42289e-05
	LOSS [training: 0.0024770114601965583 | validation: 0.003547748696650018]
	TIME [epoch: 7.72 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026266806670891306		[learning rate: 5.4037e-05]
	Learning Rate: 5.40371e-05
	LOSS [training: 0.0026266806670891306 | validation: 0.0030769901476485366]
	TIME [epoch: 7.71 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002401275282613329		[learning rate: 5.3846e-05]
	Learning Rate: 5.38461e-05
	LOSS [training: 0.002401275282613329 | validation: 0.003115840558933698]
	TIME [epoch: 7.75 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002422327781003632		[learning rate: 5.3656e-05]
	Learning Rate: 5.36556e-05
	LOSS [training: 0.002422327781003632 | validation: 0.003483546233217602]
	TIME [epoch: 7.76 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023026771911478505		[learning rate: 5.3466e-05]
	Learning Rate: 5.34659e-05
	LOSS [training: 0.0023026771911478505 | validation: 0.0037552446566063245]
	TIME [epoch: 7.79 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025753590783924862		[learning rate: 5.3277e-05]
	Learning Rate: 5.32769e-05
	LOSS [training: 0.0025753590783924862 | validation: 0.003412319941520991]
	TIME [epoch: 7.74 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00240136907734148		[learning rate: 5.3088e-05]
	Learning Rate: 5.30884e-05
	LOSS [training: 0.00240136907734148 | validation: 0.002985407107714102]
	TIME [epoch: 7.75 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002384298734564813		[learning rate: 5.2901e-05]
	Learning Rate: 5.29007e-05
	LOSS [training: 0.002384298734564813 | validation: 0.003512735331798366]
	TIME [epoch: 7.74 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026822166346003773		[learning rate: 5.2714e-05]
	Learning Rate: 5.27137e-05
	LOSS [training: 0.0026822166346003773 | validation: 0.002834868924193062]
	TIME [epoch: 7.76 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025006349881798256		[learning rate: 5.2527e-05]
	Learning Rate: 5.25272e-05
	LOSS [training: 0.0025006349881798256 | validation: 0.0035169375715265088]
	TIME [epoch: 7.74 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002286355467157962		[learning rate: 5.2342e-05]
	Learning Rate: 5.23415e-05
	LOSS [training: 0.002286355467157962 | validation: 0.003177702247870469]
	TIME [epoch: 7.72 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002317676872791121		[learning rate: 5.2156e-05]
	Learning Rate: 5.21564e-05
	LOSS [training: 0.002317676872791121 | validation: 0.0030669475557799056]
	TIME [epoch: 7.71 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023425346909487724		[learning rate: 5.1972e-05]
	Learning Rate: 5.1972e-05
	LOSS [training: 0.0023425346909487724 | validation: 0.003231685675847028]
	TIME [epoch: 7.74 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025049710676828462		[learning rate: 5.1788e-05]
	Learning Rate: 5.17882e-05
	LOSS [training: 0.0025049710676828462 | validation: 0.003010901887958553]
	TIME [epoch: 7.78 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026433619923120254		[learning rate: 5.1605e-05]
	Learning Rate: 5.16051e-05
	LOSS [training: 0.0026433619923120254 | validation: 0.003168447834964138]
	TIME [epoch: 7.75 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024016853604578925		[learning rate: 5.1423e-05]
	Learning Rate: 5.14226e-05
	LOSS [training: 0.0024016853604578925 | validation: 0.0033680188309223105]
	TIME [epoch: 7.74 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025277647893297813		[learning rate: 5.1241e-05]
	Learning Rate: 5.12407e-05
	LOSS [training: 0.0025277647893297813 | validation: 0.002995458440309986]
	TIME [epoch: 7.75 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023229400039283827		[learning rate: 5.106e-05]
	Learning Rate: 5.10596e-05
	LOSS [training: 0.0023229400039283827 | validation: 0.003201278023311236]
	TIME [epoch: 7.75 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002471135513572445		[learning rate: 5.0879e-05]
	Learning Rate: 5.0879e-05
	LOSS [training: 0.002471135513572445 | validation: 0.003709763400130902]
	TIME [epoch: 7.77 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025499096889768343		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 0.0025499096889768343 | validation: 0.0035790206300564597]
	TIME [epoch: 7.75 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024692220201975095		[learning rate: 5.052e-05]
	Learning Rate: 5.05198e-05
	LOSS [training: 0.0024692220201975095 | validation: 0.002927898230634458]
	TIME [epoch: 7.7 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024122413766285147		[learning rate: 5.0341e-05]
	Learning Rate: 5.03412e-05
	LOSS [training: 0.0024122413766285147 | validation: 0.0036201324197572184]
	TIME [epoch: 7.74 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026263778191137266		[learning rate: 5.0163e-05]
	Learning Rate: 5.01631e-05
	LOSS [training: 0.0026263778191137266 | validation: 0.0034262468975759935]
	TIME [epoch: 7.72 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021971499457106246		[learning rate: 4.9986e-05]
	Learning Rate: 4.99857e-05
	LOSS [training: 0.0021971499457106246 | validation: 0.0038251158345855105]
	TIME [epoch: 7.79 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021890766067169903		[learning rate: 4.9809e-05]
	Learning Rate: 4.9809e-05
	LOSS [training: 0.0021890766067169903 | validation: 0.003120504288574701]
	TIME [epoch: 7.72 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002219419330135192		[learning rate: 4.9633e-05]
	Learning Rate: 4.96329e-05
	LOSS [training: 0.002219419330135192 | validation: 0.0028833913168553387]
	TIME [epoch: 7.72 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022908186969441864		[learning rate: 4.9457e-05]
	Learning Rate: 4.94573e-05
	LOSS [training: 0.0022908186969441864 | validation: 0.003388458243728608]
	TIME [epoch: 7.75 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002301558975752234		[learning rate: 4.9282e-05]
	Learning Rate: 4.92825e-05
	LOSS [training: 0.002301558975752234 | validation: 0.003697401701327574]
	TIME [epoch: 7.76 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023053936051599393		[learning rate: 4.9108e-05]
	Learning Rate: 4.91082e-05
	LOSS [training: 0.0023053936051599393 | validation: 0.003501197104754757]
	TIME [epoch: 7.75 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025533086176491655		[learning rate: 4.8935e-05]
	Learning Rate: 4.89345e-05
	LOSS [training: 0.0025533086176491655 | validation: 0.0034674078863958773]
	TIME [epoch: 7.73 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024786835630127156		[learning rate: 4.8762e-05]
	Learning Rate: 4.87615e-05
	LOSS [training: 0.0024786835630127156 | validation: 0.003582962114952558]
	TIME [epoch: 7.72 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025927699050260107		[learning rate: 4.8589e-05]
	Learning Rate: 4.85891e-05
	LOSS [training: 0.0025927699050260107 | validation: 0.0030617822917199406]
	TIME [epoch: 7.72 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024286168860135084		[learning rate: 4.8417e-05]
	Learning Rate: 4.84172e-05
	LOSS [training: 0.0024286168860135084 | validation: 0.0034696375465870354]
	TIME [epoch: 7.73 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002432057773534449		[learning rate: 4.8246e-05]
	Learning Rate: 4.8246e-05
	LOSS [training: 0.002432057773534449 | validation: 0.003921579933737068]
	TIME [epoch: 7.75 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025763423146136647		[learning rate: 4.8075e-05]
	Learning Rate: 4.80754e-05
	LOSS [training: 0.0025763423146136647 | validation: 0.003670086128220283]
	TIME [epoch: 7.71 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024052137439867453		[learning rate: 4.7905e-05]
	Learning Rate: 4.79054e-05
	LOSS [training: 0.0024052137439867453 | validation: 0.003612524831343224]
	TIME [epoch: 7.74 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024435012913005235		[learning rate: 4.7736e-05]
	Learning Rate: 4.7736e-05
	LOSS [training: 0.0024435012913005235 | validation: 0.0035815707215570525]
	TIME [epoch: 7.7 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025139989237531637		[learning rate: 4.7567e-05]
	Learning Rate: 4.75672e-05
	LOSS [training: 0.0025139989237531637 | validation: 0.00326098871643294]
	TIME [epoch: 7.78 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024258368915522615		[learning rate: 4.7399e-05]
	Learning Rate: 4.7399e-05
	LOSS [training: 0.0024258368915522615 | validation: 0.00348270107759982]
	TIME [epoch: 7.76 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023494699211502957		[learning rate: 4.7231e-05]
	Learning Rate: 4.72314e-05
	LOSS [training: 0.0023494699211502957 | validation: 0.003601912145889243]
	TIME [epoch: 7.74 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024577277257586405		[learning rate: 4.7064e-05]
	Learning Rate: 4.70644e-05
	LOSS [training: 0.0024577277257586405 | validation: 0.003569993117453032]
	TIME [epoch: 7.73 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002419578759169334		[learning rate: 4.6898e-05]
	Learning Rate: 4.6898e-05
	LOSS [training: 0.002419578759169334 | validation: 0.003245812218597931]
	TIME [epoch: 7.74 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002434889925460982		[learning rate: 4.6732e-05]
	Learning Rate: 4.67321e-05
	LOSS [training: 0.002434889925460982 | validation: 0.0032838461443332754]
	TIME [epoch: 7.79 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002373023385705588		[learning rate: 4.6567e-05]
	Learning Rate: 4.65669e-05
	LOSS [training: 0.002373023385705588 | validation: 0.003551199116082329]
	TIME [epoch: 7.76 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002320776771986792		[learning rate: 4.6402e-05]
	Learning Rate: 4.64022e-05
	LOSS [training: 0.002320776771986792 | validation: 0.003756944116253946]
	TIME [epoch: 7.73 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023795715694457055		[learning rate: 4.6238e-05]
	Learning Rate: 4.62381e-05
	LOSS [training: 0.0023795715694457055 | validation: 0.003756410991689907]
	TIME [epoch: 7.73 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002512280059829137		[learning rate: 4.6075e-05]
	Learning Rate: 4.60746e-05
	LOSS [training: 0.002512280059829137 | validation: 0.0035315746850433983]
	TIME [epoch: 7.73 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023996706753596493		[learning rate: 4.5912e-05]
	Learning Rate: 4.59117e-05
	LOSS [training: 0.0023996706753596493 | validation: 0.0035789845810150706]
	TIME [epoch: 7.8 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002586832460318143		[learning rate: 4.5749e-05]
	Learning Rate: 4.57493e-05
	LOSS [training: 0.002586832460318143 | validation: 0.0030722699593011904]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20240627_143649/states/model_phi1_1a_v_mmd1_1571.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 12381.348 seconds.
