Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2654302710

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.146193302754783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.146193302754783 | validation: 6.088354822713109]
	TIME [epoch: 45.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5564277552739725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5564277552739725 | validation: 6.03700901718944]
	TIME [epoch: 0.965 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.597721236588929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.597721236588929 | validation: 6.805440307095154]
	TIME [epoch: 0.945 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.367761574039573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.367761574039573 | validation: 5.832887940986772]
	TIME [epoch: 0.947 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.374056980088185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.374056980088185 | validation: 5.420756312778622]
	TIME [epoch: 0.949 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.790847565519246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.790847565519246 | validation: 5.428664531474621]
	TIME [epoch: 0.953 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.413464363804202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.413464363804202 | validation: 5.570983292658321]
	TIME [epoch: 0.946 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.954182481703482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.954182481703482 | validation: 5.627803853419142]
	TIME [epoch: 0.946 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6790965721298416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6790965721298416 | validation: 5.732099181747]
	TIME [epoch: 0.952 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.462490692015044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.462490692015044 | validation: 5.482537505799015]
	TIME [epoch: 0.959 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5215609258247516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5215609258247516 | validation: 5.518565613091693]
	TIME [epoch: 0.946 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.737471008363367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.737471008363367 | validation: 5.270943240331125]
	TIME [epoch: 0.95 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4197628175685684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4197628175685684 | validation: 5.240020558917225]
	TIME [epoch: 0.949 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2752651912102464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2752651912102464 | validation: 5.216820968670195]
	TIME [epoch: 0.951 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2887205044437877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2887205044437877 | validation: 5.059570230369589]
	TIME [epoch: 0.949 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1829787754309025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1829787754309025 | validation: 4.876398425659133]
	TIME [epoch: 0.948 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.013260167863724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.013260167863724 | validation: 4.706058842586208]
	TIME [epoch: 0.943 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.921996271604155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.921996271604155 | validation: 4.308974242441489]
	TIME [epoch: 0.946 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7794533810772406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7794533810772406 | validation: 3.56818506792556]
	TIME [epoch: 0.954 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.413070891093216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.413070891093216 | validation: 1.8386127756794082]
	TIME [epoch: 0.942 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8148064452905623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8148064452905623 | validation: 1.8071234726871144]
	TIME [epoch: 0.944 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.346321005714523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.346321005714523 | validation: 2.285693014171535]
	TIME [epoch: 0.943 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.380835043116483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.380835043116483 | validation: 1.4869928889955362]
	TIME [epoch: 0.943 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5368981923186877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5368981923186877 | validation: 1.487318568151185]
	TIME [epoch: 0.947 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.720435813724681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.720435813724681 | validation: 1.577154581668391]
	TIME [epoch: 0.943 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5163160551003168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5163160551003168 | validation: 1.4559590571663374]
	TIME [epoch: 0.942 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3778537227334438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3778537227334438 | validation: 1.0575133931630816]
	TIME [epoch: 0.942 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6616674911785208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6616674911785208 | validation: 1.5797203971757654]
	TIME [epoch: 0.942 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3734945556934737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3734945556934737 | validation: 1.18491119230001]
	TIME [epoch: 0.938 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2765941968591437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2765941968591437 | validation: 0.9946527249794266]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3172628116398477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3172628116398477 | validation: 1.2091583452831973]
	TIME [epoch: 0.944 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2089391805529486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2089391805529486 | validation: 0.9627770552539225]
	TIME [epoch: 0.944 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1542067699682828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1542067699682828 | validation: 0.9788671128624511]
	TIME [epoch: 0.94 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1321398865210115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1321398865210115 | validation: 1.1320221311982241]
	TIME [epoch: 0.94 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.132670312496402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.132670312496402 | validation: 0.9424037192086803]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1701879026766207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1701879026766207 | validation: 1.4524617599157965]
	TIME [epoch: 0.948 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3167170552108933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3167170552108933 | validation: 0.9609199976987374]
	TIME [epoch: 0.948 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.174294993540357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.174294993540357 | validation: 1.1700493208681202]
	TIME [epoch: 0.944 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1402377284917704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1402377284917704 | validation: 0.8416167122628851]
	TIME [epoch: 0.946 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0766547209800763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0766547209800763 | validation: 1.162159930560184]
	TIME [epoch: 0.948 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0793443476608622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0793443476608622 | validation: 0.7928355948866724]
	TIME [epoch: 0.956 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0694975168139267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0694975168139267 | validation: 1.3264684283423256]
	TIME [epoch: 0.944 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102647379241894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.102647379241894 | validation: 0.7934037093597457]
	TIME [epoch: 0.947 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0474473681882739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0474473681882739 | validation: 1.2239017994702466]
	TIME [epoch: 0.951 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0264390232130407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0264390232130407 | validation: 0.7542573278164523]
	TIME [epoch: 0.949 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.995778599634708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.995778599634708 | validation: 1.1638308080146655]
	TIME [epoch: 0.943 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9770637768455928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9770637768455928 | validation: 0.7673167899829103]
	TIME [epoch: 0.946 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9849939919926777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9849939919926777 | validation: 1.340348830880255]
	TIME [epoch: 0.942 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0423766982277993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0423766982277993 | validation: 0.7521053917687474]
	TIME [epoch: 0.944 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.241947142888667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.241947142888667 | validation: 1.1451413993487056]
	TIME [epoch: 0.944 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9463576927355111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9463576927355111 | validation: 0.8247701616757578]
	TIME [epoch: 0.943 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9181186330268506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9181186330268506 | validation: 0.9711481032740018]
	TIME [epoch: 0.941 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9541536269314717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9541536269314717 | validation: 1.066383404317596]
	TIME [epoch: 0.942 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0754957652584338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0754957652584338 | validation: 0.989348569828628]
	TIME [epoch: 0.946 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.937530906851032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.937530906851032 | validation: 0.8473972211846554]
	TIME [epoch: 0.942 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9154911556477313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9154911556477313 | validation: 0.996685573574176]
	TIME [epoch: 0.94 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9036013007086876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9036013007086876 | validation: 0.694824168606785]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9775073993102047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9775073993102047 | validation: 1.2400317747851544]
	TIME [epoch: 0.95 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0394687582813555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0394687582813555 | validation: 0.6616310103081835]
	TIME [epoch: 0.948 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.089708092624984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.089708092624984 | validation: 0.8841224077885873]
	TIME [epoch: 0.948 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8857101764822417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8857101764822417 | validation: 1.0872059284986002]
	TIME [epoch: 0.947 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9999730578265869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9999730578265869 | validation: 0.6963450790346819]
	TIME [epoch: 0.946 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.038438978925931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.038438978925931 | validation: 0.7217535250421975]
	TIME [epoch: 0.946 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8799498455089036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8799498455089036 | validation: 1.1538248361389583]
	TIME [epoch: 0.946 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9990916178201655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9990916178201655 | validation: 0.6481184495059908]
	TIME [epoch: 0.949 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9911382691667272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9911382691667272 | validation: 0.7184711816491851]
	TIME [epoch: 0.949 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8700898982974286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8700898982974286 | validation: 1.028729612069349]
	TIME [epoch: 0.947 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9329050197629312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9329050197629312 | validation: 0.6681025752323481]
	TIME [epoch: 0.95 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9202956229884275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9202956229884275 | validation: 0.7367175732916603]
	TIME [epoch: 0.946 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8624865395086987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8624865395086987 | validation: 1.0713270021825945]
	TIME [epoch: 0.948 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.925545527814625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.925545527814625 | validation: 0.7498626832839165]
	TIME [epoch: 0.948 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9473562504590618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9473562504590618 | validation: 1.089839774910579]
	TIME [epoch: 0.95 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0046693040433254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0046693040433254 | validation: 0.8988509952947269]
	TIME [epoch: 0.951 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9277870189882294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9277870189882294 | validation: 0.7922720761189611]
	TIME [epoch: 0.95 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9119218033015125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9119218033015125 | validation: 0.784906181958001]
	TIME [epoch: 0.951 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604059927528809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8604059927528809 | validation: 0.8503773384690817]
	TIME [epoch: 0.95 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8477874534883378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8477874534883378 | validation: 0.6976045583892114]
	TIME [epoch: 0.948 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8641778010566975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8641778010566975 | validation: 0.965935643985294]
	TIME [epoch: 0.949 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8811642451760523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8811642451760523 | validation: 0.6687268060285705]
	TIME [epoch: 0.948 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.897361194483075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.897361194483075 | validation: 1.035665819395908]
	TIME [epoch: 0.949 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9234646059244216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9234646059244216 | validation: 0.6956574646942661]
	TIME [epoch: 0.948 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9133529807714611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9133529807714611 | validation: 0.8971851959513663]
	TIME [epoch: 0.95 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9071721473853416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9071721473853416 | validation: 0.9117130265300825]
	TIME [epoch: 0.947 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9507518455257821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9507518455257821 | validation: 0.7335286680843853]
	TIME [epoch: 0.95 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0790382397319078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0790382397319078 | validation: 0.780478027447968]
	TIME [epoch: 0.961 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8510869474584045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8510869474584045 | validation: 0.8659039354442909]
	TIME [epoch: 0.947 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8622201345806112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8622201345806112 | validation: 0.7038154038280987]
	TIME [epoch: 0.947 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9090050138871086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9090050138871086 | validation: 0.834911287938753]
	TIME [epoch: 0.953 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8918428988177636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8918428988177636 | validation: 0.7740496530545111]
	TIME [epoch: 0.952 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8968532138487956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8968532138487956 | validation: 0.7751299500309551]
	TIME [epoch: 0.947 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8780041167140885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8780041167140885 | validation: 0.8590886630659811]
	TIME [epoch: 0.947 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8897849213889174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8897849213889174 | validation: 0.7368040110297687]
	TIME [epoch: 0.955 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9080725648418531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9080725648418531 | validation: 1.1072378208707563]
	TIME [epoch: 0.948 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9723291380304468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9723291380304468 | validation: 0.6476932560457769]
	TIME [epoch: 0.946 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8786163676127597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8786163676127597 | validation: 0.8267019572238978]
	TIME [epoch: 0.947 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8464073308401617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8464073308401617 | validation: 0.6242983342015829]
	TIME [epoch: 0.947 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.886348148614779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.886348148614779 | validation: 0.943615966937583]
	TIME [epoch: 0.949 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9259972593094955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9259972593094955 | validation: 0.6406302770608735]
	TIME [epoch: 0.951 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0032604521140807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0032604521140807 | validation: 0.7819076073145199]
	TIME [epoch: 0.954 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8485884938424201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8485884938424201 | validation: 0.8404711118091669]
	TIME [epoch: 0.946 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8446997860147244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8446997860147244 | validation: 0.6819106966100638]
	TIME [epoch: 0.945 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.876394230280174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.876394230280174 | validation: 0.969177620101]
	TIME [epoch: 0.947 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9087097414808034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9087097414808034 | validation: 0.6961977338226762]
	TIME [epoch: 0.946 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9087840271008613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9087840271008613 | validation: 0.8685537380197075]
	TIME [epoch: 0.948 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.907051062715095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.907051062715095 | validation: 0.8272100852198963]
	TIME [epoch: 0.947 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9109152249423955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9109152249423955 | validation: 0.6484163263274758]
	TIME [epoch: 0.945 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9769238987023718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9769238987023718 | validation: 0.825754479324422]
	TIME [epoch: 0.946 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8614359441162321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8614359441162321 | validation: 0.6895652205660548]
	TIME [epoch: 0.944 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8370978990473757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8370978990473757 | validation: 0.6989110773856033]
	TIME [epoch: 0.945 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8275909768015975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8275909768015975 | validation: 0.7688412593449736]
	TIME [epoch: 0.947 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8345209070809538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8345209070809538 | validation: 0.6372696799743478]
	TIME [epoch: 0.959 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8628389225607944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8628389225607944 | validation: 1.0514480575240115]
	TIME [epoch: 0.946 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9436983158105312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9436983158105312 | validation: 0.6218274613163415]
	TIME [epoch: 0.944 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9394070905130832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9394070905130832 | validation: 0.9650414737383071]
	TIME [epoch: 0.947 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.902535625324901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.902535625324901 | validation: 0.6057445133976177]
	TIME [epoch: 0.944 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8404593806649094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8404593806649094 | validation: 0.7212610344047474]
	TIME [epoch: 0.945 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8177926141661567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8177926141661567 | validation: 0.7432103354368178]
	TIME [epoch: 0.945 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8271114523610738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8271114523610738 | validation: 0.6523226707839407]
	TIME [epoch: 0.944 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9450134998702795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9450134998702795 | validation: 1.0356973783295105]
	TIME [epoch: 0.944 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0993484583820547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0993484583820547 | validation: 0.7941598490176389]
	TIME [epoch: 0.943 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.053051816343401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.053051816343401 | validation: 0.660598273161291]
	TIME [epoch: 0.945 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8302367795784659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8302367795784659 | validation: 0.8874109704849138]
	TIME [epoch: 0.944 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9150406946778241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9150406946778241 | validation: 0.673985166803349]
	TIME [epoch: 0.943 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9405774869030313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9405774869030313 | validation: 0.6985720947666438]
	TIME [epoch: 0.945 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8255512573386028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8255512573386028 | validation: 0.8445039672360782]
	TIME [epoch: 0.945 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499769316989145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8499769316989145 | validation: 0.5860760789161785]
	TIME [epoch: 0.944 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8850310204429181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8850310204429181 | validation: 0.839947695540249]
	TIME [epoch: 0.948 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8474657782414834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8474657782414834 | validation: 0.6765595642601618]
	TIME [epoch: 0.949 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8387873443029807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8387873443029807 | validation: 0.859477504408871]
	TIME [epoch: 0.959 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8883088365576143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8883088365576143 | validation: 0.7552500678109562]
	TIME [epoch: 0.946 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9173184169524395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9173184169524395 | validation: 0.9595572953864526]
	TIME [epoch: 0.946 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9580438690809326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9580438690809326 | validation: 0.7255395847305799]
	TIME [epoch: 0.948 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8413060799823288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8413060799823288 | validation: 0.6212218435080842]
	TIME [epoch: 0.959 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8301216340266201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8301216340266201 | validation: 0.9043136856124996]
	TIME [epoch: 0.946 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8686171710661543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8686171710661543 | validation: 0.5814513651905255]
	TIME [epoch: 0.945 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8837901355243909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8837901355243909 | validation: 0.8187599077874932]
	TIME [epoch: 0.951 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8382692920677681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8382692920677681 | validation: 0.632702405723821]
	TIME [epoch: 0.947 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8188482359366569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8188482359366569 | validation: 0.719287815210252]
	TIME [epoch: 0.947 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8190959602890581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8190959602890581 | validation: 0.7258224288921955]
	TIME [epoch: 0.949 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8590889142459586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8590889142459586 | validation: 0.8938716930382861]
	TIME [epoch: 0.946 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.00635465291728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.00635465291728 | validation: 0.9910919447983922]
	TIME [epoch: 0.946 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1091342809485303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1091342809485303 | validation: 0.649552446593255]
	TIME [epoch: 0.946 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8221023515649099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8221023515649099 | validation: 0.7790943411996176]
	TIME [epoch: 0.952 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8617540084810665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8617540084810665 | validation: 0.7526837263282682]
	TIME [epoch: 0.946 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9387086907380225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9387086907380225 | validation: 0.7365383029940903]
	TIME [epoch: 0.946 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8432974859687482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8432974859687482 | validation: 0.6613053725842929]
	TIME [epoch: 0.947 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8173611070491018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8173611070491018 | validation: 0.7893020206320494]
	TIME [epoch: 0.944 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8228455406596941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8228455406596941 | validation: 0.5921891401036946]
	TIME [epoch: 0.948 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8514906114340647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8514906114340647 | validation: 1.032809264652119]
	TIME [epoch: 0.947 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.945975553946267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.945975553946267 | validation: 0.6203804388121917]
	TIME [epoch: 0.948 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9236541707542938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9236541707542938 | validation: 0.807913185645378]
	TIME [epoch: 0.946 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.847054476682701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.847054476682701 | validation: 0.6396290491719626]
	TIME [epoch: 0.948 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8110567256057652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8110567256057652 | validation: 0.630877270114967]
	TIME [epoch: 0.946 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8094010986748628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8094010986748628 | validation: 0.7851065305829105]
	TIME [epoch: 0.946 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.84807283129324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.84807283129324 | validation: 0.6495952289529328]
	TIME [epoch: 0.946 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9554398749805759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9554398749805759 | validation: 0.8589112939279118]
	TIME [epoch: 0.948 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.956881149766479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.956881149766479 | validation: 0.9335351512709601]
	TIME [epoch: 0.948 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9828932475667712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9828932475667712 | validation: 0.6504154067461239]
	TIME [epoch: 0.948 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8304931474158345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8304931474158345 | validation: 0.7868534405564034]
	TIME [epoch: 0.95 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8248162072428786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8248162072428786 | validation: 0.6201473184471858]
	TIME [epoch: 0.948 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8283845181436935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8283845181436935 | validation: 0.7749939372978842]
	TIME [epoch: 0.946 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8218847841339901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8218847841339901 | validation: 0.6191306279623174]
	TIME [epoch: 0.946 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8200425717598457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8200425717598457 | validation: 0.7356875527310408]
	TIME [epoch: 0.947 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8389350132106174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8389350132106174 | validation: 0.7248025789139803]
	TIME [epoch: 0.948 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8896563593133797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8896563593133797 | validation: 0.8051583074170466]
	TIME [epoch: 0.948 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9312048325073896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9312048325073896 | validation: 1.0336731444180884]
	TIME [epoch: 0.948 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9999458339449828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9999458339449828 | validation: 0.6259046465227563]
	TIME [epoch: 0.953 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8126169228436609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8126169228436609 | validation: 0.6317129243434891]
	TIME [epoch: 0.947 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8108971896969839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8108971896969839 | validation: 0.7658613471956516]
	TIME [epoch: 0.947 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8290819556046347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8290819556046347 | validation: 0.6777516500134975]
	TIME [epoch: 0.948 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8491937677334883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8491937677334883 | validation: 0.8819746788221409]
	TIME [epoch: 0.951 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.901693973865992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.901693973865992 | validation: 0.7252046429334191]
	TIME [epoch: 0.954 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8422481130847053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8422481130847053 | validation: 0.6698099687858066]
	TIME [epoch: 0.978 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8286471019157327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8286471019157327 | validation: 0.7875694030143885]
	TIME [epoch: 0.949 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8513270889210819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8513270889210819 | validation: 0.5780721635082261]
	TIME [epoch: 0.954 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9238495816731066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9238495816731066 | validation: 0.8534734704724697]
	TIME [epoch: 0.963 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8656073613574465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8656073613574465 | validation: 0.547067058999304]
	TIME [epoch: 0.95 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467886262885028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8467886262885028 | validation: 0.8144166222351239]
	TIME [epoch: 0.955 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.832211696658411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.832211696658411 | validation: 0.6017697409852664]
	TIME [epoch: 0.955 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8269056301161339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8269056301161339 | validation: 0.872308173853221]
	TIME [epoch: 0.948 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8605668597604961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8605668597604961 | validation: 0.7227819410430062]
	TIME [epoch: 0.949 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.884343067047127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.884343067047127 | validation: 0.8891310285502112]
	TIME [epoch: 0.953 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.971039378157979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.971039378157979 | validation: 0.731454629187446]
	TIME [epoch: 0.945 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8444073297739467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8444073297739467 | validation: 0.5975483964034781]
	TIME [epoch: 0.946 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8355716492471512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8355716492471512 | validation: 0.7980458323296205]
	TIME [epoch: 0.974 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8363442356877409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8363442356877409 | validation: 0.5793810587783182]
	TIME [epoch: 0.944 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8297030974456152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8297030974456152 | validation: 0.7712346263651925]
	TIME [epoch: 0.946 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8188700384055634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8188700384055634 | validation: 0.6196127504621792]
	TIME [epoch: 0.945 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.812978429967875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.812978429967875 | validation: 0.702173916524528]
	TIME [epoch: 0.945 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8023981916652976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8023981916652976 | validation: 0.6356957187951437]
	TIME [epoch: 0.954 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.811247178603274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.811247178603274 | validation: 0.8404307061392999]
	TIME [epoch: 0.949 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9221280163246734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9221280163246734 | validation: 1.013964032307943]
	TIME [epoch: 0.945 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1104736984112382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1104736984112382 | validation: 0.6229799977496198]
	TIME [epoch: 0.946 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8072446410259813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8072446410259813 | validation: 0.8238098588903366]
	TIME [epoch: 0.945 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8465737061989382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8465737061989382 | validation: 0.5430560243606655]
	TIME [epoch: 0.945 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8677365148666558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8677365148666558 | validation: 0.7558719852410731]
	TIME [epoch: 0.944 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8216472301254811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8216472301254811 | validation: 0.6125420384270379]
	TIME [epoch: 0.944 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7989899753961913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7989899753961913 | validation: 0.6801171439454121]
	TIME [epoch: 0.941 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7867092602109449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7867092602109449 | validation: 0.6628412471361761]
	TIME [epoch: 0.941 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7810648858967935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7810648858967935 | validation: 0.6292781191811776]
	TIME [epoch: 0.942 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.783958057957303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.783958057957303 | validation: 0.7577265675901037]
	TIME [epoch: 45.1 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672016118944228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8672016118944228 | validation: 0.9948421338934264]
	TIME [epoch: 1.87 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1056957049196543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1056957049196543 | validation: 0.672358604026922]
	TIME [epoch: 1.85 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565842012930948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8565842012930948 | validation: 0.9000638154447871]
	TIME [epoch: 1.85 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8545161013650173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8545161013650173 | validation: 0.5693664695465035]
	TIME [epoch: 1.86 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8247902663198218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8247902663198218 | validation: 0.7701591464838302]
	TIME [epoch: 1.86 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8171109916006608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8171109916006608 | validation: 0.5983074219019093]
	TIME [epoch: 1.85 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7976196692083679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7976196692083679 | validation: 0.6741083999664903]
	TIME [epoch: 1.85 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7791635948843504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7791635948843504 | validation: 0.7069009710855254]
	TIME [epoch: 1.85 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202803021326426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8202803021326426 | validation: 0.7362279154179636]
	TIME [epoch: 1.85 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9031056521526506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9031056521526506 | validation: 0.9836830523145553]
	TIME [epoch: 1.85 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0247898202186325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0247898202186325 | validation: 0.6830305715604131]
	TIME [epoch: 1.85 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.805683282857589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.805683282857589 | validation: 0.5834228928939321]
	TIME [epoch: 1.85 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.799389043686622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.799389043686622 | validation: 0.8153520798530863]
	TIME [epoch: 1.85 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8139249481516437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8139249481516437 | validation: 0.5549916250344396]
	TIME [epoch: 1.85 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8317015433639882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8317015433639882 | validation: 0.7913898401301496]
	TIME [epoch: 1.85 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8193850671717856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8193850671717856 | validation: 0.5766287027794009]
	TIME [epoch: 1.86 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8020276883542756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8020276883542756 | validation: 0.7427332107621253]
	TIME [epoch: 1.86 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8009782145286163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8009782145286163 | validation: 0.5873597521385729]
	TIME [epoch: 1.86 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7928491481111881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7928491481111881 | validation: 0.6761396889237516]
	TIME [epoch: 1.87 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841292287425428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7841292287425428 | validation: 0.6271754731269947]
	TIME [epoch: 1.86 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.834728151949577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.834728151949577 | validation: 1.0077128159939088]
	TIME [epoch: 1.86 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.139017292972207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.139017292972207 | validation: 0.981956629174457]
	TIME [epoch: 1.86 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1039039654764016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1039039654764016 | validation: 0.6148395865426166]
	TIME [epoch: 1.86 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7833217386904057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7833217386904057 | validation: 0.8349694113439914]
	TIME [epoch: 1.86 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9669625620066984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9669625620066984 | validation: 0.8580798058254647]
	TIME [epoch: 1.86 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9486313329496128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9486313329496128 | validation: 0.5783258218882257]
	TIME [epoch: 1.86 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.787819469590131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.787819469590131 | validation: 0.7258770350125094]
	TIME [epoch: 1.85 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8234148987774372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8234148987774372 | validation: 0.7086901472912296]
	TIME [epoch: 1.85 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8323341310063307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8323341310063307 | validation: 0.6237441230196692]
	TIME [epoch: 1.86 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7937069078818124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7937069078818124 | validation: 0.6880514531645668]
	TIME [epoch: 1.85 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7781605712301906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7781605712301906 | validation: 0.5838114040808804]
	TIME [epoch: 1.85 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.778583158489146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.778583158489146 | validation: 0.6736809395528405]
	TIME [epoch: 1.86 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7771632249648219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7771632249648219 | validation: 0.5836243572233681]
	TIME [epoch: 1.86 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794164963246206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7794164963246206 | validation: 0.9189993655616571]
	TIME [epoch: 1.86 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8636824690176488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8636824690176488 | validation: 0.6487567576071707]
	TIME [epoch: 1.86 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8906714448104583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8906714448104583 | validation: 0.8975728149571425]
	TIME [epoch: 1.86 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8721448149609063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8721448149609063 | validation: 0.6530251331565778]
	TIME [epoch: 1.85 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8040230438024335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8040230438024335 | validation: 0.5652374904151201]
	TIME [epoch: 1.86 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9015623052415634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9015623052415634 | validation: 0.7876895545058997]
	TIME [epoch: 1.85 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8808797745960664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8808797745960664 | validation: 0.7049825785657273]
	TIME [epoch: 1.85 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8701165647885476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8701165647885476 | validation: 0.6389894602778727]
	TIME [epoch: 1.86 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8088758049158695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8088758049158695 | validation: 0.7630187284869723]
	TIME [epoch: 1.86 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8096470000515507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8096470000515507 | validation: 0.5983529906115985]
	TIME [epoch: 1.86 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784495195828045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.784495195828045 | validation: 0.6808894609131588]
	TIME [epoch: 1.87 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7824024212954184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7824024212954184 | validation: 0.6254824552902711]
	TIME [epoch: 1.86 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7920183604409988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7920183604409988 | validation: 0.7044123349843129]
	TIME [epoch: 1.86 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8533486469271502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8533486469271502 | validation: 0.7834636930084202]
	TIME [epoch: 1.86 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9091004625102472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9091004625102472 | validation: 0.7495441088951882]
	TIME [epoch: 1.85 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9289651532296958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9289651532296958 | validation: 0.6243914806526135]
	TIME [epoch: 1.86 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7726300571074498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7726300571074498 | validation: 0.6807077003724648]
	TIME [epoch: 1.85 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7658385929380782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7658385929380782 | validation: 0.5833505452763138]
	TIME [epoch: 1.86 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.777200541229458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.777200541229458 | validation: 0.7421687468055191]
	TIME [epoch: 1.85 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7909875938223297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7909875938223297 | validation: 0.5530400392270612]
	TIME [epoch: 1.86 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8076144808742883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8076144808742883 | validation: 0.7959062918802884]
	TIME [epoch: 1.85 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.816825423718316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.816825423718316 | validation: 0.5400506801239222]
	TIME [epoch: 1.86 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069585036726332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8069585036726332 | validation: 0.8146378077645973]
	TIME [epoch: 1.86 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8062641490204351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8062641490204351 | validation: 0.6876287140594699]
	TIME [epoch: 1.85 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8238847452513212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8238847452513212 | validation: 0.8972426182204896]
	TIME [epoch: 1.86 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0192372544413926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0192372544413926 | validation: 0.7708748933160694]
	TIME [epoch: 1.85 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8985385511991123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8985385511991123 | validation: 0.6815960536260944]
	TIME [epoch: 1.85 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8405416596055308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8405416596055308 | validation: 0.5866758232254095]
	TIME [epoch: 1.85 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7809610908058864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7809610908058864 | validation: 0.661486074559211]
	TIME [epoch: 1.86 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7666621622712344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7666621622712344 | validation: 0.5951243073369021]
	TIME [epoch: 1.85 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619181759679478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7619181759679478 | validation: 0.6434229628043078]
	TIME [epoch: 1.86 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7542384112407368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7542384112407368 | validation: 0.5667211039796748]
	TIME [epoch: 1.85 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7529168918518172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7529168918518172 | validation: 0.6907577254545149]
	TIME [epoch: 1.86 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7693150227337414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7693150227337414 | validation: 0.6517173068699252]
	TIME [epoch: 1.86 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8179952583245836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8179952583245836 | validation: 0.9679827385648653]
	TIME [epoch: 1.86 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0247598630376924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0247598630376924 | validation: 0.7893493316101767]
	TIME [epoch: 1.86 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9218979740572395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9218979740572395 | validation: 0.6032274214474903]
	TIME [epoch: 1.86 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8488572468670688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8488572468670688 | validation: 0.6560716203401044]
	TIME [epoch: 1.85 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7607775164992953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7607775164992953 | validation: 0.6411162785354476]
	TIME [epoch: 1.86 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7539857292509491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7539857292509491 | validation: 0.600950810848304]
	TIME [epoch: 1.85 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7556879165179978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7556879165179978 | validation: 0.6425424126019115]
	TIME [epoch: 1.85 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7566292509328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7566292509328 | validation: 0.6503338109766023]
	TIME [epoch: 1.85 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757769754476687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7757769754476687 | validation: 0.65686800382154]
	TIME [epoch: 1.86 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.828098398001078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.828098398001078 | validation: 0.9913670077685321]
	TIME [epoch: 1.85 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0156241496954996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0156241496954996 | validation: 0.6596777282851387]
	TIME [epoch: 1.85 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8370706559308245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8370706559308245 | validation: 0.706751157247683]
	TIME [epoch: 1.85 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7892981992982036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7892981992982036 | validation: 0.5686960132895521]
	TIME [epoch: 1.85 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7704435171275591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7704435171275591 | validation: 0.7161915948071775]
	TIME [epoch: 1.85 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7689411087097149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7689411087097149 | validation: 0.5802083864141748]
	TIME [epoch: 1.85 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7661094467329856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7661094467329856 | validation: 0.6958855629341884]
	TIME [epoch: 1.86 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7581268633172671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7581268633172671 | validation: 0.5683638396744746]
	TIME [epoch: 1.85 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7735634075719119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7735634075719119 | validation: 0.8010811544589833]
	TIME [epoch: 1.85 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8160034095895139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8160034095895139 | validation: 0.7289753475986651]
	TIME [epoch: 1.85 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8638671456224796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8638671456224796 | validation: 0.7638546565027761]
	TIME [epoch: 1.85 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9864062462607891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9864062462607891 | validation: 0.6587763257135495]
	TIME [epoch: 1.85 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7654266815415661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7654266815415661 | validation: 0.5959510808635837]
	TIME [epoch: 1.85 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7425555424361062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7425555424361062 | validation: 0.6048481743289474]
	TIME [epoch: 1.85 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.737990744219533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.737990744219533 | validation: 0.6406742243968221]
	TIME [epoch: 1.85 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7502053845708375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7502053845708375 | validation: 0.6722909897596354]
	TIME [epoch: 1.86 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8028573155211006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8028573155211006 | validation: 0.7240297223429755]
	TIME [epoch: 1.86 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8862018517153301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8862018517153301 | validation: 0.9425807808515106]
	TIME [epoch: 1.87 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9320892125251703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9320892125251703 | validation: 0.6027456084317722]
	TIME [epoch: 1.85 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7601601610018136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7601601610018136 | validation: 0.6256328523967162]
	TIME [epoch: 1.86 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7394120552349517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7394120552349517 | validation: 0.5607903341630845]
	TIME [epoch: 1.86 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7280797360242813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7280797360242813 | validation: 0.6251831749545099]
	TIME [epoch: 1.85 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7317293574891442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7317293574891442 | validation: 0.5885363858353011]
	TIME [epoch: 1.85 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576891688890975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7576891688890975 | validation: 0.7043508648082044]
	TIME [epoch: 1.86 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8246509910665587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8246509910665587 | validation: 0.7675634471768853]
	TIME [epoch: 1.85 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9181216147300917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9181216147300917 | validation: 0.6975330528798624]
	TIME [epoch: 1.85 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8280212749614617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8280212749614617 | validation: 0.8800286557450164]
	TIME [epoch: 1.85 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320270406493256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8320270406493256 | validation: 0.551154789324712]
	TIME [epoch: 1.85 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547136400068561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7547136400068561 | validation: 0.6116884509388428]
	TIME [epoch: 1.85 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7240978122082116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7240978122082116 | validation: 0.6600077003778497]
	TIME [epoch: 1.85 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299749874029999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7299749874029999 | validation: 0.5977378740221456]
	TIME [epoch: 1.85 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729055142700677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.729055142700677 | validation: 0.690719652904297]
	TIME [epoch: 1.85 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7516801616823331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7516801616823331 | validation: 0.7093100049753772]
	TIME [epoch: 1.85 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8605192564486057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8605192564486057 | validation: 0.8861422719719219]
	TIME [epoch: 1.85 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9892127069760271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9892127069760271 | validation: 0.6029769554775375]
	TIME [epoch: 1.85 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7499526611876627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7499526611876627 | validation: 0.5614551451612997]
	TIME [epoch: 1.85 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7119316308398667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7119316308398667 | validation: 0.6149735555973099]
	TIME [epoch: 1.86 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7285394592752937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7285394592752937 | validation: 0.6275964348026952]
	TIME [epoch: 1.85 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299632619875847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7299632619875847 | validation: 0.6468790622152246]
	TIME [epoch: 1.85 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7534899090332803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7534899090332803 | validation: 0.6769208595398851]
	TIME [epoch: 1.85 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015740271877405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8015740271877405 | validation: 0.837810418733206]
	TIME [epoch: 1.86 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8737026752240152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8737026752240152 | validation: 0.6119005637209369]
	TIME [epoch: 1.85 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7659319176842155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7659319176842155 | validation: 0.6552667856171764]
	TIME [epoch: 1.86 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7363661132017546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7363661132017546 | validation: 0.6145458832235569]
	TIME [epoch: 1.85 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7125507710679096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7125507710679096 | validation: 0.6235070814865828]
	TIME [epoch: 1.85 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7087712612817699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7087712612817699 | validation: 0.61794426502586]
	TIME [epoch: 1.85 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7233406401571663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7233406401571663 | validation: 0.6371235224540543]
	TIME [epoch: 1.85 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7548939387150034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7548939387150034 | validation: 0.6850567449633014]
	TIME [epoch: 1.85 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8172807337957047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8172807337957047 | validation: 0.7855999397768301]
	TIME [epoch: 1.85 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8474332345343667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8474332345343667 | validation: 0.5950768747951865]
	TIME [epoch: 1.85 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.73353322490541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.73353322490541 | validation: 0.6452405755731407]
	TIME [epoch: 1.85 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699665179497166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.699665179497166 | validation: 0.5521212699905943]
	TIME [epoch: 1.85 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022795397004424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7022795397004424 | validation: 0.6196115414170785]
	TIME [epoch: 1.85 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6894465576100516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6894465576100516 | validation: 0.5520669540360673]
	TIME [epoch: 1.85 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6868876009968148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6868876009968148 | validation: 0.6121649222785808]
	TIME [epoch: 1.85 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7061965483828935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7061965483828935 | validation: 0.6433315890318352]
	TIME [epoch: 1.85 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485493620049536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7485493620049536 | validation: 0.7143881825083095]
	TIME [epoch: 1.85 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8750554632640186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8750554632640186 | validation: 0.8817180984733162]
	TIME [epoch: 1.85 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8832325113293011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8832325113293011 | validation: 0.5171248794409394]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.698211725393802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.698211725393802 | validation: 0.6416508539158632]
	TIME [epoch: 1.85 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7092431143078443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7092431143078443 | validation: 0.5556209196706087]
	TIME [epoch: 1.85 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6958710520139915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6958710520139915 | validation: 0.5827155291814727]
	TIME [epoch: 1.85 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6903883319584873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6903883319584873 | validation: 0.5531869691196601]
	TIME [epoch: 1.85 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.674052239266035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.674052239266035 | validation: 0.626269130103334]
	TIME [epoch: 1.85 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6912258311398095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6912258311398095 | validation: 0.5914041297650514]
	TIME [epoch: 1.85 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7240441376272031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7240441376272031 | validation: 0.7623451576593014]
	TIME [epoch: 1.85 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8341183367270578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8341183367270578 | validation: 0.6552168947780047]
	TIME [epoch: 1.86 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8010439757954968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8010439757954968 | validation: 0.6893193344984504]
	TIME [epoch: 1.85 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7334661632509487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7334661632509487 | validation: 0.5647001044320642]
	TIME [epoch: 1.85 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6898238772848544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6898238772848544 | validation: 0.5533678769039571]
	TIME [epoch: 1.85 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6639632760317541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6639632760317541 | validation: 0.5839136946817198]
	TIME [epoch: 1.85 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.662171970314183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.662171970314183 | validation: 0.5160161041782424]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6752913262226707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6752913262226707 | validation: 0.6065339176842612]
	TIME [epoch: 1.85 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6759642430395759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6759642430395759 | validation: 0.6113635624523521]
	TIME [epoch: 1.85 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6982244713991591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6982244713991591 | validation: 0.606304718343458]
	TIME [epoch: 1.84 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7068548204720719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7068548204720719 | validation: 0.5713780921007504]
	TIME [epoch: 1.85 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7265526122811405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7265526122811405 | validation: 0.7373454106215428]
	TIME [epoch: 1.84 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7954374618404942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7954374618404942 | validation: 0.5548401768341326]
	TIME [epoch: 1.85 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619817071891484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7619817071891484 | validation: 0.6937604593441259]
	TIME [epoch: 1.84 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.726548235762754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.726548235762754 | validation: 0.5137791829616456]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6526336152537593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6526336152537593 | validation: 0.6157917834370075]
	TIME [epoch: 1.85 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6694690219041439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6694690219041439 | validation: 0.5325439644809971]
	TIME [epoch: 1.85 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6746009588386778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6746009588386778 | validation: 0.5602174928329378]
	TIME [epoch: 1.85 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.663161082602451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.663161082602451 | validation: 0.5869546828665518]
	TIME [epoch: 1.85 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.646939487851299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.646939487851299 | validation: 0.5144694921457659]
	TIME [epoch: 1.85 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.647902933158154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.647902933158154 | validation: 0.6026145962120192]
	TIME [epoch: 1.85 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6496479854268082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6496479854268082 | validation: 0.5247075066740486]
	TIME [epoch: 1.85 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6573238952474111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6573238952474111 | validation: 0.6453067375562017]
	TIME [epoch: 1.85 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6882847908644429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6882847908644429 | validation: 0.6389481053083561]
	TIME [epoch: 1.85 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784108602211775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.784108602211775 | validation: 0.6481850670864909]
	TIME [epoch: 1.85 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7444205021353585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7444205021353585 | validation: 0.5767011880782384]
	TIME [epoch: 1.85 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761334850939494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6761334850939494 | validation: 0.5201402274194201]
	TIME [epoch: 1.86 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6915866734367407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6915866734367407 | validation: 0.5948657550828335]
	TIME [epoch: 1.85 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6722701896479327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6722701896479327 | validation: 0.6058448626271528]
	TIME [epoch: 1.85 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6775746902148749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6775746902148749 | validation: 0.48071709349795616]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524528806538317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6524528806538317 | validation: 0.6009234868133577]
	TIME [epoch: 1.85 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6562731194560432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6562731194560432 | validation: 0.5556556443014388]
	TIME [epoch: 1.85 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.641736828012774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.641736828012774 | validation: 0.5376104201237678]
	TIME [epoch: 1.85 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6260506890967451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6260506890967451 | validation: 0.5224152687228885]
	TIME [epoch: 1.86 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6496782803445617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6496782803445617 | validation: 0.6452649344690322]
	TIME [epoch: 1.85 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579722126865903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6579722126865903 | validation: 0.504476357751272]
	TIME [epoch: 1.85 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6941620049693279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6941620049693279 | validation: 0.5959245915945376]
	TIME [epoch: 1.85 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6386110109298114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6386110109298114 | validation: 0.572864444452985]
	TIME [epoch: 2.21 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6300892004160916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6300892004160916 | validation: 0.47154461290119304]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6698876785655796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6698876785655796 | validation: 0.5957410554227083]
	TIME [epoch: 1.85 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6325298662216359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6325298662216359 | validation: 0.655351032287764]
	TIME [epoch: 1.85 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6887050225974569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6887050225974569 | validation: 0.4782548020703017]
	TIME [epoch: 1.85 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7308285507347535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7308285507347535 | validation: 0.6775123594167325]
	TIME [epoch: 1.85 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7550030642638045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7550030642638045 | validation: 0.5173342914070113]
	TIME [epoch: 1.85 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446622923714398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6446622923714398 | validation: 0.6010849859607573]
	TIME [epoch: 1.85 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6657528587386585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6657528587386585 | validation: 0.4699513839409104]
	TIME [epoch: 1.86 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6234084330313275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6234084330313275 | validation: 0.5515273891315213]
	TIME [epoch: 1.86 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.598020427424645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.598020427424645 | validation: 0.5243021036602661]
	TIME [epoch: 1.85 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5784527562467036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5784527562467036 | validation: 0.42919207213809796]
	TIME [epoch: 1.86 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5963348122136729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5963348122136729 | validation: 0.63569122966272]
	TIME [epoch: 1.87 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6204444439759551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6204444439759551 | validation: 0.563827612922537]
	TIME [epoch: 1.85 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6872395893012754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6872395893012754 | validation: 0.48207897200829425]
	TIME [epoch: 1.85 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6366916704500036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6366916704500036 | validation: 0.5668608682361482]
	TIME [epoch: 1.85 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5928156345950812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5928156345950812 | validation: 0.4466709319276325]
	TIME [epoch: 1.85 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5759640068633822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5759640068633822 | validation: 0.6099599906609484]
	TIME [epoch: 1.85 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6028147286324037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6028147286324037 | validation: 0.501451608203895]
	TIME [epoch: 1.85 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6526890702549027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6526890702549027 | validation: 0.4646971282464282]
	TIME [epoch: 1.85 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5603787122259865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5603787122259865 | validation: 0.6563607742408124]
	TIME [epoch: 1.85 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6379874114175091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6379874114175091 | validation: 0.5242357923813513]
	TIME [epoch: 1.85 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7654301023138217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7654301023138217 | validation: 0.6733257220751016]
	TIME [epoch: 1.85 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759792067455818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.759792067455818 | validation: 0.4813652413150403]
	TIME [epoch: 1.85 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6207057978105216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6207057978105216 | validation: 0.49425542991935245]
	TIME [epoch: 1.85 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5858704986420903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5858704986420903 | validation: 0.48209090389200393]
	TIME [epoch: 1.85 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5353065706061733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5353065706061733 | validation: 0.4676620796204647]
	TIME [epoch: 1.85 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5253560861229106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5253560861229106 | validation: 0.44121748182955356]
	TIME [epoch: 1.85 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.504064606753319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.504064606753319 | validation: 0.4176775209001865]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4973145240276953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4973145240276953 | validation: 0.6114232107001422]
	TIME [epoch: 1.85 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5680671668952816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5680671668952816 | validation: 0.5514695374170906]
	TIME [epoch: 1.85 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8496681098526733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8496681098526733 | validation: 0.7536416411697123]
	TIME [epoch: 1.85 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.85054441225136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.85054441225136 | validation: 0.6284277237894476]
	TIME [epoch: 1.85 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7333663800441897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7333663800441897 | validation: 0.4649806866967747]
	TIME [epoch: 1.85 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6142118734624866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6142118734624866 | validation: 0.519947511892326]
	TIME [epoch: 1.85 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5881627378611329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5881627378611329 | validation: 0.4355246921687518]
	TIME [epoch: 1.85 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5257234435056072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5257234435056072 | validation: 0.5130342226420269]
	TIME [epoch: 1.87 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5044362505242284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5044362505242284 | validation: 0.43062294964828673]
	TIME [epoch: 1.85 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5895429874552398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5895429874552398 | validation: 0.5819116149304712]
	TIME [epoch: 1.85 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5500996572954638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5500996572954638 | validation: 0.48876951157592635]
	TIME [epoch: 1.85 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6247470428550205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6247470428550205 | validation: 0.4241240611434212]
	TIME [epoch: 1.85 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5034579510991533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5034579510991533 | validation: 0.5978168539548451]
	TIME [epoch: 1.85 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5843169982205341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5843169982205341 | validation: 0.49117230035885434]
	TIME [epoch: 1.85 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6908853108349962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6908853108349962 | validation: 0.4450083779463608]
	TIME [epoch: 1.85 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5514028094711727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5514028094711727 | validation: 0.5842531499863247]
	TIME [epoch: 1.85 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5318530858442746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5318530858442746 | validation: 0.48343830251609515]
	TIME [epoch: 1.85 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.57589126890256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.57589126890256 | validation: 0.406888691672155]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_426.pth
	Model improved!!!
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44322144872802133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44322144872802133 | validation: 0.4720877110003878]
	TIME [epoch: 1.84 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4335261618781353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4335261618781353 | validation: 0.3933231505958993]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5198406264360813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5198406264360813 | validation: 0.6553991501405021]
	TIME [epoch: 1.85 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5368410917992026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5368410917992026 | validation: 0.5056571618259329]
	TIME [epoch: 1.85 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6540361201909388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6540361201909388 | validation: 0.3920038529758594]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5454943655202641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5454943655202641 | validation: 0.5028107956166664]
	TIME [epoch: 1.85 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5179645574678422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5179645574678422 | validation: 0.3942743978057511]
	TIME [epoch: 1.85 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4407725943049101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4407725943049101 | validation: 0.42635659056146863]
	TIME [epoch: 1.84 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40237040564027726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40237040564027726 | validation: 0.3684072444080213]
	TIME [epoch: 1.86 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_435.pth
	Model improved!!!
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40329657482884884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40329657482884884 | validation: 0.5926538729641732]
	TIME [epoch: 1.85 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5203662801282368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5203662801282368 | validation: 0.5528510261293024]
	TIME [epoch: 1.84 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7927491661743937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7927491661743937 | validation: 0.5917443824586254]
	TIME [epoch: 1.85 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6946334816134224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6946334816134224 | validation: 0.43272340739562126]
	TIME [epoch: 1.85 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4589291269766636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4589291269766636 | validation: 0.47848380275092606]
	TIME [epoch: 1.86 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4754819262818084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4754819262818084 | validation: 0.41088515136487863]
	TIME [epoch: 1.85 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.433429088614796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.433429088614796 | validation: 0.47505870002315587]
	TIME [epoch: 1.84 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4318702094541325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4318702094541325 | validation: 0.34662457041530603]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43414791234290606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43414791234290606 | validation: 0.5843173265953653]
	TIME [epoch: 1.85 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42731509831421405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42731509831421405 | validation: 0.39532679172827867]
	TIME [epoch: 1.85 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5779939875183341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5779939875183341 | validation: 0.37157196629795974]
	TIME [epoch: 1.85 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38828082810025183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38828082810025183 | validation: 0.6003526258453297]
	TIME [epoch: 1.85 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49779034447104026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49779034447104026 | validation: 0.5448132802235394]
	TIME [epoch: 1.85 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7438039803597005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7438039803597005 | validation: 0.42141456112785786]
	TIME [epoch: 1.85 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5592635035700542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5592635035700542 | validation: 0.48217297324317043]
	TIME [epoch: 1.85 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45553260327803147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45553260327803147 | validation: 0.3839688896110845]
	TIME [epoch: 1.85 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37017164574217815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37017164574217815 | validation: 0.3645793282453069]
	TIME [epoch: 1.85 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34401396894397307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34401396894397307 | validation: 0.5357566594632355]
	TIME [epoch: 1.85 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4142223799328488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4142223799328488 | validation: 0.42931641894810046]
	TIME [epoch: 1.85 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6146340348547336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6146340348547336 | validation: 0.3749947456150306]
	TIME [epoch: 1.85 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3974225653516482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3974225653516482 | validation: 0.6535623297063907]
	TIME [epoch: 1.85 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6110858599763352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6110858599763352 | validation: 0.37909455998101205]
	TIME [epoch: 1.85 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47067299218868497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47067299218868497 | validation: 0.4022560279021586]
	TIME [epoch: 1.85 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3698589594172376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3698589594172376 | validation: 0.457676453706259]
	TIME [epoch: 1.85 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42509830949597766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42509830949597766 | validation: 0.4076595278586777]
	TIME [epoch: 1.85 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43771790880244255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43771790880244255 | validation: 0.3691335873158817]
	TIME [epoch: 1.85 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32374751354752607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32374751354752607 | validation: 0.3546706524545657]
	TIME [epoch: 1.85 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3042502659890884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3042502659890884 | validation: 0.35641581162529623]
	TIME [epoch: 1.85 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37164539009266767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37164539009266767 | validation: 0.6653258438458374]
	TIME [epoch: 1.86 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5410242753146683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5410242753146683 | validation: 0.4010132380700471]
	TIME [epoch: 1.87 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6189558794659693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6189558794659693 | validation: 0.4355903259305276]
	TIME [epoch: 1.86 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45546593137295915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45546593137295915 | validation: 0.4918669716895305]
	TIME [epoch: 1.85 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44180457842774956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44180457842774956 | validation: 0.3236441563832447]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33517316169618566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33517316169618566 | validation: 0.4157593025462733]
	TIME [epoch: 1.85 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3044545774588578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3044545774588578 | validation: 0.31086617224323754]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2835314967491712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2835314967491712 | validation: 0.35587779471357667]
	TIME [epoch: 1.85 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25868956317112174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25868956317112174 | validation: 0.320076432268354]
	TIME [epoch: 1.85 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2877990388611358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2877990388611358 | validation: 0.6854651476956113]
	TIME [epoch: 1.85 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5848734595372134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5848734595372134 | validation: 0.43902915672339565]
	TIME [epoch: 1.85 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.696620144995707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.696620144995707 | validation: 0.4321191777398288]
	TIME [epoch: 1.85 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5091695492043917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5091695492043917 | validation: 0.5670449680392453]
	TIME [epoch: 1.85 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5217400660906933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5217400660906933 | validation: 0.3588531487278128]
	TIME [epoch: 1.85 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33987016554422683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33987016554422683 | validation: 0.3450510735420266]
	TIME [epoch: 1.85 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2961688148782885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2961688148782885 | validation: 0.3845955253599847]
	TIME [epoch: 1.85 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2770478749111694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2770478749111694 | validation: 0.3116036073819186]
	TIME [epoch: 1.85 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32314054284118643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32314054284118643 | validation: 0.5952297814658893]
	TIME [epoch: 1.85 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4149627542608497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4149627542608497 | validation: 0.3478680028032554]
	TIME [epoch: 1.85 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4003384951391574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4003384951391574 | validation: 0.3243461342516683]
	TIME [epoch: 1.85 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27028209061107333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27028209061107333 | validation: 0.3092403918095754]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24363595643943156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24363595643943156 | validation: 0.3276525914111625]
	TIME [epoch: 1.85 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24991284614636358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24991284614636358 | validation: 0.3114954828758837]
	TIME [epoch: 1.85 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43056579410505025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43056579410505025 | validation: 0.6007882042325385]
	TIME [epoch: 1.85 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4361364674824438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4361364674824438 | validation: 0.3415168127386452]
	TIME [epoch: 1.87 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37362504471653396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37362504471653396 | validation: 0.31164064490173093]
	TIME [epoch: 1.85 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23470124109363827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23470124109363827 | validation: 0.2789823124593296]
	TIME [epoch: 1.86 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21084549016881468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21084549016881468 | validation: 0.24953412797102192]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20808200109512243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20808200109512243 | validation: 0.3307512701277447]
	TIME [epoch: 1.84 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23726858746252302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23726858746252302 | validation: 0.4093721037943301]
	TIME [epoch: 1.85 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5007596913171287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5007596913171287 | validation: 0.5306278067326232]
	TIME [epoch: 1.85 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32990450620529777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32990450620529777 | validation: 0.2905008534159341]
	TIME [epoch: 1.84 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41489921499455185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41489921499455185 | validation: 0.3465129546858375]
	TIME [epoch: 1.85 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24971040283985235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24971040283985235 | validation: 0.3168806213211204]
	TIME [epoch: 1.85 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27334069124095167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27334069124095167 | validation: 0.48464798566446315]
	TIME [epoch: 1.84 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40394099949443063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40394099949443063 | validation: 0.28872408669776334]
	TIME [epoch: 1.84 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40413382951636684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40413382951636684 | validation: 0.3285413935984118]
	TIME [epoch: 1.84 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2061191600943107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2061191600943107 | validation: 0.2652362884772865]
	TIME [epoch: 47.1 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18988469243301864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18988469243301864 | validation: 0.23529450834797175]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19172867208586597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19172867208586597 | validation: 0.40731093436334]
	TIME [epoch: 3.66 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3190839813950394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3190839813950394 | validation: 0.3293913607876059]
	TIME [epoch: 3.66 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.503338031116237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.503338031116237 | validation: 0.23883928438837554]
	TIME [epoch: 3.66 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2064666086263371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2064666086263371 | validation: 0.3163877501526578]
	TIME [epoch: 3.66 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2035661478884198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2035661478884198 | validation: 0.3009153633321231]
	TIME [epoch: 3.65 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.375596741051746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.375596741051746 | validation: 0.4109398957314248]
	TIME [epoch: 3.65 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2975569887777885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2975569887777885 | validation: 0.2473002577055371]
	TIME [epoch: 3.65 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28366349698463206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28366349698463206 | validation: 0.3235137747317604]
	TIME [epoch: 3.66 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23565175029483051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23565175029483051 | validation: 0.26997164881466407]
	TIME [epoch: 3.67 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2395871964570976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2395871964570976 | validation: 0.4309899986783652]
	TIME [epoch: 3.66 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3063012579835117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3063012579835117 | validation: 0.28113999692631414]
	TIME [epoch: 3.66 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28523857810090697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28523857810090697 | validation: 0.4979309159768695]
	TIME [epoch: 3.65 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33737607392934243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33737607392934243 | validation: 0.27218028845351416]
	TIME [epoch: 3.66 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2991968799692643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2991968799692643 | validation: 0.31871469953571846]
	TIME [epoch: 3.65 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2076632443028568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2076632443028568 | validation: 0.24157608558279656]
	TIME [epoch: 3.66 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16848244049657715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16848244049657715 | validation: 0.21285143061995573]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15748955631686598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15748955631686598 | validation: 0.32750871301486095]
	TIME [epoch: 3.65 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24595189694420513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24595189694420513 | validation: 0.31461333104798517]
	TIME [epoch: 3.66 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45600793116165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45600793116165 | validation: 0.341747686294353]
	TIME [epoch: 3.66 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19749870960493499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19749870960493499 | validation: 0.18091089135660826]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_522.pth
	Model improved!!!
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1620340245867127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1620340245867127 | validation: 0.3043738225020525]
	TIME [epoch: 3.65 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18971649527477677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18971649527477677 | validation: 0.21800807752684578]
	TIME [epoch: 3.68 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35832082124268705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35832082124268705 | validation: 0.2611198013383565]
	TIME [epoch: 3.66 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18124140038249337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18124140038249337 | validation: 0.2471926357513378]
	TIME [epoch: 3.65 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26824532303093807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26824532303093807 | validation: 0.4654619061453136]
	TIME [epoch: 3.65 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34570337420261255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34570337420261255 | validation: 0.25829140340592993]
	TIME [epoch: 3.65 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3259665217967251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3259665217967251 | validation: 0.21815783952135376]
	TIME [epoch: 3.64 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14756513742196595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14756513742196595 | validation: 0.24898943479082739]
	TIME [epoch: 3.65 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16494036492487682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16494036492487682 | validation: 0.21214992161159907]
	TIME [epoch: 3.64 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1887757237312701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1887757237312701 | validation: 0.3611596628465995]
	TIME [epoch: 3.64 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24286802348804584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24286802348804584 | validation: 0.2482297710997462]
	TIME [epoch: 3.63 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34373493840538066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34373493840538066 | validation: 0.4872334112208867]
	TIME [epoch: 3.64 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3119626609687667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3119626609687667 | validation: 0.36267834775198327]
	TIME [epoch: 3.64 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29048288737385386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29048288737385386 | validation: 0.1918869782309891]
	TIME [epoch: 3.64 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19331184768205006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19331184768205006 | validation: 0.19652408398880222]
	TIME [epoch: 3.65 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1435456849623302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1435456849623302 | validation: 0.2289180646753534]
	TIME [epoch: 3.66 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14702717802859494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14702717802859494 | validation: 0.19001252455982148]
	TIME [epoch: 3.65 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24400399528830174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24400399528830174 | validation: 0.49607437043058483]
	TIME [epoch: 3.64 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35732444799575974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35732444799575974 | validation: 0.27610873130814134]
	TIME [epoch: 3.64 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29375198820235093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29375198820235093 | validation: 0.21822705084422]
	TIME [epoch: 3.64 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1534243123101702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1534243123101702 | validation: 0.19656712253584308]
	TIME [epoch: 3.64 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11629345437244877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11629345437244877 | validation: 0.15663740461524608]
	TIME [epoch: 3.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10813476265375187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10813476265375187 | validation: 0.17588873739945587]
	TIME [epoch: 3.66 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10385858244778495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10385858244778495 | validation: 0.16757146187637223]
	TIME [epoch: 3.65 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15601290734632953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15601290734632953 | validation: 0.6783711029941979]
	TIME [epoch: 3.65 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5658609244786552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5658609244786552 | validation: 0.33268076004934627]
	TIME [epoch: 3.65 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43508925708000706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43508925708000706 | validation: 0.22630927554852676]
	TIME [epoch: 3.65 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14434515608244702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14434515608244702 | validation: 0.4142721787965541]
	TIME [epoch: 3.65 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.323272755239217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.323272755239217 | validation: 0.2679322253890008]
	TIME [epoch: 3.66 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29812484333869416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29812484333869416 | validation: 0.2666629989070963]
	TIME [epoch: 3.68 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1753354061962025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1753354061962025 | validation: 0.23207936465704027]
	TIME [epoch: 3.66 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15157573198207167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15157573198207167 | validation: 0.20989664604452898]
	TIME [epoch: 3.66 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1370860257506249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1370860257506249 | validation: 0.22654073163335783]
	TIME [epoch: 3.66 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16618086273849023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16618086273849023 | validation: 0.20890310121983974]
	TIME [epoch: 3.66 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16466801558369834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16466801558369834 | validation: 0.32114827834017023]
	TIME [epoch: 3.66 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24711469302694447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24711469302694447 | validation: 0.2271614834522736]
	TIME [epoch: 3.65 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22529661994687647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22529661994687647 | validation: 0.3241463710552779]
	TIME [epoch: 3.66 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2121329625347768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2121329625347768 | validation: 0.1980115330751027]
	TIME [epoch: 3.65 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12145715292185863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12145715292185863 | validation: 0.162259261633701]
	TIME [epoch: 3.66 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09662179846495797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09662179846495797 | validation: 0.17241273899749776]
	TIME [epoch: 3.65 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0915970130829216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0915970130829216 | validation: 0.1412495795287866]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09427607165182017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09427607165182017 | validation: 0.20115620117970334]
	TIME [epoch: 3.66 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18684955670499265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18684955670499265 | validation: 0.7073940799228811]
	TIME [epoch: 3.67 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6384973846569044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6384973846569044 | validation: 0.2334752868662644]
	TIME [epoch: 3.67 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2537652598258949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2537652598258949 | validation: 0.32127384105125056]
	TIME [epoch: 3.66 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2262617683763249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2262617683763249 | validation: 0.408656959145167]
	TIME [epoch: 3.65 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2892424134760798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2892424134760798 | validation: 0.19985075217973816]
	TIME [epoch: 3.66 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15485878553962204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15485878553962204 | validation: 0.20616656154380847]
	TIME [epoch: 3.66 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12205264163272625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12205264163272625 | validation: 0.15591608916497035]
	TIME [epoch: 3.65 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09206015355285445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09206015355285445 | validation: 0.14695014578848484]
	TIME [epoch: 3.65 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08610746171282312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08610746171282312 | validation: 0.147486872819146]
	TIME [epoch: 3.65 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08638638166643603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08638638166643603 | validation: 0.14055908443316473]
	TIME [epoch: 3.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_574.pth
	Model improved!!!
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09118600437862252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09118600437862252 | validation: 0.16601844249269646]
	TIME [epoch: 3.64 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13092867586720996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13092867586720996 | validation: 0.564352964737674]
	TIME [epoch: 3.64 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4958006645867664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4958006645867664 | validation: 0.2370021630546762]
	TIME [epoch: 3.64 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4390496997892961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4390496997892961 | validation: 0.20321017916591153]
	TIME [epoch: 3.66 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12850329568695387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12850329568695387 | validation: 0.2508936314552686]
	TIME [epoch: 3.66 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2179902306011309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2179902306011309 | validation: 0.16055338911186887]
	TIME [epoch: 3.66 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13454886980704647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13454886980704647 | validation: 0.21133830300881484]
	TIME [epoch: 3.66 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1491316309631342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1491316309631342 | validation: 0.2589906898491791]
	TIME [epoch: 3.66 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23276553404936834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23276553404936834 | validation: 0.3884339216083917]
	TIME [epoch: 3.66 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2909075474848418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2909075474848418 | validation: 0.18451784466411358]
	TIME [epoch: 3.66 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15203317504387046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15203317504387046 | validation: 0.2121879149556567]
	TIME [epoch: 3.66 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15492874021334108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15492874021334108 | validation: 0.16813263882451743]
	TIME [epoch: 3.66 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1404843980274149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1404843980274149 | validation: 0.21043745689788218]
	TIME [epoch: 3.66 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13438740383240863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13438740383240863 | validation: 0.17052664399689046]
	TIME [epoch: 3.65 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1351850828635234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1351850828635234 | validation: 0.2591076685703121]
	TIME [epoch: 3.65 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.174118746897908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.174118746897908 | validation: 0.1625805019954128]
	TIME [epoch: 3.65 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19682499390614447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19682499390614447 | validation: 0.23523026882741258]
	TIME [epoch: 3.66 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1568116716830884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1568116716830884 | validation: 0.14563546689722395]
	TIME [epoch: 3.67 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11268131822142019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11268131822142019 | validation: 0.17462656537519206]
	TIME [epoch: 3.67 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10885022213549023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10885022213549023 | validation: 0.15428388412145277]
	TIME [epoch: 3.66 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12250487731990306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12250487731990306 | validation: 0.2808844226373271]
	TIME [epoch: 3.66 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2090163253783595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2090163253783595 | validation: 0.18663784831785302]
	TIME [epoch: 3.66 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21332647195895596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21332647195895596 | validation: 0.2106185970661293]
	TIME [epoch: 3.66 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14774346363204716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14774346363204716 | validation: 0.3221152646717327]
	TIME [epoch: 3.66 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21481390174403353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21481390174403353 | validation: 0.4323371340711166]
	TIME [epoch: 3.66 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32421898794317133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32421898794317133 | validation: 0.171028806837213]
	TIME [epoch: 3.66 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14119000797838088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14119000797838088 | validation: 0.2234770094064789]
	TIME [epoch: 3.66 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17541980140860036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17541980140860036 | validation: 0.18780031259596786]
	TIME [epoch: 3.65 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1976736333465793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1976736333465793 | validation: 0.2746364306984328]
	TIME [epoch: 3.66 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20251185318566478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20251185318566478 | validation: 0.14974987639124435]
	TIME [epoch: 3.66 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12506613003398231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12506613003398231 | validation: 0.15564806716024693]
	TIME [epoch: 3.67 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08453820245871993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08453820245871993 | validation: 0.12982668069528622]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07258402404079402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07258402404079402 | validation: 0.11459972657652445]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_607.pth
	Model improved!!!
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07610667558339179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07610667558339179 | validation: 0.1524700528545003]
	TIME [epoch: 3.65 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09810151159732577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09810151159732577 | validation: 0.24514314938817]
	TIME [epoch: 3.65 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17438021799161318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17438021799161318 | validation: 0.2721181132908571]
	TIME [epoch: 3.65 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2665930550593281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2665930550593281 | validation: 0.5480874588014598]
	TIME [epoch: 3.65 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47157635385537006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47157635385537006 | validation: 0.16018616567350483]
	TIME [epoch: 3.65 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1369982672823783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1369982672823783 | validation: 0.25344863575013354]
	TIME [epoch: 3.65 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16597372892596224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16597372892596224 | validation: 0.18482384187571396]
	TIME [epoch: 3.65 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14114059238462776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14114059238462776 | validation: 0.1450814639335267]
	TIME [epoch: 3.65 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12229661880522004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12229661880522004 | validation: 0.2630718010966798]
	TIME [epoch: 3.66 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20132017072624422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20132017072624422 | validation: 0.20585559389668762]
	TIME [epoch: 3.65 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29772630493802416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29772630493802416 | validation: 0.1719624518616489]
	TIME [epoch: 3.66 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09550413784307779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09550413784307779 | validation: 0.12956216795097017]
	TIME [epoch: 3.66 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07249877835065567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07249877835065567 | validation: 0.12562005674151058]
	TIME [epoch: 3.67 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09858317323878467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09858317323878467 | validation: 0.21835755965912826]
	TIME [epoch: 3.66 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1505053490323394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1505053490323394 | validation: 0.16255736993180558]
	TIME [epoch: 3.65 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14407169773089495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14407169773089495 | validation: 0.21099938509725852]
	TIME [epoch: 3.66 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1408290459429832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1408290459429832 | validation: 0.13534292702995576]
	TIME [epoch: 3.65 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11455699109722949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11455699109722949 | validation: 0.15581277644995623]
	TIME [epoch: 3.63 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10558014660098966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10558014660098966 | validation: 0.14022335375373748]
	TIME [epoch: 3.65 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1070239497117856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1070239497117856 | validation: 0.28836482716754425]
	TIME [epoch: 3.65 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21844652845222404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21844652845222404 | validation: 0.20009257492053703]
	TIME [epoch: 3.63 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1486479530008543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1486479530008543 | validation: 0.28250849807734696]
	TIME [epoch: 3.65 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20965726565078113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20965726565078113 | validation: 0.28060275874938656]
	TIME [epoch: 3.66 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18088060420864435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18088060420864435 | validation: 0.170262048185559]
	TIME [epoch: 3.66 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13596582338700125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13596582338700125 | validation: 0.18141530331172404]
	TIME [epoch: 3.67 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12164838669165198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12164838669165198 | validation: 0.15749089082791592]
	TIME [epoch: 3.65 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20180241056046708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20180241056046708 | validation: 0.23825062590157564]
	TIME [epoch: 3.64 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1639074012996159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1639074012996159 | validation: 0.1274958818760088]
	TIME [epoch: 3.64 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12092234803931731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12092234803931731 | validation: 0.16956244398888978]
	TIME [epoch: 3.64 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11310248876064254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11310248876064254 | validation: 0.13066828112986842]
	TIME [epoch: 3.64 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11540902938860241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11540902938860241 | validation: 0.17761771459256037]
	TIME [epoch: 3.64 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12406245422853161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12406245422853161 | validation: 0.12476309480232489]
	TIME [epoch: 3.65 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10401080100871486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10401080100871486 | validation: 0.13905627355048808]
	TIME [epoch: 3.66 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09720572981735685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09720572981735685 | validation: 0.12979767152355492]
	TIME [epoch: 3.64 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10803046612048062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10803046612048062 | validation: 0.22739267260666784]
	TIME [epoch: 3.66 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17255165171306117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17255165171306117 | validation: 0.181462117604441]
	TIME [epoch: 3.66 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1850462796354134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1850462796354134 | validation: 0.2533039499389582]
	TIME [epoch: 3.66 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13992109124004337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13992109124004337 | validation: 0.2416778170834201]
	TIME [epoch: 3.65 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1792773071389823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1792773071389823 | validation: 0.269961496891629]
	TIME [epoch: 3.66 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22678276758258903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22678276758258903 | validation: 0.21533223820216643]
	TIME [epoch: 3.67 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20315930019444736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20315930019444736 | validation: 0.11396950536379857]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08443654785002995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08443654785002995 | validation: 0.18794612924167675]
	TIME [epoch: 3.65 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10831157027813255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10831157027813255 | validation: 0.12383395136396734]
	TIME [epoch: 3.66 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11640246024794187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11640246024794187 | validation: 0.17508012408537774]
	TIME [epoch: 3.66 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12202841139078338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12202841139078338 | validation: 0.14494559710013558]
	TIME [epoch: 3.65 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1295162485245529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1295162485245529 | validation: 0.20297072958259418]
	TIME [epoch: 3.65 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13979366837931226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13979366837931226 | validation: 0.13740531001943723]
	TIME [epoch: 3.64 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09756807456414017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09756807456414017 | validation: 0.13731700257238452]
	TIME [epoch: 3.65 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08976281521906325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08976281521906325 | validation: 0.14217947287896568]
	TIME [epoch: 3.64 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10627228362592185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10627228362592185 | validation: 0.26762360964905985]
	TIME [epoch: 3.65 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18335741849367998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18335741849367998 | validation: 0.2533634450344599]
	TIME [epoch: 3.64 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24080487087056618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24080487087056618 | validation: 0.19714201677063287]
	TIME [epoch: 3.66 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16792825006957693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16792825006957693 | validation: 0.14563979183179593]
	TIME [epoch: 3.65 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08853749343441576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08853749343441576 | validation: 0.08707684501036216]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05472743354262888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05472743354262888 | validation: 0.09724806378525608]
	TIME [epoch: 3.66 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05370721123727573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05370721123727573 | validation: 0.10401129343149838]
	TIME [epoch: 3.66 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09099397715856337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09099397715856337 | validation: 0.40442496288807883]
	TIME [epoch: 3.66 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3201482399603685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3201482399603685 | validation: 0.25463671312192393]
	TIME [epoch: 3.66 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30307370104796977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30307370104796977 | validation: 0.2362862085992862]
	TIME [epoch: 3.66 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1320442216730435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1320442216730435 | validation: 0.19310915005823234]
	TIME [epoch: 3.66 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14712069840802816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14712069840802816 | validation: 0.13398248070250118]
	TIME [epoch: 3.66 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10553061664968524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10553061664968524 | validation: 0.14131594356505595]
	TIME [epoch: 3.66 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09003535919111158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09003535919111158 | validation: 0.09785172453240114]
	TIME [epoch: 3.66 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07054535670930358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07054535670930358 | validation: 0.11101678277475512]
	TIME [epoch: 3.66 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06214074370665914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06214074370665914 | validation: 0.10075763400911497]
	TIME [epoch: 3.67 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07787653213785858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07787653213785858 | validation: 0.22499406156343174]
	TIME [epoch: 3.67 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1642967056591685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1642967056591685 | validation: 0.14467056686669527]
	TIME [epoch: 3.68 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1584536169230843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1584536169230843 | validation: 0.21667700226914777]
	TIME [epoch: 3.66 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16572493942801508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16572493942801508 | validation: 0.14108867943226056]
	TIME [epoch: 3.66 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07718141410492789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07718141410492789 | validation: 0.11187568834244908]
	TIME [epoch: 3.66 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08085406506580478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08085406506580478 | validation: 0.17938327326571876]
	TIME [epoch: 3.66 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11418624613663796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11418624613663796 | validation: 0.10644974079765256]
	TIME [epoch: 3.67 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10861011180222431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10861011180222431 | validation: 0.2170511758539373]
	TIME [epoch: 3.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14946678989525272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14946678989525272 | validation: 0.1486931052517015]
	TIME [epoch: 3.66 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1466113065610799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1466113065610799 | validation: 0.21274403106602557]
	TIME [epoch: 3.66 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15389259329214922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15389259329214922 | validation: 0.15642216398360098]
	TIME [epoch: 3.67 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1050354421641648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1050354421641648 | validation: 0.12756164152580748]
	TIME [epoch: 3.66 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09820781921426808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09820781921426808 | validation: 0.1432781930909852]
	TIME [epoch: 3.67 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09388971336885214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09388971336885214 | validation: 0.20475073350284237]
	TIME [epoch: 3.67 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13901356050819855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13901356050819855 | validation: 0.23254991805858127]
	TIME [epoch: 3.69 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21752359724897288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21752359724897288 | validation: 0.30236681587323555]
	TIME [epoch: 3.66 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2384804108637659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2384804108637659 | validation: 0.11048523151562258]
	TIME [epoch: 3.66 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09464219402619808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09464219402619808 | validation: 0.12103941793290063]
	TIME [epoch: 3.67 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07223649020090399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07223649020090399 | validation: 0.13110683741809953]
	TIME [epoch: 3.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10066515672043048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10066515672043048 | validation: 0.13273105175150388]
	TIME [epoch: 3.66 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13419787004502923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13419787004502923 | validation: 0.23173758457575078]
	TIME [epoch: 3.66 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16301354359185638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16301354359185638 | validation: 0.1403986457656872]
	TIME [epoch: 3.66 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09495572938753152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09495572938753152 | validation: 0.12094653768597785]
	TIME [epoch: 3.66 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07951720355406172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07951720355406172 | validation: 0.12909986074300514]
	TIME [epoch: 3.65 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07613551501437252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07613551501437252 | validation: 0.12839288178860397]
	TIME [epoch: 3.64 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09841931774273055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09841931774273055 | validation: 0.15522291748208886]
	TIME [epoch: 3.65 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10353070301709322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10353070301709322 | validation: 0.14331058008478145]
	TIME [epoch: 3.66 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13029401205317517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13029401205317517 | validation: 0.14797266415788052]
	TIME [epoch: 3.67 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15553885766800932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15553885766800932 | validation: 0.34232157792004503]
	TIME [epoch: 3.65 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24029389823005926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24029389823005926 | validation: 0.17276786511943507]
	TIME [epoch: 3.63 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17708254058097395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17708254058097395 | validation: 0.1901192370473191]
	TIME [epoch: 3.63 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1176430816578563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1176430816578563 | validation: 0.11713494661811169]
	TIME [epoch: 3.63 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0733517068678757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0733517068678757 | validation: 0.10236513144310366]
	TIME [epoch: 3.63 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056614156298623104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056614156298623104 | validation: 0.11013479120905374]
	TIME [epoch: 3.63 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07423399039381338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07423399039381338 | validation: 0.13965530574961435]
	TIME [epoch: 3.63 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1528762113722712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1528762113722712 | validation: 0.2512614788023146]
	TIME [epoch: 3.63 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.193620643545119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.193620643545119 | validation: 0.11964663172097717]
	TIME [epoch: 3.63 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08191259895852789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08191259895852789 | validation: 0.11070728656041912]
	TIME [epoch: 3.64 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055833068809542176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055833068809542176 | validation: 0.08044853117150062]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_711.pth
	Model improved!!!
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05142647422129938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05142647422129938 | validation: 0.11196514508676891]
	TIME [epoch: 3.65 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07413098113344677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07413098113344677 | validation: 0.24665242027289921]
	TIME [epoch: 3.66 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18688021913798347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18688021913798347 | validation: 0.264323364301838]
	TIME [epoch: 3.66 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19329705429995092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19329705429995092 | validation: 0.21671865846225838]
	TIME [epoch: 3.66 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15366803982615665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15366803982615665 | validation: 0.09327810780192305]
	TIME [epoch: 3.66 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050668113124847286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050668113124847286 | validation: 0.07391660766394967]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_717.pth
	Model improved!!!
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04238022797632835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04238022797632835 | validation: 0.10015084891694329]
	TIME [epoch: 3.66 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0865114480115616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0865114480115616 | validation: 0.445665668084423]
	TIME [epoch: 3.66 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34348608178041773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34348608178041773 | validation: 0.21368177167842065]
	TIME [epoch: 3.66 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25301302462639813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25301302462639813 | validation: 0.1386321921705644]
	TIME [epoch: 3.65 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07838691261562612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07838691261562612 | validation: 0.10874003537661195]
	TIME [epoch: 3.65 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07028437854691187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07028437854691187 | validation: 0.10522774731019685]
	TIME [epoch: 3.65 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08158451233153935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08158451233153935 | validation: 0.12893791762923498]
	TIME [epoch: 3.64 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07413364553479718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07413364553479718 | validation: 0.09391527518440217]
	TIME [epoch: 3.64 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05839794854089872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05839794854089872 | validation: 0.10260210034067906]
	TIME [epoch: 3.64 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0672121286540503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0672121286540503 | validation: 0.12640993988063612]
	TIME [epoch: 3.65 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10937917467769354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10937917467769354 | validation: 0.28648209124041774]
	TIME [epoch: 3.66 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21630508054122374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21630508054122374 | validation: 0.17719874678337738]
	TIME [epoch: 3.64 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14828329169534296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14828329169534296 | validation: 0.14305732794797496]
	TIME [epoch: 3.65 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09307797356488401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09307797356488401 | validation: 0.13800822159497775]
	TIME [epoch: 3.64 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07876955154972427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07876955154972427 | validation: 0.11967936649745262]
	TIME [epoch: 3.64 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1334485176451556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1334485176451556 | validation: 0.19364200100917695]
	TIME [epoch: 3.64 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1367289407308825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1367289407308825 | validation: 0.1644262292722674]
	TIME [epoch: 3.64 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14229177217316571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14229177217316571 | validation: 0.21723951965783914]
	TIME [epoch: 3.64 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15745158118022345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15745158118022345 | validation: 0.10654592037949347]
	TIME [epoch: 3.64 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1070014246972926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1070014246972926 | validation: 0.09830135371155678]
	TIME [epoch: 3.64 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05566881492111644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05566881492111644 | validation: 0.06719745063504436]
	TIME [epoch: 3.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_738.pth
	Model improved!!!
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04046264206017039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04046264206017039 | validation: 0.07665390411234088]
	TIME [epoch: 3.66 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04442121823043401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04442121823043401 | validation: 0.1031137365973516]
	TIME [epoch: 3.65 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07955573321476077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07955573321476077 | validation: 0.23560078590417516]
	TIME [epoch: 3.65 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18041859938086538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18041859938086538 | validation: 0.19761656751277396]
	TIME [epoch: 3.67 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15466101184600126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15466101184600126 | validation: 0.14175466586619298]
	TIME [epoch: 3.65 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10955643177459151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10955643177459151 | validation: 0.09922702225261718]
	TIME [epoch: 3.65 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0631573344886158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0631573344886158 | validation: 0.0827878157451781]
	TIME [epoch: 3.64 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05586443355885123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05586443355885123 | validation: 0.09552637928524421]
	TIME [epoch: 3.65 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07185883555044012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07185883555044012 | validation: 0.2304277982191998]
	TIME [epoch: 3.64 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17205159085807617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17205159085807617 | validation: 0.2624574626602589]
	TIME [epoch: 3.65 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26516219655147577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26516219655147577 | validation: 0.2569350455184166]
	TIME [epoch: 3.64 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1781987021353016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1781987021353016 | validation: 0.0816227957900928]
	TIME [epoch: 3.65 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06040772640582374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06040772640582374 | validation: 0.08083427251950306]
	TIME [epoch: 3.64 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0506367388965929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0506367388965929 | validation: 0.09064084812635116]
	TIME [epoch: 3.65 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05980474119262654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05980474119262654 | validation: 0.07826913881559353]
	TIME [epoch: 3.65 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06742505853280513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06742505853280513 | validation: 0.13929640507706278]
	TIME [epoch: 3.66 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08774229635395511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08774229635395511 | validation: 0.10096965028583833]
	TIME [epoch: 3.66 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10815536949291506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10815536949291506 | validation: 0.1470249457362341]
	TIME [epoch: 3.66 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10036234266133703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10036234266133703 | validation: 0.08545938626894928]
	TIME [epoch: 3.66 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054534787879275494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054534787879275494 | validation: 0.08177738463975953]
	TIME [epoch: 3.66 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06618624009572027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06618624009572027 | validation: 0.12828012473044895]
	TIME [epoch: 3.65 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08692141192353318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08692141192353318 | validation: 0.19260032099586621]
	TIME [epoch: 3.65 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1278591225670692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1278591225670692 | validation: 0.2469329529216399]
	TIME [epoch: 3.66 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18164448117190093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18164448117190093 | validation: 0.18994063564030414]
	TIME [epoch: 3.68 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18624453833784343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18624453833784343 | validation: 0.11567478886069767]
	TIME [epoch: 3.64 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10401667733759482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10401667733759482 | validation: 0.07377509206715342]
	TIME [epoch: 3.65 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055722159401988984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055722159401988984 | validation: 0.16067970636665696]
	TIME [epoch: 3.65 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1219292114903655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1219292114903655 | validation: 0.17442491847439398]
	TIME [epoch: 3.66 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23057414102578086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23057414102578086 | validation: 0.2330501884642549]
	TIME [epoch: 3.67 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14207068339644832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14207068339644832 | validation: 0.09779624819627386]
	TIME [epoch: 3.66 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05658661219175366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05658661219175366 | validation: 0.08021634131226453]
	TIME [epoch: 3.66 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0479241312031136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0479241312031136 | validation: 0.08377914671241626]
	TIME [epoch: 3.66 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05624160972895152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05624160972895152 | validation: 0.08614421030393836]
	TIME [epoch: 3.65 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06640013932508956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06640013932508956 | validation: 0.15214371422184422]
	TIME [epoch: 3.66 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10375685068049688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10375685068049688 | validation: 0.13022744020866703]
	TIME [epoch: 3.65 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12081514334605661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12081514334605661 | validation: 0.16374552059925504]
	TIME [epoch: 3.65 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11181543910912162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11181543910912162 | validation: 0.11729169697533215]
	TIME [epoch: 3.64 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06894936951261035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06894936951261035 | validation: 0.12553123608315975]
	TIME [epoch: 3.64 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09084566534181292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09084566534181292 | validation: 0.22659309008876535]
	TIME [epoch: 3.65 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16617910737050082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16617910737050082 | validation: 0.18253531226916309]
	TIME [epoch: 3.65 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1894620658063341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1894620658063341 | validation: 0.13896152254789337]
	TIME [epoch: 3.67 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10259337891454352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10259337891454352 | validation: 0.0796069693148524]
	TIME [epoch: 3.66 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06717687555142746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06717687555142746 | validation: 0.1110518736178562]
	TIME [epoch: 3.65 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07966188407956118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07966188407956118 | validation: 0.08538781509249582]
	TIME [epoch: 3.66 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057939799744711994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057939799744711994 | validation: 0.06705949602422125]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_783.pth
	Model improved!!!
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0583300874910021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0583300874910021 | validation: 0.0973760007398414]
	TIME [epoch: 3.65 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0718976855441396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0718976855441396 | validation: 0.16422836865113266]
	TIME [epoch: 3.66 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14868362638367805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14868362638367805 | validation: 0.1078467740056327]
	TIME [epoch: 3.65 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10718785726095316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10718785726095316 | validation: 0.18204788731953064]
	TIME [epoch: 3.66 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11509933270421228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11509933270421228 | validation: 0.100629805327523]
	TIME [epoch: 3.66 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08026228610614958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08026228610614958 | validation: 0.18187349278614487]
	TIME [epoch: 3.65 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12355296571529874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12355296571529874 | validation: 0.18969177200508502]
	TIME [epoch: 3.66 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17283807701758522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17283807701758522 | validation: 0.13843952264804282]
	TIME [epoch: 3.67 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09630150992530016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09630150992530016 | validation: 0.08309411102666117]
	TIME [epoch: 3.66 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06460336939326013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06460336939326013 | validation: 0.11018585827238928]
	TIME [epoch: 3.66 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08327018034886802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08327018034886802 | validation: 0.09319179813511365]
	TIME [epoch: 3.66 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09567642191766407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09567642191766407 | validation: 0.19345929258781633]
	TIME [epoch: 3.66 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14249197854972953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14249197854972953 | validation: 0.08886418549354592]
	TIME [epoch: 3.66 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07057915584445863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07057915584445863 | validation: 0.09294303448254315]
	TIME [epoch: 3.65 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059881041311205696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059881041311205696 | validation: 0.07274245815216089]
	TIME [epoch: 3.65 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060960800105401704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060960800105401704 | validation: 0.09665313360310561]
	TIME [epoch: 3.65 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06642708797991287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06642708797991287 | validation: 0.14153708072078883]
	TIME [epoch: 3.66 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10732209743075725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10732209743075725 | validation: 0.1651855203246124]
	TIME [epoch: 3.64 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259523418844846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1259523418844846 | validation: 0.14411567898864217]
	TIME [epoch: 3.64 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10254201376387163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10254201376387163 | validation: 0.07582377836839044]
	TIME [epoch: 3.64 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06750084339630774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06750084339630774 | validation: 0.21781687937711736]
	TIME [epoch: 3.65 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.158206782681623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.158206782681623 | validation: 0.17310069656319382]
	TIME [epoch: 3.65 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17630582293464067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17630582293464067 | validation: 0.20229484153222788]
	TIME [epoch: 3.65 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14692055884527846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14692055884527846 | validation: 0.08882508572639064]
	TIME [epoch: 3.63 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06496146146761643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06496146146761643 | validation: 0.07701503809565481]
	TIME [epoch: 3.65 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03799533097627054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03799533097627054 | validation: 0.051119790592295815]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_809.pth
	Model improved!!!
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03288444754802541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03288444754802541 | validation: 0.05693243327065831]
	TIME [epoch: 3.65 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04178706664030992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04178706664030992 | validation: 0.09917131539457946]
	TIME [epoch: 3.65 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0718377617414243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0718377617414243 | validation: 0.12547692457998819]
	TIME [epoch: 3.65 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14845313797036613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14845313797036613 | validation: 0.23017892432281362]
	TIME [epoch: 3.65 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1763088672865564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1763088672865564 | validation: 0.09961249922842669]
	TIME [epoch: 3.65 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07437837093721872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07437837093721872 | validation: 0.10629577880370011]
	TIME [epoch: 3.64 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06169426012847093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06169426012847093 | validation: 0.13742766726120875]
	TIME [epoch: 3.65 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10067875630483453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10067875630483453 | validation: 0.17056879523262422]
	TIME [epoch: 3.67 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14590012089189383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14590012089189383 | validation: 0.16816265489249813]
	TIME [epoch: 3.67 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15006077414002955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15006077414002955 | validation: 0.07458046623839798]
	TIME [epoch: 3.64 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046796231739811206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046796231739811206 | validation: 0.04858995089746274]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_820.pth
	Model improved!!!
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024854503257311106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024854503257311106 | validation: 0.03421888496373404]
	TIME [epoch: 3.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_821.pth
	Model improved!!!
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03198248344231836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03198248344231836 | validation: 0.0600205464594728]
	TIME [epoch: 3.64 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04686282534063059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04686282534063059 | validation: 0.11410275619938287]
	TIME [epoch: 3.64 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10455832764214318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10455832764214318 | validation: 0.1184382413941695]
	TIME [epoch: 3.65 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13393935745443283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13393935745443283 | validation: 0.2418132310947955]
	TIME [epoch: 3.65 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16538570880919343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16538570880919343 | validation: 0.19973068582207087]
	TIME [epoch: 3.64 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18346634552014693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18346634552014693 | validation: 0.1774427828544543]
	TIME [epoch: 3.64 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11679755522808337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11679755522808337 | validation: 0.09089616206211518]
	TIME [epoch: 3.65 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0630461264277583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0630461264277583 | validation: 0.0716369764912587]
	TIME [epoch: 3.65 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04459948848646094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04459948848646094 | validation: 0.07311679915741254]
	TIME [epoch: 3.67 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04681087219832107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04681087219832107 | validation: 0.06831648757966369]
	TIME [epoch: 3.66 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058586285575922395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058586285575922395 | validation: 0.16012697035933365]
	TIME [epoch: 3.66 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11956214975049917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11956214975049917 | validation: 0.1267955598391451]
	TIME [epoch: 3.65 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1413505779124515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1413505779124515 | validation: 0.1548906901276367]
	TIME [epoch: 3.64 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09788210679789748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09788210679789748 | validation: 0.11028506857825185]
	TIME [epoch: 3.64 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07929154136508994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07929154136508994 | validation: 0.16709496809950553]
	TIME [epoch: 3.65 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12024285107322513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12024285107322513 | validation: 0.1514217103138895]
	TIME [epoch: 3.64 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12966992700379237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12966992700379237 | validation: 0.08977263969345506]
	TIME [epoch: 3.64 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05514626493331647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05514626493331647 | validation: 0.057221467872060075]
	TIME [epoch: 3.64 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026968603780851384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026968603780851384 | validation: 0.0358682666309841]
	TIME [epoch: 3.65 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021101089510292027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021101089510292027 | validation: 0.03667178237329937]
	TIME [epoch: 3.64 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026128158353879903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026128158353879903 | validation: 0.07154154134370776]
	TIME [epoch: 3.64 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061403021698205065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061403021698205065 | validation: 0.145045112448093]
	TIME [epoch: 3.66 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20418126233534004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20418126233534004 | validation: 0.2907332163088565]
	TIME [epoch: 3.66 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21304814881805015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21304814881805015 | validation: 0.15019091592412095]
	TIME [epoch: 3.65 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1302621281573892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1302621281573892 | validation: 0.19205289172616585]
	TIME [epoch: 3.64 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15401105002161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15401105002161 | validation: 0.12264604261019159]
	TIME [epoch: 3.65 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09682890292282109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09682890292282109 | validation: 0.0690910795126839]
	TIME [epoch: 3.64 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036218515207982326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036218515207982326 | validation: 0.06158062337890194]
	TIME [epoch: 3.65 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036848015446256176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036848015446256176 | validation: 0.054459559309114364]
	TIME [epoch: 3.64 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038125560598546354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038125560598546354 | validation: 0.04461345632131362]
	TIME [epoch: 3.64 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03316280875970033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03316280875970033 | validation: 0.08726782164523626]
	TIME [epoch: 3.64 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054483473164036164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054483473164036164 | validation: 0.08065855602862197]
	TIME [epoch: 3.64 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0894416883354776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0894416883354776 | validation: 0.18260652939472835]
	TIME [epoch: 3.63 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1417455429548307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1417455429548307 | validation: 0.13444967776094582]
	TIME [epoch: 3.64 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12949721391388377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12949721391388377 | validation: 0.17063188507255755]
	TIME [epoch: 3.66 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11516335758725227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11516335758725227 | validation: 0.16312965880883]
	TIME [epoch: 3.65 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12014067019417235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12014067019417235 | validation: 0.16068418523524503]
	TIME [epoch: 3.63 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12286931944793039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12286931944793039 | validation: 0.08662292652477226]
	TIME [epoch: 3.65 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07413770874392574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07413770874392574 | validation: 0.046077112127253995]
	TIME [epoch: 3.64 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02754127197602764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02754127197602764 | validation: 0.032296752655694996]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_861.pth
	Model improved!!!
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02233057880882369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02233057880882369 | validation: 0.034779831840032596]
	TIME [epoch: 3.65 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02620684019850998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02620684019850998 | validation: 0.08064005957012871]
	TIME [epoch: 3.65 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07020740825881902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07020740825881902 | validation: 0.14706334484066563]
	TIME [epoch: 3.65 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19627492121967144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19627492121967144 | validation: 0.22747115250119304]
	TIME [epoch: 3.65 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17124101756086466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17124101756086466 | validation: 0.1325067258891083]
	TIME [epoch: 3.65 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10900591289144482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10900591289144482 | validation: 0.13821422819015947]
	TIME [epoch: 3.66 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10883237054479347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10883237054479347 | validation: 0.10275159864905398]
	TIME [epoch: 3.66 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08940421416223075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08940421416223075 | validation: 0.07634841512726112]
	TIME [epoch: 3.65 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0436055895935892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0436055895935892 | validation: 0.048273273031054775]
	TIME [epoch: 3.64 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029055616664304732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029055616664304732 | validation: 0.04234369648503111]
	TIME [epoch: 3.64 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025730509524258434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025730509524258434 | validation: 0.04834865240466721]
	TIME [epoch: 3.65 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030869200123989717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030869200123989717 | validation: 0.06318036444411727]
	TIME [epoch: 3.64 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0590865487305739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0590865487305739 | validation: 0.1909240986871982]
	TIME [epoch: 3.64 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15603777098021307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15603777098021307 | validation: 0.1382052261743922]
	TIME [epoch: 3.64 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17757139630072702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17757139630072702 | validation: 0.13354751192700234]
	TIME [epoch: 3.64 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08841249744753565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08841249744753565 | validation: 0.12405025204401268]
	TIME [epoch: 3.64 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09251128196276774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09251128196276774 | validation: 0.1915624479647652]
	TIME [epoch: 3.64 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13368637485161805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13368637485161805 | validation: 0.14058303555565685]
	TIME [epoch: 3.64 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12098878804541435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12098878804541435 | validation: 0.07465192620666515]
	TIME [epoch: 3.65 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05314388017108601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05314388017108601 | validation: 0.04588279478729719]
	TIME [epoch: 3.65 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02844518324797881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02844518324797881 | validation: 0.03724501545253002]
	TIME [epoch: 3.65 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03130710873909876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03130710873909876 | validation: 0.07740560530005758]
	TIME [epoch: 3.65 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07599661609812859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07599661609812859 | validation: 0.24480033924314945]
	TIME [epoch: 3.64 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19912519884216195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19912519884216195 | validation: 0.1308653969791677]
	TIME [epoch: 3.65 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10465086608210031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10465086608210031 | validation: 0.1534751000209878]
	TIME [epoch: 3.64 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11660583285047796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11660583285047796 | validation: 0.11999708618734353]
	TIME [epoch: 3.64 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10501870625528242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10501870625528242 | validation: 0.08653923512954625]
	TIME [epoch: 3.64 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05505967116336846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05505967116336846 | validation: 0.04820824677863195]
	TIME [epoch: 3.65 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03285858750706946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03285858750706946 | validation: 0.03452859167370133]
	TIME [epoch: 3.64 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026358189744740607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026358189744740607 | validation: 0.03659853429402521]
	TIME [epoch: 3.64 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02470627409015913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02470627409015913 | validation: 0.03383445151721801]
	TIME [epoch: 3.64 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023120039671935322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023120039671935322 | validation: 0.0288215710714568]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_893.pth
	Model improved!!!
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022978239919742027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022978239919742027 | validation: 0.036228002663309104]
	TIME [epoch: 3.66 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026306257836818014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026306257836818014 | validation: 0.05850136962786276]
	TIME [epoch: 3.66 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04107477627718209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04107477627718209 | validation: 0.14166978882059347]
	TIME [epoch: 3.65 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12070661179871423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12070661179871423 | validation: 0.4095020687194212]
	TIME [epoch: 3.65 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33723401046187784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33723401046187784 | validation: 0.0714186707656284]
	TIME [epoch: 3.65 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05287883435221914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05287883435221914 | validation: 0.06223072741130695]
	TIME [epoch: 3.65 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03383305235196677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03383305235196677 | validation: 0.1024716954804576]
	TIME [epoch: 3.65 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10132598420078288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10132598420078288 | validation: 0.2165736811075803]
	TIME [epoch: 3.65 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17186378596688387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17186378596688387 | validation: 0.1782267477242527]
	TIME [epoch: 3.64 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17086704841433986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17086704841433986 | validation: 0.2502408920523835]
	TIME [epoch: 3.66 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17913766631322783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17913766631322783 | validation: 0.08068845717763101]
	TIME [epoch: 3.66 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07305569599235512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07305569599235512 | validation: 0.03411149051015418]
	TIME [epoch: 3.66 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028688025237228888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028688025237228888 | validation: 0.043250348062771185]
	TIME [epoch: 3.66 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022976178579179118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022976178579179118 | validation: 0.03641222217695378]
	TIME [epoch: 3.68 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02507387579162425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02507387579162425 | validation: 0.038348111243739796]
	TIME [epoch: 3.67 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0360325032524991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0360325032524991 | validation: 0.0708996547734338]
	TIME [epoch: 3.65 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0638093382678303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0638093382678303 | validation: 0.19484264172788857]
	TIME [epoch: 3.66 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15778139054646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15778139054646 | validation: 0.15114285476055864]
	TIME [epoch: 3.65 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14363663046681374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14363663046681374 | validation: 0.13531119359503344]
	TIME [epoch: 3.66 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10640492602245771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10640492602245771 | validation: 0.07578311997723938]
	TIME [epoch: 3.65 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06175637938799826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06175637938799826 | validation: 0.06464397119280989]
	TIME [epoch: 3.66 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039087475852989106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039087475852989106 | validation: 0.059579992466542935]
	TIME [epoch: 3.66 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03628240029073115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03628240029073115 | validation: 0.04809260566414317]
	TIME [epoch: 3.66 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03175422170431423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03175422170431423 | validation: 0.05136748423775866]
	TIME [epoch: 3.68 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03799531363074498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03799531363074498 | validation: 0.07238624387006343]
	TIME [epoch: 3.66 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05839898003801237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05839898003801237 | validation: 0.1409097423285307]
	TIME [epoch: 3.67 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11091999524231923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11091999524231923 | validation: 0.15066133150211422]
	TIME [epoch: 3.68 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15559984939056476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15559984939056476 | validation: 0.18433096226337342]
	TIME [epoch: 3.65 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13549289736110134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13549289736110134 | validation: 0.14266581053830998]
	TIME [epoch: 3.65 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11410123125408667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11410123125408667 | validation: 0.10406463975472922]
	TIME [epoch: 3.66 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09102001321023032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09102001321023032 | validation: 0.07154260199779156]
	TIME [epoch: 3.66 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0527340253081483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0527340253081483 | validation: 0.04230944794060076]
	TIME [epoch: 3.66 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029615490121277743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029615490121277743 | validation: 0.04096234115816322]
	TIME [epoch: 3.66 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027007304230075597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027007304230075597 | validation: 0.05031522466677039]
	TIME [epoch: 3.65 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034455488005457266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034455488005457266 | validation: 0.07672517890438571]
	TIME [epoch: 3.65 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058537919405742135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058537919405742135 | validation: 0.10983313029070202]
	TIME [epoch: 3.66 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09807622421518396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09807622421518396 | validation: 0.2079406845418008]
	TIME [epoch: 3.67 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15642290182247603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15642290182247603 | validation: 0.14151720046774577]
	TIME [epoch: 3.67 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13369494438138416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13369494438138416 | validation: 0.11957606197798741]
	TIME [epoch: 3.67 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10305977888891006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10305977888891006 | validation: 0.08157343122691929]
	TIME [epoch: 3.67 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0710054633211494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0710054633211494 | validation: 0.07499375414898532]
	TIME [epoch: 3.66 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04987918858180624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04987918858180624 | validation: 0.0761981936790937]
	TIME [epoch: 3.66 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04485754032507014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04485754032507014 | validation: 0.05684446453409714]
	TIME [epoch: 3.66 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044329035628101005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044329035628101005 | validation: 0.07597627148566069]
	TIME [epoch: 3.66 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04924617440455509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04924617440455509 | validation: 0.05770069290889062]
	TIME [epoch: 3.65 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049406734443338006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049406734443338006 | validation: 0.07543554097870461]
	TIME [epoch: 3.66 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053584387691206214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053584387691206214 | validation: 0.0772141650703711]
	TIME [epoch: 3.66 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06412821746188273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06412821746188273 | validation: 0.1415823253877078]
	TIME [epoch: 3.67 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10541080315529426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10541080315529426 | validation: 0.16938374243513402]
	TIME [epoch: 3.65 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15349330941120987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15349330941120987 | validation: 0.14628076604964232]
	TIME [epoch: 3.67 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11474452024641547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11474452024641547 | validation: 0.0848767861291909]
	TIME [epoch: 3.65 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07306903596674667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07306903596674667 | validation: 0.0314728867343401]
	TIME [epoch: 3.66 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03370637472431815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03370637472431815 | validation: 0.0347002225761761]
	TIME [epoch: 3.66 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01829115029331084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01829115029331084 | validation: 0.02583427589915517]
	TIME [epoch: 3.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_947.pth
	Model improved!!!
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024456103801400744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024456103801400744 | validation: 0.0983892206087072]
	TIME [epoch: 3.65 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08422145143840103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08422145143840103 | validation: 0.16315986355138007]
	TIME [epoch: 3.65 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21378999996694922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21378999996694922 | validation: 0.129444588037931]
	TIME [epoch: 3.64 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09974393773347429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09974393773347429 | validation: 0.1052196778214183]
	TIME [epoch: 3.64 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07497281074635222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07497281074635222 | validation: 0.13107536749428728]
	TIME [epoch: 3.64 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09667882444544504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09667882444544504 | validation: 0.09387440189344944]
	TIME [epoch: 3.65 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0894530226899229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0894530226899229 | validation: 0.06914242249925172]
	TIME [epoch: 3.63 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044759251536396476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044759251536396476 | validation: 0.04792203506888831]
	TIME [epoch: 3.65 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030471167408470806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030471167408470806 | validation: 0.0650020888095958]
	TIME [epoch: 3.64 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034156710597214084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034156710597214084 | validation: 0.07495121947176091]
	TIME [epoch: 3.65 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06137866472659534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06137866472659534 | validation: 0.17389748676073669]
	TIME [epoch: 3.66 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14136547442779543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14136547442779543 | validation: 0.15272761363212647]
	TIME [epoch: 3.65 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15220432134395404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15220432134395404 | validation: 0.1323697144966499]
	TIME [epoch: 3.64 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09453723754705305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09453723754705305 | validation: 0.08663225375256216]
	TIME [epoch: 3.65 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060331883092404635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060331883092404635 | validation: 0.081533285215242]
	TIME [epoch: 3.65 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05240629448831825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05240629448831825 | validation: 0.057483995510686725]
	TIME [epoch: 3.64 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03806389211400447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03806389211400447 | validation: 0.05212598056975992]
	TIME [epoch: 3.65 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035167597312578854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035167597312578854 | validation: 0.054394846520781186]
	TIME [epoch: 3.65 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03787764481383254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03787764481383254 | validation: 0.056724889381505605]
	TIME [epoch: 3.65 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04240251980572743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04240251980572743 | validation: 0.06816689816698351]
	TIME [epoch: 3.64 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05741637104631876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05741637104631876 | validation: 0.1110926536162483]
	TIME [epoch: 3.64 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07742221009593368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07742221009593368 | validation: 0.11276548912057319]
	TIME [epoch: 3.64 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10921930302201481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10921930302201481 | validation: 0.10873398393604464]
	TIME [epoch: 3.64 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09375128490787127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09375128490787127 | validation: 0.052916780275924793]
	TIME [epoch: 3.65 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06661497013045778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06661497013045778 | validation: 0.052308838985507415]
	TIME [epoch: 3.66 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03764792688493583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03764792688493583 | validation: 0.07746254056761781]
	TIME [epoch: 3.66 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07148446993748576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07148446993748576 | validation: 0.26828518186475137]
	TIME [epoch: 3.65 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21406874108436266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21406874108436266 | validation: 0.13526608831397297]
	TIME [epoch: 3.64 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13324124005577057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13324124005577057 | validation: 0.1117858821902019]
	TIME [epoch: 3.64 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08372021943696645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08372021943696645 | validation: 0.08622392601521493]
	TIME [epoch: 3.64 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06790887902990718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06790887902990718 | validation: 0.06211518904849542]
	TIME [epoch: 3.64 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04050180782357359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04050180782357359 | validation: 0.037340091489328577]
	TIME [epoch: 3.64 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023347421945852324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023347421945852324 | validation: 0.026325797124349872]
	TIME [epoch: 3.64 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017238229388197794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017238229388197794 | validation: 0.0347957858682791]
	TIME [epoch: 3.64 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015676330357010042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015676330357010042 | validation: 0.02579174333641381]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_982.pth
	Model improved!!!
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018032609738392557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018032609738392557 | validation: 0.04539828408582547]
	TIME [epoch: 3.64 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029128434658488717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029128434658488717 | validation: 0.08275448217449075]
	TIME [epoch: 3.65 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07515223947291047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07515223947291047 | validation: 0.1598746310530117]
	TIME [epoch: 3.65 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13737005501377392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13737005501377392 | validation: 0.23523787521496056]
	TIME [epoch: 3.63 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19939714654768717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19939714654768717 | validation: 0.1518496725813595]
	TIME [epoch: 3.65 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16512645788748778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16512645788748778 | validation: 0.10832007824417751]
	TIME [epoch: 3.66 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07222927007552454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07222927007552454 | validation: 0.06425742025217397]
	TIME [epoch: 3.64 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04021972292068028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04021972292068028 | validation: 0.043681916696379254]
	TIME [epoch: 3.63 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022991566302179304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022991566302179304 | validation: 0.03572449239765484]
	TIME [epoch: 3.64 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01876215019827295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01876215019827295 | validation: 0.02356581372540687]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_992.pth
	Model improved!!!
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017202215389992562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017202215389992562 | validation: 0.03262126241336029]
	TIME [epoch: 3.65 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018446878761108395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018446878761108395 | validation: 0.025371459070801718]
	TIME [epoch: 3.65 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024957319538818995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024957319538818995 | validation: 0.08750509527573166]
	TIME [epoch: 3.65 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060458640237854765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060458640237854765 | validation: 0.19798779045782897]
	TIME [epoch: 3.66 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1719893522830382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1719893522830382 | validation: 0.22542531943784902]
	TIME [epoch: 3.67 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19714024941084765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19714024941084765 | validation: 0.11830647557700974]
	TIME [epoch: 3.66 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12047297229490254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12047297229490254 | validation: 0.1441953591605492]
	TIME [epoch: 3.66 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1103361110359379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1103361110359379 | validation: 0.0661300947619509]
	TIME [epoch: 3.66 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0344902892083708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0344902892083708 | validation: 0.056382119815934696]
	TIME [epoch: 49.7 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039177327514835186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039177327514835186 | validation: 0.06883152813910341]
	TIME [epoch: 7.95 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04608251827103817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04608251827103817 | validation: 0.06711476919445288]
	TIME [epoch: 7.91 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04985398940207993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04985398940207993 | validation: 0.08593001975922743]
	TIME [epoch: 7.91 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06392374222503569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06392374222503569 | validation: 0.08922652049957222]
	TIME [epoch: 7.91 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08211255738891556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08211255738891556 | validation: 0.10242038881049101]
	TIME [epoch: 7.93 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07215122900695166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07215122900695166 | validation: 0.06166980893551023]
	TIME [epoch: 7.91 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0487813951883888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0487813951883888 | validation: 0.059734430589569745]
	TIME [epoch: 7.94 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041187146218301704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041187146218301704 | validation: 0.06436770993738265]
	TIME [epoch: 7.92 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054754568730613934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054754568730613934 | validation: 0.10136034878904315]
	TIME [epoch: 7.92 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07892040012942464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07892040012942464 | validation: 0.14007980224798966]
	TIME [epoch: 7.92 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12397265584394174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12397265584394174 | validation: 0.11897470135069549]
	TIME [epoch: 7.92 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10463748710525575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10463748710525575 | validation: 0.05493678726543205]
	TIME [epoch: 7.92 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04129435585854983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04129435585854983 | validation: 0.055866529622429266]
	TIME [epoch: 7.95 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040335976917409457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040335976917409457 | validation: 0.08336426818716325]
	TIME [epoch: 7.92 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06994494239296314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06994494239296314 | validation: 0.12844723225434385]
	TIME [epoch: 7.92 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10690045838674304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10690045838674304 | validation: 0.14969295395151935]
	TIME [epoch: 7.92 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11649082104881962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11649082104881962 | validation: 0.08368134839927774]
	TIME [epoch: 7.93 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0784011524505828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0784011524505828 | validation: 0.061921566315892934]
	TIME [epoch: 7.91 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05057222332880141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05057222332880141 | validation: 0.04371167105207117]
	TIME [epoch: 7.93 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026399365311936825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026399365311936825 | validation: 0.019750992018350788]
	TIME [epoch: 7.93 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_1021.pth
	Model improved!!!
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019580717367263738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019580717367263738 | validation: 0.03417393518785977]
	TIME [epoch: 7.93 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01951030760936546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01951030760936546 | validation: 0.01900300714907162]
	TIME [epoch: 7.92 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_1023.pth
	Model improved!!!
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02113330347424149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02113330347424149 | validation: 0.03306172306188134]
	TIME [epoch: 7.9 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022093488839316836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022093488839316836 | validation: 0.021083595879404473]
	TIME [epoch: 7.91 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0273479126874464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0273479126874464 | validation: 0.05406338079298896]
	TIME [epoch: 7.93 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04899198401548235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04899198401548235 | validation: 0.18246613098075787]
	TIME [epoch: 7.94 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17183624817421014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17183624817421014 | validation: 0.4537039261121185]
	TIME [epoch: 7.93 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34842741202608607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34842741202608607 | validation: 0.07345363399476541]
	TIME [epoch: 7.9 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05894318113159606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05894318113159606 | validation: 0.0860196224809729]
	TIME [epoch: 7.92 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056562221397231875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056562221397231875 | validation: 0.07571993651351494]
	TIME [epoch: 7.91 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058968569572709226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058968569572709226 | validation: 0.048368564185584474]
	TIME [epoch: 7.93 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024627416041004463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024627416041004463 | validation: 0.02794834827461612]
	TIME [epoch: 7.93 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01759587078828653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01759587078828653 | validation: 0.03404964537065976]
	TIME [epoch: 7.93 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025214864652637725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025214864652637725 | validation: 0.049741115612040476]
	TIME [epoch: 7.92 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04983831877551989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04983831877551989 | validation: 0.1486906981781243]
	TIME [epoch: 7.91 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1198383277153141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1198383277153141 | validation: 0.2178477690181092]
	TIME [epoch: 7.91 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17995001376952366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17995001376952366 | validation: 0.07566547234535903]
	TIME [epoch: 7.92 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0549018459084906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0549018459084906 | validation: 0.025525333886084633]
	TIME [epoch: 7.93 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01613213091542322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01613213091542322 | validation: 0.023168274848827067]
	TIME [epoch: 7.92 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020449992313607012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020449992313607012 | validation: 0.062493260717479494]
	TIME [epoch: 7.9 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040557651249881026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040557651249881026 | validation: 0.1518001767809639]
	TIME [epoch: 7.92 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12458126265940093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12458126265940093 | validation: 0.10207836649483953]
	TIME [epoch: 7.9 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09398363532986945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09398363532986945 | validation: 0.06106443550555549]
	TIME [epoch: 7.91 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05597372950945129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05597372950945129 | validation: 0.07643204014167407]
	TIME [epoch: 7.9 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0646804576216194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0646804576216194 | validation: 0.11860906029769352]
	TIME [epoch: 7.93 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07864895000273149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07864895000273149 | validation: 0.11044754930566947]
	TIME [epoch: 7.9 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09412899089883318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09412899089883318 | validation: 0.11183246487552241]
	TIME [epoch: 7.91 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08884407180877377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08884407180877377 | validation: 0.07433036388088705]
	TIME [epoch: 7.89 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0638776551261331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0638776551261331 | validation: 0.07251288553677343]
	TIME [epoch: 7.91 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06522829523600157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06522829523600157 | validation: 0.08639475239008834]
	TIME [epoch: 7.9 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06640298523307317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06640298523307317 | validation: 0.06134161992916026]
	TIME [epoch: 7.94 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04354026864268811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04354026864268811 | validation: 0.037339779374775]
	TIME [epoch: 7.92 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02563474226630182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02563474226630182 | validation: 0.03912993825797759]
	TIME [epoch: 7.91 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021649348002976482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021649348002976482 | validation: 0.03223821192431325]
	TIME [epoch: 7.91 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026372426479702175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026372426479702175 | validation: 0.0655590094879602]
	TIME [epoch: 7.92 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05086042932187791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05086042932187791 | validation: 0.13391068762154593]
	TIME [epoch: 7.9 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12813517962611357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12813517962611357 | validation: 0.2097336954578058]
	TIME [epoch: 7.94 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16170827611949412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16170827611949412 | validation: 0.10778841624838381]
	TIME [epoch: 7.92 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09226526419229172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09226526419229172 | validation: 0.046872061194035676]
	TIME [epoch: 7.93 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05017627878735332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05017627878735332 | validation: 0.048467958164453685]
	TIME [epoch: 7.9 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034347638140022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034347638140022 | validation: 0.04462838110313925]
	TIME [epoch: 7.91 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0362232893654535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0362232893654535 | validation: 0.07431536341032367]
	TIME [epoch: 7.91 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0511287786231193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0511287786231193 | validation: 0.06775155539944612]
	TIME [epoch: 7.91 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05665606280113299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05665606280113299 | validation: 0.07438891468736786]
	TIME [epoch: 7.92 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04794442655932613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04794442655932613 | validation: 0.04362067994897559]
	TIME [epoch: 7.91 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03736555174743955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03736555174743955 | validation: 0.05010295488636038]
	TIME [epoch: 7.9 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026335391312120975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026335391312120975 | validation: 0.031123044587375262]
	TIME [epoch: 7.91 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026309680338283483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026309680338283483 | validation: 0.06362911163120186]
	TIME [epoch: 7.91 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040756938946256385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040756938946256385 | validation: 0.09884275290169736]
	TIME [epoch: 7.92 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10375650725663338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10375650725663338 | validation: 0.23192822952632763]
	TIME [epoch: 7.91 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19308194771030393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19308194771030393 | validation: 0.18555191690806752]
	TIME [epoch: 7.94 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17671796057111855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17671796057111855 | validation: 0.12674316457261012]
	TIME [epoch: 7.91 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10100598938843314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10100598938843314 | validation: 0.04228999388083359]
	TIME [epoch: 7.91 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030859044018203576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030859044018203576 | validation: 0.03287142985166173]
	TIME [epoch: 7.9 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02086391670129128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02086391670129128 | validation: 0.03328990168792894]
	TIME [epoch: 7.92 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026062376166150602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026062376166150602 | validation: 0.049288481086284364]
	TIME [epoch: 7.94 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02813725494158218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02813725494158218 | validation: 0.0412800620616406]
	TIME [epoch: 7.93 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0356186012218179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0356186012218179 | validation: 0.0639493651255612]
	TIME [epoch: 7.9 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05597609345247588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05597609345247588 | validation: 0.11193545259378386]
	TIME [epoch: 7.91 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09104379354149046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09104379354149046 | validation: 0.11414995086981078]
	TIME [epoch: 7.91 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10426663574644413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10426663574644413 | validation: 0.10169346835853493]
	TIME [epoch: 7.91 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08145084449099908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08145084449099908 | validation: 0.05900348859685355]
	TIME [epoch: 7.92 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04407454981230771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04407454981230771 | validation: 0.04591740101882266]
	TIME [epoch: 7.93 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027875803518794245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027875803518794245 | validation: 0.04173097560983057]
	TIME [epoch: 7.91 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032820964354900437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032820964354900437 | validation: 0.0807886567036106]
	TIME [epoch: 7.92 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05579075775824565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05579075775824565 | validation: 0.10682841810557533]
	TIME [epoch: 7.9 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09197093877125116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09197093877125116 | validation: 0.09738005439030395]
	TIME [epoch: 7.92 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07252480507841931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07252480507841931 | validation: 0.03941550077386849]
	TIME [epoch: 7.91 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0358927878353347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0358927878353347 | validation: 0.03716350688066585]
	TIME [epoch: 7.92 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01929723516692246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01929723516692246 | validation: 0.032183903026090664]
	TIME [epoch: 7.94 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01590713740143149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01590713740143149 | validation: 0.021459579195119018]
	TIME [epoch: 7.92 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015506686984982433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015506686984982433 | validation: 0.028869232396271816]
	TIME [epoch: 7.91 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021697945111851916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021697945111851916 | validation: 0.044083450732921275]
	TIME [epoch: 7.91 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055927966443470666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055927966443470666 | validation: 0.1420599825199901]
	TIME [epoch: 7.93 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16367917264614207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16367917264614207 | validation: 0.4081649083211472]
	TIME [epoch: 7.92 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29991173310144975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29991173310144975 | validation: 0.10639372899681683]
	TIME [epoch: 7.94 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07787295129153642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07787295129153642 | validation: 0.0573557465955203]
	TIME [epoch: 7.92 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03487580954797532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03487580954797532 | validation: 0.052511186984579544]
	TIME [epoch: 7.92 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034445483436334155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034445483436334155 | validation: 0.0525624433058018]
	TIME [epoch: 7.92 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03717818288416335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03717818288416335 | validation: 0.07819664062127116]
	TIME [epoch: 7.88 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046330049014772126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046330049014772126 | validation: 0.0684238757312699]
	TIME [epoch: 7.89 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07183846993893096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07183846993893096 | validation: 0.0912881347580578]
	TIME [epoch: 7.91 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06675199470161369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06675199470161369 | validation: 0.05469074713511821]
	TIME [epoch: 7.89 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044165408076585956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044165408076585956 | validation: 0.036027171645062576]
	TIME [epoch: 7.87 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03099130817596028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03099130817596028 | validation: 0.052032568511419956]
	TIME [epoch: 7.9 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036317928515339756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036317928515339756 | validation: 0.07638247525752902]
	TIME [epoch: 7.89 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07737010449752686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07737010449752686 | validation: 0.15159061072992866]
	TIME [epoch: 7.91 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15572172417984803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15572172417984803 | validation: 0.17658483178701886]
	TIME [epoch: 7.9 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12122641983276815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12122641983276815 | validation: 0.06385495228705158]
	TIME [epoch: 7.92 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06461461805877461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06461461805877461 | validation: 0.06719589293659978]
	TIME [epoch: 7.89 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037871032084697706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037871032084697706 | validation: 0.03208274837053152]
	TIME [epoch: 7.88 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022139355494756503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022139355494756503 | validation: 0.027708277146479957]
	TIME [epoch: 7.89 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01720934922595741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01720934922595741 | validation: 0.03138837877412638]
	TIME [epoch: 7.89 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017225211917783197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017225211917783197 | validation: 0.02599143470661469]
	TIME [epoch: 7.91 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01837008587766908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01837008587766908 | validation: 0.03485728114848994]
	TIME [epoch: 7.91 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021820935651907437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021820935651907437 | validation: 0.04296831019934008]
	TIME [epoch: 7.89 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03905446494632599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03905446494632599 | validation: 0.1172647580597051]
	TIME [epoch: 7.88 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10670339338523105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10670339338523105 | validation: 0.28486958718798083]
	TIME [epoch: 7.89 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24417442059587538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24417442059587538 | validation: 0.13908880004378443]
	TIME [epoch: 7.9 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1345792042040324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1345792042040324 | validation: 0.07851532098142831]
	TIME [epoch: 7.9 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0495543580545707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0495543580545707 | validation: 0.0409879342389225]
	TIME [epoch: 7.92 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028887833669618522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028887833669618522 | validation: 0.03517121019949746]
	TIME [epoch: 7.92 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02061326278755713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02061326278755713 | validation: 0.0362172638618143]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_141949/states/model_phi1_4a_v_mmd1_1124.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3818.450 seconds.
