Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3614506139

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.650307012633264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.650307012633264 | validation: 5.649394502545059]
	TIME [epoch: 43.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.1063716343303325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1063716343303325 | validation: 4.426600024357694]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.824903736106152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.824903736106152 | validation: 5.471732275072843]
	TIME [epoch: 1.82 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.513422475939497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.513422475939497 | validation: 4.536831629093255]
	TIME [epoch: 1.82 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4190777910666865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4190777910666865 | validation: 4.384837316556542]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.390145000972588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.390145000972588 | validation: 4.613366871012713]
	TIME [epoch: 1.83 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.335258453564327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.335258453564327 | validation: 4.305125661408219]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.20499973344818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.20499973344818 | validation: 4.244062930179751]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.164840269350653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.164840269350653 | validation: 4.219828627149179]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.107926659421905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.107926659421905 | validation: 4.0912975254701225]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0736089433819975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0736089433819975 | validation: 4.214950793045178]
	TIME [epoch: 1.83 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.157429762502334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.157429762502334 | validation: 4.058200949517022]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.429816676814319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.429816676814319 | validation: 4.16824374099572]
	TIME [epoch: 1.85 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.141128105138913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.141128105138913 | validation: 3.975719310087287]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.967704739444148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.967704739444148 | validation: 3.910363219552967]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9569809763403363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9569809763403363 | validation: 3.9963596144484343]
	TIME [epoch: 1.83 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.971951511907983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.971951511907983 | validation: 3.8541278433982322]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9530928733940085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9530928733940085 | validation: 3.9299204272403387]
	TIME [epoch: 1.82 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9389174771848414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9389174771848414 | validation: 3.7960398696911675]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.918688583834286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.918688583834286 | validation: 3.8884530775963797]
	TIME [epoch: 1.82 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9337172697986613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9337172697986613 | validation: 3.7533618025376456]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9140869961542752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9140869961542752 | validation: 3.8350964208381315]
	TIME [epoch: 1.83 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.902206812412926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.902206812412926 | validation: 3.713378334131292]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.851766543568839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.851766543568839 | validation: 3.7589986725273676]
	TIME [epoch: 1.83 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.822863076751362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.822863076751362 | validation: 3.6625319952348647]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7996500878852757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7996500878852757 | validation: 3.7071103298540673]
	TIME [epoch: 1.83 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8017451717156807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8017451717156807 | validation: 3.6328701297879284]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.802322423913571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.802322423913571 | validation: 3.7020834628920505]
	TIME [epoch: 1.83 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8318002370162807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8318002370162807 | validation: 3.590485556512735]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.79136097568556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.79136097568556 | validation: 3.6366341021852544]
	TIME [epoch: 1.83 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7628631531382366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7628631531382366 | validation: 3.5469109721293175]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7108126756177184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7108126756177184 | validation: 3.554323181749318]
	TIME [epoch: 1.83 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.689544181983429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.689544181983429 | validation: 3.4985933525349253]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6719216483287425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6719216483287425 | validation: 3.516317743867309]
	TIME [epoch: 1.83 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6710438726881196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6710438726881196 | validation: 3.474676314527599]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.682959882690934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.682959882690934 | validation: 3.5307987894389132]
	TIME [epoch: 1.82 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7207131963773703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7207131963773703 | validation: 3.4704462175772273]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.696998864513052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.696998864513052 | validation: 3.4695228485816503]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6660921993555533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6660921993555533 | validation: 3.39391744635536]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5974188789976593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5974188789976593 | validation: 3.373148415391424]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.567744214188285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.567744214188285 | validation: 3.343176969668626]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5473129484079164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5473129484079164 | validation: 3.3329498855687634]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5367140269676485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5367140269676485 | validation: 3.311194325219195]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5293638491316863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5293638491316863 | validation: 3.321690777874606]
	TIME [epoch: 1.82 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.540677728279635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.540677728279635 | validation: 3.3360043116732045]
	TIME [epoch: 1.82 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5987343910424556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5987343910424556 | validation: 3.4086721042507353]
	TIME [epoch: 1.83 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6976379741671437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6976379741671437 | validation: 3.2693888235449666]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.515097415737301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.515097415737301 | validation: 3.229127243432318]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4657791640859967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4657791640859967 | validation: 3.206325809080359]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4479394647685537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4479394647685537 | validation: 3.197080601340695]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4417644751032834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4417644751032834 | validation: 3.182296716691793]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4382309824985935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4382309824985935 | validation: 3.1714883966392438]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.433882524339932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.433882524339932 | validation: 3.165101645849518]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4403341011852895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4403341011852895 | validation: 3.174148105303119]
	TIME [epoch: 1.82 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.449457151260067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.449457151260067 | validation: 3.192505915604574]
	TIME [epoch: 1.82 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4766820578823947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4766820578823947 | validation: 3.1382807750332873]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4214215114810793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4214215114810793 | validation: 3.0942786791520716]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.385389526355849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.385389526355849 | validation: 3.073834172617896]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3603864848666047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3603864848666047 | validation: 3.0619426859943824]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.345441440082317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.345441440082317 | validation: 3.0414860907353765]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3319373431805577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3319373431805577 | validation: 3.0371963645651086]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3251173432717622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3251173432717622 | validation: 3.0181370667115828]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.311602169339866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.311602169339866 | validation: 3.0004284953271774]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.300561090007996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.300561090007996 | validation: 2.99713652322089]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.287113725879597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.287113725879597 | validation: 2.9794012144423414]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.279133496672711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.279133496672711 | validation: 2.977163635280553]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2809784064375673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2809784064375673 | validation: 2.9824757744659]
	TIME [epoch: 1.83 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.28697966271994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.28697966271994 | validation: 2.973209061595359]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.275339738266093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.275339738266093 | validation: 2.9651045683972477]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2602844694181337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2602844694181337 | validation: 2.917723212045038]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2106523317550306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2106523317550306 | validation: 2.8887303649662]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.182058077419273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.182058077419273 | validation: 2.8637552286305072]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.139318828224176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.139318828224176 | validation: 2.838667021010597]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0760337832396143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0760337832396143 | validation: 2.7981279171891185]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0219364931053656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0219364931053656 | validation: 2.830733257179153]
	TIME [epoch: 1.83 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.147541159122726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.147541159122726 | validation: 2.95694510656516]
	TIME [epoch: 1.83 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1933155726558473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1933155726558473 | validation: 2.809508877783694]
	TIME [epoch: 1.84 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.045551614633808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.045551614633808 | validation: 2.7137323299952048]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9734170865328147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9734170865328147 | validation: 2.7194334682708043]
	TIME [epoch: 1.84 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.947334400518451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.947334400518451 | validation: 2.6669440145387124]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9159820731267043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9159820731267043 | validation: 2.6560257602117687]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9041715185537216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9041715185537216 | validation: 2.665329087641635]
	TIME [epoch: 1.82 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8993799461635263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8993799461635263 | validation: 2.6466578236924327]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.885067195965025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.885067195965025 | validation: 2.632148370430472]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.871558817273055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.871558817273055 | validation: 2.6367937032855426]
	TIME [epoch: 1.83 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.865599411375368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.865599411375368 | validation: 2.6171875087153147]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8580824337672115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8580824337672115 | validation: 2.611635331686457]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.845969782544582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.845969782544582 | validation: 2.596189945657374]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.83745549973984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.83745549973984 | validation: 2.6240412548588488]
	TIME [epoch: 1.81 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.825951284712287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.825951284712287 | validation: 2.593566475064229]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7619261659782355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7619261659782355 | validation: 2.1508693674395705]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.131697586691509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.131697586691509 | validation: 1.7315229147709061]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7550606909061979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7550606909061979 | validation: 2.5913559920877733]
	TIME [epoch: 1.83 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3742275412179503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3742275412179503 | validation: 1.2989910333212498]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2245407059516127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2245407059516127 | validation: 1.2077177358969655]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2211197388209751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2211197388209751 | validation: 1.1392139455912713]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0187358626315342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0187358626315342 | validation: 1.2617131727335464]
	TIME [epoch: 1.83 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0552545489630878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0552545489630878 | validation: 1.0505464165430702]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9891075063163166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9891075063163166 | validation: 1.0178002583852723]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9728004718437936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9728004718437936 | validation: 1.1296415580915053]
	TIME [epoch: 1.82 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9473992412251908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9473992412251908 | validation: 1.047721013267451]
	TIME [epoch: 1.82 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9200207322155463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9200207322155463 | validation: 1.007750613907754]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9109959283283067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9109959283283067 | validation: 1.0165210199715995]
	TIME [epoch: 1.82 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8974643802744364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8974643802744364 | validation: 1.0223028780358643]
	TIME [epoch: 1.83 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8793542118397941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8793542118397941 | validation: 1.0238791733459747]
	TIME [epoch: 1.82 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8738198654224878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8738198654224878 | validation: 0.9858560309773172]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859887739885614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.859887739885614 | validation: 1.0002292503114505]
	TIME [epoch: 1.83 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8470923360208371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8470923360208371 | validation: 1.0113984183817986]
	TIME [epoch: 1.82 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8498975494401876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8498975494401876 | validation: 1.055649525549219]
	TIME [epoch: 1.82 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.906730777365328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.906730777365328 | validation: 1.0303055443132718]
	TIME [epoch: 1.82 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9958919874161569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9958919874161569 | validation: 1.2930813151755964]
	TIME [epoch: 1.82 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.012781668903823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.012781668903823 | validation: 1.0512406532120175]
	TIME [epoch: 1.83 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8432855189717428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8432855189717428 | validation: 0.9171308648387099]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9402639714679637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9402639714679637 | validation: 1.0772455918405723]
	TIME [epoch: 1.83 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8756513484658974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8756513484658974 | validation: 1.0638662858728187]
	TIME [epoch: 1.82 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8299571235927198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8299571235927198 | validation: 0.8543526651009297]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8112852108463133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8112852108463133 | validation: 0.8994173522651345]
	TIME [epoch: 1.83 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7979106133683914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7979106133683914 | validation: 1.0183724464077952]
	TIME [epoch: 1.83 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.81604624964165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.81604624964165 | validation: 0.9082897867160526]
	TIME [epoch: 1.85 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8125172520903867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8125172520903867 | validation: 0.9386213695864838]
	TIME [epoch: 1.83 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8135824709212546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8135824709212546 | validation: 0.958988672685451]
	TIME [epoch: 1.84 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8295743659658975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8295743659658975 | validation: 0.9871586618917458]
	TIME [epoch: 1.83 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8264300684138708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8264300684138708 | validation: 0.9267818428993714]
	TIME [epoch: 1.83 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8130471705003826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8130471705003826 | validation: 0.9427636416385572]
	TIME [epoch: 1.83 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804639693283542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.804639693283542 | validation: 0.9615465144956018]
	TIME [epoch: 1.83 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982178605309191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7982178605309191 | validation: 0.8946696628112761]
	TIME [epoch: 1.83 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7811533001280614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7811533001280614 | validation: 0.9716265367451782]
	TIME [epoch: 1.83 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7958534573565617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7958534573565617 | validation: 0.8804092101294358]
	TIME [epoch: 1.84 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069418252281167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8069418252281167 | validation: 0.9788394240528862]
	TIME [epoch: 1.83 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8468637645683031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8468637645683031 | validation: 1.0152966863366686]
	TIME [epoch: 1.83 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8382989888665432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8382989888665432 | validation: 0.9250302107704403]
	TIME [epoch: 1.83 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8257082067204243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8257082067204243 | validation: 0.9398575993046155]
	TIME [epoch: 1.82 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7747921176642584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7747921176642584 | validation: 0.9086561285626651]
	TIME [epoch: 1.82 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7649605948472669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7649605948472669 | validation: 0.8702664107892698]
	TIME [epoch: 1.83 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7641211847874224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7641211847874224 | validation: 0.982558316573731]
	TIME [epoch: 1.82 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694432930334353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7694432930334353 | validation: 0.8361250965645941]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.817923223021632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.817923223021632 | validation: 1.1814408357224]
	TIME [epoch: 1.82 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8735251975814663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8735251975814663 | validation: 0.8994731487555008]
	TIME [epoch: 1.82 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.765732208284863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.765732208284863 | validation: 0.8565822175714062]
	TIME [epoch: 1.82 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8025062021988698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8025062021988698 | validation: 1.1087689009597868]
	TIME [epoch: 1.81 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8228597715056882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8228597715056882 | validation: 0.9122299912076355]
	TIME [epoch: 1.82 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8088289239387806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8088289239387806 | validation: 0.9131330362411841]
	TIME [epoch: 1.82 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8812821445845984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8812821445845984 | validation: 1.069982756496166]
	TIME [epoch: 1.82 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8957285744857799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8957285744857799 | validation: 0.9576057084517594]
	TIME [epoch: 1.85 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7708174075057055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7708174075057055 | validation: 0.8561805218892584]
	TIME [epoch: 1.81 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7742101650140232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7742101650140232 | validation: 0.91585095713567]
	TIME [epoch: 1.83 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7781968236446589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7781968236446589 | validation: 0.9201773437480434]
	TIME [epoch: 1.82 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7802726453700484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7802726453700484 | validation: 0.8918089949449923]
	TIME [epoch: 1.82 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827465151911817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7827465151911817 | validation: 0.8703881130810238]
	TIME [epoch: 1.82 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7633391840898827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7633391840898827 | validation: 0.9251390185087836]
	TIME [epoch: 1.82 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7711731575847273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7711731575847273 | validation: 0.870837044212613]
	TIME [epoch: 1.82 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7656696546012327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7656696546012327 | validation: 0.9292921758963104]
	TIME [epoch: 1.82 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7783408119660145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7783408119660145 | validation: 0.9144591221360934]
	TIME [epoch: 1.82 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7973595006643552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7973595006643552 | validation: 0.9769836512248702]
	TIME [epoch: 1.82 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8298374460101288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8298374460101288 | validation: 0.9072731625329414]
	TIME [epoch: 1.83 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8027609224986229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8027609224986229 | validation: 0.9310400493402256]
	TIME [epoch: 1.82 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7777814126317555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7777814126317555 | validation: 0.8716839751739347]
	TIME [epoch: 1.82 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7489350330765686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7489350330765686 | validation: 0.8982588854490019]
	TIME [epoch: 1.82 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517453219382206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7517453219382206 | validation: 0.8444858095354505]
	TIME [epoch: 1.82 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7392151158679178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7392151158679178 | validation: 0.9322288231491932]
	TIME [epoch: 1.81 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7521127140970623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7521127140970623 | validation: 0.8406931406610788]
	TIME [epoch: 1.82 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7771547985197026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7771547985197026 | validation: 1.0828881609658738]
	TIME [epoch: 1.81 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8541764088698056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8541764088698056 | validation: 0.938242892211613]
	TIME [epoch: 1.81 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804712130071039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.804712130071039 | validation: 0.9451982479280799]
	TIME [epoch: 1.81 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8315527122500214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8315527122500214 | validation: 1.0453973972713535]
	TIME [epoch: 1.82 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8089293593186421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8089293593186421 | validation: 0.8607922524560554]
	TIME [epoch: 1.82 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7458498034490805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7458498034490805 | validation: 0.898310708103378]
	TIME [epoch: 1.82 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7369109224389417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7369109224389417 | validation: 0.8700846163647804]
	TIME [epoch: 1.83 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7452613844946354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7452613844946354 | validation: 0.8986313138288007]
	TIME [epoch: 1.83 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7390434340080989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7390434340080989 | validation: 0.8366946780732377]
	TIME [epoch: 1.83 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7775609375830507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7775609375830507 | validation: 1.125999190187653]
	TIME [epoch: 1.82 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8688464880416507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8688464880416507 | validation: 0.8897416237389921]
	TIME [epoch: 1.82 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764455709257243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.764455709257243 | validation: 0.8639457029073606]
	TIME [epoch: 1.82 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7813017912906896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7813017912906896 | validation: 1.0613147129619813]
	TIME [epoch: 1.81 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114094790067375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8114094790067375 | validation: 0.8545251857176887]
	TIME [epoch: 1.82 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7667858070878036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7667858070878036 | validation: 0.8439578121572948]
	TIME [epoch: 1.82 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7506921420069401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7506921420069401 | validation: 0.9849462507307]
	TIME [epoch: 1.83 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7714700183414493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7714700183414493 | validation: 0.816165130405913]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7704504175141011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7704504175141011 | validation: 0.9777700584550996]
	TIME [epoch: 1.82 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898329573382159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7898329573382159 | validation: 0.9129511375507108]
	TIME [epoch: 1.82 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7787434208761834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7787434208761834 | validation: 0.8992891221159367]
	TIME [epoch: 1.81 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8028122049853821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8028122049853821 | validation: 0.9246290411718313]
	TIME [epoch: 1.82 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745275343095429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7745275343095429 | validation: 0.8914892133297185]
	TIME [epoch: 1.82 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.746085831382549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.746085831382549 | validation: 0.8238388288554521]
	TIME [epoch: 1.82 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7296600990569294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7296600990569294 | validation: 0.9622125786689684]
	TIME [epoch: 1.82 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7504503400778182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7504503400778182 | validation: 0.7959228292230923]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7593277551947517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7593277551947517 | validation: 1.0529911761870385]
	TIME [epoch: 1.82 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8117557185323818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8117557185323818 | validation: 0.8566774092599099]
	TIME [epoch: 1.82 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7613925301273053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7613925301273053 | validation: 0.8911532444008143]
	TIME [epoch: 1.82 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8109111021249383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8109111021249383 | validation: 0.9934614319422445]
	TIME [epoch: 1.82 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7980718005925485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7980718005925485 | validation: 0.861733022910169]
	TIME [epoch: 1.82 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7472585893548147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7472585893548147 | validation: 0.8152576439049967]
	TIME [epoch: 1.82 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7192440495637231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7192440495637231 | validation: 0.8928882798446343]
	TIME [epoch: 1.83 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7214292213182969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7214292213182969 | validation: 0.7802400614481062]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7336839553863892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7336839553863892 | validation: 0.9469897636358573]
	TIME [epoch: 1.82 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7437606749740818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7437606749740818 | validation: 0.7872943802910541]
	TIME [epoch: 1.82 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7704815998124207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7704815998124207 | validation: 1.025354783709845]
	TIME [epoch: 1.82 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8625430337372373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8625430337372373 | validation: 1.0194453888370858]
	TIME [epoch: 1.82 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8184830332071203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8184830332071203 | validation: 0.8164323086276402]
	TIME [epoch: 1.82 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547676220976903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7547676220976903 | validation: 0.8740230397409559]
	TIME [epoch: 1.82 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182894801838273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7182894801838273 | validation: 0.838424794844649]
	TIME [epoch: 42.1 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7116274747838456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7116274747838456 | validation: 0.8485145255606912]
	TIME [epoch: 3.63 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.718614398930045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.718614398930045 | validation: 0.8361598682332407]
	TIME [epoch: 3.61 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7317464974884816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7317464974884816 | validation: 0.9564306133270656]
	TIME [epoch: 3.6 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7719418039915155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7719418039915155 | validation: 0.8025415006425455]
	TIME [epoch: 3.6 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.816193711052006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.816193711052006 | validation: 0.9964363948441358]
	TIME [epoch: 3.61 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8121627570623682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8121627570623682 | validation: 0.8487614532244439]
	TIME [epoch: 3.61 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7171749770162541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7171749770162541 | validation: 0.7522210553319257]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7228138742959916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7228138742959916 | validation: 0.963387298096011]
	TIME [epoch: 3.61 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7268129729874079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7268129729874079 | validation: 0.745191537012137]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7248940019619415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7248940019619415 | validation: 0.8703013933377584]
	TIME [epoch: 3.61 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.703930135537418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.703930135537418 | validation: 0.84349530311308]
	TIME [epoch: 3.61 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7060496774266357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7060496774266357 | validation: 0.7883450880548608]
	TIME [epoch: 3.61 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7795221379763886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7795221379763886 | validation: 1.117997148403018]
	TIME [epoch: 3.62 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9054866308126197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9054866308126197 | validation: 0.762320391880378]
	TIME [epoch: 3.63 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6963228932690925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6963228932690925 | validation: 0.75945625664013]
	TIME [epoch: 3.62 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.662115532818686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.662115532818686 | validation: 0.7619095782041518]
	TIME [epoch: 3.61 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.655449559932431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.655449559932431 | validation: 0.82759454974897]
	TIME [epoch: 3.66 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6726790182802828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6726790182802828 | validation: 0.8611634894840776]
	TIME [epoch: 3.61 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9842989477169363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9842989477169363 | validation: 1.1515354419388293]
	TIME [epoch: 3.6 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8871640906426445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8871640906426445 | validation: 0.9153269192147431]
	TIME [epoch: 3.61 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.702193022445189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.702193022445189 | validation: 0.7103415536471388]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7160709560676957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7160709560676957 | validation: 0.7793880637443279]
	TIME [epoch: 3.61 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6535186850150229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6535186850150229 | validation: 0.7927937863571624]
	TIME [epoch: 3.6 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.636718840912135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.636718840912135 | validation: 0.7760219721495686]
	TIME [epoch: 3.6 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6386127121193135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6386127121193135 | validation: 0.7276364238018]
	TIME [epoch: 3.6 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6549532450570698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6549532450570698 | validation: 1.0941704829859098]
	TIME [epoch: 3.61 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8471476282958225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8471476282958225 | validation: 0.7502862520136979]
	TIME [epoch: 3.61 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8608884241588867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8608884241588867 | validation: 0.793942550610087]
	TIME [epoch: 3.61 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5948704284861627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5948704284861627 | validation: 0.7564163303356958]
	TIME [epoch: 3.61 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5833200877785334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5833200877785334 | validation: 0.6564151873910897]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5915409016849209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5915409016849209 | validation: 0.8614164901080541]
	TIME [epoch: 3.6 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6250359374956252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6250359374956252 | validation: 0.7540753712633127]
	TIME [epoch: 3.6 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8794000456498762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8794000456498762 | validation: 0.7977072496334023]
	TIME [epoch: 3.6 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5855957024185785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5855957024185785 | validation: 0.8351202977216318]
	TIME [epoch: 3.61 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.660403923281487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.660403923281487 | validation: 0.694042238878822]
	TIME [epoch: 3.6 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6104037075400445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6104037075400445 | validation: 0.86425269512331]
	TIME [epoch: 3.6 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6020894242775914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6020894242775914 | validation: 0.6038133255818905]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5394226129576376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5394226129576376 | validation: 0.7725480651366152]
	TIME [epoch: 3.6 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5469184080838354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5469184080838354 | validation: 0.6288626661336396]
	TIME [epoch: 3.62 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7067800714901188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7067800714901188 | validation: 0.8202363931357749]
	TIME [epoch: 3.61 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5668369322943655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5668369322943655 | validation: 0.719780996826151]
	TIME [epoch: 3.61 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5694308246259301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5694308246259301 | validation: 0.731056831309814]
	TIME [epoch: 3.61 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6113101554496362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6113101554496362 | validation: 0.8936877185800611]
	TIME [epoch: 3.61 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.688287177516662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.688287177516662 | validation: 0.8415844987589054]
	TIME [epoch: 3.6 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6774440789718327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6774440789718327 | validation: 0.7009580209380165]
	TIME [epoch: 3.6 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5052515867871109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5052515867871109 | validation: 0.6646696365648757]
	TIME [epoch: 3.6 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4739335539986119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4739335539986119 | validation: 0.6350176419755712]
	TIME [epoch: 3.6 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4851282687021701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4851282687021701 | validation: 0.7813885354545946]
	TIME [epoch: 3.6 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5406600939709698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5406600939709698 | validation: 0.6465116720605839]
	TIME [epoch: 3.6 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6449140055052138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6449140055052138 | validation: 1.076004459098734]
	TIME [epoch: 3.6 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881113191116361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7881113191116361 | validation: 0.6521010030853447]
	TIME [epoch: 3.61 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668642523393655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7668642523393655 | validation: 0.7533566018565611]
	TIME [epoch: 3.61 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5983198450631474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5983198450631474 | validation: 0.8539672705028182]
	TIME [epoch: 3.62 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.585511587929286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.585511587929286 | validation: 0.58969323374185]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4656841933693323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4656841933693323 | validation: 0.6310719008887775]
	TIME [epoch: 3.61 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4455822307890877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4455822307890877 | validation: 0.6607319725035258]
	TIME [epoch: 3.6 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44074465946232755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44074465946232755 | validation: 0.6331232830168658]
	TIME [epoch: 3.6 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4977805884898281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4977805884898281 | validation: 0.8826158718062072]
	TIME [epoch: 3.6 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6436371216325566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6436371216325566 | validation: 0.8098074473233124]
	TIME [epoch: 3.6 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6027613448830549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6027613448830549 | validation: 0.6692858713916834]
	TIME [epoch: 3.6 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47476427075118816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47476427075118816 | validation: 0.7263406022945483]
	TIME [epoch: 3.6 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47195215372775834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47195215372775834 | validation: 0.5244309087926523]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6169912515042273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6169912515042273 | validation: 1.1287423149489058]
	TIME [epoch: 3.59 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8451756168351944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8451756168351944 | validation: 0.6653089310687328]
	TIME [epoch: 3.61 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4667549377143921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4667549377143921 | validation: 0.5594680587456241]
	TIME [epoch: 3.6 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5891446415064904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5891446415064904 | validation: 0.717195885780042]
	TIME [epoch: 3.59 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4649093779740133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4649093779740133 | validation: 0.6674439925190474]
	TIME [epoch: 3.59 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4391412671898828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4391412671898828 | validation: 0.604463208315723]
	TIME [epoch: 3.59 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47050347978732676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47050347978732676 | validation: 0.9209058521026567]
	TIME [epoch: 3.59 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5752916530303509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5752916530303509 | validation: 0.6005540401618581]
	TIME [epoch: 3.6 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48834004827681654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48834004827681654 | validation: 0.680340954130826]
	TIME [epoch: 3.59 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42592444148555275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42592444148555275 | validation: 0.6038935595378995]
	TIME [epoch: 3.58 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39128782259408584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39128782259408584 | validation: 0.5879729580421224]
	TIME [epoch: 3.59 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4066006850038298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4066006850038298 | validation: 0.7008596659836727]
	TIME [epoch: 3.58 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4362633978377255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4362633978377255 | validation: 0.6128952959299542]
	TIME [epoch: 3.58 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.537267630328993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.537267630328993 | validation: 0.8411194437768721]
	TIME [epoch: 3.59 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5289262544449862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5289262544449862 | validation: 0.4965307240311292]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5261008497232099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5261008497232099 | validation: 0.9581231149147191]
	TIME [epoch: 3.62 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6077872517229032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6077872517229032 | validation: 0.6462922591352062]
	TIME [epoch: 3.61 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4842239476386828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4842239476386828 | validation: 0.618236345037643]
	TIME [epoch: 3.6 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4490351341741148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4490351341741148 | validation: 0.6799728871763302]
	TIME [epoch: 3.6 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4144684803911611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4144684803911611 | validation: 0.554238122314703]
	TIME [epoch: 3.6 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3757256250969536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3757256250969536 | validation: 0.614896801689464]
	TIME [epoch: 3.61 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36697082074686604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36697082074686604 | validation: 0.6036063105169271]
	TIME [epoch: 3.61 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38401523461733084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38401523461733084 | validation: 0.6329978452150975]
	TIME [epoch: 3.6 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4020684695966679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4020684695966679 | validation: 0.8331392069774153]
	TIME [epoch: 3.61 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4851201595075535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4851201595075535 | validation: 0.5395126760788149]
	TIME [epoch: 3.58 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4241467308951276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4241467308951276 | validation: 0.9407932712165853]
	TIME [epoch: 3.59 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6078382909715581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6078382909715581 | validation: 0.45523980207471837]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5239701921570632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5239701921570632 | validation: 0.7580753366057241]
	TIME [epoch: 3.62 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43089550432380663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43089550432380663 | validation: 0.520892615095292]
	TIME [epoch: 3.61 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.328085573097275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.328085573097275 | validation: 0.5239146976186337]
	TIME [epoch: 3.6 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3538400941452139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3538400941452139 | validation: 0.7805137676229696]
	TIME [epoch: 3.6 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42362274849788395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42362274849788395 | validation: 0.6523533450434916]
	TIME [epoch: 3.62 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43333305983224607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43333305983224607 | validation: 0.6347763225497404]
	TIME [epoch: 3.6 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4157765624161175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4157765624161175 | validation: 0.5865993740912242]
	TIME [epoch: 3.62 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35483585048586114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35483585048586114 | validation: 0.5921549810563852]
	TIME [epoch: 3.6 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34979380654837267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34979380654837267 | validation: 0.5342460618724653]
	TIME [epoch: 3.6 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3438793876928671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3438793876928671 | validation: 0.8018248436511979]
	TIME [epoch: 3.61 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42596180248487275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42596180248487275 | validation: 0.4892981354345885]
	TIME [epoch: 3.59 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6597695655321879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6597695655321879 | validation: 0.8207269749714039]
	TIME [epoch: 3.6 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4889625389189769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4889625389189769 | validation: 0.558790334195225]
	TIME [epoch: 3.6 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3618777052883449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3618777052883449 | validation: 0.5530877697045025]
	TIME [epoch: 3.61 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38682696602219024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38682696602219024 | validation: 0.7282501152471599]
	TIME [epoch: 3.6 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3963082615015687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3963082615015687 | validation: 0.5100553053621611]
	TIME [epoch: 3.59 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3170779523662186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3170779523662186 | validation: 0.5192920841066647]
	TIME [epoch: 3.59 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2843370110052895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2843370110052895 | validation: 0.5701576147556882]
	TIME [epoch: 3.59 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28251232319594827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28251232319594827 | validation: 0.4629649924739576]
	TIME [epoch: 3.59 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2896673440182394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2896673440182394 | validation: 0.7342362221226305]
	TIME [epoch: 3.59 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3774093385555328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3774093385555328 | validation: 0.4027836928633393]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.546345984314186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.546345984314186 | validation: 0.8287599722938869]
	TIME [epoch: 3.59 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.460449104245115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.460449104245115 | validation: 0.6213423479358751]
	TIME [epoch: 3.59 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33090396894123447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33090396894123447 | validation: 0.6083344966273754]
	TIME [epoch: 3.6 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4542064344207485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4542064344207485 | validation: 0.8233198897237298]
	TIME [epoch: 3.61 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47236146337899726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47236146337899726 | validation: 0.6501655070162758]
	TIME [epoch: 3.6 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3371945792311405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3371945792311405 | validation: 0.44311163091631256]
	TIME [epoch: 3.59 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3192401382400286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3192401382400286 | validation: 0.6393843579539737]
	TIME [epoch: 3.59 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3144613399258446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3144613399258446 | validation: 0.4577917882086202]
	TIME [epoch: 3.58 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3230981270618657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3230981270618657 | validation: 0.6402382454586159]
	TIME [epoch: 3.59 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3229081668274897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3229081668274897 | validation: 0.4269954112868288]
	TIME [epoch: 3.58 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4053353987541549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4053353987541549 | validation: 0.7630581072908248]
	TIME [epoch: 3.58 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4673617755866982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4673617755866982 | validation: 0.6238624678161688]
	TIME [epoch: 3.59 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32310883870808155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32310883870808155 | validation: 0.44152298626986863]
	TIME [epoch: 3.59 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3279730679524136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3279730679524136 | validation: 0.6635356659932378]
	TIME [epoch: 3.59 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3375833060632917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3375833060632917 | validation: 0.45037496597312476]
	TIME [epoch: 3.59 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2739555019803359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2739555019803359 | validation: 0.5655081941998782]
	TIME [epoch: 3.59 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.265040858470666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.265040858470666 | validation: 0.524181479158751]
	TIME [epoch: 3.59 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34991335134440577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34991335134440577 | validation: 0.7380520305678017]
	TIME [epoch: 3.6 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5045302544096828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5045302544096828 | validation: 0.7929254064507861]
	TIME [epoch: 3.59 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40205774009172857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40205774009172857 | validation: 0.4575273170226888]
	TIME [epoch: 3.58 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3257838413581284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3257838413581284 | validation: 0.6288082209293431]
	TIME [epoch: 3.58 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34130830808165713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34130830808165713 | validation: 0.40495790894756206]
	TIME [epoch: 3.58 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3044383823384854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3044383823384854 | validation: 0.7396401697426559]
	TIME [epoch: 3.59 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3444376510070387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3444376510070387 | validation: 0.44849725462628354]
	TIME [epoch: 3.6 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2641981939615944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2641981939615944 | validation: 0.5378895578555448]
	TIME [epoch: 3.6 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26635845419334025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26635845419334025 | validation: 0.5744398535443164]
	TIME [epoch: 3.58 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3268214805711871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3268214805711871 | validation: 0.5816181918404217]
	TIME [epoch: 3.59 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3099697292275097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3099697292275097 | validation: 0.5531734247040879]
	TIME [epoch: 3.58 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2758774157352294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2758774157352294 | validation: 0.5098058256833609]
	TIME [epoch: 3.59 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2711589367760149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2711589367760149 | validation: 0.6582873149011537]
	TIME [epoch: 3.59 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31096966812254073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31096966812254073 | validation: 0.5193012348497534]
	TIME [epoch: 3.6 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27600045407981827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27600045407981827 | validation: 0.5507400146066156]
	TIME [epoch: 3.59 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24431889876484028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24431889876484028 | validation: 0.4227457201886374]
	TIME [epoch: 3.59 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2873898995938206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2873898995938206 | validation: 0.8882872710171212]
	TIME [epoch: 3.58 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5528518525877263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5528518525877263 | validation: 0.44651863723315827]
	TIME [epoch: 3.6 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30863108654451404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30863108654451404 | validation: 0.5625580506169232]
	TIME [epoch: 3.59 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27987353895102357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27987353895102357 | validation: 0.5212921307706999]
	TIME [epoch: 3.59 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2989538312302381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2989538312302381 | validation: 0.5574213304629355]
	TIME [epoch: 3.59 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.314172737798067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.314172737798067 | validation: 0.6949499012619207]
	TIME [epoch: 3.59 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3479599212739737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3479599212739737 | validation: 0.4752614607657293]
	TIME [epoch: 3.59 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23405564900051534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23405564900051534 | validation: 0.5124231387864381]
	TIME [epoch: 3.58 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2350245377782251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2350245377782251 | validation: 0.4509399524431805]
	TIME [epoch: 3.58 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24834430447404512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24834430447404512 | validation: 0.7370138249488893]
	TIME [epoch: 3.59 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3801003202231017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3801003202231017 | validation: 0.4382164961728936]
	TIME [epoch: 3.6 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36521162119721867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36521162119721867 | validation: 0.666445159510489]
	TIME [epoch: 3.6 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33322797360612116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33322797360612116 | validation: 0.46433131066083483]
	TIME [epoch: 3.59 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20777815907488598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20777815907488598 | validation: 0.4650108706669411]
	TIME [epoch: 3.58 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21598838167498202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21598838167498202 | validation: 0.6367160699778242]
	TIME [epoch: 3.6 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27033475072510565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27033475072510565 | validation: 0.5694378838691806]
	TIME [epoch: 3.58 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36758098742544504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36758098742544504 | validation: 0.6177426015062641]
	TIME [epoch: 3.6 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3315256070100975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3315256070100975 | validation: 0.6148334679543916]
	TIME [epoch: 3.58 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26286035079463654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26286035079463654 | validation: 0.4023964958049437]
	TIME [epoch: 3.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31441017841850427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31441017841850427 | validation: 0.6921625697687175]
	TIME [epoch: 3.6 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35621844303960687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35621844303960687 | validation: 0.472196300983275]
	TIME [epoch: 3.59 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2428435968387304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2428435968387304 | validation: 0.49571636307659656]
	TIME [epoch: 3.58 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21800320847165858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21800320847165858 | validation: 0.5010077860738064]
	TIME [epoch: 3.6 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2206707993645116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2206707993645116 | validation: 0.5054141292820207]
	TIME [epoch: 3.6 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24005821692332346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24005821692332346 | validation: 0.6211763448619939]
	TIME [epoch: 3.59 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30912161519499753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30912161519499753 | validation: 0.5539758058355594]
	TIME [epoch: 3.58 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2982299313751851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2982299313751851 | validation: 0.5112571492863092]
	TIME [epoch: 3.59 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26371639902639915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26371639902639915 | validation: 0.6522883853317205]
	TIME [epoch: 3.58 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2728490624894737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2728490624894737 | validation: 0.40814419892458115]
	TIME [epoch: 3.58 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3507395400115992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3507395400115992 | validation: 0.7035038900062183]
	TIME [epoch: 3.57 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3784201518874615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3784201518874615 | validation: 0.4583399030257578]
	TIME [epoch: 3.59 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21260693389386706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21260693389386706 | validation: 0.45268432042940676]
	TIME [epoch: 3.59 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19643390697809374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19643390697809374 | validation: 0.4945201538255777]
	TIME [epoch: 3.59 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20000671180517215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20000671180517215 | validation: 0.4811875194763001]
	TIME [epoch: 3.58 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21379899452930598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21379899452930598 | validation: 0.5887856711279076]
	TIME [epoch: 3.6 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2749888843478417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2749888843478417 | validation: 0.6268450438815747]
	TIME [epoch: 3.59 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3401325876245052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3401325876245052 | validation: 0.5629159092833651]
	TIME [epoch: 3.6 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3473591176045342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3473591176045342 | validation: 0.48073356972866127]
	TIME [epoch: 3.58 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19367408780156353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19367408780156353 | validation: 0.4510079907691393]
	TIME [epoch: 3.59 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2013665428403818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2013665428403818 | validation: 0.49080036094155155]
	TIME [epoch: 3.58 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2014949581420109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2014949581420109 | validation: 0.5369891913613247]
	TIME [epoch: 3.59 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1925694797505156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1925694797505156 | validation: 0.4243686509367377]
	TIME [epoch: 3.58 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3097077716682942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3097077716682942 | validation: 0.851892737196799]
	TIME [epoch: 3.59 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47055907751663667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47055907751663667 | validation: 0.4413516895362897]
	TIME [epoch: 3.6 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1984512582429553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1984512582429553 | validation: 0.41724286942100486]
	TIME [epoch: 3.58 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1761781995763737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1761781995763737 | validation: 0.5357490526679046]
	TIME [epoch: 3.59 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1944609091222764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1944609091222764 | validation: 0.48724463838341814]
	TIME [epoch: 3.58 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25071780558383067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25071780558383067 | validation: 0.6657524890216524]
	TIME [epoch: 3.59 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37836513089624235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37836513089624235 | validation: 0.7268914436485248]
	TIME [epoch: 3.6 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37360115164865265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37360115164865265 | validation: 0.44177310484062904]
	TIME [epoch: 3.59 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20515041267104359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20515041267104359 | validation: 0.5599218495678157]
	TIME [epoch: 3.58 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3040214284829917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3040214284829917 | validation: 0.4750618784072697]
	TIME [epoch: 3.59 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23748115105778048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23748115105778048 | validation: 0.5138630734733439]
	TIME [epoch: 3.6 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24139248690757914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24139248690757914 | validation: 0.5189725413717501]
	TIME [epoch: 3.59 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20078257398999932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20078257398999932 | validation: 0.4072164698117522]
	TIME [epoch: 3.59 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23109615895288754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23109615895288754 | validation: 0.6580943079122591]
	TIME [epoch: 3.6 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26683756226796024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26683756226796024 | validation: 0.4232068483970576]
	TIME [epoch: 3.59 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2799387076914577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2799387076914577 | validation: 0.562861981057436]
	TIME [epoch: 3.58 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2541304641471502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2541304641471502 | validation: 0.5579187442343365]
	TIME [epoch: 3.58 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23226482265069068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23226482265069068 | validation: 0.5651888164828945]
	TIME [epoch: 3.59 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3211827485883334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3211827485883334 | validation: 0.5558222133251124]
	TIME [epoch: 3.59 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26789695920495443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26789695920495443 | validation: 0.5713498119124418]
	TIME [epoch: 3.59 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21254756517536535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21254756517536535 | validation: 0.42208356240582195]
	TIME [epoch: 3.59 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20336309748694134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20336309748694134 | validation: 0.4895805667055436]
	TIME [epoch: 3.59 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19323046232879187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19323046232879187 | validation: 0.43291657109546655]
	TIME [epoch: 3.59 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20513585949737043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20513585949737043 | validation: 0.5469242692380961]
	TIME [epoch: 3.59 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2277927716117066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2277927716117066 | validation: 0.47879656679103655]
	TIME [epoch: 3.59 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2812057053835708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2812057053835708 | validation: 0.5520697015241083]
	TIME [epoch: 3.59 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23236636697774715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23236636697774715 | validation: 0.45191187032963653]
	TIME [epoch: 3.58 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18327173844482708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18327173844482708 | validation: 0.4776125147493505]
	TIME [epoch: 3.58 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18225149018231387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18225149018231387 | validation: 0.47756018770372183]
	TIME [epoch: 3.58 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2150496443040498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2150496443040498 | validation: 0.5676480350188958]
	TIME [epoch: 3.6 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25729759090961823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25729759090961823 | validation: 0.5065649755641336]
	TIME [epoch: 3.59 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22274352684807242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22274352684807242 | validation: 0.4761302450617437]
	TIME [epoch: 3.6 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16761849617812993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16761849617812993 | validation: 0.4496722782783895]
	TIME [epoch: 3.62 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16553163752902147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16553163752902147 | validation: 0.5173062114709478]
	TIME [epoch: 3.6 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17152001489496577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17152001489496577 | validation: 0.44669325095221124]
	TIME [epoch: 3.59 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1774055982254316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1774055982254316 | validation: 0.6074474324994248]
	TIME [epoch: 3.6 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21577603871622777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21577603871622777 | validation: 0.40615779706532473]
	TIME [epoch: 3.58 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5006779532250468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5006779532250468 | validation: 0.5425119430475877]
	TIME [epoch: 3.58 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24386352883406262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24386352883406262 | validation: 0.596304987158714]
	TIME [epoch: 3.58 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21881567168270813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21881567168270813 | validation: 0.5527833629222225]
	TIME [epoch: 3.59 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3609524275275484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3609524275275484 | validation: 0.6647442012104815]
	TIME [epoch: 3.58 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37091830161858264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37091830161858264 | validation: 0.5389360977306977]
	TIME [epoch: 3.59 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1897940854101409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1897940854101409 | validation: 0.48894523995567474]
	TIME [epoch: 3.59 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2369845428623088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2369845428623088 | validation: 0.4462476913709945]
	TIME [epoch: 3.59 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2393519994500149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2393519994500149 | validation: 0.5473337081963771]
	TIME [epoch: 3.59 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21666335636061426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21666335636061426 | validation: 0.39913868758873783]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14718323352011972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14718323352011972 | validation: 0.4121568543891558]
	TIME [epoch: 3.63 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14157028555893725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14157028555893725 | validation: 0.4575566643810037]
	TIME [epoch: 3.61 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14380856768495962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14380856768495962 | validation: 0.37403712021543134]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_435.pth
	Model improved!!!
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15649385791099188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15649385791099188 | validation: 0.5335311978002267]
	TIME [epoch: 3.59 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23567232798438945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23567232798438945 | validation: 0.4385410588324202]
	TIME [epoch: 3.59 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32541191401146835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32541191401146835 | validation: 0.482294296696957]
	TIME [epoch: 3.58 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18472956952000424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18472956952000424 | validation: 0.41243065791701333]
	TIME [epoch: 3.58 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15580694247247748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15580694247247748 | validation: 0.41978865638779156]
	TIME [epoch: 3.58 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15197768601241954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15197768601241954 | validation: 0.5192555159027782]
	TIME [epoch: 3.58 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16862588299568118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16862588299568118 | validation: 0.4875503310954756]
	TIME [epoch: 3.6 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24066912601955104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24066912601955104 | validation: 0.7360898252158745]
	TIME [epoch: 3.6 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36832244889040355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36832244889040355 | validation: 0.4156169546642885]
	TIME [epoch: 3.61 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16121800391218652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16121800391218652 | validation: 0.4247944027853445]
	TIME [epoch: 3.62 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1455726658593892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1455726658593892 | validation: 0.5319901389084507]
	TIME [epoch: 3.62 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20533624609996207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20533624609996207 | validation: 0.3787704096903098]
	TIME [epoch: 3.6 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31703586603935535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31703586603935535 | validation: 0.6077468940109002]
	TIME [epoch: 3.63 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28023722735716283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28023722735716283 | validation: 0.48467345806497025]
	TIME [epoch: 3.61 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1813192197827744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1813192197827744 | validation: 0.3996031820378181]
	TIME [epoch: 3.62 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18963973928178313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18963973928178313 | validation: 0.46838421246879935]
	TIME [epoch: 3.61 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1639732273635364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1639732273635364 | validation: 0.4494312630931163]
	TIME [epoch: 3.63 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1720742945368157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1720742945368157 | validation: 0.4332372745845836]
	TIME [epoch: 3.62 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1792896415454863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1792896415454863 | validation: 0.4451945459196225]
	TIME [epoch: 3.61 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17409546909664353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17409546909664353 | validation: 0.49420060804997146]
	TIME [epoch: 3.62 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1997487534928376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1997487534928376 | validation: 0.44572314762276355]
	TIME [epoch: 3.63 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18481479863088754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18481479863088754 | validation: 0.4817278734434348]
	TIME [epoch: 3.63 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2022765585473413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2022765585473413 | validation: 0.45329899422544334]
	TIME [epoch: 3.63 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18682145921546522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18682145921546522 | validation: 0.46116824290034275]
	TIME [epoch: 3.62 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16575819435198905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16575819435198905 | validation: 0.39961549119419315]
	TIME [epoch: 3.62 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12574307564874798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12574307564874798 | validation: 0.4152649647724972]
	TIME [epoch: 3.62 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12289852930944008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12289852930944008 | validation: 0.4528411642312591]
	TIME [epoch: 3.63 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1475996429606935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1475996429606935 | validation: 0.5377738563962523]
	TIME [epoch: 3.6 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22480190372361541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22480190372361541 | validation: 0.4734138035933792]
	TIME [epoch: 3.62 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27374130767876187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27374130767876187 | validation: 0.686527679914959]
	TIME [epoch: 3.61 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2708345705616355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2708345705616355 | validation: 0.40682781692399905]
	TIME [epoch: 3.62 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1612966674224341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1612966674224341 | validation: 0.45898931676634336]
	TIME [epoch: 3.61 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15889480430701317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15889480430701317 | validation: 0.47198414418767476]
	TIME [epoch: 3.61 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16059475959348277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16059475959348277 | validation: 0.40635095666288024]
	TIME [epoch: 3.62 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312588967195591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1312588967195591 | validation: 0.4479932900279959]
	TIME [epoch: 3.63 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16923294093364782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16923294093364782 | validation: 0.666548901078342]
	TIME [epoch: 3.61 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4192023230235973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4192023230235973 | validation: 0.6714924680063412]
	TIME [epoch: 3.61 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29990235620819145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29990235620819145 | validation: 0.3752258539513691]
	TIME [epoch: 3.6 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13378702183152577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13378702183152577 | validation: 0.4849517288219435]
	TIME [epoch: 3.63 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24182721722365458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24182721722365458 | validation: 0.4458287144894031]
	TIME [epoch: 3.61 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15548219945987518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15548219945987518 | validation: 0.4785276690308524]
	TIME [epoch: 3.59 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16945577263434872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16945577263434872 | validation: 0.37480290095621305]
	TIME [epoch: 3.61 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13719763734999613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13719763734999613 | validation: 0.3773352851451064]
	TIME [epoch: 3.6 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12123555074177446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12123555074177446 | validation: 0.37047428715316866]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_479.pth
	Model improved!!!
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12239915719078026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12239915719078026 | validation: 0.42119763955329725]
	TIME [epoch: 3.62 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13712084397661298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13712084397661298 | validation: 0.374585378839371]
	TIME [epoch: 3.62 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19379277418814767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19379277418814767 | validation: 0.6591548786434323]
	TIME [epoch: 3.63 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31672805497053524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31672805497053524 | validation: 0.4213096739022018]
	TIME [epoch: 3.61 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20370503378340035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20370503378340035 | validation: 0.3954861248374848]
	TIME [epoch: 3.62 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11493609272580368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11493609272580368 | validation: 0.3914477870359732]
	TIME [epoch: 3.61 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1110751103440265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1110751103440265 | validation: 0.40175539519156056]
	TIME [epoch: 3.61 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12774964092112623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12774964092112623 | validation: 0.4354264562386876]
	TIME [epoch: 3.62 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15866616072826056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15866616072826056 | validation: 0.5654132134550836]
	TIME [epoch: 3.62 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25802543518732923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25802543518732923 | validation: 0.5317924308437239]
	TIME [epoch: 3.61 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28406271171883446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28406271171883446 | validation: 0.578043568389217]
	TIME [epoch: 3.61 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21998022075999685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21998022075999685 | validation: 0.35791696646146104]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1128523634720652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1128523634720652 | validation: 0.36173670553641196]
	TIME [epoch: 3.59 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13631584084185405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13631584084185405 | validation: 0.3714291605954779]
	TIME [epoch: 3.58 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12940487609769505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12940487609769505 | validation: 0.40080665287272554]
	TIME [epoch: 3.6 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11668274606981008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11668274606981008 | validation: 0.377222182975561]
	TIME [epoch: 3.61 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13167781656495933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13167781656495933 | validation: 0.5057655147996067]
	TIME [epoch: 3.6 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18041787238688498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18041787238688498 | validation: 0.4537323102625168]
	TIME [epoch: 3.6 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19906263869601865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19906263869601865 | validation: 0.45532615855082653]
	TIME [epoch: 3.6 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16915639287140125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16915639287140125 | validation: 0.4165233323096913]
	TIME [epoch: 3.6 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12330236217006618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12330236217006618 | validation: 0.3983335489453485]
	TIME [epoch: 3.63 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23717132708583274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23717132708583274 | validation: 0.4870630952882626]
	TIME [epoch: 46.1 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1972033383165264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1972033383165264 | validation: 0.3363438027998387]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11520497366666249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11520497366666249 | validation: 0.38057881490079526]
	TIME [epoch: 7.85 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10944573517288088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10944573517288088 | validation: 0.37775651079927935]
	TIME [epoch: 7.83 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11979998123660326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11979998123660326 | validation: 0.43176842601434046]
	TIME [epoch: 7.84 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14645032627643262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14645032627643262 | validation: 0.42334701314788675]
	TIME [epoch: 7.82 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16342222041298976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16342222041298976 | validation: 0.39119435955996423]
	TIME [epoch: 7.84 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10803544098879904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10803544098879904 | validation: 0.3828131684112894]
	TIME [epoch: 7.83 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11268800176075898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11268800176075898 | validation: 0.45954960531084854]
	TIME [epoch: 7.81 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16351851994494004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16351851994494004 | validation: 0.4269480089078422]
	TIME [epoch: 7.82 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13969211610293886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13969211610293886 | validation: 0.4823336412659403]
	TIME [epoch: 7.8 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16700029429603747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16700029429603747 | validation: 0.5234706255872964]
	TIME [epoch: 7.81 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2800232534223106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2800232534223106 | validation: 0.6971561460770173]
	TIME [epoch: 7.85 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30586665981915634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30586665981915634 | validation: 0.3690047270279404]
	TIME [epoch: 7.85 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16544909846736797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16544909846736797 | validation: 0.4105315133492171]
	TIME [epoch: 7.81 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14010684608536234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14010684608536234 | validation: 0.3624018069844397]
	TIME [epoch: 7.84 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12484041849725674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12484041849725674 | validation: 0.3757383690561139]
	TIME [epoch: 7.8 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12155324104849775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12155324104849775 | validation: 0.3700408942886397]
	TIME [epoch: 7.83 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12229073310219078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12229073310219078 | validation: 0.38920063904484475]
	TIME [epoch: 7.83 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1248688000759811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1248688000759811 | validation: 0.41041550320216696]
	TIME [epoch: 7.82 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11698002212669732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11698002212669732 | validation: 0.40242818220087617]
	TIME [epoch: 7.81 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1325184822209174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1325184822209174 | validation: 0.44048501672140006]
	TIME [epoch: 7.81 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16116280402506775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16116280402506775 | validation: 0.44853904683470297]
	TIME [epoch: 7.83 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1961933872151721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1961933872151721 | validation: 0.4553704228387598]
	TIME [epoch: 7.82 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1536271951004786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1536271951004786 | validation: 0.3704213322899012]
	TIME [epoch: 7.83 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09777011021130189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09777011021130189 | validation: 0.37770423123864794]
	TIME [epoch: 7.84 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08964249573699501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08964249573699501 | validation: 0.38868994507640253]
	TIME [epoch: 7.83 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1077911973867262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1077911973867262 | validation: 0.45559377661070255]
	TIME [epoch: 7.82 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16454402371391388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16454402371391388 | validation: 0.4078828992568938]
	TIME [epoch: 7.82 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21137322054592808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21137322054592808 | validation: 0.5445951440059442]
	TIME [epoch: 7.81 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1956670867759095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1956670867759095 | validation: 0.34191787719441563]
	TIME [epoch: 7.83 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10740976940797341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10740976940797341 | validation: 0.48330382279273465]
	TIME [epoch: 7.81 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18704355460235078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18704355460235078 | validation: 0.442320307937097]
	TIME [epoch: 7.81 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22772521733182263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22772521733182263 | validation: 0.42437235050100297]
	TIME [epoch: 7.8 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17917633007746772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17917633007746772 | validation: 0.39960541460224475]
	TIME [epoch: 7.8 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15276597860579869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15276597860579869 | validation: 0.33880778616956264]
	TIME [epoch: 7.8 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09791259379801158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09791259379801158 | validation: 0.3287540580538936]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08166276604714108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08166276604714108 | validation: 0.36037665729238133]
	TIME [epoch: 7.83 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08548596897432678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08548596897432678 | validation: 0.34131179210788587]
	TIME [epoch: 7.83 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08442173496745646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08442173496745646 | validation: 0.3397382986899776]
	TIME [epoch: 7.79 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11212126437616718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11212126437616718 | validation: 0.39155328581918003]
	TIME [epoch: 7.81 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14673494128592054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14673494128592054 | validation: 0.39036149896912664]
	TIME [epoch: 7.81 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1304530351337031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1304530351337031 | validation: 0.4182098982489577]
	TIME [epoch: 7.8 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20557845116571657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20557845116571657 | validation: 0.6426699537709668]
	TIME [epoch: 7.81 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3048726806368742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3048726806368742 | validation: 0.34636072326891415]
	TIME [epoch: 7.82 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375964924830695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1375964924830695 | validation: 0.381381305986407]
	TIME [epoch: 7.82 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10973412751329135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10973412751329135 | validation: 0.3662887058414826]
	TIME [epoch: 7.8 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10401171853305861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10401171853305861 | validation: 0.3283460312088185]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_548.pth
	Model improved!!!
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09666655054100858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09666655054100858 | validation: 0.3464575280978352]
	TIME [epoch: 7.82 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08391377408692324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08391377408692324 | validation: 0.3163306696263693]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07681567380965058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07681567380965058 | validation: 0.3412363896814814]
	TIME [epoch: 7.84 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08288829851972805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08288829851972805 | validation: 0.374784060929348]
	TIME [epoch: 7.81 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1336010716769309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1336010716769309 | validation: 0.42219926063343555]
	TIME [epoch: 7.81 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19814102147858945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19814102147858945 | validation: 0.3721114059321245]
	TIME [epoch: 7.83 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17096573610411156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17096573610411156 | validation: 0.4239580466828285]
	TIME [epoch: 7.82 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17845646994047795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17845646994047795 | validation: 0.35246962116682096]
	TIME [epoch: 7.82 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13970354017955544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13970354017955544 | validation: 0.3601896258839296]
	TIME [epoch: 7.82 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10806639687152608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10806639687152608 | validation: 0.3099762538233606]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1122368074421103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1122368074421103 | validation: 0.4836573329238412]
	TIME [epoch: 7.8 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14413934679354037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14413934679354037 | validation: 0.3389260986743894]
	TIME [epoch: 7.79 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1511261377145933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1511261377145933 | validation: 0.3827227124134105]
	TIME [epoch: 7.8 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13006084465270223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13006084465270223 | validation: 0.32373086025252307]
	TIME [epoch: 7.78 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10348749774638023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10348749774638023 | validation: 0.29470061644749174]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08625203613899803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08625203613899803 | validation: 0.3229437807220503]
	TIME [epoch: 7.82 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08524660903069778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08524660903069778 | validation: 0.2953378217912942]
	TIME [epoch: 7.8 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0903898145477639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0903898145477639 | validation: 0.3589871608944909]
	TIME [epoch: 7.8 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09779606365362281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09779606365362281 | validation: 0.3116714637633744]
	TIME [epoch: 7.8 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10963147084215823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10963147084215823 | validation: 0.38276781059629195]
	TIME [epoch: 7.8 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11490040689808761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11490040689808761 | validation: 0.2992135368204808]
	TIME [epoch: 7.81 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1275574171855044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1275574171855044 | validation: 0.4002904265395631]
	TIME [epoch: 7.81 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1891723547205375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1891723547205375 | validation: 0.43800437482530685]
	TIME [epoch: 7.81 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18032360065031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18032360065031 | validation: 0.4127304591784194]
	TIME [epoch: 7.81 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23887531635037668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23887531635037668 | validation: 0.569876175998867]
	TIME [epoch: 7.81 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20478245855271812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20478245855271812 | validation: 0.3668314532429007]
	TIME [epoch: 7.78 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1241576108772173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1241576108772173 | validation: 0.29531883171459594]
	TIME [epoch: 7.8 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0964874372988433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0964874372988433 | validation: 0.31876333837916815]
	TIME [epoch: 7.8 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0910731305002361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0910731305002361 | validation: 0.30145482882435726]
	TIME [epoch: 7.83 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07183101310833555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07183101310833555 | validation: 0.2676293095512368]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_578.pth
	Model improved!!!
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06105413363493156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06105413363493156 | validation: 0.27888772383519117]
	TIME [epoch: 7.82 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060847551679579455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060847551679579455 | validation: 0.2654691445109025]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06262474649907881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06262474649907881 | validation: 0.29097615716716935]
	TIME [epoch: 7.84 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07942747308342436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07942747308342436 | validation: 0.32952674702288953]
	TIME [epoch: 7.86 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14791870188732556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14791870188732556 | validation: 0.5733663680823419]
	TIME [epoch: 7.81 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.258400303541228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.258400303541228 | validation: 0.3392310562480564]
	TIME [epoch: 7.84 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18617202714429298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18617202714429298 | validation: 0.3390807960762744]
	TIME [epoch: 7.81 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312887258176503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1312887258176503 | validation: 0.2951255311854409]
	TIME [epoch: 7.83 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11547919269627499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11547919269627499 | validation: 0.2513284968441119]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_587.pth
	Model improved!!!
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09740701436816954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09740701436816954 | validation: 0.27305054068636575]
	TIME [epoch: 7.8 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08247494096474892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08247494096474892 | validation: 0.26504106766033764]
	TIME [epoch: 7.76 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07527417084275827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07527417084275827 | validation: 0.2754567812511645]
	TIME [epoch: 7.79 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07737996048210509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07737996048210509 | validation: 0.27193595621610117]
	TIME [epoch: 7.78 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08165460396754606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08165460396754606 | validation: 0.2809421632353516]
	TIME [epoch: 7.79 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09250937732181855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09250937732181855 | validation: 0.34775703304219235]
	TIME [epoch: 7.78 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1231711929151139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1231711929151139 | validation: 0.31210651141618634]
	TIME [epoch: 7.8 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1716258960059787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1716258960059787 | validation: 0.4956113257267302]
	TIME [epoch: 7.78 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19717794048010603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19717794048010603 | validation: 0.26518762289753156]
	TIME [epoch: 7.78 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10687047620370853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10687047620370853 | validation: 0.24872859378542878]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06337102192277103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06337102192277103 | validation: 0.24078824745117855]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06670347420915836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06670347420915836 | validation: 0.2711194369941296]
	TIME [epoch: 7.84 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07499712673091989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07499712673091989 | validation: 0.2444005877190455]
	TIME [epoch: 7.84 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0926821526739182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0926821526739182 | validation: 0.2906917629834446]
	TIME [epoch: 7.83 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12220835880725109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12220835880725109 | validation: 0.26537794674060045]
	TIME [epoch: 7.83 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1229795764491026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1229795764491026 | validation: 0.2815479595763374]
	TIME [epoch: 7.82 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12179387673811091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12179387673811091 | validation: 0.2180830672606592]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06547106114349065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06547106114349065 | validation: 0.23355868784877704]
	TIME [epoch: 7.83 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07934451665848091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07934451665848091 | validation: 0.26666414208783384]
	TIME [epoch: 7.84 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1263570598146839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1263570598146839 | validation: 0.481566674882704]
	TIME [epoch: 7.83 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2037914114518853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2037914114518853 | validation: 0.2925985677764415]
	TIME [epoch: 7.83 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14074087325523765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14074087325523765 | validation: 0.2770051106976768]
	TIME [epoch: 7.82 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09941612047712906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09941612047712906 | validation: 0.2127042566359784]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07247856675299877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07247856675299877 | validation: 0.253357131727083]
	TIME [epoch: 7.83 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05838641319321194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05838641319321194 | validation: 0.19710584256432]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_612.pth
	Model improved!!!
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05691096687041916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05691096687041916 | validation: 0.3090488935625277]
	TIME [epoch: 7.84 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06281429207313044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06281429207313044 | validation: 0.21334077654289685]
	TIME [epoch: 7.83 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07129474839247682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07129474839247682 | validation: 0.2818238836418555]
	TIME [epoch: 7.81 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09366088573348037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09366088573348037 | validation: 0.2580459752932523]
	TIME [epoch: 7.82 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1336125301340086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1336125301340086 | validation: 0.33581883872395785]
	TIME [epoch: 7.83 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12444149435216244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12444149435216244 | validation: 0.3359932778887364]
	TIME [epoch: 7.83 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22281868972283186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22281868972283186 | validation: 0.4167524977398082]
	TIME [epoch: 7.82 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21615739538022546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21615739538022546 | validation: 0.20936939687441425]
	TIME [epoch: 7.82 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0708231410254652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0708231410254652 | validation: 0.23984483999914208]
	TIME [epoch: 7.82 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07921331549648118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07921331549648118 | validation: 0.1969425158262202]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09401722416306005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09401722416306005 | validation: 0.27389349168706983]
	TIME [epoch: 7.85 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10443055686112805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10443055686112805 | validation: 0.20234139977884447]
	TIME [epoch: 7.85 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07031140390997762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07031140390997762 | validation: 0.21624488083840437]
	TIME [epoch: 7.86 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09515342146381318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09515342146381318 | validation: 0.24868510609466093]
	TIME [epoch: 7.84 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12697257713162297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12697257713162297 | validation: 0.33277804579538894]
	TIME [epoch: 7.83 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11492979712334753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11492979712334753 | validation: 0.2028843460285987]
	TIME [epoch: 7.85 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08607422956819245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08607422956819245 | validation: 0.2570661220274568]
	TIME [epoch: 7.83 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0940022247673026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0940022247673026 | validation: 0.22313594216420296]
	TIME [epoch: 7.84 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08790437979043648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08790437979043648 | validation: 0.2908233043540797]
	TIME [epoch: 7.83 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07779355731486341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07779355731486341 | validation: 0.18702446287558205]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_632.pth
	Model improved!!!
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07029890843715024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07029890843715024 | validation: 0.24209799862354597]
	TIME [epoch: 7.84 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07710678887522687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07710678887522687 | validation: 0.24694908637394458]
	TIME [epoch: 7.85 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1200915513615164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1200915513615164 | validation: 0.22668066878086784]
	TIME [epoch: 7.83 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1421881028446578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1421881028446578 | validation: 0.24167180230056573]
	TIME [epoch: 7.88 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11505030630995754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11505030630995754 | validation: 0.22333865580501477]
	TIME [epoch: 7.83 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06261572203156382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06261572203156382 | validation: 0.162404977030118]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04641435095177811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04641435095177811 | validation: 0.17539776395153642]
	TIME [epoch: 7.84 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045969664037100885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045969664037100885 | validation: 0.18144635585032798]
	TIME [epoch: 7.82 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053662953284368715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053662953284368715 | validation: 0.1575950163151865]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_641.pth
	Model improved!!!
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10277914187114029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10277914187114029 | validation: 0.34682480686334904]
	TIME [epoch: 7.84 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21003738853306275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21003738853306275 | validation: 0.1839241039902121]
	TIME [epoch: 7.85 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10833957212839712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10833957212839712 | validation: 0.16869129237327252]
	TIME [epoch: 7.86 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07223580159269596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07223580159269596 | validation: 0.22664987033576722]
	TIME [epoch: 7.86 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12286829961521838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12286829961521838 | validation: 0.3624858386813471]
	TIME [epoch: 7.85 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11803238922193904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11803238922193904 | validation: 0.23763030799216406]
	TIME [epoch: 7.86 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12193497538702502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12193497538702502 | validation: 0.2927270433307372]
	TIME [epoch: 7.86 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12244962416974467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12244962416974467 | validation: 0.206142137157881]
	TIME [epoch: 7.86 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09199638249857787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09199638249857787 | validation: 0.20664188291441007]
	TIME [epoch: 7.84 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05082937097428234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05082937097428234 | validation: 0.1428550104769717]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04334841484439339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04334841484439339 | validation: 0.17799834806903378]
	TIME [epoch: 7.77 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05170876384327541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05170876384327541 | validation: 0.1797141888714335]
	TIME [epoch: 7.84 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07596226071284376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07596226071284376 | validation: 0.19946448408127182]
	TIME [epoch: 7.79 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10940541150491814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10940541150491814 | validation: 0.1938867648003869]
	TIME [epoch: 7.79 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08822519298613074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08822519298613074 | validation: 0.1806120528683035]
	TIME [epoch: 7.8 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09210217258029736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09210217258029736 | validation: 0.27208846123394553]
	TIME [epoch: 7.84 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1517533167823917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1517533167823917 | validation: 0.2552220386375074]
	TIME [epoch: 7.77 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16940294323186067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16940294323186067 | validation: 0.29152575115935153]
	TIME [epoch: 7.84 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07414308025976568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07414308025976568 | validation: 0.14416320419789777]
	TIME [epoch: 7.83 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06735206197576481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06735206197576481 | validation: 0.19678234285573432]
	TIME [epoch: 7.82 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0816601947323273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0816601947323273 | validation: 0.21370102537953628]
	TIME [epoch: 7.84 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06099634766362131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06099634766362131 | validation: 0.2114205951097847]
	TIME [epoch: 7.87 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07915575376928648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07915575376928648 | validation: 0.22537880613743125]
	TIME [epoch: 7.84 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09796954926953891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09796954926953891 | validation: 0.18017167136402013]
	TIME [epoch: 7.87 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07225856896259608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07225856896259608 | validation: 0.21216596151861672]
	TIME [epoch: 7.84 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05777102661227277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05777102661227277 | validation: 0.1602580404446146]
	TIME [epoch: 7.83 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06513770064360169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06513770064360169 | validation: 0.31355557350714114]
	TIME [epoch: 7.88 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10469889604544122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10469889604544122 | validation: 0.17637308693146347]
	TIME [epoch: 7.88 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10908546228984993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10908546228984993 | validation: 0.22821016558506158]
	TIME [epoch: 7.87 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13238381332146093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13238381332146093 | validation: 0.21235141022552356]
	TIME [epoch: 7.87 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11946346064080364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11946346064080364 | validation: 0.15052855522229125]
	TIME [epoch: 7.86 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09159569750806217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09159569750806217 | validation: 0.19575366236939226]
	TIME [epoch: 7.84 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10458931808388186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10458931808388186 | validation: 0.22036588038913482]
	TIME [epoch: 7.86 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050354812240182074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050354812240182074 | validation: 0.13762900979801793]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055388335775090225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055388335775090225 | validation: 0.22088436559955182]
	TIME [epoch: 7.82 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09736279593420878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09736279593420878 | validation: 0.2421722621748471]
	TIME [epoch: 7.82 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0727237694244019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0727237694244019 | validation: 0.1694107618531552]
	TIME [epoch: 7.82 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056417111661414304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056417111661414304 | validation: 0.16573268933621807]
	TIME [epoch: 7.82 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05522814510506807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05522814510506807 | validation: 0.12440864144504969]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045197822148659215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045197822148659215 | validation: 0.1942252219139876]
	TIME [epoch: 7.83 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0639756379103167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0639756379103167 | validation: 0.16524506248206158]
	TIME [epoch: 7.9 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09783006906941374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09783006906941374 | validation: 0.31532421507082053]
	TIME [epoch: 7.91 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08518028275817166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08518028275817166 | validation: 0.11289812121963294]
	TIME [epoch: 7.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_684.pth
	Model improved!!!
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03925221853374625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03925221853374625 | validation: 0.15818077152510948]
	TIME [epoch: 7.81 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04913378939195989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04913378939195989 | validation: 0.1995686377316152]
	TIME [epoch: 7.77 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12261110364446648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12261110364446648 | validation: 0.31482794872824765]
	TIME [epoch: 7.77 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24997707039177447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24997707039177447 | validation: 0.21670325887163894]
	TIME [epoch: 7.78 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12262362800208707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12262362800208707 | validation: 0.17144557965123083]
	TIME [epoch: 7.79 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08508128015860777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08508128015860777 | validation: 0.17334287527048242]
	TIME [epoch: 7.78 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09431429840745632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09431429840745632 | validation: 0.15444379720039492]
	TIME [epoch: 7.76 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08110942592486216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08110942592486216 | validation: 0.14349285400701972]
	TIME [epoch: 7.78 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052476925948800046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052476925948800046 | validation: 0.1390155262233451]
	TIME [epoch: 7.76 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061793413317854325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061793413317854325 | validation: 0.1555609610160199]
	TIME [epoch: 7.77 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07470482169298365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07470482169298365 | validation: 0.15302738265314303]
	TIME [epoch: 7.79 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09016602179573299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09016602179573299 | validation: 0.2354305633605713]
	TIME [epoch: 7.78 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10643563776979638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10643563776979638 | validation: 0.17246584071499027]
	TIME [epoch: 7.77 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09960092620612794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09960092620612794 | validation: 0.14725233259265713]
	TIME [epoch: 7.77 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05911578239852311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05911578239852311 | validation: 0.12020330941970242]
	TIME [epoch: 7.78 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04743465803592457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04743465803592457 | validation: 0.0997511359135703]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_700.pth
	Model improved!!!
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05280111000770136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05280111000770136 | validation: 0.13491574751932353]
	TIME [epoch: 7.82 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062026629436316844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062026629436316844 | validation: 0.12361058147972544]
	TIME [epoch: 7.8 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058186325351868094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058186325351868094 | validation: 0.13355294693488304]
	TIME [epoch: 7.77 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06461460823796018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06461460823796018 | validation: 0.11287647318758989]
	TIME [epoch: 7.77 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060293224764523215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060293224764523215 | validation: 0.12318102056933232]
	TIME [epoch: 7.76 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0544229647099589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0544229647099589 | validation: 0.116818428640808]
	TIME [epoch: 7.79 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05412444660579375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05412444660579375 | validation: 0.13454083022970617]
	TIME [epoch: 7.78 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0667815949239784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0667815949239784 | validation: 0.1671704591309247]
	TIME [epoch: 7.77 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09066022248152096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09066022248152096 | validation: 0.2447127914253886]
	TIME [epoch: 7.79 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16012050354719398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16012050354719398 | validation: 0.1579443743572064]
	TIME [epoch: 7.77 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054597332938865525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054597332938865525 | validation: 0.14977538139096788]
	TIME [epoch: 7.78 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07776990677669217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07776990677669217 | validation: 0.15833150760372464]
	TIME [epoch: 7.77 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06241455417009123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06241455417009123 | validation: 0.14580770801572152]
	TIME [epoch: 7.79 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04335188115523916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04335188115523916 | validation: 0.21471407797144498]
	TIME [epoch: 7.76 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06942942182485153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06942942182485153 | validation: 0.16648271167717305]
	TIME [epoch: 7.76 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07699165190176623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07699165190176623 | validation: 0.17363600944422233]
	TIME [epoch: 7.76 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09388966903249485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09388966903249485 | validation: 0.14970703213434644]
	TIME [epoch: 7.76 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10408152375440373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10408152375440373 | validation: 0.21640198937266375]
	TIME [epoch: 7.76 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12792286713446863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12792286713446863 | validation: 0.15858115479712423]
	TIME [epoch: 7.79 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10637080777045728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10637080777045728 | validation: 0.18232749350448293]
	TIME [epoch: 7.77 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13490630287114716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13490630287114716 | validation: 0.1387113911313353]
	TIME [epoch: 7.76 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06319614338478377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06319614338478377 | validation: 0.10112475146655744]
	TIME [epoch: 7.77 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026059441740300286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026059441740300286 | validation: 0.08487348764858893]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027749764969615787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027749764969615787 | validation: 0.10029965696073234]
	TIME [epoch: 7.81 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034722087878543965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034722087878543965 | validation: 0.11555679219664775]
	TIME [epoch: 7.82 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06514688455771556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06514688455771556 | validation: 0.15018898291586202]
	TIME [epoch: 7.76 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07873848517479042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07873848517479042 | validation: 0.13899557980687796]
	TIME [epoch: 7.78 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0646286478280598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0646286478280598 | validation: 0.10974359160437037]
	TIME [epoch: 7.77 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0626763402610233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0626763402610233 | validation: 0.15851421010525904]
	TIME [epoch: 7.79 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0737728037039702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0737728037039702 | validation: 0.13163063315262172]
	TIME [epoch: 7.81 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0638208905132682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0638208905132682 | validation: 0.1774246755887695]
	TIME [epoch: 7.82 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07305396400869606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07305396400869606 | validation: 0.16326722972833096]
	TIME [epoch: 7.8 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07084352253872733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07084352253872733 | validation: 0.23575394692365848]
	TIME [epoch: 7.81 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057562329092516705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057562329092516705 | validation: 0.13606959175810612]
	TIME [epoch: 7.78 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05868229458369347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05868229458369347 | validation: 0.1541687688365924]
	TIME [epoch: 7.8 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0957144184360266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0957144184360266 | validation: 0.13565780498696883]
	TIME [epoch: 7.79 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09326036930945378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09326036930945378 | validation: 0.18307733235692727]
	TIME [epoch: 7.81 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10947564465537749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10947564465537749 | validation: 0.14900184408491532]
	TIME [epoch: 7.78 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1190797615889797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1190797615889797 | validation: 0.09426652451527286]
	TIME [epoch: 7.78 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045636917026991596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045636917026991596 | validation: 0.05937452604997076]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_740.pth
	Model improved!!!
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02461146447076773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02461146447076773 | validation: 0.07417227577526893]
	TIME [epoch: 7.79 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02083473034245503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02083473034245503 | validation: 0.06803917320097369]
	TIME [epoch: 7.8 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019426590172221855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019426590172221855 | validation: 0.08390317146317325]
	TIME [epoch: 7.82 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030001024149299664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030001024149299664 | validation: 0.143496183903242]
	TIME [epoch: 7.8 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06792292875291077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06792292875291077 | validation: 0.19608567388805873]
	TIME [epoch: 7.79 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16106746231163654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16106746231163654 | validation: 0.19623388320431948]
	TIME [epoch: 7.77 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1309248028107708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1309248028107708 | validation: 0.1250274769631596]
	TIME [epoch: 7.78 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11519579707854127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11519579707854127 | validation: 0.20029793491115977]
	TIME [epoch: 7.81 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13187895755418502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13187895755418502 | validation: 0.11815077947205171]
	TIME [epoch: 7.82 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056358496768734875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056358496768734875 | validation: 0.0963440546188817]
	TIME [epoch: 7.78 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035944393175441895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035944393175441895 | validation: 0.11721044736109142]
	TIME [epoch: 7.78 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04701533396539672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04701533396539672 | validation: 0.08762093456504799]
	TIME [epoch: 7.77 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040893480568574656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040893480568574656 | validation: 0.10539011462489203]
	TIME [epoch: 7.78 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043637723819795894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043637723819795894 | validation: 0.11552618863521324]
	TIME [epoch: 7.77 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04831464965970256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04831464965970256 | validation: 0.11803886440872208]
	TIME [epoch: 7.8 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05051356569021057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05051356569021057 | validation: 0.11487182600799362]
	TIME [epoch: 7.8 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05641817289418283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05641817289418283 | validation: 0.09886110662025048]
	TIME [epoch: 7.79 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05341272880640626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05341272880640626 | validation: 0.11039705648833205]
	TIME [epoch: 7.79 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06641098050290734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06641098050290734 | validation: 0.15158508815055305]
	TIME [epoch: 7.8 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08757432523257108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08757432523257108 | validation: 0.15008397185376993]
	TIME [epoch: 7.8 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10703227076997764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10703227076997764 | validation: 0.20839613928105705]
	TIME [epoch: 7.8 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15236169330764177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15236169330764177 | validation: 0.1145908081039784]
	TIME [epoch: 7.79 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0824708594482959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0824708594482959 | validation: 0.09733475184317436]
	TIME [epoch: 7.77 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03496123759116505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03496123759116505 | validation: 0.0977180119762291]
	TIME [epoch: 7.77 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0332951023781182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0332951023781182 | validation: 0.06734252384367728]
	TIME [epoch: 7.78 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0338246110138463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0338246110138463 | validation: 0.09505146003950145]
	TIME [epoch: 7.81 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031715215339332026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031715215339332026 | validation: 0.0733836404448219]
	TIME [epoch: 7.82 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029254220513834583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029254220513834583 | validation: 0.09430000040166808]
	TIME [epoch: 7.8 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03411772279291106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03411772279291106 | validation: 0.08241915474844409]
	TIME [epoch: 7.78 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04510938253098453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04510938253098453 | validation: 0.11719215253089313]
	TIME [epoch: 7.82 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06159450738336645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06159450738336645 | validation: 0.10613721681484961]
	TIME [epoch: 7.77 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07008916605182032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07008916605182032 | validation: 0.1781304625704537]
	TIME [epoch: 7.78 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12652385985736647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12652385985736647 | validation: 0.2539014219906597]
	TIME [epoch: 7.79 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15287397444906736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15287397444906736 | validation: 0.15217506889140317]
	TIME [epoch: 7.78 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10847419339936246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10847419339936246 | validation: 0.12722419719352285]
	TIME [epoch: 7.77 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06505788240949416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06505788240949416 | validation: 0.0711173012667164]
	TIME [epoch: 7.79 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05190910733748898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05190910733748898 | validation: 0.09639940135597996]
	TIME [epoch: 7.79 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03118114934994339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03118114934994339 | validation: 0.08136310593199125]
	TIME [epoch: 7.8 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019553546387022332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019553546387022332 | validation: 0.09878466645589529]
	TIME [epoch: 7.83 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02590416239847718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02590416239847718 | validation: 0.09035313743576018]
	TIME [epoch: 7.8 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037625977001775515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037625977001775515 | validation: 0.09633654811690098]
	TIME [epoch: 7.79 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049329931963960426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049329931963960426 | validation: 0.1055848213632889]
	TIME [epoch: 7.81 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07179097241940005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07179097241940005 | validation: 0.15213804107173273]
	TIME [epoch: 7.8 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07971351691474234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07971351691474234 | validation: 0.13783647663672113]
	TIME [epoch: 7.78 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10487846641207092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10487846641207092 | validation: 0.18760231174116795]
	TIME [epoch: 7.79 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12502435042939586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12502435042939586 | validation: 0.13485089661639957]
	TIME [epoch: 7.8 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0740849836658347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0740849836658347 | validation: 0.11625423262222428]
	TIME [epoch: 7.77 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05514725274460361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05514725274460361 | validation: 0.10213900563451367]
	TIME [epoch: 7.76 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05262446990576849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05262446990576849 | validation: 0.0716324282540268]
	TIME [epoch: 7.77 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020867384282770036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020867384282770036 | validation: 0.05783718011361988]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_790.pth
	Model improved!!!
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018367021171752068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018367021171752068 | validation: 0.0705858196213485]
	TIME [epoch: 7.82 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0180887763021677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0180887763021677 | validation: 0.07390620336877746]
	TIME [epoch: 7.81 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01655616330214184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01655616330214184 | validation: 0.06311160772118533]
	TIME [epoch: 7.78 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020248800156236174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020248800156236174 | validation: 0.0880635606219356]
	TIME [epoch: 7.78 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05432639190102771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05432639190102771 | validation: 0.18817887182424095]
	TIME [epoch: 7.77 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15405793373838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15405793373838 | validation: 0.22023710828409687]
	TIME [epoch: 7.79 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15878050994465004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15878050994465004 | validation: 0.12645122150832153]
	TIME [epoch: 7.78 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10057808037172507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10057808037172507 | validation: 0.11006112711955884]
	TIME [epoch: 7.77 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05245210931251508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05245210931251508 | validation: 0.09711050831928963]
	TIME [epoch: 7.76 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046536638860417166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046536638860417166 | validation: 0.11462953172317084]
	TIME [epoch: 7.76 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0635784899960488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0635784899960488 | validation: 0.11294054532952846]
	TIME [epoch: 7.8 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05991674980945304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05991674980945304 | validation: 0.061443972680509065]
	TIME [epoch: 7.77 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03897866655083303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03897866655083303 | validation: 0.09598818374850632]
	TIME [epoch: 7.82 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04349004894885322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04349004894885322 | validation: 0.0752140012991139]
	TIME [epoch: 7.77 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03959431836407414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03959431836407414 | validation: 0.09881886108655331]
	TIME [epoch: 7.79 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04203730231634193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04203730231634193 | validation: 0.08187199421126296]
	TIME [epoch: 7.77 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038679882006817476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038679882006817476 | validation: 0.07361524784678783]
	TIME [epoch: 7.77 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029229720005061743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029229720005061743 | validation: 0.06442728616347503]
	TIME [epoch: 7.79 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03109048782799972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03109048782799972 | validation: 0.09193020238904966]
	TIME [epoch: 7.81 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05013276384913939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05013276384913939 | validation: 0.1063546489339248]
	TIME [epoch: 7.79 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048531561349742625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048531561349742625 | validation: 0.09314061649270522]
	TIME [epoch: 7.78 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047392919896196206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047392919896196206 | validation: 0.10360676129637521]
	TIME [epoch: 7.79 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058524955466684105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058524955466684105 | validation: 0.13326463885361775]
	TIME [epoch: 7.76 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10885953495856043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10885953495856043 | validation: 0.23540666308478905]
	TIME [epoch: 7.78 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1721107282872144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1721107282872144 | validation: 0.12148769346333799]
	TIME [epoch: 7.81 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08514070557710618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08514070557710618 | validation: 0.07604287925924466]
	TIME [epoch: 7.78 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032774434581807135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032774434581807135 | validation: 0.09754681349052832]
	TIME [epoch: 7.77 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038465357644451706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038465357644451706 | validation: 0.08601386885885591]
	TIME [epoch: 7.78 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04433069676746469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04433069676746469 | validation: 0.10004807832700383]
	TIME [epoch: 7.76 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03989313740023499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03989313740023499 | validation: 0.07876789254576316]
	TIME [epoch: 7.77 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04097828033801129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04097828033801129 | validation: 0.08526981263749994]
	TIME [epoch: 7.78 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05924015163675936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05924015163675936 | validation: 0.12364171574819244]
	TIME [epoch: 7.79 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08729555664162693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08729555664162693 | validation: 0.09367908383097266]
	TIME [epoch: 7.77 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03873220037750576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03873220037750576 | validation: 0.05849836614866635]
	TIME [epoch: 7.78 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022285668082777512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022285668082777512 | validation: 0.06380076192902273]
	TIME [epoch: 7.77 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022655723116502113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022655723116502113 | validation: 0.05026772999467142]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022305552546240583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022305552546240583 | validation: 0.07733746365058734]
	TIME [epoch: 7.83 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03682898338135804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03682898338135804 | validation: 0.10190785199226818]
	TIME [epoch: 7.77 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08186371776734572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08186371776734572 | validation: 0.16044667809581525]
	TIME [epoch: 7.78 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1173586610022024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1173586610022024 | validation: 0.10020807598660593]
	TIME [epoch: 7.77 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07051027784091381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07051027784091381 | validation: 0.058759434095723644]
	TIME [epoch: 7.78 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023837396022608988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023837396022608988 | validation: 0.05952564363125612]
	TIME [epoch: 7.8 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022252821763044228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022252821763044228 | validation: 0.08643300455314092]
	TIME [epoch: 7.82 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03608207426182989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03608207426182989 | validation: 0.13560250451204486]
	TIME [epoch: 7.79 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08456650576637634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08456650576637634 | validation: 0.11457012460994864]
	TIME [epoch: 7.81 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08344895700023511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08344895700023511 | validation: 0.08771366491682966]
	TIME [epoch: 7.8 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05099196554415098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05099196554415098 | validation: 0.08115297346852783]
	TIME [epoch: 7.78 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0573471514876953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0573471514876953 | validation: 0.20208271460669175]
	TIME [epoch: 7.77 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13025749246341306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13025749246341306 | validation: 0.16787261423481703]
	TIME [epoch: 7.81 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13288197689965778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13288197689965778 | validation: 0.10127891553655313]
	TIME [epoch: 7.78 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0516910395617586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0516910395617586 | validation: 0.07199477400116017]
	TIME [epoch: 7.78 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0159365855718944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0159365855718944 | validation: 0.05189816744916387]
	TIME [epoch: 7.77 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019310327819076576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019310327819076576 | validation: 0.06717250919134753]
	TIME [epoch: 7.77 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024547498076153643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024547498076153643 | validation: 0.07244668371444256]
	TIME [epoch: 7.77 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025834821366436153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025834821366436153 | validation: 0.06987004905171128]
	TIME [epoch: 7.8 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0306398100853922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0306398100853922 | validation: 0.06740731942684101]
	TIME [epoch: 7.81 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04047428720860472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04047428720860472 | validation: 0.08599291669362802]
	TIME [epoch: 7.82 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07012836061058428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07012836061058428 | validation: 0.12349875157729993]
	TIME [epoch: 7.82 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08029065459520641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08029065459520641 | validation: 0.10268096205580635]
	TIME [epoch: 7.82 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04320287302559118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04320287302559118 | validation: 0.05559066015907145]
	TIME [epoch: 7.82 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019751537829370194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019751537829370194 | validation: 0.07120810052778997]
	TIME [epoch: 7.85 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02231746622570718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02231746622570718 | validation: 0.06573916479973647]
	TIME [epoch: 7.81 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03202184972342736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03202184972342736 | validation: 0.09108119452596312]
	TIME [epoch: 7.82 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05907695959168256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05907695959168256 | validation: 0.14151171203190946]
	TIME [epoch: 7.81 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10012413707269467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10012413707269467 | validation: 0.1477220370661724]
	TIME [epoch: 7.81 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13067007959646407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13067007959646407 | validation: 0.17225268675056538]
	TIME [epoch: 7.82 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13255729740475503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13255729740475503 | validation: 0.07745160181355738]
	TIME [epoch: 7.83 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05530808019636916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05530808019636916 | validation: 0.06935615268547846]
	TIME [epoch: 7.81 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02159991915654932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02159991915654932 | validation: 0.06718765688199949]
	TIME [epoch: 7.82 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02819534994607012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02819534994607012 | validation: 0.0919390024765409]
	TIME [epoch: 7.81 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04008065923181522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04008065923181522 | validation: 0.10819624346467577]
	TIME [epoch: 7.82 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042925416351221546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042925416351221546 | validation: 0.06965659365549219]
	TIME [epoch: 7.83 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04286356668403598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04286356668403598 | validation: 0.07544446266755706]
	TIME [epoch: 7.85 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04186348725158709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04186348725158709 | validation: 0.08032014186353822]
	TIME [epoch: 7.8 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04459401369832065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04459401369832065 | validation: 0.09643704143202594]
	TIME [epoch: 7.82 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03980042562975067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03980042562975067 | validation: 0.06502010653566302]
	TIME [epoch: 7.8 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038128679303707474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038128679303707474 | validation: 0.08099417834056152]
	TIME [epoch: 7.82 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0436964614144426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0436964614144426 | validation: 0.08277387923319028]
	TIME [epoch: 7.81 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05813674228931452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05813674228931452 | validation: 0.13507296493866572]
	TIME [epoch: 7.85 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07798403983819634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07798403983819634 | validation: 0.11403253283404902]
	TIME [epoch: 7.81 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08747028282451119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08747028282451119 | validation: 0.10310324193722648]
	TIME [epoch: 7.83 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07008690911895243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07008690911895243 | validation: 0.1112175648991832]
	TIME [epoch: 7.81 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06800170576374864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06800170576374864 | validation: 0.08158626708694931]
	TIME [epoch: 7.81 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05258949484163211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05258949484163211 | validation: 0.061989603629250906]
	TIME [epoch: 7.81 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02824588169315928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02824588169315928 | validation: 0.05037846971572987]
	TIME [epoch: 7.86 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026631411520157765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026631411520157765 | validation: 0.06726397498839151]
	TIME [epoch: 7.83 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03358817193814979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03358817193814979 | validation: 0.07736205069966741]
	TIME [epoch: 7.85 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04895333230664229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04895333230664229 | validation: 0.10558523573187908]
	TIME [epoch: 7.81 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06500391984207163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06500391984207163 | validation: 0.08374075361130945]
	TIME [epoch: 7.83 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055651629733410125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055651629733410125 | validation: 0.06982283011821779]
	TIME [epoch: 7.84 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03381216129107229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03381216129107229 | validation: 0.07080117060256648]
	TIME [epoch: 7.84 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030452479460299966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030452479460299966 | validation: 0.07600112720844904]
	TIME [epoch: 7.8 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04376317296412533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04376317296412533 | validation: 0.10887585303085734]
	TIME [epoch: 7.8 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06478590593808198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06478590593808198 | validation: 0.0758468315392535]
	TIME [epoch: 7.79 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04701443531101782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04701443531101782 | validation: 0.08828702381644632]
	TIME [epoch: 7.82 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045791636494665944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045791636494665944 | validation: 0.0925834844519926]
	TIME [epoch: 7.82 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05925501134103941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05925501134103941 | validation: 0.1061782587225209]
	TIME [epoch: 7.84 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061376429376255634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061376429376255634 | validation: 0.12155778697509295]
	TIME [epoch: 7.81 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06911065810833829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06911065810833829 | validation: 0.11156327054825797]
	TIME [epoch: 7.81 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0741226678404189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0741226678404189 | validation: 0.09791163742282664]
	TIME [epoch: 7.81 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05092339931975969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05092339931975969 | validation: 0.06026910850979092]
	TIME [epoch: 7.82 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03159841780683469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03159841780683469 | validation: 0.057049001166509454]
	TIME [epoch: 7.82 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019845243739619812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019845243739619812 | validation: 0.04768435465812874]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_893.pth
	Model improved!!!
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013318004406842228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013318004406842228 | validation: 0.050968347310181294]
	TIME [epoch: 7.82 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013606030927317287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013606030927317287 | validation: 0.05468811647132347]
	TIME [epoch: 7.86 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020848201004048092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020848201004048092 | validation: 0.08169335444133319]
	TIME [epoch: 7.85 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04764824363190421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04764824363190421 | validation: 0.14497042308504723]
	TIME [epoch: 7.8 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10937607963906515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10937607963906515 | validation: 0.13409888053812447]
	TIME [epoch: 7.8 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12437171840433524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12437171840433524 | validation: 0.12197200528083929]
	TIME [epoch: 7.86 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08977649560274677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08977649560274677 | validation: 0.06074500755106345]
	TIME [epoch: 7.81 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038433024082596585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038433024082596585 | validation: 0.05129187101060239]
	TIME [epoch: 7.82 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01498662584310203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01498662584310203 | validation: 0.04628692943498756]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_902.pth
	Model improved!!!
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012983783919569756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012983783919569756 | validation: 0.04788965794011263]
	TIME [epoch: 7.8 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022763431127408112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022763431127408112 | validation: 0.09215108623372026]
	TIME [epoch: 7.79 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05064532419071007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05064532419071007 | validation: 0.1386713274912248]
	TIME [epoch: 7.82 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10332340563740612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10332340563740612 | validation: 0.12535105282903786]
	TIME [epoch: 7.78 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09240362081930935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09240362081930935 | validation: 0.07390128732744491]
	TIME [epoch: 7.8 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03836292001023416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03836292001023416 | validation: 0.05561723240012825]
	TIME [epoch: 7.78 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02187852641888523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02187852641888523 | validation: 0.05227439812366163]
	TIME [epoch: 7.77 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02905330314411289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02905330314411289 | validation: 0.07847514663800922]
	TIME [epoch: 7.83 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04519865203047826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04519865203047826 | validation: 0.08403881234566153]
	TIME [epoch: 7.81 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05104748579512186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05104748579512186 | validation: 0.07790268590930034]
	TIME [epoch: 7.78 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04770576290311884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04770576290311884 | validation: 0.08504953859394991]
	TIME [epoch: 7.77 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04790762309357991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04790762309357991 | validation: 0.1061527848089924]
	TIME [epoch: 7.81 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07283719678520904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07283719678520904 | validation: 0.10715303663363572]
	TIME [epoch: 7.8 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07067978622322771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07067978622322771 | validation: 0.060671007246393585]
	TIME [epoch: 7.81 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06190721311726414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06190721311726414 | validation: 0.10022739823535254]
	TIME [epoch: 7.84 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03299832900742682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03299832900742682 | validation: 0.0781794078008221]
	TIME [epoch: 7.8 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01465132001286812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01465132001286812 | validation: 0.04726366527863405]
	TIME [epoch: 7.81 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017232766011455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017232766011455 | validation: 0.0837127841151268]
	TIME [epoch: 7.8 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026271354083503527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026271354083503527 | validation: 0.057270697230943884]
	TIME [epoch: 7.82 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032821242506550566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032821242506550566 | validation: 0.09425256868705431]
	TIME [epoch: 7.81 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05675291412305863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05675291412305863 | validation: 0.0933756944465838]
	TIME [epoch: 7.84 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06320073855423193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06320073855423193 | validation: 0.07068760766022608]
	TIME [epoch: 7.8 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0483868132599007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0483868132599007 | validation: 0.12865163405631716]
	TIME [epoch: 7.83 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07964891421881341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07964891421881341 | validation: 0.09794300876736206]
	TIME [epoch: 7.79 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07939172796708129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07939172796708129 | validation: 0.08081124192841382]
	TIME [epoch: 7.8 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039840071710298776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039840071710298776 | validation: 0.054714963043876366]
	TIME [epoch: 7.81 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026405063643044038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026405063643044038 | validation: 0.06664933797235424]
	TIME [epoch: 7.79 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04147591984089626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04147591984089626 | validation: 0.10120190695264125]
	TIME [epoch: 7.8 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06850434812795077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06850434812795077 | validation: 0.09636263488087365]
	TIME [epoch: 7.81 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0752083017391294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0752083017391294 | validation: 0.09192406915702711]
	TIME [epoch: 7.81 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051358942989460366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051358942989460366 | validation: 0.062314429219602445]
	TIME [epoch: 7.82 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02793132966216552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02793132966216552 | validation: 0.04850235098446235]
	TIME [epoch: 7.81 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01862947281605866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01862947281605866 | validation: 0.05518673874060012]
	TIME [epoch: 7.85 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026038782063928175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026038782063928175 | validation: 0.07982913760034119]
	TIME [epoch: 7.82 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0335582893492946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0335582893492946 | validation: 0.07654544321805189]
	TIME [epoch: 7.82 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04128209034815645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04128209034815645 | validation: 0.0817495979683704]
	TIME [epoch: 7.81 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045086533834167185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045086533834167185 | validation: 0.08164801877748974]
	TIME [epoch: 7.79 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05780227334419984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05780227334419984 | validation: 0.10851252964916372]
	TIME [epoch: 7.8 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06504066621747592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06504066621747592 | validation: 0.0751804849517463]
	TIME [epoch: 7.85 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046950865279529415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046950865279529415 | validation: 0.10186304310336425]
	TIME [epoch: 7.81 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040790037620445516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040790037620445516 | validation: 0.06374973651289628]
	TIME [epoch: 7.82 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028390076471285153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028390076471285153 | validation: 0.0654274575886869]
	TIME [epoch: 7.83 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01728567889943886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01728567889943886 | validation: 0.04342206239037123]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_945.pth
	Model improved!!!
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014462118625953107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014462118625953107 | validation: 0.05730696639741748]
	TIME [epoch: 7.84 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01773630841179981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01773630841179981 | validation: 0.05914821088022883]
	TIME [epoch: 7.87 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03381238591649747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03381238591649747 | validation: 0.1038181488934539]
	TIME [epoch: 7.86 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06945793968256576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06945793968256576 | validation: 0.1297947912519821]
	TIME [epoch: 7.83 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09641901236375904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09641901236375904 | validation: 0.1147082527092273]
	TIME [epoch: 7.88 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09952238650783712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09952238650783712 | validation: 0.14407403497593305]
	TIME [epoch: 7.86 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10995464596107972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10995464596107972 | validation: 0.0778307973565426]
	TIME [epoch: 7.88 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0464529304828775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0464529304828775 | validation: 0.09099947666251297]
	TIME [epoch: 7.89 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020781067182910222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020781067182910222 | validation: 0.06085069147361241]
	TIME [epoch: 7.9 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01461528404905943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01461528404905943 | validation: 0.03986575678859116]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_955.pth
	Model improved!!!
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019047305554430155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019047305554430155 | validation: 0.06914057559041652]
	TIME [epoch: 7.87 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028080139376898582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028080139376898582 | validation: 0.060144473864031094]
	TIME [epoch: 7.79 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030531162594546857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030531162594546857 | validation: 0.08934650534071109]
	TIME [epoch: 7.86 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05595866372114847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05595866372114847 | validation: 0.13267828378140098]
	TIME [epoch: 7.79 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09638243654063597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09638243654063597 | validation: 0.13061379029782302]
	TIME [epoch: 7.8 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08864896760862781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08864896760862781 | validation: 0.05825078988038234]
	TIME [epoch: 7.82 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0324745915293337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0324745915293337 | validation: 0.04189891814292092]
	TIME [epoch: 7.81 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009777599712621144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009777599712621144 | validation: 0.04393468787118644]
	TIME [epoch: 7.8 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014466243136243907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014466243136243907 | validation: 0.044142875325777535]
	TIME [epoch: 7.8 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018572351939052906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018572351939052906 | validation: 0.06736939041605626]
	TIME [epoch: 7.82 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027946307346861134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027946307346861134 | validation: 0.08381859283777081]
	TIME [epoch: 7.79 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06612152906544466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06612152906544466 | validation: 0.15831523106368392]
	TIME [epoch: 7.77 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13335976139562009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13335976139562009 | validation: 0.1031633761987914]
	TIME [epoch: 7.79 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08576884879018126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08576884879018126 | validation: 0.0551341678872938]
	TIME [epoch: 7.8 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01840861898228671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01840861898228671 | validation: 0.03201557607654613]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_970.pth
	Model improved!!!
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008371802935339283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008371802935339283 | validation: 0.050748676933594306]
	TIME [epoch: 7.82 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015433106615238081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015433106615238081 | validation: 0.05337791699202258]
	TIME [epoch: 7.81 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026996628282692815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026996628282692815 | validation: 0.06746303623499415]
	TIME [epoch: 7.77 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050661724628149996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050661724628149996 | validation: 0.07087732827516806]
	TIME [epoch: 7.78 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04205114233135597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04205114233135597 | validation: 0.07570918369185979]
	TIME [epoch: 7.77 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045932564529986036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045932564529986036 | validation: 0.08409800095029106]
	TIME [epoch: 7.78 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03745292716986802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03745292716986802 | validation: 0.060777462258584186]
	TIME [epoch: 7.79 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03473605840111763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03473605840111763 | validation: 0.10748201627731065]
	TIME [epoch: 7.77 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06754264068466916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06754264068466916 | validation: 0.09102583219118535]
	TIME [epoch: 7.77 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08979068879635464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08979068879635464 | validation: 0.08126239849129045]
	TIME [epoch: 7.78 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052045414032585086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052045414032585086 | validation: 0.05477946350660462]
	TIME [epoch: 7.79 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026788096830354897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026788096830354897 | validation: 0.07009024719129839]
	TIME [epoch: 7.78 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042803460359164096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042803460359164096 | validation: 0.12739383048784636]
	TIME [epoch: 7.8 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06978777187771706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06978777187771706 | validation: 0.09983647533832635]
	TIME [epoch: 7.78 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06110047665063295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06110047665063295 | validation: 0.07690440303618037]
	TIME [epoch: 7.79 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042003902667179585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042003902667179585 | validation: 0.055624641810498754]
	TIME [epoch: 7.79 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03735987873579285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03735987873579285 | validation: 0.04685496108180505]
	TIME [epoch: 7.79 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019166530685746638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019166530685746638 | validation: 0.048643762140289294]
	TIME [epoch: 7.81 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01967116915755796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01967116915755796 | validation: 0.05852059238402666]
	TIME [epoch: 7.85 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03108166032542468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03108166032542468 | validation: 0.08894107049364307]
	TIME [epoch: 7.8 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05089487951620319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05089487951620319 | validation: 0.0781593857887008]
	TIME [epoch: 7.82 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04891882649067746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04891882649067746 | validation: 0.07006105270297865]
	TIME [epoch: 7.81 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04565597864054951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04565597864054951 | validation: 0.09015234185918107]
	TIME [epoch: 7.82 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05709237319488459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05709237319488459 | validation: 0.0680184741149222]
	TIME [epoch: 7.8 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054774523797105845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054774523797105845 | validation: 0.07475620231103701]
	TIME [epoch: 7.85 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03954500201559213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03954500201559213 | validation: 0.061598438117344105]
	TIME [epoch: 7.81 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03551538204876029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03551538204876029 | validation: 0.07457462648563327]
	TIME [epoch: 7.81 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0418072368271598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0418072368271598 | validation: 0.07337448013657842]
	TIME [epoch: 7.83 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04549531065529281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04549531065529281 | validation: 0.06900161873433222]
	TIME [epoch: 7.81 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03966860130820108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03966860130820108 | validation: 0.06544008277421845]
	TIME [epoch: 7.81 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02923798793769197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02923798793769197 | validation: 0.04727061339960162]
	TIME [epoch: 54.8 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024457800254645088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024457800254645088 | validation: 0.049484667545823494]
	TIME [epoch: 16.7 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021766081333613338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021766081333613338 | validation: 0.08076353108259593]
	TIME [epoch: 16.6 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03840804142304875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03840804142304875 | validation: 0.0757782743960976]
	TIME [epoch: 16.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039316599782750185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039316599782750185 | validation: 0.09220982627569228]
	TIME [epoch: 16.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057694270205982334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057694270205982334 | validation: 0.17562097388542056]
	TIME [epoch: 16.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10109672727673708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10109672727673708 | validation: 0.12058036833275754]
	TIME [epoch: 16.6 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08990110756425164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08990110756425164 | validation: 0.07880802557093278]
	TIME [epoch: 16.6 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04434817048889938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04434817048889938 | validation: 0.06643548091021917]
	TIME [epoch: 16.6 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021574065723678024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021574065723678024 | validation: 0.05481614841176152]
	TIME [epoch: 16.6 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02022974771914657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02022974771914657 | validation: 0.059034885444534224]
	TIME [epoch: 16.6 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022904924999028957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022904924999028957 | validation: 0.06019516017468536]
	TIME [epoch: 16.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0313259211668193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0313259211668193 | validation: 0.07669179598818658]
	TIME [epoch: 16.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03672514477700409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03672514477700409 | validation: 0.06318779842776492]
	TIME [epoch: 16.6 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04054828473316725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04054828473316725 | validation: 0.06984924192418981]
	TIME [epoch: 16.6 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03743146338415939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03743146338415939 | validation: 0.052476163563669215]
	TIME [epoch: 16.6 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02475256636641185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02475256636641185 | validation: 0.05173266011106653]
	TIME [epoch: 16.6 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021171284789018127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021171284789018127 | validation: 0.05389248182138765]
	TIME [epoch: 16.6 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027951355926328924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027951355926328924 | validation: 0.05770768635242368]
	TIME [epoch: 16.6 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03620063400235127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03620063400235127 | validation: 0.07417198615837976]
	TIME [epoch: 16.6 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04885765703983991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04885765703983991 | validation: 0.07667181354036537]
	TIME [epoch: 16.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04829077976401735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04829077976401735 | validation: 0.058009977118858386]
	TIME [epoch: 16.6 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039708498426375284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039708498426375284 | validation: 0.06258734211593796]
	TIME [epoch: 16.6 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034087626641834116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034087626641834116 | validation: 0.08413756680498558]
	TIME [epoch: 16.6 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04054980544914523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04054980544914523 | validation: 0.09381023658284852]
	TIME [epoch: 16.6 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0641131052556138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0641131052556138 | validation: 0.14240678883610972]
	TIME [epoch: 16.6 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08633597195407246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08633597195407246 | validation: 0.13440177527626074]
	TIME [epoch: 16.7 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05715245381725473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05715245381725473 | validation: 0.06531026169134314]
	TIME [epoch: 16.6 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03238721928467387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03238721928467387 | validation: 0.061791270326728834]
	TIME [epoch: 16.7 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02653148683094442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02653148683094442 | validation: 0.07823117978331012]
	TIME [epoch: 16.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02390426004213071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02390426004213071 | validation: 0.059125321946518816]
	TIME [epoch: 16.6 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023567526002545757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023567526002545757 | validation: 0.06173212527358877]
	TIME [epoch: 16.6 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038469325996962005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038469325996962005 | validation: 0.10976425995896713]
	TIME [epoch: 16.7 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07952509801964769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07952509801964769 | validation: 0.10635226012839803]
	TIME [epoch: 16.6 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08861636008463389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08861636008463389 | validation: 0.07520984911932638]
	TIME [epoch: 16.6 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04030288618400808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04030288618400808 | validation: 0.0456452458388714]
	TIME [epoch: 16.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01574405702104931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01574405702104931 | validation: 0.04117797190441319]
	TIME [epoch: 16.7 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009814617784345113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009814617784345113 | validation: 0.036892163978896456]
	TIME [epoch: 16.6 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014007169501053967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014007169501053967 | validation: 0.04757361094302155]
	TIME [epoch: 16.6 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02388302607646491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02388302607646491 | validation: 0.07655458761839969]
	TIME [epoch: 16.6 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04513305265646671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04513305265646671 | validation: 0.11459961862324075]
	TIME [epoch: 16.7 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08594379555853557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08594379555853557 | validation: 0.13128729843017878]
	TIME [epoch: 16.6 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07924329752852964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07924329752852964 | validation: 0.06363304648928787]
	TIME [epoch: 16.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028198513730300804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028198513730300804 | validation: 0.039474598661470066]
	TIME [epoch: 16.6 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0314944814135811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0314944814135811 | validation: 0.07210655348034116]
	TIME [epoch: 16.6 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04916647614554471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04916647614554471 | validation: 0.06377242877357141]
	TIME [epoch: 16.6 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042032147533011044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042032147533011044 | validation: 0.07959647049449822]
	TIME [epoch: 16.6 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051829513620287475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051829513620287475 | validation: 0.08450819132148896]
	TIME [epoch: 16.6 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06801417584126475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06801417584126475 | validation: 0.07978998080837706]
	TIME [epoch: 16.6 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047911968044209365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047911968044209365 | validation: 0.045853826243670286]
	TIME [epoch: 16.6 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017175838465703638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017175838465703638 | validation: 0.03387311654806387]
	TIME [epoch: 16.7 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007333653164055123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.007333653164055123 | validation: 0.038371939292798835]
	TIME [epoch: 16.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0076588489235153955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0076588489235153955 | validation: 0.04024834453524839]
	TIME [epoch: 16.6 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01058498579935616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01058498579935616 | validation: 0.044052849855034826]
	TIME [epoch: 16.7 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017874809363341137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017874809363341137 | validation: 0.0769563095071229]
	TIME [epoch: 16.6 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03711763955355281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03711763955355281 | validation: 0.07198789783621352]
	TIME [epoch: 16.6 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049867862331509374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049867862331509374 | validation: 0.07188872908847342]
	TIME [epoch: 16.7 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043385874744706426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043385874744706426 | validation: 0.08184018833428673]
	TIME [epoch: 16.6 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06847204775184418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06847204775184418 | validation: 0.14034880844138517]
	TIME [epoch: 16.6 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10353949037804322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10353949037804322 | validation: 0.06846721983683997]
	TIME [epoch: 16.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04716639758277031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04716639758277031 | validation: 0.03422109235532977]
	TIME [epoch: 16.6 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014831291290194715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014831291290194715 | validation: 0.05083318840581718]
	TIME [epoch: 16.6 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018574682944777775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018574682944777775 | validation: 0.07748952989968873]
	TIME [epoch: 16.6 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0483763211006097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0483763211006097 | validation: 0.12298985296630323]
	TIME [epoch: 16.6 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09028061219740632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09028061219740632 | validation: 0.07802388820651196]
	TIME [epoch: 16.6 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06490783974512757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06490783974512757 | validation: 0.06473234473373965]
	TIME [epoch: 16.6 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03851565526553221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03851565526553221 | validation: 0.06715628101916322]
	TIME [epoch: 16.6 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01673759772464289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01673759772464289 | validation: 0.04017287029683889]
	TIME [epoch: 16.6 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007554837243790369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.007554837243790369 | validation: 0.04466485309262963]
	TIME [epoch: 16.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008786340747413966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008786340747413966 | validation: 0.03642358316703003]
	TIME [epoch: 16.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012989713987785836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012989713987785836 | validation: 0.046754541914475804]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144616/states/model_phi1_4b_v_mmd1_1071.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6796.642 seconds.
