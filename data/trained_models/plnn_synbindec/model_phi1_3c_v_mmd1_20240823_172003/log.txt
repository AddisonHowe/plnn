Args:
Namespace(name='model_phi1_3c_v_mmd1', outdir='out/model_training/model_phi1_3c_v_mmd1', training_data='data/training_data/data_phi1_3c/training', validation_data='data/training_data/data_phi1_3c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3039831597

Training model...

Saving initial model state to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.750618343469283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.750618343469283 | validation: 6.225377499505475]
	TIME [epoch: 29.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.002411941479846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.002411941479846 | validation: 6.32009224252146]
	TIME [epoch: 3.8 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.1912644274549375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1912644274549375 | validation: 6.033381580053739]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.0818513648710315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0818513648710315 | validation: 5.255710134925691]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.147949910831752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.147949910831752 | validation: 5.57949104958548]
	TIME [epoch: 3.78 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.2514212010128745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2514212010128745 | validation: 5.14237375689405]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.84908047651642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.84908047651642 | validation: 5.275874524099881]
	TIME [epoch: 3.8 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.145566321457055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.145566321457055 | validation: 5.164363116902367]
	TIME [epoch: 3.8 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.902907773754842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.902907773754842 | validation: 4.935274482136586]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.637993613980843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.637993613980843 | validation: 4.939480136183103]
	TIME [epoch: 3.81 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.653473830617482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.653473830617482 | validation: 4.815254952385701]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.537109874200936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.537109874200936 | validation: 4.795824474627844]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.532059717232886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.532059717232886 | validation: 4.762909524932031]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.476893630211789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.476893630211789 | validation: 4.728292022719842]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.446189162127699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.446189162127699 | validation: 4.67075648089769]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.419379691849402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.419379691849402 | validation: 4.663350031142186]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.385669051727127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.385669051727127 | validation: 4.652502360994553]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.385196092839616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.385196092839616 | validation: 4.640193356965449]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.35411435372109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.35411435372109 | validation: 4.559303497448912]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3173885196277775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3173885196277775 | validation: 4.589394478621052]
	TIME [epoch: 3.8 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.299045669900202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.299045669900202 | validation: 4.535840720435037]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.282776894326959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.282776894326959 | validation: 4.567364530696575]
	TIME [epoch: 3.82 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.271347056860994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.271347056860994 | validation: 4.4801211505762675]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.243449354848053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.243449354848053 | validation: 4.52620318371019]
	TIME [epoch: 3.81 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.2306829911862165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2306829911862165 | validation: 4.440161748173567]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.196330146425267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.196330146425267 | validation: 4.4655633994971184]
	TIME [epoch: 3.81 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.169965007584061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.169965007584061 | validation: 4.39060112523564]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.143636699432436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.143636699432436 | validation: 4.4302779299375565]
	TIME [epoch: 3.8 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.131927345258307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.131927345258307 | validation: 4.3591151217823025]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1114248680452725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1114248680452725 | validation: 4.414220966004609]
	TIME [epoch: 3.79 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.11103813513778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.11103813513778 | validation: 4.333264647701981]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.089235012549289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.089235012549289 | validation: 4.3838163438088005]
	TIME [epoch: 3.8 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.079336303213365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.079336303213365 | validation: 4.287832622691471]
	TIME [epoch: 3.82 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.039634997683554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.039634997683554 | validation: 4.311615637390935]
	TIME [epoch: 3.8 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.020601751509368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.020601751509368 | validation: 4.24966851049808]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999875712377627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.999875712377627 | validation: 4.286253519328354]
	TIME [epoch: 3.79 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.989891075010948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.989891075010948 | validation: 4.222042971603382]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9699720577504323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9699720577504323 | validation: 4.257938645310541]
	TIME [epoch: 3.8 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.964350307157601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.964350307157601 | validation: 4.195823078746853]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9437529088214456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9437529088214456 | validation: 4.227742627073316]
	TIME [epoch: 3.79 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.934994957906362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.934994957906362 | validation: 4.155854307919781]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.908120022139872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.908120022139872 | validation: 4.17808353694042]
	TIME [epoch: 3.79 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8875491716092516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8875491716092516 | validation: 4.118118322209416]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8603367465567398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8603367465567398 | validation: 4.127950760462593]
	TIME [epoch: 3.8 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.846833020333253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.846833020333253 | validation: 4.097115892998221]
	TIME [epoch: 3.82 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8341564956136738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8341564956136738 | validation: 4.114069301937973]
	TIME [epoch: 3.8 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.829196421173975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.829196421173975 | validation: 4.081203791380058]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8160667930493632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8160667930493632 | validation: 4.108716312576924]
	TIME [epoch: 3.8 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8164446171027557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8164446171027557 | validation: 4.037939660677136]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7842118716305646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7842118716305646 | validation: 4.038219047936557]
	TIME [epoch: 3.79 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7614191132463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7614191132463 | validation: 3.9905231651618998]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7325172312260326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7325172312260326 | validation: 3.987660784462812]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.706851214310626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.706851214310626 | validation: 3.948942558249394]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.690503693572584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.690503693572584 | validation: 3.9431972787377307]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.668334773719866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.668334773719866 | validation: 3.8886998331967906]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.638908642762024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.638908642762024 | validation: 3.7368474574527015]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.505110417619675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.505110417619675 | validation: 5.791489335011701]
	TIME [epoch: 3.82 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.715453620029416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.715453620029416 | validation: 3.778458915240952]
	TIME [epoch: 3.81 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5296341594531406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5296341594531406 | validation: 3.8587357963873377]
	TIME [epoch: 3.8 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.599225092809706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.599225092809706 | validation: 3.6648894151273677]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4348477443821515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4348477443821515 | validation: 3.8798185442362496]
	TIME [epoch: 3.8 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5748047856587735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5748047856587735 | validation: 3.5448562060174496]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.326490873284706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.326490873284706 | validation: 3.548600949635406]
	TIME [epoch: 3.78 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3467286343047524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3467286343047524 | validation: 3.6072714185507664]
	TIME [epoch: 3.78 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.357175848026992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.357175848026992 | validation: 3.5131814860420194]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.288596003433796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.288596003433796 | validation: 3.4879166972091937]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.272288897012265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.272288897012265 | validation: 3.476119792987674]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2608064775936696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2608064775936696 | validation: 3.4607631590028123]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2414858156037476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2414858156037476 | validation: 3.493262163605673]
	TIME [epoch: 3.81 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2565087080512227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2565087080512227 | validation: 3.5176142506904826]
	TIME [epoch: 3.8 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.315196194483898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.315196194483898 | validation: 3.689429434791228]
	TIME [epoch: 3.81 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.42079841019675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.42079841019675 | validation: 3.443937707302712]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2317073238730565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2317073238730565 | validation: 3.3925142134659683]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.183533426720724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.183533426720724 | validation: 3.4327171254772733]
	TIME [epoch: 3.8 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.189821190073285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.189821190073285 | validation: 3.4422872856056403]
	TIME [epoch: 3.8 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2341090851393766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2341090851393766 | validation: 3.60068002035096]
	TIME [epoch: 3.8 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3398995408876404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3398995408876404 | validation: 3.4166413524977415]
	TIME [epoch: 3.8 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.209760575304407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.209760575304407 | validation: 3.3869790713821035]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1511291223563496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1511291223563496 | validation: 3.3621565849573822]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1366366189270525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1366366189270525 | validation: 3.348414790165737]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.125778755409614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.125778755409614 | validation: 3.3706111489661676]
	TIME [epoch: 3.79 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.163989479712673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.163989479712673 | validation: 3.5960123286933077]
	TIME [epoch: 3.78 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3398090165278758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3398090165278758 | validation: 3.4588526110186137]
	TIME [epoch: 3.78 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.249184834666753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.249184834666753 | validation: 3.3739711743794167]
	TIME [epoch: 3.8 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1540240240671444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1540240240671444 | validation: 3.313158053582514]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.093443157373546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.093443157373546 | validation: 3.3061453946577006]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1049802965038134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1049802965038134 | validation: 3.34787635142316]
	TIME [epoch: 3.81 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.102296272158201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.102296272158201 | validation: 3.355570463426486]
	TIME [epoch: 3.8 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1734849874974036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1734849874974036 | validation: 3.46565832463024]
	TIME [epoch: 3.81 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.233156823695302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.233156823695302 | validation: 3.2965523694310437]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.105557358655866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.105557358655866 | validation: 3.286321671654342]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0610806154489025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0610806154489025 | validation: 3.2673732996696128]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0541517760558214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0541517760558214 | validation: 3.274527324095654]
	TIME [epoch: 3.79 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0789150519170607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0789150519170607 | validation: 3.4241684694131296]
	TIME [epoch: 3.77 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.194358291995085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.194358291995085 | validation: 3.389668019904725]
	TIME [epoch: 3.78 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.19074396590208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.19074396590208 | validation: 3.3137121942124277]
	TIME [epoch: 3.77 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1021343526827834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1021343526827834 | validation: 3.249974864959006]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.041030294576218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.041030294576218 | validation: 3.260361568403842]
	TIME [epoch: 3.8 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0707314150316507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0707314150316507 | validation: 3.3849910297195893]
	TIME [epoch: 3.79 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1442356787050074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1442356787050074 | validation: 3.28760771394188]
	TIME [epoch: 3.8 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.097272392794297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.097272392794297 | validation: 3.261826158837711]
	TIME [epoch: 3.79 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.053276188685617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.053276188685617 | validation: 3.2275566516957372]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0126105365676814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0126105365676814 | validation: 3.218490529016007]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.020683457169543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.020683457169543 | validation: 3.2601743362420326]
	TIME [epoch: 3.8 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.048121466831325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.048121466831325 | validation: 3.2555176724724975]
	TIME [epoch: 3.8 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.067647936980789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.067647936980789 | validation: 3.2338295942614055]
	TIME [epoch: 3.79 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0494643881256542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0494643881256542 | validation: 3.1549766242535315]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9809228974707707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9809228974707707 | validation: 3.155003271895545]
	TIME [epoch: 3.8 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9799778808006456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9799778808006456 | validation: 3.0767621270785535]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.928057305961888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.928057305961888 | validation: 2.8173723772625845]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7158074752118466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7158074752118466 | validation: 2.402071338920032]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3383840277093775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3383840277093775 | validation: 1.906104993788211]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9455210684828916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9455210684828916 | validation: 1.7539356308463825]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8588599717343983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8588599717343983 | validation: 1.6193067631242368]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.630311686017297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.630311686017297 | validation: 1.364385203352354]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4132011112566694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4132011112566694 | validation: 1.2421548772248547]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.31604227939817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.31604227939817 | validation: 1.2158683898065115]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.382202027621188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.382202027621188 | validation: 1.151290730660663]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.151461633396921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.151461633396921 | validation: 1.2381858278369753]
	TIME [epoch: 3.8 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2676086871473835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2676086871473835 | validation: 1.0160886421512785]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0697760681570117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0697760681570117 | validation: 1.0358371049298691]
	TIME [epoch: 3.78 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0186733890368584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0186733890368584 | validation: 0.894525379125076]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891516924640054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.891516924640054 | validation: 0.9021593659622251]
	TIME [epoch: 3.79 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8706810152530352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8706810152530352 | validation: 0.8650218389399902]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8407117123042296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8407117123042296 | validation: 0.8245448420558099]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990266309029909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7990266309029909 | validation: 0.8295547104690268]
	TIME [epoch: 3.82 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7911268947100376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7911268947100376 | validation: 0.8168789555150614]
	TIME [epoch: 3.82 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7780995818584459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7780995818584459 | validation: 0.8066875507543809]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7776550098590107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7776550098590107 | validation: 0.7888663964525198]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.772279372586508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.772279372586508 | validation: 0.7839711387294582]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7585164504008285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7585164504008285 | validation: 0.7883430082108539]
	TIME [epoch: 3.79 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7739981266200121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7739981266200121 | validation: 0.888572871126711]
	TIME [epoch: 3.8 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9845222533346514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9845222533346514 | validation: 0.9772250280161501]
	TIME [epoch: 3.8 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9997640683221386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9997640683221386 | validation: 0.7632792774160295]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7885344942735714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7885344942735714 | validation: 0.8195716165007984]
	TIME [epoch: 3.79 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8156122344122059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8156122344122059 | validation: 0.7802570357200634]
	TIME [epoch: 3.79 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7515945402106238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7515945402106238 | validation: 0.7862033859753221]
	TIME [epoch: 4.05 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7512336069336445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7512336069336445 | validation: 0.8415566746528832]
	TIME [epoch: 3.79 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8506036294595021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8506036294595021 | validation: 0.7431682718454024]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7148070910230182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7148070910230182 | validation: 0.7475669765369751]
	TIME [epoch: 3.82 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7373944357453448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7373944357453448 | validation: 0.7953535880936374]
	TIME [epoch: 3.81 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7812485215750863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7812485215750863 | validation: 0.7588166726123001]
	TIME [epoch: 3.81 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7215100165681563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7215100165681563 | validation: 0.7375355779187992]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7203385371935062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7203385371935062 | validation: 0.782441334579143]
	TIME [epoch: 3.81 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7553946877903988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7553946877903988 | validation: 0.9021422370736861]
	TIME [epoch: 3.81 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8842374081204358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8842374081204358 | validation: 0.7357161777293035]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7329772816631959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7329772816631959 | validation: 0.7678231928134444]
	TIME [epoch: 3.8 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7570693101055082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7570693101055082 | validation: 0.7340809604031127]
	TIME [epoch: 3.81 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7165750779660428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7165750779660428 | validation: 0.7627792612693773]
	TIME [epoch: 3.79 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7348259613374899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7348259613374899 | validation: 0.7910856305176902]
	TIME [epoch: 3.8 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573652332864445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7573652332864445 | validation: 0.7414424602196757]
	TIME [epoch: 3.81 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7072732158347196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7072732158347196 | validation: 0.7323493546747978]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7059790076995452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7059790076995452 | validation: 0.7648379438970706]
	TIME [epoch: 3.78 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7211593489423856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7211593489423856 | validation: 0.746216012321832]
	TIME [epoch: 3.79 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7309453303544118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7309453303544118 | validation: 0.7618866439181599]
	TIME [epoch: 3.79 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7370065760524724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7370065760524724 | validation: 0.7700688130307688]
	TIME [epoch: 3.78 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.768441764455421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.768441764455421 | validation: 0.7111287223484732]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6979566169982968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6979566169982968 | validation: 0.7658937405940027]
	TIME [epoch: 3.78 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7307268658982026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7307268658982026 | validation: 0.7530610606076712]
	TIME [epoch: 3.78 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7453601054921394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7453601054921394 | validation: 0.7151879612997515]
	TIME [epoch: 3.78 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6734837501150231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6734837501150231 | validation: 0.755211848932408]
	TIME [epoch: 3.78 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733663356257498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.733663356257498 | validation: 0.8165374397255718]
	TIME [epoch: 3.88 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082888567467356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8082888567467356 | validation: 0.7933925672348094]
	TIME [epoch: 3.79 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657153752750986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7657153752750986 | validation: 0.8201825442145151]
	TIME [epoch: 3.78 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8052488340787349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8052488340787349 | validation: 0.725997026901591]
	TIME [epoch: 3.78 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7336797638771404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7336797638771404 | validation: 0.7218741395435949]
	TIME [epoch: 3.78 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7075640552302508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7075640552302508 | validation: 0.7401191205436786]
	TIME [epoch: 3.78 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825632157639927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6825632157639927 | validation: 0.7397437421511008]
	TIME [epoch: 3.78 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.693848244155748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.693848244155748 | validation: 0.7489005367180247]
	TIME [epoch: 3.78 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7176733003233952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7176733003233952 | validation: 0.7552724023418617]
	TIME [epoch: 3.78 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7424873367042808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7424873367042808 | validation: 0.7042563495642841]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.67534267153664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.67534267153664 | validation: 0.6968637885919378]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.675080935342034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.675080935342034 | validation: 0.7342903650362443]
	TIME [epoch: 3.79 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7062407883396975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7062407883396975 | validation: 0.7361321476667472]
	TIME [epoch: 3.79 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7325563029538646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7325563029538646 | validation: 0.6979652311645216]
	TIME [epoch: 3.81 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6654737217340554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6654737217340554 | validation: 0.7203114183025487]
	TIME [epoch: 3.79 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830506470735602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6830506470735602 | validation: 0.7141091564934299]
	TIME [epoch: 3.8 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7155305152492373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7155305152492373 | validation: 0.7016369663169955]
	TIME [epoch: 3.79 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6626795772532005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6626795772532005 | validation: 0.7405495255163453]
	TIME [epoch: 3.79 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7009432902268572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7009432902268572 | validation: 0.8238215004661505]
	TIME [epoch: 3.79 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8208640373940316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8208640373940316 | validation: 0.8353274014416914]
	TIME [epoch: 3.79 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8116104051742903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8116104051742903 | validation: 0.8199139439087922]
	TIME [epoch: 3.8 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8439159278675714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8439159278675714 | validation: 0.7214638860513836]
	TIME [epoch: 3.79 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7309177154046859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7309177154046859 | validation: 0.8153927436165044]
	TIME [epoch: 3.79 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7874924751696688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7874924751696688 | validation: 0.7156481474490524]
	TIME [epoch: 3.79 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6873203753145433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6873203753145433 | validation: 0.7261582532390843]
	TIME [epoch: 3.79 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6763243294225996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6763243294225996 | validation: 0.7287717350465126]
	TIME [epoch: 3.8 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6769504036278534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6769504036278534 | validation: 0.7151282536084387]
	TIME [epoch: 3.81 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6628880374411358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6628880374411358 | validation: 0.7036110722703448]
	TIME [epoch: 3.8 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.661416998662545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.661416998662545 | validation: 0.7143206344633605]
	TIME [epoch: 3.79 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6718241681290005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6718241681290005 | validation: 0.7100757933277709]
	TIME [epoch: 3.79 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6715934345065153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6715934345065153 | validation: 0.7553319054081169]
	TIME [epoch: 3.79 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6837181127656708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6837181127656708 | validation: 0.7601172919028237]
	TIME [epoch: 3.79 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7248473583167565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7248473583167565 | validation: 0.7611484174040903]
	TIME [epoch: 3.8 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.708272306602412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.708272306602412 | validation: 0.7886292941638455]
	TIME [epoch: 3.79 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8153420281181329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8153420281181329 | validation: 0.7230750300619783]
	TIME [epoch: 3.79 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7587555951064099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7587555951064099 | validation: 0.7312249143751132]
	TIME [epoch: 3.79 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7375773729536688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7375773729536688 | validation: 0.8530043311691256]
	TIME [epoch: 3.79 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9329901606495291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9329901606495291 | validation: 0.852123319233489]
	TIME [epoch: 3.79 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8532468901759603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8532468901759603 | validation: 0.7528660785926048]
	TIME [epoch: 3.79 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7035205032105634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7035205032105634 | validation: 0.737839331756032]
	TIME [epoch: 31.5 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6765151183658079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6765151183658079 | validation: 0.7261606817346886]
	TIME [epoch: 8.25 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6866210300622939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6866210300622939 | validation: 0.7527325435503882]
	TIME [epoch: 8.25 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6904188671775802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6904188671775802 | validation: 0.7197420731697872]
	TIME [epoch: 8.25 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7212693106356907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7212693106356907 | validation: 0.7194083487360016]
	TIME [epoch: 8.26 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6662641687063834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6662641687063834 | validation: 0.7253018299643961]
	TIME [epoch: 8.25 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7124827757659716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7124827757659716 | validation: 0.7024685395817365]
	TIME [epoch: 8.25 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7150675367786539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7150675367786539 | validation: 0.7589483156993183]
	TIME [epoch: 8.24 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7151453772741527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7151453772741527 | validation: 0.7320298177174023]
	TIME [epoch: 8.25 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7078033736422689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7078033736422689 | validation: 0.7103749351894262]
	TIME [epoch: 8.25 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6572837736007862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6572837736007862 | validation: 0.7182742698248186]
	TIME [epoch: 8.26 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6731171989082888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6731171989082888 | validation: 0.7086523363682234]
	TIME [epoch: 8.24 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6681441475879258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6681441475879258 | validation: 0.7025765046400917]
	TIME [epoch: 8.25 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6588226866102216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6588226866102216 | validation: 0.7006746688936005]
	TIME [epoch: 8.24 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.662946700036367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.662946700036367 | validation: 0.7275721633037208]
	TIME [epoch: 8.23 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7710812150931773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7710812150931773 | validation: 0.8144612771248156]
	TIME [epoch: 8.27 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8518243401788449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8518243401788449 | validation: 0.7946721101428939]
	TIME [epoch: 8.25 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8494562208325832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8494562208325832 | validation: 0.7604701456001598]
	TIME [epoch: 8.25 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7734145795979401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7734145795979401 | validation: 0.7650507443599552]
	TIME [epoch: 8.25 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7781559205595946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7781559205595946 | validation: 0.6966751317714361]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6901121760017611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6901121760017611 | validation: 0.7284878966341696]
	TIME [epoch: 8.24 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7025314881006385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7025314881006385 | validation: 0.7308363419421084]
	TIME [epoch: 8.26 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6920652726586966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6920652726586966 | validation: 0.7223348972605708]
	TIME [epoch: 8.34 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6737009202827258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6737009202827258 | validation: 0.7121776797506343]
	TIME [epoch: 8.25 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6628950878056173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6628950878056173 | validation: 0.716612754669257]
	TIME [epoch: 8.25 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6630396921925424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6630396921925424 | validation: 0.7253336536876512]
	TIME [epoch: 8.25 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6613666012771438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6613666012771438 | validation: 0.7192339351166845]
	TIME [epoch: 8.46 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6626381738133551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6626381738133551 | validation: 0.7458526076226519]
	TIME [epoch: 8.24 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6899424820232163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6899424820232163 | validation: 0.7857234132481716]
	TIME [epoch: 8.25 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77806454478097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.77806454478097 | validation: 0.7873912114770072]
	TIME [epoch: 8.25 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7287842536942546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7287842536942546 | validation: 0.7049520825808571]
	TIME [epoch: 8.26 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6933330464527531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6933330464527531 | validation: 0.6916321890956816]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6605001309371062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6605001309371062 | validation: 0.6924672794497708]
	TIME [epoch: 8.26 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569096336644745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6569096336644745 | validation: 0.6997234030110587]
	TIME [epoch: 8.25 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6587574904743414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6587574904743414 | validation: 0.709470299163058]
	TIME [epoch: 8.25 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6588178162298349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6588178162298349 | validation: 0.7038150406444947]
	TIME [epoch: 8.26 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6942249824272138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6942249824272138 | validation: 0.7611984008464611]
	TIME [epoch: 8.26 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7174917038024177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7174917038024177 | validation: 0.7783953428537855]
	TIME [epoch: 8.26 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7698984529003106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7698984529003106 | validation: 0.7239672079168065]
	TIME [epoch: 8.25 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6730233249148367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6730233249148367 | validation: 0.6734965582669532]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6694354122848722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6694354122848722 | validation: 0.6880937948781222]
	TIME [epoch: 8.26 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6594883466731065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6594883466731065 | validation: 0.6999723190705027]
	TIME [epoch: 8.25 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6501702317725224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6501702317725224 | validation: 0.6871172734884922]
	TIME [epoch: 8.25 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6655507857777032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6655507857777032 | validation: 0.7489588849013216]
	TIME [epoch: 8.25 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057911498279232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8057911498279232 | validation: 0.7133680673634656]
	TIME [epoch: 8.25 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7621799694358343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7621799694358343 | validation: 0.6994595386488777]
	TIME [epoch: 8.24 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7211904764360009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7211904764360009 | validation: 0.70049469171436]
	TIME [epoch: 8.24 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6812456730165003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6812456730165003 | validation: 0.7023362704370805]
	TIME [epoch: 8.24 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6666339664534426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6666339664534426 | validation: 0.7090976112067042]
	TIME [epoch: 8.26 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6749272515243776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6749272515243776 | validation: 0.7509089847725674]
	TIME [epoch: 8.25 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6884048679645551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6884048679645551 | validation: 0.7992707478456187]
	TIME [epoch: 8.24 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7602149064725222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7602149064725222 | validation: 0.7567919363434167]
	TIME [epoch: 8.25 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6910899730729991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6910899730729991 | validation: 0.6943484499797479]
	TIME [epoch: 8.25 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6738400182588445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6738400182588445 | validation: 0.7034513915774491]
	TIME [epoch: 8.25 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6502699260235432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6502699260235432 | validation: 0.6923844003777648]
	TIME [epoch: 8.26 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6512310612800122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6512310612800122 | validation: 0.6975537232450014]
	TIME [epoch: 8.27 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6583097176703646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6583097176703646 | validation: 0.6937129528635458]
	TIME [epoch: 8.25 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6729701387837969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6729701387837969 | validation: 0.7120221287749997]
	TIME [epoch: 8.29 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7276962914742794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7276962914742794 | validation: 0.7464222125242248]
	TIME [epoch: 8.33 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7282581222974703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7282581222974703 | validation: 0.7488439940764526]
	TIME [epoch: 8.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7018890561793053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7018890561793053 | validation: 0.7104645696713288]
	TIME [epoch: 8.31 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7365852885275854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7365852885275854 | validation: 0.7352027687424467]
	TIME [epoch: 8.33 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.695942806751675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.695942806751675 | validation: 0.7607495405839256]
	TIME [epoch: 8.31 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7562235226359385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7562235226359385 | validation: 0.6820065582495357]
	TIME [epoch: 8.32 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6661155997016749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6661155997016749 | validation: 0.7638213512929033]
	TIME [epoch: 8.31 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7081003364462669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7081003364462669 | validation: 0.7621983729762538]
	TIME [epoch: 8.32 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.728126231214861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.728126231214861 | validation: 0.7375629351199487]
	TIME [epoch: 8.31 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.714343552477883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.714343552477883 | validation: 0.708163644801768]
	TIME [epoch: 8.29 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7361373194250099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7361373194250099 | validation: 0.7104058031787198]
	TIME [epoch: 8.28 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6904391540759401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6904391540759401 | validation: 0.7245719617175009]
	TIME [epoch: 8.33 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6885500944818032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6885500944818032 | validation: 0.7240775133238426]
	TIME [epoch: 8.32 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6865571163951446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6865571163951446 | validation: 0.7203946396082067]
	TIME [epoch: 8.29 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6721550967261145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6721550967261145 | validation: 0.7106655197137771]
	TIME [epoch: 8.32 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6570541495680028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6570541495680028 | validation: 0.7135479020387981]
	TIME [epoch: 8.28 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6683284273578504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6683284273578504 | validation: 0.7078556220952529]
	TIME [epoch: 8.31 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6622543668115778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6622543668115778 | validation: 0.740509504527755]
	TIME [epoch: 8.29 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6873818873402252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6873818873402252 | validation: 0.7979435956987834]
	TIME [epoch: 8.28 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7892477222162222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7892477222162222 | validation: 0.7249930570134833]
	TIME [epoch: 8.26 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687650174664181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687650174664181 | validation: 0.7128526597307308]
	TIME [epoch: 8.28 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7160396306677919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7160396306677919 | validation: 0.6709594004338545]
	TIME [epoch: 8.29 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6575127979287132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6575127979287132 | validation: 0.71163696254278]
	TIME [epoch: 8.31 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6788579499230524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6788579499230524 | validation: 0.6968732317435978]
	TIME [epoch: 8.31 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687312912196119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687312912196119 | validation: 0.7495306518476988]
	TIME [epoch: 8.27 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6884738941281678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6884738941281678 | validation: 0.7512266344059685]
	TIME [epoch: 8.33 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7200336389252927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7200336389252927 | validation: 0.7006377778359458]
	TIME [epoch: 8.33 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6653798342145676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6653798342145676 | validation: 0.6944671705351562]
	TIME [epoch: 8.32 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6498248052481296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6498248052481296 | validation: 0.7148758252372783]
	TIME [epoch: 8.31 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.647973234339984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.647973234339984 | validation: 0.7064670022502851]
	TIME [epoch: 8.31 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6673672440234433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6673672440234433 | validation: 0.7246545494884193]
	TIME [epoch: 8.3 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6659869915132766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6659869915132766 | validation: 0.7191848369285643]
	TIME [epoch: 8.3 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6991451069219456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6991451069219456 | validation: 0.7487723696951906]
	TIME [epoch: 8.31 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7277948930083767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7277948930083767 | validation: 0.7280369926073181]
	TIME [epoch: 8.34 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7531752394825776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7531752394825776 | validation: 0.7090214551413709]
	TIME [epoch: 8.32 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6676915043855476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6676915043855476 | validation: 0.7066250772371974]
	TIME [epoch: 8.31 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6486333681528649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6486333681528649 | validation: 0.732406321765956]
	TIME [epoch: 8.29 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6902898400227061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6902898400227061 | validation: 0.7442760607865152]
	TIME [epoch: 8.31 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6886004113829313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6886004113829313 | validation: 0.7092716097652826]
	TIME [epoch: 8.3 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6803474198931799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6803474198931799 | validation: 0.7094281650672775]
	TIME [epoch: 8.32 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6650888230809079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6650888230809079 | validation: 0.6880799079034943]
	TIME [epoch: 8.32 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.677512343447321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.677512343447321 | validation: 0.6935201237786218]
	TIME [epoch: 8.28 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6537532959432985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6537532959432985 | validation: 0.7081442425048723]
	TIME [epoch: 8.28 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6544622948043449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6544622948043449 | validation: 0.7040292687965798]
	TIME [epoch: 8.28 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.665012188563097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.665012188563097 | validation: 0.7156439021455393]
	TIME [epoch: 8.31 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6814820650878366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6814820650878366 | validation: 0.7349353347844197]
	TIME [epoch: 8.3 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7221432877481727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7221432877481727 | validation: 0.7433283183062772]
	TIME [epoch: 8.31 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7037884347918372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7037884347918372 | validation: 0.8198686485845311]
	TIME [epoch: 8.27 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9259963946589247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9259963946589247 | validation: 0.7807900176976799]
	TIME [epoch: 8.31 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8017790919218157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8017790919218157 | validation: 0.7571847828776663]
	TIME [epoch: 8.3 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8088517747067324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8088517747067324 | validation: 0.7034146976040794]
	TIME [epoch: 8.31 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6655534958357652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6655534958357652 | validation: 0.7338359980455252]
	TIME [epoch: 8.27 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6904383192878941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6904383192878941 | validation: 0.694268236128139]
	TIME [epoch: 8.41 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6515989430983729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6515989430983729 | validation: 0.7261470825706771]
	TIME [epoch: 8.28 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6577161828451644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6577161828451644 | validation: 0.7055905897758686]
	TIME [epoch: 8.3 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6504603930127228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6504603930127228 | validation: 0.7030230065305025]
	TIME [epoch: 8.3 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.647972623690169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.647972623690169 | validation: 0.7042033554180072]
	TIME [epoch: 8.27 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6460810720668496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6460810720668496 | validation: 0.7064399469978885]
	TIME [epoch: 8.24 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6551182000548045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6551182000548045 | validation: 0.6960819744871447]
	TIME [epoch: 8.27 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6482195119702562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6482195119702562 | validation: 0.6805871883946127]
	TIME [epoch: 8.25 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6467334062133344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6467334062133344 | validation: 0.7097421302544284]
	TIME [epoch: 8.26 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7138240528224412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7138240528224412 | validation: 0.76634471571559]
	TIME [epoch: 8.26 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8267118010853425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8267118010853425 | validation: 0.7161495169365536]
	TIME [epoch: 8.26 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7541607132751003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7541607132751003 | validation: 0.7401131493219074]
	TIME [epoch: 8.24 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7169526760883731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7169526760883731 | validation: 0.7282435437527394]
	TIME [epoch: 8.25 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7129571363188714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7129571363188714 | validation: 0.706244128700513]
	TIME [epoch: 8.25 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6705983606069768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6705983606069768 | validation: 0.7177098514752371]
	TIME [epoch: 8.27 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6900539635449789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6900539635449789 | validation: 0.7478232019194224]
	TIME [epoch: 8.23 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6966474818365314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6966474818365314 | validation: 0.7337721437993869]
	TIME [epoch: 8.24 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.682537802029941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.682537802029941 | validation: 0.7076057422611938]
	TIME [epoch: 8.27 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6655079759715884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6655079759715884 | validation: 0.7189317251454086]
	TIME [epoch: 8.28 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563806396358496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6563806396358496 | validation: 0.6925699439468244]
	TIME [epoch: 8.28 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6484273604256556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6484273604256556 | validation: 0.706439114117213]
	TIME [epoch: 8.3 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6483704592352502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6483704592352502 | validation: 0.7035236040344612]
	TIME [epoch: 8.28 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6438829378764749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6438829378764749 | validation: 0.6897859485618302]
	TIME [epoch: 8.29 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.641729918176176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.641729918176176 | validation: 0.7010599872416375]
	TIME [epoch: 8.29 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.645147048054206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.645147048054206 | validation: 0.6935560830223845]
	TIME [epoch: 8.28 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6692146843540171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6692146843540171 | validation: 0.8367844790991108]
	TIME [epoch: 8.29 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8239736039876983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8239736039876983 | validation: 0.8660477789727242]
	TIME [epoch: 8.28 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8531304210175864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8531304210175864 | validation: 0.6862412510674583]
	TIME [epoch: 8.27 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6606554271510175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6606554271510175 | validation: 0.7365450597291946]
	TIME [epoch: 8.29 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.732958450618141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.732958450618141 | validation: 0.7276799957218825]
	TIME [epoch: 8.26 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.692784872664866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.692784872664866 | validation: 0.6981716338926589]
	TIME [epoch: 8.29 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6922889063001122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6922889063001122 | validation: 0.6862056421418437]
	TIME [epoch: 8.26 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6972607916400609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6972607916400609 | validation: 0.714627598302654]
	TIME [epoch: 8.26 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7094298486333651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7094298486333651 | validation: 0.6929358686296552]
	TIME [epoch: 8.29 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6558227372765051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6558227372765051 | validation: 0.6947583048306623]
	TIME [epoch: 8.25 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6695385931337325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6695385931337325 | validation: 0.6892090499284826]
	TIME [epoch: 8.27 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658996146668274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.658996146668274 | validation: 0.7278352261111971]
	TIME [epoch: 8.26 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6624943371709239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6624943371709239 | validation: 0.7021286245759841]
	TIME [epoch: 8.24 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6618951673705431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6618951673705431 | validation: 0.6969773447054165]
	TIME [epoch: 8.27 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6644274150757867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6644274150757867 | validation: 0.6808567596384107]
	TIME [epoch: 8.23 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.692501308896644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.692501308896644 | validation: 0.7171059663635599]
	TIME [epoch: 8.27 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6681925334218082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6681925334218082 | validation: 0.7160673339545018]
	TIME [epoch: 8.24 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7024854041090319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7024854041090319 | validation: 0.6819279437164957]
	TIME [epoch: 8.23 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6404992508661608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6404992508661608 | validation: 0.698925760348251]
	TIME [epoch: 8.27 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6423869198928364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6423869198928364 | validation: 0.672367759000659]
	TIME [epoch: 8.28 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6477348669128492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6477348669128492 | validation: 0.6820911234884894]
	TIME [epoch: 8.24 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6468158181648606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6468158181648606 | validation: 0.6835534758203349]
	TIME [epoch: 8.25 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6498295216212405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6498295216212405 | validation: 0.7251629481519544]
	TIME [epoch: 8.26 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7101341592257319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7101341592257319 | validation: 0.7437334732796035]
	TIME [epoch: 8.26 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7951185433961182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7951185433961182 | validation: 0.7036386185246041]
	TIME [epoch: 8.26 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7586070788220382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7586070788220382 | validation: 0.774670217926608]
	TIME [epoch: 8.26 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7607355560081582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7607355560081582 | validation: 0.7381006146772685]
	TIME [epoch: 8.23 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7034397896224135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7034397896224135 | validation: 0.6785255634665512]
	TIME [epoch: 8.26 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6690290080646812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6690290080646812 | validation: 0.7092291870997368]
	TIME [epoch: 8.27 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6776991932441692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6776991932441692 | validation: 0.7239870082785975]
	TIME [epoch: 8.27 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6482814472341873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6482814472341873 | validation: 0.699935335713457]
	TIME [epoch: 8.26 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6468573995616385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6468573995616385 | validation: 0.6960288968833201]
	TIME [epoch: 8.26 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429224768961995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6429224768961995 | validation: 0.6866506096885372]
	TIME [epoch: 8.26 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650652848541605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.650652848541605 | validation: 0.6726539549197617]
	TIME [epoch: 8.27 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6469530824261233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6469530824261233 | validation: 0.6894875412903012]
	TIME [epoch: 8.24 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6556223264697087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6556223264697087 | validation: 0.6868839915960124]
	TIME [epoch: 8.26 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6984720531761633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6984720531761633 | validation: 0.695902454769422]
	TIME [epoch: 8.21 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503514202729531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6503514202729531 | validation: 0.7452016485056481]
	TIME [epoch: 8.22 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7259439594861619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7259439594861619 | validation: 0.7326884603911332]
	TIME [epoch: 8.23 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6838203322268436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6838203322268436 | validation: 0.7243370252662724]
	TIME [epoch: 8.27 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759622627492652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.759622627492652 | validation: 0.7205323383149491]
	TIME [epoch: 8.27 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6671442884382753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6671442884382753 | validation: 0.7142759504003596]
	TIME [epoch: 8.32 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6868956984271675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6868956984271675 | validation: 0.6874967364083279]
	TIME [epoch: 8.32 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6345861824352421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6345861824352421 | validation: 0.6741981380790091]
	TIME [epoch: 8.32 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6580946460242033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6580946460242033 | validation: 0.6825760029756252]
	TIME [epoch: 8.33 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6374882090927991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6374882090927991 | validation: 0.6896207698421071]
	TIME [epoch: 8.33 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335428772541635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6335428772541635 | validation: 0.6838898289425912]
	TIME [epoch: 8.34 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6667024670053836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6667024670053836 | validation: 0.6906500777724305]
	TIME [epoch: 8.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6811694781359917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6811694781359917 | validation: 0.6607671107921571]
	TIME [epoch: 8.34 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6460429271527602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6460429271527602 | validation: 0.7109964796655585]
	TIME [epoch: 8.33 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6555477431026705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6555477431026705 | validation: 0.8083566446793509]
	TIME [epoch: 8.3 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7806274788566526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7806274788566526 | validation: 0.7348534735502373]
	TIME [epoch: 8.29 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7101749116287018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7101749116287018 | validation: 0.6945047624431334]
	TIME [epoch: 8.31 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7268690309118654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7268690309118654 | validation: 0.6823276288033346]
	TIME [epoch: 8.3 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7140551460272567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7140551460272567 | validation: 0.6879867126338313]
	TIME [epoch: 8.3 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6634881380971703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6634881380971703 | validation: 0.713176111217912]
	TIME [epoch: 8.3 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6684444622044983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6684444622044983 | validation: 0.7016721095377735]
	TIME [epoch: 8.28 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6961577795431901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6961577795431901 | validation: 0.7467434348296804]
	TIME [epoch: 8.29 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6783956418772803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6783956418772803 | validation: 0.7115749513829628]
	TIME [epoch: 8.29 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.65835103974457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.65835103974457 | validation: 0.6880402180252294]
	TIME [epoch: 8.3 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6424076480467489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6424076480467489 | validation: 0.6955222252896187]
	TIME [epoch: 8.34 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6382211197211246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6382211197211246 | validation: 0.6772708793533394]
	TIME [epoch: 8.32 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6310246126381305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6310246126381305 | validation: 0.6727383628095883]
	TIME [epoch: 8.29 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6366931790466406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6366931790466406 | validation: 0.6946735365183998]
	TIME [epoch: 8.27 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6388451141473422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6388451141473422 | validation: 0.6704882969468149]
	TIME [epoch: 8.26 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6841006025682402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6841006025682402 | validation: 0.7346800784083268]
	TIME [epoch: 8.26 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6913711021038931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6913711021038931 | validation: 0.7893904315392188]
	TIME [epoch: 8.27 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745863293877042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7745863293877042 | validation: 0.6908615256464659]
	TIME [epoch: 8.27 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.660505179553582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.660505179553582 | validation: 0.6788336564496511]
	TIME [epoch: 8.27 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6493255036621634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6493255036621634 | validation: 0.673336283035411]
	TIME [epoch: 8.3 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6405507989754518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6405507989754518 | validation: 0.6826712342149893]
	TIME [epoch: 8.28 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6414017755470568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6414017755470568 | validation: 0.6787727531712552]
	TIME [epoch: 8.32 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6497153592891375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6497153592891375 | validation: 0.6794745430766218]
	TIME [epoch: 8.28 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6307948766016656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6307948766016656 | validation: 0.6822114157736779]
	TIME [epoch: 8.33 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6539206618593462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6539206618593462 | validation: 0.7293203732988293]
	TIME [epoch: 8.31 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6945662672865937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6945662672865937 | validation: 0.6830043398656871]
	TIME [epoch: 8.33 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7201518692520128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7201518692520128 | validation: 0.661122255723487]
	TIME [epoch: 8.32 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6510527685833151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6510527685833151 | validation: 0.6957775699072365]
	TIME [epoch: 8.33 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6611710328186768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6611710328186768 | validation: 0.6928237375855058]
	TIME [epoch: 8.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6565402764446299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6565402764446299 | validation: 0.6996050117145268]
	TIME [epoch: 8.31 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6642861968113798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6642861968113798 | validation: 0.7425645313721668]
	TIME [epoch: 8.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715562147938265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.715562147938265 | validation: 0.7157002462356868]
	TIME [epoch: 8.29 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.722538767016213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.722538767016213 | validation: 0.6964059695975748]
	TIME [epoch: 8.31 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.669687391827797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.669687391827797 | validation: 0.702138936275479]
	TIME [epoch: 8.31 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6596573289409553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6596573289409553 | validation: 0.6835830295014299]
	TIME [epoch: 8.29 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6288992957504619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6288992957504619 | validation: 0.6676719610200669]
	TIME [epoch: 8.31 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6339845418753883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6339845418753883 | validation: 0.6756372469338974]
	TIME [epoch: 8.28 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.628118703433154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.628118703433154 | validation: 0.6920293418870023]
	TIME [epoch: 8.3 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6609960312567356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6609960312567356 | validation: 0.6814535265915492]
	TIME [epoch: 8.29 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6492198839121767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6492198839121767 | validation: 0.6837970394547012]
	TIME [epoch: 8.31 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332777946513636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6332777946513636 | validation: 0.7037205440420439]
	TIME [epoch: 8.3 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603664631592229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6603664631592229 | validation: 0.7288881090281618]
	TIME [epoch: 8.32 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7484853856515012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7484853856515012 | validation: 0.7370136736128045]
	TIME [epoch: 8.31 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7189851313165897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7189851313165897 | validation: 0.7439664048525318]
	TIME [epoch: 8.31 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7282614289194507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7282614289194507 | validation: 0.6824089339315691]
	TIME [epoch: 8.3 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6391886759308711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6391886759308711 | validation: 0.7129070390897699]
	TIME [epoch: 8.3 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6610728404552085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6610728404552085 | validation: 0.66526039688781]
	TIME [epoch: 8.3 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6385789697666756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6385789697666756 | validation: 0.6932572129876164]
	TIME [epoch: 8.31 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6413468819998264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6413468819998264 | validation: 0.6772197515208482]
	TIME [epoch: 8.3 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.635416348835449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.635416348835449 | validation: 0.6998718017818512]
	TIME [epoch: 8.32 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6375828070353963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6375828070353963 | validation: 0.6906027856719176]
	TIME [epoch: 8.3 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6447576725035302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6447576725035302 | validation: 0.704477606225383]
	TIME [epoch: 8.29 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6571452409210073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6571452409210073 | validation: 0.6971024323661024]
	TIME [epoch: 8.31 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6944592402308559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6944592402308559 | validation: 0.7158593212300066]
	TIME [epoch: 8.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761004584678271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6761004584678271 | validation: 0.7213936643834358]
	TIME [epoch: 8.31 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6798623829304884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6798623829304884 | validation: 0.695778932131821]
	TIME [epoch: 8.32 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446729657527738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6446729657527738 | validation: 0.6663099311245666]
	TIME [epoch: 8.33 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6525458955654609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6525458955654609 | validation: 0.6832170505607292]
	TIME [epoch: 8.32 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6211086338524069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6211086338524069 | validation: 0.679421324463152]
	TIME [epoch: 8.28 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6132866544413594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6132866544413594 | validation: 0.6654700776602239]
	TIME [epoch: 8.33 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6267897221683134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6267897221683134 | validation: 0.7026753219457276]
	TIME [epoch: 8.28 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6670939656000264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6670939656000264 | validation: 0.7060333485583284]
	TIME [epoch: 8.26 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6316405556299984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6316405556299984 | validation: 0.7124119006682975]
	TIME [epoch: 8.25 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7047858333615917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7047858333615917 | validation: 0.7967639596600335]
	TIME [epoch: 8.29 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893967939503302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7893967939503302 | validation: 0.7218630099616258]
	TIME [epoch: 8.25 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7333658221471557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7333658221471557 | validation: 0.6538910805433448]
	TIME [epoch: 8.27 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6372074924942644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6372074924942644 | validation: 0.6999909799366911]
	TIME [epoch: 8.25 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6501936670135882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6501936670135882 | validation: 0.6916547916453308]
	TIME [epoch: 8.25 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6616796353892064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6616796353892064 | validation: 0.685742344385873]
	TIME [epoch: 8.25 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6225459776975971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6225459776975971 | validation: 0.647523218976501]
	TIME [epoch: 8.26 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6302558231351215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6302558231351215 | validation: 0.6918605605546015]
	TIME [epoch: 8.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.660971827212818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.660971827212818 | validation: 0.6860953113425707]
	TIME [epoch: 8.29 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6331370359862973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6331370359862973 | validation: 0.6598289965680009]
	TIME [epoch: 8.29 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6490141152709356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6490141152709356 | validation: 0.6839625030405904]
	TIME [epoch: 8.31 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6322513004303639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6322513004303639 | validation: 0.670632155347135]
	TIME [epoch: 8.32 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6254831659276984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6254831659276984 | validation: 0.6824970251033534]
	TIME [epoch: 8.32 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6218333916213916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6218333916213916 | validation: 0.6531391181242612]
	TIME [epoch: 8.32 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6292177300049457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6292177300049457 | validation: 0.7119263289517129]
	TIME [epoch: 8.3 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6571489634175677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6571489634175677 | validation: 0.7047623161124802]
	TIME [epoch: 8.31 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7433740481007254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7433740481007254 | validation: 0.7021972170122992]
	TIME [epoch: 8.29 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.673232036090276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.673232036090276 | validation: 0.7473243350909121]
	TIME [epoch: 8.3 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7288211770722651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7288211770722651 | validation: 0.666363442413114]
	TIME [epoch: 8.31 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6302880061265268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6302880061265268 | validation: 0.730109523536966]
	TIME [epoch: 8.3 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6654560550829389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6654560550829389 | validation: 0.6748121578293246]
	TIME [epoch: 8.28 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6299770167184484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6299770167184484 | validation: 0.677972966860194]
	TIME [epoch: 8.27 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6364723578619128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6364723578619128 | validation: 0.6647763515010379]
	TIME [epoch: 8.26 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6431118086942865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6431118086942865 | validation: 0.6845896926647445]
	TIME [epoch: 8.29 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6260784512960246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6260784512960246 | validation: 0.6821966549997165]
	TIME [epoch: 8.28 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.632607565742246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.632607565742246 | validation: 0.6846001913529731]
	TIME [epoch: 8.29 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6167613762638715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6167613762638715 | validation: 0.6773604189463666]
	TIME [epoch: 8.3 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6895078239715778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6895078239715778 | validation: 0.7207972987620547]
	TIME [epoch: 8.31 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6417230028455022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6417230028455022 | validation: 0.7317158551923446]
	TIME [epoch: 8.32 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7053929403561875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7053929403561875 | validation: 0.7034795530729562]
	TIME [epoch: 8.32 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.644046869908324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.644046869908324 | validation: 0.659262628690185]
	TIME [epoch: 8.3 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6701011148523865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6701011148523865 | validation: 0.6438981003872124]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6011606149841721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6011606149841721 | validation: 0.6830843083643695]
	TIME [epoch: 8.3 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6356992176084936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6356992176084936 | validation: 0.6652401343635463]
	TIME [epoch: 8.28 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521928526899822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6521928526899822 | validation: 0.7375325790742677]
	TIME [epoch: 8.32 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6630615105287743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6630615105287743 | validation: 0.6706482113301145]
	TIME [epoch: 8.28 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6325415933881482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6325415933881482 | validation: 0.7026969100883447]
	TIME [epoch: 8.3 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6419408394883115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6419408394883115 | validation: 0.6700206383510188]
	TIME [epoch: 8.28 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6656831490287318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6656831490287318 | validation: 0.7373436497123191]
	TIME [epoch: 8.31 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6880170300659038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6880170300659038 | validation: 0.6455993824121272]
	TIME [epoch: 8.31 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6002254219504949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6002254219504949 | validation: 0.6223542240503623]
	TIME [epoch: 8.31 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5650015434396236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5650015434396236 | validation: 0.6259017116249503]
	TIME [epoch: 8.29 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5447258782471314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5447258782471314 | validation: 0.7253957111592231]
	TIME [epoch: 8.31 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8284935981711007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8284935981711007 | validation: 1.066174135314683]
	TIME [epoch: 8.31 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5202785161354804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5202785161354804 | validation: 2.5453679580498796]
	TIME [epoch: 8.31 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.954835409441328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.954835409441328 | validation: 1.5079464973291539]
	TIME [epoch: 8.31 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9692796362933052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9692796362933052 | validation: 1.032985314521396]
	TIME [epoch: 8.29 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.137183700077883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.137183700077883 | validation: 1.0533638379354264]
	TIME [epoch: 8.28 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0559791399048493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0559791399048493 | validation: 0.7771982954834963]
	TIME [epoch: 8.3 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572522113653406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7572522113653406 | validation: 0.7400607903911139]
	TIME [epoch: 8.3 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7420096786415264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7420096786415264 | validation: 0.6914067875886633]
	TIME [epoch: 8.3 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6510112049004423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6510112049004423 | validation: 0.6831350957554951]
	TIME [epoch: 8.3 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6388765958277438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6388765958277438 | validation: 0.6584793983029781]
	TIME [epoch: 42.1 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6200824006758758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6200824006758758 | validation: 0.6511791946463169]
	TIME [epoch: 17.8 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.617835299558867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.617835299558867 | validation: 0.6636440059117539]
	TIME [epoch: 17.8 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6045000488483017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6045000488483017 | validation: 0.6614273648997683]
	TIME [epoch: 17.9 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6092161197507077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6092161197507077 | validation: 0.6700786238867195]
	TIME [epoch: 17.8 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6082832778767293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6082832778767293 | validation: 0.663623292985382]
	TIME [epoch: 17.8 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6023758128782281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6023758128782281 | validation: 0.6520605201718779]
	TIME [epoch: 17.8 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5970260691988175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5970260691988175 | validation: 0.6608590713027799]
	TIME [epoch: 17.8 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5991131208554142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5991131208554142 | validation: 0.6517803960832521]
	TIME [epoch: 17.8 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5891452214277498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5891452214277498 | validation: 0.6493215709530986]
	TIME [epoch: 17.8 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5963073572735704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5963073572735704 | validation: 0.6499209386396999]
	TIME [epoch: 17.7 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5862435862139418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5862435862139418 | validation: 0.6469147041821569]
	TIME [epoch: 17.7 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5873604528379246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5873604528379246 | validation: 0.6633174158060999]
	TIME [epoch: 17.7 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5853978368942023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5853978368942023 | validation: 0.6675538525219779]
	TIME [epoch: 17.8 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5707078086379619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5707078086379619 | validation: 0.6467800146473061]
	TIME [epoch: 17.8 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5726936813629528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5726936813629528 | validation: 0.6797522871754252]
	TIME [epoch: 17.8 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5833416405258898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5833416405258898 | validation: 0.6978766434087014]
	TIME [epoch: 17.8 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.661672142277984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.661672142277984 | validation: 0.7893064201624261]
	TIME [epoch: 17.8 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952843103996625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7952843103996625 | validation: 0.606975210575424]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6043369657645429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6043369657645429 | validation: 0.6219464355968068]
	TIME [epoch: 17.7 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6027541127593798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6027541127593798 | validation: 0.6491611101291465]
	TIME [epoch: 17.7 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6200805297420383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6200805297420383 | validation: 0.6154634620089279]
	TIME [epoch: 17.7 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5617897010964568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5617897010964568 | validation: 0.6222170215772692]
	TIME [epoch: 17.8 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5333584226189866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5333584226189866 | validation: 0.6432197243655614]
	TIME [epoch: 17.7 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5298980197459435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5298980197459435 | validation: 0.6097156323253876]
	TIME [epoch: 17.7 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5274078345264662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5274078345264662 | validation: 0.6647262728338398]
	TIME [epoch: 17.7 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6957419269700686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6957419269700686 | validation: 0.7269599193240025]
	TIME [epoch: 17.7 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657425005769101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7657425005769101 | validation: 0.5481173061664322]
	TIME [epoch: 17.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5702270939137897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5702270939137897 | validation: 0.5944653026405463]
	TIME [epoch: 17.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5132107478425655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5132107478425655 | validation: 0.6371538964835365]
	TIME [epoch: 17.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6899265995824514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6899265995824514 | validation: 0.9021634512003479]
	TIME [epoch: 17.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2637907181359729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2637907181359729 | validation: 0.7746726915049383]
	TIME [epoch: 17.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9779121789192066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9779121789192066 | validation: 0.8438715515027809]
	TIME [epoch: 17.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9193896748095273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9193896748095273 | validation: 0.7316353088198175]
	TIME [epoch: 17.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.763593926612189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.763593926612189 | validation: 0.6742745388644504]
	TIME [epoch: 17.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6776929477216082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6776929477216082 | validation: 0.6581618392928066]
	TIME [epoch: 17.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6547366011617903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6547366011617903 | validation: 0.6467454965785111]
	TIME [epoch: 17.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6352047823594407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6352047823594407 | validation: 0.6225071491787025]
	TIME [epoch: 17.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.620793416778996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.620793416778996 | validation: 0.6269980038497849]
	TIME [epoch: 17.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6104026872364384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6104026872364384 | validation: 0.626335186431526]
	TIME [epoch: 17.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5994372545594167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5994372545594167 | validation: 0.625941372074194]
	TIME [epoch: 17.6 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5900600062975322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5900600062975322 | validation: 0.6161308365001242]
	TIME [epoch: 17.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5878260842247809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5878260842247809 | validation: 0.6213322885689581]
	TIME [epoch: 17.6 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5795766212897303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5795766212897303 | validation: 0.6306254723607957]
	TIME [epoch: 17.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5721913304589271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5721913304589271 | validation: 0.614300097565142]
	TIME [epoch: 17.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5619610618612694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5619610618612694 | validation: 0.6210819016630694]
	TIME [epoch: 17.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5551992918922557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5551992918922557 | validation: 0.6026237754086066]
	TIME [epoch: 17.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5445018866188686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5445018866188686 | validation: 0.6231158382442786]
	TIME [epoch: 17.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5309437451575376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5309437451575376 | validation: 0.6161007643766497]
	TIME [epoch: 17.7 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5067152459696505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5067152459696505 | validation: 0.5758412969963776]
	TIME [epoch: 17.7 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5026885119293193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5026885119293193 | validation: 0.7595975412396181]
	TIME [epoch: 17.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6739084559218955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6739084559218955 | validation: 1.4343389352664655]
	TIME [epoch: 17.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7216852424657858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7216852424657858 | validation: 0.609503034134317]
	TIME [epoch: 17.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6259809840045061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6259809840045061 | validation: 0.5983254873034854]
	TIME [epoch: 17.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.603765209094536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.603765209094536 | validation: 0.59683749640592]
	TIME [epoch: 17.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6029509480141557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6029509480141557 | validation: 0.6025051922100637]
	TIME [epoch: 17.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6051762708714885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6051762708714885 | validation: 0.6069415610364827]
	TIME [epoch: 17.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5833224677281705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5833224677281705 | validation: 0.6222580854916459]
	TIME [epoch: 17.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.56586156861477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.56586156861477 | validation: 0.603251575643024]
	TIME [epoch: 17.7 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.558583793400029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.558583793400029 | validation: 0.599303962322501]
	TIME [epoch: 17.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5487625885897198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5487625885897198 | validation: 0.5986625033540262]
	TIME [epoch: 17.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5437679136850215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5437679136850215 | validation: 0.6147234046467083]
	TIME [epoch: 17.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5282839589433859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5282839589433859 | validation: 0.588744953405761]
	TIME [epoch: 17.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5376905359550435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5376905359550435 | validation: 0.6484753006941496]
	TIME [epoch: 17.6 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5364884928343221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5364884928343221 | validation: 0.614806282462533]
	TIME [epoch: 17.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.593259727497159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.593259727497159 | validation: 0.7312280521832378]
	TIME [epoch: 17.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6088846937293567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6088846937293567 | validation: 0.562891841712806]
	TIME [epoch: 17.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5390208098071431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5390208098071431 | validation: 0.5640141527564115]
	TIME [epoch: 17.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4899960768928315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4899960768928315 | validation: 0.49653887356004056]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_569.pth
	Model improved!!!
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4494313460648117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4494313460648117 | validation: 0.5043022013112782]
	TIME [epoch: 17.7 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44237996862462214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44237996862462214 | validation: 0.6968124300902556]
	TIME [epoch: 17.7 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.675716471376254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.675716471376254 | validation: 0.9309808182697396]
	TIME [epoch: 17.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.267873710429811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.267873710429811 | validation: 0.6279229365984937]
	TIME [epoch: 17.7 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7992757102437221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7992757102437221 | validation: 0.8682164265898049]
	TIME [epoch: 17.7 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9397719539005095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9397719539005095 | validation: 0.6412166219024231]
	TIME [epoch: 17.7 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772554083433977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6772554083433977 | validation: 0.6073845857368373]
	TIME [epoch: 17.7 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6042848452854848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6042848452854848 | validation: 0.47195767600239497]
	TIME [epoch: 17.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4647866533850009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4647866533850009 | validation: 0.5342648940569612]
	TIME [epoch: 17.7 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5247026177817297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5247026177817297 | validation: 0.601035287817008]
	TIME [epoch: 17.7 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5218497914126813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5218497914126813 | validation: 0.5058433951586527]
	TIME [epoch: 17.7 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4236513440623046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4236513440623046 | validation: 0.4720603456432538]
	TIME [epoch: 17.7 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4048530879446065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4048530879446065 | validation: 0.46554690295042644]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_582.pth
	Model improved!!!
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3919239250671882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3919239250671882 | validation: 0.4716642298948445]
	TIME [epoch: 17.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4155426738263042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4155426738263042 | validation: 0.5657778602599359]
	TIME [epoch: 17.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.499871480998564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.499871480998564 | validation: 0.6538493135265618]
	TIME [epoch: 17.7 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7602768225895951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7602768225895951 | validation: 0.4213881115315026]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_586.pth
	Model improved!!!
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5098076972060116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5098076972060116 | validation: 0.6263109638253628]
	TIME [epoch: 17.8 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6531652432047587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6531652432047587 | validation: 0.47988543371110765]
	TIME [epoch: 17.7 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4190327838848873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4190327838848873 | validation: 0.4671017030186619]
	TIME [epoch: 17.7 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.364595051198138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.364595051198138 | validation: 0.4533400025836235]
	TIME [epoch: 17.7 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.360706306924215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.360706306924215 | validation: 0.4221308551481749]
	TIME [epoch: 17.7 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3472297050711467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3472297050711467 | validation: 0.45431344908939153]
	TIME [epoch: 17.7 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36577787370443887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36577787370443887 | validation: 0.599347705457282]
	TIME [epoch: 17.7 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6122492133667848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6122492133667848 | validation: 0.5128949364384013]
	TIME [epoch: 17.7 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4552675792212028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4552675792212028 | validation: 0.41844391552656157]
	TIME [epoch: 17.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3470382069912102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3470382069912102 | validation: 0.5152773935655848]
	TIME [epoch: 17.7 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39930895769399827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39930895769399827 | validation: 0.6618085254208285]
	TIME [epoch: 17.7 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8066030341537499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8066030341537499 | validation: 0.42595104663133987]
	TIME [epoch: 17.7 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4617092935237204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4617092935237204 | validation: 0.5437511352849749]
	TIME [epoch: 17.7 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.540784613345007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.540784613345007 | validation: 0.47042857880261046]
	TIME [epoch: 17.7 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3984484955763714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3984484955763714 | validation: 0.49796483106552536]
	TIME [epoch: 17.7 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38341470234941605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38341470234941605 | validation: 0.5568450282194205]
	TIME [epoch: 17.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.593661676004056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.593661676004056 | validation: 0.4843717536477007]
	TIME [epoch: 17.7 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4307518260330964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4307518260330964 | validation: 0.37549751358198064]
	TIME [epoch: 17.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31327152707012595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31327152707012595 | validation: 0.41990019213616475]
	TIME [epoch: 17.7 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34800454994218993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34800454994218993 | validation: 0.7859140014387735]
	TIME [epoch: 17.7 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7603072495567775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7603072495567775 | validation: 0.606892580149928]
	TIME [epoch: 17.7 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889752639663727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7889752639663727 | validation: 0.4554005422452747]
	TIME [epoch: 17.7 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6423420708487307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6423420708487307 | validation: 0.739256584864965]
	TIME [epoch: 17.7 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7719626275884082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7719626275884082 | validation: 0.4628058227179782]
	TIME [epoch: 17.7 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4589004356281252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4589004356281252 | validation: 0.5241589701725217]
	TIME [epoch: 17.7 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5000214080827801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5000214080827801 | validation: 0.4987510268046007]
	TIME [epoch: 17.7 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41251891032701876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41251891032701876 | validation: 0.4425470723143475]
	TIME [epoch: 17.7 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36023246909086465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36023246909086465 | validation: 0.5493605975185528]
	TIME [epoch: 17.8 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4824775444840404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4824775444840404 | validation: 0.35474813046495296]
	TIME [epoch: 17.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3120558621864402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3120558621864402 | validation: 0.35725362665390553]
	TIME [epoch: 17.7 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2864670800444567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2864670800444567 | validation: 0.36163176379273826]
	TIME [epoch: 17.7 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2749898983335593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2749898983335593 | validation: 0.40392180784771203]
	TIME [epoch: 17.7 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3173362448066739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3173362448066739 | validation: 0.5760974017038737]
	TIME [epoch: 17.7 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5117232144218025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5117232144218025 | validation: 0.36997307306970795]
	TIME [epoch: 17.7 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33122300159030105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33122300159030105 | validation: 0.3251640432662679]
	TIME [epoch: 17.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_621.pth
	Model improved!!!
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2700412183006695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2700412183006695 | validation: 0.3415312883181345]
	TIME [epoch: 17.7 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24620113163552135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24620113163552135 | validation: 0.3553099304718135]
	TIME [epoch: 17.7 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27691179969532426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27691179969532426 | validation: 0.551431022168652]
	TIME [epoch: 17.7 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4805339261965096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4805339261965096 | validation: 0.5624034599626893]
	TIME [epoch: 17.7 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6415678753357424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6415678753357424 | validation: 0.6193698815108007]
	TIME [epoch: 17.7 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5372888386447116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5372888386447116 | validation: 0.34617692999126126]
	TIME [epoch: 17.7 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28761383728419954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28761383728419954 | validation: 0.3894438094007555]
	TIME [epoch: 17.7 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32264879309965483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32264879309965483 | validation: 0.43752217091035944]
	TIME [epoch: 17.7 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36217784712821416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36217784712821416 | validation: 0.5126363256220169]
	TIME [epoch: 17.7 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.503879031827272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.503879031827272 | validation: 0.34393285828930886]
	TIME [epoch: 17.7 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32169083579147717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32169083579147717 | validation: 0.3095539444511933]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_632.pth
	Model improved!!!
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2822846568320679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2822846568320679 | validation: 0.4047655568704376]
	TIME [epoch: 17.7 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3172445857233574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3172445857233574 | validation: 0.6510154907428637]
	TIME [epoch: 17.7 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5480978181498217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5480978181498217 | validation: 0.5035472559411185]
	TIME [epoch: 17.7 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5869691213822813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5869691213822813 | validation: 0.6503652513362751]
	TIME [epoch: 17.7 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5786021725803632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5786021725803632 | validation: 0.42691524759660604]
	TIME [epoch: 17.7 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3399153890577225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3399153890577225 | validation: 0.40889751726733403]
	TIME [epoch: 17.7 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33806026199261147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33806026199261147 | validation: 0.3584318349503433]
	TIME [epoch: 17.7 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2591655219879055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2591655219879055 | validation: 0.38731846562897576]
	TIME [epoch: 17.7 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29761944840779836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29761944840779836 | validation: 0.35868368681074003]
	TIME [epoch: 17.7 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2977421704471092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2977421704471092 | validation: 0.3749285811186968]
	TIME [epoch: 17.7 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32805339334893396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32805339334893396 | validation: 0.28938516314441437]
	TIME [epoch: 17.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_643.pth
	Model improved!!!
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2338228232901826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2338228232901826 | validation: 0.30547532659640037]
	TIME [epoch: 17.7 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20722540670641493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20722540670641493 | validation: 0.30841474057130275]
	TIME [epoch: 17.7 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22430654886601453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22430654886601453 | validation: 0.3804968687037075]
	TIME [epoch: 17.7 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3364118602247953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3364118602247953 | validation: 0.3569007356306866]
	TIME [epoch: 17.8 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.301175880508564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.301175880508564 | validation: 0.32402229961046763]
	TIME [epoch: 17.8 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2654475398607155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2654475398607155 | validation: 0.3648728641111069]
	TIME [epoch: 17.8 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30473135170064103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30473135170064103 | validation: 0.43156279711494894]
	TIME [epoch: 17.8 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4595988276685371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4595988276685371 | validation: 0.5148688819604056]
	TIME [epoch: 17.8 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4257944343373911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4257944343373911 | validation: 0.374221336552891]
	TIME [epoch: 17.8 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2706008241430324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2706008241430324 | validation: 0.5096562451321008]
	TIME [epoch: 17.8 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4642170665287361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4642170665287361 | validation: 0.28212682473583417]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_654.pth
	Model improved!!!
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23865706308539025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23865706308539025 | validation: 0.2946849735027035]
	TIME [epoch: 17.8 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19669058173277976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19669058173277976 | validation: 0.268215503407796]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_656.pth
	Model improved!!!
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1800572909788383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1800572909788383 | validation: 0.2548382341423617]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_657.pth
	Model improved!!!
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17370177187707125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17370177187707125 | validation: 0.2997526945556406]
	TIME [epoch: 17.8 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2035129510650086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2035129510650086 | validation: 0.42269109310811875]
	TIME [epoch: 17.8 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3348195958335721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3348195958335721 | validation: 0.6557559484514115]
	TIME [epoch: 17.8 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6399422464818669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6399422464818669 | validation: 0.31235407460556974]
	TIME [epoch: 17.8 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4137210154506432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4137210154506432 | validation: 0.5045248674890076]
	TIME [epoch: 17.8 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.432074325437752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.432074325437752 | validation: 0.303877842269584]
	TIME [epoch: 17.8 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23493899299814675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23493899299814675 | validation: 0.5800856383818273]
	TIME [epoch: 17.8 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5010779485212301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5010779485212301 | validation: 0.5628470073959628]
	TIME [epoch: 17.8 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6038109609900861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6038109609900861 | validation: 0.3206931044459722]
	TIME [epoch: 17.8 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31314666703483823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31314666703483823 | validation: 0.30212148521542614]
	TIME [epoch: 17.8 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2773645776745447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2773645776745447 | validation: 0.27845928412258386]
	TIME [epoch: 17.8 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21110654188009315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21110654188009315 | validation: 0.3574530515712666]
	TIME [epoch: 17.8 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3031033235734039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3031033235734039 | validation: 0.5120468576989611]
	TIME [epoch: 17.8 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48637539357357923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48637539357357923 | validation: 0.2578179105586977]
	TIME [epoch: 17.8 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.262588336019674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.262588336019674 | validation: 0.3243827117533091]
	TIME [epoch: 17.8 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2543882806610767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2543882806610767 | validation: 0.29036993387854937]
	TIME [epoch: 17.8 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23064832677123304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23064832677123304 | validation: 0.38890626618216095]
	TIME [epoch: 17.8 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34190395206839813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34190395206839813 | validation: 0.5824446006391402]
	TIME [epoch: 17.8 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.551624266008654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.551624266008654 | validation: 0.29289592237974593]
	TIME [epoch: 17.8 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.380012670412967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.380012670412967 | validation: 0.5634031489165686]
	TIME [epoch: 17.8 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4762204785729024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4762204785729024 | validation: 0.2757160133521648]
	TIME [epoch: 17.7 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19792409692872168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19792409692872168 | validation: 0.3626814885924582]
	TIME [epoch: 17.8 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3005009994875373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3005009994875373 | validation: 0.5234819414640056]
	TIME [epoch: 17.7 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40909998360868444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40909998360868444 | validation: 0.3133602319488468]
	TIME [epoch: 17.7 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23649184342217225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23649184342217225 | validation: 0.24890903721203916]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21893040181690265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21893040181690265 | validation: 0.25922391259818384]
	TIME [epoch: 17.7 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1722272473230165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1722272473230165 | validation: 0.22961557345496325]
	TIME [epoch: 17.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_684.pth
	Model improved!!!
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17243705657840688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17243705657840688 | validation: 0.35023212374322366]
	TIME [epoch: 17.7 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29140631791944344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29140631791944344 | validation: 0.4030869248466733]
	TIME [epoch: 17.7 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3541770440627386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3541770440627386 | validation: 0.3804852973989609]
	TIME [epoch: 17.7 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3665225257311259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3665225257311259 | validation: 0.3950761728454591]
	TIME [epoch: 17.7 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29083265336051534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29083265336051534 | validation: 0.2518118809513601]
	TIME [epoch: 17.6 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18692809388583517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18692809388583517 | validation: 0.30889107997686754]
	TIME [epoch: 17.6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2110977558323222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2110977558323222 | validation: 0.5445065447703535]
	TIME [epoch: 17.6 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4556874559198123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4556874559198123 | validation: 0.494004167862275]
	TIME [epoch: 17.6 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6035853168531563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6035853168531563 | validation: 0.3173195871755577]
	TIME [epoch: 17.8 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4176806053761071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4176806053761071 | validation: 0.49113704930063595]
	TIME [epoch: 17.8 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39673288975452464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39673288975452464 | validation: 0.26071205119327256]
	TIME [epoch: 17.8 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17794061266727368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17794061266727368 | validation: 0.36278491363227716]
	TIME [epoch: 17.8 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24221890578299263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24221890578299263 | validation: 0.4621610800658757]
	TIME [epoch: 17.8 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3863624834971775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3863624834971775 | validation: 0.3044347048502705]
	TIME [epoch: 17.8 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26428735195748493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26428735195748493 | validation: 0.21283878373650955]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_699.pth
	Model improved!!!
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.191086214205627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.191086214205627 | validation: 0.28986513289275034]
	TIME [epoch: 17.8 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19795864181348127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19795864181348127 | validation: 0.48627661755403406]
	TIME [epoch: 17.8 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3833918046381946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3833918046381946 | validation: 0.5379602147420341]
	TIME [epoch: 17.8 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5821630605423248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5821630605423248 | validation: 0.5284477650083803]
	TIME [epoch: 17.8 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5092279137174633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5092279137174633 | validation: 0.29511131756908243]
	TIME [epoch: 17.8 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2520018868966752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2520018868966752 | validation: 0.5332954958973175]
	TIME [epoch: 17.8 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48965807456680915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48965807456680915 | validation: 0.23373626449589013]
	TIME [epoch: 17.8 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1793400281565053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1793400281565053 | validation: 0.22092029188433085]
	TIME [epoch: 17.8 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1715474780273529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1715474780273529 | validation: 0.3036874300757443]
	TIME [epoch: 17.8 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22345517116649613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22345517116649613 | validation: 0.3987952887830908]
	TIME [epoch: 17.8 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3155243126463018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3155243126463018 | validation: 0.37083970403382827]
	TIME [epoch: 17.8 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3672992783710995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3672992783710995 | validation: 0.224640052060611]
	TIME [epoch: 17.8 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19684349717292604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19684349717292604 | validation: 0.270823256588121]
	TIME [epoch: 17.8 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18906159767939584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18906159767939584 | validation: 0.31340590348001857]
	TIME [epoch: 17.8 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25327666207737365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25327666207737365 | validation: 0.4611513575499266]
	TIME [epoch: 17.7 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36501634684221856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36501634684221856 | validation: 0.42405187092870966]
	TIME [epoch: 17.8 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39057060168052843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39057060168052843 | validation: 0.25048369139979804]
	TIME [epoch: 17.8 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2747559303655109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2747559303655109 | validation: 0.32245707690510067]
	TIME [epoch: 17.8 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.255347374413042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.255347374413042 | validation: 0.2588912197431541]
	TIME [epoch: 17.6 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19503841652684473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19503841652684473 | validation: 0.32543570773687863]
	TIME [epoch: 17.6 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3065795220308158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3065795220308158 | validation: 0.42355836895905585]
	TIME [epoch: 17.6 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49118171293203816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49118171293203816 | validation: 0.23421662092053266]
	TIME [epoch: 17.6 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.254276848262761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.254276848262761 | validation: 0.2436965407750066]
	TIME [epoch: 17.6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2526810789601086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2526810789601086 | validation: 0.29312242201365524]
	TIME [epoch: 17.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23332486734894572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23332486734894572 | validation: 0.42940242226735087]
	TIME [epoch: 17.6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.363178591738247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.363178591738247 | validation: 0.45989955183661435]
	TIME [epoch: 17.6 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4028902009368531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4028902009368531 | validation: 0.2754405009606098]
	TIME [epoch: 17.6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2480809538773693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2480809538773693 | validation: 0.24525465569930047]
	TIME [epoch: 17.6 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22219339574733737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22219339574733737 | validation: 0.236667115182727]
	TIME [epoch: 17.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16701667550809682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16701667550809682 | validation: 0.262723095104187]
	TIME [epoch: 17.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21582060278909565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21582060278909565 | validation: 0.4046697547160252]
	TIME [epoch: 17.6 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3461042979169463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3461042979169463 | validation: 0.3473554540750894]
	TIME [epoch: 17.7 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29694759977008894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29694759977008894 | validation: 0.23301075662221324]
	TIME [epoch: 17.8 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1556463321771196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1556463321771196 | validation: 0.20516277674338143]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_733.pth
	Model improved!!!
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16347695235636564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16347695235636564 | validation: 0.3193491752868174]
	TIME [epoch: 17.8 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3229771679629836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3229771679629836 | validation: 0.5031227851795281]
	TIME [epoch: 17.8 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4336065131472199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4336065131472199 | validation: 0.21581993258326793]
	TIME [epoch: 17.8 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24267094666390168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24267094666390168 | validation: 0.3287834861598302]
	TIME [epoch: 17.8 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26948880303105865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26948880303105865 | validation: 0.40595847314774736]
	TIME [epoch: 17.8 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3477554677416417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3477554677416417 | validation: 0.40305077830812036]
	TIME [epoch: 17.8 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34823023706175393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34823023706175393 | validation: 0.35520810882443243]
	TIME [epoch: 17.8 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27429644835927747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27429644835927747 | validation: 0.25924326929164526]
	TIME [epoch: 17.8 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19104405425137536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19104405425137536 | validation: 0.20966594183532875]
	TIME [epoch: 17.8 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14715582348207787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14715582348207787 | validation: 0.24221591523476418]
	TIME [epoch: 17.8 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18624763566567992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18624763566567992 | validation: 0.45113885533583914]
	TIME [epoch: 17.8 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4127844671113624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4127844671113624 | validation: 0.27551344659372584]
	TIME [epoch: 17.8 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27926044478528234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27926044478528234 | validation: 0.23949753407407154]
	TIME [epoch: 17.8 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18061001406494712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18061001406494712 | validation: 0.17868057975963242]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13469184006963808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13469184006963808 | validation: 0.21609623947745815]
	TIME [epoch: 17.8 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15407408220126217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15407408220126217 | validation: 0.21210999579907713]
	TIME [epoch: 17.8 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19406028749134346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19406028749134346 | validation: 0.31453596528949335]
	TIME [epoch: 17.8 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22011754365056654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22011754365056654 | validation: 0.3061711452551919]
	TIME [epoch: 17.8 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2433388231065694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2433388231065694 | validation: 0.6175456178970166]
	TIME [epoch: 17.8 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5273246825449153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5273246825449153 | validation: 0.38605631501438936]
	TIME [epoch: 17.8 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29020612524957146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29020612524957146 | validation: 0.23880675256122147]
	TIME [epoch: 17.8 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21612585904324313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21612585904324313 | validation: 0.21475434802952853]
	TIME [epoch: 17.8 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1494653652661852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1494653652661852 | validation: 0.18825007976735242]
	TIME [epoch: 17.8 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14440814818410577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14440814818410577 | validation: 0.30113953064444626]
	TIME [epoch: 17.8 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20833154243023783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20833154243023783 | validation: 0.33539923009293376]
	TIME [epoch: 17.8 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40093250729117663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40093250729117663 | validation: 0.3774249517208104]
	TIME [epoch: 17.7 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3258038234339881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3258038234339881 | validation: 0.4965712287881194]
	TIME [epoch: 17.6 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4140178259647111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4140178259647111 | validation: 0.2822884827052618]
	TIME [epoch: 17.6 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2284845068308574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2284845068308574 | validation: 0.27359440954791797]
	TIME [epoch: 17.6 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22554327333756316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22554327333756316 | validation: 0.44332582906995077]
	TIME [epoch: 17.6 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3614933984979294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3614933984979294 | validation: 0.23264988789640428]
	TIME [epoch: 17.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1717824201718289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1717824201718289 | validation: 0.24712776802502057]
	TIME [epoch: 17.6 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2832927214085825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2832927214085825 | validation: 0.38150643816291874]
	TIME [epoch: 17.6 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3146051078908365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3146051078908365 | validation: 0.27129239202334143]
	TIME [epoch: 17.6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2614163653183594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2614163653183594 | validation: 0.22918253339131472]
	TIME [epoch: 17.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21566517620347891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21566517620347891 | validation: 0.3625241058761956]
	TIME [epoch: 17.6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2639844301133569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2639844301133569 | validation: 0.3726942775912571]
	TIME [epoch: 17.6 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29172165726712523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29172165726712523 | validation: 0.4361547482456245]
	TIME [epoch: 17.6 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44637487642822493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44637487642822493 | validation: 0.32121962976910023]
	TIME [epoch: 17.6 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3142959716190457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3142959716190457 | validation: 0.24915906346026973]
	TIME [epoch: 17.6 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2067887168649789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2067887168649789 | validation: 0.21653650743472808]
	TIME [epoch: 17.6 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14992258070686462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14992258070686462 | validation: 0.3320200417113357]
	TIME [epoch: 17.6 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2513133784348486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2513133784348486 | validation: 0.4820389571883212]
	TIME [epoch: 17.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46990426162162635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46990426162162635 | validation: 0.2400891957530571]
	TIME [epoch: 17.6 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.299541229644752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.299541229644752 | validation: 0.4248323328564985]
	TIME [epoch: 17.6 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3652151031909292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3652151031909292 | validation: 0.2544576048078049]
	TIME [epoch: 17.6 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18664015335647477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18664015335647477 | validation: 0.32109979181338677]
	TIME [epoch: 17.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27191838238664084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27191838238664084 | validation: 0.4635833013060858]
	TIME [epoch: 17.6 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3848796857306449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3848796857306449 | validation: 0.24452658011330908]
	TIME [epoch: 17.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20140260118459768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20140260118459768 | validation: 0.1833540129302489]
	TIME [epoch: 17.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14740871656216445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14740871656216445 | validation: 0.21543903068580184]
	TIME [epoch: 17.6 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15839462597385487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15839462597385487 | validation: 0.2572347211519013]
	TIME [epoch: 17.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20309784499657185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20309784499657185 | validation: 0.37958572588412154]
	TIME [epoch: 17.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31788034194946113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31788034194946113 | validation: 0.2571110907750501]
	TIME [epoch: 17.6 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2559574709141644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2559574709141644 | validation: 0.2152964402354768]
	TIME [epoch: 17.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14314344479722466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14314344479722466 | validation: 0.17749776972273534]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14107758898131123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14107758898131123 | validation: 0.18220706441478549]
	TIME [epoch: 17.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1310093974165977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1310093974165977 | validation: 0.2411448109886176]
	TIME [epoch: 17.7 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20574191621463966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20574191621463966 | validation: 0.3956717445724821]
	TIME [epoch: 17.6 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47145579017797973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47145579017797973 | validation: 0.3471838173719409]
	TIME [epoch: 17.6 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24716684123765403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24716684123765403 | validation: 0.27853522676888537]
	TIME [epoch: 17.6 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2138633246370295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2138633246370295 | validation: 0.465426676504449]
	TIME [epoch: 17.6 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40716953495553654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40716953495553654 | validation: 0.2439439541538318]
	TIME [epoch: 17.7 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18395511116004185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18395511116004185 | validation: 0.2225013574174172]
	TIME [epoch: 17.6 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1698267233020538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1698267233020538 | validation: 0.3570543660075925]
	TIME [epoch: 17.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2623960198841112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2623960198841112 | validation: 0.500106388258683]
	TIME [epoch: 17.6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4252564277098813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4252564277098813 | validation: 0.3759094540564224]
	TIME [epoch: 17.6 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4575933066651574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4575933066651574 | validation: 0.33662680693977587]
	TIME [epoch: 17.7 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4292150259245655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4292150259245655 | validation: 0.6734890111453269]
	TIME [epoch: 17.7 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7351520837957369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7351520837957369 | validation: 0.28726687663077854]
	TIME [epoch: 17.7 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29110828269325195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29110828269325195 | validation: 0.5056256966576601]
	TIME [epoch: 17.7 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42316703288530366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42316703288530366 | validation: 0.22734165561672343]
	TIME [epoch: 17.7 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19964615498532354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19964615498532354 | validation: 0.2834252924803268]
	TIME [epoch: 17.7 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2634740901745939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2634740901745939 | validation: 0.26030148158451505]
	TIME [epoch: 17.7 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21304262999586687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21304262999586687 | validation: 0.2521268274298524]
	TIME [epoch: 17.8 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20362548500363667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20362548500363667 | validation: 0.3159768592188813]
	TIME [epoch: 17.7 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2505106230422465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2505106230422465 | validation: 0.2918985016439365]
	TIME [epoch: 17.7 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22482777145101726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22482777145101726 | validation: 0.22938405746901422]
	TIME [epoch: 17.7 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15574828800842905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15574828800842905 | validation: 0.18726931659644241]
	TIME [epoch: 17.7 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1614236302930371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1614236302930371 | validation: 0.22533979484964584]
	TIME [epoch: 17.7 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1769722495915581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1769722495915581 | validation: 0.19163796827065857]
	TIME [epoch: 17.7 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17323824941761373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17323824941761373 | validation: 0.2164865616485007]
	TIME [epoch: 17.7 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1798565107337246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1798565107337246 | validation: 0.3395770602394035]
	TIME [epoch: 17.7 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3121625328198815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3121625328198815 | validation: 0.5944727388460781]
	TIME [epoch: 17.7 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8003264012925911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8003264012925911 | validation: 0.41657730506032165]
	TIME [epoch: 17.7 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2938070575765978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2938070575765978 | validation: 0.38392451291331153]
	TIME [epoch: 17.7 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34024130920924545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34024130920924545 | validation: 0.25944341833157564]
	TIME [epoch: 17.7 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23384971695665058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23384971695665058 | validation: 0.23003159838823684]
	TIME [epoch: 17.7 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17628806817693346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17628806817693346 | validation: 0.20092991529032284]
	TIME [epoch: 17.7 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15999208127778927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15999208127778927 | validation: 0.3596248617426501]
	TIME [epoch: 17.7 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30551895235327214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30551895235327214 | validation: 0.30213516937696094]
	TIME [epoch: 17.7 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23551545933205562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23551545933205562 | validation: 0.27822308048080646]
	TIME [epoch: 17.7 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24734723458455882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24734723458455882 | validation: 0.22189337811655896]
	TIME [epoch: 17.7 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.207053681617004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.207053681617004 | validation: 0.18434303379101769]
	TIME [epoch: 17.8 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13980694106334693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13980694106334693 | validation: 0.18692296050512006]
	TIME [epoch: 17.7 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13995069683460953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13995069683460953 | validation: 0.18935313724491143]
	TIME [epoch: 17.7 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15445459581789617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15445459581789617 | validation: 0.24336096992519016]
	TIME [epoch: 17.7 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19464147849419072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19464147849419072 | validation: 0.3621341174987847]
	TIME [epoch: 17.7 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32073478589087695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32073478589087695 | validation: 0.26093534047850425]
	TIME [epoch: 17.7 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3002803524115252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3002803524115252 | validation: 0.1830115665483152]
	TIME [epoch: 17.7 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1542979000955579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1542979000955579 | validation: 0.15727868804706147]
	TIME [epoch: 17.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1277286022294793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1277286022294793 | validation: 0.20173226453108808]
	TIME [epoch: 17.7 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1586816215129665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1586816215129665 | validation: 0.3552039568665598]
	TIME [epoch: 17.7 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2941547109447477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2941547109447477 | validation: 0.27607439689500474]
	TIME [epoch: 17.7 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2310181433374728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2310181433374728 | validation: 0.21159536323742373]
	TIME [epoch: 17.7 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13153334493438887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13153334493438887 | validation: 0.22978986024581605]
	TIME [epoch: 17.7 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1990031871158806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1990031871158806 | validation: 0.3344719805136021]
	TIME [epoch: 17.7 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.365973537936913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.365973537936913 | validation: 0.27697971987370085]
	TIME [epoch: 17.7 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20189774295047408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20189774295047408 | validation: 0.200798786832353]
	TIME [epoch: 17.7 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16377723348688306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16377723348688306 | validation: 0.40291077191589786]
	TIME [epoch: 17.7 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35664371673221484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35664371673221484 | validation: 0.16610072216574487]
	TIME [epoch: 17.8 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13097249521421886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13097249521421886 | validation: 0.17703611756549445]
	TIME [epoch: 17.7 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1221043691939254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1221043691939254 | validation: 0.1634902235021777]
	TIME [epoch: 17.7 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13038883359734063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13038883359734063 | validation: 0.1815191146345717]
	TIME [epoch: 17.7 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14466234256735774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14466234256735774 | validation: 0.19082338069575863]
	TIME [epoch: 17.7 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17484953504326453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17484953504326453 | validation: 0.21681370111016562]
	TIME [epoch: 17.7 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18385512881483806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18385512881483806 | validation: 0.5646980216729361]
	TIME [epoch: 17.7 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4720462009839151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4720462009839151 | validation: 0.21812496427085]
	TIME [epoch: 17.7 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16709471644487167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16709471644487167 | validation: 0.29684861558077874]
	TIME [epoch: 17.7 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24255927795068588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24255927795068588 | validation: 0.5893599418597446]
	TIME [epoch: 17.7 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5281024923181247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5281024923181247 | validation: 0.5088407946560926]
	TIME [epoch: 17.7 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38675738431314544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38675738431314544 | validation: 0.5173246392335357]
	TIME [epoch: 17.7 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46367690205548456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46367690205548456 | validation: 0.30957449773865536]
	TIME [epoch: 17.7 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2475500564499992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2475500564499992 | validation: 0.4076212230675193]
	TIME [epoch: 17.7 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30571934872577594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30571934872577594 | validation: 0.22423216596151385]
	TIME [epoch: 17.7 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13893777699668095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13893777699668095 | validation: 0.26137753676222736]
	TIME [epoch: 17.7 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21139685356344373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21139685356344373 | validation: 0.16764763322288606]
	TIME [epoch: 17.7 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11921865617568134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11921865617568134 | validation: 0.23094239715709117]
	TIME [epoch: 17.7 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1625262408303698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1625262408303698 | validation: 0.38586749140835497]
	TIME [epoch: 17.7 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30363232107430377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30363232107430377 | validation: 0.32346611710569717]
	TIME [epoch: 17.7 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3334214938068169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3334214938068169 | validation: 0.4511151063269388]
	TIME [epoch: 17.7 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36949989638592284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36949989638592284 | validation: 0.30040066306251373]
	TIME [epoch: 17.7 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2524276665832824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2524276665832824 | validation: 0.19280512302628627]
	TIME [epoch: 17.7 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14763032590590486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14763032590590486 | validation: 0.1766790745612843]
	TIME [epoch: 17.7 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14219729729364486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14219729729364486 | validation: 0.15651476571339065]
	TIME [epoch: 17.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_868.pth
	Model improved!!!
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13643392601995466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13643392601995466 | validation: 0.19651665414275635]
	TIME [epoch: 17.7 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15552889105220108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15552889105220108 | validation: 0.2716273606759847]
	TIME [epoch: 17.7 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24454816375491398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24454816375491398 | validation: 0.23999225531341964]
	TIME [epoch: 17.7 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19664903973496944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19664903973496944 | validation: 0.3208162389615461]
	TIME [epoch: 17.7 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2610690757753175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2610690757753175 | validation: 0.19624292474046034]
	TIME [epoch: 17.7 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15470940881526665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15470940881526665 | validation: 0.17061419330618205]
	TIME [epoch: 17.7 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15130445711013388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15130445711013388 | validation: 0.39047007207274625]
	TIME [epoch: 17.7 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3999696656032964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3999696656032964 | validation: 0.20348637761167507]
	TIME [epoch: 17.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21422074643895656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21422074643895656 | validation: 0.25566991783775384]
	TIME [epoch: 17.7 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19259764693228323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19259764693228323 | validation: 0.1604263077232819]
	TIME [epoch: 17.7 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13105411278903473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13105411278903473 | validation: 0.19911614842224715]
	TIME [epoch: 17.7 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12024829044406181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12024829044406181 | validation: 0.40781310816031874]
	TIME [epoch: 17.6 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34551234625754473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34551234625754473 | validation: 0.46364962222545314]
	TIME [epoch: 17.7 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4700567406220419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4700567406220419 | validation: 0.6701008849625358]
	TIME [epoch: 17.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6979421378324341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6979421378324341 | validation: 1.5877399456242742]
	TIME [epoch: 17.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8061991326780866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8061991326780866 | validation: 2.0543284588252813]
	TIME [epoch: 17.7 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3869329785208158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3869329785208158 | validation: 2.2523176188128073]
	TIME [epoch: 17.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5898360051812186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5898360051812186 | validation: 2.4763369200580225]
	TIME [epoch: 17.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8099086478028164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8099086478028164 | validation: 2.4581229051877553]
	TIME [epoch: 17.7 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.759224375761328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.759224375761328 | validation: 2.3727768805916614]
	TIME [epoch: 17.7 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.683974134922671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.683974134922671 | validation: 2.297308917585265]
	TIME [epoch: 17.6 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.66203944605327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.66203944605327 | validation: 2.2319830141295927]
	TIME [epoch: 17.7 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5776971143329117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5776971143329117 | validation: 2.165571676266914]
	TIME [epoch: 17.7 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4939262196990954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4939262196990954 | validation: 2.111216975552262]
	TIME [epoch: 17.8 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4199437438908977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4199437438908977 | validation: 2.015723164678827]
	TIME [epoch: 17.9 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.333847322412727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.333847322412727 | validation: 1.891207046480216]
	TIME [epoch: 17.8 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2305761717525945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2305761717525945 | validation: 1.9075652983496436]
	TIME [epoch: 17.8 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.223673512074956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.223673512074956 | validation: 1.7840027694081546]
	TIME [epoch: 17.8 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.058257300640165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.058257300640165 | validation: 1.7758040470630925]
	TIME [epoch: 17.8 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.077071949044812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.077071949044812 | validation: 1.5994611854089127]
	TIME [epoch: 17.8 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9357599091673092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9357599091673092 | validation: 1.7363019469774656]
	TIME [epoch: 17.7 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.047523282635032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.047523282635032 | validation: 1.6096110562713535]
	TIME [epoch: 17.8 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8815089609196134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8815089609196134 | validation: 1.6947854092130268]
	TIME [epoch: 17.8 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9629709480267752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9629709480267752 | validation: 1.4546014331318555]
	TIME [epoch: 17.8 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6830161419149698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6830161419149698 | validation: 1.464895118681482]
	TIME [epoch: 17.7 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.629234851291047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.629234851291047 | validation: 1.2503506249050595]
	TIME [epoch: 17.7 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3792747536629026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3792747536629026 | validation: 1.359762575083789]
	TIME [epoch: 17.7 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.42713929703597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.42713929703597 | validation: 1.1488139687059096]
	TIME [epoch: 17.7 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2802541581054565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2802541581054565 | validation: 1.1619649842153876]
	TIME [epoch: 17.8 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2995986388153948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2995986388153948 | validation: 1.5514518433012865]
	TIME [epoch: 17.8 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0046643289220833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0046643289220833 | validation: 0.9449465502669215]
	TIME [epoch: 17.8 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.068366929636052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.068366929636052 | validation: 0.48650763680204556]
	TIME [epoch: 17.8 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4066781969349691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4066781969349691 | validation: 0.5545138595497664]
	TIME [epoch: 17.9 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45755505550202585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45755505550202585 | validation: 0.48086972109149584]
	TIME [epoch: 17.8 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4083822248115848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4083822248115848 | validation: 0.3474724824089263]
	TIME [epoch: 17.8 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24835264559075654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24835264559075654 | validation: 0.2980504039564883]
	TIME [epoch: 17.8 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2130457099368232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2130457099368232 | validation: 0.37847253789743157]
	TIME [epoch: 17.7 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31501228124447056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31501228124447056 | validation: 0.30294113424564184]
	TIME [epoch: 17.7 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23918026098032172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23918026098032172 | validation: 0.2658969833248368]
	TIME [epoch: 17.8 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21317203029936602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21317203029936602 | validation: 0.23786355315157623]
	TIME [epoch: 17.9 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1732873351749241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1732873351749241 | validation: 0.24910835089557246]
	TIME [epoch: 17.8 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17505233734647108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17505233734647108 | validation: 0.29733424766071676]
	TIME [epoch: 17.8 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2414543158160022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2414543158160022 | validation: 0.3594433256829497]
	TIME [epoch: 17.8 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.361112697272882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.361112697272882 | validation: 0.2676642717414502]
	TIME [epoch: 17.8 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2924789996820127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2924789996820127 | validation: 0.9479640762096134]
	TIME [epoch: 17.8 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.983392502872349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.983392502872349 | validation: 0.5837034803225384]
	TIME [epoch: 17.9 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5007891215793019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5007891215793019 | validation: 0.29018928192942234]
	TIME [epoch: 17.8 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2223118886481384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2223118886481384 | validation: 0.4949717771891181]
	TIME [epoch: 17.8 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3970018540746743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3970018540746743 | validation: 0.3006625739842019]
	TIME [epoch: 17.8 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.260416468086136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.260416468086136 | validation: 0.30842344036806013]
	TIME [epoch: 17.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24202072397173743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24202072397173743 | validation: 0.26291380426145955]
	TIME [epoch: 17.7 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1930494906679371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1930494906679371 | validation: 0.21535491397771753]
	TIME [epoch: 17.7 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1440160564373516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1440160564373516 | validation: 0.2190811597882276]
	TIME [epoch: 17.8 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1561847758925099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1561847758925099 | validation: 0.18930403085383474]
	TIME [epoch: 17.8 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.142277096129296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.142277096129296 | validation: 0.22989339318426183]
	TIME [epoch: 17.8 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16375932768630846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16375932768630846 | validation: 0.2602134024061047]
	TIME [epoch: 17.8 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2086817446001015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2086817446001015 | validation: 0.25097988337218585]
	TIME [epoch: 17.8 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20262602388653406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20262602388653406 | validation: 0.18001133725538826]
	TIME [epoch: 17.8 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1385538242380089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1385538242380089 | validation: 0.18631054002902075]
	TIME [epoch: 17.8 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12171337143298801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12171337143298801 | validation: 0.17627285202225892]
	TIME [epoch: 17.7 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12657840947489646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12657840947489646 | validation: 0.23764760245562566]
	TIME [epoch: 17.7 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18169048900004106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18169048900004106 | validation: 0.25068050860414165]
	TIME [epoch: 17.7 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22360601155229753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22360601155229753 | validation: 0.30667073994493843]
	TIME [epoch: 17.7 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2669242326468311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2669242326468311 | validation: 0.16090477688889773]
	TIME [epoch: 17.8 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15088439730150666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15088439730150666 | validation: 0.2582311585691994]
	TIME [epoch: 17.8 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24543625283362722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24543625283362722 | validation: 0.5926578907432605]
	TIME [epoch: 17.8 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.550279207238456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.550279207238456 | validation: 0.4782437983148885]
	TIME [epoch: 17.8 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46473145528093746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46473145528093746 | validation: 0.40102813647108015]
	TIME [epoch: 17.8 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43983009665494704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43983009665494704 | validation: 0.38065224525322006]
	TIME [epoch: 17.8 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39931771163809404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39931771163809404 | validation: 0.34020303134479485]
	TIME [epoch: 17.8 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32166031621777635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32166031621777635 | validation: 0.6300283637111094]
	TIME [epoch: 17.8 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5851276614625635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5851276614625635 | validation: 0.2789690562601286]
	TIME [epoch: 17.8 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2708189639658987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2708189639658987 | validation: 0.34448611039624866]
	TIME [epoch: 17.8 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28585428112973454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28585428112973454 | validation: 0.18392495770620274]
	TIME [epoch: 17.7 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1498974328197294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1498974328197294 | validation: 0.19579208881100332]
	TIME [epoch: 17.7 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1639063120846025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1639063120846025 | validation: 0.18164009375610127]
	TIME [epoch: 17.7 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12763251808553058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12763251808553058 | validation: 0.18531742848586105]
	TIME [epoch: 17.7 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12762154217502564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12762154217502564 | validation: 0.186495633544657]
	TIME [epoch: 17.8 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13321013984528557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13321013984528557 | validation: 0.28993790227366495]
	TIME [epoch: 17.8 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24106430106492455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24106430106492455 | validation: 0.34824109120884544]
	TIME [epoch: 17.8 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3379592530589646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3379592530589646 | validation: 0.216397220113587]
	TIME [epoch: 17.8 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22876384006163356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22876384006163356 | validation: 0.16403383688366724]
	TIME [epoch: 17.8 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1698164488910938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1698164488910938 | validation: 0.1410577870759618]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_961.pth
	Model improved!!!
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12590812633473697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12590812633473697 | validation: 0.2696348152326304]
	TIME [epoch: 17.8 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22836826091104637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22836826091104637 | validation: 0.4249372037220539]
	TIME [epoch: 17.7 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3902900090657006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3902900090657006 | validation: 0.25258539446919137]
	TIME [epoch: 17.7 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23347424695874375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23347424695874375 | validation: 0.27034113795435877]
	TIME [epoch: 17.6 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2535123846175233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2535123846175233 | validation: 0.21409907805202702]
	TIME [epoch: 17.6 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1470933114363808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1470933114363808 | validation: 0.1841938206787246]
	TIME [epoch: 17.6 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17209043044510508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17209043044510508 | validation: 0.29156690585500267]
	TIME [epoch: 17.6 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24714524517302963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24714524517302963 | validation: 0.21908047012190457]
	TIME [epoch: 17.6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1674198726725327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1674198726725327 | validation: 0.14862218574807617]
	TIME [epoch: 17.7 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13725573897509727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13725573897509727 | validation: 0.17585965285214622]
	TIME [epoch: 17.7 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1347778895012787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1347778895012787 | validation: 0.17567827131667235]
	TIME [epoch: 17.6 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14182388368371823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14182388368371823 | validation: 0.2629190632030576]
	TIME [epoch: 17.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.215812804790015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.215812804790015 | validation: 0.1870850619837475]
	TIME [epoch: 17.7 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14163578231145424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14163578231145424 | validation: 0.15277701812581518]
	TIME [epoch: 17.6 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10381629982705777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10381629982705777 | validation: 0.14648363726591254]
	TIME [epoch: 17.6 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1210827568857338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1210827568857338 | validation: 0.25790024265038025]
	TIME [epoch: 17.7 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25013752376306275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25013752376306275 | validation: 0.22293147802227972]
	TIME [epoch: 17.6 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22430407428929186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22430407428929186 | validation: 0.26983095980970306]
	TIME [epoch: 17.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2001219286039329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2001219286039329 | validation: 0.2188936000174501]
	TIME [epoch: 17.7 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17987051105778498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17987051105778498 | validation: 0.21314029004987592]
	TIME [epoch: 17.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13226145278148682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13226145278148682 | validation: 0.4889433168228459]
	TIME [epoch: 17.6 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39268947812702915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39268947812702915 | validation: 0.26869133594557776]
	TIME [epoch: 17.6 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2980416503668787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2980416503668787 | validation: 0.6643973695755849]
	TIME [epoch: 17.6 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5157439316762868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5157439316762868 | validation: 0.33339968662855357]
	TIME [epoch: 17.6 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3260024170353988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3260024170353988 | validation: 0.2583544809348725]
	TIME [epoch: 17.6 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2263984545212459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2263984545212459 | validation: 0.2816665532848961]
	TIME [epoch: 17.6 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21421536407174224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21421536407174224 | validation: 0.18590932318865594]
	TIME [epoch: 17.7 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17122258364044893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17122258364044893 | validation: 0.21935551988620128]
	TIME [epoch: 17.7 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1819576609489789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1819576609489789 | validation: 0.19469545532791288]
	TIME [epoch: 17.7 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14772509014596114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14772509014596114 | validation: 0.1582139671200256]
	TIME [epoch: 17.6 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1090476209475174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1090476209475174 | validation: 0.16678731995704388]
	TIME [epoch: 17.6 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11436106409004579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11436106409004579 | validation: 0.13558510551249073]
	TIME [epoch: 17.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_993.pth
	Model improved!!!
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10841119718873589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10841119718873589 | validation: 0.18458870410374673]
	TIME [epoch: 17.6 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13556056182829004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13556056182829004 | validation: 0.16776858846015627]
	TIME [epoch: 17.7 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14678294446260723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14678294446260723 | validation: 0.21959739405034556]
	TIME [epoch: 17.6 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16880812275072352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16880812275072352 | validation: 0.14863912756553646]
	TIME [epoch: 17.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10828087260531996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10828087260531996 | validation: 0.13999924266986483]
	TIME [epoch: 17.7 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0937002474074903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0937002474074903 | validation: 0.15125650469712207]
	TIME [epoch: 17.7 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1321840660401552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1321840660401552 | validation: 0.2826255597433095]
	TIME [epoch: 17.6 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3032165194124823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3032165194124823 | validation: 0.23087673004541137]
	TIME [epoch: 58.9 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2572116222641652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2572116222641652 | validation: 0.1956922193977201]
	TIME [epoch: 36.6 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16022771053193005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16022771053193005 | validation: 0.15418526864668788]
	TIME [epoch: 36.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10245568874003336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10245568874003336 | validation: 0.163517989322814]
	TIME [epoch: 36.6 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12943360497923123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12943360497923123 | validation: 0.30791996680220385]
	TIME [epoch: 36.6 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2956311950804747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2956311950804747 | validation: 0.24627948712398764]
	TIME [epoch: 36.6 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19697244978757802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19697244978757802 | validation: 0.20337535849822116]
	TIME [epoch: 36.7 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18905150126668893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18905150126668893 | validation: 0.512742556592764]
	TIME [epoch: 36.6 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3862373586493882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3862373586493882 | validation: 0.19760768276936364]
	TIME [epoch: 36.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17038092871271818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17038092871271818 | validation: 0.2334314936571348]
	TIME [epoch: 36.6 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22582098800021366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22582098800021366 | validation: 0.2243470709703308]
	TIME [epoch: 36.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1894434426111569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1894434426111569 | validation: 0.16036060558338217]
	TIME [epoch: 36.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11927692193674216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11927692193674216 | validation: 0.14094595365975313]
	TIME [epoch: 36.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10930356266831587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10930356266831587 | validation: 0.16759769229407284]
	TIME [epoch: 36.6 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1666970746667128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1666970746667128 | validation: 0.23149561072988947]
	TIME [epoch: 36.6 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2537439027656574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2537439027656574 | validation: 0.17351281348071668]
	TIME [epoch: 36.6 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.150508851852434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.150508851852434 | validation: 0.1132608613078108]
	TIME [epoch: 36.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_1017.pth
	Model improved!!!
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10821372028386353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10821372028386353 | validation: 0.10704020165079108]
	TIME [epoch: 36.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_1018.pth
	Model improved!!!
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10472502617688015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10472502617688015 | validation: 0.118088024945946]
	TIME [epoch: 36.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09737883556572262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09737883556572262 | validation: 0.2634143898771977]
	TIME [epoch: 36.6 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2367473460159524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2367473460159524 | validation: 0.5107200917703806]
	TIME [epoch: 36.6 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43176046889004555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43176046889004555 | validation: 0.4966488559661342]
	TIME [epoch: 36.6 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5204273741082104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5204273741082104 | validation: 0.4371720041491518]
	TIME [epoch: 36.6 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44354573762483973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44354573762483973 | validation: 0.5632193716794975]
	TIME [epoch: 36.6 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5745705273557488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5745705273557488 | validation: 0.70577052466035]
	TIME [epoch: 36.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5961148294167816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5961148294167816 | validation: 0.4351592876782011]
	TIME [epoch: 36.6 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4035711162624655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4035711162624655 | validation: 0.40434687006213754]
	TIME [epoch: 36.6 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36001775060432345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36001775060432345 | validation: 0.30359874476942283]
	TIME [epoch: 36.6 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22180928516184437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22180928516184437 | validation: 0.2583482895636197]
	TIME [epoch: 36.6 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1871044456241124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1871044456241124 | validation: 0.277853366257472]
	TIME [epoch: 36.6 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20985934073298637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20985934073298637 | validation: 0.20021344900889979]
	TIME [epoch: 36.6 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13758173206925212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13758173206925212 | validation: 0.18022532818923978]
	TIME [epoch: 36.6 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12754523998371184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12754523998371184 | validation: 0.15025454197146312]
	TIME [epoch: 36.6 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11528374582895491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11528374582895491 | validation: 0.1800416675736326]
	TIME [epoch: 36.6 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12553552163834042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12553552163834042 | validation: 0.1390551041971102]
	TIME [epoch: 36.6 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11608144428953546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11608144428953546 | validation: 0.21055359866095671]
	TIME [epoch: 36.6 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1453304435819417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1453304435819417 | validation: 0.13744607363152941]
	TIME [epoch: 36.6 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11504719378219427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11504719378219427 | validation: 0.19832934688465595]
	TIME [epoch: 36.6 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14767993955614253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14767993955614253 | validation: 0.129364465393734]
	TIME [epoch: 36.6 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1060947804237108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1060947804237108 | validation: 0.1774757996266744]
	TIME [epoch: 36.6 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11466549708953387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11466549708953387 | validation: 0.19674927571033796]
	TIME [epoch: 36.6 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16603631947984365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16603631947984365 | validation: 0.2624048303985968]
	TIME [epoch: 36.6 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2477940216235939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2477940216235939 | validation: 0.20656599024273314]
	TIME [epoch: 36.6 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20581067568318548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20581067568318548 | validation: 0.1646175807087898]
	TIME [epoch: 36.6 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14342060235332707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14342060235332707 | validation: 0.17762603219094894]
	TIME [epoch: 36.7 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12371885698426362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12371885698426362 | validation: 0.10245464657165543]
	TIME [epoch: 36.9 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_1046.pth
	Model improved!!!
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08260151087253308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08260151087253308 | validation: 0.143846618048353]
	TIME [epoch: 36.6 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10819455444267223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10819455444267223 | validation: 0.15477928762406246]
	TIME [epoch: 36.6 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13651146885306312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13651146885306312 | validation: 0.228074169430023]
	TIME [epoch: 36.6 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18995391333255768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18995391333255768 | validation: 0.2348855156381942]
	TIME [epoch: 36.6 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19817648114888428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19817648114888428 | validation: 0.309627518122218]
	TIME [epoch: 36.6 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28616099643608156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28616099643608156 | validation: 0.2126795419309743]
	TIME [epoch: 36.6 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22938295469864045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22938295469864045 | validation: 0.22890677969942008]
	TIME [epoch: 36.6 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2019428442151131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2019428442151131 | validation: 0.1602682875807615]
	TIME [epoch: 36.6 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12255201828241927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12255201828241927 | validation: 0.12524713486758432]
	TIME [epoch: 36.6 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1477796760404738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1477796760404738 | validation: 0.27721134004240755]
	TIME [epoch: 36.6 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26083531628619994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26083531628619994 | validation: 0.14907662443205577]
	TIME [epoch: 36.6 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1527763210824556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1527763210824556 | validation: 0.3060968991452995]
	TIME [epoch: 36.6 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21246111746575608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21246111746575608 | validation: 0.13201521892119175]
	TIME [epoch: 36.6 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10742948064024649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10742948064024649 | validation: 0.14183394088468632]
	TIME [epoch: 36.6 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13366056114565084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13366056114565084 | validation: 0.3865553948658293]
	TIME [epoch: 36.6 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30989961049006315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30989961049006315 | validation: 0.17839102834841067]
	TIME [epoch: 36.6 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12811882831073537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12811882831073537 | validation: 0.10631240989864624]
	TIME [epoch: 36.6 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09337794886047891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09337794886047891 | validation: 0.21099530172533298]
	TIME [epoch: 36.6 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15381481645222167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15381481645222167 | validation: 0.17419631947092473]
	TIME [epoch: 36.6 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1891039501255098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1891039501255098 | validation: 0.2337427099668239]
	TIME [epoch: 36.6 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2476408236548261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2476408236548261 | validation: 0.22250972058449747]
	TIME [epoch: 36.6 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16073747204616845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16073747204616845 | validation: 0.11540877953745428]
	TIME [epoch: 36.6 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10092067356309864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10092067356309864 | validation: 0.14869730767897474]
	TIME [epoch: 36.6 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13315798388979172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13315798388979172 | validation: 0.19717275349454644]
	TIME [epoch: 36.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22207453840849717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22207453840849717 | validation: 0.3004878395508177]
	TIME [epoch: 36.6 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27305044676432816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27305044676432816 | validation: 0.1930575189052841]
	TIME [epoch: 36.6 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20420513965321754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20420513965321754 | validation: 0.19477989433994536]
	TIME [epoch: 36.6 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1631719417401574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1631719417401574 | validation: 0.5599023507842488]
	TIME [epoch: 36.6 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4757127268516929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4757127268516929 | validation: 0.24256495266184422]
	TIME [epoch: 36.6 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1803238377635909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1803238377635909 | validation: 0.2355436029244797]
	TIME [epoch: 36.6 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1875967162025848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1875967162025848 | validation: 0.25251299354543216]
	TIME [epoch: 36.6 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17543637508248128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17543637508248128 | validation: 0.1947365854167708]
	TIME [epoch: 36.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1226056418016954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1226056418016954 | validation: 0.20205276082756943]
	TIME [epoch: 36.7 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11613532360341693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11613532360341693 | validation: 0.12640033453401414]
	TIME [epoch: 36.6 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11484912550590078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11484912550590078 | validation: 0.20904851342381336]
	TIME [epoch: 36.5 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15007404118423984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15007404118423984 | validation: 0.12889607400469583]
	TIME [epoch: 36.6 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10390975198195967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10390975198195967 | validation: 0.1723301333734876]
	TIME [epoch: 36.6 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09489709249863072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09489709249863072 | validation: 0.1554376235129613]
	TIME [epoch: 36.6 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14358132938333878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14358132938333878 | validation: 0.2324084930520609]
	TIME [epoch: 36.6 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2053900487026697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2053900487026697 | validation: 0.19096564351943757]
	TIME [epoch: 36.6 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11843256261547999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11843256261547999 | validation: 0.10910026915840228]
	TIME [epoch: 36.6 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0909416155442909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0909416155442909 | validation: 0.16744075828037777]
	TIME [epoch: 36.6 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13345983235017658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13345983235017658 | validation: 0.21180290357157242]
	TIME [epoch: 36.6 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20276197886029576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20276197886029576 | validation: 0.2301781463398386]
	TIME [epoch: 36.6 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21234615246971342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21234615246971342 | validation: 1.3451020473598752]
	TIME [epoch: 36.6 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.648483588458477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.648483588458477 | validation: 0.843453630505459]
	TIME [epoch: 36.6 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1950558060731098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1950558060731098 | validation: 0.46824240703704734]
	TIME [epoch: 36.6 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6648548221155037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6648548221155037 | validation: 0.44586408221284163]
	TIME [epoch: 36.6 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42704021343952564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42704021343952564 | validation: 0.24879450314669072]
	TIME [epoch: 36.6 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29026786784359226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29026786784359226 | validation: 0.1720252632690883]
	TIME [epoch: 36.6 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19809266082441107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19809266082441107 | validation: 0.2427197580444661]
	TIME [epoch: 36.6 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20255172575303504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20255172575303504 | validation: 0.15282387680787932]
	TIME [epoch: 36.6 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11368162485552936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11368162485552936 | validation: 0.1238333003330921]
	TIME [epoch: 36.6 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11171557869519078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11171557869519078 | validation: 0.21379528893446964]
	TIME [epoch: 36.6 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15109844969670208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15109844969670208 | validation: 0.17489105641377956]
	TIME [epoch: 36.6 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1594049482422467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1594049482422467 | validation: 0.23721301497899816]
	TIME [epoch: 36.6 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18688512253964878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18688512253964878 | validation: 0.18854617331094414]
	TIME [epoch: 36.6 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12292978412381313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12292978412381313 | validation: 0.12558025180393503]
	TIME [epoch: 36.6 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11841676492936584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11841676492936584 | validation: 0.20177026488075148]
	TIME [epoch: 36.6 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14831429639470473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14831429639470473 | validation: 0.10454283494670343]
	TIME [epoch: 36.6 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08445880917372746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08445880917372746 | validation: 0.09703862604334516]
	TIME [epoch: 36.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_1107.pth
	Model improved!!!
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07471054095463854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07471054095463854 | validation: 0.08401586672547835]
	TIME [epoch: 36.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_1108.pth
	Model improved!!!
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07687900861081412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07687900861081412 | validation: 0.10351206031047254]
	TIME [epoch: 36.6 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07834494175860879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07834494175860879 | validation: 0.33103110148575254]
	TIME [epoch: 36.6 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30506376523755824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30506376523755824 | validation: 0.2622767134044606]
	TIME [epoch: 36.6 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2513684974929535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2513684974929535 | validation: 0.20364486695872272]
	TIME [epoch: 36.6 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19736512531366834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19736512531366834 | validation: 0.2268719795588889]
	TIME [epoch: 36.6 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18811142864806646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18811142864806646 | validation: 0.17683661822128696]
	TIME [epoch: 36.6 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10482847098396192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10482847098396192 | validation: 0.09145893804858751]
	TIME [epoch: 36.6 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10337098885696433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10337098885696433 | validation: 0.15234042381432927]
	TIME [epoch: 36.6 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16435590865608657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16435590865608657 | validation: 0.14624389271131674]
	TIME [epoch: 36.6 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18618428738040535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18618428738040535 | validation: 0.19651796651787082]
	TIME [epoch: 36.6 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20574286394270863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20574286394270863 | validation: 0.14242575336349797]
	TIME [epoch: 36.6 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13930149378717735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13930149378717735 | validation: 0.1421832862230991]
	TIME [epoch: 36.6 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11303794760334497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11303794760334497 | validation: 0.12535249544628344]
	TIME [epoch: 36.6 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1118965044370775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1118965044370775 | validation: 0.1543989224606248]
	TIME [epoch: 36.6 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14335749893672584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14335749893672584 | validation: 0.3680001089471106]
	TIME [epoch: 36.6 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2979009647597352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2979009647597352 | validation: 0.22620900302254704]
	TIME [epoch: 36.6 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1484947455612497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1484947455612497 | validation: 0.12755253080791099]
	TIME [epoch: 36.6 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11521833181831094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11521833181831094 | validation: 0.2447099960669421]
	TIME [epoch: 36.6 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1598960974921759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1598960974921759 | validation: 0.12111011442666358]
	TIME [epoch: 36.6 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11601404472792905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11601404472792905 | validation: 0.18126210308420493]
	TIME [epoch: 36.6 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15009301795782465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15009301795782465 | validation: 0.27482451534400837]
	TIME [epoch: 36.6 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23281776575221105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23281776575221105 | validation: 0.16203351532985022]
	TIME [epoch: 36.6 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17746468785734937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17746468785734937 | validation: 0.26973565488786033]
	TIME [epoch: 36.6 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2126355068930052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2126355068930052 | validation: 0.11643388642579494]
	TIME [epoch: 36.6 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09531765229403456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09531765229403456 | validation: 0.13032628264553436]
	TIME [epoch: 36.6 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0909490608391597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0909490608391597 | validation: 0.12228873217309129]
	TIME [epoch: 36.6 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10124802718726139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10124802718726139 | validation: 0.20415102233427862]
	TIME [epoch: 36.7 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14282670018281052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14282670018281052 | validation: 0.12271945642674004]
	TIME [epoch: 36.6 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10037789306476606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10037789306476606 | validation: 0.18862000096898504]
	TIME [epoch: 36.6 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10562673680102937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10562673680102937 | validation: 0.10932164602082875]
	TIME [epoch: 36.6 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09499085586478952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09499085586478952 | validation: 0.19124477909398535]
	TIME [epoch: 36.6 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14773257049712163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14773257049712163 | validation: 0.11096960450033584]
	TIME [epoch: 36.6 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09983530717632273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09983530717632273 | validation: 0.1502003951232018]
	TIME [epoch: 36.6 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1167980167778836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1167980167778836 | validation: 0.10343784141187116]
	TIME [epoch: 36.6 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08362460667946611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08362460667946611 | validation: 0.12075076788685113]
	TIME [epoch: 36.6 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09956143383824223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09956143383824223 | validation: 0.1273886881899294]
	TIME [epoch: 36.6 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11746637512252996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11746637512252996 | validation: 0.3172215889591273]
	TIME [epoch: 36.6 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2767837099431584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2767837099431584 | validation: 0.13154457930424276]
	TIME [epoch: 36.6 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12115763162470317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12115763162470317 | validation: 1.1256023348009392]
	TIME [epoch: 36.7 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2443199097762665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2443199097762665 | validation: 0.292668553166669]
	TIME [epoch: 36.6 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3518615681721904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3518615681721904 | validation: 0.3644454076582057]
	TIME [epoch: 36.6 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2982700245827737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2982700245827737 | validation: 0.30120880322616994]
	TIME [epoch: 36.6 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20490691891609175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20490691891609175 | validation: 0.1111147413350112]
	TIME [epoch: 36.6 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11222943535835408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11222943535835408 | validation: 0.13379220025583463]
	TIME [epoch: 36.6 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10878598372305025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10878598372305025 | validation: 0.17386727123664936]
	TIME [epoch: 36.6 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18007373778236418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18007373778236418 | validation: 0.22371325630950933]
	TIME [epoch: 36.6 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2076878791100493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2076878791100493 | validation: 0.2018189014042637]
	TIME [epoch: 36.7 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.159392497306678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.159392497306678 | validation: 0.24224236045728664]
	TIME [epoch: 36.6 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24587928688801872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24587928688801872 | validation: 0.11857439145415756]
	TIME [epoch: 36.6 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09296455031075482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09296455031075482 | validation: 0.1566515614410201]
	TIME [epoch: 36.6 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1271099697705164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1271099697705164 | validation: 0.0994970428667109]
	TIME [epoch: 36.6 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10277053300055712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10277053300055712 | validation: 0.13539678346689005]
	TIME [epoch: 36.6 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12830998873661956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12830998873661956 | validation: 0.10672401541738497]
	TIME [epoch: 36.6 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08686074805314327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08686074805314327 | validation: 0.09971388733471144]
	TIME [epoch: 36.6 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07021602307655758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07021602307655758 | validation: 0.08808070366714549]
	TIME [epoch: 36.6 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08125718516463008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08125718516463008 | validation: 0.10716802194587952]
	TIME [epoch: 36.6 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11707005991100136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11707005991100136 | validation: 0.19144295875497935]
	TIME [epoch: 36.7 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14941655491101247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14941655491101247 | validation: 0.1160447847191724]
	TIME [epoch: 36.6 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11304145081679685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11304145081679685 | validation: 0.07958545317316666]
	TIME [epoch: 36.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_1167.pth
	Model improved!!!
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07728492912083283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07728492912083283 | validation: 0.2253659827720681]
	TIME [epoch: 36.7 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14633124102170078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14633124102170078 | validation: 0.15764250284064418]
	TIME [epoch: 36.6 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14552473427862497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14552473427862497 | validation: 0.22502357835679387]
	TIME [epoch: 36.6 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18287469574650023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18287469574650023 | validation: 0.6346080655245663]
	TIME [epoch: 36.6 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5933668460057875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5933668460057875 | validation: 0.423379736416392]
	TIME [epoch: 36.6 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36860973390028734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36860973390028734 | validation: 0.23537440370564375]
	TIME [epoch: 36.6 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23412658706192233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23412658706192233 | validation: 0.16736757418129306]
	TIME [epoch: 36.6 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16887808571998725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16887808571998725 | validation: 0.18443581346546983]
	TIME [epoch: 36.6 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1551885263523329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1551885263523329 | validation: 0.1917558498952674]
	TIME [epoch: 36.6 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16805965649871876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16805965649871876 | validation: 0.1722259825165219]
	TIME [epoch: 36.6 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12821301232685783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12821301232685783 | validation: 0.12593735671236483]
	TIME [epoch: 36.6 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09281053287898682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09281053287898682 | validation: 0.14570275652941964]
	TIME [epoch: 36.6 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07707390706948702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07707390706948702 | validation: 0.08932601164205704]
	TIME [epoch: 36.6 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08328578122865642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08328578122865642 | validation: 0.19264935990840235]
	TIME [epoch: 36.6 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14009469476785533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14009469476785533 | validation: 0.11061454042960778]
	TIME [epoch: 36.6 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08582898221560654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08582898221560654 | validation: 0.13267189775420082]
	TIME [epoch: 36.6 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0787143563133032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0787143563133032 | validation: 0.1035929649314932]
	TIME [epoch: 36.6 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10126244453611775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10126244453611775 | validation: 0.23604190431085117]
	TIME [epoch: 36.6 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19753040115506765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19753040115506765 | validation: 0.14267878453519597]
	TIME [epoch: 36.6 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09090729638175801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09090729638175801 | validation: 0.0684958378087434]
	TIME [epoch: 36.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_1187.pth
	Model improved!!!
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06488975238899373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06488975238899373 | validation: 0.14860772249502627]
	TIME [epoch: 36.6 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11520606673720839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11520606673720839 | validation: 0.19018513598724374]
	TIME [epoch: 36.6 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.172458330270836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.172458330270836 | validation: 0.13234133706931703]
	TIME [epoch: 36.6 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15784297467165562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15784297467165562 | validation: 0.0994676584738749]
	TIME [epoch: 36.6 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07913787260197921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07913787260197921 | validation: 0.10855940211827747]
	TIME [epoch: 36.6 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10217691951723183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10217691951723183 | validation: 0.31461611308148457]
	TIME [epoch: 36.6 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2602782790087324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2602782790087324 | validation: 0.1027205298197629]
	TIME [epoch: 36.7 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08267801125832552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08267801125832552 | validation: 0.06966975947217281]
	TIME [epoch: 36.7 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05838696108125634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05838696108125634 | validation: 0.16209338181349653]
	TIME [epoch: 36.6 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12476359725232225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12476359725232225 | validation: 0.18599848521018592]
	TIME [epoch: 36.6 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18202991710609204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18202991710609204 | validation: 0.24386902166397154]
	TIME [epoch: 36.6 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20135366437379607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20135366437379607 | validation: 0.184379402199012]
	TIME [epoch: 36.6 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2065852538013851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2065852538013851 | validation: 0.2044903978545722]
	TIME [epoch: 36.5 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14928423012815661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14928423012815661 | validation: 0.08225131039352802]
	TIME [epoch: 36.6 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10349059639726389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10349059639726389 | validation: 0.11596397895388635]
	TIME [epoch: 36.5 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11910376886038554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11910376886038554 | validation: 0.17455515847256367]
	TIME [epoch: 36.6 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14715496250724283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14715496250724283 | validation: 0.1808497334803605]
	TIME [epoch: 36.5 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15418731686314807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15418731686314807 | validation: 0.13706814832467837]
	TIME [epoch: 36.5 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10365379198986902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10365379198986902 | validation: 0.08882774563816491]
	TIME [epoch: 36.5 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06525541096618713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06525541096618713 | validation: 0.09665301843986347]
	TIME [epoch: 36.5 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08375710593792103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08375710593792103 | validation: 0.24256307632727508]
	TIME [epoch: 36.5 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17952716409063424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17952716409063424 | validation: 0.175590626407726]
	TIME [epoch: 36.5 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1572041070612694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1572041070612694 | validation: 0.1936889383401594]
	TIME [epoch: 36.5 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13709647680137427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13709647680137427 | validation: 0.10286395627969797]
	TIME [epoch: 36.6 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08920510406749439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08920510406749439 | validation: 0.1445444530478612]
	TIME [epoch: 36.5 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09512113249473551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09512113249473551 | validation: 0.0808422476257538]
	TIME [epoch: 36.5 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06862445773206304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06862445773206304 | validation: 0.07989333281401768]
	TIME [epoch: 36.5 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06458701776402886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06458701776402886 | validation: 0.07416259057639128]
	TIME [epoch: 36.5 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06127373019923144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06127373019923144 | validation: 0.10060476702506081]
	TIME [epoch: 36.5 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09405882251563875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09405882251563875 | validation: 0.15882545085596372]
	TIME [epoch: 36.5 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1448666055869271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1448666055869271 | validation: 0.18620300650220473]
	TIME [epoch: 36.5 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18905488005557394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18905488005557394 | validation: 0.14928293984287191]
	TIME [epoch: 36.5 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11605940204923795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11605940204923795 | validation: 0.10460529201563014]
	TIME [epoch: 36.6 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0983074136902729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0983074136902729 | validation: 0.28997768730729184]
	TIME [epoch: 36.6 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24270771742441113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24270771742441113 | validation: 0.11845412061224861]
	TIME [epoch: 36.5 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14905340115461457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14905340115461457 | validation: 0.18956048028986486]
	TIME [epoch: 36.6 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12146983009618385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12146983009618385 | validation: 0.07773731500014676]
	TIME [epoch: 36.6 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.071363067160798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.071363067160798 | validation: 0.07842660611984575]
	TIME [epoch: 36.6 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05165660480913543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05165660480913543 | validation: 0.07483252183073318]
	TIME [epoch: 36.6 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05804976420653202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05804976420653202 | validation: 0.1085074845621092]
	TIME [epoch: 36.5 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09231234013416552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09231234013416552 | validation: 0.23382361811254207]
	TIME [epoch: 36.5 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19858714361278446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19858714361278446 | validation: 0.2768189119805437]
	TIME [epoch: 36.5 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2271515925505878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2271515925505878 | validation: 0.2012756550201715]
	TIME [epoch: 36.5 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1261669893739622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1261669893739622 | validation: 0.12258535697877711]
	TIME [epoch: 36.6 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12116008504738048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12116008504738048 | validation: 0.13930010398843437]
	TIME [epoch: 36.6 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1266327456664761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1266327456664761 | validation: 0.09866805927451813]
	TIME [epoch: 36.5 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09629707414085802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09629707414085802 | validation: 0.11958311611575972]
	TIME [epoch: 36.5 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09233460605973248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09233460605973248 | validation: 0.07282313072622859]
	TIME [epoch: 36.5 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0694264022341823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0694264022341823 | validation: 0.08424178284786603]
	TIME [epoch: 36.5 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08660130254758201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08660130254758201 | validation: 0.2374564856222642]
	TIME [epoch: 36.5 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20910481915245882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20910481915245882 | validation: 0.11043160100166854]
	TIME [epoch: 36.6 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11024648028955199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11024648028955199 | validation: 0.0950265653630737]
	TIME [epoch: 36.6 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07510263790798201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07510263790798201 | validation: 0.058607927320272035]
	TIME [epoch: 36.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_1240.pth
	Model improved!!!
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06318306165272182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06318306165272182 | validation: 0.0730047797821635]
	TIME [epoch: 36.6 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04858041578886017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04858041578886017 | validation: 0.04298900167068183]
	TIME [epoch: 36.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_1242.pth
	Model improved!!!
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04073017581321297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04073017581321297 | validation: 0.05606979347152394]
	TIME [epoch: 36.7 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04842871451399197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04842871451399197 | validation: 0.05708061502658046]
	TIME [epoch: 36.6 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06146804868067946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06146804868067946 | validation: 0.1974330689266723]
	TIME [epoch: 36.5 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15605197534714685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15605197534714685 | validation: 0.17126736304892468]
	TIME [epoch: 36.6 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21275467958793173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21275467958793173 | validation: 0.13340238010383706]
	TIME [epoch: 36.5 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08543097776653102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08543097776653102 | validation: 0.14252376816838438]
	TIME [epoch: 36.6 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13514388574999403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13514388574999403 | validation: 0.3331494290430226]
	TIME [epoch: 36.6 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3070528437953627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3070528437953627 | validation: 0.226807623162021]
	TIME [epoch: 36.6 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21180797055648296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21180797055648296 | validation: 0.10790389849497357]
	TIME [epoch: 36.5 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0924314720478377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0924314720478377 | validation: 0.22825324658945376]
	TIME [epoch: 36.7 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14935740356690083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14935740356690083 | validation: 0.09468412754256472]
	TIME [epoch: 36.7 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08429341290584326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08429341290584326 | validation: 0.12770873642009437]
	TIME [epoch: 36.7 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08644547808840164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08644547808840164 | validation: 0.09485251805205927]
	TIME [epoch: 36.6 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07894105938976434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07894105938976434 | validation: 0.1179900443502175]
	TIME [epoch: 36.7 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10588107472383562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10588107472383562 | validation: 0.10715379681811568]
	TIME [epoch: 36.7 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11267537895030257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11267537895030257 | validation: 0.194780695926689]
	TIME [epoch: 36.6 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13441352779265311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13441352779265311 | validation: 0.11207284383741377]
	TIME [epoch: 36.6 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06775172118943984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06775172118943984 | validation: 0.057372257577738055]
	TIME [epoch: 36.6 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055070793139863045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055070793139863045 | validation: 0.14133925045111453]
	TIME [epoch: 36.5 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09921321444645947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09921321444645947 | validation: 0.26879216420341895]
	TIME [epoch: 36.6 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21942834526844826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21942834526844826 | validation: 0.2607795016091052]
	TIME [epoch: 36.5 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1894302407143375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1894302407143375 | validation: 0.20506602230422974]
	TIME [epoch: 36.6 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13756584334502028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13756584334502028 | validation: 0.22433129111884598]
	TIME [epoch: 36.5 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16993943564868247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16993943564868247 | validation: 0.13522149807647454]
	TIME [epoch: 36.6 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14251019926148567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14251019926148567 | validation: 0.1967308744359136]
	TIME [epoch: 36.5 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14015759521632823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14015759521632823 | validation: 0.12548180996417865]
	TIME [epoch: 36.6 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10179271171373547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10179271171373547 | validation: 0.11680282518970064]
	TIME [epoch: 36.5 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0876498496218356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0876498496218356 | validation: 0.07923248282901885]
	TIME [epoch: 36.5 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.070698792228676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.070698792228676 | validation: 0.060552769748337634]
	TIME [epoch: 36.5 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05758772459580013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05758772459580013 | validation: 0.16319931430010523]
	TIME [epoch: 36.5 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11090895358617439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11090895358617439 | validation: 0.2724804420800808]
	TIME [epoch: 36.5 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2454485663225664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2454485663225664 | validation: 0.1874403721653923]
	TIME [epoch: 36.6 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14221400473315993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14221400473315993 | validation: 0.16741833721603328]
	TIME [epoch: 36.5 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09091330082171852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09091330082171852 | validation: 0.10637602905762318]
	TIME [epoch: 36.6 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12213887299177793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12213887299177793 | validation: 0.2273593208682196]
	TIME [epoch: 36.5 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21137840633922064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21137840633922064 | validation: 0.15220536539852192]
	TIME [epoch: 36.6 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1233601766207082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1233601766207082 | validation: 0.13256851383951426]
	TIME [epoch: 36.6 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13080599569117624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13080599569117624 | validation: 0.2997478758062881]
	TIME [epoch: 36.6 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20528764224148585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20528764224148585 | validation: 0.13956350234126397]
	TIME [epoch: 36.6 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10829460149276868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10829460149276868 | validation: 0.1226373607158678]
	TIME [epoch: 36.6 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09787098917561668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09787098917561668 | validation: 0.09348029279630315]
	TIME [epoch: 36.6 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09048382842172042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09048382842172042 | validation: 0.15812308398855462]
	TIME [epoch: 36.6 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1428455802681242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1428455802681242 | validation: 0.07237470552557428]
	TIME [epoch: 36.6 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08101227230622687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08101227230622687 | validation: 0.1407667948815188]
	TIME [epoch: 36.6 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09088846385593759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09088846385593759 | validation: 0.08579555595456974]
	TIME [epoch: 36.6 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08567674505023658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08567674505023658 | validation: 0.12656647108857905]
	TIME [epoch: 36.6 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07522224532245307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07522224532245307 | validation: 0.08259053677667039]
	TIME [epoch: 36.6 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10234442898878954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10234442898878954 | validation: 0.19698105695201132]
	TIME [epoch: 36.5 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1572921752463343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1572921752463343 | validation: 0.11094112928742289]
	TIME [epoch: 36.6 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07647616214660621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07647616214660621 | validation: 0.048880931331369865]
	TIME [epoch: 36.6 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05208812400940179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05208812400940179 | validation: 0.07347626742122143]
	TIME [epoch: 36.6 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07079983613697072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07079983613697072 | validation: 0.06841703912252395]
	TIME [epoch: 36.5 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07614015555331928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07614015555331928 | validation: 0.10458590304793731]
	TIME [epoch: 36.6 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09006830136022581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09006830136022581 | validation: 0.13053650081462018]
	TIME [epoch: 36.5 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11611056080773131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11611056080773131 | validation: 0.12482897264867623]
	TIME [epoch: 36.6 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1620172207992691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1620172207992691 | validation: 0.28649880695131785]
	TIME [epoch: 36.6 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22088224918796823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22088224918796823 | validation: 0.22368565629475903]
	TIME [epoch: 36.5 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16007300264156732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16007300264156732 | validation: 0.13916135221966228]
	TIME [epoch: 36.4 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08466618681890722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08466618681890722 | validation: 0.15921247619810514]
	TIME [epoch: 36.6 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1531260418594134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1531260418594134 | validation: 0.35623656534243137]
	TIME [epoch: 36.6 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25775773302299926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25775773302299926 | validation: 0.21623187861675466]
	TIME [epoch: 36.6 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375909046840428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1375909046840428 | validation: 0.13350171064754432]
	TIME [epoch: 36.5 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09027024845203555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09027024845203555 | validation: 0.08985209507040526]
	TIME [epoch: 36.6 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08734123523398644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08734123523398644 | validation: 0.2347808208932991]
	TIME [epoch: 36.6 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19814747302439073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19814747302439073 | validation: 0.07508530956536215]
	TIME [epoch: 36.6 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09602014027670365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09602014027670365 | validation: 0.06573539909654508]
	TIME [epoch: 36.6 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07351112585330599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07351112585330599 | validation: 0.071454660857647]
	TIME [epoch: 36.6 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08191169895180561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08191169895180561 | validation: 0.06585903915753316]
	TIME [epoch: 36.6 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08515201705786642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08515201705786642 | validation: 0.19677563931778722]
	TIME [epoch: 36.7 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13563101916320666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13563101916320666 | validation: 0.0975900851173768]
	TIME [epoch: 36.6 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07380429957374714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07380429957374714 | validation: 0.057943991312902204]
	TIME [epoch: 36.6 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056355468225916826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056355468225916826 | validation: 0.09145373173106938]
	TIME [epoch: 36.5 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08319550127864901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08319550127864901 | validation: 0.1085244478713487]
	TIME [epoch: 36.6 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11070822343295252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11070822343295252 | validation: 0.18974037497171908]
	TIME [epoch: 36.6 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14871132806078005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14871132806078005 | validation: 0.0796821581584865]
	TIME [epoch: 36.6 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07253184735302616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07253184735302616 | validation: 0.04859532570457954]
	TIME [epoch: 36.6 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05836297206121212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05836297206121212 | validation: 0.1543003279064639]
	TIME [epoch: 36.5 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12681316638658793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12681316638658793 | validation: 0.07055467603821196]
	TIME [epoch: 36.6 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07732003079800671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07732003079800671 | validation: 0.10070966654811393]
	TIME [epoch: 36.4 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10218858795224808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10218858795224808 | validation: 0.11756108742533936]
	TIME [epoch: 36.5 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11565209825896475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11565209825896475 | validation: 0.12119138653977068]
	TIME [epoch: 36.5 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11257625570208926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11257625570208926 | validation: 0.07617285981339994]
	TIME [epoch: 36.5 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08774759126435829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08774759126435829 | validation: 0.19923263161690824]
	TIME [epoch: 36.5 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1310430268954003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1310430268954003 | validation: 0.06656087358132513]
	TIME [epoch: 36.6 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07291313346645481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07291313346645481 | validation: 0.07427136716999067]
	TIME [epoch: 36.5 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04451795807524463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04451795807524463 | validation: 0.03591016794333777]
	TIME [epoch: 36.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_1328.pth
	Model improved!!!
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04961364559238943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04961364559238943 | validation: 0.12622884788019764]
	TIME [epoch: 36.5 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10494276129294541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10494276129294541 | validation: 0.0823703145567229]
	TIME [epoch: 36.6 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08584964919834388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08584964919834388 | validation: 0.10124754950632352]
	TIME [epoch: 36.6 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09252137948503052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09252137948503052 | validation: 0.10265888789599253]
	TIME [epoch: 36.6 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13191623133714317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13191623133714317 | validation: 0.20243421376525217]
	TIME [epoch: 36.5 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13920932062403293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13920932062403293 | validation: 0.08582983498058856]
	TIME [epoch: 36.6 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0831794157224069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0831794157224069 | validation: 0.08749678450468341]
	TIME [epoch: 36.6 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09347263635873496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09347263635873496 | validation: 0.1737924730912353]
	TIME [epoch: 36.6 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16668622002827263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16668622002827263 | validation: 0.112667664055]
	TIME [epoch: 36.6 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10738349978810378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10738349978810378 | validation: 0.07279021535603822]
	TIME [epoch: 36.6 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09205363001241938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09205363001241938 | validation: 0.2026103671186839]
	TIME [epoch: 36.7 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1701549084203829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1701549084203829 | validation: 0.06514533367776443]
	TIME [epoch: 36.6 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07812397797715026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07812397797715026 | validation: 0.05940163881893319]
	TIME [epoch: 36.7 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06577329401174702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06577329401174702 | validation: 0.11931360611052454]
	TIME [epoch: 36.6 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09698033428982542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09698033428982542 | validation: 0.06527294182561745]
	TIME [epoch: 36.6 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09231237589305312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09231237589305312 | validation: 0.2223764265259991]
	TIME [epoch: 36.6 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1373783199795814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1373783199795814 | validation: 0.14100530376279768]
	TIME [epoch: 36.6 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14248088147991067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14248088147991067 | validation: 0.11405212761830517]
	TIME [epoch: 36.6 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09857993949911872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09857993949911872 | validation: 0.08333259167268278]
	TIME [epoch: 36.6 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0789873480813516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0789873480813516 | validation: 0.07069318023159368]
	TIME [epoch: 36.6 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0783286637987806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0783286637987806 | validation: 0.16833729846497691]
	TIME [epoch: 36.6 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1326656776480552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1326656776480552 | validation: 0.04891945104800365]
	TIME [epoch: 36.6 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06327669204984442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06327669204984442 | validation: 0.05464637627521175]
	TIME [epoch: 36.6 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050571199071107575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050571199071107575 | validation: 0.06080655611290027]
	TIME [epoch: 36.6 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06935258445866153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06935258445866153 | validation: 0.08686658461302499]
	TIME [epoch: 36.6 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08428811324582025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08428811324582025 | validation: 0.11335078615138364]
	TIME [epoch: 36.7 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10813080724485598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10813080724485598 | validation: 0.06567494431216589]
	TIME [epoch: 36.6 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06823652158912326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06823652158912326 | validation: 0.15159778928132006]
	TIME [epoch: 36.7 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08488306175982363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08488306175982363 | validation: 0.07501389489394494]
	TIME [epoch: 36.6 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07843821162572233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07843821162572233 | validation: 0.14401073807563367]
	TIME [epoch: 36.5 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08952313450800713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08952313450800713 | validation: 0.06956024627901816]
	TIME [epoch: 36.6 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0839866859861748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0839866859861748 | validation: 0.11513777539962758]
	TIME [epoch: 36.6 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06887715931128156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06887715931128156 | validation: 0.04036981708911851]
	TIME [epoch: 36.6 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04497719135012182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04497719135012182 | validation: 0.07734023883254325]
	TIME [epoch: 36.6 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051030832451317586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051030832451317586 | validation: 0.06013537253377735]
	TIME [epoch: 36.7 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09194393135495765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09194393135495765 | validation: 0.18426506197845327]
	TIME [epoch: 36.7 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13586887863298763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13586887863298763 | validation: 0.2622547339246308]
	TIME [epoch: 36.6 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18163449661895925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18163449661895925 | validation: 0.10863730928730725]
	TIME [epoch: 36.6 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1372002500537742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1372002500537742 | validation: 0.03703166501559633]
	TIME [epoch: 36.6 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04704358971142865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04704358971142865 | validation: 0.0624236730434642]
	TIME [epoch: 36.7 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062291292517197665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062291292517197665 | validation: 0.07364234235364302]
	TIME [epoch: 36.6 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08115661570053682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08115661570053682 | validation: 0.05864663345692302]
	TIME [epoch: 36.6 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061575897294544005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061575897294544005 | validation: 0.12877878780106464]
	TIME [epoch: 36.7 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09254150350692623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09254150350692623 | validation: 0.07271957499151614]
	TIME [epoch: 36.6 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08091318320116227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08091318320116227 | validation: 0.11317186078346714]
	TIME [epoch: 36.6 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09464199954317723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09464199954317723 | validation: 0.07774419804542544]
	TIME [epoch: 36.6 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07431202852000675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07431202852000675 | validation: 0.05903381265319834]
	TIME [epoch: 36.6 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05977167226823027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05977167226823027 | validation: 0.04607653472186313]
	TIME [epoch: 36.6 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04326992909153635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04326992909153635 | validation: 0.03885829040911009]
	TIME [epoch: 36.6 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0309914081057062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0309914081057062 | validation: 0.04568332631255489]
	TIME [epoch: 36.5 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05145707604075697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05145707604075697 | validation: 0.16629575301698785]
	TIME [epoch: 36.6 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1420296792264466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1420296792264466 | validation: 0.11559222280954505]
	TIME [epoch: 36.6 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13153638859831876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13153638859831876 | validation: 0.2537502419868387]
	TIME [epoch: 36.6 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19557763236947406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19557763236947406 | validation: 0.07962101704841053]
	TIME [epoch: 36.6 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08660810557507392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08660810557507392 | validation: 0.039880505561368795]
	TIME [epoch: 36.6 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041723702762605175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041723702762605175 | validation: 0.06886767480323538]
	TIME [epoch: 36.5 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07701069520952078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07701069520952078 | validation: 0.07477868459064879]
	TIME [epoch: 36.6 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10558435404662937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10558435404662937 | validation: 0.05518513613180681]
	TIME [epoch: 36.6 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05225989014673138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05225989014673138 | validation: 0.04318574508388884]
	TIME [epoch: 36.6 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03491061579571058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03491061579571058 | validation: 0.05847525080020082]
	TIME [epoch: 36.6 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08218472110127373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08218472110127373 | validation: 0.2682403251379148]
	TIME [epoch: 36.6 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.196341880055758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.196341880055758 | validation: 0.07367435000042735]
	TIME [epoch: 36.6 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08688049267688644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08688049267688644 | validation: 0.13498154954779823]
	TIME [epoch: 36.7 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16145339484119364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16145339484119364 | validation: 0.0973710427087435]
	TIME [epoch: 36.6 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10010070980283196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10010070980283196 | validation: 0.07805901970629592]
	TIME [epoch: 36.6 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05741262573039855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05741262573039855 | validation: 0.03298409101219509]
	TIME [epoch: 36.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_1394.pth
	Model improved!!!
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04235889863980294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04235889863980294 | validation: 0.10377148190839827]
	TIME [epoch: 36.6 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06946516101715855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06946516101715855 | validation: 0.10264412663353228]
	TIME [epoch: 36.5 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10957139232164341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10957139232164341 | validation: 0.20365310838613163]
	TIME [epoch: 36.6 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13781975614459793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13781975614459793 | validation: 0.14013817802392592]
	TIME [epoch: 36.4 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10794563933758693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10794563933758693 | validation: 0.06994297839286999]
	TIME [epoch: 36.6 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06593137288879101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06593137288879101 | validation: 0.0728711912885727]
	TIME [epoch: 36.4 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07661800757714003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07661800757714003 | validation: 0.07499575935987157]
	TIME [epoch: 36.6 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08818769986487102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08818769986487102 | validation: 0.12858382020693987]
	TIME [epoch: 36.5 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11671230554967921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11671230554967921 | validation: 0.04432943201147319]
	TIME [epoch: 36.6 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04717083045022836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04717083045022836 | validation: 0.04506705457197081]
	TIME [epoch: 36.6 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03506685237758183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03506685237758183 | validation: 0.035577858712817344]
	TIME [epoch: 36.6 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04759033822343712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04759033822343712 | validation: 0.12116858251412628]
	TIME [epoch: 36.7 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08130380127115742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08130380127115742 | validation: 0.08871172788246849]
	TIME [epoch: 36.6 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1096926182346627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1096926182346627 | validation: 0.12992981524470354]
	TIME [epoch: 36.5 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09136473886539351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09136473886539351 | validation: 0.03162155381077364]
	TIME [epoch: 36.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172003/states/model_phi1_3c_v_mmd1_1409.pth
	Model improved!!!
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04130733999845443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04130733999845443 | validation: 0.06399270679053547]
	TIME [epoch: 36.5 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04640334281085805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04640334281085805 | validation: 0.0625137800563335]
	TIME [epoch: 36.5 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09991506176324741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09991506176324741 | validation: 0.20891516265196658]
	TIME [epoch: 36.5 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16533135329511126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16533135329511126 | validation: 0.09571424614946245]
	TIME [epoch: 36.5 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09647546976883702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09647546976883702 | validation: 0.10915628352851224]
	TIME [epoch: 36.5 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10530006617183663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10530006617183663 | validation: 0.06909681737929356]
	TIME [epoch: 36.5 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09197989629356973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09197989629356973 | validation: 0.06629511334129964]
	TIME [epoch: 36.5 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1074389276329488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1074389276329488 | validation: 0.053208838992362166]
	TIME [epoch: 36.5 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0728875988118348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0728875988118348 | validation: 0.12569738963515512]
	TIME [epoch: 37 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11419793794928214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11419793794928214 | validation: 0.040841640139506245]
	TIME [epoch: 36.6 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0647273604764251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0647273604764251 | validation: 0.04387749307231409]
	TIME [epoch: 36.5 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051437126512147116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051437126512147116 | validation: 0.03856363989798357]
	TIME [epoch: 36.6 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04370727561710252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04370727561710252 | validation: 0.08282872184062118]
	TIME [epoch: 36.5 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07167930187972157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07167930187972157 | validation: 0.07457728062023587]
	TIME [epoch: 36.6 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08969193177359344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08969193177359344 | validation: 0.13743240649256783]
	TIME [epoch: 36.5 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09960471749875996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09960471749875996 | validation: 0.06283327380499686]
	TIME [epoch: 36.5 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06091656373091651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06091656373091651 | validation: 0.06277555864417163]
	TIME [epoch: 36.5 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05649576264767533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05649576264767533 | validation: 0.07085797590212672]
	TIME [epoch: 36.5 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08720867595167447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08720867595167447 | validation: 0.11128125177830911]
	TIME [epoch: 36.5 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09610771025572942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09610771025572942 | validation: 0.06937528516845144]
	TIME [epoch: 36.5 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08260858837759358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08260858837759358 | validation: 0.0716389931956825]
	TIME [epoch: 36.5 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07744374358993733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07744374358993733 | validation: 0.1433333016809738]
	TIME [epoch: 36.5 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11886122893768622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11886122893768622 | validation: 0.0644824391671582]
	TIME [epoch: 36.5 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08213208265526765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08213208265526765 | validation: 0.08396705627203693]
	TIME [epoch: 36.5 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06825256419627913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06825256419627913 | validation: 0.0640866611772019]
	TIME [epoch: 36.5 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07438157074180783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07438157074180783 | validation: 0.10075552602500977]
	TIME [epoch: 36.5 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05736020534033131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05736020534033131 | validation: 0.05174320062273818]
	TIME [epoch: 36.5 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04267738012367506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04267738012367506 | validation: 0.07941456285059333]
	TIME [epoch: 36.5 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04993560675811363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04993560675811363 | validation: 0.05653596070550653]
	TIME [epoch: 36.5 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06645780403827249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06645780403827249 | validation: 0.08627540838784226]
	TIME [epoch: 36.6 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10336230255671428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10336230255671428 | validation: 0.06822710032275899]
	TIME [epoch: 36.5 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06362197800896378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06362197800896378 | validation: 0.04208259281834105]
	TIME [epoch: 36.6 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042783553503088684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042783553503088684 | validation: 0.035131007221358805]
	TIME [epoch: 36.5 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0282914742934904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0282914742934904 | validation: 0.03897857038762259]
	TIME [epoch: 36.5 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035345399178788026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035345399178788026 | validation: 0.09849647334571897]
	TIME [epoch: 36.5 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08654565596354714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08654565596354714 | validation: 0.15423545898998725]
	TIME [epoch: 36.6 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19928887695687464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19928887695687464 | validation: 0.15378257931090666]
	TIME [epoch: 36.6 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1037142910443955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1037142910443955 | validation: 0.09675583005358891]
	TIME [epoch: 36.6 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1220294438928273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1220294438928273 | validation: 0.2315237356877435]
	TIME [epoch: 36.5 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17725425418473742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17725425418473742 | validation: 0.18950886882209061]
	TIME [epoch: 36.4 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13417494949856998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13417494949856998 | validation: 0.4016273234814673]
	TIME [epoch: 36.4 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25412369435887416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25412369435887416 | validation: 0.25800915836292465]
	TIME [epoch: 36.4 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15666734572895472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15666734572895472 | validation: 0.05168393790885424]
	TIME [epoch: 36.5 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059485829028550694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059485829028550694 | validation: 0.04600870073676494]
	TIME [epoch: 36.4 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06590190501829478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06590190501829478 | validation: 0.032524684377174755]
	TIME [epoch: 36.4 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04788267427341904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04788267427341904 | validation: 0.03992897265180811]
	TIME [epoch: 36.5 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050598465949792885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050598465949792885 | validation: 0.09004521593691187]
	TIME [epoch: 36.5 sec]
EPOCH 1457/2000:
	Training over batches...
