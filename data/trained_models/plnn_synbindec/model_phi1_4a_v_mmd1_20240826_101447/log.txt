Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1130778330

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.781164492948339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.781164492948339 | validation: 3.739409194835014]
	TIME [epoch: 41.9 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.396779956241343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.396779956241343 | validation: 3.6244396746747243]
	TIME [epoch: 0.993 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.373258489032397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.373258489032397 | validation: 3.331627687162387]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.086280417997485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.086280417997485 | validation: 3.2216188036352955]
	TIME [epoch: 0.915 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.167770910424083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.167770910424083 | validation: 3.076408451413909]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7613744692838833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7613744692838833 | validation: 3.0868764201493533]
	TIME [epoch: 0.915 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.62917724112501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.62917724112501 | validation: 3.022732784049969]
	TIME [epoch: 0.915 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4678953210818553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4678953210818553 | validation: 3.0339587480049652]
	TIME [epoch: 0.912 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2495000922811625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2495000922811625 | validation: 3.09860837820579]
	TIME [epoch: 0.911 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.975179278751797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.975179278751797 | validation: 2.7456975503286403]
	TIME [epoch: 0.911 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5517944602511062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5517944602511062 | validation: 2.0082287144061843]
	TIME [epoch: 0.915 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4102230815801757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4102230815801757 | validation: 1.5486833366689847]
	TIME [epoch: 0.914 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.828069244840696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.828069244840696 | validation: 1.7932664576967696]
	TIME [epoch: 0.91 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4959374447869114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4959374447869114 | validation: 1.5497212367560915]
	TIME [epoch: 0.909 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.673322521322497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.673322521322497 | validation: 1.6823796028725237]
	TIME [epoch: 0.909 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5610797595880903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5610797595880903 | validation: 1.3114556248649163]
	TIME [epoch: 0.911 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4344041775810872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4344041775810872 | validation: 1.0353502643155819]
	TIME [epoch: 0.916 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3629600871908811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3629600871908811 | validation: 0.9516840783211704]
	TIME [epoch: 0.91 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.308961143743807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.308961143743807 | validation: 0.9373139300165886]
	TIME [epoch: 0.915 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3264099676661745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3264099676661745 | validation: 1.015967868895987]
	TIME [epoch: 0.914 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4950604978906017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4950604978906017 | validation: 1.0722701697110197]
	TIME [epoch: 0.913 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4940620015166843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4940620015166843 | validation: 1.1117250465263788]
	TIME [epoch: 0.913 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3112201626831024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3112201626831024 | validation: 0.9782314649267687]
	TIME [epoch: 0.912 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.228206735911028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.228206735911028 | validation: 0.8757655608632651]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1959696761822034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1959696761822034 | validation: 0.8476221993559364]
	TIME [epoch: 0.914 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1931390726988815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1931390726988815 | validation: 0.9085299166765245]
	TIME [epoch: 0.914 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.223826120634656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.223826120634656 | validation: 0.9548184379882703]
	TIME [epoch: 0.915 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2569754551329755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2569754551329755 | validation: 0.8627358280627825]
	TIME [epoch: 0.913 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2282933397374924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2282933397374924 | validation: 0.8586720653919255]
	TIME [epoch: 0.914 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1378258988174752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1378258988174752 | validation: 0.8945768254157342]
	TIME [epoch: 0.913 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1144066143684979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1144066143684979 | validation: 0.8049883065310077]
	TIME [epoch: 0.921 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1097979843382642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1097979843382642 | validation: 0.8800096383480785]
	TIME [epoch: 0.93 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1160708817637477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1160708817637477 | validation: 0.8626571421921277]
	TIME [epoch: 0.914 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1201074185459619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1201074185459619 | validation: 0.8038212264789277]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1311327307233674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1311327307233674 | validation: 0.8815721684370059]
	TIME [epoch: 0.914 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0721964522413479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0721964522413479 | validation: 0.684979260501986]
	TIME [epoch: 0.917 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.052670436756377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.052670436756377 | validation: 0.9855401692818031]
	TIME [epoch: 0.913 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0516521765627764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0516521765627764 | validation: 0.7348246340563933]
	TIME [epoch: 0.913 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2724440153687588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2724440153687588 | validation: 1.562486848812564]
	TIME [epoch: 0.91 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.317922686506842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.317922686506842 | validation: 0.7353522848106154]
	TIME [epoch: 0.913 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9998128867496021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9998128867496021 | validation: 0.6634656936236367]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0614726569228885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0614726569228885 | validation: 0.6971770196744203]
	TIME [epoch: 0.913 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9760190668578553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9760190668578553 | validation: 0.8247418497529871]
	TIME [epoch: 0.918 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9702315832330782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9702315832330782 | validation: 0.6777389162086224]
	TIME [epoch: 0.915 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9448038750103027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9448038750103027 | validation: 0.7201668510247857]
	TIME [epoch: 0.914 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9327537296747035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9327537296747035 | validation: 0.7304091858175025]
	TIME [epoch: 0.915 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9335346596453664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9335346596453664 | validation: 0.7258874428609278]
	TIME [epoch: 0.914 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9424431124543834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9424431124543834 | validation: 0.7176097530309358]
	TIME [epoch: 0.909 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9753827185298153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9753827185298153 | validation: 0.7483175505668946]
	TIME [epoch: 0.909 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9386944283496567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9386944283496567 | validation: 0.6805588036829975]
	TIME [epoch: 0.915 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9354522239842413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9354522239842413 | validation: 0.7281621538567498]
	TIME [epoch: 0.916 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8868238361746871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8868238361746871 | validation: 0.6765827713210967]
	TIME [epoch: 0.909 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8877736743273849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8877736743273849 | validation: 0.7346171492387278]
	TIME [epoch: 0.912 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8769753959942511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8769753959942511 | validation: 0.7010168109568395]
	TIME [epoch: 0.913 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9075202905973754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9075202905973754 | validation: 0.7172860849781544]
	TIME [epoch: 0.911 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9030352208351067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9030352208351067 | validation: 1.0248833297172486]
	TIME [epoch: 0.911 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9711893201407673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9711893201407673 | validation: 0.5563474535288452]
	TIME [epoch: 0.91 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9602036424172857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9602036424172857 | validation: 1.220687068329402]
	TIME [epoch: 0.911 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1237262908323897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1237262908323897 | validation: 0.5190724849500817]
	TIME [epoch: 0.909 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0892288825189913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0892288825189913 | validation: 0.6172787121698877]
	TIME [epoch: 0.912 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8206424240949467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8206424240949467 | validation: 0.9772212442649647]
	TIME [epoch: 0.912 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9696104459853416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9696104459853416 | validation: 0.5387323274725031]
	TIME [epoch: 0.914 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9114928864877375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9114928864877375 | validation: 0.6686403032632934]
	TIME [epoch: 0.912 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8151869011706321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8151869011706321 | validation: 0.7771037615155141]
	TIME [epoch: 0.913 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580776041385635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8580776041385635 | validation: 0.6508642468906004]
	TIME [epoch: 0.916 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8381358069569993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8381358069569993 | validation: 0.6524270769872053]
	TIME [epoch: 0.915 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8146687459695758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8146687459695758 | validation: 0.7944238538691105]
	TIME [epoch: 0.914 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8162067115706265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8162067115706265 | validation: 0.6062280310466784]
	TIME [epoch: 0.914 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8585898390487772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8585898390487772 | validation: 1.007691005255195]
	TIME [epoch: 0.914 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9143952674248738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9143952674248738 | validation: 0.595014882317674]
	TIME [epoch: 0.917 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8416573012357264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8416573012357264 | validation: 0.7379004785461998]
	TIME [epoch: 0.915 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8083977345768475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8083977345768475 | validation: 0.6615714720044554]
	TIME [epoch: 0.915 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7969198665716799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7969198665716799 | validation: 0.6582655586808812]
	TIME [epoch: 0.915 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8072240368907794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8072240368907794 | validation: 0.7808677278238273]
	TIME [epoch: 0.916 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8427411716899311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8427411716899311 | validation: 0.6186996912950997]
	TIME [epoch: 0.916 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9298089306326484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9298089306326484 | validation: 0.7537392780928949]
	TIME [epoch: 0.916 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8177650761630062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8177650761630062 | validation: 0.5695814181086803]
	TIME [epoch: 0.915 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8163011023058209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8163011023058209 | validation: 0.930159731395111]
	TIME [epoch: 0.916 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8634810331053078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8634810331053078 | validation: 0.6725875478822063]
	TIME [epoch: 0.919 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9477962464272931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9477962464272931 | validation: 1.1273175722461575]
	TIME [epoch: 0.926 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9451278582850998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9451278582850998 | validation: 0.6158899809948837]
	TIME [epoch: 0.916 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7802314660623278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7802314660623278 | validation: 0.5767296226203488]
	TIME [epoch: 0.916 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8076807873230513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8076807873230513 | validation: 0.8677728133860966]
	TIME [epoch: 0.922 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8305085571386439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8305085571386439 | validation: 0.585883837671616]
	TIME [epoch: 0.921 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7975730807570034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7975730807570034 | validation: 0.712786778110936]
	TIME [epoch: 0.915 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7710699879153299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7710699879153299 | validation: 0.6174079135723526]
	TIME [epoch: 0.916 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982302099335753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7982302099335753 | validation: 0.8161320693301075]
	TIME [epoch: 0.919 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9028415617868807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9028415617868807 | validation: 0.7705103220878228]
	TIME [epoch: 0.917 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0224440401105472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0224440401105472 | validation: 0.6576394608634146]
	TIME [epoch: 0.915 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7623792834896591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7623792834896591 | validation: 0.823177369460082]
	TIME [epoch: 0.917 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8233647573191348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8233647573191348 | validation: 0.5622218269559314]
	TIME [epoch: 0.918 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8862353625999652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8862353625999652 | validation: 0.7704507000991524]
	TIME [epoch: 0.917 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7809184385183542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7809184385183542 | validation: 0.654199393592866]
	TIME [epoch: 0.915 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.769105469422955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.769105469422955 | validation: 0.813559901489867]
	TIME [epoch: 0.915 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8109638212202839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8109638212202839 | validation: 0.747669260248438]
	TIME [epoch: 0.915 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8834045029626597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8834045029626597 | validation: 1.0128851800431466]
	TIME [epoch: 0.916 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9217452242300627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9217452242300627 | validation: 0.6301615174009264]
	TIME [epoch: 0.915 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7624540954719765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7624540954719765 | validation: 0.5765133635855264]
	TIME [epoch: 0.915 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7818892358912984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7818892358912984 | validation: 0.919852628640429]
	TIME [epoch: 0.914 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8526777551831657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8526777551831657 | validation: 0.5377951280679839]
	TIME [epoch: 0.913 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8886382854617856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8886382854617856 | validation: 0.8241354687263542]
	TIME [epoch: 0.912 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8512160875608784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8512160875608784 | validation: 0.6294786791435965]
	TIME [epoch: 0.914 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8526908604819607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8526908604819607 | validation: 0.6815668742419911]
	TIME [epoch: 0.914 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7733228982102921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7733228982102921 | validation: 0.7558704096961784]
	TIME [epoch: 0.912 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7807770175386649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7807770175386649 | validation: 0.6097095845185683]
	TIME [epoch: 0.914 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7727466601946452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7727466601946452 | validation: 0.754261887609676]
	TIME [epoch: 0.913 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7629368903597924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7629368903597924 | validation: 0.6378806366453654]
	TIME [epoch: 0.916 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7698652520773093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7698652520773093 | validation: 0.7618574945563696]
	TIME [epoch: 0.912 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.809753651720267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.809753651720267 | validation: 0.7466921431851805]
	TIME [epoch: 0.914 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8245770180573263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8245770180573263 | validation: 0.6157486421362003]
	TIME [epoch: 0.913 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9088313735204983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9088313735204983 | validation: 0.8024095185052611]
	TIME [epoch: 0.913 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872621573558257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7872621573558257 | validation: 0.5786786650125982]
	TIME [epoch: 0.913 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7814640676590594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7814640676590594 | validation: 0.7730365694156349]
	TIME [epoch: 0.912 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7729317699720193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7729317699720193 | validation: 0.589008613889623]
	TIME [epoch: 0.914 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7622129572133634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7622129572133634 | validation: 0.7608825267744866]
	TIME [epoch: 0.917 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7610596979091091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7610596979091091 | validation: 0.6215175277927428]
	TIME [epoch: 0.913 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7495980069823662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7495980069823662 | validation: 0.6835957956440412]
	TIME [epoch: 0.914 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7526331152690839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7526331152690839 | validation: 0.7283676961879062]
	TIME [epoch: 0.912 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7845140417609429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7845140417609429 | validation: 0.7451229532315856]
	TIME [epoch: 0.916 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9583224308482777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9583224308482777 | validation: 0.7546256906922917]
	TIME [epoch: 0.912 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8251585426848679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8251585426848679 | validation: 0.5553794426795496]
	TIME [epoch: 0.913 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896489525429891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7896489525429891 | validation: 0.8057056137975644]
	TIME [epoch: 0.914 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668215659285358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7668215659285358 | validation: 0.5628348604233618]
	TIME [epoch: 0.915 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7631135633638383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7631135633638383 | validation: 0.809677580471891]
	TIME [epoch: 0.915 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7790414657849019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7790414657849019 | validation: 0.6217507488941098]
	TIME [epoch: 0.924 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630265258162797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7630265258162797 | validation: 0.7810469220636451]
	TIME [epoch: 0.913 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7575471837754529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7575471837754529 | validation: 0.6158178573157277]
	TIME [epoch: 0.913 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7448961052297748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7448961052297748 | validation: 0.7180066114620971]
	TIME [epoch: 0.922 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.778788339741706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.778788339741706 | validation: 0.7656991228040326]
	TIME [epoch: 0.919 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8552096826587976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8552096826587976 | validation: 0.6804265533694215]
	TIME [epoch: 0.914 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9466143103413276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9466143103413276 | validation: 0.718457717713369]
	TIME [epoch: 0.914 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733854971797274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.733854971797274 | validation: 0.5970122264665395]
	TIME [epoch: 0.916 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299722933424407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7299722933424407 | validation: 0.7506369511628082]
	TIME [epoch: 0.912 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668326475812507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7668326475812507 | validation: 0.6930309613528953]
	TIME [epoch: 0.914 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7704347603629083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7704347603629083 | validation: 0.6601791645859424]
	TIME [epoch: 0.914 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7713216362927456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7713216362927456 | validation: 0.6625759049210618]
	TIME [epoch: 0.911 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7370554143168241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7370554143168241 | validation: 0.5891831775425873]
	TIME [epoch: 0.917 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7514341604061033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7514341604061033 | validation: 0.7613624288159013]
	TIME [epoch: 0.915 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897954509718508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7897954509718508 | validation: 0.5214011475287185]
	TIME [epoch: 0.914 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8345983134818429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8345983134818429 | validation: 0.7906805926376966]
	TIME [epoch: 0.914 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7625972915459045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7625972915459045 | validation: 0.5436226903474334]
	TIME [epoch: 0.914 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7343057633510897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7343057633510897 | validation: 0.6932881131353713]
	TIME [epoch: 0.912 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7119953038470578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7119953038470578 | validation: 0.6327351919918716]
	TIME [epoch: 0.913 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7123644514280089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7123644514280089 | validation: 0.6749824259291012]
	TIME [epoch: 0.914 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7437102065095448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7437102065095448 | validation: 0.7313875709454684]
	TIME [epoch: 0.914 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8059640015507124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8059640015507124 | validation: 0.7424923341288685]
	TIME [epoch: 0.911 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9003805418829908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9003805418829908 | validation: 0.6599849420624447]
	TIME [epoch: 0.912 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7420860807487796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7420860807487796 | validation: 0.6385789537189299]
	TIME [epoch: 0.915 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.700130108516685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.700130108516685 | validation: 0.5958139125556049]
	TIME [epoch: 0.915 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6892154169009796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6892154169009796 | validation: 0.646627797296939]
	TIME [epoch: 0.912 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.688331276962555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.688331276962555 | validation: 0.5791886217107368]
	TIME [epoch: 0.914 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6921518471857162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6921518471857162 | validation: 0.7727022767620548]
	TIME [epoch: 0.915 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7538110135973737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7538110135973737 | validation: 0.5299107202789405]
	TIME [epoch: 0.913 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9372681841800065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9372681841800065 | validation: 0.7679510102719669]
	TIME [epoch: 0.914 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765237659916651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7765237659916651 | validation: 0.6216081122813256]
	TIME [epoch: 0.913 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173268119122811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7173268119122811 | validation: 0.5882180176102937]
	TIME [epoch: 0.914 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6958307222450804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6958307222450804 | validation: 0.6225841559218405]
	TIME [epoch: 0.916 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6915571173322375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6915571173322375 | validation: 0.6493813095990896]
	TIME [epoch: 0.911 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6987316464843923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6987316464843923 | validation: 0.811957424087915]
	TIME [epoch: 0.912 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861078948960365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7861078948960365 | validation: 0.6572466251654134]
	TIME [epoch: 0.914 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7909873733987459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7909873733987459 | validation: 0.7526306001019082]
	TIME [epoch: 0.916 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7788487978217481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7788487978217481 | validation: 0.6457244189682965]
	TIME [epoch: 0.914 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7269581498899925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7269581498899925 | validation: 0.5569857136138848]
	TIME [epoch: 0.912 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7117830088052199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7117830088052199 | validation: 0.7493269487436763]
	TIME [epoch: 0.914 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7104766038630507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7104766038630507 | validation: 0.5191624373058165]
	TIME [epoch: 0.915 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7372168083831291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7372168083831291 | validation: 0.7360568049847876]
	TIME [epoch: 0.913 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7205747063484467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7205747063484467 | validation: 0.6083199849141712]
	TIME [epoch: 0.914 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7426526469850739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7426526469850739 | validation: 0.6389765912357727]
	TIME [epoch: 0.913 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7230217802135306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7230217802135306 | validation: 0.6328453515529819]
	TIME [epoch: 0.914 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6967994021005204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6967994021005204 | validation: 0.5844322229076229]
	TIME [epoch: 0.925 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781250851851014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6781250851851014 | validation: 0.5980663153062888]
	TIME [epoch: 0.914 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.668132016983262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.668132016983262 | validation: 0.6113883480944382]
	TIME [epoch: 0.913 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918098985898666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6918098985898666 | validation: 0.6580681167245678]
	TIME [epoch: 0.92 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.727867414378371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.727867414378371 | validation: 0.6623461819439274]
	TIME [epoch: 0.919 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7440313403715064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7440313403715064 | validation: 0.5730500145833908]
	TIME [epoch: 0.912 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7351065520339581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7351065520339581 | validation: 0.7289060112686124]
	TIME [epoch: 0.912 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6965850590480156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6965850590480156 | validation: 0.4564929722536157]
	TIME [epoch: 0.917 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7059511993799349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7059511993799349 | validation: 0.7358075692971067]
	TIME [epoch: 0.918 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6850603546880245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6850603546880245 | validation: 0.5043419871832963]
	TIME [epoch: 0.912 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6476114301160202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6476114301160202 | validation: 0.6049484371858244]
	TIME [epoch: 0.911 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6351342587636594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6351342587636594 | validation: 0.5980785723501897]
	TIME [epoch: 0.914 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6266654420647502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6266654420647502 | validation: 0.482961423950661]
	TIME [epoch: 0.913 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6530013265669916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6530013265669916 | validation: 0.7496000780885803]
	TIME [epoch: 0.913 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7482853066016674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7482853066016674 | validation: 0.6433762367073981]
	TIME [epoch: 0.911 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8505063342141276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8505063342141276 | validation: 0.5534425243419995]
	TIME [epoch: 0.914 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7295462020975836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7295462020975836 | validation: 0.712760631747682]
	TIME [epoch: 0.91 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.67292241922174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.67292241922174 | validation: 0.4756828349205935]
	TIME [epoch: 0.911 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6380094775784778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6380094775784778 | validation: 0.5876665088300246]
	TIME [epoch: 0.914 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6386610082302534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6386610082302534 | validation: 0.5363984042591403]
	TIME [epoch: 0.915 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6192188947491774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6192188947491774 | validation: 0.5557253299871748]
	TIME [epoch: 0.916 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6355880472469833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6355880472469833 | validation: 0.602163679495811]
	TIME [epoch: 0.915 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6936649629746852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6936649629746852 | validation: 0.6463388427612862]
	TIME [epoch: 0.925 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7359726717933603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7359726717933603 | validation: 0.5374938341829223]
	TIME [epoch: 0.914 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6803519873332641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6803519873332641 | validation: 0.5926535999728504]
	TIME [epoch: 0.912 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6206981348430506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6206981348430506 | validation: 0.45220098117144697]
	TIME [epoch: 0.907 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6044797807869559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6044797807869559 | validation: 0.7124523801859421]
	TIME [epoch: 0.916 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6466230470045693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6466230470045693 | validation: 0.44062860613648724]
	TIME [epoch: 0.914 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6985514460692704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6985514460692704 | validation: 0.5813493636276716]
	TIME [epoch: 0.916 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6059139040060525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6059139040060525 | validation: 0.5895283182087522]
	TIME [epoch: 0.916 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5960185537764581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5960185537764581 | validation: 0.4200622856793996]
	TIME [epoch: 0.912 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6355978357971679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6355978357971679 | validation: 0.625820165066293]
	TIME [epoch: 40.3 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6665753925344511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6665753925344511 | validation: 0.6268900246041325]
	TIME [epoch: 1.82 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6894999070104976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6894999070104976 | validation: 0.4683304584393767]
	TIME [epoch: 1.8 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658645055323271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.658645055323271 | validation: 0.5326431338825331]
	TIME [epoch: 1.79 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5860365301443968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5860365301443968 | validation: 0.4355353131162975]
	TIME [epoch: 1.79 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5590787796435402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5590787796435402 | validation: 0.5404822596627349]
	TIME [epoch: 1.79 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5662469663518289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5662469663518289 | validation: 0.37248377711421377]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6073611639013065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6073611639013065 | validation: 0.6706481979137702]
	TIME [epoch: 1.79 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6235187320503006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6235187320503006 | validation: 0.42809225324162264]
	TIME [epoch: 1.79 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6313154305935812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6313154305935812 | validation: 0.4500090570176356]
	TIME [epoch: 1.79 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.576756003765927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.576756003765927 | validation: 0.5968392160209361]
	TIME [epoch: 1.8 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6045652888898055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6045652888898055 | validation: 0.4044882433591031]
	TIME [epoch: 1.79 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5827814918112264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5827814918112264 | validation: 0.4965478862253451]
	TIME [epoch: 1.79 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5398448953713982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5398448953713982 | validation: 0.4992363299401406]
	TIME [epoch: 1.79 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5568807854012733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5568807854012733 | validation: 0.42790635031863555]
	TIME [epoch: 1.8 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.631325167037468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.631325167037468 | validation: 0.487403303934616]
	TIME [epoch: 1.82 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5324570746554285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5324570746554285 | validation: 0.4410719794584752]
	TIME [epoch: 1.79 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5186673898377276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5186673898377276 | validation: 0.4133104524085129]
	TIME [epoch: 1.8 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5365553887371499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5365553887371499 | validation: 0.6471455139719835]
	TIME [epoch: 1.8 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5964668604823183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5964668604823183 | validation: 0.3874388602158896]
	TIME [epoch: 1.8 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6528699998921976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6528699998921976 | validation: 0.4213809240011848]
	TIME [epoch: 1.79 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5129333807020982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5129333807020982 | validation: 0.5535182711669714]
	TIME [epoch: 1.79 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5480676999640903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5480676999640903 | validation: 0.3522342307540231]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5129891335443654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5129891335443654 | validation: 0.4188214671249519]
	TIME [epoch: 1.79 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.466554616239393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.466554616239393 | validation: 0.4060334478619436]
	TIME [epoch: 1.79 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4603346356851574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4603346356851574 | validation: 0.39204038072898884]
	TIME [epoch: 1.79 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4840754205865308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4840754205865308 | validation: 0.47391672936301393]
	TIME [epoch: 1.79 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5737672221422657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5737672221422657 | validation: 0.5708756332592229]
	TIME [epoch: 1.79 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7247508959318961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7247508959318961 | validation: 0.4631603915366257]
	TIME [epoch: 1.79 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5524740145308731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5524740145308731 | validation: 0.6131966316274874]
	TIME [epoch: 1.79 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650510253838431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.650510253838431 | validation: 0.35406116618083444]
	TIME [epoch: 1.8 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5276611908977357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5276611908977357 | validation: 0.4555811429304365]
	TIME [epoch: 1.79 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.489496877271681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.489496877271681 | validation: 0.43767299202480975]
	TIME [epoch: 1.8 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46382372305363917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46382372305363917 | validation: 0.3365143267227835]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4414090674500579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4414090674500579 | validation: 0.41274781559469886]
	TIME [epoch: 1.79 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4374782529494119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4374782529494119 | validation: 0.33186522969195725]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4274753246687935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4274753246687935 | validation: 0.37052203681003637]
	TIME [epoch: 1.79 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40993801342098435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40993801342098435 | validation: 0.36507755482599213]
	TIME [epoch: 1.79 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39880991041850883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39880991041850883 | validation: 0.3390863242873406]
	TIME [epoch: 1.8 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4298922993917735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4298922993917735 | validation: 0.6125512677465883]
	TIME [epoch: 1.79 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5947250140185376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5947250140185376 | validation: 0.4550946095871856]
	TIME [epoch: 1.79 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7353069293757989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7353069293757989 | validation: 0.4961698241476966]
	TIME [epoch: 1.8 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5414554550264247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5414554550264247 | validation: 0.5189746159306351]
	TIME [epoch: 1.8 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579074043866906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6579074043866906 | validation: 0.34032399098109994]
	TIME [epoch: 1.8 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4056916761958276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4056916761958276 | validation: 0.3951728596984887]
	TIME [epoch: 1.8 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45045152136323424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45045152136323424 | validation: 0.35579325973495307]
	TIME [epoch: 1.79 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4434280762392794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4434280762392794 | validation: 0.35467520571750916]
	TIME [epoch: 1.79 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38683324631946703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38683324631946703 | validation: 0.3149836567441831]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38277996154123123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38277996154123123 | validation: 0.37497826249652827]
	TIME [epoch: 1.79 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3896818631538565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3896818631538565 | validation: 0.33936998412873276]
	TIME [epoch: 1.79 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41409512620275196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41409512620275196 | validation: 0.5157298983510078]
	TIME [epoch: 1.79 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4768778281006327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4768778281006327 | validation: 0.37319694187359664]
	TIME [epoch: 1.79 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48329012955350215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48329012955350215 | validation: 0.46460241456273776]
	TIME [epoch: 1.79 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42938394920753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42938394920753 | validation: 0.25833445171401853]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3898668333405233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3898668333405233 | validation: 0.3674585168989302]
	TIME [epoch: 1.79 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3672218760276108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3672218760276108 | validation: 0.31097326426283217]
	TIME [epoch: 1.8 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36580049550869176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36580049550869176 | validation: 0.40007458514652017]
	TIME [epoch: 1.8 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42239642146305023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42239642146305023 | validation: 0.3661763401266861]
	TIME [epoch: 1.79 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4380506512188893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4380506512188893 | validation: 0.3865137029756799]
	TIME [epoch: 1.79 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3839052847087545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3839052847087545 | validation: 0.30186654739886243]
	TIME [epoch: 1.79 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35480508554459905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35480508554459905 | validation: 0.3824123709959401]
	TIME [epoch: 1.79 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34579408915160464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34579408915160464 | validation: 0.2993759204818913]
	TIME [epoch: 1.79 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3876771865962985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3876771865962985 | validation: 0.5514532759215521]
	TIME [epoch: 1.79 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47505411522143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47505411522143 | validation: 0.3337796356980751]
	TIME [epoch: 1.8 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4442809989486534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4442809989486534 | validation: 0.4042846522460193]
	TIME [epoch: 1.79 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3734658520762446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3734658520762446 | validation: 0.3352415360701801]
	TIME [epoch: 1.79 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34376742622181844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34376742622181844 | validation: 0.2784290250505696]
	TIME [epoch: 1.79 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3406609878417272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3406609878417272 | validation: 0.37486335947564753]
	TIME [epoch: 1.8 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3216801528631198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3216801528631198 | validation: 0.2631660877739372]
	TIME [epoch: 1.8 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33241064780366075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33241064780366075 | validation: 0.34875189110803123]
	TIME [epoch: 1.8 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30680520671029043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30680520671029043 | validation: 0.3329631994379605]
	TIME [epoch: 1.8 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32853198396004696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32853198396004696 | validation: 0.39227606560649747]
	TIME [epoch: 1.79 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3515967704595771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3515967704595771 | validation: 0.36820966083174966]
	TIME [epoch: 1.8 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34471327151204334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34471327151204334 | validation: 0.3145682992175776]
	TIME [epoch: 1.79 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2709701744121182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2709701744121182 | validation: 0.2579182300365379]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2586843288066884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2586843288066884 | validation: 0.3662243761925055]
	TIME [epoch: 1.79 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2829531663701627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2829531663701627 | validation: 0.23403187006475923]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4659144396078802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4659144396078802 | validation: 0.549719866044552]
	TIME [epoch: 1.79 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49149897511150953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49149897511150953 | validation: 0.5139354818814537]
	TIME [epoch: 1.79 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47252556908683885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47252556908683885 | validation: 0.33842041537391965]
	TIME [epoch: 1.79 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3031237890652717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3031237890652717 | validation: 0.3026960990004313]
	TIME [epoch: 1.79 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2361082135864669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2361082135864669 | validation: 0.3148957076838468]
	TIME [epoch: 1.79 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24526750559098037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24526750559098037 | validation: 0.2922606561404308]
	TIME [epoch: 1.79 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2551309050719504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2551309050719504 | validation: 0.2721719101633423]
	TIME [epoch: 1.79 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2430445243647082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2430445243647082 | validation: 0.3084010314903498]
	TIME [epoch: 1.79 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2223832094459297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2223832094459297 | validation: 0.25871791520823295]
	TIME [epoch: 1.79 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27746899846967904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27746899846967904 | validation: 0.5131696877809079]
	TIME [epoch: 1.79 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4221233359743408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4221233359743408 | validation: 0.34807493298984576]
	TIME [epoch: 1.79 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36186837013871825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36186837013871825 | validation: 0.39557116467508313]
	TIME [epoch: 1.79 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2597386549106818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2597386549106818 | validation: 0.21831348138455065]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19824191063228305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19824191063228305 | validation: 0.34617586738825956]
	TIME [epoch: 1.79 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26111704191825164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26111704191825164 | validation: 0.2804052662582809]
	TIME [epoch: 1.8 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3927450838224537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3927450838224537 | validation: 0.29193604905535303]
	TIME [epoch: 1.8 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20279782385877898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20279782385877898 | validation: 0.2431281274216167]
	TIME [epoch: 1.8 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18123052357610447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18123052357610447 | validation: 0.31299468632800465]
	TIME [epoch: 1.8 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22288260723323083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22288260723323083 | validation: 0.44571646660442]
	TIME [epoch: 1.8 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3198427189394931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3198427189394931 | validation: 0.30884550930245713]
	TIME [epoch: 1.79 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24565687913794726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24565687913794726 | validation: 0.2303656495630576]
	TIME [epoch: 1.8 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16571084459422497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16571084459422497 | validation: 0.2316585205884354]
	TIME [epoch: 1.79 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14149309564390877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14149309564390877 | validation: 0.2027477362804113]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1464126611801305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1464126611801305 | validation: 0.3055453794064275]
	TIME [epoch: 1.78 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1965043335865982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1965043335865982 | validation: 0.2524776487268858]
	TIME [epoch: 1.78 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44723146987728174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44723146987728174 | validation: 0.4624851008219988]
	TIME [epoch: 1.78 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3681689044910331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3681689044910331 | validation: 0.25321795522817914]
	TIME [epoch: 1.78 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2092909099378694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2092909099378694 | validation: 0.29491064218700613]
	TIME [epoch: 1.78 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2512837453018542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2512837453018542 | validation: 0.4525098276454103]
	TIME [epoch: 1.78 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2665598224811853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2665598224811853 | validation: 0.22435778964267108]
	TIME [epoch: 1.78 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14911871143681016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14911871143681016 | validation: 0.267921225527985]
	TIME [epoch: 1.79 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18149265446628357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18149265446628357 | validation: 0.3280986292632379]
	TIME [epoch: 1.78 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2286445551638645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2286445551638645 | validation: 0.3347559400267236]
	TIME [epoch: 1.78 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22025820365351756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22025820365351756 | validation: 0.23944715025882687]
	TIME [epoch: 1.78 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.209171048705171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.209171048705171 | validation: 0.31839973220072015]
	TIME [epoch: 1.78 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20624593914166145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20624593914166145 | validation: 0.19451775050022221]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.168619771036933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.168619771036933 | validation: 0.27661079247487236]
	TIME [epoch: 1.79 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16843384012243273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16843384012243273 | validation: 0.25071296518695146]
	TIME [epoch: 1.79 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27933450482191186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27933450482191186 | validation: 0.31205547833425656]
	TIME [epoch: 1.8 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21080474431448323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21080474431448323 | validation: 0.216727102983726]
	TIME [epoch: 1.79 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14829386649476817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14829386649476817 | validation: 0.2836387657694674]
	TIME [epoch: 1.79 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16845050047712598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16845050047712598 | validation: 0.3695141396144901]
	TIME [epoch: 1.8 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2591289851373727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2591289851373727 | validation: 0.32091356558519335]
	TIME [epoch: 1.81 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23634323963210213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23634323963210213 | validation: 0.23088950539739478]
	TIME [epoch: 1.79 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18920970879178967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18920970879178967 | validation: 0.23912581279304962]
	TIME [epoch: 1.79 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13177949529465338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13177949529465338 | validation: 0.15949507875999158]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13439647074492644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13439647074492644 | validation: 0.2716996904206578]
	TIME [epoch: 1.8 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17224985452909075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17224985452909075 | validation: 0.19554824385812122]
	TIME [epoch: 1.79 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25922948741765595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25922948741765595 | validation: 0.2936901566189046]
	TIME [epoch: 1.79 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17188789334149449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17188789334149449 | validation: 0.1546848132704548]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13512385970989507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13512385970989507 | validation: 0.22856761060779288]
	TIME [epoch: 1.78 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13192621240596003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13192621240596003 | validation: 0.19407188891506313]
	TIME [epoch: 1.78 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14844583429413172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14844583429413172 | validation: 0.39506175282271033]
	TIME [epoch: 1.78 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24424947731649405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24424947731649405 | validation: 0.5130234662469586]
	TIME [epoch: 1.78 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3064293620189946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3064293620189946 | validation: 0.2116967517698241]
	TIME [epoch: 1.78 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14036923837501614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14036923837501614 | validation: 0.22640879198354666]
	TIME [epoch: 1.78 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12611205513681215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12611205513681215 | validation: 0.23942414881453283]
	TIME [epoch: 1.78 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14908229846165022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14908229846165022 | validation: 0.2933561723706604]
	TIME [epoch: 1.78 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18561430279870866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18561430279870866 | validation: 0.19805065253447307]
	TIME [epoch: 1.78 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2331758379602195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2331758379602195 | validation: 0.29612222617547596]
	TIME [epoch: 1.78 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20160692757358337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20160692757358337 | validation: 0.15262585203003678]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1525678818627722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1525678818627722 | validation: 0.1944095008294641]
	TIME [epoch: 1.79 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1018487224744977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1018487224744977 | validation: 0.14622251148599127]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09228045352132813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09228045352132813 | validation: 0.20069616385491795]
	TIME [epoch: 1.79 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11020375903582515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11020375903582515 | validation: 0.17940776911933518]
	TIME [epoch: 1.79 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1423526282436608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1423526282436608 | validation: 0.23761788680279805]
	TIME [epoch: 1.79 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.553218077985493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.553218077985493 | validation: 0.6750044430750508]
	TIME [epoch: 1.8 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5508161583702854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5508161583702854 | validation: 0.5966135463044622]
	TIME [epoch: 1.8 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37287348568534967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37287348568534967 | validation: 0.23762691612595024]
	TIME [epoch: 1.79 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2414820178389168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2414820178389168 | validation: 0.3490497244653092]
	TIME [epoch: 1.8 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28214416964595673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28214416964595673 | validation: 0.14734233489867707]
	TIME [epoch: 1.8 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14946317761930508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14946317761930508 | validation: 0.2635660736382707]
	TIME [epoch: 1.79 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3247770429052281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3247770429052281 | validation: 0.2536120106207751]
	TIME [epoch: 1.79 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1788213013965411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1788213013965411 | validation: 0.26058222926752206]
	TIME [epoch: 1.79 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23335389867230846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23335389867230846 | validation: 0.17008324397097635]
	TIME [epoch: 1.79 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12724458347095888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12724458347095888 | validation: 0.16321298017695543]
	TIME [epoch: 1.79 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12272856803368669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12272856803368669 | validation: 0.18500746382340746]
	TIME [epoch: 1.79 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10845129331342206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10845129331342206 | validation: 0.20050465784724267]
	TIME [epoch: 1.79 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13038084897809976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13038084897809976 | validation: 0.23969913539802726]
	TIME [epoch: 1.79 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1646043542368239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1646043542368239 | validation: 0.24316250291518499]
	TIME [epoch: 1.79 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18503742014054259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18503742014054259 | validation: 0.24976355443104672]
	TIME [epoch: 1.79 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1606266596869653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1606266596869653 | validation: 0.15008860145597439]
	TIME [epoch: 1.79 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11262377536514027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11262377536514027 | validation: 0.17126787359063944]
	TIME [epoch: 1.79 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09545465723990759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09545465723990759 | validation: 0.1365758501889541]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10470717964926571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10470717964926571 | validation: 0.23426439861797968]
	TIME [epoch: 1.79 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1703673636821779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1703673636821779 | validation: 0.20662144751430056]
	TIME [epoch: 1.79 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2311753322928506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2311753322928506 | validation: 0.22088553456961782]
	TIME [epoch: 1.78 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12150479295142301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12150479295142301 | validation: 0.13575147676472335]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07929024410937872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07929024410937872 | validation: 0.16893232499677682]
	TIME [epoch: 1.79 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10017398972336782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10017398972336782 | validation: 0.3069170965702662]
	TIME [epoch: 1.79 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20194029043212103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20194029043212103 | validation: 0.47834378074108974]
	TIME [epoch: 1.79 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34802304659594824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34802304659594824 | validation: 0.25058044345753655]
	TIME [epoch: 1.8 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16839857373702416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16839857373702416 | validation: 0.16421609146888122]
	TIME [epoch: 1.79 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08066163169031974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08066163169031974 | validation: 0.13273019595654345]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08143140145478948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08143140145478948 | validation: 0.1803036188744844]
	TIME [epoch: 1.79 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09586109064834931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09586109064834931 | validation: 0.13404453441594041]
	TIME [epoch: 1.79 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12364206172723673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12364206172723673 | validation: 0.21675825651826336]
	TIME [epoch: 1.79 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11946156321102785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11946156321102785 | validation: 0.1481575356906021]
	TIME [epoch: 1.79 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21201029234556565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21201029234556565 | validation: 0.19796089200637224]
	TIME [epoch: 1.79 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12878819594030597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12878819594030597 | validation: 0.15555317103666208]
	TIME [epoch: 1.79 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15527437919059747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15527437919059747 | validation: 0.2637195036930808]
	TIME [epoch: 1.79 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1899700724141642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1899700724141642 | validation: 0.1567559275691152]
	TIME [epoch: 1.79 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13839947300087138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13839947300087138 | validation: 0.17770978466911586]
	TIME [epoch: 1.79 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10691744472784961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10691744472784961 | validation: 0.2841799522614431]
	TIME [epoch: 1.79 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.146415651138148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.146415651138148 | validation: 0.3012613242987292]
	TIME [epoch: 1.79 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1787658595435824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1787658595435824 | validation: 0.23098886088628748]
	TIME [epoch: 1.79 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.157354892047434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.157354892047434 | validation: 0.20658885961667628]
	TIME [epoch: 1.79 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12272827274338667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12272827274338667 | validation: 0.12858406955004628]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12250389789291902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12250389789291902 | validation: 0.2059588008959844]
	TIME [epoch: 1.78 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11976327103302449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11976327103302449 | validation: 0.12441888221979279]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09962084518463747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09962084518463747 | validation: 0.16033633094198163]
	TIME [epoch: 1.79 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09367323993658076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09367323993658076 | validation: 0.16775957134737748]
	TIME [epoch: 1.79 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11078389820827175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11078389820827175 | validation: 0.2688043507235996]
	TIME [epoch: 1.79 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1711735541312571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1711735541312571 | validation: 0.3361868628828938]
	TIME [epoch: 1.79 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21171636279225806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21171636279225806 | validation: 0.20170586700748086]
	TIME [epoch: 1.8 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14220280635042862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14220280635042862 | validation: 0.16437759821215292]
	TIME [epoch: 1.8 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0970201561753462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0970201561753462 | validation: 0.14587292668304513]
	TIME [epoch: 1.8 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09059786726187462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09059786726187462 | validation: 0.17838497909768303]
	TIME [epoch: 1.79 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09490604237005329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09490604237005329 | validation: 0.16087036899792315]
	TIME [epoch: 1.79 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1108867634366955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1108867634366955 | validation: 0.22132846027026576]
	TIME [epoch: 1.8 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1385790300657145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1385790300657145 | validation: 0.18378099941512313]
	TIME [epoch: 1.79 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1773335977433746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1773335977433746 | validation: 0.23585089543202128]
	TIME [epoch: 1.79 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15594754581756823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15594754581756823 | validation: 0.15958818987642515]
	TIME [epoch: 1.79 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11726036366173084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11726036366173084 | validation: 0.2129157212735273]
	TIME [epoch: 1.78 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15393971229656342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15393971229656342 | validation: 0.30853270519258474]
	TIME [epoch: 1.79 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18799340801582345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18799340801582345 | validation: 0.19540836216627383]
	TIME [epoch: 1.79 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1315903896251432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1315903896251432 | validation: 0.16345000005639282]
	TIME [epoch: 1.78 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07748934297238941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07748934297238941 | validation: 0.13811710645514125]
	TIME [epoch: 1.78 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061484681016083335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061484681016083335 | validation: 0.10715781573924053]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.064198144447501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.064198144447501 | validation: 0.15787967136265413]
	TIME [epoch: 1.79 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07891035402582214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07891035402582214 | validation: 0.12698987180582308]
	TIME [epoch: 1.79 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10869363964389513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10869363964389513 | validation: 0.21431020448428417]
	TIME [epoch: 1.79 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14072087909956274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14072087909956274 | validation: 0.19997228353158694]
	TIME [epoch: 1.79 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18066671414181726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18066671414181726 | validation: 0.3348615105399448]
	TIME [epoch: 1.79 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25433385994250074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25433385994250074 | validation: 0.3050005051268185]
	TIME [epoch: 1.79 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18281905086438718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18281905086438718 | validation: 0.14713586755537245]
	TIME [epoch: 1.79 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09104927124628978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09104927124628978 | validation: 0.14170682770475482]
	TIME [epoch: 1.79 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06721212756461074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06721212756461074 | validation: 0.11962916067640848]
	TIME [epoch: 1.79 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06979067315257956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06979067315257956 | validation: 0.1575023396515666]
	TIME [epoch: 1.79 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08098337901343321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08098337901343321 | validation: 0.12656215552145422]
	TIME [epoch: 1.79 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10108374227413572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10108374227413572 | validation: 0.1805815257968709]
	TIME [epoch: 1.8 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11994876459634295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11994876459634295 | validation: 0.13724666813540462]
	TIME [epoch: 1.8 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11032232243880746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11032232243880746 | validation: 0.1879671061885266]
	TIME [epoch: 1.79 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10934619844029735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10934619844029735 | validation: 0.15780752312488494]
	TIME [epoch: 1.79 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10064500474888588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10064500474888588 | validation: 0.19657983905615772]
	TIME [epoch: 1.79 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12244540862039656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12244540862039656 | validation: 0.30683519177198915]
	TIME [epoch: 1.8 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17401137880621362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17401137880621362 | validation: 0.20101787261052004]
	TIME [epoch: 1.79 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12525937486512148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12525937486512148 | validation: 0.17867928528826774]
	TIME [epoch: 1.79 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.089540166984116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.089540166984116 | validation: 0.10503137843301485]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_426.pth
	Model improved!!!
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06905697769712042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06905697769712042 | validation: 0.1694044594636277]
	TIME [epoch: 1.79 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08980260066212814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08980260066212814 | validation: 0.1709392946703587]
	TIME [epoch: 1.79 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19129628778735458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19129628778735458 | validation: 0.2198460832578599]
	TIME [epoch: 1.79 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13028323437990605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13028323437990605 | validation: 0.117044280141994]
	TIME [epoch: 1.8 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0765076652710375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0765076652710375 | validation: 0.12877076367550058]
	TIME [epoch: 1.8 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07877448169941792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07877448169941792 | validation: 0.1744199909458272]
	TIME [epoch: 1.8 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14430672880480244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14430672880480244 | validation: 0.29700728019362255]
	TIME [epoch: 1.8 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25071613557697575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25071613557697575 | validation: 0.2955346744175066]
	TIME [epoch: 1.8 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17371687113257053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17371687113257053 | validation: 0.1400536805304465]
	TIME [epoch: 1.79 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07341850888027353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07341850888027353 | validation: 0.13781637218359244]
	TIME [epoch: 1.79 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06318115299063778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06318115299063778 | validation: 0.11970215302946477]
	TIME [epoch: 1.79 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06446444472938007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06446444472938007 | validation: 0.1375340763280126]
	TIME [epoch: 1.79 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07388743128600557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07388743128600557 | validation: 0.11757624936466626]
	TIME [epoch: 1.79 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07183253864551878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07183253864551878 | validation: 0.148236177400649]
	TIME [epoch: 1.79 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0819811112928095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0819811112928095 | validation: 0.1577282898097019]
	TIME [epoch: 1.79 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12470738426119334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12470738426119334 | validation: 0.26944058681339084]
	TIME [epoch: 1.8 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.174039736527983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.174039736527983 | validation: 0.256707469596257]
	TIME [epoch: 1.8 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17994106488423442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17994106488423442 | validation: 0.18708864228248784]
	TIME [epoch: 1.81 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1110724449074371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1110724449074371 | validation: 0.1811895809937295]
	TIME [epoch: 1.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08890109633563036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08890109633563036 | validation: 0.09874658527128341]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0764098803876634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0764098803876634 | validation: 0.14609238622565765]
	TIME [epoch: 1.8 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07338166897841869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07338166897841869 | validation: 0.10093597026401571]
	TIME [epoch: 1.79 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07804291276858141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07804291276858141 | validation: 0.14050227561184814]
	TIME [epoch: 1.79 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07411816878064167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07411816878064167 | validation: 0.11678109885908966]
	TIME [epoch: 1.79 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0913675005829688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0913675005829688 | validation: 0.2124842121675733]
	TIME [epoch: 1.8 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12973941389403798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12973941389403798 | validation: 0.16260302692374726]
	TIME [epoch: 1.79 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18684890459807804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18684890459807804 | validation: 0.20016182831369023]
	TIME [epoch: 1.79 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18234524671611596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18234524671611596 | validation: 0.15732969132621422]
	TIME [epoch: 1.79 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08061232614590777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08061232614590777 | validation: 0.17190271745769248]
	TIME [epoch: 1.8 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09982137124760632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09982137124760632 | validation: 0.23427078914650182]
	TIME [epoch: 1.8 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11127595458880075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11127595458880075 | validation: 0.11839475052435199]
	TIME [epoch: 1.8 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06486323642512633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06486323642512633 | validation: 0.1161943206412341]
	TIME [epoch: 1.8 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06165211688530995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06165211688530995 | validation: 0.13763045797363663]
	TIME [epoch: 1.79 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06440060391882367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06440060391882367 | validation: 0.11335556516420248]
	TIME [epoch: 1.8 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08779683074113911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08779683074113911 | validation: 0.21838063756947124]
	TIME [epoch: 1.79 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15422890569918113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15422890569918113 | validation: 0.2165064121407448]
	TIME [epoch: 1.79 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17699583647831874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17699583647831874 | validation: 0.16774781158625385]
	TIME [epoch: 1.79 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09633191698533253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09633191698533253 | validation: 0.15596941358970462]
	TIME [epoch: 1.8 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10668645181302516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10668645181302516 | validation: 0.20277462614395964]
	TIME [epoch: 1.81 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17251255519795666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17251255519795666 | validation: 0.16427125575007995]
	TIME [epoch: 1.8 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10236295299949408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10236295299949408 | validation: 0.1323448604867198]
	TIME [epoch: 1.79 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07004318220294063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07004318220294063 | validation: 0.11047968946783181]
	TIME [epoch: 1.8 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06437999484356201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06437999484356201 | validation: 0.12577708304644197]
	TIME [epoch: 1.8 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05409917245455308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05409917245455308 | validation: 0.09895520849042592]
	TIME [epoch: 1.81 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05512075727104542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05512075727104542 | validation: 0.15117916813454352]
	TIME [epoch: 1.8 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06839577307336506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06839577307336506 | validation: 0.14077639750380566]
	TIME [epoch: 1.8 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10651737931859348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10651737931859348 | validation: 0.27942503809759134]
	TIME [epoch: 1.8 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1728953670055912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1728953670055912 | validation: 0.15966299373553913]
	TIME [epoch: 1.8 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11781316817020923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11781316817020923 | validation: 0.13809159277912525]
	TIME [epoch: 1.79 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07039271475439916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07039271475439916 | validation: 0.1234406534132842]
	TIME [epoch: 1.8 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05711903895265939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05711903895265939 | validation: 0.09204145346646692]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0638681353553647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0638681353553647 | validation: 0.13933814895621896]
	TIME [epoch: 1.8 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054383528880792745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054383528880792745 | validation: 0.10608744637956927]
	TIME [epoch: 1.79 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059642653981295465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059642653981295465 | validation: 0.1464028506351401]
	TIME [epoch: 1.8 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10135137548868996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10135137548868996 | validation: 0.28205052890877313]
	TIME [epoch: 1.79 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27046593793355583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27046593793355583 | validation: 0.4778194200656429]
	TIME [epoch: 1.79 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3625356578798406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3625356578798406 | validation: 0.1448746378085554]
	TIME [epoch: 1.8 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07883182834123195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07883182834123195 | validation: 0.17313484408348326]
	TIME [epoch: 1.8 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10265923209171249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10265923209171249 | validation: 0.1498410068825767]
	TIME [epoch: 1.8 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07908267583767434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07908267583767434 | validation: 0.10467537799049215]
	TIME [epoch: 1.79 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0695791605428324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0695791605428324 | validation: 0.13471125092595257]
	TIME [epoch: 1.8 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07198911056881693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07198911056881693 | validation: 0.09688201849414896]
	TIME [epoch: 1.79 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08823390504743045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08823390504743045 | validation: 0.16036821770007237]
	TIME [epoch: 1.8 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07008645687397273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07008645687397273 | validation: 0.08876853374236704]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05210710961320503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05210710961320503 | validation: 0.12015924598036856]
	TIME [epoch: 1.8 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04522017227815688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04522017227815688 | validation: 0.09286957860089218]
	TIME [epoch: 1.8 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047902055988466544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047902055988466544 | validation: 0.14600925267770246]
	TIME [epoch: 1.8 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09674621768541648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09674621768541648 | validation: 0.187740554713715]
	TIME [epoch: 1.8 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18043483469166752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18043483469166752 | validation: 0.20811500838069652]
	TIME [epoch: 1.8 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16323749459629786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16323749459629786 | validation: 0.08598664152266015]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_496.pth
	Model improved!!!
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054154206499417365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054154206499417365 | validation: 0.1489647281637063]
	TIME [epoch: 1.8 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07234470705621705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07234470705621705 | validation: 0.09571795636679603]
	TIME [epoch: 1.8 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0748583422567022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0748583422567022 | validation: 0.12531209788903483]
	TIME [epoch: 1.79 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051308690243259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051308690243259 | validation: 0.1087276952538266]
	TIME [epoch: 1.8 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05364567288713199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05364567288713199 | validation: 0.1642239754435888]
	TIME [epoch: 42.9 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06885046172496276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06885046172496276 | validation: 0.16473001690121514]
	TIME [epoch: 3.57 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11545402764922552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11545402764922552 | validation: 0.2872271955808741]
	TIME [epoch: 3.55 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15572808496183665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15572808496183665 | validation: 0.12580599471414217]
	TIME [epoch: 3.55 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09285793052791914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09285793052791914 | validation: 0.15499753141676573]
	TIME [epoch: 3.54 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12682282493679733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12682282493679733 | validation: 0.17449177509058822]
	TIME [epoch: 3.55 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14836985152619941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14836985152619941 | validation: 0.1836416038611617]
	TIME [epoch: 3.55 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.120840117397603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.120840117397603 | validation: 0.1923417914041246]
	TIME [epoch: 3.55 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12413630981846456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12413630981846456 | validation: 0.08358211222197681]
	TIME [epoch: 3.54 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08192016169880048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08192016169880048 | validation: 0.11755791538357563]
	TIME [epoch: 3.55 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057561849285955854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057561849285955854 | validation: 0.08425622078885844]
	TIME [epoch: 3.54 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04779115641335875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04779115641335875 | validation: 0.09616501220928558]
	TIME [epoch: 3.54 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0493260875693591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0493260875693591 | validation: 0.09182512101645396]
	TIME [epoch: 3.55 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045862441940836565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045862441940836565 | validation: 0.12242329682965636]
	TIME [epoch: 3.54 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060650534262642206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060650534262642206 | validation: 0.2039743106705617]
	TIME [epoch: 3.54 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11280064674512133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11280064674512133 | validation: 0.21857249423579309]
	TIME [epoch: 3.55 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13180559132341021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13180559132341021 | validation: 0.15737524008198644]
	TIME [epoch: 3.55 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11853587944707936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11853587944707936 | validation: 0.19044067152013566]
	TIME [epoch: 3.54 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1462990583418975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1462990583418975 | validation: 0.15699212211717206]
	TIME [epoch: 3.53 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13615436368743405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13615436368743405 | validation: 0.14249904833966465]
	TIME [epoch: 3.54 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0863284931082594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0863284931082594 | validation: 0.09832986411207267]
	TIME [epoch: 3.54 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04420218913975237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04420218913975237 | validation: 0.09227159976330337]
	TIME [epoch: 3.54 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04326877426927327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04326877426927327 | validation: 0.11622639073616636]
	TIME [epoch: 3.53 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051114563427666425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051114563427666425 | validation: 0.08336422777514044]
	TIME [epoch: 3.54 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09122746838821193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09122746838821193 | validation: 0.18247725940135787]
	TIME [epoch: 3.54 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10201920468480868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10201920468480868 | validation: 0.10941989844780363]
	TIME [epoch: 3.54 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08336775604121323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08336775604121323 | validation: 0.16948556325300715]
	TIME [epoch: 3.54 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.092851952452728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.092851952452728 | validation: 0.26140834917945865]
	TIME [epoch: 3.54 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16618801212002382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16618801212002382 | validation: 0.18442634748835218]
	TIME [epoch: 3.54 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1501453231871142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1501453231871142 | validation: 0.13957341347305316]
	TIME [epoch: 3.55 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07974646552850748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07974646552850748 | validation: 0.09431837944370403]
	TIME [epoch: 3.55 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052833052660162264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052833052660162264 | validation: 0.09724268081978577]
	TIME [epoch: 3.54 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051939893986431916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051939893986431916 | validation: 0.09961880049529986]
	TIME [epoch: 3.54 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0539777477240234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0539777477240234 | validation: 0.08909932800406839]
	TIME [epoch: 3.54 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04640936337893833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04640936337893833 | validation: 0.09974058722736119]
	TIME [epoch: 3.55 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04563331110291637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04563331110291637 | validation: 0.12022618938804919]
	TIME [epoch: 3.55 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06366705981926578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06366705981926578 | validation: 0.18477403883435028]
	TIME [epoch: 3.54 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0987631167412408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0987631167412408 | validation: 0.19273637499029794]
	TIME [epoch: 3.53 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12551912205947297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12551912205947297 | validation: 0.13669549634814326]
	TIME [epoch: 3.54 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09414081891561388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09414081891561388 | validation: 0.12660774630591787]
	TIME [epoch: 3.54 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08770747739806382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08770747739806382 | validation: 0.23275613645554902]
	TIME [epoch: 3.54 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16328503153861315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16328503153861315 | validation: 0.14062147308965015]
	TIME [epoch: 3.55 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09762534126208645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09762534126208645 | validation: 0.10337926460381941]
	TIME [epoch: 3.55 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043931620857914834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043931620857914834 | validation: 0.09376192717957604]
	TIME [epoch: 3.54 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0381707788287618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0381707788287618 | validation: 0.12384996347879083]
	TIME [epoch: 3.56 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05212738427338252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05212738427338252 | validation: 0.1105153617810897]
	TIME [epoch: 3.54 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0887970324858737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0887970324858737 | validation: 0.20013105521511163]
	TIME [epoch: 3.53 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10641380881486648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10641380881486648 | validation: 0.09147951307819836]
	TIME [epoch: 3.53 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07802095429231932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07802095429231932 | validation: 0.10899934392126398]
	TIME [epoch: 3.54 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04340962568029724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04340962568029724 | validation: 0.11979096305754777]
	TIME [epoch: 3.54 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06465993418990483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06465993418990483 | validation: 0.21090283152156736]
	TIME [epoch: 3.54 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1301954361412214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1301954361412214 | validation: 0.18192235777979748]
	TIME [epoch: 3.54 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09842307127436468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09842307127436468 | validation: 0.11684730209455968]
	TIME [epoch: 3.54 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04734781112730946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04734781112730946 | validation: 0.07502837224375832]
	TIME [epoch: 3.54 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04177294114675515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04177294114675515 | validation: 0.11266563105638112]
	TIME [epoch: 3.52 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05732020367274747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05732020367274747 | validation: 0.12469735501474691]
	TIME [epoch: 3.53 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08638954101995446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08638954101995446 | validation: 0.218904292910482]
	TIME [epoch: 3.52 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1576109990387603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1576109990387603 | validation: 0.22668221645926165]
	TIME [epoch: 3.53 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23261702799456424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23261702799456424 | validation: 0.15685930283641727]
	TIME [epoch: 3.53 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12644486617702058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12644486617702058 | validation: 0.13042166639383396]
	TIME [epoch: 3.52 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05177198400218996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05177198400218996 | validation: 0.1082771687815186]
	TIME [epoch: 3.52 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07071697443984054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07071697443984054 | validation: 0.13649872095450727]
	TIME [epoch: 3.52 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060479206105410424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060479206105410424 | validation: 0.07659673504953313]
	TIME [epoch: 3.52 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05095737921248396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05095737921248396 | validation: 0.11798314781867664]
	TIME [epoch: 3.52 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055334595407936574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055334595407936574 | validation: 0.07378969635093519]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05109604303526446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05109604303526446 | validation: 0.10816623649826096]
	TIME [epoch: 3.53 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042970000206721995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042970000206721995 | validation: 0.08950311811144646]
	TIME [epoch: 3.54 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04242235290498947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04242235290498947 | validation: 0.09841221444814977]
	TIME [epoch: 3.54 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05242119459503407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05242119459503407 | validation: 0.08895170615539116]
	TIME [epoch: 3.52 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06309738688226817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06309738688226817 | validation: 0.12031452741198101]
	TIME [epoch: 3.53 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06320057340459885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06320057340459885 | validation: 0.09605154073852529]
	TIME [epoch: 3.53 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061819879967115014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061819879967115014 | validation: 0.09321175712418857]
	TIME [epoch: 3.54 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03950838586499062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03950838586499062 | validation: 0.16466743859086685]
	TIME [epoch: 3.53 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08058594024078658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08058594024078658 | validation: 0.4573188895537314]
	TIME [epoch: 3.53 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25965267335913406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25965267335913406 | validation: 0.18569388328167594]
	TIME [epoch: 3.52 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21400767380038388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21400767380038388 | validation: 0.13848503977551024]
	TIME [epoch: 3.53 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07919691190349026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07919691190349026 | validation: 0.13463506734447725]
	TIME [epoch: 3.52 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0657573763430191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0657573763430191 | validation: 0.08207542349149599]
	TIME [epoch: 3.52 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07592069796183702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07592069796183702 | validation: 0.09717625278469548]
	TIME [epoch: 3.52 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04275667862157212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04275667862157212 | validation: 0.07284294096885321]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03541492326541134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03541492326541134 | validation: 0.10116182700136099]
	TIME [epoch: 3.54 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04264624620185223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04264624620185223 | validation: 0.09393689649641676]
	TIME [epoch: 3.54 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06879567718004474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06879567718004474 | validation: 0.20876858966840606]
	TIME [epoch: 3.54 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375984515903542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1375984515903542 | validation: 0.19026148712858035]
	TIME [epoch: 3.55 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14841996819080155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14841996819080155 | validation: 0.133305131396083]
	TIME [epoch: 3.55 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08812680640291039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08812680640291039 | validation: 0.10267675006889264]
	TIME [epoch: 3.55 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05228875373990777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05228875373990777 | validation: 0.07262165982586337]
	TIME [epoch: 3.55 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_587.pth
	Model improved!!!
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05796937770178721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05796937770178721 | validation: 0.10682504237989648]
	TIME [epoch: 3.53 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048024532721864556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048024532721864556 | validation: 0.06346878590630804]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04778451007942389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04778451007942389 | validation: 0.10205042052677733]
	TIME [epoch: 3.54 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040368409991081045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040368409991081045 | validation: 0.06588629045042148]
	TIME [epoch: 3.54 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036003041128475466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036003041128475466 | validation: 0.08906284033754518]
	TIME [epoch: 3.54 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043639941863909314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043639941863909314 | validation: 0.11703918676884309]
	TIME [epoch: 3.54 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08052089775422258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08052089775422258 | validation: 0.27683530392247774]
	TIME [epoch: 3.54 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17826807202356698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17826807202356698 | validation: 0.24585369686922248]
	TIME [epoch: 3.56 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19985476600706822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19985476600706822 | validation: 0.11467439816358188]
	TIME [epoch: 3.54 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0746992813058986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0746992813058986 | validation: 0.08584407759044382]
	TIME [epoch: 3.54 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06776974235130666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06776974235130666 | validation: 0.14124271507012107]
	TIME [epoch: 3.54 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07014569763774117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07014569763774117 | validation: 0.08508347275938993]
	TIME [epoch: 3.55 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06200740135014438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06200740135014438 | validation: 0.1164267579358343]
	TIME [epoch: 3.56 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057856167984877555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057856167984877555 | validation: 0.07562546637838904]
	TIME [epoch: 3.54 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05254228558643963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05254228558643963 | validation: 0.09863008992287292]
	TIME [epoch: 3.53 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04788441792300247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04788441792300247 | validation: 0.09375003928685018]
	TIME [epoch: 3.54 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05167247789738161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05167247789738161 | validation: 0.13506766076306015]
	TIME [epoch: 3.54 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0733280181759425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0733280181759425 | validation: 0.16883735679495493]
	TIME [epoch: 3.54 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11876987417802722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11876987417802722 | validation: 0.13826295199582456]
	TIME [epoch: 3.54 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08994022235509108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08994022235509108 | validation: 0.11221698181919293]
	TIME [epoch: 3.54 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0695505142321826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0695505142321826 | validation: 0.0806214881645968]
	TIME [epoch: 3.54 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09197791976597035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09197791976597035 | validation: 0.11596610271296622]
	TIME [epoch: 3.54 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06270944112951816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06270944112951816 | validation: 0.0795227268391]
	TIME [epoch: 3.55 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04776329498311476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04776329498311476 | validation: 0.07365581421011803]
	TIME [epoch: 3.55 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06335345795394602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06335345795394602 | validation: 0.12641726908465284]
	TIME [epoch: 3.54 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07326123489724207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07326123489724207 | validation: 0.13006264113286478]
	TIME [epoch: 3.54 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0866466843144665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0866466843144665 | validation: 0.1681040958542134]
	TIME [epoch: 3.55 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11096075724920175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11096075724920175 | validation: 0.12687095072058327]
	TIME [epoch: 3.54 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09844331046478635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09844331046478635 | validation: 0.124820809250406]
	TIME [epoch: 3.54 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08096549368843907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08096549368843907 | validation: 0.14817574745260678]
	TIME [epoch: 3.54 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06949383167503646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06949383167503646 | validation: 0.10219459343167997]
	TIME [epoch: 3.54 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045292756279237005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045292756279237005 | validation: 0.07212537020738762]
	TIME [epoch: 3.55 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03418863166081837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03418863166081837 | validation: 0.09230666196372611]
	TIME [epoch: 3.53 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03308108783000002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03308108783000002 | validation: 0.06780787897116039]
	TIME [epoch: 3.55 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040830016321991666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040830016321991666 | validation: 0.1132705306311665]
	TIME [epoch: 3.55 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05719386766645666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05719386766645666 | validation: 0.10325210575715578]
	TIME [epoch: 3.54 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08813819857378015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08813819857378015 | validation: 0.1635936899467113]
	TIME [epoch: 3.54 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10953570894952414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10953570894952414 | validation: 0.17061844626297284]
	TIME [epoch: 3.54 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15568162184171425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15568162184171425 | validation: 0.1924317564550828]
	TIME [epoch: 3.55 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11304949665716682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11304949665716682 | validation: 0.12346258252055438]
	TIME [epoch: 3.55 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06191685266015418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06191685266015418 | validation: 0.08625241733211254]
	TIME [epoch: 3.56 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04473052481290143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04473052481290143 | validation: 0.08468013485370196]
	TIME [epoch: 3.54 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03313762059754495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03313762059754495 | validation: 0.08039088712498788]
	TIME [epoch: 3.54 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029975187904933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029975187904933 | validation: 0.07168251517003053]
	TIME [epoch: 3.54 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03153346706822382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03153346706822382 | validation: 0.09753906448449527]
	TIME [epoch: 3.54 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0422075830284164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0422075830284164 | validation: 0.1398168979824514]
	TIME [epoch: 3.55 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08686428039195147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08686428039195147 | validation: 0.22685028657076992]
	TIME [epoch: 3.54 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1579788014653452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1579788014653452 | validation: 0.18134149565101046]
	TIME [epoch: 3.54 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16245655508003573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16245655508003573 | validation: 0.2628029639234861]
	TIME [epoch: 3.54 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17463248971493364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17463248971493364 | validation: 0.10079425912410787]
	TIME [epoch: 3.55 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06192713093203872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06192713093203872 | validation: 0.09304352104155654]
	TIME [epoch: 3.55 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04856450722536332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04856450722536332 | validation: 0.10637933396283256]
	TIME [epoch: 3.55 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045866550546557736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045866550546557736 | validation: 0.07474849861124534]
	TIME [epoch: 3.55 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038896100264957205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038896100264957205 | validation: 0.0663583564156751]
	TIME [epoch: 3.55 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026885442993711785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026885442993711785 | validation: 0.0830706947608965]
	TIME [epoch: 3.56 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02403874209766257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02403874209766257 | validation: 0.06332652597463148]
	TIME [epoch: 3.55 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_643.pth
	Model improved!!!
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02353998231825679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02353998231825679 | validation: 0.07926733377976568]
	TIME [epoch: 3.53 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027490822921485265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027490822921485265 | validation: 0.10152959009237202]
	TIME [epoch: 3.54 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04832171854428502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04832171854428502 | validation: 0.20361074630585574]
	TIME [epoch: 3.53 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12831859222054726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12831859222054726 | validation: 0.2542596300598829]
	TIME [epoch: 3.54 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21453519270192153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21453519270192153 | validation: 0.10108692013986831]
	TIME [epoch: 3.54 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14391302510922066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14391302510922066 | validation: 0.1251888117183324]
	TIME [epoch: 3.54 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057101547712241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057101547712241 | validation: 0.10981951316457837]
	TIME [epoch: 3.54 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07193675174077219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07193675174077219 | validation: 0.10749443102652514]
	TIME [epoch: 3.55 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05499368496372389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05499368496372389 | validation: 0.08224415154028976]
	TIME [epoch: 3.54 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054611209879786314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054611209879786314 | validation: 0.12137935059024603]
	TIME [epoch: 3.54 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07064914856902106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07064914856902106 | validation: 0.07734840028135131]
	TIME [epoch: 3.54 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0518947029212823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0518947029212823 | validation: 0.08160213764131624]
	TIME [epoch: 3.55 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03200538288905063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03200538288905063 | validation: 0.07245226844679059]
	TIME [epoch: 3.55 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026827735080509285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026827735080509285 | validation: 0.09467123733607274]
	TIME [epoch: 3.54 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034010436246173524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034010436246173524 | validation: 0.09490605047374356]
	TIME [epoch: 3.54 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05558317499862984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05558317499862984 | validation: 0.18904117890836158]
	TIME [epoch: 3.53 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09456088221524436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09456088221524436 | validation: 0.13997719384445148]
	TIME [epoch: 3.53 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13130495908770795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13130495908770795 | validation: 0.14220679397499456]
	TIME [epoch: 3.54 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12255735840776939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12255735840776939 | validation: 0.1419752315599053]
	TIME [epoch: 3.54 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08087707264418313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08087707264418313 | validation: 0.08757250827535328]
	TIME [epoch: 3.54 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07565648790799222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07565648790799222 | validation: 0.11095201474964136]
	TIME [epoch: 3.54 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05330088717898942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05330088717898942 | validation: 0.06235606262175208]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_665.pth
	Model improved!!!
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030034336057006596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030034336057006596 | validation: 0.08374444719939744]
	TIME [epoch: 3.53 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026053419019331843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026053419019331843 | validation: 0.07761378170757048]
	TIME [epoch: 3.54 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02745909827444561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02745909827444561 | validation: 0.06232137253426401]
	TIME [epoch: 3.54 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_668.pth
	Model improved!!!
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02866647295014586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02866647295014586 | validation: 0.0881251812649031]
	TIME [epoch: 3.55 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04273243733016188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04273243733016188 | validation: 0.14090590625846885]
	TIME [epoch: 3.53 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11846205942067975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11846205942067975 | validation: 0.27889297272109925]
	TIME [epoch: 3.53 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21549676000825407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21549676000825407 | validation: 0.12876290013938474]
	TIME [epoch: 3.54 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08238401919207519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08238401919207519 | validation: 0.06517047751141068]
	TIME [epoch: 3.53 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04042237938848525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04042237938848525 | validation: 0.11150451191009819]
	TIME [epoch: 3.53 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05981795181440939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05981795181440939 | validation: 0.07146039885692056]
	TIME [epoch: 3.54 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05702076391176731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05702076391176731 | validation: 0.11628124521814603]
	TIME [epoch: 3.54 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06706862862387318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06706862862387318 | validation: 0.10489961946717266]
	TIME [epoch: 3.54 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0782469463474307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0782469463474307 | validation: 0.09777391476203028]
	TIME [epoch: 3.54 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06756809761499856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06756809761499856 | validation: 0.10504021496912506]
	TIME [epoch: 3.54 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045893558124541815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045893558124541815 | validation: 0.06633086202700099]
	TIME [epoch: 3.55 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03401991290549787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03401991290549787 | validation: 0.06929028926048231]
	TIME [epoch: 3.55 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028896438014333073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028896438014333073 | validation: 0.09595426394442963]
	TIME [epoch: 3.54 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0332553094514602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0332553094514602 | validation: 0.10767232149026572]
	TIME [epoch: 3.56 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07254958537044895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07254958537044895 | validation: 0.26676150711895213]
	TIME [epoch: 3.52 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16533009651809985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16533009651809985 | validation: 0.17677290914296662]
	TIME [epoch: 3.53 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13015330398421635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13015330398421635 | validation: 0.0939849730290983]
	TIME [epoch: 3.55 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05899499942474247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05899499942474247 | validation: 0.09409847548584986]
	TIME [epoch: 3.55 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04966370611536426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04966370611536426 | validation: 0.06805081485093159]
	TIME [epoch: 3.55 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04602854735136409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04602854735136409 | validation: 0.08400181236573695]
	TIME [epoch: 3.55 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031046924880494996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031046924880494996 | validation: 0.06180756859271239]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_690.pth
	Model improved!!!
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023810264240267758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023810264240267758 | validation: 0.06193175357163876]
	TIME [epoch: 3.54 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018930902540672347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018930902540672347 | validation: 0.08300639017458075]
	TIME [epoch: 3.54 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027370253301824722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027370253301824722 | validation: 0.09214638332880937]
	TIME [epoch: 3.53 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047596256346214984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047596256346214984 | validation: 0.18225632966157357]
	TIME [epoch: 3.53 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10858001127924594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10858001127924594 | validation: 0.12716188100238493]
	TIME [epoch: 3.54 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1087077363550539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1087077363550539 | validation: 0.0967826931241065]
	TIME [epoch: 3.54 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09689649208646936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09689649208646936 | validation: 0.12112565158421003]
	TIME [epoch: 3.55 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0878790452722205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0878790452722205 | validation: 0.13478080126470438]
	TIME [epoch: 3.54 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11523094410017345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11523094410017345 | validation: 0.16310068975091305]
	TIME [epoch: 3.54 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760038635918826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07760038635918826 | validation: 0.1160962304036287]
	TIME [epoch: 3.54 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042022418070065334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042022418070065334 | validation: 0.05985050284160847]
	TIME [epoch: 3.54 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_701.pth
	Model improved!!!
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031433856800258286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031433856800258286 | validation: 0.08772640928843743]
	TIME [epoch: 3.54 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029887023502874213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029887023502874213 | validation: 0.06285068962410928]
	TIME [epoch: 3.55 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028469506087709644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028469506087709644 | validation: 0.07550467452980571]
	TIME [epoch: 3.54 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025648528668240092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025648528668240092 | validation: 0.0644483616082161]
	TIME [epoch: 3.56 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03221904140336328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03221904140336328 | validation: 0.09396187450123265]
	TIME [epoch: 3.54 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04805396896850867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04805396896850867 | validation: 0.07637307858428836]
	TIME [epoch: 3.55 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052756351881272104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052756351881272104 | validation: 0.09004375389607891]
	TIME [epoch: 3.54 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05961269760965863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05961269760965863 | validation: 0.09084086492740315]
	TIME [epoch: 3.54 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03720840184148944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03720840184148944 | validation: 0.081303459486601]
	TIME [epoch: 3.54 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051632917610915784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051632917610915784 | validation: 0.20197274925405828]
	TIME [epoch: 3.55 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08913808606886615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08913808606886615 | validation: 0.11631515986507002]
	TIME [epoch: 3.54 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07927241430375595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07927241430375595 | validation: 0.10986819928708194]
	TIME [epoch: 3.55 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048875360686162835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048875360686162835 | validation: 0.09247204669766296]
	TIME [epoch: 3.54 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042237795007872996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042237795007872996 | validation: 0.0746071752988331]
	TIME [epoch: 3.53 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05536600841917573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05536600841917573 | validation: 0.1413517792734451]
	TIME [epoch: 3.55 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06993574457108309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06993574457108309 | validation: 0.15352027358161244]
	TIME [epoch: 3.53 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12129878398208735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12129878398208735 | validation: 0.22189626941319354]
	TIME [epoch: 3.53 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22794784210699262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22794784210699262 | validation: 0.12032803680917042]
	TIME [epoch: 3.52 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07563812694281205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07563812694281205 | validation: 0.08559255760584406]
	TIME [epoch: 3.54 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04162475065633645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04162475065633645 | validation: 0.06119293870917958]
	TIME [epoch: 3.53 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05570906463876778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05570906463876778 | validation: 0.0917127132025728]
	TIME [epoch: 3.54 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03777553112906406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03777553112906406 | validation: 0.06951941946977376]
	TIME [epoch: 3.54 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02668137948530678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02668137948530678 | validation: 0.06933055270010015]
	TIME [epoch: 3.54 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02146613143812692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02146613143812692 | validation: 0.06388643695317567]
	TIME [epoch: 3.54 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021668010027889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021668010027889 | validation: 0.058152470315407635]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_726.pth
	Model improved!!!
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02488530510674185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02488530510674185 | validation: 0.09847109430378954]
	TIME [epoch: 3.52 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03932209103330835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03932209103330835 | validation: 0.1559736596408753]
	TIME [epoch: 3.52 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08437736452466948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08437736452466948 | validation: 0.14782730121738108]
	TIME [epoch: 3.52 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10826266972444955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10826266972444955 | validation: 0.1373973409877416]
	TIME [epoch: 3.52 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12470040470762016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12470040470762016 | validation: 0.09147245171279307]
	TIME [epoch: 3.52 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09599762761718017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09599762761718017 | validation: 0.09890978220161231]
	TIME [epoch: 3.52 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048669434217667985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048669434217667985 | validation: 0.123677955355805]
	TIME [epoch: 3.52 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07980043165172795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07980043165172795 | validation: 0.07379323519618469]
	TIME [epoch: 3.52 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06772672987863332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06772672987863332 | validation: 0.11146112204839503]
	TIME [epoch: 3.52 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046966005839797015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046966005839797015 | validation: 0.09461611686909557]
	TIME [epoch: 3.52 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05412004801839416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05412004801839416 | validation: 0.10426757374749036]
	TIME [epoch: 3.53 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058634132147951185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058634132147951185 | validation: 0.09643237778690296]
	TIME [epoch: 3.53 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06415514386612228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06415514386612228 | validation: 0.11467298499557801]
	TIME [epoch: 3.52 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06629183605520346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06629183605520346 | validation: 0.10786028940174922]
	TIME [epoch: 3.53 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059204360119744746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059204360119744746 | validation: 0.08939763913035241]
	TIME [epoch: 3.54 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054638678149221465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054638678149221465 | validation: 0.10247958228673229]
	TIME [epoch: 3.54 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045839808418059215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045839808418059215 | validation: 0.07152098785165498]
	TIME [epoch: 3.53 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047204117394168146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047204117394168146 | validation: 0.08569984432945132]
	TIME [epoch: 3.54 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04008439473850617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04008439473850617 | validation: 0.06712291820913054]
	TIME [epoch: 3.54 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03718021872445304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03718021872445304 | validation: 0.08622513608760421]
	TIME [epoch: 3.54 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03351876974847246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03351876974847246 | validation: 0.06367299567558297]
	TIME [epoch: 3.53 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027683422540278883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027683422540278883 | validation: 0.09648185141844268]
	TIME [epoch: 3.54 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037173423284998654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037173423284998654 | validation: 0.07159064617816295]
	TIME [epoch: 3.54 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05349696759177133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05349696759177133 | validation: 0.12173808260508046]
	TIME [epoch: 3.55 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060832927315688005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060832927315688005 | validation: 0.07999369727671582]
	TIME [epoch: 3.54 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04822868299590057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04822868299590057 | validation: 0.12398890914167289]
	TIME [epoch: 3.54 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06230189587211133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06230189587211133 | validation: 0.14346015167610232]
	TIME [epoch: 3.52 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09360352773568924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09360352773568924 | validation: 0.08137013772756374]
	TIME [epoch: 3.54 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06562561871691071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06562561871691071 | validation: 0.09978857819450532]
	TIME [epoch: 3.53 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05883948896213445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05883948896213445 | validation: 0.21099987891402822]
	TIME [epoch: 3.53 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11323738168183006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11323738168183006 | validation: 0.12441437166602479]
	TIME [epoch: 3.54 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08780280906740867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08780280906740867 | validation: 0.1291534921790479]
	TIME [epoch: 3.54 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06853443537075668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06853443537075668 | validation: 0.08104960236208106]
	TIME [epoch: 3.54 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04121211949458282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04121211949458282 | validation: 0.07549225418540867]
	TIME [epoch: 3.54 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028198715692073863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028198715692073863 | validation: 0.06650840455944161]
	TIME [epoch: 3.55 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024095917288971314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024095917288971314 | validation: 0.08560673233835783]
	TIME [epoch: 3.55 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02569114352790112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02569114352790112 | validation: 0.062074552460141964]
	TIME [epoch: 3.55 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03525593010815146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03525593010815146 | validation: 0.08765346787893524]
	TIME [epoch: 3.57 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04135964366104352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04135964366104352 | validation: 0.08284560385779308]
	TIME [epoch: 3.55 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0635252612263868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0635252612263868 | validation: 0.14617836032620912]
	TIME [epoch: 3.52 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10171427486085946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10171427486085946 | validation: 0.22828732043677147]
	TIME [epoch: 3.55 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16459080490490188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16459080490490188 | validation: 0.15646305667487406]
	TIME [epoch: 3.55 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13045535676319897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13045535676319897 | validation: 0.10773628875400383]
	TIME [epoch: 3.55 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04389735862335114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04389735862335114 | validation: 0.1240530583638177]
	TIME [epoch: 3.56 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04896727891223455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04896727891223455 | validation: 0.07614840714145038]
	TIME [epoch: 3.54 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038923054747618306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038923054747618306 | validation: 0.06190036098997799]
	TIME [epoch: 3.52 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026706699146662353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026706699146662353 | validation: 0.06581516408374638]
	TIME [epoch: 3.54 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02550333836278416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02550333836278416 | validation: 0.07076579290800682]
	TIME [epoch: 3.53 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027711196381004274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027711196381004274 | validation: 0.10950714725477051]
	TIME [epoch: 3.53 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04932125980743942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04932125980743942 | validation: 0.09834584206304071]
	TIME [epoch: 3.54 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06193388810049608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06193388810049608 | validation: 0.11738431590108099]
	TIME [epoch: 3.53 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07747796006193594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07747796006193594 | validation: 0.08793118918609591]
	TIME [epoch: 3.53 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09317217210784094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09317217210784094 | validation: 0.1249107067719262]
	TIME [epoch: 3.52 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08045407596212012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08045407596212012 | validation: 0.17200538281355426]
	TIME [epoch: 3.55 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07954872834072027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07954872834072027 | validation: 0.0793331612701646]
	TIME [epoch: 3.53 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03905149610747169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03905149610747169 | validation: 0.06045855805872801]
	TIME [epoch: 3.55 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019842947361555785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019842947361555785 | validation: 0.06408785559204548]
	TIME [epoch: 3.53 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02408764390408922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02408764390408922 | validation: 0.07997573005463919]
	TIME [epoch: 3.54 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025332807326957224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025332807326957224 | validation: 0.06746695069978907]
	TIME [epoch: 3.54 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034869096696339266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034869096696339266 | validation: 0.1165293512443411]
	TIME [epoch: 3.54 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056726109574993425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056726109574993425 | validation: 0.1220312077499379]
	TIME [epoch: 3.53 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0882589778305232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0882589778305232 | validation: 0.09569637112181648]
	TIME [epoch: 3.55 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053518118644333194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053518118644333194 | validation: 0.08412554944445759]
	TIME [epoch: 3.54 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041806686480596385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041806686480596385 | validation: 0.11450222445074548]
	TIME [epoch: 3.54 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07951860023720854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07951860023720854 | validation: 0.15953886713632032]
	TIME [epoch: 3.55 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16064500935124948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16064500935124948 | validation: 0.18627660919827194]
	TIME [epoch: 3.53 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09661950363816253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09661950363816253 | validation: 0.08210298305532382]
	TIME [epoch: 3.53 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03692341723323206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03692341723323206 | validation: 0.05063891755637334]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_794.pth
	Model improved!!!
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04161214692500943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04161214692500943 | validation: 0.09595322380564592]
	TIME [epoch: 3.53 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04551599385080741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04551599385080741 | validation: 0.06953330467506612]
	TIME [epoch: 3.54 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03731226409597443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03731226409597443 | validation: 0.07386765374232916]
	TIME [epoch: 3.54 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029789950595994492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029789950595994492 | validation: 0.06830092560418953]
	TIME [epoch: 3.55 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028337474031591172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028337474031591172 | validation: 0.07942783875671866]
	TIME [epoch: 3.54 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04999072597956461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04999072597956461 | validation: 0.12340164162764222]
	TIME [epoch: 3.54 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08320426975828442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08320426975828442 | validation: 0.15297753890580137]
	TIME [epoch: 3.52 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10477845802723561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10477845802723561 | validation: 0.10105022902247661]
	TIME [epoch: 3.53 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056655178774811514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056655178774811514 | validation: 0.08969154765847477]
	TIME [epoch: 3.53 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03665774881099691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03665774881099691 | validation: 0.04455415749519579]
	TIME [epoch: 3.54 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_804.pth
	Model improved!!!
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03015872893175633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03015872893175633 | validation: 0.0802289907951776]
	TIME [epoch: 3.52 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02485670712432974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02485670712432974 | validation: 0.054063516463442846]
	TIME [epoch: 3.53 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028360218678365703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028360218678365703 | validation: 0.10123461012187973]
	TIME [epoch: 3.53 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03750582462694899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03750582462694899 | validation: 0.08235045109609768]
	TIME [epoch: 3.53 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06630799685055415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06630799685055415 | validation: 0.1530737952722095]
	TIME [epoch: 3.52 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08056108157095838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08056108157095838 | validation: 0.13316606802408534]
	TIME [epoch: 3.54 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09066532355821101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09066532355821101 | validation: 0.09356143413840093]
	TIME [epoch: 3.53 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08349220517332614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08349220517332614 | validation: 0.1312018340751025]
	TIME [epoch: 3.52 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06409599288020401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06409599288020401 | validation: 0.07561750843712498]
	TIME [epoch: 3.52 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041442392785916445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041442392785916445 | validation: 0.07260711921575666]
	TIME [epoch: 3.53 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025616371198145076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025616371198145076 | validation: 0.059068317885644296]
	TIME [epoch: 3.53 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025458783196145137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025458783196145137 | validation: 0.09984410464811934]
	TIME [epoch: 3.54 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042763628336563254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042763628336563254 | validation: 0.153866332402915]
	TIME [epoch: 3.54 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09070473381332449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09070473381332449 | validation: 0.12304727687266409]
	TIME [epoch: 3.53 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07584615304537089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07584615304537089 | validation: 0.08689424867695321]
	TIME [epoch: 3.52 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05277501372876033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05277501372876033 | validation: 0.0671156776501533]
	TIME [epoch: 3.53 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05716612430724027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05716612430724027 | validation: 0.10743111486977451]
	TIME [epoch: 3.52 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06358406655369701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06358406655369701 | validation: 0.06181249874910564]
	TIME [epoch: 3.52 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04971445983062788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04971445983062788 | validation: 0.06665884958912811]
	TIME [epoch: 3.52 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027454294482611385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027454294482611385 | validation: 0.07202844737006614]
	TIME [epoch: 3.53 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028508437885508144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028508437885508144 | validation: 0.08178418545450324]
	TIME [epoch: 3.52 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04007326311426068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04007326311426068 | validation: 0.12434007453038945]
	TIME [epoch: 3.53 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05975199007942659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05975199007942659 | validation: 0.07582095179433102]
	TIME [epoch: 3.53 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048979374812202325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048979374812202325 | validation: 0.10252585888118255]
	TIME [epoch: 3.53 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04322622007708993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04322622007708993 | validation: 0.06504007248047484]
	TIME [epoch: 3.55 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06397260600952556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06397260600952556 | validation: 0.07918673916358174]
	TIME [epoch: 3.54 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08367664964256286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08367664964256286 | validation: 0.1138827899985428]
	TIME [epoch: 3.52 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06111544600843592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06111544600843592 | validation: 0.09474466475386618]
	TIME [epoch: 3.52 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044967933894084204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044967933894084204 | validation: 0.06156958803975415]
	TIME [epoch: 3.52 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029057569773508866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029057569773508866 | validation: 0.07460685391956752]
	TIME [epoch: 3.53 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03641234068148893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03641234068148893 | validation: 0.057683940037198095]
	TIME [epoch: 3.53 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03438668543910249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03438668543910249 | validation: 0.06907349487462781]
	TIME [epoch: 3.53 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023999007565927554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023999007565927554 | validation: 0.0573294561764267]
	TIME [epoch: 3.52 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026439852178514123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026439852178514123 | validation: 0.10594228188676046]
	TIME [epoch: 3.52 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0519657962502993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0519657962502993 | validation: 0.12728235712959557]
	TIME [epoch: 3.52 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09626224563648272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09626224563648272 | validation: 0.14753022462081983]
	TIME [epoch: 3.53 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11452518958755033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11452518958755033 | validation: 0.14587708416053116]
	TIME [epoch: 3.52 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10662541279028406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10662541279028406 | validation: 0.13670923547289013]
	TIME [epoch: 3.53 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10947047052767764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10947047052767764 | validation: 0.09986167849923859]
	TIME [epoch: 3.53 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038231736618806716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038231736618806716 | validation: 0.0770872649077788]
	TIME [epoch: 3.53 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03371579384156189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03371579384156189 | validation: 0.06606120971793067]
	TIME [epoch: 3.52 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042049092545301744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042049092545301744 | validation: 0.07775663365759594]
	TIME [epoch: 3.52 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03306645721871237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03306645721871237 | validation: 0.06335081870150307]
	TIME [epoch: 3.53 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03187868519679085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03187868519679085 | validation: 0.07715241752082944]
	TIME [epoch: 3.52 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042576414679345885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042576414679345885 | validation: 0.0906709014904134]
	TIME [epoch: 3.52 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0515954605121001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0515954605121001 | validation: 0.0844567935160928]
	TIME [epoch: 3.52 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05978358878555375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05978358878555375 | validation: 0.10339634465768095]
	TIME [epoch: 3.52 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051703358256911545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051703358256911545 | validation: 0.07400336917177157]
	TIME [epoch: 3.52 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06282701057196309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06282701057196309 | validation: 0.07414844878360702]
	TIME [epoch: 3.52 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058983071540273685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058983071540273685 | validation: 0.1023704840542356]
	TIME [epoch: 3.52 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053275887634155125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053275887634155125 | validation: 0.16960730195504967]
	TIME [epoch: 3.52 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07712388338911845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07712388338911845 | validation: 0.13485084646829223]
	TIME [epoch: 3.53 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06425218191374395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06425218191374395 | validation: 0.06602421915105822]
	TIME [epoch: 3.52 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03370618994511001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03370618994511001 | validation: 0.06374140709231253]
	TIME [epoch: 3.52 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030725824222232703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030725824222232703 | validation: 0.05356830538341502]
	TIME [epoch: 3.52 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019936970436926355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019936970436926355 | validation: 0.0427862479943399]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_860.pth
	Model improved!!!
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01794860205305295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01794860205305295 | validation: 0.07206803076305726]
	TIME [epoch: 3.52 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02769784502273133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02769784502273133 | validation: 0.06476328081459365]
	TIME [epoch: 3.52 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060241886632708345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060241886632708345 | validation: 0.1476631938599738]
	TIME [epoch: 3.52 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07708544549299721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07708544549299721 | validation: 0.05896013217966517]
	TIME [epoch: 3.52 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030955855383012425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030955855383012425 | validation: 0.04821105758302307]
	TIME [epoch: 3.52 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02132882274618484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02132882274618484 | validation: 0.09920618822223175]
	TIME [epoch: 3.52 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06002563610493139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06002563610493139 | validation: 0.14552422424776423]
	TIME [epoch: 3.51 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15541364394348034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15541364394348034 | validation: 0.20554545790202]
	TIME [epoch: 3.53 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13333226489008979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13333226489008979 | validation: 0.13001948746415223]
	TIME [epoch: 3.52 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06739098041137984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06739098041137984 | validation: 0.061056304161459254]
	TIME [epoch: 3.53 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04896109993611031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04896109993611031 | validation: 0.08888284013673606]
	TIME [epoch: 3.51 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045481692179948546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045481692179948546 | validation: 0.08446457808472031]
	TIME [epoch: 3.52 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03435771097306074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03435771097306074 | validation: 0.05213889543177237]
	TIME [epoch: 3.52 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022811978308555397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022811978308555397 | validation: 0.05545418075619224]
	TIME [epoch: 3.52 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015625507544274286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015625507544274286 | validation: 0.045690022209899664]
	TIME [epoch: 3.52 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014668563956412646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014668563956412646 | validation: 0.04890906538664047]
	TIME [epoch: 3.53 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015239925486884704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015239925486884704 | validation: 0.039320616522521216]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_877.pth
	Model improved!!!
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025142987698126423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025142987698126423 | validation: 0.0935084962123397]
	TIME [epoch: 3.52 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04101152505994261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04101152505994261 | validation: 0.07171633676745628]
	TIME [epoch: 3.52 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05865847647962613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05865847647962613 | validation: 0.1577059610781916]
	TIME [epoch: 3.52 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08888137222113006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08888137222113006 | validation: 0.22138378801238187]
	TIME [epoch: 3.52 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1740749884754743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1740749884754743 | validation: 0.11838559068365112]
	TIME [epoch: 3.53 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09733121906719072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09733121906719072 | validation: 0.11559941461080303]
	TIME [epoch: 3.53 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04930064343676462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04930064343676462 | validation: 0.08291037340023079]
	TIME [epoch: 3.53 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037527135253214765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037527135253214765 | validation: 0.049073651558341175]
	TIME [epoch: 3.52 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02382500377724464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02382500377724464 | validation: 0.058865989809582346]
	TIME [epoch: 3.52 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01966488672318843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01966488672318843 | validation: 0.06526261923877062]
	TIME [epoch: 3.52 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02163827086436143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02163827086436143 | validation: 0.0630933530749431]
	TIME [epoch: 3.52 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025989647248897418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025989647248897418 | validation: 0.09476223210779797]
	TIME [epoch: 3.52 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03624702358709041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03624702358709041 | validation: 0.11687486003552301]
	TIME [epoch: 3.52 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06446359702517723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06446359702517723 | validation: 0.1237023226845367]
	TIME [epoch: 3.52 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07761050963567599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07761050963567599 | validation: 0.0889124559490334]
	TIME [epoch: 3.52 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10189047384993893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10189047384993893 | validation: 0.06129255807062394]
	TIME [epoch: 3.52 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0628361121396706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0628361121396706 | validation: 0.09540090704926146]
	TIME [epoch: 3.53 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041878284441338086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041878284441338086 | validation: 0.0498699478690747]
	TIME [epoch: 3.53 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032038796878921055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032038796878921055 | validation: 0.06785427903539741]
	TIME [epoch: 3.53 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02753309619552697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02753309619552697 | validation: 0.055498563047507644]
	TIME [epoch: 3.53 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01848981645717586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01848981645717586 | validation: 0.04010885062639348]
	TIME [epoch: 3.53 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01627906717203008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01627906717203008 | validation: 0.05605412139311992]
	TIME [epoch: 3.52 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018081536441199805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018081536441199805 | validation: 0.0435515504816063]
	TIME [epoch: 3.52 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029275502739291442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029275502739291442 | validation: 0.11026243463918683]
	TIME [epoch: 3.51 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08065274286597006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08065274286597006 | validation: 0.09818038656632248]
	TIME [epoch: 3.52 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1077997906381173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1077997906381173 | validation: 0.10284839198586562]
	TIME [epoch: 3.53 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06446301375818826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06446301375818826 | validation: 0.16245594623077186]
	TIME [epoch: 3.52 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08365869392250795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08365869392250795 | validation: 0.1168448981963258]
	TIME [epoch: 3.52 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07226537451820575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07226537451820575 | validation: 0.07674403377485599]
	TIME [epoch: 3.52 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04810896927863555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04810896927863555 | validation: 0.07314242477146524]
	TIME [epoch: 3.52 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040238113788191206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040238113788191206 | validation: 0.06223240482354009]
	TIME [epoch: 3.53 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04143415727747283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04143415727747283 | validation: 0.09005006787026998]
	TIME [epoch: 3.53 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042260498315505046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042260498315505046 | validation: 0.053323534783680085]
	TIME [epoch: 3.52 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02845140451243287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02845140451243287 | validation: 0.06429181708028707]
	TIME [epoch: 3.52 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024689525375659917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024689525375659917 | validation: 0.09653090361016181]
	TIME [epoch: 3.53 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038335207103911306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038335207103911306 | validation: 0.09631319473953497]
	TIME [epoch: 3.53 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06462977008887894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06462977008887894 | validation: 0.14199323486155244]
	TIME [epoch: 3.52 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09159129076213138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09159129076213138 | validation: 0.10349713639236527]
	TIME [epoch: 3.52 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06546838904273997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06546838904273997 | validation: 0.06334380599465349]
	TIME [epoch: 3.52 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03886308698923421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03886308698923421 | validation: 0.09073932506730627]
	TIME [epoch: 3.52 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033589434134188204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033589434134188204 | validation: 0.07358768032030755]
	TIME [epoch: 3.52 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03858275727095964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03858275727095964 | validation: 0.08163953803209323]
	TIME [epoch: 3.52 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03961120630460747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03961120630460747 | validation: 0.0816720397513873]
	TIME [epoch: 3.52 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03699677016244032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03699677016244032 | validation: 0.06010276321811713]
	TIME [epoch: 3.52 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028250324999376117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028250324999376117 | validation: 0.07151737312540307]
	TIME [epoch: 3.54 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02492532643295287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02492532643295287 | validation: 0.05148712405990098]
	TIME [epoch: 3.52 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031588509079539175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031588509079539175 | validation: 0.1194328319083372]
	TIME [epoch: 3.52 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05116706194798007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05116706194798007 | validation: 0.08318518722159268]
	TIME [epoch: 3.52 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05990892754229793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05990892754229793 | validation: 0.09406609601427156]
	TIME [epoch: 3.52 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038293168168907425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038293168168907425 | validation: 0.07227803349180789]
	TIME [epoch: 3.52 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032276281765641644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032276281765641644 | validation: 0.08525364273817558]
	TIME [epoch: 3.52 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05615713692444241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05615713692444241 | validation: 0.1945868622455705]
	TIME [epoch: 3.52 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13689751046531753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13689751046531753 | validation: 0.16226240714822915]
	TIME [epoch: 3.52 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14044096771231337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14044096771231337 | validation: 0.06985839349556201]
	TIME [epoch: 3.52 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058384163657174304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058384163657174304 | validation: 0.08976594018196193]
	TIME [epoch: 3.52 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03364090635882693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03364090635882693 | validation: 0.07802835042413872]
	TIME [epoch: 3.52 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03734907737019557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03734907737019557 | validation: 0.04811179803883552]
	TIME [epoch: 3.51 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025220618791664373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025220618791664373 | validation: 0.0454060249045992]
	TIME [epoch: 3.53 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016692698866509398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016692698866509398 | validation: 0.03709523682128946]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_936.pth
	Model improved!!!
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017709902511251045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017709902511251045 | validation: 0.059483869798985844]
	TIME [epoch: 3.52 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022089914541482732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022089914541482732 | validation: 0.05815224681042234]
	TIME [epoch: 3.51 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03667738083867498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03667738083867498 | validation: 0.11407341666633922]
	TIME [epoch: 3.52 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06291505522970582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06291505522970582 | validation: 0.12744683633702913]
	TIME [epoch: 3.52 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08345125131507089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08345125131507089 | validation: 0.08783593075922075]
	TIME [epoch: 3.52 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050508561264318054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050508561264318054 | validation: 0.10773143584396311]
	TIME [epoch: 3.52 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07423810692086083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07423810692086083 | validation: 0.13223507520406927]
	TIME [epoch: 3.52 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0771555760286099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0771555760286099 | validation: 0.156499493933427]
	TIME [epoch: 3.51 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08047349869192742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08047349869192742 | validation: 0.14141216079994187]
	TIME [epoch: 3.51 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05655564196159584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05655564196159584 | validation: 0.06457314781724165]
	TIME [epoch: 3.51 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028246566785649387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028246566785649387 | validation: 0.07232046532217054]
	TIME [epoch: 3.52 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022175184582775814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022175184582775814 | validation: 0.061176938540034136]
	TIME [epoch: 3.52 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025103685158752387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025103685158752387 | validation: 0.0737856510513623]
	TIME [epoch: 3.53 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02648272304500806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02648272304500806 | validation: 0.06574496280128178]
	TIME [epoch: 3.51 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032369219100837975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032369219100837975 | validation: 0.08675956005036739]
	TIME [epoch: 3.52 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053616494493895675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053616494493895675 | validation: 0.10623913746959171]
	TIME [epoch: 3.51 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07972120686934779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07972120686934779 | validation: 0.11224117307582611]
	TIME [epoch: 3.52 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09294872795026056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09294872795026056 | validation: 0.11147810816712062]
	TIME [epoch: 3.51 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057008646196200385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057008646196200385 | validation: 0.05325778978764281]
	TIME [epoch: 3.52 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033400373011163775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033400373011163775 | validation: 0.09237733586969121]
	TIME [epoch: 3.51 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031456480017452666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031456480017452666 | validation: 0.06979747165989517]
	TIME [epoch: 3.52 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03205566133035703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03205566133035703 | validation: 0.06644216304984284]
	TIME [epoch: 3.52 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02390351273740029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02390351273740029 | validation: 0.04786609720175283]
	TIME [epoch: 3.51 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019633751462353556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019633751462353556 | validation: 0.05932917366193635]
	TIME [epoch: 3.52 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021802223951104472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021802223951104472 | validation: 0.07646442419832233]
	TIME [epoch: 3.53 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040549422284142544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040549422284142544 | validation: 0.13503186703668535]
	TIME [epoch: 3.53 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09353450937713177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09353450937713177 | validation: 0.16091935476536978]
	TIME [epoch: 3.53 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09727969529500832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09727969529500832 | validation: 0.053807506044889875]
	TIME [epoch: 3.51 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03549411295124933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03549411295124933 | validation: 0.05596581250652869]
	TIME [epoch: 3.52 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018584843291578373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018584843291578373 | validation: 0.0482445680414474]
	TIME [epoch: 3.52 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025090152248793737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025090152248793737 | validation: 0.07710237429324196]
	TIME [epoch: 3.52 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03866229158088607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03866229158088607 | validation: 0.05517483349918223]
	TIME [epoch: 3.53 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054782271829341866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054782271829341866 | validation: 0.1130673449849533]
	TIME [epoch: 3.52 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0527479621023914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0527479621023914 | validation: 0.05570623162869374]
	TIME [epoch: 3.52 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03703659379781751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03703659379781751 | validation: 0.0877028882581576]
	TIME [epoch: 3.52 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044090931220188985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044090931220188985 | validation: 0.13935541932395398]
	TIME [epoch: 3.52 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08762991095340933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08762991095340933 | validation: 0.08151761122683153]
	TIME [epoch: 3.52 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05041408415912433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05041408415912433 | validation: 0.08287449097364295]
	TIME [epoch: 3.52 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05669592917336012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05669592917336012 | validation: 0.09437868857014273]
	TIME [epoch: 3.52 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07684249365205269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07684249365205269 | validation: 0.08253109682188546]
	TIME [epoch: 3.54 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056070420222695753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056070420222695753 | validation: 0.10332019457917646]
	TIME [epoch: 3.53 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04190072121333881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04190072121333881 | validation: 0.059974853378503834]
	TIME [epoch: 3.53 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03418932510210782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03418932510210782 | validation: 0.055866349681728]
	TIME [epoch: 3.52 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019192061557847816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019192061557847816 | validation: 0.04125599449210322]
	TIME [epoch: 3.52 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014152780045626819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014152780045626819 | validation: 0.04236005474429884]
	TIME [epoch: 3.52 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01357985180390986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01357985180390986 | validation: 0.04679523217734375]
	TIME [epoch: 3.52 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016181062503928905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016181062503928905 | validation: 0.059128375761306876]
	TIME [epoch: 3.52 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024409556978009422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024409556978009422 | validation: 0.10195393872800007]
	TIME [epoch: 3.52 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06821075607935835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06821075607935835 | validation: 0.1595671899808863]
	TIME [epoch: 3.51 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12908406207742704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12908406207742704 | validation: 0.09830277037842988]
	TIME [epoch: 3.52 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10125218499328764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10125218499328764 | validation: 0.11239428792419873]
	TIME [epoch: 3.51 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055378459839170786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055378459839170786 | validation: 0.14957852835356356]
	TIME [epoch: 3.52 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061864150549084104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061864150549084104 | validation: 0.06639917304479406]
	TIME [epoch: 3.52 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031110764567429942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031110764567429942 | validation: 0.0612401658011355]
	TIME [epoch: 3.52 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01718264077076039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01718264077076039 | validation: 0.0590747431856459]
	TIME [epoch: 3.51 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016876815537818553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016876815537818553 | validation: 0.07134595499136075]
	TIME [epoch: 3.51 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018795423336986045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018795423336986045 | validation: 0.043497896654868]
	TIME [epoch: 3.51 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02616593119920654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02616593119920654 | validation: 0.09767592991947063]
	TIME [epoch: 3.52 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04139389113580794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04139389113580794 | validation: 0.06373643117323938]
	TIME [epoch: 3.52 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06187365784812934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06187365784812934 | validation: 0.10950839685318443]
	TIME [epoch: 3.52 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04724040962012775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04724040962012775 | validation: 0.05343257871890558]
	TIME [epoch: 3.52 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03438711088363412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03438711088363412 | validation: 0.06666794148118553]
	TIME [epoch: 3.52 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05152649807483902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05152649807483902 | validation: 0.13293575163498814]
	TIME [epoch: 3.52 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08658142828548869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08658142828548869 | validation: 0.18402885720111364]
	TIME [epoch: 3.51 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11801014485755384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11801014485755384 | validation: 0.10142490042493657]
	TIME [epoch: 47.1 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06300438410901643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06300438410901643 | validation: 0.07397134658844826]
	TIME [epoch: 7.62 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02359106186284373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02359106186284373 | validation: 0.07034349063645633]
	TIME [epoch: 7.62 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03382128819672663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03382128819672663 | validation: 0.07572149704262923]
	TIME [epoch: 7.62 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03214793323637609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03214793323637609 | validation: 0.046608969409477397]
	TIME [epoch: 7.62 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018582054770655214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018582054770655214 | validation: 0.07459095330799231]
	TIME [epoch: 7.61 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023172201509726695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023172201509726695 | validation: 0.04443024627700107]
	TIME [epoch: 7.61 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019068121492771612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019068121492771612 | validation: 0.05738912104280848]
	TIME [epoch: 7.61 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01496161268340785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01496161268340785 | validation: 0.05593231012667074]
	TIME [epoch: 7.62 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018151800658237852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018151800658237852 | validation: 0.07228833455125859]
	TIME [epoch: 7.62 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034590751515063224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034590751515063224 | validation: 0.07941043733094938]
	TIME [epoch: 7.62 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0651265570054868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0651265570054868 | validation: 0.1573379356402341]
	TIME [epoch: 7.61 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08551537808189888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08551537808189888 | validation: 0.0817420027992099]
	TIME [epoch: 7.62 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06031663752268719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06031663752268719 | validation: 0.07306309271984973]
	TIME [epoch: 7.61 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0503168783939293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0503168783939293 | validation: 0.10596706559086369]
	TIME [epoch: 7.64 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07308654315945441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07308654315945441 | validation: 0.09697039486035192]
	TIME [epoch: 7.62 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06560184869743145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06560184869743145 | validation: 0.08825294938260686]
	TIME [epoch: 7.63 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046508482308048056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046508482308048056 | validation: 0.06048420673986628]
	TIME [epoch: 7.61 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024293272467479476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024293272467479476 | validation: 0.043812479056902165]
	TIME [epoch: 7.62 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019232828444430333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019232828444430333 | validation: 0.07836875370902126]
	TIME [epoch: 7.62 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029705805218391293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029705805218391293 | validation: 0.07175545622405514]
	TIME [epoch: 7.63 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05658890871603939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05658890871603939 | validation: 0.11131492223494935]
	TIME [epoch: 7.62 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05350848499242318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05350848499242318 | validation: 0.08181707820624724]
	TIME [epoch: 7.64 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052469396642789205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052469396642789205 | validation: 0.10125759064082691]
	TIME [epoch: 7.63 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057830794891510245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057830794891510245 | validation: 0.1212904109290725]
	TIME [epoch: 7.62 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07176575704620755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07176575704620755 | validation: 0.06842564154923991]
	TIME [epoch: 7.61 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042773730237562174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042773730237562174 | validation: 0.06192460010671624]
	TIME [epoch: 7.62 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026222307419794755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026222307419794755 | validation: 0.058377309959551905]
	TIME [epoch: 7.61 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028544953864139515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028544953864139515 | validation: 0.06999244007801785]
	TIME [epoch: 7.62 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03221442882140844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03221442882140844 | validation: 0.0675887664338867]
	TIME [epoch: 7.63 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03648131751453889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03648131751453889 | validation: 0.08259056827955724]
	TIME [epoch: 7.62 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03584276287998954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03584276287998954 | validation: 0.06912953234465129]
	TIME [epoch: 7.61 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039711680240777585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039711680240777585 | validation: 0.09689152745216706]
	TIME [epoch: 7.61 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0457912175123316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0457912175123316 | validation: 0.05663583693663726]
	TIME [epoch: 7.6 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046235941633091804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046235941633091804 | validation: 0.07497168587020162]
	TIME [epoch: 7.61 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044492993405176706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044492993405176706 | validation: 0.08017749055593282]
	TIME [epoch: 7.61 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05045048117390765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05045048117390765 | validation: 0.07848748302480477]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_101447/states/model_phi1_4a_v_mmd1_1037.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3000.420 seconds.
