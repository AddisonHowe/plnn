Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 614967606

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.99638159481362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.99638159481362 | validation: 6.334481951492589]
	TIME [epoch: 43.5 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.818173156836661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.818173156836661 | validation: 6.399651497330558]
	TIME [epoch: 0.991 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.697927056960973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.697927056960973 | validation: 6.025790989980862]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.48074235321271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.48074235321271 | validation: 6.56347076023574]
	TIME [epoch: 0.936 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.902920421143611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.902920421143611 | validation: 5.155798382588056]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.311127842269134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.311127842269134 | validation: 4.323916023308076]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7962673283182324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7962673283182324 | validation: 5.3047493339965115]
	TIME [epoch: 0.934 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.108352732566472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.108352732566472 | validation: 3.464724929510089]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1315859038573493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1315859038573493 | validation: 3.450659835782819]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.130789756315417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.130789756315417 | validation: 2.9906480227348085]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0592206604127585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0592206604127585 | validation: 4.4121116958076305]
	TIME [epoch: 0.942 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.241707665386211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.241707665386211 | validation: 2.6232710481702823]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.502911572259051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.502911572259051 | validation: 2.3877214616094835]
	TIME [epoch: 0.932 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4977431262112684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4977431262112684 | validation: 1.9115065084447573]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1191976456578114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1191976456578114 | validation: 1.3869270536770488]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8090275114693979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8090275114693979 | validation: 1.3316301435089]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7600518520901798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7600518520901798 | validation: 1.1142613232634955]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7095195530089302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7095195530089302 | validation: 1.770720940002677]
	TIME [epoch: 0.943 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8698943674129043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8698943674129043 | validation: 2.1725759324767795]
	TIME [epoch: 0.938 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.273204521512666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.273204521512666 | validation: 1.530737286220716]
	TIME [epoch: 0.942 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5782918257679608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5782918257679608 | validation: 1.118177262952334]
	TIME [epoch: 0.936 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6318953641191543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6318953641191543 | validation: 0.9850593045145617]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.446038246709398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.446038246709398 | validation: 1.8132596232426381]
	TIME [epoch: 0.94 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.606698748822566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.606698748822566 | validation: 0.9863418846836383]
	TIME [epoch: 0.939 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4801611520206746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4801611520206746 | validation: 1.2972328285996946]
	TIME [epoch: 0.937 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.362258782005528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.362258782005528 | validation: 1.568707695374437]
	TIME [epoch: 0.936 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.534754858144698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.534754858144698 | validation: 1.4409317202485703]
	TIME [epoch: 0.94 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.453326699502227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.453326699502227 | validation: 0.8634278383950232]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.31814622582229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.31814622582229 | validation: 1.4219453439769145]
	TIME [epoch: 0.939 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4007407286236737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4007407286236737 | validation: 1.076818915256285]
	TIME [epoch: 0.939 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2166591637224675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2166591637224675 | validation: 0.7739041587685023]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20948418586561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.20948418586561 | validation: 1.1383018918041075]
	TIME [epoch: 0.935 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.139205394343374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.139205394343374 | validation: 0.8720984784758973]
	TIME [epoch: 0.934 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2958006582381325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2958006582381325 | validation: 1.068188986617069]
	TIME [epoch: 0.937 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1491056138015188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1491056138015188 | validation: 1.4297908946884113]
	TIME [epoch: 0.947 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3672005297889178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3672005297889178 | validation: 1.2471485908369355]
	TIME [epoch: 0.935 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1360669871979305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1360669871979305 | validation: 0.7912608755132994]
	TIME [epoch: 0.935 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.254053546500435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.254053546500435 | validation: 0.9327032876326795]
	TIME [epoch: 0.939 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0227828021638192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0227828021638192 | validation: 1.0510394240582037]
	TIME [epoch: 0.939 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2769526389128647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2769526389128647 | validation: 1.3260772347960685]
	TIME [epoch: 0.935 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2456230367654575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2456230367654575 | validation: 0.7240149788144095]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0785431156003003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0785431156003003 | validation: 0.7192339555850659]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9775797987627348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9775797987627348 | validation: 1.0382486964914062]
	TIME [epoch: 0.936 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.996778545435293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.996778545435293 | validation: 0.797466366838111]
	TIME [epoch: 0.935 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9440530997156537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9440530997156537 | validation: 0.792475200253603]
	TIME [epoch: 0.935 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.928194793256747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.928194793256747 | validation: 0.9529787315067051]
	TIME [epoch: 0.936 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.945660227367138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.945660227367138 | validation: 0.6733392571078615]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9888992741568763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9888992741568763 | validation: 0.9118351587629671]
	TIME [epoch: 0.939 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9358515966402341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9358515966402341 | validation: 0.9537280660518273]
	TIME [epoch: 0.935 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9341638898129734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9341638898129734 | validation: 0.6162453197496846]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9937279946075487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9937279946075487 | validation: 0.5615033483021618]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0586206438898569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0586206438898569 | validation: 0.8594478309202538]
	TIME [epoch: 0.937 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.922077933335718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.922077933335718 | validation: 0.8305352134578632]
	TIME [epoch: 0.938 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9161588548210284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9161588548210284 | validation: 0.7640162972286167]
	TIME [epoch: 0.937 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.923682462554003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.923682462554003 | validation: 0.7473437481444067]
	TIME [epoch: 0.942 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0187089190594998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0187089190594998 | validation: 0.9816123724526072]
	TIME [epoch: 0.931 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9869072171284446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9869072171284446 | validation: 0.5777012080298912]
	TIME [epoch: 0.937 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0047499641205693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0047499641205693 | validation: 0.683228633423964]
	TIME [epoch: 0.939 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8814782264621336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8814782264621336 | validation: 0.7411938661643417]
	TIME [epoch: 0.938 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8774621795547063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8774621795547063 | validation: 0.7596808740552771]
	TIME [epoch: 0.938 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8923693055741656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8923693055741656 | validation: 0.7861879347088925]
	TIME [epoch: 0.938 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8961819331106295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8961819331106295 | validation: 0.7323358881654185]
	TIME [epoch: 0.938 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9064154100038824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9064154100038824 | validation: 0.6914099524081874]
	TIME [epoch: 0.939 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.890975733463768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.890975733463768 | validation: 0.8330840251565512]
	TIME [epoch: 0.938 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8949127297680375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8949127297680375 | validation: 0.6144682124492666]
	TIME [epoch: 0.937 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9395626705712015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9395626705712015 | validation: 0.9754793300521728]
	TIME [epoch: 0.939 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9581005341353546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9581005341353546 | validation: 0.6081134070139202]
	TIME [epoch: 0.939 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9361425372520086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9361425372520086 | validation: 0.9503203409335973]
	TIME [epoch: 0.939 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9292415719402476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9292415719402476 | validation: 0.6140300955308957]
	TIME [epoch: 0.938 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8823856989893898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8823856989893898 | validation: 0.7869262992463245]
	TIME [epoch: 0.949 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.871484461333217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.871484461333217 | validation: 0.7881610342135873]
	TIME [epoch: 0.941 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8639465152115179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8639465152115179 | validation: 0.628903016456129]
	TIME [epoch: 0.933 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.866300048459785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.866300048459785 | validation: 1.000779458157593]
	TIME [epoch: 0.937 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9423298210969085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9423298210969085 | validation: 0.5951706618774245]
	TIME [epoch: 0.94 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.918945345443411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.918945345443411 | validation: 0.8540806054302466]
	TIME [epoch: 0.942 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8651287183198433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8651287183198433 | validation: 0.7361111728583977]
	TIME [epoch: 0.953 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8547495354596606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8547495354596606 | validation: 0.7052315985927082]
	TIME [epoch: 0.938 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8671590551647731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8671590551647731 | validation: 0.8111644264458736]
	TIME [epoch: 0.94 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.962993180705218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.962993180705218 | validation: 0.8217238850257866]
	TIME [epoch: 0.938 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9753167616544982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9753167616544982 | validation: 0.9038749266288589]
	TIME [epoch: 0.943 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9379962928197797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9379962928197797 | validation: 0.5889695855398283]
	TIME [epoch: 0.938 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8977350624764137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8977350624764137 | validation: 0.85185087483829]
	TIME [epoch: 0.939 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8979500331826017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8979500331826017 | validation: 0.706371573730436]
	TIME [epoch: 0.94 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.882429064116279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.882429064116279 | validation: 0.6721010567945526]
	TIME [epoch: 0.938 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8691630905162724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8691630905162724 | validation: 0.771820233238484]
	TIME [epoch: 0.938 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8582447329561335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8582447329561335 | validation: 0.6586659531554546]
	TIME [epoch: 0.938 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8498383521002021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8498383521002021 | validation: 0.543855645171207]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9084208600293749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9084208600293749 | validation: 0.8775326378036006]
	TIME [epoch: 0.937 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9044685225234631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9044685225234631 | validation: 0.6625091115319385]
	TIME [epoch: 0.933 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8632607374185227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8632607374185227 | validation: 0.6621452853588585]
	TIME [epoch: 0.937 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8537090326314454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8537090326314454 | validation: 0.9043701640502843]
	TIME [epoch: 0.937 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8957472304834593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8957472304834593 | validation: 0.6353528009643976]
	TIME [epoch: 0.937 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9050001801116234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9050001801116234 | validation: 0.7770374768248893]
	TIME [epoch: 0.937 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9020672735394004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9020672735394004 | validation: 0.7808147393577458]
	TIME [epoch: 0.938 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831700297896333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8831700297896333 | validation: 0.5429071460209565]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9200848688258958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9200848688258958 | validation: 0.8239715854143717]
	TIME [epoch: 0.936 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8728984050607218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8728984050607218 | validation: 0.6462330714831584]
	TIME [epoch: 0.935 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8436559897047746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8436559897047746 | validation: 0.6296500448515926]
	TIME [epoch: 0.936 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8356585763647622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8356585763647622 | validation: 0.83921519812976]
	TIME [epoch: 0.935 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8730958901908128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8730958901908128 | validation: 0.561150751486311]
	TIME [epoch: 0.935 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8947152852003138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8947152852003138 | validation: 0.7968325055598351]
	TIME [epoch: 0.933 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8496501747254734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8496501747254734 | validation: 0.6839078222693307]
	TIME [epoch: 0.931 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8304822557965359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8304822557965359 | validation: 0.6489694369572473]
	TIME [epoch: 0.931 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580672937484706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8580672937484706 | validation: 0.8199405454429141]
	TIME [epoch: 0.936 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9134361314332654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9134361314332654 | validation: 0.9181774553578306]
	TIME [epoch: 0.939 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9856330366162291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9856330366162291 | validation: 0.6746545758764518]
	TIME [epoch: 0.934 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775696109904388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8775696109904388 | validation: 0.7689896578961584]
	TIME [epoch: 0.937 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8494325310886066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8494325310886066 | validation: 0.62002909763515]
	TIME [epoch: 0.934 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831009334432826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.831009334432826 | validation: 0.7412934121804697]
	TIME [epoch: 0.937 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8269089196990461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8269089196990461 | validation: 0.6437329749445048]
	TIME [epoch: 0.935 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8244307796461587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8244307796461587 | validation: 0.730392933155295]
	TIME [epoch: 0.935 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8371179211709476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8371179211709476 | validation: 0.6107767956857568]
	TIME [epoch: 0.935 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8690565801266648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8690565801266648 | validation: 0.7823820194808457]
	TIME [epoch: 0.936 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.882187964282543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.882187964282543 | validation: 0.7708192691355622]
	TIME [epoch: 0.953 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.909586241247722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.909586241247722 | validation: 0.640703313960501]
	TIME [epoch: 0.935 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8806393044395275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8806393044395275 | validation: 0.9069421641848079]
	TIME [epoch: 0.936 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8823414804140981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8823414804140981 | validation: 0.5925564515657765]
	TIME [epoch: 0.94 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8424998640241529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8424998640241529 | validation: 0.6792803049742637]
	TIME [epoch: 0.943 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8075750886126366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8075750886126366 | validation: 0.7019342579825659]
	TIME [epoch: 0.937 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8144074904680874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8144074904680874 | validation: 0.6399148355555782]
	TIME [epoch: 0.933 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8093721067615303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8093721067615303 | validation: 0.677530841784701]
	TIME [epoch: 0.939 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.811980714179345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.811980714179345 | validation: 0.7096344653893646]
	TIME [epoch: 0.938 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8339063769565521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8339063769565521 | validation: 0.7244259997765603]
	TIME [epoch: 0.932 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9876036586358464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9876036586358464 | validation: 0.7687890327906436]
	TIME [epoch: 0.937 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9012746048954179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9012746048954179 | validation: 0.9163935442181222]
	TIME [epoch: 0.937 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873939124932875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8873939124932875 | validation: 0.5641911481074375]
	TIME [epoch: 0.936 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8573559365136982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8573559365136982 | validation: 0.7420061717055086]
	TIME [epoch: 0.937 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8174008872500872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8174008872500872 | validation: 0.6256253623685133]
	TIME [epoch: 0.935 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.796556647839814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.796556647839814 | validation: 0.6469164386753149]
	TIME [epoch: 0.937 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8054152237796348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8054152237796348 | validation: 0.7666993906960111]
	TIME [epoch: 0.931 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8145771189746427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8145771189746427 | validation: 0.640726733723108]
	TIME [epoch: 0.936 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8498938784036097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8498938784036097 | validation: 0.793027760921706]
	TIME [epoch: 0.935 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8883829609801142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8883829609801142 | validation: 0.7694120018918226]
	TIME [epoch: 0.937 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9145083579615966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9145083579615966 | validation: 0.6361073173590215]
	TIME [epoch: 0.94 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888192140065485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8888192140065485 | validation: 0.6733885321032677]
	TIME [epoch: 0.936 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7973214092688153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7973214092688153 | validation: 0.6777585823543896]
	TIME [epoch: 0.94 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.796568948404976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.796568948404976 | validation: 0.6693491807834439]
	TIME [epoch: 0.933 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990285581912275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7990285581912275 | validation: 0.6995347944294115]
	TIME [epoch: 0.941 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8178543063004492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8178543063004492 | validation: 0.7286113067753172]
	TIME [epoch: 0.928 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.84993997556115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.84993997556115 | validation: 0.7358176610732585]
	TIME [epoch: 0.933 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8789117876494947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8789117876494947 | validation: 0.9433815319081239]
	TIME [epoch: 0.929 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.892910641714827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.892910641714827 | validation: 0.6220985457065678]
	TIME [epoch: 0.937 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8099743932290584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8099743932290584 | validation: 0.6647849831813465]
	TIME [epoch: 0.93 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7776123845887224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7776123845887224 | validation: 0.6176047837479293]
	TIME [epoch: 0.932 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7722034127245087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7722034127245087 | validation: 0.627006158031331]
	TIME [epoch: 0.932 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.767919390936953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.767919390936953 | validation: 0.6914130313171636]
	TIME [epoch: 0.941 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769582733195654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7769582733195654 | validation: 0.6119814013666804]
	TIME [epoch: 0.937 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8109049746314065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8109049746314065 | validation: 0.9534184468667342]
	TIME [epoch: 0.931 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9135027060259111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9135027060259111 | validation: 0.7579763408756579]
	TIME [epoch: 0.934 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9420602711510663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9420602711510663 | validation: 0.6418415655011391]
	TIME [epoch: 0.937 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8899494026005629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8899494026005629 | validation: 0.6727829645304058]
	TIME [epoch: 0.938 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7739946431271435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7739946431271435 | validation: 0.6521004792670767]
	TIME [epoch: 0.939 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7713607526904562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7713607526904562 | validation: 0.6547299417502672]
	TIME [epoch: 0.939 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895706125960126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7895706125960126 | validation: 0.6606915126468892]
	TIME [epoch: 0.943 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8108204675804906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8108204675804906 | validation: 0.7135254855914265]
	TIME [epoch: 0.953 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8294254981477295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8294254981477295 | validation: 0.656657947727828]
	TIME [epoch: 0.937 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8056574194482145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8056574194482145 | validation: 0.7462578646407544]
	TIME [epoch: 0.939 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8012887345446319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8012887345446319 | validation: 0.6025707655058534]
	TIME [epoch: 0.941 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856746252993759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7856746252993759 | validation: 0.7060038042382042]
	TIME [epoch: 0.934 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7820855872123224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7820855872123224 | validation: 0.6386535459790315]
	TIME [epoch: 0.939 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7743487750438734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7743487750438734 | validation: 0.6891144052015905]
	TIME [epoch: 0.936 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.814493762222389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.814493762222389 | validation: 0.7128049377581283]
	TIME [epoch: 0.939 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831291443020517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.831291443020517 | validation: 0.6583031253918364]
	TIME [epoch: 0.939 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8321785840159465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8321785840159465 | validation: 0.654492185191097]
	TIME [epoch: 0.936 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7707145755262781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7707145755262781 | validation: 0.6448932034287227]
	TIME [epoch: 0.936 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7544652186769071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7544652186769071 | validation: 0.6059880467125125]
	TIME [epoch: 0.941 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7555820437820253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7555820437820253 | validation: 0.7688811013866061]
	TIME [epoch: 0.936 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7823593002174987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7823593002174987 | validation: 0.6421761683791832]
	TIME [epoch: 0.934 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8247791986556452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8247791986556452 | validation: 0.8310220909673598]
	TIME [epoch: 0.93 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8224333669816238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8224333669816238 | validation: 0.6441509292967891]
	TIME [epoch: 0.931 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7753858644178098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7753858644178098 | validation: 0.5894769441285806]
	TIME [epoch: 0.931 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7973098077438191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7973098077438191 | validation: 0.7083299661917394]
	TIME [epoch: 0.931 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8160188553890757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8160188553890757 | validation: 0.6008003696209756]
	TIME [epoch: 0.931 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861894417035652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7861894417035652 | validation: 0.6257099921055151]
	TIME [epoch: 0.931 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524531653717023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7524531653717023 | validation: 0.676207834389908]
	TIME [epoch: 0.93 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7437319329618873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7437319329618873 | validation: 0.5808914080713036]
	TIME [epoch: 0.931 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7588139986910138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7588139986910138 | validation: 0.7628507246204794]
	TIME [epoch: 0.931 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7620187621968194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7620187621968194 | validation: 0.6318897477200385]
	TIME [epoch: 0.935 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7799350946027209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7799350946027209 | validation: 0.7056123837631303]
	TIME [epoch: 0.935 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069473205180012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8069473205180012 | validation: 0.6920939422338563]
	TIME [epoch: 0.939 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8079537582084452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8079537582084452 | validation: 0.6225883557432687]
	TIME [epoch: 0.933 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7652388838598065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7652388838598065 | validation: 0.6196829608539964]
	TIME [epoch: 0.937 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7343300934983334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7343300934983334 | validation: 0.6219255228204601]
	TIME [epoch: 0.935 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7186014507067091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7186014507067091 | validation: 0.5853610271668936]
	TIME [epoch: 0.935 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7221914687061676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7221914687061676 | validation: 0.7144124875733138]
	TIME [epoch: 0.934 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7324126687858373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7324126687858373 | validation: 0.6206743039863678]
	TIME [epoch: 0.939 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7815845299392925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7815845299392925 | validation: 0.8242517261053696]
	TIME [epoch: 0.939 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8322200196070699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8322200196070699 | validation: 0.6766035088474756]
	TIME [epoch: 0.936 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7909871825507953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7909871825507953 | validation: 0.5647724935306002]
	TIME [epoch: 0.933 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7642056953781511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7642056953781511 | validation: 0.6978666262468259]
	TIME [epoch: 0.935 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.738214041355468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.738214041355468 | validation: 0.5484586834919303]
	TIME [epoch: 0.935 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7221594594675466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7221594594675466 | validation: 0.6058848950888525]
	TIME [epoch: 0.935 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7070573322674705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7070573322674705 | validation: 0.5973815505495044]
	TIME [epoch: 0.937 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6993527008998536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6993527008998536 | validation: 0.5514583831083312]
	TIME [epoch: 0.944 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7109183169153088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7109183169153088 | validation: 0.662076422803396]
	TIME [epoch: 0.95 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7450901156454307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7450901156454307 | validation: 0.607238104116173]
	TIME [epoch: 0.928 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8181102208572492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8181102208572492 | validation: 0.7170478665336939]
	TIME [epoch: 0.931 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.805356425278053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.805356425278053 | validation: 0.8109643037571633]
	TIME [epoch: 0.94 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7902249306413103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7902249306413103 | validation: 0.5526692499006185]
	TIME [epoch: 0.935 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7422338944361017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7422338944361017 | validation: 0.6417967450149993]
	TIME [epoch: 0.934 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7012054083158015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7012054083158015 | validation: 0.5971414146384897]
	TIME [epoch: 42.8 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6889635200092437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6889635200092437 | validation: 0.5683942524026616]
	TIME [epoch: 2.48 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825951787264901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6825951787264901 | validation: 0.6013271239549248]
	TIME [epoch: 1.83 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685171248657415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.685171248657415 | validation: 0.5492585771368399]
	TIME [epoch: 1.83 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6951604544731305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6951604544731305 | validation: 0.7033083239752065]
	TIME [epoch: 1.84 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.706278167705955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.706278167705955 | validation: 0.6133878320482875]
	TIME [epoch: 1.82 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8014814656474718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8014814656474718 | validation: 0.8464693897396309]
	TIME [epoch: 1.85 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8495035588092862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8495035588092862 | validation: 0.6766165739303001]
	TIME [epoch: 1.83 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7605597027988598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7605597027988598 | validation: 0.4856696492525915]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7159079710845481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7159079710845481 | validation: 0.6537072457640842]
	TIME [epoch: 1.84 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6856219072138013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6856219072138013 | validation: 0.5910579963732372]
	TIME [epoch: 1.84 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6814442479985374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6814442479985374 | validation: 0.5610173357737904]
	TIME [epoch: 1.84 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869249835809224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6869249835809224 | validation: 0.6195869685598969]
	TIME [epoch: 1.83 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6901679802846108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6901679802846108 | validation: 0.5849402376038706]
	TIME [epoch: 1.84 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7059277843198717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7059277843198717 | validation: 0.6232258368937755]
	TIME [epoch: 1.84 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7314480248630706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7314480248630706 | validation: 0.6705540498899154]
	TIME [epoch: 1.83 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7262635507146298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7262635507146298 | validation: 0.5781074973600998]
	TIME [epoch: 1.82 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6916161165082942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6916161165082942 | validation: 0.6508698239533166]
	TIME [epoch: 1.82 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6672860695938008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6672860695938008 | validation: 0.5317635305417133]
	TIME [epoch: 1.84 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.669849546017407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.669849546017407 | validation: 0.7031899597322692]
	TIME [epoch: 1.84 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6907425917248904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6907425917248904 | validation: 0.4984275054681396]
	TIME [epoch: 1.83 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699967529764747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.699967529764747 | validation: 0.685795536225771]
	TIME [epoch: 1.82 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6817124247242662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6817124247242662 | validation: 0.5561306561254333]
	TIME [epoch: 1.84 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6624463467788394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6624463467788394 | validation: 0.5407604668275081]
	TIME [epoch: 1.82 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6790783203936528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6790783203936528 | validation: 0.6740864890873474]
	TIME [epoch: 1.82 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7084219452254559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7084219452254559 | validation: 0.5363334531879647]
	TIME [epoch: 1.82 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6911480565184271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6911480565184271 | validation: 0.5723491666291476]
	TIME [epoch: 1.84 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6424676835113037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6424676835113037 | validation: 0.6178357555738023]
	TIME [epoch: 1.84 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6450355827143508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6450355827143508 | validation: 0.47009997756946076]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6557785190960245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6557785190960245 | validation: 0.7422746195894168]
	TIME [epoch: 1.84 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6741427146973892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6741427146973892 | validation: 0.4951767877137144]
	TIME [epoch: 1.82 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6745512848755554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6745512848755554 | validation: 0.5970793105360478]
	TIME [epoch: 1.84 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6571204822874084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6571204822874084 | validation: 0.5729239857087877]
	TIME [epoch: 1.82 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6359983168079556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6359983168079556 | validation: 0.5124482521834424]
	TIME [epoch: 1.84 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6451940581220561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6451940581220561 | validation: 0.5671781679226021]
	TIME [epoch: 1.83 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239567746014453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6239567746014453 | validation: 0.5445620240882321]
	TIME [epoch: 1.83 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6102206918211031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6102206918211031 | validation: 0.4733546619130703]
	TIME [epoch: 1.83 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6361838983866777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6361838983866777 | validation: 0.7857431639517842]
	TIME [epoch: 1.84 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6988821091671747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6988821091671747 | validation: 0.4403052761790733]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6775806348456835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6775806348456835 | validation: 0.5738260912843185]
	TIME [epoch: 1.82 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6031100744638319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6031100744638319 | validation: 0.536931385013715]
	TIME [epoch: 1.81 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5817799640961923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5817799640961923 | validation: 0.4733639768541184]
	TIME [epoch: 1.83 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5906353568336501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5906353568336501 | validation: 0.6004496215541821]
	TIME [epoch: 1.83 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6136415471921851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6136415471921851 | validation: 0.44742087126139424]
	TIME [epoch: 1.81 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569289739689039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6569289739689039 | validation: 0.7137274659683551]
	TIME [epoch: 1.83 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6413732867029949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6413732867029949 | validation: 0.40506981204305526]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5919793296249845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5919793296249845 | validation: 0.5719740937279031]
	TIME [epoch: 1.83 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5774307998678624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5774307998678624 | validation: 0.4246539294831566]
	TIME [epoch: 1.83 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.585974442565188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.585974442565188 | validation: 0.5094537367178718]
	TIME [epoch: 1.83 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5856580980101093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5856580980101093 | validation: 0.5367751080554461]
	TIME [epoch: 1.83 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6256268625601932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6256268625601932 | validation: 0.46867277915842687]
	TIME [epoch: 1.83 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6455709646469282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6455709646469282 | validation: 0.5178835423877265]
	TIME [epoch: 1.84 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5653538101966763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5653538101966763 | validation: 0.4987688679053206]
	TIME [epoch: 1.82 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.55563484877324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.55563484877324 | validation: 0.4450393367832186]
	TIME [epoch: 1.84 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5881219109632226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5881219109632226 | validation: 0.5989810171499171]
	TIME [epoch: 1.83 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5978174738414453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5978174738414453 | validation: 0.37700878711699665]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5712070777159499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5712070777159499 | validation: 0.5989957698518132]
	TIME [epoch: 1.83 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5701998657035381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5701998657035381 | validation: 0.3454077145877706]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5562403691772841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5562403691772841 | validation: 0.5534744685867897]
	TIME [epoch: 1.83 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5386607587981165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5386607587981165 | validation: 0.4001170831684265]
	TIME [epoch: 1.82 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5106175896133719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5106175896133719 | validation: 0.451482061335277]
	TIME [epoch: 1.82 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4869725660994345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4869725660994345 | validation: 0.42228977584722077]
	TIME [epoch: 1.82 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4807368381494672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4807368381494672 | validation: 0.39889093467935033]
	TIME [epoch: 1.82 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47509933299708396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47509933299708396 | validation: 0.5061444796069925]
	TIME [epoch: 1.82 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.500166406004305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.500166406004305 | validation: 0.3905589067714935]
	TIME [epoch: 1.83 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.744107461858493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.744107461858493 | validation: 0.7175059509889277]
	TIME [epoch: 1.81 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346609531573734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6346609531573734 | validation: 0.721273514434352]
	TIME [epoch: 2.17 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9024553301551684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9024553301551684 | validation: 0.7003569628469083]
	TIME [epoch: 1.83 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8083739706874038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8083739706874038 | validation: 0.6673175867183966]
	TIME [epoch: 1.83 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6951021229437204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6951021229437204 | validation: 0.40387980336140494]
	TIME [epoch: 1.83 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5111608735196244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5111608735196244 | validation: 0.5042484259378274]
	TIME [epoch: 1.83 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5212191166371498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5212191166371498 | validation: 0.48619031754612196]
	TIME [epoch: 1.83 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.482602788141997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.482602788141997 | validation: 0.36805017261784806]
	TIME [epoch: 1.83 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4826405549654869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4826405549654869 | validation: 0.5381084562256213]
	TIME [epoch: 1.83 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4908895556001559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4908895556001559 | validation: 0.3569225136396263]
	TIME [epoch: 1.84 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5030769801771003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5030769801771003 | validation: 0.6804022768086273]
	TIME [epoch: 1.82 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5882294713727482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5882294713727482 | validation: 0.43856066582622183]
	TIME [epoch: 1.83 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5431299395801021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5431299395801021 | validation: 0.3783957128256852]
	TIME [epoch: 1.82 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4607738519781752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4607738519781752 | validation: 0.45718172735517726]
	TIME [epoch: 1.84 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43547219204143495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43547219204143495 | validation: 0.3495737176508005]
	TIME [epoch: 1.82 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4407488528833019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4407488528833019 | validation: 0.5337018694852523]
	TIME [epoch: 1.84 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4728175156592843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4728175156592843 | validation: 0.31055279623061793]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48585713468343467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48585713468343467 | validation: 0.6065566678595651]
	TIME [epoch: 1.84 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5022507843105524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5022507843105524 | validation: 0.4062747318832347]
	TIME [epoch: 1.83 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4628252840673851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4628252840673851 | validation: 0.43595292606725544]
	TIME [epoch: 1.84 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4592976704477417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4592976704477417 | validation: 0.4244522260966548]
	TIME [epoch: 1.83 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4633003287053134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4633003287053134 | validation: 0.4060847199343828]
	TIME [epoch: 1.83 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4117036031008422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4117036031008422 | validation: 0.3941951038731801]
	TIME [epoch: 1.83 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40444266728634914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40444266728634914 | validation: 0.3951322134604025]
	TIME [epoch: 1.83 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3722432205354353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3722432205354353 | validation: 0.3410456894043585]
	TIME [epoch: 1.83 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3859457667323687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3859457667323687 | validation: 0.48943737219250816]
	TIME [epoch: 1.83 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43439555350434267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43439555350434267 | validation: 0.38807844681153864]
	TIME [epoch: 1.83 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.614935892132338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.614935892132338 | validation: 0.4862145751149316]
	TIME [epoch: 1.84 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4072340647239865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4072340647239865 | validation: 0.40636998894710885]
	TIME [epoch: 1.83 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6338914406526908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6338914406526908 | validation: 0.4959992968543202]
	TIME [epoch: 1.83 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48090993154471817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48090993154471817 | validation: 0.5450136531508554]
	TIME [epoch: 1.8 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5949150546462832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5949150546462832 | validation: 0.41554564310396835]
	TIME [epoch: 1.81 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4407974524623917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4407974524623917 | validation: 0.5818635213670128]
	TIME [epoch: 1.81 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4918629759177021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4918629759177021 | validation: 0.3810505458225023]
	TIME [epoch: 1.83 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3870356748608289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3870356748608289 | validation: 0.38772392450640797]
	TIME [epoch: 1.84 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33405958241411504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33405958241411504 | validation: 0.4157832627275781]
	TIME [epoch: 1.84 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36415404935731355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36415404935731355 | validation: 0.37163067608683314]
	TIME [epoch: 1.81 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38424878950137087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38424878950137087 | validation: 0.41931915556294985]
	TIME [epoch: 1.82 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3534987951409781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3534987951409781 | validation: 0.36062206082876525]
	TIME [epoch: 1.8 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3462028783354114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3462028783354114 | validation: 0.4203377745671386]
	TIME [epoch: 1.83 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33656225289237257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33656225289237257 | validation: 0.30026215664813966]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4204870821771464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4204870821771464 | validation: 0.7255960916135624]
	TIME [epoch: 1.83 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5414511074952847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5414511074952847 | validation: 0.4134997618265572]
	TIME [epoch: 1.83 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46399409551117077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46399409551117077 | validation: 0.33361788215515187]
	TIME [epoch: 1.83 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4188898649030726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4188898649030726 | validation: 0.5182422472630229]
	TIME [epoch: 1.83 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36784490237256606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36784490237256606 | validation: 0.3714157992774245]
	TIME [epoch: 1.83 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2996437032743887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2996437032743887 | validation: 0.3387311115580343]
	TIME [epoch: 1.83 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28981625175979325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28981625175979325 | validation: 0.42633747562127405]
	TIME [epoch: 1.83 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2943267103575684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2943267103575684 | validation: 0.3252551070239109]
	TIME [epoch: 1.83 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29630360331750744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29630360331750744 | validation: 0.4289245065504981]
	TIME [epoch: 1.83 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3183151518671467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3183151518671467 | validation: 0.44196972412189955]
	TIME [epoch: 1.83 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5824581525819421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5824581525819421 | validation: 0.4488865354927778]
	TIME [epoch: 1.83 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3139800731687776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3139800731687776 | validation: 0.39841657827772803]
	TIME [epoch: 1.83 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47167063120955866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47167063120955866 | validation: 0.4241033971386385]
	TIME [epoch: 1.83 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36589104890653323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36589104890653323 | validation: 0.40465967692854027]
	TIME [epoch: 1.84 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2783484281779651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2783484281779651 | validation: 0.33702499586878293]
	TIME [epoch: 1.83 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26980232270664534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26980232270664534 | validation: 0.513396656904248]
	TIME [epoch: 1.85 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3426794034627273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3426794034627273 | validation: 0.3119766838250604]
	TIME [epoch: 1.84 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38454288384109936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38454288384109936 | validation: 0.3829937903355181]
	TIME [epoch: 1.84 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2684102849259435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2684102849259435 | validation: 0.39753085415376055]
	TIME [epoch: 1.84 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3008538819460741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3008538819460741 | validation: 0.33998748038621296]
	TIME [epoch: 1.83 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37262692972285244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37262692972285244 | validation: 0.43287668621265074]
	TIME [epoch: 1.83 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3351597665248524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3351597665248524 | validation: 0.5947030796117786]
	TIME [epoch: 1.82 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5204350205284757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5204350205284757 | validation: 0.3041276465897294]
	TIME [epoch: 1.83 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39098019773354237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39098019773354237 | validation: 0.428273827016886]
	TIME [epoch: 1.83 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.264712708783427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.264712708783427 | validation: 0.4553552836799128]
	TIME [epoch: 1.83 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3390677331339025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3390677331339025 | validation: 0.3604563148777601]
	TIME [epoch: 1.83 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.318578820506248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.318578820506248 | validation: 0.37429682856555235]
	TIME [epoch: 1.83 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23641041499499507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23641041499499507 | validation: 0.3606995818517542]
	TIME [epoch: 1.83 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2495896892589661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2495896892589661 | validation: 0.37076354886366386]
	TIME [epoch: 1.84 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26580311594485695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26580311594485695 | validation: 0.35564746902931993]
	TIME [epoch: 1.82 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26286864823100264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26286864823100264 | validation: 0.43149262452279563]
	TIME [epoch: 1.84 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3238341912276815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3238341912276815 | validation: 0.32642225805452113]
	TIME [epoch: 1.84 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27785279467522656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27785279467522656 | validation: 0.48492535003388976]
	TIME [epoch: 1.83 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2760131796395402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2760131796395402 | validation: 0.28827805084569846]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.253367538119667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.253367538119667 | validation: 0.4266026631727111]
	TIME [epoch: 1.83 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2404103622324814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2404103622324814 | validation: 0.3347622506577943]
	TIME [epoch: 1.82 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3414803019950719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3414803019950719 | validation: 0.5977392116508299]
	TIME [epoch: 1.83 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4041451959988404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4041451959988404 | validation: 0.35353762487539464]
	TIME [epoch: 1.82 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31311929455564724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31311929455564724 | validation: 0.2879195071922899]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2890143604588875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2890143604588875 | validation: 0.5399041192868215]
	TIME [epoch: 1.84 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3184607620474856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3184607620474856 | validation: 0.3091955864394844]
	TIME [epoch: 1.83 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.219427875372481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.219427875372481 | validation: 0.33122618653001656]
	TIME [epoch: 1.83 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20513429189923668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20513429189923668 | validation: 0.3583577935611449]
	TIME [epoch: 1.83 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23723663480421978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23723663480421978 | validation: 0.3614925189724203]
	TIME [epoch: 1.82 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32661705358584214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32661705358584214 | validation: 0.4110723014447472]
	TIME [epoch: 1.81 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2629978605675033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2629978605675033 | validation: 0.30847680796236177]
	TIME [epoch: 1.82 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23808824464475883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23808824464475883 | validation: 0.30576775854658844]
	TIME [epoch: 1.82 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19893843710988662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19893843710988662 | validation: 0.33900021172473505]
	TIME [epoch: 1.82 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18516820582465326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18516820582465326 | validation: 0.2859097913107857]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20675985727861232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20675985727861232 | validation: 0.5566332080250316]
	TIME [epoch: 1.83 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33044152165265517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33044152165265517 | validation: 0.3567546344804541]
	TIME [epoch: 1.83 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3474288802807041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3474288802807041 | validation: 0.30825573247888266]
	TIME [epoch: 1.83 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18857837229160992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18857837229160992 | validation: 0.4657912552196615]
	TIME [epoch: 1.83 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25482093883840196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25482093883840196 | validation: 0.3197689620255051]
	TIME [epoch: 1.83 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34072052128595837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34072052128595837 | validation: 0.3837950280012097]
	TIME [epoch: 1.83 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2197871745959501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2197871745959501 | validation: 0.3084878255549028]
	TIME [epoch: 1.83 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20861009809595493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20861009809595493 | validation: 0.3010902031896997]
	TIME [epoch: 1.83 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2094727190398208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2094727190398208 | validation: 0.3601684797345219]
	TIME [epoch: 1.83 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23203453021976855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23203453021976855 | validation: 0.3093042206308291]
	TIME [epoch: 1.83 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2303841537379805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2303841537379805 | validation: 0.37427290147622544]
	TIME [epoch: 1.84 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3100647326638876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3100647326638876 | validation: 0.3239756016309652]
	TIME [epoch: 1.84 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17403664828341697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17403664828341697 | validation: 0.28774707107279257]
	TIME [epoch: 1.84 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15056948548239316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15056948548239316 | validation: 0.23493262977352167]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1571618882429622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1571618882429622 | validation: 0.42346575310558504]
	TIME [epoch: 1.84 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.219156743044654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.219156743044654 | validation: 0.3618574154046332]
	TIME [epoch: 1.83 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38586483135059424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38586483135059424 | validation: 0.5626084553497739]
	TIME [epoch: 1.83 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3480184166479897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3480184166479897 | validation: 0.3195103476359484]
	TIME [epoch: 1.84 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19096452978485556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19096452978485556 | validation: 0.2597825863593425]
	TIME [epoch: 1.83 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24769147912263073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24769147912263073 | validation: 0.49997229188050957]
	TIME [epoch: 1.83 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2999537808912822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2999537808912822 | validation: 0.329708163407191]
	TIME [epoch: 1.83 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18830883575127708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18830883575127708 | validation: 0.24338206824242337]
	TIME [epoch: 1.83 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19509971774548734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19509971774548734 | validation: 0.34923995804709307]
	TIME [epoch: 1.84 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2014134023167695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2014134023167695 | validation: 0.2997732292255807]
	TIME [epoch: 1.83 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1859297364388219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1859297364388219 | validation: 0.28491228633529825]
	TIME [epoch: 1.83 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19616586314610715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19616586314610715 | validation: 0.405355053298558]
	TIME [epoch: 1.85 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22000877266970917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22000877266970917 | validation: 0.29480326773536664]
	TIME [epoch: 1.84 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2126649413938175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2126649413938175 | validation: 0.32930403455074764]
	TIME [epoch: 1.83 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26817703225758427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26817703225758427 | validation: 0.4192741115434875]
	TIME [epoch: 1.83 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38728376872741477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38728376872741477 | validation: 0.2464603940744818]
	TIME [epoch: 1.83 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24859806508935767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24859806508935767 | validation: 0.32470158045387293]
	TIME [epoch: 1.83 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1629576552521518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1629576552521518 | validation: 0.2837718587925671]
	TIME [epoch: 1.84 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16689598994780078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16689598994780078 | validation: 0.2815052518864019]
	TIME [epoch: 1.83 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1504733969956009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1504733969956009 | validation: 0.2773060642284683]
	TIME [epoch: 1.84 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17419231685955466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17419231685955466 | validation: 0.3335488491095153]
	TIME [epoch: 1.84 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1988073040338945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1988073040338945 | validation: 0.3513235432275579]
	TIME [epoch: 1.84 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2035944287620756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2035944287620756 | validation: 0.4172199963722742]
	TIME [epoch: 1.86 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19832623016104198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19832623016104198 | validation: 0.23727163668393045]
	TIME [epoch: 1.83 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1745269007415341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1745269007415341 | validation: 0.38480999689466633]
	TIME [epoch: 1.84 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18074120845698835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18074120845698835 | validation: 0.24402506897935347]
	TIME [epoch: 1.83 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18221127380471916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18221127380471916 | validation: 0.45411437215273714]
	TIME [epoch: 1.83 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2749909482726364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2749909482726364 | validation: 0.28951210911433684]
	TIME [epoch: 1.83 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24690715568766222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24690715568766222 | validation: 0.27843474480810004]
	TIME [epoch: 1.83 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2878890079998343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2878890079998343 | validation: 0.37532188001501743]
	TIME [epoch: 1.83 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17019745115010665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17019745115010665 | validation: 0.24874460259492726]
	TIME [epoch: 1.83 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1475562182907987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1475562182907987 | validation: 0.3059546445167671]
	TIME [epoch: 1.82 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14559323512620218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14559323512620218 | validation: 0.26523520583765264]
	TIME [epoch: 1.82 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1383892402548616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1383892402548616 | validation: 0.28552537618832385]
	TIME [epoch: 1.82 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13339022025011266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13339022025011266 | validation: 0.26312997071304456]
	TIME [epoch: 1.83 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13773623451670736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13773623451670736 | validation: 0.37788982248079295]
	TIME [epoch: 1.82 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1906308895938912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1906308895938912 | validation: 0.36538546107708647]
	TIME [epoch: 1.82 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2698994173252254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2698994173252254 | validation: 0.3509080467014436]
	TIME [epoch: 1.82 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2028799801224595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2028799801224595 | validation: 0.3484656503486767]
	TIME [epoch: 1.82 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1987554357478529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1987554357478529 | validation: 0.32533800205142005]
	TIME [epoch: 1.83 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30202508086473073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30202508086473073 | validation: 0.3715335989448907]
	TIME [epoch: 1.82 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31194238544374403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31194238544374403 | validation: 0.2722229139191578]
	TIME [epoch: 1.83 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12990519551119176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12990519551119176 | validation: 0.2552938781369069]
	TIME [epoch: 1.84 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.131247631857672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.131247631857672 | validation: 0.2720515616210832]
	TIME [epoch: 1.84 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14062137231435998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14062137231435998 | validation: 0.24029996816293095]
	TIME [epoch: 1.85 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11657567045639546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11657567045639546 | validation: 0.22614059155318667]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10512303433764933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10512303433764933 | validation: 0.24495605276688945]
	TIME [epoch: 1.83 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10391033168659551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10391033168659551 | validation: 0.2115200656121774]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_417.pth
	Model improved!!!
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12237852248476504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12237852248476504 | validation: 0.4595332050152534]
	TIME [epoch: 1.83 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24123676127174068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24123676127174068 | validation: 0.4201270482420718]
	TIME [epoch: 1.83 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.339794284746568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.339794284746568 | validation: 0.28581751605716177]
	TIME [epoch: 1.83 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12767751759882143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12767751759882143 | validation: 0.24714566935459065]
	TIME [epoch: 1.83 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10821383257841866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10821383257841866 | validation: 0.22200715784581143]
	TIME [epoch: 1.83 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12322338514045318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12322338514045318 | validation: 0.2906020848401904]
	TIME [epoch: 1.82 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15685833306741212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15685833306741212 | validation: 0.3859443031732876]
	TIME [epoch: 1.83 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29311843491971984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29311843491971984 | validation: 0.262617504791502]
	TIME [epoch: 1.83 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25168014898298974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25168014898298974 | validation: 0.36536676635719023]
	TIME [epoch: 1.82 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18227638759394946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18227638759394946 | validation: 0.29429377268341]
	TIME [epoch: 1.83 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22012342057174628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22012342057174628 | validation: 0.34760268259954763]
	TIME [epoch: 1.83 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23153236562478585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23153236562478585 | validation: 0.25380222694646226]
	TIME [epoch: 1.83 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13847656906611633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13847656906611633 | validation: 0.24089752959080754]
	TIME [epoch: 1.81 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10581874643948398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10581874643948398 | validation: 0.21785737889396578]
	TIME [epoch: 1.83 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10936724813669059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10936724813669059 | validation: 0.20872668251796003]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10924553802762882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10924553802762882 | validation: 0.2385742403751704]
	TIME [epoch: 1.83 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11342399643966411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11342399643966411 | validation: 0.24214276500948093]
	TIME [epoch: 1.82 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13908557181567094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13908557181567094 | validation: 0.35056903829191066]
	TIME [epoch: 1.81 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21494816037689465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21494816037689465 | validation: 0.37954609883189755]
	TIME [epoch: 1.82 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41377031804105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41377031804105 | validation: 0.21753544979906217]
	TIME [epoch: 1.82 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14786289878104064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14786289878104064 | validation: 0.2898093820077793]
	TIME [epoch: 1.82 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14539345279451682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14539345279451682 | validation: 0.2913079590409795]
	TIME [epoch: 1.82 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15506961409578282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15506961409578282 | validation: 0.24464029198400272]
	TIME [epoch: 1.82 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10594311000114903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10594311000114903 | validation: 0.18565781421524094]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15254911398287196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15254911398287196 | validation: 0.4586428931113633]
	TIME [epoch: 1.83 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27621644487143804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27621644487143804 | validation: 0.321183772498656]
	TIME [epoch: 1.82 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2710819488840102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2710819488840102 | validation: 0.28295641719522396]
	TIME [epoch: 1.83 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15637933734292891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15637933734292891 | validation: 0.28077466860461137]
	TIME [epoch: 1.82 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1673712981560427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1673712981560427 | validation: 0.21300960496617988]
	TIME [epoch: 1.82 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22865839598145388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22865839598145388 | validation: 0.23522280585405078]
	TIME [epoch: 1.82 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11748557166542885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11748557166542885 | validation: 0.23879203976696753]
	TIME [epoch: 1.82 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12862420544545347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12862420544545347 | validation: 0.23468867081428557]
	TIME [epoch: 1.82 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11577114486683644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11577114486683644 | validation: 0.19289768529588439]
	TIME [epoch: 1.81 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11367243605525568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11367243605525568 | validation: 0.27308594116181156]
	TIME [epoch: 1.82 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13213163710597367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13213163710597367 | validation: 0.20916877423722832]
	TIME [epoch: 1.82 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17216759112056848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17216759112056848 | validation: 0.33465933385396673]
	TIME [epoch: 1.82 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18150459347997192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18150459347997192 | validation: 0.2041865860094838]
	TIME [epoch: 1.83 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1465689813758163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1465689813758163 | validation: 0.23896646409893574]
	TIME [epoch: 1.8 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1010253208478753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1010253208478753 | validation: 0.2104432072694011]
	TIME [epoch: 1.82 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10800180739298648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10800180739298648 | validation: 0.23758383637151204]
	TIME [epoch: 1.82 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15266390783740316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15266390783740316 | validation: 0.3384901479226845]
	TIME [epoch: 1.83 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25659983968579003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25659983968579003 | validation: 0.49009037599210653]
	TIME [epoch: 1.84 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2822554069769231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2822554069769231 | validation: 0.23353885642296263]
	TIME [epoch: 1.83 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19715668348291338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19715668348291338 | validation: 0.2764579206431327]
	TIME [epoch: 1.83 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13969534161974756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13969534161974756 | validation: 0.2832870605387882]
	TIME [epoch: 1.83 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1705264963326236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1705264963326236 | validation: 0.27572941067337037]
	TIME [epoch: 1.84 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15652338891732692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15652338891732692 | validation: 0.26604041594947997]
	TIME [epoch: 1.83 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1173780605550827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1173780605550827 | validation: 0.20250214208442063]
	TIME [epoch: 1.83 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09886357480986992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09886357480986992 | validation: 0.21007103526614374]
	TIME [epoch: 1.83 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0957329090755253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0957329090755253 | validation: 0.1888638495854549]
	TIME [epoch: 1.83 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11371650140842203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11371650140842203 | validation: 0.24841366443299667]
	TIME [epoch: 1.82 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17618145104745406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17618145104745406 | validation: 0.3892365294949955]
	TIME [epoch: 1.82 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2906072879373629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2906072879373629 | validation: 0.2634351388058309]
	TIME [epoch: 1.83 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19666406270491163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19666406270491163 | validation: 0.34722083567965134]
	TIME [epoch: 1.82 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16270920980812342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16270920980812342 | validation: 0.23086451379035575]
	TIME [epoch: 1.82 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18494045677755497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18494045677755497 | validation: 0.2320379665136385]
	TIME [epoch: 1.83 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1188314221714942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1188314221714942 | validation: 0.2299669904224254]
	TIME [epoch: 1.83 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10435996631178647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10435996631178647 | validation: 0.2005254062210157]
	TIME [epoch: 1.83 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11946872470820896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11946872470820896 | validation: 0.25041567406905474]
	TIME [epoch: 1.83 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14280489144030337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14280489144030337 | validation: 0.27947477879922433]
	TIME [epoch: 1.83 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.150052516868894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.150052516868894 | validation: 0.2692162798814804]
	TIME [epoch: 1.83 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16257658265782554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16257658265782554 | validation: 0.3099715902690202]
	TIME [epoch: 1.83 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15071841681100423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15071841681100423 | validation: 0.18449040589607232]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14048053891885418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14048053891885418 | validation: 0.5448116129197911]
	TIME [epoch: 1.84 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35624002823003376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35624002823003376 | validation: 0.27568352237409555]
	TIME [epoch: 1.84 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20147018634952518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20147018634952518 | validation: 0.19642072074315642]
	TIME [epoch: 1.83 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15131154758644394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15131154758644394 | validation: 0.23964905801574565]
	TIME [epoch: 1.84 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1489911897968294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1489911897968294 | validation: 0.21429651891459162]
	TIME [epoch: 1.84 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10298594612194731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10298594612194731 | validation: 0.21873105966692563]
	TIME [epoch: 1.83 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11455469823030048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11455469823030048 | validation: 0.2361984420247074]
	TIME [epoch: 1.83 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11405551616171086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11405551616171086 | validation: 0.20793731977744115]
	TIME [epoch: 1.83 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1387927072119481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1387927072119481 | validation: 0.25827942268934895]
	TIME [epoch: 1.83 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21583159866705337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21583159866705337 | validation: 0.2704985528260377]
	TIME [epoch: 1.83 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14973581995943913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14973581995943913 | validation: 0.18324192114028823]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0987196585489695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0987196585489695 | validation: 0.18284620129254286]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1000279842636311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1000279842636311 | validation: 0.2223535842631398]
	TIME [epoch: 1.83 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12098908220537567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12098908220537567 | validation: 0.3321294532200342]
	TIME [epoch: 1.83 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20564870499305307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20564870499305307 | validation: 0.24941251347649374]
	TIME [epoch: 1.83 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15052950072145657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15052950072145657 | validation: 0.20921266156351545]
	TIME [epoch: 1.83 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12358844329031192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12358844329031192 | validation: 0.40311600199757136]
	TIME [epoch: 1.83 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1916874045785494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1916874045785494 | validation: 0.22122880477278084]
	TIME [epoch: 1.83 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14764209355961386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14764209355961386 | validation: 0.2307586285018962]
	TIME [epoch: 1.82 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1461864041311933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1461864041311933 | validation: 0.24657146836796717]
	TIME [epoch: 1.83 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14560858148252645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14560858148252645 | validation: 0.19796807509058334]
	TIME [epoch: 43.8 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10503944213755215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10503944213755215 | validation: 0.18850235367238805]
	TIME [epoch: 3.63 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08928167002273936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08928167002273936 | validation: 0.17481612220768383]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_503.pth
	Model improved!!!
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0798083729335033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0798083729335033 | validation: 0.17503851257218145]
	TIME [epoch: 3.61 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08270567525485416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08270567525485416 | validation: 0.22156429978012537]
	TIME [epoch: 3.57 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11539630006298786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11539630006298786 | validation: 0.30926928008954013]
	TIME [epoch: 3.63 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26274511766700887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26274511766700887 | validation: 0.48936486176648586]
	TIME [epoch: 3.62 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30694728464962767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30694728464962767 | validation: 0.26309067085838816]
	TIME [epoch: 3.61 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14980660886917502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14980660886917502 | validation: 0.27235488157186316]
	TIME [epoch: 3.61 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19865585081336337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19865585081336337 | validation: 0.27877319539070844]
	TIME [epoch: 3.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20018255802088242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20018255802088242 | validation: 0.2746844018168641]
	TIME [epoch: 3.59 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1431705497295926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1431705497295926 | validation: 0.18189952021555694]
	TIME [epoch: 3.58 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1189977158251745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1189977158251745 | validation: 0.22311847712741661]
	TIME [epoch: 3.58 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10210932837090482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10210932837090482 | validation: 0.17404510651457636]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08442054972473581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08442054972473581 | validation: 0.17024754885533927]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08298209528932793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08298209528932793 | validation: 0.17746369929651767]
	TIME [epoch: 3.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09764833844870126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09764833844870126 | validation: 0.20024974244628477]
	TIME [epoch: 3.62 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13841773686792333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13841773686792333 | validation: 0.2989618246005109]
	TIME [epoch: 3.58 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2295572544016438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2295572544016438 | validation: 0.27920302574860384]
	TIME [epoch: 3.59 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1649578410358642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1649578410358642 | validation: 0.1880606504942516]
	TIME [epoch: 3.59 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08887666853676277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08887666853676277 | validation: 0.15833420565942158]
	TIME [epoch: 3.58 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_521.pth
	Model improved!!!
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08709943934624274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08709943934624274 | validation: 0.21810176107488852]
	TIME [epoch: 3.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11160030495982627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11160030495982627 | validation: 0.1815619154886172]
	TIME [epoch: 3.58 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1495659315521661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1495659315521661 | validation: 0.3274907326892764]
	TIME [epoch: 3.57 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16446929723432974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16446929723432974 | validation: 0.19791925919860962]
	TIME [epoch: 3.58 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13562029937529022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13562029937529022 | validation: 0.2447834336510516]
	TIME [epoch: 3.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1711029066806769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1711029066806769 | validation: 0.2510016921987707]
	TIME [epoch: 3.58 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15312746663104704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15312746663104704 | validation: 0.17726051027184722]
	TIME [epoch: 3.57 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0916726282671212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0916726282671212 | validation: 0.15840084436748733]
	TIME [epoch: 3.57 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07599818664414457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07599818664414457 | validation: 0.16539214486492068]
	TIME [epoch: 3.56 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08296810081093677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08296810081093677 | validation: 0.18448539879218315]
	TIME [epoch: 3.58 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12239252311556603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12239252311556603 | validation: 0.2849040860092833]
	TIME [epoch: 3.65 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28422257568150244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28422257568150244 | validation: 0.44444989399361745]
	TIME [epoch: 3.55 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2414827784787074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2414827784787074 | validation: 0.19009439866694544]
	TIME [epoch: 3.55 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11779110525688197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11779110525688197 | validation: 0.25869931614140673]
	TIME [epoch: 3.58 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11479016954541814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11479016954541814 | validation: 0.19530912538765594]
	TIME [epoch: 3.61 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11270008083447518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11270008083447518 | validation: 0.21868786369540305]
	TIME [epoch: 3.61 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10118619285352642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10118619285352642 | validation: 0.17106616890900997]
	TIME [epoch: 3.59 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08526599254743708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08526599254743708 | validation: 0.1850187828578103]
	TIME [epoch: 3.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07570787549333775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07570787549333775 | validation: 0.16309145867627695]
	TIME [epoch: 3.61 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08558410056950937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08558410056950937 | validation: 0.21000120402523517]
	TIME [epoch: 3.58 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12642068337644113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12642068337644113 | validation: 0.34069128690116773]
	TIME [epoch: 3.57 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28492802211854346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28492802211854346 | validation: 0.40508138305038266]
	TIME [epoch: 3.58 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21590629044527399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21590629044527399 | validation: 0.21330314076188547]
	TIME [epoch: 3.58 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1149855844694233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1149855844694233 | validation: 0.16005552548767074]
	TIME [epoch: 3.63 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08223289471904514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08223289471904514 | validation: 0.18991976957054502]
	TIME [epoch: 3.61 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09470406641484315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09470406641484315 | validation: 0.18181867157472936]
	TIME [epoch: 3.59 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11173520757818076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11173520757818076 | validation: 0.24982542938040586]
	TIME [epoch: 3.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13261191208220818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13261191208220818 | validation: 0.1987166777597661]
	TIME [epoch: 3.59 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14357921879118796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14357921879118796 | validation: 0.2215481349326406]
	TIME [epoch: 3.59 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.138437365924058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.138437365924058 | validation: 0.173778435074979]
	TIME [epoch: 3.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1716947906767111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1716947906767111 | validation: 0.23801395412976403]
	TIME [epoch: 3.57 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1471446585720693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1471446585720693 | validation: 0.24800918339101266]
	TIME [epoch: 3.56 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12867737519278374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12867737519278374 | validation: 0.1720167872765322]
	TIME [epoch: 3.58 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10278822624779163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10278822624779163 | validation: 0.2033007695411609]
	TIME [epoch: 3.57 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0895803363754361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0895803363754361 | validation: 0.15484060315816278]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09357055000089905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09357055000089905 | validation: 0.3349053190583995]
	TIME [epoch: 3.62 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18456460817869316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18456460817869316 | validation: 0.23163557291423564]
	TIME [epoch: 3.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23619420942681685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23619420942681685 | validation: 0.19274810714753132]
	TIME [epoch: 3.61 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11508885123228005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11508885123228005 | validation: 0.22163549600054094]
	TIME [epoch: 3.59 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09797455327597629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09797455327597629 | validation: 0.22410760461064294]
	TIME [epoch: 3.56 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11663139636519945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11663139636519945 | validation: 0.23520390659736068]
	TIME [epoch: 3.57 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10172812260260608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10172812260260608 | validation: 0.18017847247827057]
	TIME [epoch: 3.57 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09582523331569961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09582523331569961 | validation: 0.22163233425359127]
	TIME [epoch: 3.56 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14011504962651938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14011504962651938 | validation: 0.24228533193861768]
	TIME [epoch: 3.59 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1781999586477772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1781999586477772 | validation: 0.20938365184691576]
	TIME [epoch: 3.57 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1225803146528345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1225803146528345 | validation: 0.17495885086986931]
	TIME [epoch: 3.56 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08883478894361246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08883478894361246 | validation: 0.1758936737210727]
	TIME [epoch: 3.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08235802891637736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08235802891637736 | validation: 0.17601193427385553]
	TIME [epoch: 3.61 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09389579253085316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09389579253085316 | validation: 0.26898684483425506]
	TIME [epoch: 3.61 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1286266147605907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1286266147605907 | validation: 0.23153789563508287]
	TIME [epoch: 3.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26847212011254007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26847212011254007 | validation: 0.3215117460830786]
	TIME [epoch: 3.59 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14831603890909972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14831603890909972 | validation: 0.25359617580223537]
	TIME [epoch: 3.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12772058293006977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12772058293006977 | validation: 0.2240107273776682]
	TIME [epoch: 3.59 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13522232203444856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13522232203444856 | validation: 0.18157980791474376]
	TIME [epoch: 3.58 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10039701092463227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10039701092463227 | validation: 0.19598023457903613]
	TIME [epoch: 3.59 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08066498967060146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08066498967060146 | validation: 0.16149816832184635]
	TIME [epoch: 3.6 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09627689850668503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09627689850668503 | validation: 0.22573582912464538]
	TIME [epoch: 3.62 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16359916791732218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16359916791732218 | validation: 0.272938572129142]
	TIME [epoch: 3.62 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2584200516484538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2584200516484538 | validation: 0.26600393958980323]
	TIME [epoch: 3.61 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18108623193328455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18108623193328455 | validation: 0.27482541282950024]
	TIME [epoch: 3.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16839181884934717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16839181884934717 | validation: 0.1950079970549231]
	TIME [epoch: 3.62 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13048275104175633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13048275104175633 | validation: 0.18487325655015274]
	TIME [epoch: 3.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08637146328634752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08637146328634752 | validation: 0.1794612194231473]
	TIME [epoch: 3.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08047447834974378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08047447834974378 | validation: 0.14065482576332963]
	TIME [epoch: 3.58 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07084360385302332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07084360385302332 | validation: 0.15874513870684567]
	TIME [epoch: 3.63 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07146398625065835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07146398625065835 | validation: 0.14940027971814454]
	TIME [epoch: 3.58 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08504633759603668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08504633759603668 | validation: 0.2938947100727387]
	TIME [epoch: 3.62 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14502226984486632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14502226984486632 | validation: 0.28142933214176064]
	TIME [epoch: 3.61 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21331888842163885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21331888842163885 | validation: 0.29909939407176195]
	TIME [epoch: 3.61 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19746606886513043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19746606886513043 | validation: 0.21642078302841927]
	TIME [epoch: 3.61 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13236320950985406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13236320950985406 | validation: 0.1928119010086202]
	TIME [epoch: 3.61 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08230083390582901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08230083390582901 | validation: 0.1679591629541725]
	TIME [epoch: 3.61 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09886300101113339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09886300101113339 | validation: 0.1830498264973277]
	TIME [epoch: 3.61 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11179544317797392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11179544317797392 | validation: 0.1770210653293581]
	TIME [epoch: 3.62 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09859129243430292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09859129243430292 | validation: 0.19629520256301697]
	TIME [epoch: 3.6 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10991816063375093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10991816063375093 | validation: 0.21238666997638292]
	TIME [epoch: 3.61 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12384504991082942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12384504991082942 | validation: 0.1773623145757644]
	TIME [epoch: 3.62 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13760761781618544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13760761781618544 | validation: 0.23884396964077104]
	TIME [epoch: 3.59 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1418777513945514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1418777513945514 | validation: 0.12774756354983943]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_600.pth
	Model improved!!!
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1095569694084113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1095569694084113 | validation: 0.17992323159054435]
	TIME [epoch: 3.61 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09802542244737891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09802542244737891 | validation: 0.18747887054010756]
	TIME [epoch: 3.61 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1245510734273029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1245510734273029 | validation: 0.21964661589393952]
	TIME [epoch: 3.65 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12994848620970814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12994848620970814 | validation: 0.2214615904850546]
	TIME [epoch: 3.62 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11237007241584979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11237007241584979 | validation: 0.1544227939157452]
	TIME [epoch: 3.61 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09958460409712644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09958460409712644 | validation: 0.24175574916987058]
	TIME [epoch: 3.61 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1019649448481001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1019649448481001 | validation: 0.1841732353456953]
	TIME [epoch: 3.61 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11784962388500603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11784962388500603 | validation: 0.2810915440483163]
	TIME [epoch: 3.62 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14246491579455384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14246491579455384 | validation: 0.2247468789000857]
	TIME [epoch: 3.61 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16612694756452592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16612694756452592 | validation: 0.18938072421026386]
	TIME [epoch: 3.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09910535698893991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09910535698893991 | validation: 0.19701312054555015]
	TIME [epoch: 3.59 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08568733453717699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08568733453717699 | validation: 0.17156024361676178]
	TIME [epoch: 3.59 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0921924546282684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0921924546282684 | validation: 0.20036845951858007]
	TIME [epoch: 3.59 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08789887989559275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08789887989559275 | validation: 0.16753274142341465]
	TIME [epoch: 3.62 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0948461476287625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0948461476287625 | validation: 0.24832901304481345]
	TIME [epoch: 3.62 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16569915543523067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16569915543523067 | validation: 0.25443038660250233]
	TIME [epoch: 3.61 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1992450853552419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1992450853552419 | validation: 0.156972382449923]
	TIME [epoch: 3.61 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10722620227385621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10722620227385621 | validation: 0.21584904223176468]
	TIME [epoch: 3.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10990104012106099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10990104012106099 | validation: 0.16683156725389467]
	TIME [epoch: 3.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11574992531270063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11574992531270063 | validation: 0.22410785731297111]
	TIME [epoch: 3.61 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11002089479492475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11002089479492475 | validation: 0.19825635615813575]
	TIME [epoch: 3.63 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09881100701210883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09881100701210883 | validation: 0.1749372033763944]
	TIME [epoch: 3.61 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11469386209912934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11469386209912934 | validation: 0.22665728854298195]
	TIME [epoch: 3.59 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10709650022317335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10709650022317335 | validation: 0.1771815992023491]
	TIME [epoch: 3.58 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09789771069998592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09789771069998592 | validation: 0.18320685441525084]
	TIME [epoch: 3.61 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10419699112203307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10419699112203307 | validation: 0.22554812916624717]
	TIME [epoch: 3.62 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17037513910324864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17037513910324864 | validation: 0.19766240943681831]
	TIME [epoch: 3.62 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17111618859089198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17111618859089198 | validation: 0.24981056015765832]
	TIME [epoch: 3.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18908986400152852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18908986400152852 | validation: 0.27974439397511797]
	TIME [epoch: 3.6 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20217705044380022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20217705044380022 | validation: 0.20786136255807044]
	TIME [epoch: 3.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16946486705281424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16946486705281424 | validation: 0.17090705900639724]
	TIME [epoch: 3.62 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08586446307022745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08586446307022745 | validation: 0.15211593587754257]
	TIME [epoch: 3.59 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08306261277143101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08306261277143101 | validation: 0.13440332091379373]
	TIME [epoch: 3.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06967819794616464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06967819794616464 | validation: 0.13846797400634614]
	TIME [epoch: 3.6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06503241001821548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06503241001821548 | validation: 0.11665598082754468]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0736219816814515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0736219816814515 | validation: 0.17945172513156415]
	TIME [epoch: 3.63 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0831727784901398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0831727784901398 | validation: 0.12661604580936223]
	TIME [epoch: 3.6 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10306841682606624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10306841682606624 | validation: 0.2478675268139989]
	TIME [epoch: 3.62 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13422163890014888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13422163890014888 | validation: 0.12773696471834733]
	TIME [epoch: 3.62 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09337720879175462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09337720879175462 | validation: 0.20712073078413687]
	TIME [epoch: 3.62 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18266539243303678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18266539243303678 | validation: 0.39488608128229036]
	TIME [epoch: 3.62 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18983918027946764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18983918027946764 | validation: 0.17676803836450602]
	TIME [epoch: 3.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0933127489717166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0933127489717166 | validation: 0.16084035385110584]
	TIME [epoch: 3.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09185253313711188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09185253313711188 | validation: 0.19867107152210942]
	TIME [epoch: 3.6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11764211276431517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11764211276431517 | validation: 0.23469590652008182]
	TIME [epoch: 3.61 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1412579205163074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1412579205163074 | validation: 0.22887637904460478]
	TIME [epoch: 3.61 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13827137134664733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13827137134664733 | validation: 0.2120344463673071]
	TIME [epoch: 3.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10971020584090643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10971020584090643 | validation: 0.14803798946140828]
	TIME [epoch: 3.59 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07969143094016139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07969143094016139 | validation: 0.14030238116937885]
	TIME [epoch: 3.6 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06506007098661809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06506007098661809 | validation: 0.1541459103023955]
	TIME [epoch: 3.62 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06937505680987718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06937505680987718 | validation: 0.11438878707828569]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07530524002517369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07530524002517369 | validation: 0.18539050098707027]
	TIME [epoch: 3.6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10158845621136835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10158845621136835 | validation: 0.12896359014338163]
	TIME [epoch: 3.59 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12167971490396202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12167971490396202 | validation: 0.18607465757883068]
	TIME [epoch: 3.59 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11634603786583114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11634603786583114 | validation: 0.1308283971383752]
	TIME [epoch: 3.6 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10113432330916627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10113432330916627 | validation: 0.19592327769732648]
	TIME [epoch: 3.58 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12626648098632978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12626648098632978 | validation: 0.2509801341110875]
	TIME [epoch: 3.57 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14445819783294095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14445819783294095 | validation: 0.20404848053740623]
	TIME [epoch: 3.59 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13278008070129335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13278008070129335 | validation: 0.16816909663157475]
	TIME [epoch: 3.57 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10438415491836832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10438415491836832 | validation: 0.1275404296295712]
	TIME [epoch: 3.58 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06695610369824366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06695610369824366 | validation: 0.17026109493557431]
	TIME [epoch: 3.56 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09580189283400053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09580189283400053 | validation: 0.4861052022232786]
	TIME [epoch: 3.59 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2407207620441153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2407207620441153 | validation: 0.20935048157312106]
	TIME [epoch: 3.59 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15021358898094372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15021358898094372 | validation: 0.17587543003532555]
	TIME [epoch: 3.6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09081145290736992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09081145290736992 | validation: 0.16785875818305843]
	TIME [epoch: 3.59 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0771361763080861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0771361763080861 | validation: 0.1629049235690251]
	TIME [epoch: 3.58 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07086605722917921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07086605722917921 | validation: 0.1589804445222267]
	TIME [epoch: 3.59 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07220720877034102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07220720877034102 | validation: 0.15301389371654658]
	TIME [epoch: 3.57 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07200640150284164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07200640150284164 | validation: 0.19380385614811013]
	TIME [epoch: 3.58 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08937998202020535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08937998202020535 | validation: 0.1898372292063098]
	TIME [epoch: 3.59 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12799998074658434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12799998074658434 | validation: 0.276251091941068]
	TIME [epoch: 3.59 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1534310459671345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1534310459671345 | validation: 0.12426083317896591]
	TIME [epoch: 3.62 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08724424466278663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08724424466278663 | validation: 0.15224817657912038]
	TIME [epoch: 3.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07164847358374389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07164847358374389 | validation: 0.13384652745401313]
	TIME [epoch: 3.62 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11367602387631826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11367602387631826 | validation: 0.24779318724245145]
	TIME [epoch: 3.6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20507542686962715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20507542686962715 | validation: 0.1965719629719937]
	TIME [epoch: 3.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15322216436817923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15322216436817923 | validation: 0.17812125655513747]
	TIME [epoch: 3.57 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11015527147667208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11015527147667208 | validation: 0.25927313472322305]
	TIME [epoch: 3.59 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11572310670796185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11572310670796185 | validation: 0.13747187988739165]
	TIME [epoch: 3.59 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07953274838465683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07953274838465683 | validation: 0.1621939134980741]
	TIME [epoch: 3.62 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06992493431192495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06992493431192495 | validation: 0.17724845450474214]
	TIME [epoch: 3.62 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09206436904533367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09206436904533367 | validation: 0.1897356651759171]
	TIME [epoch: 3.61 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11617726772792716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11617726772792716 | validation: 0.16107046778649173]
	TIME [epoch: 3.62 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10573202738221492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10573202738221492 | validation: 0.17009153280143724]
	TIME [epoch: 3.62 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09162083582436684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09162083582436684 | validation: 0.17289442200464078]
	TIME [epoch: 3.62 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09065088028281564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09065088028281564 | validation: 0.1858626199037138]
	TIME [epoch: 3.61 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09997059649655544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09997059649655544 | validation: 0.19283418010559203]
	TIME [epoch: 3.61 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13697153980228954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13697153980228954 | validation: 0.3840730973530084]
	TIME [epoch: 3.61 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19182730847607357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19182730847607357 | validation: 0.2060860288454135]
	TIME [epoch: 3.61 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12110560446739896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12110560446739896 | validation: 0.15412264555411972]
	TIME [epoch: 3.62 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10199366401381887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10199366401381887 | validation: 0.20493582101880425]
	TIME [epoch: 3.62 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10177860727034933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10177860727034933 | validation: 0.13625935051038796]
	TIME [epoch: 3.62 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0657988822714973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0657988822714973 | validation: 0.120251922178155]
	TIME [epoch: 3.6 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05885229910422379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05885229910422379 | validation: 0.14782110615091804]
	TIME [epoch: 3.59 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0644546951368012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0644546951368012 | validation: 0.13327573917848518]
	TIME [epoch: 3.62 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0680571798779648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0680571798779648 | validation: 0.18256228139478825]
	TIME [epoch: 3.63 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10945501068620156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10945501068620156 | validation: 0.3479198575810812]
	TIME [epoch: 3.63 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2402829726400849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2402829726400849 | validation: 0.20949149657397267]
	TIME [epoch: 3.62 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15970492064126787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15970492064126787 | validation: 0.2284645758289485]
	TIME [epoch: 3.61 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11369611009953651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11369611009953651 | validation: 0.1918565997899438]
	TIME [epoch: 3.6 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11892319393227924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11892319393227924 | validation: 0.21292648024790115]
	TIME [epoch: 3.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11836968316312417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11836968316312417 | validation: 0.1316511289983455]
	TIME [epoch: 3.61 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08299757416866677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08299757416866677 | validation: 0.161287167801276]
	TIME [epoch: 3.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07935371329910074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07935371329910074 | validation: 0.17752882637549067]
	TIME [epoch: 3.59 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08967315671725203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08967315671725203 | validation: 0.15698629825224764]
	TIME [epoch: 3.61 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10463978043036772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10463978043036772 | validation: 0.16387417922709777]
	TIME [epoch: 3.61 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10583242531827386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10583242531827386 | validation: 0.1719403101556247]
	TIME [epoch: 3.62 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09609418608666825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09609418608666825 | validation: 0.12516726312814272]
	TIME [epoch: 3.61 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07731653180418337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07731653180418337 | validation: 0.12007493604336963]
	TIME [epoch: 3.61 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07111478292162927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07111478292162927 | validation: 0.12741545813072605]
	TIME [epoch: 3.62 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08484733906813281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08484733906813281 | validation: 0.19361862439087224]
	TIME [epoch: 3.61 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12056773881233276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12056773881233276 | validation: 0.2268192019209596]
	TIME [epoch: 3.61 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11267921740765563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11267921740765563 | validation: 0.19974270011858086]
	TIME [epoch: 3.61 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13851198475850188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13851198475850188 | validation: 0.291137412315033]
	TIME [epoch: 3.63 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1337714021454683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1337714021454683 | validation: 0.14926995384427055]
	TIME [epoch: 3.63 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10186232039797986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10186232039797986 | validation: 0.20056917128353044]
	TIME [epoch: 3.61 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11128614964027929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11128614964027929 | validation: 0.1825070306394353]
	TIME [epoch: 3.61 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10616396882163485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10616396882163485 | validation: 0.1463540607058642]
	TIME [epoch: 3.61 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09408035880222178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09408035880222178 | validation: 0.16696199314108348]
	TIME [epoch: 3.62 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13159623959940384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13159623959940384 | validation: 0.20668286881976813]
	TIME [epoch: 3.62 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11970775941192416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11970775941192416 | validation: 0.1903728197238813]
	TIME [epoch: 3.62 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1307437656607031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1307437656607031 | validation: 0.1758362977743309]
	TIME [epoch: 3.63 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11020297003283762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11020297003283762 | validation: 0.1314686011655198]
	TIME [epoch: 3.63 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10448777076588218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10448777076588218 | validation: 0.1718938089976195]
	TIME [epoch: 3.62 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0788825192934601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0788825192934601 | validation: 0.12112426499482486]
	TIME [epoch: 3.62 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08867399307626947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08867399307626947 | validation: 0.17028182700286926]
	TIME [epoch: 3.62 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09448239337601465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09448239337601465 | validation: 0.10561689614161757]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_727.pth
	Model improved!!!
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09569383718401067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09569383718401067 | validation: 0.15944072613023424]
	TIME [epoch: 3.63 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09622402626464677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09622402626464677 | validation: 0.1960813287008291]
	TIME [epoch: 3.62 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10429365139893994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10429365139893994 | validation: 0.20136161191017632]
	TIME [epoch: 3.62 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12440420184883472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12440420184883472 | validation: 0.2239838417529577]
	TIME [epoch: 3.62 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09328153642638191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09328153642638191 | validation: 0.11074446651185262]
	TIME [epoch: 3.61 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060130726342673864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060130726342673864 | validation: 0.14928817925726348]
	TIME [epoch: 3.62 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07658209496107801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07658209496107801 | validation: 0.18582070142835702]
	TIME [epoch: 3.62 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09346991517443791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09346991517443791 | validation: 0.18122543880795247]
	TIME [epoch: 3.62 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10599089621774306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10599089621774306 | validation: 0.1624723960818202]
	TIME [epoch: 3.63 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12100355076714243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12100355076714243 | validation: 0.2077836372640387]
	TIME [epoch: 3.6 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11776628254847311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11776628254847311 | validation: 0.1933156944103537]
	TIME [epoch: 3.6 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09967645649567416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09967645649567416 | validation: 0.21077500098718482]
	TIME [epoch: 3.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09414557942221681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09414557942221681 | validation: 0.1246318717563138]
	TIME [epoch: 3.62 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07300427227558791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07300427227558791 | validation: 0.16425621348900787]
	TIME [epoch: 3.62 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0689218254816364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0689218254816364 | validation: 0.1366858757942181]
	TIME [epoch: 3.61 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06136252570648785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06136252570648785 | validation: 0.11027573112804911]
	TIME [epoch: 3.62 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09382448241663366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09382448241663366 | validation: 0.20550982586697747]
	TIME [epoch: 3.62 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18201300162621606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18201300162621606 | validation: 0.12606543917060287]
	TIME [epoch: 3.61 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15474750289949246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15474750289949246 | validation: 0.14285399176948396]
	TIME [epoch: 3.62 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0833741142959122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0833741142959122 | validation: 0.14250934878779917]
	TIME [epoch: 3.63 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09025240040048829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09025240040048829 | validation: 0.161044835064967]
	TIME [epoch: 3.61 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09916644325157577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09916644325157577 | validation: 0.20992861431535845]
	TIME [epoch: 3.61 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11571757400950033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11571757400950033 | validation: 0.2020378558804616]
	TIME [epoch: 3.61 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1189439660503508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1189439660503508 | validation: 0.20791202702991565]
	TIME [epoch: 3.61 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10061838532158358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10061838532158358 | validation: 0.12861848857181532]
	TIME [epoch: 3.61 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09354411827555449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09354411827555449 | validation: 0.1558177461736523]
	TIME [epoch: 3.61 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08892994424281288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08892994424281288 | validation: 0.1498700976085099]
	TIME [epoch: 3.61 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08588315757539465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08588315757539465 | validation: 0.14148970809045913]
	TIME [epoch: 3.61 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06415815130173316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06415815130173316 | validation: 0.10739584297754874]
	TIME [epoch: 3.62 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061746227192627304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061746227192627304 | validation: 0.13972933996382061]
	TIME [epoch: 3.62 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06412337970262108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06412337970262108 | validation: 0.12204298030321387]
	TIME [epoch: 3.62 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07469945687758373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07469945687758373 | validation: 0.2118409818753551]
	TIME [epoch: 3.63 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12246657670789421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12246657670789421 | validation: 0.19632076698592257]
	TIME [epoch: 3.63 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1343672736658776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1343672736658776 | validation: 0.19851350992112765]
	TIME [epoch: 3.61 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13973533424508564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13973533424508564 | validation: 0.33517807468934635]
	TIME [epoch: 3.62 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1556889387340785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1556889387340785 | validation: 0.1423641068296639]
	TIME [epoch: 3.61 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08010022376138858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08010022376138858 | validation: 0.11822858067584843]
	TIME [epoch: 3.62 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0588630370625032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0588630370625032 | validation: 0.13511367675381428]
	TIME [epoch: 3.63 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05903745621105596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05903745621105596 | validation: 0.14055739368560083]
	TIME [epoch: 3.62 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0651771796136565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0651771796136565 | validation: 0.13114218945769857]
	TIME [epoch: 3.61 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09237879391334901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09237879391334901 | validation: 0.20053000534276855]
	TIME [epoch: 3.61 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1845279165679097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1845279165679097 | validation: 0.15751599241960745]
	TIME [epoch: 3.63 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11063871641143508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11063871641143508 | validation: 0.18595637010037003]
	TIME [epoch: 3.63 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11083088685759675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11083088685759675 | validation: 0.2257645636836521]
	TIME [epoch: 3.62 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1687475571975846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1687475571975846 | validation: 0.16433593840616167]
	TIME [epoch: 3.61 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14872971386144906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14872971386144906 | validation: 0.17342636960477467]
	TIME [epoch: 3.61 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0915345591956689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0915345591956689 | validation: 0.17575563282664833]
	TIME [epoch: 3.62 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09895798878092464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09895798878092464 | validation: 0.15411784843212428]
	TIME [epoch: 3.61 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07950469417934962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07950469417934962 | validation: 0.16422412574188805]
	TIME [epoch: 3.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07817086315821711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07817086315821711 | validation: 0.09978092152206136]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_777.pth
	Model improved!!!
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06636428122284223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06636428122284223 | validation: 0.14009965782965936]
	TIME [epoch: 3.63 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05874950121827846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05874950121827846 | validation: 0.11134292389237795]
	TIME [epoch: 3.62 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06091366995757825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06091366995757825 | validation: 0.14480584180698328]
	TIME [epoch: 3.62 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07784407140998059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07784407140998059 | validation: 0.16622000027755046]
	TIME [epoch: 3.61 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1167079572060789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1167079572060789 | validation: 0.20626690644615683]
	TIME [epoch: 3.63 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13758404128661403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13758404128661403 | validation: 0.3180296494460426]
	TIME [epoch: 3.63 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13434664255217785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13434664255217785 | validation: 0.12743776056021408]
	TIME [epoch: 3.63 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07848377350015082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07848377350015082 | validation: 0.16317914573680384]
	TIME [epoch: 3.59 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08262720163773868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08262720163773868 | validation: 0.10723530749647758]
	TIME [epoch: 3.61 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06079108085296909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06079108085296909 | validation: 0.13013330467463438]
	TIME [epoch: 3.61 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059530800232883346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059530800232883346 | validation: 0.15529786994001893]
	TIME [epoch: 3.61 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08137039899712663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08137039899712663 | validation: 0.20803617151992776]
	TIME [epoch: 3.62 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17011105850628347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17011105850628347 | validation: 0.2819824552157481]
	TIME [epoch: 3.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15714571768075208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15714571768075208 | validation: 0.18951707493149264]
	TIME [epoch: 3.61 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1314887698442276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1314887698442276 | validation: 0.1613729817077282]
	TIME [epoch: 3.61 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10650325799774897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10650325799774897 | validation: 0.12692594096612608]
	TIME [epoch: 3.61 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09385492996813956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09385492996813956 | validation: 0.0917096293206382]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_794.pth
	Model improved!!!
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06872932543779799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06872932543779799 | validation: 0.1527989091574732]
	TIME [epoch: 3.64 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0766553306019688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0766553306019688 | validation: 0.11207897375760517]
	TIME [epoch: 3.62 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07348388211290098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07348388211290098 | validation: 0.11528721356046545]
	TIME [epoch: 3.62 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06082489670154307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06082489670154307 | validation: 0.10569567521798318]
	TIME [epoch: 3.61 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0710183674599549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0710183674599549 | validation: 0.1375871736738142]
	TIME [epoch: 3.61 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07611754002465797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07611754002465797 | validation: 0.1606417505622988]
	TIME [epoch: 3.63 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09530310193293079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09530310193293079 | validation: 0.19945699439738787]
	TIME [epoch: 3.62 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14258463393139034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14258463393139034 | validation: 0.297705285409554]
	TIME [epoch: 3.61 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12343688157900014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12343688157900014 | validation: 0.1087554488350254]
	TIME [epoch: 3.62 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07811021556914921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07811021556914921 | validation: 0.19727650722792756]
	TIME [epoch: 3.62 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09654702847813582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09654702847813582 | validation: 0.15439638521990384]
	TIME [epoch: 3.63 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07417070260312061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07417070260312061 | validation: 0.13114266175245987]
	TIME [epoch: 3.63 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08434342225132395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08434342225132395 | validation: 0.14984225040233845]
	TIME [epoch: 3.63 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13544818797543812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13544818797543812 | validation: 0.25567154570483885]
	TIME [epoch: 3.63 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13556486815188357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13556486815188357 | validation: 0.24075564402157712]
	TIME [epoch: 3.61 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13573487677707252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13573487677707252 | validation: 0.20646376208305722]
	TIME [epoch: 3.63 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10735770117141887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10735770117141887 | validation: 0.12760446670342693]
	TIME [epoch: 3.6 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0872002161712296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0872002161712296 | validation: 0.14257663473428336]
	TIME [epoch: 3.62 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07122815934897771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07122815934897771 | validation: 0.15151324695141577]
	TIME [epoch: 3.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0880912187935607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0880912187935607 | validation: 0.147167758734266]
	TIME [epoch: 3.6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07566773070573618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07566773070573618 | validation: 0.11588557412512235]
	TIME [epoch: 3.61 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07028636540814769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07028636540814769 | validation: 0.14032734938527647]
	TIME [epoch: 3.64 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09455871539154771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09455871539154771 | validation: 0.0897312750683737]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_817.pth
	Model improved!!!
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0972646088365446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0972646088365446 | validation: 0.13078411470266743]
	TIME [epoch: 3.65 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06670632033231634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06670632033231634 | validation: 0.09751957449539406]
	TIME [epoch: 3.63 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048215863039395214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048215863039395214 | validation: 0.09820050345724539]
	TIME [epoch: 3.64 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05877808853605278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05877808853605278 | validation: 0.19592528079832938]
	TIME [epoch: 3.59 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09688903374666684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09688903374666684 | validation: 0.20717385165688784]
	TIME [epoch: 3.62 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15641650453499595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15641650453499595 | validation: 0.14844260708434506]
	TIME [epoch: 3.62 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10239722656499367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10239722656499367 | validation: 0.11193404848077378]
	TIME [epoch: 3.62 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05001247913136625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05001247913136625 | validation: 0.094361371793658]
	TIME [epoch: 3.61 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049804655452254244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049804655452254244 | validation: 0.13160371701240758]
	TIME [epoch: 3.63 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0659026471049339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0659026471049339 | validation: 0.25276944647752647]
	TIME [epoch: 3.63 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15506521030440593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15506521030440593 | validation: 0.4121729360893535]
	TIME [epoch: 3.62 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21772517244332776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21772517244332776 | validation: 0.12512245699468325]
	TIME [epoch: 3.63 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0864212234599796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0864212234599796 | validation: 0.1586632352192616]
	TIME [epoch: 3.65 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10374657024113425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10374657024113425 | validation: 0.17088631857959244]
	TIME [epoch: 3.63 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08382855344754479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08382855344754479 | validation: 0.12204830014947335]
	TIME [epoch: 3.62 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07352399186826178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07352399186826178 | validation: 0.1437996841229055]
	TIME [epoch: 3.64 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07668663503010498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07668663503010498 | validation: 0.1616279335014953]
	TIME [epoch: 3.61 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0738363855517616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0738363855517616 | validation: 0.18669677983325705]
	TIME [epoch: 3.62 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09667631061831791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09667631061831791 | validation: 0.14494436593087487]
	TIME [epoch: 3.65 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10279154783839513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10279154783839513 | validation: 0.16779435876900634]
	TIME [epoch: 3.61 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08666556664352934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08666556664352934 | validation: 0.10884657802498068]
	TIME [epoch: 3.62 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05500537284978213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05500537284978213 | validation: 0.08944222291086502]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_839.pth
	Model improved!!!
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04713233521098567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04713233521098567 | validation: 0.11479928610109265]
	TIME [epoch: 3.63 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05918149617889739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05918149617889739 | validation: 0.10567987430978465]
	TIME [epoch: 3.63 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11118768385302362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11118768385302362 | validation: 0.21687190629555286]
	TIME [epoch: 3.64 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1925350454903511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1925350454903511 | validation: 0.08841886441367969]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_843.pth
	Model improved!!!
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10607797441379177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10607797441379177 | validation: 0.08576043089485946]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_844.pth
	Model improved!!!
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04729447629363577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04729447629363577 | validation: 0.1242240832303698]
	TIME [epoch: 3.63 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05769541325020649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05769541325020649 | validation: 0.09047543874001444]
	TIME [epoch: 3.61 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048265998526115315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048265998526115315 | validation: 0.08111100720093603]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_847.pth
	Model improved!!!
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045410576939722225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045410576939722225 | validation: 0.1379858828598106]
	TIME [epoch: 3.62 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06455956040786047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06455956040786047 | validation: 0.20815817013158658]
	TIME [epoch: 3.62 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16189837060022477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16189837060022477 | validation: 0.33538790660402773]
	TIME [epoch: 3.62 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21485420859394333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21485420859394333 | validation: 0.15183370923596382]
	TIME [epoch: 3.62 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07578894119489979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07578894119489979 | validation: 0.14452311668923803]
	TIME [epoch: 3.63 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08445655676926203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08445655676926203 | validation: 0.2910847446139066]
	TIME [epoch: 3.63 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14038872588490256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14038872588490256 | validation: 0.16286176566935226]
	TIME [epoch: 3.64 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07550688037282412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07550688037282412 | validation: 0.11649580506858535]
	TIME [epoch: 3.63 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07282456450180304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07282456450180304 | validation: 0.12702448998681892]
	TIME [epoch: 3.63 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06185196345268497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06185196345268497 | validation: 0.11060493755752493]
	TIME [epoch: 3.64 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048085167764935516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048085167764935516 | validation: 0.10048661492754683]
	TIME [epoch: 3.62 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056393391177023515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056393391177023515 | validation: 0.18462679397980464]
	TIME [epoch: 3.63 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09495847161620768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09495847161620768 | validation: 0.17231071959284674]
	TIME [epoch: 3.62 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13359848148616257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13359848148616257 | validation: 0.1870414120847442]
	TIME [epoch: 3.63 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09938767647931938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09938767647931938 | validation: 0.10715351348453382]
	TIME [epoch: 3.62 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06508062781960465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06508062781960465 | validation: 0.09699037251097758]
	TIME [epoch: 3.62 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0673500592659489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0673500592659489 | validation: 0.1549542393704831]
	TIME [epoch: 3.63 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10051970092902293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10051970092902293 | validation: 0.1561951329949318]
	TIME [epoch: 3.63 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14850898391295672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14850898391295672 | validation: 0.14834889845263838]
	TIME [epoch: 3.64 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08987083824563383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08987083824563383 | validation: 0.23402118520719425]
	TIME [epoch: 3.62 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08532579757349755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08532579757349755 | validation: 0.12307367358651683]
	TIME [epoch: 3.62 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08987041937780145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08987041937780145 | validation: 0.1257941099183694]
	TIME [epoch: 3.62 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07998035470736013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07998035470736013 | validation: 0.13672709403254382]
	TIME [epoch: 3.62 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060270422208694704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060270422208694704 | validation: 0.09402821822178797]
	TIME [epoch: 3.63 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05782142492554499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05782142492554499 | validation: 0.1497313787164322]
	TIME [epoch: 3.61 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0679886357819619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0679886357819619 | validation: 0.1013350940206129]
	TIME [epoch: 3.61 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06423762328301785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06423762328301785 | validation: 0.1627171866380389]
	TIME [epoch: 3.62 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08611107671771755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08611107671771755 | validation: 0.13482482817714528]
	TIME [epoch: 3.63 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10300655274577608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10300655274577608 | validation: 0.13257780425336518]
	TIME [epoch: 3.63 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08278362481474971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08278362481474971 | validation: 0.12386670050279519]
	TIME [epoch: 3.63 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05939244794692761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05939244794692761 | validation: 0.08334997876376325]
	TIME [epoch: 3.64 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04807845200431538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04807845200431538 | validation: 0.12376793150185202]
	TIME [epoch: 3.63 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0709288498294488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0709288498294488 | validation: 0.23083366180943235]
	TIME [epoch: 3.63 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15976701845914584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15976701845914584 | validation: 0.2060636156409945]
	TIME [epoch: 3.62 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12375582328688621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12375582328688621 | validation: 0.09204252348748632]
	TIME [epoch: 3.62 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0527788192232404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0527788192232404 | validation: 0.10209554584641234]
	TIME [epoch: 3.61 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053094013971535646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053094013971535646 | validation: 0.09765412394417644]
	TIME [epoch: 3.62 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059877389710154647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059877389710154647 | validation: 0.0693640366040507]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_885.pth
	Model improved!!!
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055463445763812985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055463445763812985 | validation: 0.11619172297344725]
	TIME [epoch: 3.61 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056751250678103056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056751250678103056 | validation: 0.17334651197474998]
	TIME [epoch: 3.61 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15279737893371495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15279737893371495 | validation: 0.3997194026253986]
	TIME [epoch: 3.62 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2434200126331076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2434200126331076 | validation: 0.14376240059664144]
	TIME [epoch: 3.62 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08576045309041248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08576045309041248 | validation: 0.07558079200875852]
	TIME [epoch: 3.62 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07543573656761689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07543573656761689 | validation: 0.10798357326860573]
	TIME [epoch: 3.6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06791006839603074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06791006839603074 | validation: 0.1126206027302926]
	TIME [epoch: 3.62 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06883550998385189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06883550998385189 | validation: 0.10996817067750536]
	TIME [epoch: 3.6 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07260773855158727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07260773855158727 | validation: 0.17626865729716737]
	TIME [epoch: 3.62 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09895448379896216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09895448379896216 | validation: 0.19327864333918687]
	TIME [epoch: 3.61 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1369670982121939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1369670982121939 | validation: 0.1849502929124671]
	TIME [epoch: 3.61 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10021478801576607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10021478801576607 | validation: 0.11871917237774446]
	TIME [epoch: 3.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06426060266655939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06426060266655939 | validation: 0.09816383197985608]
	TIME [epoch: 3.61 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047166023079462875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047166023079462875 | validation: 0.08137387277520233]
	TIME [epoch: 3.61 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052079642011152706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052079642011152706 | validation: 0.13546793397386248]
	TIME [epoch: 3.61 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053899823156469025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053899823156469025 | validation: 0.0871526295599639]
	TIME [epoch: 3.64 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05095151797240269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05095151797240269 | validation: 0.17376095783895523]
	TIME [epoch: 3.63 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0666558579351832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0666558579351832 | validation: 0.08294331481732285]
	TIME [epoch: 3.62 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07227766542989487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07227766542989487 | validation: 0.158081091564243]
	TIME [epoch: 3.63 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08306730004505283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08306730004505283 | validation: 0.21568501402229573]
	TIME [epoch: 3.64 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16556158395618167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16556158395618167 | validation: 0.21376001298859393]
	TIME [epoch: 3.62 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16199851250316372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16199851250316372 | validation: 0.1538372586794574]
	TIME [epoch: 3.62 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05890241023064071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05890241023064071 | validation: 0.1257244610045401]
	TIME [epoch: 3.62 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062090352530086504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062090352530086504 | validation: 0.12983201415678036]
	TIME [epoch: 3.62 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07071235613913053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07071235613913053 | validation: 0.14034592071854818]
	TIME [epoch: 3.62 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07418763917312943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07418763917312943 | validation: 0.1978468657092942]
	TIME [epoch: 3.62 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.152643087728932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.152643087728932 | validation: 0.2450321316377532]
	TIME [epoch: 3.63 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15583985706067316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15583985706067316 | validation: 0.27519660420872116]
	TIME [epoch: 3.63 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18294869312171533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18294869312171533 | validation: 0.2783741391060577]
	TIME [epoch: 3.64 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1840044046039774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1840044046039774 | validation: 0.10817222295828975]
	TIME [epoch: 3.62 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0928611773330556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0928611773330556 | validation: 0.10025848305722967]
	TIME [epoch: 3.62 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05656215062772672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05656215062772672 | validation: 0.093668800495701]
	TIME [epoch: 3.62 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05937219134674059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05937219134674059 | validation: 0.11240162939170562]
	TIME [epoch: 3.62 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0555015172552657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0555015172552657 | validation: 0.06925177548234955]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055517322285566134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055517322285566134 | validation: 0.0966930981679952]
	TIME [epoch: 3.61 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04884798967580113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04884798967580113 | validation: 0.08554671689055694]
	TIME [epoch: 3.62 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052373669190627296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052373669190627296 | validation: 0.11377341898546188]
	TIME [epoch: 3.61 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06714770205044433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06714770205044433 | validation: 0.14761017552188466]
	TIME [epoch: 3.62 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07174310549467557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07174310549467557 | validation: 0.15550292147558342]
	TIME [epoch: 3.62 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09193820777570785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09193820777570785 | validation: 0.13672944603838727]
	TIME [epoch: 3.61 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06565871419748329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06565871419748329 | validation: 0.11398303252458307]
	TIME [epoch: 3.62 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04992022716639571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04992022716639571 | validation: 0.08839713750940502]
	TIME [epoch: 3.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052260514665531284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052260514665531284 | validation: 0.1749285794402239]
	TIME [epoch: 3.62 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07825272967390698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07825272967390698 | validation: 0.13029128145736985]
	TIME [epoch: 3.61 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09741436934205375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09741436934205375 | validation: 0.19267440585897863]
	TIME [epoch: 3.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12083636376072886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12083636376072886 | validation: 0.15259490519733282]
	TIME [epoch: 3.61 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11044062410678634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11044062410678634 | validation: 0.10253624509281885]
	TIME [epoch: 3.6 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10223935116857731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10223935116857731 | validation: 0.17817754245590633]
	TIME [epoch: 3.61 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10070215451195924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10070215451195924 | validation: 0.15529318794124367]
	TIME [epoch: 3.61 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08285643988095977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08285643988095977 | validation: 0.1615708494450046]
	TIME [epoch: 3.61 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10587354003009544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10587354003009544 | validation: 0.16862533853799344]
	TIME [epoch: 3.62 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07689681802699508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07689681802699508 | validation: 0.1264563566319464]
	TIME [epoch: 3.63 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06276639893911755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06276639893911755 | validation: 0.09662583050452034]
	TIME [epoch: 3.62 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05703273408836631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05703273408836631 | validation: 0.08762976827657135]
	TIME [epoch: 3.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04851311549274314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04851311549274314 | validation: 0.07404448537826651]
	TIME [epoch: 3.61 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04258398150779248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04258398150779248 | validation: 0.07808245818549014]
	TIME [epoch: 3.63 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04657667093730789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04657667093730789 | validation: 0.1302141206330505]
	TIME [epoch: 3.61 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06947149403435406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06947149403435406 | validation: 0.11988010132773008]
	TIME [epoch: 3.6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09378411365059144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09378411365059144 | validation: 0.18509248639686812]
	TIME [epoch: 3.61 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1081656247678915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1081656247678915 | validation: 0.1125256481178389]
	TIME [epoch: 3.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07720808691753214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07720808691753214 | validation: 0.2157886671943703]
	TIME [epoch: 3.62 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1182744301481725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1182744301481725 | validation: 0.19974999104741933]
	TIME [epoch: 3.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1618444902114875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1618444902114875 | validation: 0.17219547265425394]
	TIME [epoch: 3.62 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09839060048023023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09839060048023023 | validation: 0.13894746671948344]
	TIME [epoch: 3.62 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09634950901886452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09634950901886452 | validation: 0.09385375566399025]
	TIME [epoch: 3.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08876301951064462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08876301951064462 | validation: 0.11769242751846637]
	TIME [epoch: 3.61 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07751682579074043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07751682579074043 | validation: 0.0690022153188509]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_952.pth
	Model improved!!!
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047218889357169565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047218889357169565 | validation: 0.14144443119835373]
	TIME [epoch: 3.62 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05449723878776208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05449723878776208 | validation: 0.11111571949171135]
	TIME [epoch: 3.62 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07421531387498025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07421531387498025 | validation: 0.27371735919341555]
	TIME [epoch: 3.62 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1243045158407793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1243045158407793 | validation: 0.18054277630488857]
	TIME [epoch: 3.62 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12390256488886316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12390256488886316 | validation: 0.14394495689419803]
	TIME [epoch: 3.61 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09162461399773354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09162461399773354 | validation: 0.15758465572936614]
	TIME [epoch: 3.61 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07421417134482457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07421417134482457 | validation: 0.10483099466988298]
	TIME [epoch: 3.65 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05826713787257634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05826713787257634 | validation: 0.08052436141152422]
	TIME [epoch: 3.6 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06059054872086808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06059054872086808 | validation: 0.1236060005830745]
	TIME [epoch: 3.62 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06513221926540673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06513221926540673 | validation: 0.06540272176870697]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_962.pth
	Model improved!!!
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04924698060772923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04924698060772923 | validation: 0.08736016907355504]
	TIME [epoch: 3.61 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04123837599678593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04123837599678593 | validation: 0.08595559028450642]
	TIME [epoch: 3.61 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04187279323436178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04187279323436178 | validation: 0.08789729195652489]
	TIME [epoch: 3.61 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04774802179265464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04774802179265464 | validation: 0.1800303868328099]
	TIME [epoch: 3.61 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10249305320509919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10249305320509919 | validation: 0.2597520370835537]
	TIME [epoch: 3.61 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19685424502841145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19685424502841145 | validation: 0.2685297185580397]
	TIME [epoch: 3.61 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24744818771307386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24744818771307386 | validation: 0.3440575181007507]
	TIME [epoch: 3.61 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2850709205725871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2850709205725871 | validation: 0.10646196674233202]
	TIME [epoch: 3.61 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0892933798428099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0892933798428099 | validation: 0.20358543959588743]
	TIME [epoch: 3.63 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11799724341543436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11799724341543436 | validation: 0.12592088841337828]
	TIME [epoch: 3.63 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053005438818394646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053005438818394646 | validation: 0.11624299976603876]
	TIME [epoch: 3.63 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06236102833580421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06236102833580421 | validation: 0.09871864046428422]
	TIME [epoch: 3.62 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044633676612343576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044633676612343576 | validation: 0.10552521090511374]
	TIME [epoch: 3.62 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044793380587003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044793380587003 | validation: 0.07503805543930117]
	TIME [epoch: 3.62 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04455074795600645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04455074795600645 | validation: 0.08099289718831908]
	TIME [epoch: 3.62 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03762366088590734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03762366088590734 | validation: 0.06964312615696192]
	TIME [epoch: 3.62 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03472360713075077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03472360713075077 | validation: 0.0661605938887385]
	TIME [epoch: 3.62 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03574283041262772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03574283041262772 | validation: 0.0767701695416214]
	TIME [epoch: 3.62 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04351144681436976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04351144681436976 | validation: 0.15513021898851567]
	TIME [epoch: 3.63 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07552610386711202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07552610386711202 | validation: 0.22137133182082738]
	TIME [epoch: 3.62 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17209360642289212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17209360642289212 | validation: 0.19058526233539308]
	TIME [epoch: 3.61 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17671190467132766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17671190467132766 | validation: 0.2743730736057261]
	TIME [epoch: 3.63 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21157061711036185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21157061711036185 | validation: 0.15821015019421575]
	TIME [epoch: 3.63 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09133952345674638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09133952345674638 | validation: 0.1077820619384919]
	TIME [epoch: 3.6 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05846567848761323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05846567848761323 | validation: 0.09664490837284695]
	TIME [epoch: 3.61 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061080212797205775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061080212797205775 | validation: 0.10021948736142208]
	TIME [epoch: 3.61 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04200259617823624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04200259617823624 | validation: 0.10233745649868263]
	TIME [epoch: 3.61 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04142035928339612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04142035928339612 | validation: 0.07409172802285245]
	TIME [epoch: 3.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03886609945072811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03886609945072811 | validation: 0.07931381926463162]
	TIME [epoch: 3.62 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03811393393429773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03811393393429773 | validation: 0.0651536340736093]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_992.pth
	Model improved!!!
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04153434866155136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04153434866155136 | validation: 0.15772887707916683]
	TIME [epoch: 3.62 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08189633187884972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08189633187884972 | validation: 0.2642962269470771]
	TIME [epoch: 3.62 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21032755605170497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21032755605170497 | validation: 0.24501118333035155]
	TIME [epoch: 3.61 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1364818679053344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1364818679053344 | validation: 0.13726777846459584]
	TIME [epoch: 3.63 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0629861125358004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0629861125358004 | validation: 0.10248762479830727]
	TIME [epoch: 3.63 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07759663992679029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07759663992679029 | validation: 0.14051300748749967]
	TIME [epoch: 3.6 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08878550569839913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08878550569839913 | validation: 0.09004271272298864]
	TIME [epoch: 3.62 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055122779550573285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055122779550573285 | validation: 0.08672985502792657]
	TIME [epoch: 3.59 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045623187876995104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045623187876995104 | validation: 0.12228359632990547]
	TIME [epoch: 48.6 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05058392159933618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05058392159933618 | validation: 0.1256448778943928]
	TIME [epoch: 7.88 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07867568395480094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07867568395480094 | validation: 0.21878573936975407]
	TIME [epoch: 7.82 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10175741908999214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10175741908999214 | validation: 0.18984035754153006]
	TIME [epoch: 7.84 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12240359693340803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12240359693340803 | validation: 0.09668833194820473]
	TIME [epoch: 7.84 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0735399330064265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0735399330064265 | validation: 0.08915491898933868]
	TIME [epoch: 7.87 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058436831053287765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058436831053287765 | validation: 0.1336526064146201]
	TIME [epoch: 7.85 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08128423807762598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08128423807762598 | validation: 0.09297583800717102]
	TIME [epoch: 7.87 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06924761736713304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06924761736713304 | validation: 0.12087244011513497]
	TIME [epoch: 7.82 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053238175975971025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053238175975971025 | validation: 0.07480883728589165]
	TIME [epoch: 7.86 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03961130086596664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03961130086596664 | validation: 0.09420799150020284]
	TIME [epoch: 7.84 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04447543989255376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04447543989255376 | validation: 0.10009825871733412]
	TIME [epoch: 7.86 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06437106177593337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06437106177593337 | validation: 0.22172143385859144]
	TIME [epoch: 7.86 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13731931049431373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13731931049431373 | validation: 0.1391336186452019]
	TIME [epoch: 7.86 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10424654150723886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10424654150723886 | validation: 0.0932310568518694]
	TIME [epoch: 7.81 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04537604757362744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04537604757362744 | validation: 0.08117482602222653]
	TIME [epoch: 7.82 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05502027915639077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05502027915639077 | validation: 0.13901575760035134]
	TIME [epoch: 7.84 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09505167488293338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09505167488293338 | validation: 0.13871231292741798]
	TIME [epoch: 7.85 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09752514868855461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09752514868855461 | validation: 0.11468044053942178]
	TIME [epoch: 7.86 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06294388553198946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06294388553198946 | validation: 0.1053369181461875]
	TIME [epoch: 7.87 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04795780224545651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04795780224545651 | validation: 0.07904376075771019]
	TIME [epoch: 7.82 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06083644484268206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06083644484268206 | validation: 0.1343642212313559]
	TIME [epoch: 7.83 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07458660250285437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07458660250285437 | validation: 0.1899638792699645]
	TIME [epoch: 7.76 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11754219553509558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11754219553509558 | validation: 0.1749996087988801]
	TIME [epoch: 7.85 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12308469238089359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12308469238089359 | validation: 0.30505496927456843]
	TIME [epoch: 7.86 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13846985886598448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13846985886598448 | validation: 0.08080650232904349]
	TIME [epoch: 7.86 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0867365933537339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0867365933537339 | validation: 0.13101939503202398]
	TIME [epoch: 7.84 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0878690594699248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0878690594699248 | validation: 0.2130973502918641]
	TIME [epoch: 7.82 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11907535273309847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11907535273309847 | validation: 0.16528817144968097]
	TIME [epoch: 7.8 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0879019569922621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0879019569922621 | validation: 0.09623460956599755]
	TIME [epoch: 7.81 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059140137025626774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059140137025626774 | validation: 0.10577221570636378]
	TIME [epoch: 7.84 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05660150016297581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05660150016297581 | validation: 0.07734820764474978]
	TIME [epoch: 7.87 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042260919855800536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042260919855800536 | validation: 0.08231224871164283]
	TIME [epoch: 7.83 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03725744817643359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03725744817643359 | validation: 0.06490234268211507]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_1034.pth
	Model improved!!!
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03884596378884607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03884596378884607 | validation: 0.12614498010250585]
	TIME [epoch: 7.85 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05818000347787045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05818000347787045 | validation: 0.07789279340377547]
	TIME [epoch: 7.84 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09165339610165368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09165339610165368 | validation: 0.13795568987681342]
	TIME [epoch: 7.86 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09863023743730061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09863023743730061 | validation: 0.06055377098070963]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_1038.pth
	Model improved!!!
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040627944766665244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040627944766665244 | validation: 0.0658397939482404]
	TIME [epoch: 7.84 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04259445620265668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04259445620265668 | validation: 0.10231618730744461]
	TIME [epoch: 7.85 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0575396075313033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0575396075313033 | validation: 0.19263821150786653]
	TIME [epoch: 7.83 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12357046009206676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12357046009206676 | validation: 0.299743053054617]
	TIME [epoch: 7.85 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21169092650864813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21169092650864813 | validation: 0.20001754142906752]
	TIME [epoch: 7.84 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08302961194773385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08302961194773385 | validation: 0.09350190944110447]
	TIME [epoch: 7.85 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05633851586007751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05633851586007751 | validation: 0.1020033796847783]
	TIME [epoch: 7.82 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07586997915617452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07586997915617452 | validation: 0.1884928656728384]
	TIME [epoch: 7.83 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10427721597581283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10427721597581283 | validation: 0.17701604864641862]
	TIME [epoch: 7.84 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09178093786575801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09178093786575801 | validation: 0.08976666862517901]
	TIME [epoch: 7.84 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08110679401159203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08110679401159203 | validation: 0.10729864167394179]
	TIME [epoch: 7.83 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07376531073018582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07376531073018582 | validation: 0.1369943952675708]
	TIME [epoch: 7.83 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06539303033265895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06539303033265895 | validation: 0.08420474340675649]
	TIME [epoch: 7.81 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041360664621242334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041360664621242334 | validation: 0.081671581999804]
	TIME [epoch: 7.85 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035661084806153744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035661084806153744 | validation: 0.08260136238121554]
	TIME [epoch: 7.81 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03462315127974404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03462315127974404 | validation: 0.07565363924122154]
	TIME [epoch: 7.87 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03705371013948708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03705371013948708 | validation: 0.12977903163857502]
	TIME [epoch: 7.85 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07788928889491926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07788928889491926 | validation: 0.24129844669378886]
	TIME [epoch: 7.86 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13482876653710738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13482876653710738 | validation: 0.19980113626496154]
	TIME [epoch: 7.83 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11672975747938427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11672975747938427 | validation: 0.11251480849434747]
	TIME [epoch: 7.85 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06770831069894871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06770831069894871 | validation: 0.11831901902800604]
	TIME [epoch: 7.84 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06010641782028104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06010641782028104 | validation: 0.14633671463897976]
	TIME [epoch: 7.85 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15841750146034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15841750146034 | validation: 0.2135412998800703]
	TIME [epoch: 7.86 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19496507344925826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19496507344925826 | validation: 0.14109537906845032]
	TIME [epoch: 7.85 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07549603103121894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07549603103121894 | validation: 0.10046601066487347]
	TIME [epoch: 7.84 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05549144130577474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05549144130577474 | validation: 0.09280330266573078]
	TIME [epoch: 7.86 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04909493790121909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04909493790121909 | validation: 0.09355453324874355]
	TIME [epoch: 7.85 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05175282464583681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05175282464583681 | validation: 0.11409089169301107]
	TIME [epoch: 7.87 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07095659843497071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07095659843497071 | validation: 0.16649151783209326]
	TIME [epoch: 7.86 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11461109170632076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11461109170632076 | validation: 0.1844342773440095]
	TIME [epoch: 7.86 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1336844969540198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1336844969540198 | validation: 0.1439630056904695]
	TIME [epoch: 7.85 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09960042663653258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09960042663653258 | validation: 0.10152351761567502]
	TIME [epoch: 7.86 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054077461017924836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054077461017924836 | validation: 0.07324771362107266]
	TIME [epoch: 7.86 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049525820216473455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049525820216473455 | validation: 0.06855988503009765]
	TIME [epoch: 7.86 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03685055929153196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03685055929153196 | validation: 0.08880698168506364]
	TIME [epoch: 7.86 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04475121590712883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04475121590712883 | validation: 0.14116841792894128]
	TIME [epoch: 7.86 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07688423687306532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07688423687306532 | validation: 0.18667034386864698]
	TIME [epoch: 7.85 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0890344812559959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0890344812559959 | validation: 0.14365866789425863]
	TIME [epoch: 7.85 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07177684421924994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07177684421924994 | validation: 0.09197674365960697]
	TIME [epoch: 7.85 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03779105848888001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03779105848888001 | validation: 0.08354912044376708]
	TIME [epoch: 7.86 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046184052276701054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046184052276701054 | validation: 0.12399130020053017]
	TIME [epoch: 7.86 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054398817110311554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054398817110311554 | validation: 0.08722643865554536]
	TIME [epoch: 7.89 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05111397726199814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05111397726199814 | validation: 0.13527049991939238]
	TIME [epoch: 7.84 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057379416413889626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057379416413889626 | validation: 0.07781637212688297]
	TIME [epoch: 7.86 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05093584704626716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05093584704626716 | validation: 0.12571170737291923]
	TIME [epoch: 7.85 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06582655331734984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06582655331734984 | validation: 0.21788315404334818]
	TIME [epoch: 7.89 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14729053160885572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14729053160885572 | validation: 0.20626843790754626]
	TIME [epoch: 7.88 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15219534828138961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15219534828138961 | validation: 0.18643890577870886]
	TIME [epoch: 7.86 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16380240060972376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16380240060972376 | validation: 0.20169356149919257]
	TIME [epoch: 7.86 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18006352669214265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18006352669214265 | validation: 0.11588993062179848]
	TIME [epoch: 7.85 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08660415888795683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08660415888795683 | validation: 0.09375765974160835]
	TIME [epoch: 7.85 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06342206722046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06342206722046 | validation: 0.09141255715039186]
	TIME [epoch: 7.86 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05344803736706112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05344803736706112 | validation: 0.0863640268371975]
	TIME [epoch: 7.86 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03663478318486406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03663478318486406 | validation: 0.0736584920568901]
	TIME [epoch: 7.85 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03945039927825854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03945039927825854 | validation: 0.0702828742610904]
	TIME [epoch: 7.84 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03371028064426696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03371028064426696 | validation: 0.052015055599626284]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_1094.pth
	Model improved!!!
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0331181631316383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0331181631316383 | validation: 0.07957311541430251]
	TIME [epoch: 7.84 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04041140053365689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04041140053365689 | validation: 0.0943812460049264]
	TIME [epoch: 7.85 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07721696981341887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07721696981341887 | validation: 0.23718687094500607]
	TIME [epoch: 7.86 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15678171008293815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15678171008293815 | validation: 0.11826105216868443]
	TIME [epoch: 7.85 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0666077811421943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0666077811421943 | validation: 0.09828761689106935]
	TIME [epoch: 7.83 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03685291570357105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03685291570357105 | validation: 0.07640582445426403]
	TIME [epoch: 7.84 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04139911967001117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04139911967001117 | validation: 0.09126706326395422]
	TIME [epoch: 7.91 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052313337982159314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052313337982159314 | validation: 0.14485005401506923]
	TIME [epoch: 7.85 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07669754060731973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07669754060731973 | validation: 0.10190452074388393]
	TIME [epoch: 7.86 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057973995352831605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057973995352831605 | validation: 0.07129520646358137]
	TIME [epoch: 7.84 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05926285274140014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05926285274140014 | validation: 0.1541157706114723]
	TIME [epoch: 7.84 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12397215159037356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12397215159037356 | validation: 0.0759489848426236]
	TIME [epoch: 7.84 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08017655247883103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08017655247883103 | validation: 0.08651202799717027]
	TIME [epoch: 7.84 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041223871805128605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041223871805128605 | validation: 0.10680637193741803]
	TIME [epoch: 7.85 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04660916686834077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04660916686834077 | validation: 0.07822672103395917]
	TIME [epoch: 7.86 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045676102040266196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045676102040266196 | validation: 0.12666699085559613]
	TIME [epoch: 7.86 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04840334643111357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04840334643111357 | validation: 0.07956280721479679]
	TIME [epoch: 7.84 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04407488779380741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04407488779380741 | validation: 0.08247311214654864]
	TIME [epoch: 7.85 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05428078029357506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05428078029357506 | validation: 0.11051199726941621]
	TIME [epoch: 7.84 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08673948116082564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08673948116082564 | validation: 0.24959536984649083]
	TIME [epoch: 7.86 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17036132039996774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17036132039996774 | validation: 0.23314767431255135]
	TIME [epoch: 8.35 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16720455392574785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16720455392574785 | validation: 0.15912905356597698]
	TIME [epoch: 7.86 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07637091342442368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07637091342442368 | validation: 0.09470984950579729]
	TIME [epoch: 7.84 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08549337199699066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08549337199699066 | validation: 0.06903300300078241]
	TIME [epoch: 7.86 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05383961019179359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05383961019179359 | validation: 0.11324820454709794]
	TIME [epoch: 7.88 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04339691547795492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04339691547795492 | validation: 0.06713754749852748]
	TIME [epoch: 7.87 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564206230519999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03564206230519999 | validation: 0.06040699619619203]
	TIME [epoch: 7.85 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03055616389367408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03055616389367408 | validation: 0.06988984795628998]
	TIME [epoch: 7.86 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029898304666740448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029898304666740448 | validation: 0.052941110077736366]
	TIME [epoch: 7.85 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030811995057178816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030811995057178816 | validation: 0.12007120567986514]
	TIME [epoch: 7.86 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0897179852325305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0897179852325305 | validation: 0.23345185187228307]
	TIME [epoch: 7.84 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2327359863542978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2327359863542978 | validation: 0.19525922919466893]
	TIME [epoch: 7.86 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10275506899507023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10275506899507023 | validation: 0.15222536428682493]
	TIME [epoch: 7.86 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11604627380136169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11604627380136169 | validation: 0.10192131817777086]
	TIME [epoch: 7.87 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07125063462269583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07125063462269583 | validation: 0.08413177485147016]
	TIME [epoch: 7.84 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07377690553048011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07377690553048011 | validation: 0.20724464738947487]
	TIME [epoch: 7.85 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15638571901497542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15638571901497542 | validation: 0.12071779199654604]
	TIME [epoch: 7.85 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07722024466310101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07722024466310101 | validation: 0.12197923654664806]
	TIME [epoch: 7.86 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05952077493156648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05952077493156648 | validation: 0.11114069654749964]
	TIME [epoch: 7.86 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054711479642485066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054711479642485066 | validation: 0.06657116563089714]
	TIME [epoch: 7.86 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04019683753330066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04019683753330066 | validation: 0.07925944662695834]
	TIME [epoch: 7.84 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035271403993624745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035271403993624745 | validation: 0.05893495669584368]
	TIME [epoch: 7.85 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03273169738891997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03273169738891997 | validation: 0.07233137241374202]
	TIME [epoch: 7.87 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03365541102258814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03365541102258814 | validation: 0.07273632221756603]
	TIME [epoch: 7.86 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03927226023473658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03927226023473658 | validation: 0.11382902914844878]
	TIME [epoch: 7.86 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046635622017053555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046635622017053555 | validation: 0.12595364033701179]
	TIME [epoch: 7.85 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06831433636522619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06831433636522619 | validation: 0.09595914423118802]
	TIME [epoch: 7.85 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05851724315263861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05851724315263861 | validation: 0.10774074620222587]
	TIME [epoch: 7.85 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0461569411208292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0461569411208292 | validation: 0.049559290837273506]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_1143.pth
	Model improved!!!
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031090863257216092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031090863257216092 | validation: 0.06765184324865424]
	TIME [epoch: 7.87 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03031835674065508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03031835674065508 | validation: 0.053036036517606015]
	TIME [epoch: 7.85 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02778611287931804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02778611287931804 | validation: 0.06441946093322169]
	TIME [epoch: 7.85 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03785337340751086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03785337340751086 | validation: 0.2254236187161489]
	TIME [epoch: 7.85 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14770809219396672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14770809219396672 | validation: 0.3727586950623738]
	TIME [epoch: 7.84 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25277089185524976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25277089185524976 | validation: 0.2207071569299182]
	TIME [epoch: 7.84 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15049862709302558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15049862709302558 | validation: 0.2080553693367822]
	TIME [epoch: 7.86 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.160280889578274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.160280889578274 | validation: 0.18649708429174222]
	TIME [epoch: 7.85 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11150156474480069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11150156474480069 | validation: 0.12913857174680632]
	TIME [epoch: 7.86 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10221665153409136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10221665153409136 | validation: 0.14520394645310722]
	TIME [epoch: 7.84 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07064697609614491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07064697609614491 | validation: 0.11665643665118969]
	TIME [epoch: 7.86 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05424107599678875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05424107599678875 | validation: 0.08003375522822853]
	TIME [epoch: 7.88 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03754662762797328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03754662762797328 | validation: 0.07013826443874714]
	TIME [epoch: 7.86 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03711305412717729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03711305412717729 | validation: 0.08370377615919769]
	TIME [epoch: 7.85 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03441717161247831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03441717161247831 | validation: 0.07989937365868777]
	TIME [epoch: 7.85 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04099631737648957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04099631737648957 | validation: 0.11716025534109759]
	TIME [epoch: 7.85 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06384411902992353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06384411902992353 | validation: 0.10940970192368488]
	TIME [epoch: 7.85 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07466694310989545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07466694310989545 | validation: 0.14544394029019292]
	TIME [epoch: 7.85 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07310603160062865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07310603160062865 | validation: 0.07506142823532223]
	TIME [epoch: 7.86 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04978357662799814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04978357662799814 | validation: 0.1001554744470234]
	TIME [epoch: 7.85 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06489615463221948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06489615463221948 | validation: 0.08309821647047182]
	TIME [epoch: 7.86 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1043830540697296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1043830540697296 | validation: 0.12304985261079329]
	TIME [epoch: 7.8 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0739078483279608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0739078483279608 | validation: 0.17386358364235074]
	TIME [epoch: 7.84 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0696984348619695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0696984348619695 | validation: 0.10059829472588391]
	TIME [epoch: 7.78 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058909002623700996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058909002623700996 | validation: 0.10727272284675214]
	TIME [epoch: 7.83 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054616346138116986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054616346138116986 | validation: 0.14157687715726436]
	TIME [epoch: 7.86 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08718132283881651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08718132283881651 | validation: 0.12146285317549886]
	TIME [epoch: 7.81 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0817216259097368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0817216259097368 | validation: 0.08411189548672826]
	TIME [epoch: 7.8 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04873432257913331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04873432257913331 | validation: 0.06906225532670653]
	TIME [epoch: 7.86 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03461726110283873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03461726110283873 | validation: 0.05102873868434328]
	TIME [epoch: 7.86 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029765380451256766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029765380451256766 | validation: 0.054206545477423075]
	TIME [epoch: 7.82 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028981648891548292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028981648891548292 | validation: 0.07546475840564192]
	TIME [epoch: 7.81 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03537297389036418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03537297389036418 | validation: 0.1286684851180799]
	TIME [epoch: 7.84 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08096677010032977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08096677010032977 | validation: 0.1580358024079973]
	TIME [epoch: 7.83 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14074699342507865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14074699342507865 | validation: 0.16130937902874376]
	TIME [epoch: 7.79 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0810151932007442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0810151932007442 | validation: 0.09922763710041191]
	TIME [epoch: 7.83 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0578930666374059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0578930666374059 | validation: 0.10660413317631812]
	TIME [epoch: 7.86 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09588171230993318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09588171230993318 | validation: 0.13539331054421286]
	TIME [epoch: 7.86 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09197380496101833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09197380496101833 | validation: 0.17366950590154262]
	TIME [epoch: 7.79 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0919623012866116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0919623012866116 | validation: 0.08076927689071291]
	TIME [epoch: 7.84 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04150193431728589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04150193431728589 | validation: 0.08560746235886169]
	TIME [epoch: 7.86 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04423621872942725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04423621872942725 | validation: 0.10447760929993927]
	TIME [epoch: 7.84 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04754915603285415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04754915603285415 | validation: 0.08396107539217151]
	TIME [epoch: 7.86 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04523094240430656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04523094240430656 | validation: 0.0694832536199797]
	TIME [epoch: 7.85 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03759566122998524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03759566122998524 | validation: 0.11536143521274358]
	TIME [epoch: 7.84 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0469030325708521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0469030325708521 | validation: 0.06832468920759233]
	TIME [epoch: 7.82 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035957566674928984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035957566674928984 | validation: 0.061126374375635666]
	TIME [epoch: 7.85 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0420812836700006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0420812836700006 | validation: 0.14314115913887343]
	TIME [epoch: 7.85 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09880028426207897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09880028426207897 | validation: 0.11564007269893689]
	TIME [epoch: 7.87 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13198222454844147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13198222454844147 | validation: 0.16237910622641044]
	TIME [epoch: 7.83 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07982890751402126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07982890751402126 | validation: 0.1676731151869289]
	TIME [epoch: 7.85 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1213760685210011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1213760685210011 | validation: 0.14049217811599532]
	TIME [epoch: 7.84 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07675501143456077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07675501143456077 | validation: 0.09481859942024717]
	TIME [epoch: 7.84 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060729837640474904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060729837640474904 | validation: 0.15600751916535233]
	TIME [epoch: 7.85 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06931114897417553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06931114897417553 | validation: 0.07058284077250593]
	TIME [epoch: 7.85 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04327611091517616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04327611091517616 | validation: 0.05808593066248331]
	TIME [epoch: 7.86 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03685154353254786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03685154353254786 | validation: 0.07674602689269494]
	TIME [epoch: 7.86 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03702256149816795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03702256149816795 | validation: 0.12363230606905962]
	TIME [epoch: 7.85 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06742746250779419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06742746250779419 | validation: 0.15169349953592454]
	TIME [epoch: 7.84 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10035195609070284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10035195609070284 | validation: 0.1909830072413905]
	TIME [epoch: 7.84 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.104054040125827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.104054040125827 | validation: 0.14600901571521185]
	TIME [epoch: 7.86 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08629322165216576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08629322165216576 | validation: 0.19968651913757351]
	TIME [epoch: 7.85 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17185482272277458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17185482272277458 | validation: 0.11842826505636415]
	TIME [epoch: 7.85 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10429041914065401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10429041914065401 | validation: 0.20140357351372598]
	TIME [epoch: 7.84 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10731996690171794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10731996690171794 | validation: 0.10335712396645534]
	TIME [epoch: 7.85 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055862830079535575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055862830079535575 | validation: 0.07888888370891509]
	TIME [epoch: 7.84 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046013887291512935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046013887291512935 | validation: 0.10447597348906748]
	TIME [epoch: 7.86 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04754625856474954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04754625856474954 | validation: 0.06404128575361907]
	TIME [epoch: 7.84 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03426641270370372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03426641270370372 | validation: 0.05409894661639977]
	TIME [epoch: 7.86 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029385437694671418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029385437694671418 | validation: 0.05935950743065537]
	TIME [epoch: 7.84 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029511615438589683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029511615438589683 | validation: 0.051437828100423244]
	TIME [epoch: 7.85 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028310892810747734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028310892810747734 | validation: 0.06249935438838716]
	TIME [epoch: 7.84 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0285077405582544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0285077405582544 | validation: 0.05327638975722612]
	TIME [epoch: 7.86 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044137008201188425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044137008201188425 | validation: 0.19633511697008216]
	TIME [epoch: 7.85 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13315290812453603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13315290812453603 | validation: 0.17284914378994035]
	TIME [epoch: 7.85 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12994527722209903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12994527722209903 | validation: 0.17176277367695936]
	TIME [epoch: 7.84 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12707320338354744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12707320338354744 | validation: 0.13607739698887392]
	TIME [epoch: 7.85 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10884437447125538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10884437447125538 | validation: 0.07060315654800779]
	TIME [epoch: 7.83 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04437555980843005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04437555980843005 | validation: 0.0958616831446289]
	TIME [epoch: 7.87 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052138542621299966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052138542621299966 | validation: 0.10230411857861726]
	TIME [epoch: 7.84 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05889752833898126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05889752833898126 | validation: 0.1310113467116245]
	TIME [epoch: 7.83 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08734321922946783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08734321922946783 | validation: 0.13025907236257947]
	TIME [epoch: 7.84 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08540510117294713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08540510117294713 | validation: 0.22526745003571863]
	TIME [epoch: 7.85 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1067226188372679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1067226188372679 | validation: 0.10131198870573899]
	TIME [epoch: 7.87 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06845914692788786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06845914692788786 | validation: 0.07949543558044633]
	TIME [epoch: 7.86 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04809343239822879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04809343239822879 | validation: 0.07933776852793328]
	TIME [epoch: 7.82 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039691232958097435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039691232958097435 | validation: 0.10358073513028901]
	TIME [epoch: 7.85 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052223244599456535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052223244599456535 | validation: 0.08454506294963338]
	TIME [epoch: 7.84 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05218624490815912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05218624490815912 | validation: 0.12402646213352231]
	TIME [epoch: 7.85 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06626048191783011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06626048191783011 | validation: 0.0762481375855708]
	TIME [epoch: 7.85 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054863356445729816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054863356445729816 | validation: 0.08355955581900489]
	TIME [epoch: 7.87 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03689236588445433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03689236588445433 | validation: 0.04832927918502764]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_1235.pth
	Model improved!!!
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029472503839789752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029472503839789752 | validation: 0.08862335067439478]
	TIME [epoch: 7.84 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043677952775369115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043677952775369115 | validation: 0.08391775439876256]
	TIME [epoch: 7.85 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09345258451081832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09345258451081832 | validation: 0.17950537627928848]
	TIME [epoch: 7.85 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12400994753658676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12400994753658676 | validation: 0.09445242574044364]
	TIME [epoch: 7.84 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04262481680896506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04262481680896506 | validation: 0.050574630945157295]
	TIME [epoch: 7.87 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04349119331841094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04349119331841094 | validation: 0.08418210673768159]
	TIME [epoch: 7.85 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03679716054195363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03679716054195363 | validation: 0.07792740970521195]
	TIME [epoch: 7.84 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03404906263737741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03404906263737741 | validation: 0.05690785788854106]
	TIME [epoch: 7.85 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029971163080253646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029971163080253646 | validation: 0.0554557442151999]
	TIME [epoch: 7.87 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044222204699223965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044222204699223965 | validation: 0.2515614357246298]
	TIME [epoch: 7.93 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17439409873103615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17439409873103615 | validation: 0.4679163102606236]
	TIME [epoch: 7.87 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3512273312043746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3512273312043746 | validation: 0.21179236889910558]
	TIME [epoch: 7.84 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11334149458693966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11334149458693966 | validation: 0.21130045451861204]
	TIME [epoch: 7.85 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17105405459015668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17105405459015668 | validation: 0.2553895878081069]
	TIME [epoch: 7.84 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18181611473589387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18181611473589387 | validation: 0.1830568696371362]
	TIME [epoch: 7.84 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12018862167780438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12018862167780438 | validation: 0.09055092334081323]
	TIME [epoch: 7.84 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05937608041779953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05937608041779953 | validation: 0.09440744654170652]
	TIME [epoch: 7.87 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05754076741700952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05754076741700952 | validation: 0.08055083782860371]
	TIME [epoch: 7.84 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036203394581068304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036203394581068304 | validation: 0.07309156561716049]
	TIME [epoch: 7.85 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0334933826893215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0334933826893215 | validation: 0.060013440193689906]
	TIME [epoch: 7.85 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032046291169228226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032046291169228226 | validation: 0.06617580415184882]
	TIME [epoch: 7.85 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02860603191756212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02860603191756212 | validation: 0.05313566586224211]
	TIME [epoch: 7.86 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027087886962576996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027087886962576996 | validation: 0.061788537589485104]
	TIME [epoch: 7.85 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03165745315534809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03165745315534809 | validation: 0.05233230339258909]
	TIME [epoch: 7.85 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05545132664520507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05545132664520507 | validation: 0.14792599006385918]
	TIME [epoch: 7.84 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08879335265541795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08879335265541795 | validation: 0.0700046776606238]
	TIME [epoch: 7.85 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06423644798897557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06423644798897557 | validation: 0.09021283374788906]
	TIME [epoch: 7.86 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04076535218447641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04076535218447641 | validation: 0.07701197728276817]
	TIME [epoch: 7.85 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04197775335459995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04197775335459995 | validation: 0.0817729950126837]
	TIME [epoch: 7.86 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043581266867277986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043581266867277986 | validation: 0.07797846232273474]
	TIME [epoch: 7.85 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04501797925417497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04501797925417497 | validation: 0.10857801264442012]
	TIME [epoch: 7.85 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05348059205325717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05348059205325717 | validation: 0.09113951389972547]
	TIME [epoch: 7.85 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06864145917514737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06864145917514737 | validation: 0.1493357107406638]
	TIME [epoch: 7.85 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08758648472043941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08758648472043941 | validation: 0.08167495778346717]
	TIME [epoch: 7.84 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05043572603798577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05043572603798577 | validation: 0.07070758187987862]
	TIME [epoch: 7.86 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07064924921689418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07064924921689418 | validation: 0.14888829886794888]
	TIME [epoch: 7.84 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10378530589536425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10378530589536425 | validation: 0.06224642119275084]
	TIME [epoch: 7.85 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03758569969786509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03758569969786509 | validation: 0.06192007265991212]
	TIME [epoch: 7.84 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035739283153067486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035739283153067486 | validation: 0.09452095310765887]
	TIME [epoch: 7.85 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04318186082940853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04318186082940853 | validation: 0.07316563451909716]
	TIME [epoch: 7.83 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03716992191927254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03716992191927254 | validation: 0.07115292468585913]
	TIME [epoch: 7.85 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040685410986082894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040685410986082894 | validation: 0.06764493174439695]
	TIME [epoch: 7.83 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05864582600726493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05864582600726493 | validation: 0.2383560990867927]
	TIME [epoch: 7.85 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14397937950214862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14397937950214862 | validation: 0.2550732175152694]
	TIME [epoch: 7.83 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16755904938931016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16755904938931016 | validation: 0.12070735217580814]
	TIME [epoch: 7.88 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05173018316800123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05173018316800123 | validation: 0.08173691524793017]
	TIME [epoch: 7.85 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04779385449996226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04779385449996226 | validation: 0.1064270803129614]
	TIME [epoch: 7.84 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05741711553995856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05741711553995856 | validation: 0.18588719601342102]
	TIME [epoch: 7.85 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10387629991680393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10387629991680393 | validation: 0.13831922111856546]
	TIME [epoch: 7.85 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06657170993738562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06657170993738562 | validation: 0.07807463759485689]
	TIME [epoch: 7.85 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051310575545609576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051310575545609576 | validation: 0.09040196115136924]
	TIME [epoch: 7.86 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04037350316426184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04037350316426184 | validation: 0.06721265749777154]
	TIME [epoch: 7.85 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049865408956219694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049865408956219694 | validation: 0.12908289109706525]
	TIME [epoch: 7.92 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05732731603843382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05732731603843382 | validation: 0.06837937935040227]
	TIME [epoch: 7.85 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041902267743768075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041902267743768075 | validation: 0.09238633228500981]
	TIME [epoch: 7.87 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03506523209157975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03506523209157975 | validation: 0.051892162730009996]
	TIME [epoch: 7.85 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027335504327735176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027335504327735176 | validation: 0.04919247131828678]
	TIME [epoch: 7.86 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027674821456741133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027674821456741133 | validation: 0.06961511032778761]
	TIME [epoch: 7.86 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03726456851780464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03726456851780464 | validation: 0.07668790334857783]
	TIME [epoch: 7.86 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08581324939348088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08581324939348088 | validation: 0.16688714834962073]
	TIME [epoch: 7.84 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14550869708213054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14550869708213054 | validation: 0.07246023130214986]
	TIME [epoch: 7.84 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04296829589727855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04296829589727855 | validation: 0.0964161833601858]
	TIME [epoch: 7.83 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06059288807739968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06059288807739968 | validation: 0.12943549805353502]
	TIME [epoch: 7.88 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10651058141455706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10651058141455706 | validation: 0.20971509999568916]
	TIME [epoch: 7.85 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1321668682259593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1321668682259593 | validation: 0.22621477948458774]
	TIME [epoch: 7.87 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15011812023775695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15011812023775695 | validation: 0.17138137592700756]
	TIME [epoch: 7.84 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10944983702242775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10944983702242775 | validation: 0.08178581040809234]
	TIME [epoch: 7.86 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06857684299139909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06857684299139909 | validation: 0.1304947854777026]
	TIME [epoch: 7.84 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07719612229396942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07719612229396942 | validation: 0.1072930192804619]
	TIME [epoch: 7.85 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050676922114324556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050676922114324556 | validation: 0.07042278798008204]
	TIME [epoch: 7.85 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0346096120843313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0346096120843313 | validation: 0.06285219279901248]
	TIME [epoch: 7.86 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02974215194190017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02974215194190017 | validation: 0.05060882629931495]
	TIME [epoch: 7.84 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028769812382067043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028769812382067043 | validation: 0.05945805807034679]
	TIME [epoch: 7.84 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026567473730134558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026567473730134558 | validation: 0.03981554804717648]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_1309.pth
	Model improved!!!
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0239464236018848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0239464236018848 | validation: 0.05312415298918938]
	TIME [epoch: 7.85 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02516399696692763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02516399696692763 | validation: 0.042370107524814175]
	TIME [epoch: 7.86 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03721794593908031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03721794593908031 | validation: 0.17106382191770805]
	TIME [epoch: 7.85 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10194135748853542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10194135748853542 | validation: 0.149467629110653]
	TIME [epoch: 7.85 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13311683658658063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13311683658658063 | validation: 0.13096195005456143]
	TIME [epoch: 7.85 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06484698168163602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06484698168163602 | validation: 0.10298559595098498]
	TIME [epoch: 7.88 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04265164375088858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04265164375088858 | validation: 0.07269418711056544]
	TIME [epoch: 7.9 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03669817757769112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03669817757769112 | validation: 0.061064019821939645]
	TIME [epoch: 7.86 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03307363512958849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03307363512958849 | validation: 0.08326066975584367]
	TIME [epoch: 7.86 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039854375357357176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039854375357357176 | validation: 0.08853602895850758]
	TIME [epoch: 7.86 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057929490770766014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057929490770766014 | validation: 0.18753689745768554]
	TIME [epoch: 7.84 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12140838170749535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12140838170749535 | validation: 0.18011515202058814]
	TIME [epoch: 7.86 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1414757091477955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1414757091477955 | validation: 0.17447309865372415]
	TIME [epoch: 7.85 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09723297338545844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09723297338545844 | validation: 0.15054538951796712]
	TIME [epoch: 7.86 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1083224192388119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1083224192388119 | validation: 0.15176327628374697]
	TIME [epoch: 7.85 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1088522044026526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1088522044026526 | validation: 0.22552471508560618]
	TIME [epoch: 7.85 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1729914875021888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1729914875021888 | validation: 0.20416185648852855]
	TIME [epoch: 7.84 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20495491996572618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20495491996572618 | validation: 0.08172440936600935]
	TIME [epoch: 7.84 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07913690431662729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07913690431662729 | validation: 0.07606497036774808]
	TIME [epoch: 7.85 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03890555950042174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03890555950042174 | validation: 0.0761675565610551]
	TIME [epoch: 7.86 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05064908933899835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05064908933899835 | validation: 0.0707691853928499]
	TIME [epoch: 7.85 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03528221452511643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03528221452511643 | validation: 0.05804314504231276]
	TIME [epoch: 7.86 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033580209717799826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033580209717799826 | validation: 0.060849349458145685]
	TIME [epoch: 7.83 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02924437435902582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02924437435902582 | validation: 0.046609143693033796]
	TIME [epoch: 7.84 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02634309374528095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02634309374528095 | validation: 0.054115759652165175]
	TIME [epoch: 7.86 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02560284088692716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02560284088692716 | validation: 0.045495768136880214]
	TIME [epoch: 7.85 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024172459203045102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024172459203045102 | validation: 0.051768342170479945]
	TIME [epoch: 7.84 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02465763278768396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02465763278768396 | validation: 0.051693684754714135]
	TIME [epoch: 7.79 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0386995993191851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0386995993191851 | validation: 0.16340962953267643]
	TIME [epoch: 7.79 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10113049137286914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10113049137286914 | validation: 0.1927511957684463]
	TIME [epoch: 7.84 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1595082221374175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1595082221374175 | validation: 0.13360648886204646]
	TIME [epoch: 7.85 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05249258069135751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05249258069135751 | validation: 0.11383178822474874]
	TIME [epoch: 7.8 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04827675145979928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04827675145979928 | validation: 0.11810946048492091]
	TIME [epoch: 7.77 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049272976191456624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049272976191456624 | validation: 0.06285766669802005]
	TIME [epoch: 7.85 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030748613142943877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030748613142943877 | validation: 0.07281071565333841]
	TIME [epoch: 7.75 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03256816464429716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03256816464429716 | validation: 0.05960212590084535]
	TIME [epoch: 7.78 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051281695789101106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051281695789101106 | validation: 0.14862064962180668]
	TIME [epoch: 7.8 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1031800490053291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1031800490053291 | validation: 0.05790505413896067]
	TIME [epoch: 7.85 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04326745996148556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04326745996148556 | validation: 0.04464365501159352]
	TIME [epoch: 7.86 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026771964097552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026771964097552 | validation: 0.06994682745804386]
	TIME [epoch: 7.83 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029352782823322193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029352782823322193 | validation: 0.04800600967803456]
	TIME [epoch: 7.82 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029747109379988885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029747109379988885 | validation: 0.08134521403533362]
	TIME [epoch: 7.82 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041615725712299385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041615725712299385 | validation: 0.10785801449090632]
	TIME [epoch: 7.85 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760635906100208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07760635906100208 | validation: 0.1976158848448738]
	TIME [epoch: 7.85 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14667435066818993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14667435066818993 | validation: 0.23514678362627578]
	TIME [epoch: 7.84 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15684257408632976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15684257408632976 | validation: 0.4876982960558343]
	TIME [epoch: 7.84 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2604522352049325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2604522352049325 | validation: 0.08930674082127033]
	TIME [epoch: 7.83 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06350054425325524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06350054425325524 | validation: 0.11619280393912766]
	TIME [epoch: 7.85 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07258367293854834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07258367293854834 | validation: 0.06112640959483623]
	TIME [epoch: 7.84 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04514899905306633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04514899905306633 | validation: 0.07217216846809751]
	TIME [epoch: 7.83 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036036438197927095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036036438197927095 | validation: 0.07916670419444799]
	TIME [epoch: 7.84 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03245052898501592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03245052898501592 | validation: 0.0538622721512168]
	TIME [epoch: 7.85 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028898800822742727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028898800822742727 | validation: 0.05766123979388208]
	TIME [epoch: 7.84 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02343745089858859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02343745089858859 | validation: 0.04952058932032055]
	TIME [epoch: 7.85 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021932374510628953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021932374510628953 | validation: 0.04255225950159141]
	TIME [epoch: 7.83 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022403220435914015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022403220435914015 | validation: 0.04655510067370044]
	TIME [epoch: 7.84 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02313436788157584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02313436788157584 | validation: 0.043502212615971786]
	TIME [epoch: 7.85 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023278925345968197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023278925345968197 | validation: 0.05702704420487036]
	TIME [epoch: 7.83 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037203116398056284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037203116398056284 | validation: 0.08352260846952031]
	TIME [epoch: 7.85 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07592578574149124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07592578574149124 | validation: 0.13603888837631614]
	TIME [epoch: 7.83 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09779721317337917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09779721317337917 | validation: 0.07548071720813554]
	TIME [epoch: 7.86 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045000620512618356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045000620512618356 | validation: 0.1304215088463836]
	TIME [epoch: 7.85 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07769327467038446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07769327467038446 | validation: 0.25914862060423993]
	TIME [epoch: 7.9 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23220460730902764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23220460730902764 | validation: 0.165426275795033]
	TIME [epoch: 7.84 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11628676080931107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11628676080931107 | validation: 0.16021044162012893]
	TIME [epoch: 7.84 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18197366398185622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18197366398185622 | validation: 0.179137418932514]
	TIME [epoch: 7.84 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17174758742124485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17174758742124485 | validation: 0.2676497038447189]
	TIME [epoch: 7.84 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15226290613492752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15226290613492752 | validation: 0.22192725280457104]
	TIME [epoch: 7.85 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12240588145838556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12240588145838556 | validation: 0.12496460074849605]
	TIME [epoch: 7.84 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0625198259630845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0625198259630845 | validation: 0.08339486409591171]
	TIME [epoch: 7.83 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0475394146014733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0475394146014733 | validation: 0.07985005603497831]
	TIME [epoch: 7.84 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04033654733907297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04033654733907297 | validation: 0.06925513231296]
	TIME [epoch: 7.84 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032831369827546523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032831369827546523 | validation: 0.05560568769161939]
	TIME [epoch: 7.84 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029927834304895002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029927834304895002 | validation: 0.05884129368352273]
	TIME [epoch: 7.85 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0285108350202629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0285108350202629 | validation: 0.053747938617483094]
	TIME [epoch: 7.85 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02693953222329079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02693953222329079 | validation: 0.06894478540713146]
	TIME [epoch: 7.83 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02981320660379702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02981320660379702 | validation: 0.0736651229918273]
	TIME [epoch: 7.84 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0460130270845378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0460130270845378 | validation: 0.15467337408481718]
	TIME [epoch: 7.84 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09610838891833139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09610838891833139 | validation: 0.0846196965474321]
	TIME [epoch: 7.87 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04672327429668826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04672327429668826 | validation: 0.06965839398376797]
	TIME [epoch: 7.85 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03707548854396469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03707548854396469 | validation: 0.12889857038783328]
	TIME [epoch: 7.85 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059063553557077986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059063553557077986 | validation: 0.07132251014964831]
	TIME [epoch: 7.84 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037026800632736515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037026800632736515 | validation: 0.06442126771562579]
	TIME [epoch: 7.84 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029808579806300427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029808579806300427 | validation: 0.06825586137740024]
	TIME [epoch: 7.84 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029058242197665462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029058242197665462 | validation: 0.04934200530798416]
	TIME [epoch: 7.83 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04075417137640867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04075417137640867 | validation: 0.14538072453836567]
	TIME [epoch: 7.84 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07984534112702249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07984534112702249 | validation: 0.12433819424299403]
	TIME [epoch: 7.85 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09060699770142584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09060699770142584 | validation: 0.11590393696878208]
	TIME [epoch: 7.83 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05949545860799134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05949545860799134 | validation: 0.04027429459847029]
	TIME [epoch: 7.85 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027149523003314257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027149523003314257 | validation: 0.052685451070400624]
	TIME [epoch: 7.84 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027164482752986902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027164482752986902 | validation: 0.05735075312517738]
	TIME [epoch: 7.85 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048565919201749866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048565919201749866 | validation: 0.11749822273394997]
	TIME [epoch: 7.86 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09779737838127801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09779737838127801 | validation: 0.05783841834440853]
	TIME [epoch: 7.84 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052653172651001084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052653172651001084 | validation: 0.07316046661660393]
	TIME [epoch: 7.84 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041951925901956344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041951925901956344 | validation: 0.14397608042846952]
	TIME [epoch: 7.84 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07430879478257127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07430879478257127 | validation: 0.17890406518225294]
	TIME [epoch: 7.84 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13967372451613577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13967372451613577 | validation: 0.06318616697331646]
	TIME [epoch: 7.91 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04723000887965032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04723000887965032 | validation: 0.07593957061171652]
	TIME [epoch: 7.85 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04558846612609063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04558846612609063 | validation: 0.14613264155338898]
	TIME [epoch: 7.83 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06193790530604695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06193790530604695 | validation: 0.13089883769388747]
	TIME [epoch: 7.82 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060099250471592126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060099250471592126 | validation: 0.08617356995789364]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240826_102013/states/model_phi1_4a_v_mmd1_1410.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6027.458 seconds.
