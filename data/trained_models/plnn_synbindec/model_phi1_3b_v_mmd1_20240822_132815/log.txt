Args:
Namespace(name='model_phi1_3b_v_mmd1', outdir='out/model_training/model_phi1_3b_v_mmd1', training_data='data/training_data/data_phi1_3b/training', validation_data='data/training_data/data_phi1_3b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4113069148

Training model...

Saving initial model state to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.023250863331904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.023250863331904 | validation: 4.26240864287428]
	TIME [epoch: 27.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9853342694099583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9853342694099583 | validation: 4.57207249972541]
	TIME [epoch: 1.9 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.290247266804622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.290247266804622 | validation: 2.891990717773212]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.38649721468032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.38649721468032 | validation: 5.434425113584045]
	TIME [epoch: 1.89 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.335671314631925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.335671314631925 | validation: 2.780835330170547]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9599856143476972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9599856143476972 | validation: 2.860880457556142]
	TIME [epoch: 1.88 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.107911017471039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.107911017471039 | validation: 2.5762137836591648]
	TIME [epoch: 1.94 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6222865225371095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6222865225371095 | validation: 1.9852413267870685]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.223093771784643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.223093771784643 | validation: 1.2750786162098184]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.601235492460724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.601235492460724 | validation: 1.1137244105086006]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2824805217077815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2824805217077815 | validation: 2.6910917554701728]
	TIME [epoch: 1.88 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6034321802564024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6034321802564024 | validation: 1.7236602181565812]
	TIME [epoch: 1.88 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6456261684438491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6456261684438491 | validation: 1.0182584681805524]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1858421480643744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1858421480643744 | validation: 1.1934400557984648]
	TIME [epoch: 1.89 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3126393815481805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3126393815481805 | validation: 0.842539440403834]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.052740413711366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.052740413711366 | validation: 0.8435518937586637]
	TIME [epoch: 1.88 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0180029950561424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0180029950561424 | validation: 0.8578820876444786]
	TIME [epoch: 1.89 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0432652963506983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0432652963506983 | validation: 0.8407300143154999]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0157501047292659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0157501047292659 | validation: 0.797485851537505]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9793394033331816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9793394033331816 | validation: 0.7810348360550345]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9687130179744756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9687130179744756 | validation: 0.8271085793675192]
	TIME [epoch: 1.89 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9596365140247021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9596365140247021 | validation: 0.7778009332418995]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9488084396067462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9488084396067462 | validation: 0.7746321802010914]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9256658684900313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9256658684900313 | validation: 0.8083304399924143]
	TIME [epoch: 1.88 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.924109746939206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.924109746939206 | validation: 0.7961316033785522]
	TIME [epoch: 1.88 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9307025034563896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9307025034563896 | validation: 0.8300027209425308]
	TIME [epoch: 1.9 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9490020335210605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9490020335210605 | validation: 0.9492299650686236]
	TIME [epoch: 1.87 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0634900036906045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0634900036906045 | validation: 0.9937981517194396]
	TIME [epoch: 1.88 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1239943851255825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1239943851255825 | validation: 0.8404363237332405]
	TIME [epoch: 1.88 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.982681222726318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.982681222726318 | validation: 0.7257053968886742]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8827927218555921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8827927218555921 | validation: 0.7100926470442479]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8413166458074228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8413166458074228 | validation: 0.6935821041670992]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8336937382050142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8336937382050142 | validation: 0.7025138265302223]
	TIME [epoch: 1.88 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8309428657076359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8309428657076359 | validation: 0.7303373097357536]
	TIME [epoch: 1.89 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8603950431474661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8603950431474661 | validation: 0.7970051701390795]
	TIME [epoch: 1.9 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9094246684145673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9094246684145673 | validation: 1.004064610841332]
	TIME [epoch: 1.89 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0696565057957208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0696565057957208 | validation: 0.8265825001671321]
	TIME [epoch: 1.89 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.870717322429608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.870717322429608 | validation: 0.735574260560041]
	TIME [epoch: 1.88 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8319941824613531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8319941824613531 | validation: 0.8325495243797807]
	TIME [epoch: 1.89 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8988015046696006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8988015046696006 | validation: 0.8127028895503543]
	TIME [epoch: 1.88 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9205872196421834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9205872196421834 | validation: 0.82593533711749]
	TIME [epoch: 1.9 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8712294279106716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8712294279106716 | validation: 0.6946126331405332]
	TIME [epoch: 1.9 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7958522353845789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7958522353845789 | validation: 0.6967853355728111]
	TIME [epoch: 1.89 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694950606178195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7694950606178195 | validation: 0.6931599749419424]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7629217042177584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7629217042177584 | validation: 0.6995622533102493]
	TIME [epoch: 1.88 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657683193689079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7657683193689079 | validation: 0.7058253781673987]
	TIME [epoch: 1.89 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7590593303198871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7590593303198871 | validation: 0.6872825195140685]
	TIME [epoch: 2.13 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756140984489671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.756140984489671 | validation: 0.7071013892586954]
	TIME [epoch: 1.9 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.76345170608549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.76345170608549 | validation: 0.7099695916646369]
	TIME [epoch: 1.9 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869635458002151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7869635458002151 | validation: 0.908143672511116]
	TIME [epoch: 1.89 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9403347402184525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9403347402184525 | validation: 0.8831325589704342]
	TIME [epoch: 1.89 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9556972507755326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9556972507755326 | validation: 0.8554344321173496]
	TIME [epoch: 1.89 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8764612645378728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8764612645378728 | validation: 0.6960444185618533]
	TIME [epoch: 1.89 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7712428756382907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7712428756382907 | validation: 1.034690878845301]
	TIME [epoch: 1.89 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9232741260233831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9232741260233831 | validation: 0.7505512949586599]
	TIME [epoch: 1.88 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8038968696667854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8038968696667854 | validation: 0.7028168976984083]
	TIME [epoch: 1.9 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7457842437308142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7457842437308142 | validation: 0.6961545294088795]
	TIME [epoch: 1.89 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7543068876860483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7543068876860483 | validation: 0.7195977659363586]
	TIME [epoch: 1.89 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7612902728806324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7612902728806324 | validation: 0.682701563514081]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7636282909038482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7636282909038482 | validation: 0.7772739441888569]
	TIME [epoch: 1.88 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7830393953590082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7830393953590082 | validation: 0.7362543192840937]
	TIME [epoch: 1.89 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7919488073620595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7919488073620595 | validation: 0.8804707666051332]
	TIME [epoch: 1.88 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831919205588542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.831919205588542 | validation: 0.7184920912226403]
	TIME [epoch: 1.89 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.750241268202457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.750241268202457 | validation: 0.7076145228248915]
	TIME [epoch: 1.88 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7255192582215131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7255192582215131 | validation: 0.7086187797237484]
	TIME [epoch: 1.88 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7223172466367698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7223172466367698 | validation: 0.7204500355672779]
	TIME [epoch: 1.89 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7388543058498679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7388543058498679 | validation: 0.7505172263160109]
	TIME [epoch: 1.88 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7427418535764378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7427418535764378 | validation: 0.7497052721521338]
	TIME [epoch: 1.89 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7738117889887317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7738117889887317 | validation: 0.7502621611633473]
	TIME [epoch: 1.88 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7616384442699708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7616384442699708 | validation: 0.8178423720700256]
	TIME [epoch: 1.89 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7965089175976772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7965089175976772 | validation: 0.7852807354621464]
	TIME [epoch: 1.9 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8273373946870842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8273373946870842 | validation: 0.9187006157569346]
	TIME [epoch: 1.9 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8611208225316735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8611208225316735 | validation: 0.759110931658707]
	TIME [epoch: 1.89 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.779459871803162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.779459871803162 | validation: 0.8232431484784137]
	TIME [epoch: 1.89 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688486925751649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7688486925751649 | validation: 0.7338339260591011]
	TIME [epoch: 1.89 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.751355156003507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.751355156003507 | validation: 0.7194289161208834]
	TIME [epoch: 1.88 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7308585152629926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7308585152629926 | validation: 0.7549851503012331]
	TIME [epoch: 1.9 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7311264126091084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7311264126091084 | validation: 0.7136731730946755]
	TIME [epoch: 1.88 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7302009202785785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7302009202785785 | validation: 0.79442561116175]
	TIME [epoch: 1.89 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7391538985188294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7391538985188294 | validation: 0.7494740982699435]
	TIME [epoch: 1.89 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7491744440197996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7491744440197996 | validation: 0.7977804277044038]
	TIME [epoch: 1.88 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7516562808206109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7516562808206109 | validation: 0.7267897672940143]
	TIME [epoch: 1.89 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7372224146808177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7372224146808177 | validation: 0.7623570110684205]
	TIME [epoch: 1.89 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7503397506823256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7503397506823256 | validation: 0.7821848768267512]
	TIME [epoch: 1.89 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941095550216641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7941095550216641 | validation: 0.8464860729753996]
	TIME [epoch: 1.89 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8329478690278266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8329478690278266 | validation: 0.7196691840315124]
	TIME [epoch: 1.89 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7209438914661204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7209438914661204 | validation: 0.7276807571348457]
	TIME [epoch: 1.89 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7109419726873096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7109419726873096 | validation: 0.7426283332192679]
	TIME [epoch: 1.89 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7444042421992013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7444042421992013 | validation: 0.7756367435071667]
	TIME [epoch: 1.89 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7348848325513029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7348848325513029 | validation: 0.7370334293744243]
	TIME [epoch: 1.89 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7409707172472386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7409707172472386 | validation: 0.9670623726132388]
	TIME [epoch: 1.89 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8391844715358485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8391844715358485 | validation: 0.8173946928984904]
	TIME [epoch: 1.89 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8546813847590095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8546813847590095 | validation: 0.8588180848939242]
	TIME [epoch: 1.89 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8367596995115707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8367596995115707 | validation: 0.7346774233214559]
	TIME [epoch: 1.89 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.707190005665368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.707190005665368 | validation: 0.7253993877138014]
	TIME [epoch: 1.9 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7084925398328296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7084925398328296 | validation: 0.7612498743264804]
	TIME [epoch: 1.9 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7559059574927693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7559059574927693 | validation: 0.7175057832384795]
	TIME [epoch: 1.89 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7145813933666065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7145813933666065 | validation: 0.7796105669499842]
	TIME [epoch: 1.9 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7216766455454584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7216766455454584 | validation: 0.7426896314420272]
	TIME [epoch: 1.89 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7295746654212437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7295746654212437 | validation: 0.8672099876611172]
	TIME [epoch: 1.89 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7780091945844541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7780091945844541 | validation: 0.7371913262532831]
	TIME [epoch: 1.89 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7445804513117298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7445804513117298 | validation: 0.775260028636105]
	TIME [epoch: 1.88 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7154813965639858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7154813965639858 | validation: 0.7378261886263359]
	TIME [epoch: 1.89 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7306658119249485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7306658119249485 | validation: 0.8038434347424241]
	TIME [epoch: 1.88 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7742293870313643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7742293870313643 | validation: 0.7504308504324533]
	TIME [epoch: 1.88 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.753334762357908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.753334762357908 | validation: 0.8192922734302673]
	TIME [epoch: 1.88 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7456727844961616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7456727844961616 | validation: 0.7302958626625116]
	TIME [epoch: 1.88 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7196642227198096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7196642227198096 | validation: 0.7882982939203015]
	TIME [epoch: 1.89 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7206950404157465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7206950404157465 | validation: 0.7227654413319452]
	TIME [epoch: 1.88 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7118635057095815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7118635057095815 | validation: 0.7763857493285218]
	TIME [epoch: 1.89 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7015591891974559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7015591891974559 | validation: 0.7132241461348386]
	TIME [epoch: 1.89 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085034463614673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7085034463614673 | validation: 0.8273373653093956]
	TIME [epoch: 1.88 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7425043299490508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7425043299490508 | validation: 0.766957469528492]
	TIME [epoch: 1.88 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.763304107205432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.763304107205432 | validation: 0.8366877925042225]
	TIME [epoch: 1.88 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7527598043468604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7527598043468604 | validation: 0.7153351469577353]
	TIME [epoch: 1.89 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066249121125966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066249121125966 | validation: 0.7979099150242634]
	TIME [epoch: 1.88 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7742334197442609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7742334197442609 | validation: 0.8139026087197383]
	TIME [epoch: 1.89 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8086696515283012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8086696515283012 | validation: 0.8486463960644213]
	TIME [epoch: 1.89 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080954792052062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8080954792052062 | validation: 0.7320491059600338]
	TIME [epoch: 1.89 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7140638199207772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7140638199207772 | validation: 0.7898843864208207]
	TIME [epoch: 1.89 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7163699574677793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7163699574677793 | validation: 0.6996177161674842]
	TIME [epoch: 1.88 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6897412886895424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6897412886895424 | validation: 0.707493147171973]
	TIME [epoch: 1.89 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761817073857558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6761817073857558 | validation: 0.7150413443283962]
	TIME [epoch: 1.87 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6694804071157873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6694804071157873 | validation: 0.683449868355702]
	TIME [epoch: 1.89 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6691366335438881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6691366335438881 | validation: 0.7599951279245871]
	TIME [epoch: 1.88 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6884180927180183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6884180927180183 | validation: 0.8811711445910533]
	TIME [epoch: 1.88 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.907546990855733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.907546990855733 | validation: 1.236028600187886]
	TIME [epoch: 1.88 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131099355039633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.131099355039633 | validation: 0.7316842055646797]
	TIME [epoch: 1.88 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7023431955183093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7023431955183093 | validation: 0.7662504227989597]
	TIME [epoch: 1.88 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7673836069454874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7673836069454874 | validation: 0.8013306747838546]
	TIME [epoch: 1.88 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7409099784130398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7409099784130398 | validation: 0.7037322822664618]
	TIME [epoch: 1.88 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6803264757176658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6803264757176658 | validation: 0.7226009905260458]
	TIME [epoch: 1.89 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6893610607108718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6893610607108718 | validation: 0.712509325174158]
	TIME [epoch: 1.88 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687072226748064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687072226748064 | validation: 0.691503528669386]
	TIME [epoch: 1.88 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781404653508609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6781404653508609 | validation: 0.7573315207743194]
	TIME [epoch: 1.89 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6953429173385388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6953429173385388 | validation: 0.7078076106701036]
	TIME [epoch: 1.89 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6879372407206162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6879372407206162 | validation: 0.7325728306246659]
	TIME [epoch: 1.88 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6802146525009871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6802146525009871 | validation: 0.6807537527662562]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6835941656357393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6835941656357393 | validation: 0.8361864852465246]
	TIME [epoch: 1.88 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7601272485909474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7601272485909474 | validation: 0.8054727780245383]
	TIME [epoch: 1.89 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8110383149690833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8110383149690833 | validation: 0.8563193383780535]
	TIME [epoch: 1.89 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7807493819862961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7807493819862961 | validation: 0.6826535048493373]
	TIME [epoch: 1.88 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6722041764667298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6722041764667298 | validation: 0.6957545533373883]
	TIME [epoch: 1.88 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6618941342238287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6618941342238287 | validation: 0.7661912218765619]
	TIME [epoch: 1.88 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.692387144634257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.692387144634257 | validation: 0.6907964776962752]
	TIME [epoch: 1.89 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6820531201389016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6820531201389016 | validation: 0.7028743860256021]
	TIME [epoch: 1.88 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6577160923236184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6577160923236184 | validation: 0.6563919765733562]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6478329585997732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6478329585997732 | validation: 0.6751689750863116]
	TIME [epoch: 1.88 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403274587558004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6403274587558004 | validation: 0.6578199513524619]
	TIME [epoch: 1.88 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6383472471545956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6383472471545956 | validation: 0.7136593301572098]
	TIME [epoch: 1.89 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6493855318496177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6493855318496177 | validation: 0.7276223337443756]
	TIME [epoch: 1.89 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7346741812829878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7346741812829878 | validation: 0.9765610104550847]
	TIME [epoch: 1.89 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8098400185841835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8098400185841835 | validation: 0.8517845689006531]
	TIME [epoch: 1.87 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86935223039122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.86935223039122 | validation: 1.0077966249964303]
	TIME [epoch: 1.88 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9817077712131451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9817077712131451 | validation: 0.6678577359227819]
	TIME [epoch: 1.89 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.666373204898278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.666373204898278 | validation: 0.7276032162650323]
	TIME [epoch: 1.87 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7331200364200361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7331200364200361 | validation: 0.7865972806611863]
	TIME [epoch: 1.88 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7253274713504095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7253274713504095 | validation: 0.6744030644612518]
	TIME [epoch: 1.87 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603724688309294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6603724688309294 | validation: 0.6773561894589684]
	TIME [epoch: 1.88 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6509330215180993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6509330215180993 | validation: 0.6600810671984159]
	TIME [epoch: 1.88 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.639680556582219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.639680556582219 | validation: 0.6522673475760883]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6477983487915896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6477983487915896 | validation: 0.7600089806606238]
	TIME [epoch: 1.88 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6739321283727608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6739321283727608 | validation: 0.7336874951007376]
	TIME [epoch: 1.88 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.749954889665849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.749954889665849 | validation: 0.9178989458800837]
	TIME [epoch: 1.89 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8207883832627519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8207883832627519 | validation: 0.6682125696146969]
	TIME [epoch: 1.89 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6819842742165755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6819842742165755 | validation: 0.663703515314616]
	TIME [epoch: 1.89 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6334287192722183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6334287192722183 | validation: 0.6600828284429248]
	TIME [epoch: 1.9 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6200775414316951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6200775414316951 | validation: 0.6342918254805071]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6176394504942458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6176394504942458 | validation: 0.6662401974339094]
	TIME [epoch: 1.88 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.626441020756793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.626441020756793 | validation: 0.6563829701456922]
	TIME [epoch: 1.89 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6620604578644316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6620604578644316 | validation: 0.9267842848848885]
	TIME [epoch: 1.9 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031062658003632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8031062658003632 | validation: 0.7997096075208093]
	TIME [epoch: 1.89 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.836985647677986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.836985647677986 | validation: 0.7443314757475951]
	TIME [epoch: 1.89 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6920787178114516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6920787178114516 | validation: 0.6244683972600058]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6161742890367015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6161742890367015 | validation: 0.6121378026805472]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284440490179647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6284440490179647 | validation: 0.6902147135085066]
	TIME [epoch: 1.88 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6385774616566894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6385774616566894 | validation: 0.6198265022544215]
	TIME [epoch: 1.88 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459746662610464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6459746662610464 | validation: 0.7123039919559206]
	TIME [epoch: 1.88 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6670442006445574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6670442006445574 | validation: 0.6452189546905337]
	TIME [epoch: 1.88 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687679373569282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687679373569282 | validation: 0.779871296304919]
	TIME [epoch: 1.89 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7086060172069375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7086060172069375 | validation: 0.6485596560350296]
	TIME [epoch: 1.89 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6639636364687702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6639636364687702 | validation: 0.7483432384892257]
	TIME [epoch: 1.89 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6659133380080675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6659133380080675 | validation: 0.6428879507072862]
	TIME [epoch: 1.88 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.656631470020974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.656631470020974 | validation: 0.672578641338449]
	TIME [epoch: 1.88 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6178651519837426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6178651519837426 | validation: 0.5957784021055776]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6109253946104866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6109253946104866 | validation: 0.6900882102033464]
	TIME [epoch: 1.89 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6310896365333776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6310896365333776 | validation: 0.6230261721488788]
	TIME [epoch: 1.89 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6784888598230476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6784888598230476 | validation: 0.8325709517773219]
	TIME [epoch: 1.89 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7429367198768495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7429367198768495 | validation: 0.6434088501491715]
	TIME [epoch: 1.89 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.665709730233653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.665709730233653 | validation: 0.6512326673454323]
	TIME [epoch: 1.89 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6076187553849942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6076187553849942 | validation: 0.616901693891954]
	TIME [epoch: 1.88 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6270166738587856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6270166738587856 | validation: 0.6481795662093789]
	TIME [epoch: 1.89 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6139678525405146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6139678525405146 | validation: 0.5759171136439355]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5830258361778609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5830258361778609 | validation: 0.5476157954778021]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5537915903945297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5537915903945297 | validation: 0.5985120798827112]
	TIME [epoch: 1.89 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5523446947940646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5523446947940646 | validation: 0.5759451275333547]
	TIME [epoch: 1.89 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5993183618096297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5993183618096297 | validation: 0.9251506054893849]
	TIME [epoch: 1.89 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7756605700202936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7756605700202936 | validation: 0.704027509347238]
	TIME [epoch: 1.88 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756694785573789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.756694785573789 | validation: 0.6634669160593458]
	TIME [epoch: 1.88 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6205359877747689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6205359877747689 | validation: 0.5294821434209933]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5470653928101598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5470653928101598 | validation: 0.5514319904386584]
	TIME [epoch: 27 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5686062409028945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5686062409028945 | validation: 0.6680055482886597]
	TIME [epoch: 3.74 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6007604124280771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6007604124280771 | validation: 0.5359425418612441]
	TIME [epoch: 3.72 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5451524825744244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5451524825744244 | validation: 0.5298174353562844]
	TIME [epoch: 3.72 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5098764771924572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5098764771924572 | validation: 0.5118527923599879]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.503767972331841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.503767972331841 | validation: 0.6002099840296258]
	TIME [epoch: 3.74 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5736911798425685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5736911798425685 | validation: 0.5748819522356629]
	TIME [epoch: 3.73 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6106326491784839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6106326491784839 | validation: 0.6083600438911686]
	TIME [epoch: 3.76 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5763436334255211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5763436334255211 | validation: 0.4812402689895066]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49693463068017474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49693463068017474 | validation: 0.5352432742401695]
	TIME [epoch: 3.76 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49031555538478006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49031555538478006 | validation: 0.5728745683769646]
	TIME [epoch: 3.74 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6095322052852921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6095322052852921 | validation: 0.9415464697515971]
	TIME [epoch: 3.74 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8112286688689195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8112286688689195 | validation: 0.5709268430407505]
	TIME [epoch: 3.73 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5841760841301616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5841760841301616 | validation: 0.4903782203633372]
	TIME [epoch: 3.71 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47309999772319744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47309999772319744 | validation: 0.5549755752162387]
	TIME [epoch: 3.73 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5393239871715215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5393239871715215 | validation: 0.5295596648922164]
	TIME [epoch: 3.73 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5525926545377886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5525926545377886 | validation: 0.47840654697756674]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4681932650609161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4681932650609161 | validation: 0.4644079699542131]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4331375601967314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4331375601967314 | validation: 0.4486801840990413]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4351346631470163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4351346631470163 | validation: 0.5737426056872581]
	TIME [epoch: 3.74 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5183466744472952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5183466744472952 | validation: 0.7775885071983568]
	TIME [epoch: 3.73 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7736577339253594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7736577339253594 | validation: 0.8685326911367863]
	TIME [epoch: 3.72 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7101472498979507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7101472498979507 | validation: 0.48166773182787637]
	TIME [epoch: 3.74 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46835373218342896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46835373218342896 | validation: 0.589909682060289]
	TIME [epoch: 3.74 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.615974587253698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.615974587253698 | validation: 0.5056545110531158]
	TIME [epoch: 3.74 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47334779231586976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47334779231586976 | validation: 0.4403579634111234]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43319596601086274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43319596601086274 | validation: 0.4699308573829468]
	TIME [epoch: 3.72 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4663822323261495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4663822323261495 | validation: 0.4733089254861625]
	TIME [epoch: 3.74 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48582929061400015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48582929061400015 | validation: 0.48840161666166915]
	TIME [epoch: 3.74 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.465462850061136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.465462850061136 | validation: 0.450456702933162]
	TIME [epoch: 3.73 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4523281787379705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4523281787379705 | validation: 0.5140038038026723]
	TIME [epoch: 3.74 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44868952040333326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44868952040333326 | validation: 0.45816748828878434]
	TIME [epoch: 3.72 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44451469705548763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44451469705548763 | validation: 0.4760362309854381]
	TIME [epoch: 3.73 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4212932057951588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4212932057951588 | validation: 0.4342421772404171]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4253018345292459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4253018345292459 | validation: 0.48773944376103273]
	TIME [epoch: 3.73 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.527261558594051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.527261558594051 | validation: 0.5314450549928876]
	TIME [epoch: 3.73 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.526266619154759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.526266619154759 | validation: 0.4173023937947787]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40718702401488843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40718702401488843 | validation: 0.5382345028578307]
	TIME [epoch: 3.73 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48529136828407676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48529136828407676 | validation: 0.4928058596750915]
	TIME [epoch: 3.75 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4820954424218604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4820954424218604 | validation: 0.6034485191406903]
	TIME [epoch: 3.72 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5354727780983858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5354727780983858 | validation: 0.39184933229612784]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36653415874214945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36653415874214945 | validation: 0.385759009551047]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34706035344759334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34706035344759334 | validation: 0.40317470793342497]
	TIME [epoch: 3.74 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3494008358408551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3494008358408551 | validation: 0.4296381889246548]
	TIME [epoch: 3.74 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37880029155350775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37880029155350775 | validation: 0.5285615344048944]
	TIME [epoch: 3.72 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4340835019708528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4340835019708528 | validation: 0.5158887407329905]
	TIME [epoch: 3.75 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45178158560303533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45178158560303533 | validation: 0.45758343391703954]
	TIME [epoch: 3.76 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38555362201333526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38555362201333526 | validation: 0.37653310173038945]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3232471551341318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3232471551341318 | validation: 0.3634261046987967]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_249.pth
	Model improved!!!
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3308138659945823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3308138659945823 | validation: 0.7360006551222725]
	TIME [epoch: 3.72 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7886221289421699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7886221289421699 | validation: 0.7136194919186721]
	TIME [epoch: 3.73 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6034166732887717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6034166732887717 | validation: 0.37043622640223484]
	TIME [epoch: 3.73 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3486384035154812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3486384035154812 | validation: 0.39340394430554276]
	TIME [epoch: 3.72 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35204151674409034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35204151674409034 | validation: 0.49864149557637827]
	TIME [epoch: 3.74 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41926494496348665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41926494496348665 | validation: 0.4329605669601366]
	TIME [epoch: 3.73 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4284691911604641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4284691911604641 | validation: 0.617051161682966]
	TIME [epoch: 3.73 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5643670407433851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5643670407433851 | validation: 0.40549447555675117]
	TIME [epoch: 3.73 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3536365762922922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3536365762922922 | validation: 0.4660079830531614]
	TIME [epoch: 3.72 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39491126793089676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39491126793089676 | validation: 0.4637478755311712]
	TIME [epoch: 3.84 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43585629381658997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43585629381658997 | validation: 0.40959099970557244]
	TIME [epoch: 3.74 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4143506958676408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4143506958676408 | validation: 0.4232358690461796]
	TIME [epoch: 3.74 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41188005888968493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41188005888968493 | validation: 0.43105210778117836]
	TIME [epoch: 3.73 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42424595392707476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42424595392707476 | validation: 0.36609850361398455]
	TIME [epoch: 3.73 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3307176377691351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3307176377691351 | validation: 0.3484660515945917]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31606114655919226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31606114655919226 | validation: 0.40261703342048644]
	TIME [epoch: 3.74 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33945708939773384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33945708939773384 | validation: 0.37336486269602265]
	TIME [epoch: 3.74 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3496373375897906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3496373375897906 | validation: 0.5428745743489058]
	TIME [epoch: 3.73 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46219975783609524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46219975783609524 | validation: 0.38290542243597864]
	TIME [epoch: 3.73 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3826164136722726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3826164136722726 | validation: 0.3668061560804483]
	TIME [epoch: 3.74 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3351743115644456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3351743115644456 | validation: 0.38002601307841605]
	TIME [epoch: 3.72 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3600744837079188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3600744837079188 | validation: 0.39216833738972023]
	TIME [epoch: 3.74 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3815731188066867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3815731188066867 | validation: 0.42443827402827283]
	TIME [epoch: 3.75 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.450373461141365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.450373461141365 | validation: 0.33920467527207043]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29962043617892997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29962043617892997 | validation: 0.3230572828074712]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25531532588063965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25531532588063965 | validation: 0.31750853186327577]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24940835688246302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24940835688246302 | validation: 0.31450448562146316]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2520761415668718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2520761415668718 | validation: 0.3889835922598228]
	TIME [epoch: 3.75 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3358389252683398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3358389252683398 | validation: 1.2681054658793205]
	TIME [epoch: 3.76 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9184699801822137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9184699801822137 | validation: 0.3045367284796169]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25883630695680804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25883630695680804 | validation: 0.7632935864853758]
	TIME [epoch: 3.75 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8032751049542018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8032751049542018 | validation: 0.4404064460358774]
	TIME [epoch: 3.75 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39878423327417556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39878423327417556 | validation: 0.5283410017538617]
	TIME [epoch: 3.75 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4948479710202495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4948479710202495 | validation: 0.3460461760637505]
	TIME [epoch: 3.75 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3005658932998341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3005658932998341 | validation: 0.30875315717808766]
	TIME [epoch: 3.74 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.243018950698137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.243018950698137 | validation: 0.30980016460183485]
	TIME [epoch: 3.74 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2572572967489358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2572572967489358 | validation: 0.3166512196848001]
	TIME [epoch: 3.74 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28629118854586255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28629118854586255 | validation: 0.3546218871018987]
	TIME [epoch: 3.75 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3355908087679435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3355908087679435 | validation: 0.37614120646260923]
	TIME [epoch: 3.74 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35142715019739657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35142715019739657 | validation: 0.3453238849003345]
	TIME [epoch: 3.74 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28407405281899645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28407405281899645 | validation: 0.32097089651229577]
	TIME [epoch: 3.75 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2761230534625333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2761230534625333 | validation: 0.3499107216363804]
	TIME [epoch: 3.74 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.309991760107255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.309991760107255 | validation: 0.35761258537556456]
	TIME [epoch: 3.74 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35447507164098435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35447507164098435 | validation: 0.4034907735366644]
	TIME [epoch: 3.74 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3289040361588298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3289040361588298 | validation: 0.30218763854801467]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.251157567568281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.251157567568281 | validation: 0.37248178410787347]
	TIME [epoch: 3.76 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28055540486699715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28055540486699715 | validation: 0.38722425425266427]
	TIME [epoch: 3.74 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.318557800169167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.318557800169167 | validation: 0.5022162051566097]
	TIME [epoch: 3.74 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3957712590030933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3957712590030933 | validation: 0.30128598624527575]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27374505083658685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27374505083658685 | validation: 0.3722356474986682]
	TIME [epoch: 3.75 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3337391858876442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3337391858876442 | validation: 0.38407446222577085]
	TIME [epoch: 3.74 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3800544557315944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3800544557315944 | validation: 0.3184150863687279]
	TIME [epoch: 3.73 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25216527390394483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25216527390394483 | validation: 0.2736087551092807]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21600466280671612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21600466280671612 | validation: 0.3092179120930023]
	TIME [epoch: 3.72 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24756952987238343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24756952987238343 | validation: 0.3147305148709721]
	TIME [epoch: 3.72 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2962273638914955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2962273638914955 | validation: 0.6017914142638375]
	TIME [epoch: 3.73 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4637349760770322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4637349760770322 | validation: 0.27689935700241314]
	TIME [epoch: 3.72 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23258441022790755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23258441022790755 | validation: 0.26228136056707085]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19408764386804656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19408764386804656 | validation: 0.25322232942673506]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19265288959887966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19265288959887966 | validation: 0.26714623774517793]
	TIME [epoch: 3.71 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19788294445652027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19788294445652027 | validation: 0.27683617345499745]
	TIME [epoch: 3.72 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22477747471702184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22477747471702184 | validation: 0.37015277040272443]
	TIME [epoch: 3.71 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3161377275965868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3161377275965868 | validation: 0.44860986326524266]
	TIME [epoch: 3.72 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5008756170106338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5008756170106338 | validation: 0.45060807317054263]
	TIME [epoch: 3.71 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33572560588062644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33572560588062644 | validation: 0.37710984526887464]
	TIME [epoch: 3.73 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3516020521183187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3516020521183187 | validation: 0.576687768622585]
	TIME [epoch: 3.71 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45434121250412224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45434121250412224 | validation: 0.2782487678216812]
	TIME [epoch: 3.73 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22318610251414833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22318610251414833 | validation: 0.3269956501381614]
	TIME [epoch: 3.73 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24661936155519443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24661936155519443 | validation: 0.27810843043480116]
	TIME [epoch: 3.72 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2278692129505408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2278692129505408 | validation: 0.27276227666683683]
	TIME [epoch: 3.74 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2404727144691381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2404727144691381 | validation: 0.3662888401627258]
	TIME [epoch: 3.73 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31854054352084377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31854054352084377 | validation: 0.3088559696457349]
	TIME [epoch: 3.72 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28028298001961377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28028298001961377 | validation: 0.28912342048724815]
	TIME [epoch: 3.71 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2165832495773492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2165832495773492 | validation: 0.24401154714254478]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21541754576396457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21541754576396457 | validation: 0.372280550416894]
	TIME [epoch: 3.74 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26799164081903315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26799164081903315 | validation: 0.29011892023999214]
	TIME [epoch: 3.73 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2675857432782538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2675857432782538 | validation: 0.4400818597708459]
	TIME [epoch: 3.75 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31009128795157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31009128795157 | validation: 0.26948255409826727]
	TIME [epoch: 3.73 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21517822057249697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21517822057249697 | validation: 0.26155848690341627]
	TIME [epoch: 3.73 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20450831279760617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20450831279760617 | validation: 0.2493379957407937]
	TIME [epoch: 3.74 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2081368947168297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2081368947168297 | validation: 0.2691991129244921]
	TIME [epoch: 3.74 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2296443471922215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2296443471922215 | validation: 0.3230047674390213]
	TIME [epoch: 3.75 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3210847773286462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3210847773286462 | validation: 0.42711868891890675]
	TIME [epoch: 3.72 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29852816287660444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29852816287660444 | validation: 0.2506238756203965]
	TIME [epoch: 3.72 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18837123880693407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18837123880693407 | validation: 0.23022253767917444]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16681542360607574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16681542360607574 | validation: 0.23620863113266666]
	TIME [epoch: 3.74 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17304895653185923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17304895653185923 | validation: 0.25232392913271406]
	TIME [epoch: 3.73 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18233259426016094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18233259426016094 | validation: 0.2538742055342235]
	TIME [epoch: 3.72 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2233871310487588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2233871310487588 | validation: 0.7008353760601191]
	TIME [epoch: 3.73 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5298424162697932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5298424162697932 | validation: 0.2980541172280648]
	TIME [epoch: 3.73 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2802409299556646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2802409299556646 | validation: 0.26867400665254365]
	TIME [epoch: 3.72 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.211373555391369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.211373555391369 | validation: 0.21531134452135953]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17351768128479683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17351768128479683 | validation: 0.2718717956504858]
	TIME [epoch: 3.75 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20216735872577082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20216735872577082 | validation: 0.2806097939047361]
	TIME [epoch: 3.72 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21832718992597586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21832718992597586 | validation: 0.3191719806313063]
	TIME [epoch: 3.72 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24766574151093268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24766574151093268 | validation: 0.31710909336364124]
	TIME [epoch: 3.7 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36766307550646515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36766307550646515 | validation: 0.27975966545183256]
	TIME [epoch: 3.72 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2025595859138106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2025595859138106 | validation: 0.23827847302501098]
	TIME [epoch: 3.73 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18685601489334278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18685601489334278 | validation: 0.3315830785377971]
	TIME [epoch: 3.71 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2282930009206885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2282930009206885 | validation: 0.29993539731200447]
	TIME [epoch: 3.72 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2435461678360539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2435461678360539 | validation: 0.4069240557625496]
	TIME [epoch: 3.73 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.305689140269405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.305689140269405 | validation: 0.25619707240830797]
	TIME [epoch: 3.72 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2569960783729869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2569960783729869 | validation: 0.271968398603821]
	TIME [epoch: 3.72 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19619763770149157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19619763770149157 | validation: 0.2606613117367068]
	TIME [epoch: 3.71 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18818758743281955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18818758743281955 | validation: 0.23167209269453656]
	TIME [epoch: 3.73 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17929732319855077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17929732319855077 | validation: 0.27238442743609265]
	TIME [epoch: 3.74 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18736593534438942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18736593534438942 | validation: 0.2644741420939391]
	TIME [epoch: 3.72 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23632560488380472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23632560488380472 | validation: 0.4744158254387066]
	TIME [epoch: 3.72 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33691703006502655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33691703006502655 | validation: 0.23123058505276495]
	TIME [epoch: 3.73 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21093546003910205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21093546003910205 | validation: 0.23258987429916667]
	TIME [epoch: 3.73 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16503189018850628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16503189018850628 | validation: 0.24103089578961387]
	TIME [epoch: 3.72 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16182698391964556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16182698391964556 | validation: 0.2451404016955257]
	TIME [epoch: 3.71 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17404354453688253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17404354453688253 | validation: 0.23804303451478992]
	TIME [epoch: 3.73 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17399930690910043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17399930690910043 | validation: 0.25181730384903805]
	TIME [epoch: 3.72 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20230520026538248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20230520026538248 | validation: 0.41927731576404126]
	TIME [epoch: 3.71 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3171729626275951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3171729626275951 | validation: 0.29240124774015736]
	TIME [epoch: 3.71 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.362312880459763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.362312880459763 | validation: 0.2694809183095012]
	TIME [epoch: 3.72 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18788923137137734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18788923137137734 | validation: 0.21306664655045707]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14968824073221032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14968824073221032 | validation: 0.2122468809095402]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12990586890565634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12990586890565634 | validation: 0.19352041350353077]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12113586394855015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12113586394855015 | validation: 0.18619318092338943]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11959063244032259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11959063244032259 | validation: 0.20181720230771633]
	TIME [epoch: 3.7 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12308244039732028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12308244039732028 | validation: 0.20168272139461996]
	TIME [epoch: 3.71 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1401317400605427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1401317400605427 | validation: 0.29483676443721135]
	TIME [epoch: 3.7 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2588601979244972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2588601979244972 | validation: 0.9176144140641687]
	TIME [epoch: 3.71 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7511091180211774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7511091180211774 | validation: 0.40526033825914604]
	TIME [epoch: 3.7 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39117590886498493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39117590886498493 | validation: 0.25598255177287393]
	TIME [epoch: 3.71 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2169239374619547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2169239374619547 | validation: 0.21646718654788338]
	TIME [epoch: 3.72 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21166269132459836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21166269132459836 | validation: 0.20725141263440028]
	TIME [epoch: 3.72 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14672196395936868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14672196395936868 | validation: 0.19229875745379396]
	TIME [epoch: 3.7 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14148418612267746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14148418612267746 | validation: 0.18830860220466716]
	TIME [epoch: 3.73 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12459655453064118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12459655453064118 | validation: 0.17424004908277482]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1211903283087264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1211903283087264 | validation: 0.18499742364705876]
	TIME [epoch: 3.73 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12467426848172782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12467426848172782 | validation: 0.18191209729232544]
	TIME [epoch: 3.76 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13176944330274853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13176944330274853 | validation: 0.3349493132682326]
	TIME [epoch: 3.74 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25611001770371905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25611001770371905 | validation: 0.3133771084994734]
	TIME [epoch: 3.73 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4399972025143379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4399972025143379 | validation: 0.2596701726017955]
	TIME [epoch: 3.74 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18612935439106035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18612935439106035 | validation: 0.279725921952664]
	TIME [epoch: 3.72 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2168431664196848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2168431664196848 | validation: 0.31073466173869924]
	TIME [epoch: 3.75 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26388528538773726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26388528538773726 | validation: 0.2042490755179583]
	TIME [epoch: 3.75 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14109289857557428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14109289857557428 | validation: 0.18407145739740846]
	TIME [epoch: 3.75 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12101786089490073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12101786089490073 | validation: 0.2015555433635594]
	TIME [epoch: 3.74 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1276878241697001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1276878241697001 | validation: 0.2040684500320262]
	TIME [epoch: 3.74 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14726222343846934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14726222343846934 | validation: 0.2526466547002028]
	TIME [epoch: 3.73 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16920750209503552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16920750209503552 | validation: 0.2659143964016965]
	TIME [epoch: 3.75 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30172418816188457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30172418816188457 | validation: 0.49948903422466434]
	TIME [epoch: 3.71 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37502122374948044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37502122374948044 | validation: 0.19938073260634048]
	TIME [epoch: 3.75 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1340621219955498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1340621219955498 | validation: 0.19944101546332504]
	TIME [epoch: 3.75 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1409494531237658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1409494531237658 | validation: 0.21206232794507526]
	TIME [epoch: 3.73 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14700005129159738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14700005129159738 | validation: 0.182423312202925]
	TIME [epoch: 3.74 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11887635395866973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11887635395866973 | validation: 0.1764180720842115]
	TIME [epoch: 3.71 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11043587632558367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11043587632558367 | validation: 0.16451742319105978]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10711563741254372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10711563741254372 | validation: 0.20006051695568222]
	TIME [epoch: 3.74 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11861253460054742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11861253460054742 | validation: 0.2418678762393249]
	TIME [epoch: 3.75 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22699375939491503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22699375939491503 | validation: 0.468916466369705]
	TIME [epoch: 3.72 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3449249728962645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3449249728962645 | validation: 0.19484798793885774]
	TIME [epoch: 3.73 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1604229748223584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1604229748223584 | validation: 0.19089945716804407]
	TIME [epoch: 3.74 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14181058950257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14181058950257 | validation: 0.23796641013210773]
	TIME [epoch: 3.72 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20456999960162914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20456999960162914 | validation: 0.31896749582191614]
	TIME [epoch: 3.71 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27141562881496495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27141562881496495 | validation: 0.2400086378796072]
	TIME [epoch: 3.74 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1715491891213719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1715491891213719 | validation: 0.2007191739394193]
	TIME [epoch: 3.72 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11359891344658773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11359891344658773 | validation: 0.1940290619037085]
	TIME [epoch: 3.72 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13221492357407658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13221492357407658 | validation: 0.2679554386026386]
	TIME [epoch: 3.73 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19399636265519782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19399636265519782 | validation: 0.2745840577150891]
	TIME [epoch: 3.73 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3462367888748611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3462367888748611 | validation: 0.308485677518076]
	TIME [epoch: 3.75 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21830415029571984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21830415029571984 | validation: 0.17367604852520777]
	TIME [epoch: 3.72 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1074351044154643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1074351044154643 | validation: 0.20024876443832262]
	TIME [epoch: 3.84 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1482382495796139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1482382495796139 | validation: 0.24073663745538645]
	TIME [epoch: 3.72 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16181760951404356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16181760951404356 | validation: 0.16617785731478618]
	TIME [epoch: 3.73 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11007598456094854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11007598456094854 | validation: 0.16138973253603434]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_419.pth
	Model improved!!!
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09558654387722904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09558654387722904 | validation: 0.17138176511276262]
	TIME [epoch: 3.72 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10378018269282785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10378018269282785 | validation: 0.17142573620842214]
	TIME [epoch: 3.75 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12925222004293033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12925222004293033 | validation: 0.24790483047862255]
	TIME [epoch: 3.74 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17348302157669232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17348302157669232 | validation: 0.24261127136632998]
	TIME [epoch: 3.73 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24605586209223326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24605586209223326 | validation: 0.390513519021269]
	TIME [epoch: 3.73 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2645865731760212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2645865731760212 | validation: 0.3524554040943076]
	TIME [epoch: 3.73 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33424538251426283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33424538251426283 | validation: 0.2758970876254883]
	TIME [epoch: 3.76 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24462934169011838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24462934169011838 | validation: 0.19309321456860387]
	TIME [epoch: 3.73 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1346970478607252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1346970478607252 | validation: 0.1929821657805186]
	TIME [epoch: 3.72 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1516963371255088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1516963371255088 | validation: 0.19209662875145833]
	TIME [epoch: 3.72 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12643811113313258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12643811113313258 | validation: 0.2037615617011137]
	TIME [epoch: 3.73 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12776756865434755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12776756865434755 | validation: 0.17518590929173947]
	TIME [epoch: 3.73 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11261806757234183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11261806757234183 | validation: 0.2208931408063175]
	TIME [epoch: 3.73 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14068430223028155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14068430223028155 | validation: 0.24140297999404103]
	TIME [epoch: 3.73 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20989953620197344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20989953620197344 | validation: 0.3032669464487562]
	TIME [epoch: 3.74 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2162878034183511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2162878034183511 | validation: 0.18891664997510238]
	TIME [epoch: 3.74 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1868053766418285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1868053766418285 | validation: 0.17767486838587485]
	TIME [epoch: 3.74 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13340331595487487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13340331595487487 | validation: 0.16212922390738502]
	TIME [epoch: 3.74 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09787398752615711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09787398752615711 | validation: 0.1588904384956268]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09328903614155368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09328903614155368 | validation: 0.19368150993779998]
	TIME [epoch: 3.73 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12635113038819412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12635113038819412 | validation: 0.20548854824921514]
	TIME [epoch: 3.75 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14470954045933596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14470954045933596 | validation: 0.1937513174200506]
	TIME [epoch: 3.72 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13797959685736552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13797959685736552 | validation: 0.1953667345714085]
	TIME [epoch: 3.74 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15761302362138394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15761302362138394 | validation: 0.2245259481788418]
	TIME [epoch: 3.74 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15085813073956067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15085813073956067 | validation: 0.25663871892607476]
	TIME [epoch: 3.74 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19795718542377608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19795718542377608 | validation: 0.21117137639617561]
	TIME [epoch: 3.72 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15597653745896084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15597653745896084 | validation: 0.18225854530960164]
	TIME [epoch: 3.74 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1635172907509542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1635172907509542 | validation: 0.17233594174671688]
	TIME [epoch: 3.73 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09936486914207861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09936486914207861 | validation: 0.16727191580714626]
	TIME [epoch: 3.74 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09317710664224071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09317710664224071 | validation: 0.16778483695852536]
	TIME [epoch: 3.72 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09752359705816821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09752359705816821 | validation: 0.16206607594332434]
	TIME [epoch: 3.76 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0994207769277024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0994207769277024 | validation: 0.21643465355935965]
	TIME [epoch: 3.72 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13738171545941297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13738171545941297 | validation: 0.2887623457988933]
	TIME [epoch: 3.73 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3370241084149818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3370241084149818 | validation: 0.3705455263427949]
	TIME [epoch: 3.73 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2873017094520958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2873017094520958 | validation: 0.21831022240188613]
	TIME [epoch: 3.72 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15477557535390712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15477557535390712 | validation: 0.2758114381737111]
	TIME [epoch: 3.72 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21492167228101217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21492167228101217 | validation: 0.15191573198139913]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09808959572540416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09808959572540416 | validation: 0.16033838207951986]
	TIME [epoch: 3.71 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10826135272272237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10826135272272237 | validation: 0.13881944514341257]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08595218172395704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08595218172395704 | validation: 0.14329463439651993]
	TIME [epoch: 3.73 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08397282195136452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08397282195136452 | validation: 0.13945832223593535]
	TIME [epoch: 3.71 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08528676553874995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08528676553874995 | validation: 0.16367879118992765]
	TIME [epoch: 3.73 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0955551662280421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0955551662280421 | validation: 0.17295856675975096]
	TIME [epoch: 3.73 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13859916127574606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13859916127574606 | validation: 0.242088511690302]
	TIME [epoch: 3.73 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2099840671854568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2099840671854568 | validation: 0.1967572794656356]
	TIME [epoch: 3.73 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14778512224078755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14778512224078755 | validation: 0.15813629163444065]
	TIME [epoch: 3.71 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11802667711213267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11802667711213267 | validation: 0.14078084627821416]
	TIME [epoch: 3.77 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09297917316669572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09297917316669572 | validation: 0.13702697688936852]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08493836635876832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08493836635876832 | validation: 0.1601214187644069]
	TIME [epoch: 3.73 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09755983147376877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09755983147376877 | validation: 0.21602141041702022]
	TIME [epoch: 3.73 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15991498521917552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15991498521917552 | validation: 0.1490721740499347]
	TIME [epoch: 3.71 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08746100328236704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08746100328236704 | validation: 0.17921307281523993]
	TIME [epoch: 3.74 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1808806618872262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1808806618872262 | validation: 0.2803857332374065]
	TIME [epoch: 3.73 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19835597668298807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19835597668298807 | validation: 0.23489897886620562]
	TIME [epoch: 3.73 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16973029008847015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16973029008847015 | validation: 0.18460253248925892]
	TIME [epoch: 3.75 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13490690480292192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13490690480292192 | validation: 0.24899352001470146]
	TIME [epoch: 3.73 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19175246267299972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19175246267299972 | validation: 0.18705750764614026]
	TIME [epoch: 3.74 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12824108680128465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12824108680128465 | validation: 0.17393170908920336]
	TIME [epoch: 3.73 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11000539777655831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11000539777655831 | validation: 0.1502408542160999]
	TIME [epoch: 3.72 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08780677274624896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08780677274624896 | validation: 0.1341281198937359]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_479.pth
	Model improved!!!
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08780096821312613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08780096821312613 | validation: 0.15481309963847278]
	TIME [epoch: 3.72 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08586985622909417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08586985622909417 | validation: 0.14036466690807467]
	TIME [epoch: 3.73 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08791815886504342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08791815886504342 | validation: 0.17016808237290504]
	TIME [epoch: 3.73 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1149434280823931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1149434280823931 | validation: 0.19339367265810453]
	TIME [epoch: 3.75 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16282799508268284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16282799508268284 | validation: 0.2516326622590658]
	TIME [epoch: 3.75 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28548220851071066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28548220851071066 | validation: 0.2529409288803109]
	TIME [epoch: 3.75 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19989779389265658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19989779389265658 | validation: 0.12883774678317658]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08252991456308176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08252991456308176 | validation: 0.17228249828499356]
	TIME [epoch: 3.74 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11532640513362574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11532640513362574 | validation: 0.14078440862450267]
	TIME [epoch: 3.72 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0927233179526451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0927233179526451 | validation: 0.11737713126660934]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08416601782508465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08416601782508465 | validation: 0.11425630556841544]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0679048325921382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0679048325921382 | validation: 0.11107119448044878]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06505423239924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06505423239924 | validation: 0.11534693859446397]
	TIME [epoch: 3.74 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07998185761816676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07998185761816676 | validation: 0.18140506854874486]
	TIME [epoch: 3.72 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12280765557503533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12280765557503533 | validation: 0.18924973180316745]
	TIME [epoch: 3.75 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1329652333928766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1329652333928766 | validation: 0.16163335698323078]
	TIME [epoch: 3.74 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11229777741236398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11229777741236398 | validation: 0.12802489546260173]
	TIME [epoch: 3.74 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09047683420722452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09047683420722452 | validation: 0.16281768569196822]
	TIME [epoch: 3.74 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11636295351849567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11636295351849567 | validation: 0.23939832931568908]
	TIME [epoch: 3.74 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1977541464948755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1977541464948755 | validation: 0.15398041048538158]
	TIME [epoch: 3.75 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09566392434857977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09566392434857977 | validation: 0.1321234806179777]
	TIME [epoch: 3.84 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07730559780453451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07730559780453451 | validation: 0.11996857796897396]
	TIME [epoch: 31.5 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08625820956377954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08625820956377954 | validation: 0.11823719998381027]
	TIME [epoch: 8.08 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06166985231527045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06166985231527045 | validation: 0.11203170415986702]
	TIME [epoch: 8.05 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06771026755381392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06771026755381392 | validation: 0.1363199498063533]
	TIME [epoch: 8.06 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08197701443285575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08197701443285575 | validation: 0.15434343962021624]
	TIME [epoch: 8.07 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1038923200156135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1038923200156135 | validation: 0.1775986810418445]
	TIME [epoch: 8.06 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10665552628589046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10665552628589046 | validation: 0.1549763619923632]
	TIME [epoch: 8.06 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1096029796973124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1096029796973124 | validation: 0.17555779932801618]
	TIME [epoch: 8.07 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13753121088580172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13753121088580172 | validation: 0.2629127818327165]
	TIME [epoch: 8.09 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2266228041968814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2266228041968814 | validation: 0.15596576762209585]
	TIME [epoch: 8.08 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09968279065657164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09968279065657164 | validation: 0.09683518851279727]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058827947311659065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058827947311659065 | validation: 0.11346492080033009]
	TIME [epoch: 8.1 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07411213507855335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07411213507855335 | validation: 0.12344333243472422]
	TIME [epoch: 8.09 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0698361140661548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0698361140661548 | validation: 0.10824170475470507]
	TIME [epoch: 8.1 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06062545485932464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06062545485932464 | validation: 0.10215652447155424]
	TIME [epoch: 8.1 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056070115763858265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056070115763858265 | validation: 0.09445405669663116]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_516.pth
	Model improved!!!
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04934440763743969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04934440763743969 | validation: 0.07984388815004297]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04943412153408892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04943412153408892 | validation: 0.14437243253591087]
	TIME [epoch: 8.07 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09357499450650039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09357499450650039 | validation: 0.14988091281256133]
	TIME [epoch: 8.06 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1742926902580725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1742926902580725 | validation: 0.22939743448879632]
	TIME [epoch: 8.08 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1706002908999268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1706002908999268 | validation: 0.2320818935447589]
	TIME [epoch: 8.08 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15790400179589587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15790400179589587 | validation: 0.16307369440903774]
	TIME [epoch: 8.05 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0974391023370446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0974391023370446 | validation: 0.13269903626549556]
	TIME [epoch: 8.05 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09938120500850892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09938120500850892 | validation: 0.10915054686610284]
	TIME [epoch: 8.04 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06224264067123277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06224264067123277 | validation: 0.10531252138197084]
	TIME [epoch: 8.06 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08141170755625328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08141170755625328 | validation: 0.14049901915004617]
	TIME [epoch: 8.06 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13245849219938638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13245849219938638 | validation: 0.1275869405145249]
	TIME [epoch: 8.05 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11556057103059442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11556057103059442 | validation: 0.14718061080781028]
	TIME [epoch: 8.05 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08975206746811633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08975206746811633 | validation: 0.10010851595883899]
	TIME [epoch: 8.07 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06091813051683893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06091813051683893 | validation: 0.0843811740693985]
	TIME [epoch: 8.05 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053080279766467675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053080279766467675 | validation: 0.1011273150335391]
	TIME [epoch: 8.06 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04487059011055079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04487059011055079 | validation: 0.08327421353103162]
	TIME [epoch: 8.07 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04086772690305354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04086772690305354 | validation: 0.09716401346997466]
	TIME [epoch: 8.05 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05527823029618387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05527823029618387 | validation: 0.11216702156205481]
	TIME [epoch: 8.06 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09500350448601598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09500350448601598 | validation: 0.19560674250581783]
	TIME [epoch: 8.06 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1678116551474041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1678116551474041 | validation: 0.09126301757250652]
	TIME [epoch: 8.08 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06965655101174735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06965655101174735 | validation: 0.1270198052730249]
	TIME [epoch: 8.1 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11571946780410697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11571946780410697 | validation: 0.22949952469680068]
	TIME [epoch: 8.08 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15557046029374547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15557046029374547 | validation: 0.11897184770153238]
	TIME [epoch: 8.1 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08221726922608745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08221726922608745 | validation: 0.12663389504260125]
	TIME [epoch: 8.09 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14855496765265608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14855496765265608 | validation: 0.162819975106151]
	TIME [epoch: 8.07 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09891961452933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09891961452933 | validation: 0.13175473642449598]
	TIME [epoch: 8.08 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08107167651522673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08107167651522673 | validation: 0.09606207750805024]
	TIME [epoch: 8.1 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06759625821753887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06759625821753887 | validation: 0.08555886807713647]
	TIME [epoch: 8.08 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06650516484783758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06650516484783758 | validation: 0.11023926047265908]
	TIME [epoch: 8.09 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06336765247375056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06336765247375056 | validation: 0.0827464078795515]
	TIME [epoch: 8.06 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05462952422765154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05462952422765154 | validation: 0.09771837891537975]
	TIME [epoch: 8.08 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06989742430346668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06989742430346668 | validation: 0.10979717057444138]
	TIME [epoch: 8.09 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08829609359340107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08829609359340107 | validation: 0.14837282526132958]
	TIME [epoch: 8.08 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09834658617179759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09834658617179759 | validation: 0.09842298416123847]
	TIME [epoch: 8.09 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10794018540241926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10794018540241926 | validation: 0.1545321078951]
	TIME [epoch: 8.05 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12015474764963285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12015474764963285 | validation: 0.07944890996801367]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051858002721413464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051858002721413464 | validation: 0.07589074199002963]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06971642074566133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06971642074566133 | validation: 0.14253150513539198]
	TIME [epoch: 8.1 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08122223085527999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08122223085527999 | validation: 0.08814422967666899]
	TIME [epoch: 8.06 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046359386567537256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046359386567537256 | validation: 0.06913056963589788]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05955363025452152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05955363025452152 | validation: 0.11083503978961169]
	TIME [epoch: 8.11 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05827075327411515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05827075327411515 | validation: 0.06270072626508096]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043273869418377214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043273869418377214 | validation: 0.08468212035976758]
	TIME [epoch: 8.13 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06255170081507803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06255170081507803 | validation: 0.09064818679742258]
	TIME [epoch: 8.14 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07019517250601191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07019517250601191 | validation: 0.10458933059826095]
	TIME [epoch: 8.12 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08106465006851206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08106465006851206 | validation: 0.0770275305392466]
	TIME [epoch: 8.14 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08422879649651256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08422879649651256 | validation: 0.14191438381077434]
	TIME [epoch: 8.13 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10777774175229357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10777774175229357 | validation: 0.12939347888533334]
	TIME [epoch: 8.11 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11099840112115039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11099840112115039 | validation: 0.1823504714341443]
	TIME [epoch: 8.14 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13345589937196325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13345589937196325 | validation: 0.12561593648235325]
	TIME [epoch: 8.11 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0825127718246229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0825127718246229 | validation: 0.0718453678569448]
	TIME [epoch: 8.1 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08146871716119755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08146871716119755 | validation: 0.12105559473427396]
	TIME [epoch: 8.1 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07396288799267457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07396288799267457 | validation: 0.08499948426367383]
	TIME [epoch: 8.1 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042979924049128336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042979924049128336 | validation: 0.05973876024686303]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06187711121361519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06187711121361519 | validation: 0.10633389082364729]
	TIME [epoch: 8.13 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057224290239082355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057224290239082355 | validation: 0.062259676519775466]
	TIME [epoch: 8.12 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0299197045556574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0299197045556574 | validation: 0.04411300477373309]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_573.pth
	Model improved!!!
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0321183454416861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0321183454416861 | validation: 0.07367313639269758]
	TIME [epoch: 8.08 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04894510956639591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04894510956639591 | validation: 0.08665002326908632]
	TIME [epoch: 8.1 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09560378253150242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09560378253150242 | validation: 0.18242176701683305]
	TIME [epoch: 8.12 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1899913934335416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1899913934335416 | validation: 0.12101022382518667]
	TIME [epoch: 8.12 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10616349140028081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10616349140028081 | validation: 0.09487068289959838]
	TIME [epoch: 8.13 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07187623508772925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07187623508772925 | validation: 0.10348733200396079]
	TIME [epoch: 8.11 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059437770657969756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059437770657969756 | validation: 0.05404777904681521]
	TIME [epoch: 8.11 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03350021446563573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03350021446563573 | validation: 0.05642984713417496]
	TIME [epoch: 8.14 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0264788632203115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0264788632203115 | validation: 0.045357304404760594]
	TIME [epoch: 8.15 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02482745915815995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02482745915815995 | validation: 0.04079629261222392]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_583.pth
	Model improved!!!
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021584130254856216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021584130254856216 | validation: 0.05138990396353898]
	TIME [epoch: 8.12 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025448472129753076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025448472129753076 | validation: 0.07369533833902377]
	TIME [epoch: 8.13 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052160464599282745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052160464599282745 | validation: 0.1518818048482186]
	TIME [epoch: 8.1 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12436347439352613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12436347439352613 | validation: 0.15914489892421232]
	TIME [epoch: 8.12 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16196014154411054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16196014154411054 | validation: 0.15583651612786498]
	TIME [epoch: 8.12 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15044519506417667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15044519506417667 | validation: 0.07374274451246139]
	TIME [epoch: 8.13 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09967466768560553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09967466768560553 | validation: 0.19134466928892135]
	TIME [epoch: 8.13 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.143318993193309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.143318993193309 | validation: 0.16843248558776366]
	TIME [epoch: 8.12 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12892502025984787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12892502025984787 | validation: 0.05578676198894603]
	TIME [epoch: 8.13 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051424761536757416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051424761536757416 | validation: 0.07792189063514648]
	TIME [epoch: 8.13 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0538007666742217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0538007666742217 | validation: 0.060774100241963225]
	TIME [epoch: 8.12 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03877446846489138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03877446846489138 | validation: 0.05510291634156446]
	TIME [epoch: 8.11 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03112387255215448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03112387255215448 | validation: 0.04606159632718581]
	TIME [epoch: 8.11 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027981686610584415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027981686610584415 | validation: 0.04502153298263222]
	TIME [epoch: 8.1 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02500009366856621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02500009366856621 | validation: 0.05226396765871345]
	TIME [epoch: 8.13 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02618760637338554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02618760637338554 | validation: 0.04506104263276183]
	TIME [epoch: 8.16 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0400832356972548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0400832356972548 | validation: 0.14236912375397023]
	TIME [epoch: 8.13 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09494976972752174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09494976972752174 | validation: 0.07553537386922371]
	TIME [epoch: 8.11 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0664656097343441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0664656097343441 | validation: 0.08711158670147653]
	TIME [epoch: 8.16 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07980418245783277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07980418245783277 | validation: 0.20837724513902955]
	TIME [epoch: 8.11 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14995481492413315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14995481492413315 | validation: 0.10074193179670328]
	TIME [epoch: 8.12 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0831130729938385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0831130729938385 | validation: 0.06306210946726241]
	TIME [epoch: 8.14 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031569457602923304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031569457602923304 | validation: 0.0557571180690725]
	TIME [epoch: 8.12 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031032645325072567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031032645325072567 | validation: 0.04000137949192817]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_607.pth
	Model improved!!!
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033817038912338164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033817038912338164 | validation: 0.05850397558580944]
	TIME [epoch: 8.1 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03909303816837481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03909303816837481 | validation: 0.043116769787060194]
	TIME [epoch: 8.13 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04108046553107794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04108046553107794 | validation: 0.0871632698854744]
	TIME [epoch: 8.13 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06228894639298672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06228894639298672 | validation: 0.036015151750444674]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_611.pth
	Model improved!!!
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02856837540553569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02856837540553569 | validation: 0.06601842246189642]
	TIME [epoch: 8.06 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036148471534511575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036148471534511575 | validation: 0.06912715232683646]
	TIME [epoch: 8.05 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07031746691860842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07031746691860842 | validation: 0.18306716870000084]
	TIME [epoch: 8.06 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14714089953906032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14714089953906032 | validation: 0.06821946188483324]
	TIME [epoch: 8.06 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09205895842448347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09205895842448347 | validation: 0.08506005820691838]
	TIME [epoch: 8.07 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05882030873722016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05882030873722016 | validation: 0.11151890400894882]
	TIME [epoch: 8.07 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0885760775469968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0885760775469968 | validation: 0.08872227501821027]
	TIME [epoch: 8.05 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08308617933671858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08308617933671858 | validation: 0.0796627895931691]
	TIME [epoch: 8.11 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06560187134840959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06560187134840959 | validation: 0.049309776113265105]
	TIME [epoch: 8.13 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048719749467693405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048719749467693405 | validation: 0.043131591733694234]
	TIME [epoch: 8.05 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024028185238019698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024028185238019698 | validation: 0.02877822559945391]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018426908531090462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018426908531090462 | validation: 0.057165879585524305]
	TIME [epoch: 8.07 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03157312571538712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03157312571538712 | validation: 0.034737834666437296]
	TIME [epoch: 8.11 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059024486634744876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059024486634744876 | validation: 0.1502435757039891]
	TIME [epoch: 8.11 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12824283632719588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12824283632719588 | validation: 0.09664925710594936]
	TIME [epoch: 8.08 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09296627997849324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09296627997849324 | validation: 0.10570397188905643]
	TIME [epoch: 8.11 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12497373626382048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12497373626382048 | validation: 0.11868388408043287]
	TIME [epoch: 8.08 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07226948476612503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07226948476612503 | validation: 0.05225601476428204]
	TIME [epoch: 8.11 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03312634018421128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03312634018421128 | validation: 0.041924592416983035]
	TIME [epoch: 8.08 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046953022991236104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046953022991236104 | validation: 0.07071176838058428]
	TIME [epoch: 8.12 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04057938419600953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04057938419600953 | validation: 0.03172692439828593]
	TIME [epoch: 8.09 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022209130430794196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022209130430794196 | validation: 0.03035109235416851]
	TIME [epoch: 8.14 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0219614042317571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0219614042317571 | validation: 0.03634640937781889]
	TIME [epoch: 8.08 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02773753413468709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02773753413468709 | validation: 0.07031462416677603]
	TIME [epoch: 8.12 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05906330547113118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05906330547113118 | validation: 0.10763946168805825]
	TIME [epoch: 8.09 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1317548839747579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1317548839747579 | validation: 0.16501202331507656]
	TIME [epoch: 8.08 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1605482616690451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1605482616690451 | validation: 0.05116429641956203]
	TIME [epoch: 8.12 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04908785855718268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04908785855718268 | validation: 0.04564562221136992]
	TIME [epoch: 8.1 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030214441795795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030214441795795 | validation: 0.05072090060816023]
	TIME [epoch: 8.12 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0321494206469624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0321494206469624 | validation: 0.027079051431906323]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_641.pth
	Model improved!!!
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01935791355546445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01935791355546445 | validation: 0.03156768294657663]
	TIME [epoch: 8.08 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020364668850341584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020364668850341584 | validation: 0.0350757961373516]
	TIME [epoch: 8.09 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022297284144919872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022297284144919872 | validation: 0.07359330849438582]
	TIME [epoch: 8.12 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0415086440008548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0415086440008548 | validation: 0.04401158832765017]
	TIME [epoch: 8.08 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06663128560548837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06663128560548837 | validation: 0.1445464803860288]
	TIME [epoch: 8.09 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08993107176167342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08993107176167342 | validation: 0.0978965312172118]
	TIME [epoch: 8.08 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08315884178443081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08315884178443081 | validation: 0.1545728391644561]
	TIME [epoch: 8.11 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1258363137220584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1258363137220584 | validation: 0.08927106009355593]
	TIME [epoch: 8.1 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07189864841620683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07189864841620683 | validation: 0.053852365599745725]
	TIME [epoch: 8.13 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052057029930029815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052057029930029815 | validation: 0.06815758976053413]
	TIME [epoch: 8.08 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052931718579667476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052931718579667476 | validation: 0.03536242617197728]
	TIME [epoch: 8.1 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028799433478941237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028799433478941237 | validation: 0.04691994058811791]
	TIME [epoch: 8.08 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025818410637668814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025818410637668814 | validation: 0.021297212743476193]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_654.pth
	Model improved!!!
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025120106528260387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025120106528260387 | validation: 0.05611772918777504]
	TIME [epoch: 8.08 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03693268388960253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03693268388960253 | validation: 0.025849567442094348]
	TIME [epoch: 8.11 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055020570783741486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055020570783741486 | validation: 0.10938575837270964]
	TIME [epoch: 8.1 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07712090454904023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07712090454904023 | validation: 0.04229282929159287]
	TIME [epoch: 8.08 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04591654702249606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04591654702249606 | validation: 0.05124084261879811]
	TIME [epoch: 8.09 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07727420204472801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07727420204472801 | validation: 0.183016936508351]
	TIME [epoch: 8.09 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10509528462456758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10509528462456758 | validation: 0.17687807845105463]
	TIME [epoch: 8.1 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13653320198680416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13653320198680416 | validation: 0.09478272426462776]
	TIME [epoch: 8.08 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08093497131490635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08093497131490635 | validation: 0.05659751883270123]
	TIME [epoch: 8.06 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04728416066534158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04728416066534158 | validation: 0.03459026206060241]
	TIME [epoch: 8.08 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031005272632329304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031005272632329304 | validation: 0.035555898410558194]
	TIME [epoch: 8.08 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022706167826709943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022706167826709943 | validation: 0.023452388490331568]
	TIME [epoch: 8.09 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018238336937068986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018238336937068986 | validation: 0.018728107448612696]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_667.pth
	Model improved!!!
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01515200957782088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01515200957782088 | validation: 0.05410484207073194]
	TIME [epoch: 8.07 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025894162048419243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025894162048419243 | validation: 0.05071329839006007]
	TIME [epoch: 8.06 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059656805581913996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059656805581913996 | validation: 0.16467483287620743]
	TIME [epoch: 8.11 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12985888764766085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12985888764766085 | validation: 0.11659859666765299]
	TIME [epoch: 8.12 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1050148131477093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1050148131477093 | validation: 0.04511623932996685]
	TIME [epoch: 8.11 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056302050777387154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056302050777387154 | validation: 0.09915116790331567]
	TIME [epoch: 8.11 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07529533342970413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07529533342970413 | validation: 0.04810154950147767]
	TIME [epoch: 8.1 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0513252936248019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0513252936248019 | validation: 0.05965759431157952]
	TIME [epoch: 8.1 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049647124122106846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049647124122106846 | validation: 0.0360968915453355]
	TIME [epoch: 8.11 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02349653430722184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02349653430722184 | validation: 0.026108282359736015]
	TIME [epoch: 8.1 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01942091872308225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01942091872308225 | validation: 0.03456486396116132]
	TIME [epoch: 8.12 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023781092146624633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023781092146624633 | validation: 0.024273910957590662]
	TIME [epoch: 8.13 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022840008710783982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022840008710783982 | validation: 0.055150693691136225]
	TIME [epoch: 8.11 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035491018854909566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035491018854909566 | validation: 0.030632876198781456]
	TIME [epoch: 8.11 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07763878569967878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07763878569967878 | validation: 0.13654205368165948]
	TIME [epoch: 8.05 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11210102342217639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11210102342217639 | validation: 0.07726655331471191]
	TIME [epoch: 8.09 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06879381149485422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06879381149485422 | validation: 0.10883827292173559]
	TIME [epoch: 8.09 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11444544641639581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11444544641639581 | validation: 0.13846313221255163]
	TIME [epoch: 8.03 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10073373775785481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10073373775785481 | validation: 0.04220024242673727]
	TIME [epoch: 8.07 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04058917630833765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04058917630833765 | validation: 0.02686250765478211]
	TIME [epoch: 8.09 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025721794781010426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025721794781010426 | validation: 0.03468978813797764]
	TIME [epoch: 8.1 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025907188119897496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025907188119897496 | validation: 0.021513834993512993]
	TIME [epoch: 8.16 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024864395770998603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024864395770998603 | validation: 0.0315244127318024]
	TIME [epoch: 8.1 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017950980095407174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017950980095407174 | validation: 0.019159540747490236]
	TIME [epoch: 8.1 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022425109660090495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022425109660090495 | validation: 0.040128614486135486]
	TIME [epoch: 8.11 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030574509155316613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030574509155316613 | validation: 0.03911007331302361]
	TIME [epoch: 8.1 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04601627023987649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04601627023987649 | validation: 0.10160374794172543]
	TIME [epoch: 8.11 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09509369243877687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09509369243877687 | validation: 0.09866599083395966]
	TIME [epoch: 8.09 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10788648523766478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10788648523766478 | validation: 0.07040658289588203]
	TIME [epoch: 8.14 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05928496971108077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05928496971108077 | validation: 0.03449238408428006]
	TIME [epoch: 8.07 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022365312081059425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022365312081059425 | validation: 0.015467384636509818]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_698.pth
	Model improved!!!
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019618433697798036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019618433697798036 | validation: 0.08236282250234696]
	TIME [epoch: 8.13 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04745634415076486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04745634415076486 | validation: 0.038618864420532265]
	TIME [epoch: 8.13 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04848990896401004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04848990896401004 | validation: 0.09969122182113321]
	TIME [epoch: 8.1 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0609478004779956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0609478004779956 | validation: 0.05372616089563233]
	TIME [epoch: 8.06 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04911556635635044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04911556635635044 | validation: 0.05671683216456998]
	TIME [epoch: 8.07 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05918164127046046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05918164127046046 | validation: 0.08220555048492072]
	TIME [epoch: 8.07 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0667365806377817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0667365806377817 | validation: 0.036098444873853296]
	TIME [epoch: 8.07 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048543368845182656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048543368845182656 | validation: 0.05059590438410797]
	TIME [epoch: 8.1 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03204211837870287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03204211837870287 | validation: 0.017599449027674196]
	TIME [epoch: 8.08 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016372937306943044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016372937306943044 | validation: 0.02729940967578024]
	TIME [epoch: 8.09 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01573637092187615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01573637092187615 | validation: 0.02434850308633856]
	TIME [epoch: 8.08 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034322640665095376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034322640665095376 | validation: 0.13020654705129728]
	TIME [epoch: 8.08 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09407538892932689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09407538892932689 | validation: 0.0921578374677728]
	TIME [epoch: 8.1 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08012194139847449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08012194139847449 | validation: 0.07068697544471579]
	TIME [epoch: 8.09 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06938284869028666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06938284869028666 | validation: 0.08122361594719575]
	TIME [epoch: 8.08 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06468612343570693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06468612343570693 | validation: 0.03223996924792651]
	TIME [epoch: 8.09 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03632400628233355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03632400628233355 | validation: 0.029438548857940353]
	TIME [epoch: 8.1 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0193248530415115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0193248530415115 | validation: 0.027178884754880863]
	TIME [epoch: 8.07 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020453516113776975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020453516113776975 | validation: 0.025195158114992778]
	TIME [epoch: 8.09 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03685322678957741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03685322678957741 | validation: 0.06660933599024405]
	TIME [epoch: 8.11 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06373718181426141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06373718181426141 | validation: 0.025269738744053673]
	TIME [epoch: 8.09 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05082708658311069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05082708658311069 | validation: 0.07654455101819252]
	TIME [epoch: 8.08 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05721920611113552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05721920611113552 | validation: 0.02730571203848875]
	TIME [epoch: 8.1 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026513495292742776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026513495292742776 | validation: 0.02870464171430517]
	TIME [epoch: 8.09 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025933624055825534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025933624055825534 | validation: 0.0435520839771153]
	TIME [epoch: 8.1 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03214881523677495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03214881523677495 | validation: 0.08025192976913267]
	TIME [epoch: 8.12 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07331290571136397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07331290571136397 | validation: 0.11550885639282421]
	TIME [epoch: 8.11 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11992102509240088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11992102509240088 | validation: 0.12501488946576705]
	TIME [epoch: 8.11 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0973625194011905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0973625194011905 | validation: 0.023874660340176726]
	TIME [epoch: 8.1 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032326660649291655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032326660649291655 | validation: 0.025817189238109352]
	TIME [epoch: 8.09 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019160375536834286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019160375536834286 | validation: 0.0265447975435808]
	TIME [epoch: 8.13 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018999333957702282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018999333957702282 | validation: 0.021286642628533404]
	TIME [epoch: 8.11 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020128145098100472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020128145098100472 | validation: 0.028091604846803078]
	TIME [epoch: 8.09 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020299085181485897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020299085181485897 | validation: 0.025356196644215304]
	TIME [epoch: 8.08 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02055686381759122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02055686381759122 | validation: 0.030928942545981067]
	TIME [epoch: 8.11 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028661923106839442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028661923106839442 | validation: 0.07292263842087952]
	TIME [epoch: 8.09 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0639322743510123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0639322743510123 | validation: 0.06350874828009603]
	TIME [epoch: 8.09 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10985341184044188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10985341184044188 | validation: 0.1342150506989321]
	TIME [epoch: 8.09 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12320686218756188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12320686218756188 | validation: 0.04808187349967409]
	TIME [epoch: 8.11 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04066665885297567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04066665885297567 | validation: 0.03298287161675445]
	TIME [epoch: 8.08 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04748819482796833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04748819482796833 | validation: 0.05638258204925494]
	TIME [epoch: 8.07 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03847046446390439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03847046446390439 | validation: 0.02962087645432565]
	TIME [epoch: 8.13 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027740063975911857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027740063975911857 | validation: 0.0517968606288886]
	TIME [epoch: 8.13 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03547085486643655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03547085486643655 | validation: 0.04792929013436867]
	TIME [epoch: 8.11 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050922676857295984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050922676857295984 | validation: 0.09244573784008177]
	TIME [epoch: 8.1 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0744909943212876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0744909943212876 | validation: 0.04526001543865679]
	TIME [epoch: 8.1 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05577161701387641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05577161701387641 | validation: 0.039048709003693886]
	TIME [epoch: 8.12 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032673351466473724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032673351466473724 | validation: 0.02272908356288359]
	TIME [epoch: 8.13 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018215642225847307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018215642225847307 | validation: 0.022327721265714563]
	TIME [epoch: 8.16 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013136823975478861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013136823975478861 | validation: 0.01706829322517156]
	TIME [epoch: 8.1 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011725255219203352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011725255219203352 | validation: 0.008876420921598516]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01353061117106428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01353061117106428 | validation: 0.05550272712089124]
	TIME [epoch: 8.13 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04191289849317707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04191289849317707 | validation: 0.046463842870783784]
	TIME [epoch: 8.13 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10842139177885735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10842139177885735 | validation: 0.11662888907395286]
	TIME [epoch: 8.15 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10086807399648265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10086807399648265 | validation: 0.035707403918782334]
	TIME [epoch: 8.14 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03626854518383188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03626854518383188 | validation: 0.02590089897462823]
	TIME [epoch: 8.12 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03965357445225614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03965357445225614 | validation: 0.05823943142452061]
	TIME [epoch: 8.11 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037529020891183616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037529020891183616 | validation: 0.049073969512342566]
	TIME [epoch: 8.14 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04439128574903202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04439128574903202 | validation: 0.07148339379048596]
	TIME [epoch: 8.15 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07468151610849694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07468151610849694 | validation: 0.09797713213811246]
	TIME [epoch: 8.14 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10061180878854742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10061180878854742 | validation: 0.07048096851260682]
	TIME [epoch: 8.13 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054775542729209425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054775542729209425 | validation: 0.02220787835555954]
	TIME [epoch: 8.12 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023123677408191094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023123677408191094 | validation: 0.027015835333636995]
	TIME [epoch: 8.11 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02002402279132828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02002402279132828 | validation: 0.016052846347616955]
	TIME [epoch: 8.1 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018450392689181486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018450392689181486 | validation: 0.027856857097064293]
	TIME [epoch: 8.11 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019593525302039335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019593525302039335 | validation: 0.016299983022504683]
	TIME [epoch: 8.09 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019468121180599148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019468121180599148 | validation: 0.028392742219139802]
	TIME [epoch: 8.09 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02209639698236491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02209639698236491 | validation: 0.06688952604366859]
	TIME [epoch: 8.09 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050433731609939886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050433731609939886 | validation: 0.07602045056042638]
	TIME [epoch: 8.09 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09818832374369003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09818832374369003 | validation: 0.12490476277145098]
	TIME [epoch: 8.11 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10220280306028905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10220280306028905 | validation: 0.03343624556635161]
	TIME [epoch: 8.1 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03072671268612372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03072671268612372 | validation: 0.037082498344562086]
	TIME [epoch: 8.1 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05181450958501582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05181450958501582 | validation: 0.07415262231845061]
	TIME [epoch: 8.06 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055749508825048415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055749508825048415 | validation: 0.026332181124179024]
	TIME [epoch: 8.07 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031108042194668985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031108042194668985 | validation: 0.01720820975078886]
	TIME [epoch: 8.09 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024297480817821484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024297480817821484 | validation: 0.03622353407743696]
	TIME [epoch: 8.09 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022028492664578826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022028492664578826 | validation: 0.018914932545662712]
	TIME [epoch: 8.09 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022089755357397985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022089755357397985 | validation: 0.024429087357622406]
	TIME [epoch: 8.08 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017564522119272185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017564522119272185 | validation: 0.019989569936693388]
	TIME [epoch: 8.07 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021038998691286365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021038998691286365 | validation: 0.06537100581102026]
	TIME [epoch: 8.11 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05103098838078242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05103098838078242 | validation: 0.07562032422817579]
	TIME [epoch: 8.09 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08014619531060665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08014619531060665 | validation: 0.11917246895670251]
	TIME [epoch: 8.13 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08637271824428662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08637271824428662 | validation: 0.030653616898260728]
	TIME [epoch: 8.12 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05434971916531345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05434971916531345 | validation: 0.05659501846013725]
	TIME [epoch: 8.07 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052334152410384735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052334152410384735 | validation: 0.07120090625806295]
	TIME [epoch: 8.1 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06805902612547657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06805902612547657 | validation: 0.06547161302101864]
	TIME [epoch: 8.1 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06692053162128274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06692053162128274 | validation: 0.039490230608148304]
	TIME [epoch: 8.13 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03285483015805025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03285483015805025 | validation: 0.015211828469320865]
	TIME [epoch: 8.12 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01126401489352967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01126401489352967 | validation: 0.012060703450581379]
	TIME [epoch: 8.09 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011831415890443666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011831415890443666 | validation: 0.013411439911188495]
	TIME [epoch: 8.09 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009469332305386375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009469332305386375 | validation: 0.0124144497308747]
	TIME [epoch: 8.1 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013134812656695366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013134812656695366 | validation: 0.01905020353448358]
	TIME [epoch: 8.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02991022691081013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02991022691081013 | validation: 0.09034950593570411]
	TIME [epoch: 8.11 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09116495943170434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09116495943170434 | validation: 0.07269788052006836]
	TIME [epoch: 8.11 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10691108555489046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10691108555489046 | validation: 0.0649659813636596]
	TIME [epoch: 8.1 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05961526701684546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05961526701684546 | validation: 0.06654387867947473]
	TIME [epoch: 8.1 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04812880875389717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04812880875389717 | validation: 0.034543774760844546]
	TIME [epoch: 8.11 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04267560257368153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04267560257368153 | validation: 0.04517641585156449]
	TIME [epoch: 8.13 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032452034192668365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032452034192668365 | validation: 0.01566210055158004]
	TIME [epoch: 8.09 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014156924018073976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014156924018073976 | validation: 0.012923107874296381]
	TIME [epoch: 8.11 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019183370753626063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019183370753626063 | validation: 0.029774154472587522]
	TIME [epoch: 8.1 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022936499747887376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022936499747887376 | validation: 0.015008587379657313]
	TIME [epoch: 8.12 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031115127812191296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031115127812191296 | validation: 0.05016583467940264]
	TIME [epoch: 8.07 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04401313593781486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04401313593781486 | validation: 0.042683084277169984]
	TIME [epoch: 8.09 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04624558518562256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04624558518562256 | validation: 0.048127666657042316]
	TIME [epoch: 8.08 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052547513974154934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052547513974154934 | validation: 0.05270994026264794]
	TIME [epoch: 8.11 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05228766679531864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05228766679531864 | validation: 0.04176347938843679]
	TIME [epoch: 8.11 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045612599944628816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045612599944628816 | validation: 0.09112440833610783]
	TIME [epoch: 8.07 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06811692773396043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06811692773396043 | validation: 0.052575128927800446]
	TIME [epoch: 8.1 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07219571631473586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07219571631473586 | validation: 0.06099812237192977]
	TIME [epoch: 8.11 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048990603153930924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048990603153930924 | validation: 0.018390675616125885]
	TIME [epoch: 8.11 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018129086650853984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018129086650853984 | validation: 0.025710580208226898]
	TIME [epoch: 8.08 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018987497390442948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018987497390442948 | validation: 0.024714445431981535]
	TIME [epoch: 8.1 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024863653699004548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024863653699004548 | validation: 0.022858611158304176]
	TIME [epoch: 8.07 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025526399720811128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025526399720811128 | validation: 0.024938269254041658]
	TIME [epoch: 8.1 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025458654585363335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025458654585363335 | validation: 0.040102127580341755]
	TIME [epoch: 8.11 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04431006651598334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04431006651598334 | validation: 0.03585018255332467]
	TIME [epoch: 8.12 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035896591109961455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035896591109961455 | validation: 0.0339396887648075]
	TIME [epoch: 8.09 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03987708909322429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03987708909322429 | validation: 0.07747319674329267]
	TIME [epoch: 8.07 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06360859252174265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06360859252174265 | validation: 0.07794454036025092]
	TIME [epoch: 8.1 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07156836388308803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07156836388308803 | validation: 0.05052237835531701]
	TIME [epoch: 8.09 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04368742729490115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04368742729490115 | validation: 0.015227831444012397]
	TIME [epoch: 8.11 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016316057247461834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016316057247461834 | validation: 0.015682389294922827]
	TIME [epoch: 8.11 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011434541409452704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011434541409452704 | validation: 0.015925590854891813]
	TIME [epoch: 8.12 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036767993966621275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036767993966621275 | validation: 0.1277997428443698]
	TIME [epoch: 8.14 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12097139876125677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12097139876125677 | validation: 0.042234278548239336]
	TIME [epoch: 8.09 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05600871589525903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05600871589525903 | validation: 0.0423999688927314]
	TIME [epoch: 8.08 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0626281802284658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0626281802284658 | validation: 0.058667399716486504]
	TIME [epoch: 8.1 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0509314768670483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0509314768670483 | validation: 0.033278036056620934]
	TIME [epoch: 8.11 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03335508442597456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03335508442597456 | validation: 0.018819784831131472]
	TIME [epoch: 8.08 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023798414439532307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023798414439532307 | validation: 0.03129546127604025]
	TIME [epoch: 8.08 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02796516582216823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02796516582216823 | validation: 0.018408397304565883]
	TIME [epoch: 8.1 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019936868731860888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019936868731860888 | validation: 0.019780437050015743]
	TIME [epoch: 8.12 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018315074071366393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018315074071366393 | validation: 0.023812217871251286]
	TIME [epoch: 8.13 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03024264329096936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03024264329096936 | validation: 0.055101750094708746]
	TIME [epoch: 8.07 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057230573035577416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057230573035577416 | validation: 0.10093136606634848]
	TIME [epoch: 8.07 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09001507097778134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09001507097778134 | validation: 0.06906479660188695]
	TIME [epoch: 8.11 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06884124319098073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06884124319098073 | validation: 0.055505321731586545]
	TIME [epoch: 8.1 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03589967847722934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03589967847722934 | validation: 0.014582628959014588]
	TIME [epoch: 8.11 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0271392780660491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0271392780660491 | validation: 0.05569915985094901]
	TIME [epoch: 8.1 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039578723675553824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039578723675553824 | validation: 0.01578427412021971]
	TIME [epoch: 8.08 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02328195412713006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02328195412713006 | validation: 0.03050803928073538]
	TIME [epoch: 8.07 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022046790101916756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022046790101916756 | validation: 0.025366765112032466]
	TIME [epoch: 8.09 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021645337922504976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021645337922504976 | validation: 0.024194241195086133]
	TIME [epoch: 8.11 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02744167259981316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02744167259981316 | validation: 0.04929568963320387]
	TIME [epoch: 8.1 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03741180920373717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03741180920373717 | validation: 0.025131511300643207]
	TIME [epoch: 8.1 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03576316321386083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03576316321386083 | validation: 0.05369511148653903]
	TIME [epoch: 8.09 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04045328842326905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04045328842326905 | validation: 0.032361192880342615]
	TIME [epoch: 8.08 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03165435944287291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03165435944287291 | validation: 0.0792004168183567]
	TIME [epoch: 8.09 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054469072842161806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054469072842161806 | validation: 0.06789857024258086]
	TIME [epoch: 8.13 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06399375832025045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06399375832025045 | validation: 0.04674345006743319]
	TIME [epoch: 8.1 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05197428674184848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05197428674184848 | validation: 0.04651467161808048]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_132815/states/model_phi1_3b_v_mmd1_850.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4503.228 seconds.
