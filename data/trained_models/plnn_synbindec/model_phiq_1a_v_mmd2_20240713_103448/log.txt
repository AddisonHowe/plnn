Args:
Namespace(name='model_phiq_1a_v_mmd2', outdir='out/model_training/model_phiq_1a_v_mmd2', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=10.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 872202177

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 4.182922185262956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.182922185262956 | validation: 4.174193557828199]
	TIME [epoch: 174 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9780703637098167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9780703637098167 | validation: 3.970880577203711]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.784930753794353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.784930753794353 | validation: 3.8514353918130655]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7034445262669724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7034445262669724 | validation: 3.7595282757983837]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7157971290955127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7157971290955127 | validation: 3.73415028082692]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.571400816848646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.571400816848646 | validation: 3.644123154977716]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4662309960353177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4662309960353177 | validation: 3.5469407615657476]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4131345983483206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4131345983483206 | validation: 3.421684642365147]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.308385785718497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.308385785718497 | validation: 3.344128992204465]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.236359901479654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.236359901479654 | validation: 3.20750671951089]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209546708836391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.209546708836391 | validation: 3.4471668245647216]
	TIME [epoch: 7.73 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.163101558767738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.163101558767738 | validation: 3.0713324472702066]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.947147446935235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.947147446935235 | validation: 2.912199890141209]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.853712878558368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.853712878558368 | validation: 2.89756421324331]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8160208013090093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8160208013090093 | validation: 2.736982407094178]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6651594018551585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6651594018551585 | validation: 2.617686621556348]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5327863252465557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5327863252465557 | validation: 2.920470452755679]
	TIME [epoch: 7.65 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6612364999893727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6612364999893727 | validation: 2.46858061505008]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3800575707688734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3800575707688734 | validation: 2.4397108694101517]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2223967546056618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2223967546056618 | validation: 2.2847513169264806]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1945209717315124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1945209717315124 | validation: 2.382075593667731]
	TIME [epoch: 7.69 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.184625851313588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.184625851313588 | validation: 2.1701974710438554]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.025038996655837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.025038996655837 | validation: 2.0884919940431095]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9634504191330733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9634504191330733 | validation: 2.1115570441711546]
	TIME [epoch: 7.63 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9904340447109234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9904340447109234 | validation: 2.037800835743047]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8857337790403543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8857337790403543 | validation: 1.9188601066820044]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8541660176463053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8541660176463053 | validation: 1.9041821210998564]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8325103353062493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8325103353062493 | validation: 1.885452225669304]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7616854405840023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7616854405840023 | validation: 1.827635202598586]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7216027045853732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7216027045853732 | validation: 1.7456226055974247]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6664855287769749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6664855287769749 | validation: 2.0099446134924235]
	TIME [epoch: 7.69 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7770619443684197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7770619443684197 | validation: 1.7713796439118843]
	TIME [epoch: 7.65 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6649470856130932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6649470856130932 | validation: 1.6291255009638101]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.591599469382236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.591599469382236 | validation: 1.5941875657190236]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5595333736921941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5595333736921941 | validation: 1.557299753333018]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5317396309964577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5317396309964577 | validation: 1.7039116496714977]
	TIME [epoch: 7.67 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.565626705503633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.565626705503633 | validation: 1.536024297548522]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4985354798690924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4985354798690924 | validation: 1.4616164772446743]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4562865636860431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4562865636860431 | validation: 1.450000076358466]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4581065060734621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4581065060734621 | validation: 1.4763496580875035]
	TIME [epoch: 7.67 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4310437451522486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4310437451522486 | validation: 1.4857904496466379]
	TIME [epoch: 7.63 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4223847714528866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4223847714528866 | validation: 1.4139534033210222]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3869091961162412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3869091961162412 | validation: 1.4187409049743187]
	TIME [epoch: 7.65 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.457430014897371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.457430014897371 | validation: 1.4115124392140062]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3696558773868615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3696558773868615 | validation: 1.385414378577066]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3480115190418507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3480115190418507 | validation: 1.3683003180423068]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3516924047200312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3516924047200312 | validation: 1.3670463512444466]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3345418199116876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3345418199116876 | validation: 1.3697679652931245]
	TIME [epoch: 7.68 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3429201272378806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3429201272378806 | validation: 1.3474651329963372]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3087453611303774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3087453611303774 | validation: 1.3533766986764348]
	TIME [epoch: 7.67 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3216382739474408		[learning rate: 0.0099823]
	Learning Rate: 0.0099823
	LOSS [training: 1.3216382739474408 | validation: 1.3390747768071927]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3139268688328636		[learning rate: 0.0099588]
	Learning Rate: 0.00995876
	LOSS [training: 1.3139268688328636 | validation: 1.348353249489581]
	TIME [epoch: 7.66 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.294615396189064		[learning rate: 0.0099353]
	Learning Rate: 0.00993527
	LOSS [training: 1.294615396189064 | validation: 1.322239956819367]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3004723258714002		[learning rate: 0.0099118]
	Learning Rate: 0.00991183
	LOSS [training: 1.3004723258714002 | validation: 1.3011360683404263]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.300595033916539		[learning rate: 0.0098884]
	Learning Rate: 0.00988845
	LOSS [training: 1.300595033916539 | validation: 1.3348183234506925]
	TIME [epoch: 7.68 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.30388933920036		[learning rate: 0.0098651]
	Learning Rate: 0.00986512
	LOSS [training: 1.30388933920036 | validation: 1.3253634034773247]
	TIME [epoch: 7.7 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2901323584401019		[learning rate: 0.0098419]
	Learning Rate: 0.00984185
	LOSS [training: 1.2901323584401019 | validation: 1.2912258978706297]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2723055625696276		[learning rate: 0.0098186]
	Learning Rate: 0.00981864
	LOSS [training: 1.2723055625696276 | validation: 1.2892805569157435]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2696660729451343		[learning rate: 0.0097955]
	Learning Rate: 0.00979548
	LOSS [training: 1.2696660729451343 | validation: 1.2909550122242814]
	TIME [epoch: 7.65 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2822084394699007		[learning rate: 0.0097724]
	Learning Rate: 0.00977237
	LOSS [training: 1.2822084394699007 | validation: 1.3030813359392632]
	TIME [epoch: 7.62 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3021523313055574		[learning rate: 0.0097493]
	Learning Rate: 0.00974932
	LOSS [training: 1.3021523313055574 | validation: 1.327729975509875]
	TIME [epoch: 7.64 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3090071871749245		[learning rate: 0.0097263]
	Learning Rate: 0.00972632
	LOSS [training: 1.3090071871749245 | validation: 1.298095020716962]
	TIME [epoch: 7.63 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.312615282544713		[learning rate: 0.0097034]
	Learning Rate: 0.00970338
	LOSS [training: 1.312615282544713 | validation: 1.347005047154425]
	TIME [epoch: 7.65 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.325098658720727		[learning rate: 0.0096805]
	Learning Rate: 0.00968049
	LOSS [training: 1.325098658720727 | validation: 1.3319856845037976]
	TIME [epoch: 7.67 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3002516451284807		[learning rate: 0.0096577]
	Learning Rate: 0.00965766
	LOSS [training: 1.3002516451284807 | validation: 1.2983867852578013]
	TIME [epoch: 7.64 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2437760517028602		[learning rate: 0.0096349]
	Learning Rate: 0.00963488
	LOSS [training: 1.2437760517028602 | validation: 1.2299855671177076]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.245373981298875		[learning rate: 0.0096121]
	Learning Rate: 0.00961215
	LOSS [training: 1.245373981298875 | validation: 1.2497089605829945]
	TIME [epoch: 7.63 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.241306709907005		[learning rate: 0.0095895]
	Learning Rate: 0.00958948
	LOSS [training: 1.241306709907005 | validation: 1.2256012313836395]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.231839794486477		[learning rate: 0.0095669]
	Learning Rate: 0.00956686
	LOSS [training: 1.231839794486477 | validation: 1.196248227171969]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2327547156761023		[learning rate: 0.0095443]
	Learning Rate: 0.00954429
	LOSS [training: 1.2327547156761023 | validation: 1.2166452999235022]
	TIME [epoch: 7.66 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2375626785877558		[learning rate: 0.0095218]
	Learning Rate: 0.00952177
	LOSS [training: 1.2375626785877558 | validation: 1.2479432341814012]
	TIME [epoch: 7.69 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2974546436436185		[learning rate: 0.0094993]
	Learning Rate: 0.00949931
	LOSS [training: 1.2974546436436185 | validation: 1.2921342013286408]
	TIME [epoch: 7.67 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2742080999844008		[learning rate: 0.0094769]
	Learning Rate: 0.00947691
	LOSS [training: 1.2742080999844008 | validation: 1.261512229757276]
	TIME [epoch: 7.7 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2505666112585507		[learning rate: 0.0094546]
	Learning Rate: 0.00945455
	LOSS [training: 1.2505666112585507 | validation: 1.177845249883661]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.228518227450385		[learning rate: 0.0094323]
	Learning Rate: 0.00943225
	LOSS [training: 1.228518227450385 | validation: 1.1657265541669706]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2289403190205306		[learning rate: 0.00941]
	Learning Rate: 0.00941
	LOSS [training: 1.2289403190205306 | validation: 1.1985044332068453]
	TIME [epoch: 7.68 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2157722219881513		[learning rate: 0.0093878]
	Learning Rate: 0.00938781
	LOSS [training: 1.2157722219881513 | validation: 1.1946448768642224]
	TIME [epoch: 7.68 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1731471198128927		[learning rate: 0.0093657]
	Learning Rate: 0.00936566
	LOSS [training: 1.1731471198128927 | validation: 1.2585673935598614]
	TIME [epoch: 7.74 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2809284762990187		[learning rate: 0.0093436]
	Learning Rate: 0.00934357
	LOSS [training: 1.2809284762990187 | validation: 1.1702398775313263]
	TIME [epoch: 7.7 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1630333863751545		[learning rate: 0.0093215]
	Learning Rate: 0.00932153
	LOSS [training: 1.1630333863751545 | validation: 1.1376105753589458]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.150423668956258		[learning rate: 0.0092995]
	Learning Rate: 0.00929954
	LOSS [training: 1.150423668956258 | validation: 1.151237039471101]
	TIME [epoch: 7.67 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1954701800404606		[learning rate: 0.0092776]
	Learning Rate: 0.00927761
	LOSS [training: 1.1954701800404606 | validation: 1.1951048633319181]
	TIME [epoch: 7.68 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1854072359235035		[learning rate: 0.0092557]
	Learning Rate: 0.00925572
	LOSS [training: 1.1854072359235035 | validation: 1.1060755819076566]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1219371435202516		[learning rate: 0.0092339]
	Learning Rate: 0.00923389
	LOSS [training: 1.1219371435202516 | validation: 1.0577038098889477]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1153038680976093		[learning rate: 0.0092121]
	Learning Rate: 0.00921211
	LOSS [training: 1.1153038680976093 | validation: 1.069988632025261]
	TIME [epoch: 7.69 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1248964526526157		[learning rate: 0.0091904]
	Learning Rate: 0.00919038
	LOSS [training: 1.1248964526526157 | validation: 1.0895252731769416]
	TIME [epoch: 7.69 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1324862670119227		[learning rate: 0.0091687]
	Learning Rate: 0.0091687
	LOSS [training: 1.1324862670119227 | validation: 1.2161331902050616]
	TIME [epoch: 7.65 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1186387834801939		[learning rate: 0.0091471]
	Learning Rate: 0.00914707
	LOSS [training: 1.1186387834801939 | validation: 1.1185269051723388]
	TIME [epoch: 7.73 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0866237542256798		[learning rate: 0.0091255]
	Learning Rate: 0.00912549
	LOSS [training: 1.0866237542256798 | validation: 1.1085345199449255]
	TIME [epoch: 7.69 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1586491976936562		[learning rate: 0.009104]
	Learning Rate: 0.00910397
	LOSS [training: 1.1586491976936562 | validation: 1.4291655920398214]
	TIME [epoch: 7.69 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2107061129176215		[learning rate: 0.0090825]
	Learning Rate: 0.00908249
	LOSS [training: 1.2107061129176215 | validation: 1.1145677825500115]
	TIME [epoch: 7.7 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.100643473860716		[learning rate: 0.0090611]
	Learning Rate: 0.00906107
	LOSS [training: 1.100643473860716 | validation: 1.0240081839380308]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0502668478167454		[learning rate: 0.0090397]
	Learning Rate: 0.0090397
	LOSS [training: 1.0502668478167454 | validation: 1.131092162238557]
	TIME [epoch: 7.65 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.222651236877761		[learning rate: 0.0090184]
	Learning Rate: 0.00901837
	LOSS [training: 1.222651236877761 | validation: 1.236033376728253]
	TIME [epoch: 7.6 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3465030641256075		[learning rate: 0.0089971]
	Learning Rate: 0.0089971
	LOSS [training: 1.3465030641256075 | validation: 1.0519085785824593]
	TIME [epoch: 7.59 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1073653449360772		[learning rate: 0.0089759]
	Learning Rate: 0.00897588
	LOSS [training: 1.1073653449360772 | validation: 1.0143366626503207]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0930402777255162		[learning rate: 0.0089547]
	Learning Rate: 0.0089547
	LOSS [training: 1.0930402777255162 | validation: 0.9971704992705195]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0684777187740107		[learning rate: 0.0089336]
	Learning Rate: 0.00893358
	LOSS [training: 1.0684777187740107 | validation: 1.2841180134737045]
	TIME [epoch: 7.73 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1600378393040751		[learning rate: 0.0089125]
	Learning Rate: 0.00891251
	LOSS [training: 1.1600378393040751 | validation: 1.0077803368856344]
	TIME [epoch: 7.66 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.086775535763782		[learning rate: 0.0088915]
	Learning Rate: 0.00889149
	LOSS [training: 1.086775535763782 | validation: 1.0655300240617502]
	TIME [epoch: 7.67 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.099149414337401		[learning rate: 0.0088705]
	Learning Rate: 0.00887051
	LOSS [training: 1.099149414337401 | validation: 1.0873287105778293]
	TIME [epoch: 7.69 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1070651875052482		[learning rate: 0.0088496]
	Learning Rate: 0.00884959
	LOSS [training: 1.1070651875052482 | validation: 1.0922509147161574]
	TIME [epoch: 7.68 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1047741584083919		[learning rate: 0.0088287]
	Learning Rate: 0.00882871
	LOSS [training: 1.1047741584083919 | validation: 1.0136651637755265]
	TIME [epoch: 7.73 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0165040777959333		[learning rate: 0.0088079]
	Learning Rate: 0.00880789
	LOSS [training: 1.0165040777959333 | validation: 0.9696168351639128]
	TIME [epoch: 7.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1329765373064438		[learning rate: 0.0087871]
	Learning Rate: 0.00878711
	LOSS [training: 1.1329765373064438 | validation: 1.3646850542570719]
	TIME [epoch: 7.66 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.423865144894551		[learning rate: 0.0087664]
	Learning Rate: 0.00876638
	LOSS [training: 1.423865144894551 | validation: 1.1835942015121328]
	TIME [epoch: 7.66 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3148348996639827		[learning rate: 0.0087457]
	Learning Rate: 0.00874571
	LOSS [training: 1.3148348996639827 | validation: 1.0644415641290257]
	TIME [epoch: 7.69 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0930458519357102		[learning rate: 0.0087251]
	Learning Rate: 0.00872508
	LOSS [training: 1.0930458519357102 | validation: 1.053163257680291]
	TIME [epoch: 7.7 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0334434772401888		[learning rate: 0.0087045]
	Learning Rate: 0.00870449
	LOSS [training: 1.0334434772401888 | validation: 0.998018888939133]
	TIME [epoch: 7.63 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0504815192343298		[learning rate: 0.008684]
	Learning Rate: 0.00868396
	LOSS [training: 1.0504815192343298 | validation: 1.0362708182930829]
	TIME [epoch: 7.67 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.04503696494287		[learning rate: 0.0086635]
	Learning Rate: 0.00866348
	LOSS [training: 1.04503696494287 | validation: 0.9779350765546289]
	TIME [epoch: 7.67 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1211694329872612		[learning rate: 0.008643]
	Learning Rate: 0.00864304
	LOSS [training: 1.1211694329872612 | validation: 1.491035002332167]
	TIME [epoch: 7.68 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4097343718344046		[learning rate: 0.0086227]
	Learning Rate: 0.00862265
	LOSS [training: 1.4097343718344046 | validation: 1.3515641651117105]
	TIME [epoch: 7.7 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3091294997181229		[learning rate: 0.0086023]
	Learning Rate: 0.00860232
	LOSS [training: 1.3091294997181229 | validation: 1.2714757881318626]
	TIME [epoch: 7.66 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.274430695230366		[learning rate: 0.008582]
	Learning Rate: 0.00858202
	LOSS [training: 1.274430695230366 | validation: 1.2127441699869341]
	TIME [epoch: 7.68 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2377376510186906		[learning rate: 0.0085618]
	Learning Rate: 0.00856178
	LOSS [training: 1.2377376510186906 | validation: 1.1981227880061978]
	TIME [epoch: 7.66 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1603024790982321		[learning rate: 0.0085416]
	Learning Rate: 0.00854158
	LOSS [training: 1.1603024790982321 | validation: 1.0809323426170132]
	TIME [epoch: 7.69 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0904722965449996		[learning rate: 0.0085214]
	Learning Rate: 0.00852144
	LOSS [training: 1.0904722965449996 | validation: 1.037181123537053]
	TIME [epoch: 7.7 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0844012351719656		[learning rate: 0.0085013]
	Learning Rate: 0.00850134
	LOSS [training: 1.0844012351719656 | validation: 1.0122205999289497]
	TIME [epoch: 7.66 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0287870480763983		[learning rate: 0.0084813]
	Learning Rate: 0.00848128
	LOSS [training: 1.0287870480763983 | validation: 1.0807647253273152]
	TIME [epoch: 7.65 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2361908536132216		[learning rate: 0.0084613]
	Learning Rate: 0.00846128
	LOSS [training: 1.2361908536132216 | validation: 1.4780239422309012]
	TIME [epoch: 7.65 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4930336237905608		[learning rate: 0.0084413]
	Learning Rate: 0.00844132
	LOSS [training: 1.4930336237905608 | validation: 1.4583376262636807]
	TIME [epoch: 7.68 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4192137143712105		[learning rate: 0.0084214]
	Learning Rate: 0.00842141
	LOSS [training: 1.4192137143712105 | validation: 1.3347777646355832]
	TIME [epoch: 7.7 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2786729236917997		[learning rate: 0.0084015]
	Learning Rate: 0.00840154
	LOSS [training: 1.2786729236917997 | validation: 1.1615582770359714]
	TIME [epoch: 7.63 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.140657367228057		[learning rate: 0.0083817]
	Learning Rate: 0.00838172
	LOSS [training: 1.140657367228057 | validation: 1.0942445132522356]
	TIME [epoch: 7.64 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.107589403624268		[learning rate: 0.008362]
	Learning Rate: 0.00836195
	LOSS [training: 1.107589403624268 | validation: 1.054137783556957]
	TIME [epoch: 7.66 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0702094423725512		[learning rate: 0.0083422]
	Learning Rate: 0.00834223
	LOSS [training: 1.0702094423725512 | validation: 1.0128567444449748]
	TIME [epoch: 7.67 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0798255555214316		[learning rate: 0.0083225]
	Learning Rate: 0.00832255
	LOSS [training: 1.0798255555214316 | validation: 1.0405792956243152]
	TIME [epoch: 7.71 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.072159956958588		[learning rate: 0.0083029]
	Learning Rate: 0.00830292
	LOSS [training: 1.072159956958588 | validation: 0.990159396313141]
	TIME [epoch: 7.66 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0277492194090734		[learning rate: 0.0082833]
	Learning Rate: 0.00828333
	LOSS [training: 1.0277492194090734 | validation: 0.9931195822448899]
	TIME [epoch: 7.65 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0533655966612612		[learning rate: 0.0082638]
	Learning Rate: 0.00826379
	LOSS [training: 1.0533655966612612 | validation: 1.0392932662398686]
	TIME [epoch: 7.66 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0711418584067842		[learning rate: 0.0082443]
	Learning Rate: 0.0082443
	LOSS [training: 1.0711418584067842 | validation: 1.1028082892696034]
	TIME [epoch: 7.69 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1075281771017456		[learning rate: 0.0082249]
	Learning Rate: 0.00822485
	LOSS [training: 1.1075281771017456 | validation: 1.10540834160171]
	TIME [epoch: 7.7 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1094611229500353		[learning rate: 0.0082055]
	Learning Rate: 0.00820545
	LOSS [training: 1.1094611229500353 | validation: 1.0876763851237896]
	TIME [epoch: 7.68 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1190353267838462		[learning rate: 0.0081861]
	Learning Rate: 0.0081861
	LOSS [training: 1.1190353267838462 | validation: 1.1037996909688117]
	TIME [epoch: 7.65 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1014437451669556		[learning rate: 0.0081668]
	Learning Rate: 0.00816679
	LOSS [training: 1.1014437451669556 | validation: 1.069494702199759]
	TIME [epoch: 7.68 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0777913124870915		[learning rate: 0.0081475]
	Learning Rate: 0.00814752
	LOSS [training: 1.0777913124870915 | validation: 1.082089200543425]
	TIME [epoch: 7.68 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.088468546980352		[learning rate: 0.0081283]
	Learning Rate: 0.0081283
	LOSS [training: 1.088468546980352 | validation: 1.0641373155166494]
	TIME [epoch: 7.71 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.080827188535552		[learning rate: 0.0081091]
	Learning Rate: 0.00810913
	LOSS [training: 1.080827188535552 | validation: 1.0539151301799645]
	TIME [epoch: 7.65 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0535434366319152		[learning rate: 0.00809]
	Learning Rate: 0.00809
	LOSS [training: 1.0535434366319152 | validation: 1.0267086944540358]
	TIME [epoch: 7.67 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0205382221020607		[learning rate: 0.0080709]
	Learning Rate: 0.00807092
	LOSS [training: 1.0205382221020607 | validation: 1.0053788221402145]
	TIME [epoch: 7.68 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0544876547299158		[learning rate: 0.0080519]
	Learning Rate: 0.00805188
	LOSS [training: 1.0544876547299158 | validation: 1.013110400241406]
	TIME [epoch: 7.68 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0152749466783315		[learning rate: 0.0080329]
	Learning Rate: 0.00803289
	LOSS [training: 1.0152749466783315 | validation: 0.9514255749619804]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9750563236798493		[learning rate: 0.0080139]
	Learning Rate: 0.00801394
	LOSS [training: 0.9750563236798493 | validation: 0.9434997063276828]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9861656340491454		[learning rate: 0.007995]
	Learning Rate: 0.00799504
	LOSS [training: 0.9861656340491454 | validation: 0.9269411087477454]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9530278820116164		[learning rate: 0.0079762]
	Learning Rate: 0.00797618
	LOSS [training: 0.9530278820116164 | validation: 0.8619905348082195]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9453641833866238		[learning rate: 0.0079574]
	Learning Rate: 0.00795736
	LOSS [training: 0.9453641833866238 | validation: 0.8241167625131873]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9394363821872788		[learning rate: 0.0079386]
	Learning Rate: 0.00793859
	LOSS [training: 0.9394363821872788 | validation: 0.8608399406709042]
	TIME [epoch: 7.67 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8804828652490047		[learning rate: 0.0079199]
	Learning Rate: 0.00791987
	LOSS [training: 0.8804828652490047 | validation: 0.8652252795333985]
	TIME [epoch: 7.64 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9499794452719924		[learning rate: 0.0079012]
	Learning Rate: 0.00790119
	LOSS [training: 0.9499794452719924 | validation: 0.8495847825124606]
	TIME [epoch: 7.63 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8970445026592271		[learning rate: 0.0078825]
	Learning Rate: 0.00788255
	LOSS [training: 0.8970445026592271 | validation: 0.8012001077375798]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.93008764203528		[learning rate: 0.007864]
	Learning Rate: 0.00786396
	LOSS [training: 0.93008764203528 | validation: 0.9986669461117701]
	TIME [epoch: 7.68 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9476618507663067		[learning rate: 0.0078454]
	Learning Rate: 0.00784541
	LOSS [training: 0.9476618507663067 | validation: 0.827738496228223]
	TIME [epoch: 7.66 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8598288130765969		[learning rate: 0.0078269]
	Learning Rate: 0.0078269
	LOSS [training: 0.8598288130765969 | validation: 0.7598280115371483]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9030493453359869		[learning rate: 0.0078084]
	Learning Rate: 0.00780844
	LOSS [training: 0.9030493453359869 | validation: 0.930348036544148]
	TIME [epoch: 7.67 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0114366686373024		[learning rate: 0.00779]
	Learning Rate: 0.00779002
	LOSS [training: 1.0114366686373024 | validation: 0.9455095254817643]
	TIME [epoch: 7.66 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.932650831909968		[learning rate: 0.0077716]
	Learning Rate: 0.00777164
	LOSS [training: 0.932650831909968 | validation: 0.8249001298162023]
	TIME [epoch: 7.69 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9183007849959031		[learning rate: 0.0077533]
	Learning Rate: 0.00775331
	LOSS [training: 0.9183007849959031 | validation: 0.9739071571952229]
	TIME [epoch: 7.68 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0087428392424125		[learning rate: 0.007735]
	Learning Rate: 0.00773502
	LOSS [training: 1.0087428392424125 | validation: 0.9443027348244033]
	TIME [epoch: 7.67 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9136426559073265		[learning rate: 0.0077168]
	Learning Rate: 0.00771678
	LOSS [training: 0.9136426559073265 | validation: 0.7998276142474103]
	TIME [epoch: 7.64 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8070459954736878		[learning rate: 0.0076986]
	Learning Rate: 0.00769857
	LOSS [training: 0.8070459954736878 | validation: 0.8012664102524938]
	TIME [epoch: 7.65 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8920060320915171		[learning rate: 0.0076804]
	Learning Rate: 0.00768041
	LOSS [training: 0.8920060320915171 | validation: 0.9158077969000331]
	TIME [epoch: 7.71 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9363859001189697		[learning rate: 0.0076623]
	Learning Rate: 0.0076623
	LOSS [training: 0.9363859001189697 | validation: 0.8801950757965924]
	TIME [epoch: 7.69 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8349526655725368		[learning rate: 0.0076442]
	Learning Rate: 0.00764422
	LOSS [training: 0.8349526655725368 | validation: 1.0244788231552062]
	TIME [epoch: 7.68 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8790366918113084		[learning rate: 0.0076262]
	Learning Rate: 0.00762619
	LOSS [training: 0.8790366918113084 | validation: 0.8330697126541204]
	TIME [epoch: 7.66 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9078924383104099		[learning rate: 0.0076082]
	Learning Rate: 0.0076082
	LOSS [training: 0.9078924383104099 | validation: 0.868778131475368]
	TIME [epoch: 7.67 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8587809570273977		[learning rate: 0.0075903]
	Learning Rate: 0.00759025
	LOSS [training: 0.8587809570273977 | validation: 0.9499448895614009]
	TIME [epoch: 7.7 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8368550875302238		[learning rate: 0.0075724]
	Learning Rate: 0.00757235
	LOSS [training: 0.8368550875302238 | validation: 0.7048153374578119]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7774926507516913		[learning rate: 0.0075545]
	Learning Rate: 0.00755449
	LOSS [training: 0.7774926507516913 | validation: 0.7707517663851291]
	TIME [epoch: 7.64 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8103006988863437		[learning rate: 0.0075367]
	Learning Rate: 0.00753667
	LOSS [training: 0.8103006988863437 | validation: 0.8365095037272119]
	TIME [epoch: 7.68 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8751187334528886		[learning rate: 0.0075189]
	Learning Rate: 0.00751889
	LOSS [training: 0.8751187334528886 | validation: 0.9389272847892571]
	TIME [epoch: 7.69 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.845747136840313		[learning rate: 0.0075012]
	Learning Rate: 0.00750116
	LOSS [training: 0.845747136840313 | validation: 0.8212582241970695]
	TIME [epoch: 7.73 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7623877911094143		[learning rate: 0.0074835]
	Learning Rate: 0.00748346
	LOSS [training: 0.7623877911094143 | validation: 0.8944847054967762]
	TIME [epoch: 7.7 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8445320640517233		[learning rate: 0.0074658]
	Learning Rate: 0.00746581
	LOSS [training: 0.8445320640517233 | validation: 0.7894528079062054]
	TIME [epoch: 7.7 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8001229468071536		[learning rate: 0.0074482]
	Learning Rate: 0.0074482
	LOSS [training: 0.8001229468071536 | validation: 0.6951361330993272]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7556735792984717		[learning rate: 0.0074306]
	Learning Rate: 0.00743063
	LOSS [training: 0.7556735792984717 | validation: 0.7088396521936854]
	TIME [epoch: 7.69 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.785153167302544		[learning rate: 0.0074131]
	Learning Rate: 0.0074131
	LOSS [training: 0.785153167302544 | validation: 0.8345505212341249]
	TIME [epoch: 7.73 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8147399687186756		[learning rate: 0.0073956]
	Learning Rate: 0.00739562
	LOSS [training: 0.8147399687186756 | validation: 0.9092063628327192]
	TIME [epoch: 7.69 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8226705599987791		[learning rate: 0.0073782]
	Learning Rate: 0.00737817
	LOSS [training: 0.8226705599987791 | validation: 0.6663456839843289]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7434283280009113		[learning rate: 0.0073608]
	Learning Rate: 0.00736077
	LOSS [training: 0.7434283280009113 | validation: 0.7585702279921969]
	TIME [epoch: 7.68 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7546051992832583		[learning rate: 0.0073434]
	Learning Rate: 0.0073434
	LOSS [training: 0.7546051992832583 | validation: 0.8659929214913082]
	TIME [epoch: 7.67 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8017564224460556		[learning rate: 0.0073261]
	Learning Rate: 0.00732608
	LOSS [training: 0.8017564224460556 | validation: 0.694463003087324]
	TIME [epoch: 7.73 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7267164579475629		[learning rate: 0.0073088]
	Learning Rate: 0.0073088
	LOSS [training: 0.7267164579475629 | validation: 0.7738700148435012]
	TIME [epoch: 7.68 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7354233056496923		[learning rate: 0.0072916]
	Learning Rate: 0.00729156
	LOSS [training: 0.7354233056496923 | validation: 0.7292251384711224]
	TIME [epoch: 7.67 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.720469735059082		[learning rate: 0.0072744]
	Learning Rate: 0.00727436
	LOSS [training: 0.720469735059082 | validation: 1.117770457314153]
	TIME [epoch: 7.68 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9463435992490448		[learning rate: 0.0072572]
	Learning Rate: 0.0072572
	LOSS [training: 0.9463435992490448 | validation: 0.8333091615248808]
	TIME [epoch: 7.67 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7712657685126306		[learning rate: 0.0072401]
	Learning Rate: 0.00724008
	LOSS [training: 0.7712657685126306 | validation: 0.6796021418997141]
	TIME [epoch: 7.71 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7179885530728185		[learning rate: 0.007223]
	Learning Rate: 0.007223
	LOSS [training: 0.7179885530728185 | validation: 1.0209999544131196]
	TIME [epoch: 7.68 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7270559572386331		[learning rate: 0.007206]
	Learning Rate: 0.00720597
	LOSS [training: 0.7270559572386331 | validation: 0.6967414269200967]
	TIME [epoch: 7.68 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8195860317301749		[learning rate: 0.007189]
	Learning Rate: 0.00718897
	LOSS [training: 0.8195860317301749 | validation: 0.8614736653985934]
	TIME [epoch: 7.67 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7932166520205175		[learning rate: 0.007172]
	Learning Rate: 0.00717201
	LOSS [training: 0.7932166520205175 | validation: 0.7785793448212821]
	TIME [epoch: 7.68 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6720527718518547		[learning rate: 0.0071551]
	Learning Rate: 0.00715509
	LOSS [training: 0.6720527718518547 | validation: 0.9397701338592246]
	TIME [epoch: 7.73 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7883453775297931		[learning rate: 0.0071382]
	Learning Rate: 0.00713822
	LOSS [training: 0.7883453775297931 | validation: 0.978261506076133]
	TIME [epoch: 7.68 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7738566350799054		[learning rate: 0.0071214]
	Learning Rate: 0.00712138
	LOSS [training: 0.7738566350799054 | validation: 0.8326479207228712]
	TIME [epoch: 7.68 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8386398570623109		[learning rate: 0.0071046]
	Learning Rate: 0.00710458
	LOSS [training: 0.8386398570623109 | validation: 0.9180192669146856]
	TIME [epoch: 7.67 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8706144509561176		[learning rate: 0.0070878]
	Learning Rate: 0.00708782
	LOSS [training: 0.8706144509561176 | validation: 0.7394618732041223]
	TIME [epoch: 7.68 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7070690776811268		[learning rate: 0.0070711]
	Learning Rate: 0.0070711
	LOSS [training: 0.7070690776811268 | validation: 0.7497027871701867]
	TIME [epoch: 7.73 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7400427100102142		[learning rate: 0.0070544]
	Learning Rate: 0.00705442
	LOSS [training: 0.7400427100102142 | validation: 0.6979202859759556]
	TIME [epoch: 7.68 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9413714391953047		[learning rate: 0.0070378]
	Learning Rate: 0.00703778
	LOSS [training: 0.9413714391953047 | validation: 1.5310258964729764]
	TIME [epoch: 7.66 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.295839889409931		[learning rate: 0.0070212]
	Learning Rate: 0.00702118
	LOSS [training: 1.295839889409931 | validation: 1.3365242764746714]
	TIME [epoch: 7.66 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1246245263635106		[learning rate: 0.0070046]
	Learning Rate: 0.00700462
	LOSS [training: 1.1246245263635106 | validation: 1.135666548327999]
	TIME [epoch: 123 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0150279132564055		[learning rate: 0.0069881]
	Learning Rate: 0.0069881
	LOSS [training: 1.0150279132564055 | validation: 1.0773938672218164]
	TIME [epoch: 15.1 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9292058202662407		[learning rate: 0.0069716]
	Learning Rate: 0.00697161
	LOSS [training: 0.9292058202662407 | validation: 0.9509591981956602]
	TIME [epoch: 15 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8618682168622915		[learning rate: 0.0069552]
	Learning Rate: 0.00695517
	LOSS [training: 0.8618682168622915 | validation: 0.8822461331850667]
	TIME [epoch: 15 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8255508364048174		[learning rate: 0.0069388]
	Learning Rate: 0.00693876
	LOSS [training: 0.8255508364048174 | validation: 0.8171191642174662]
	TIME [epoch: 15 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6868190437388377		[learning rate: 0.0069224]
	Learning Rate: 0.00692239
	LOSS [training: 0.6868190437388377 | validation: 0.7188880802701549]
	TIME [epoch: 14.9 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7217323523504855		[learning rate: 0.0069061]
	Learning Rate: 0.00690607
	LOSS [training: 0.7217323523504855 | validation: 0.6308019431677134]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5724773172067251		[learning rate: 0.0068898]
	Learning Rate: 0.00688978
	LOSS [training: 0.5724773172067251 | validation: 0.5976317068609853]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.625172823631863		[learning rate: 0.0068735]
	Learning Rate: 0.00687352
	LOSS [training: 0.625172823631863 | validation: 0.6377531891176094]
	TIME [epoch: 15.1 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6186715854333307		[learning rate: 0.0068573]
	Learning Rate: 0.00685731
	LOSS [training: 0.6186715854333307 | validation: 0.8589117507989243]
	TIME [epoch: 15 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6581401870925083		[learning rate: 0.0068411]
	Learning Rate: 0.00684114
	LOSS [training: 0.6581401870925083 | validation: 0.6266812063124201]
	TIME [epoch: 15 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6402486382436025		[learning rate: 0.006825]
	Learning Rate: 0.006825
	LOSS [training: 0.6402486382436025 | validation: 0.7929064926908149]
	TIME [epoch: 15 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5661051007831596		[learning rate: 0.0068089]
	Learning Rate: 0.0068089
	LOSS [training: 0.5661051007831596 | validation: 0.5075806114169131]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.601744834955344		[learning rate: 0.0067928]
	Learning Rate: 0.00679284
	LOSS [training: 0.601744834955344 | validation: 0.8137892399569775]
	TIME [epoch: 15 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7658317919491677		[learning rate: 0.0067768]
	Learning Rate: 0.00677681
	LOSS [training: 0.7658317919491677 | validation: 1.0533480157087973]
	TIME [epoch: 15.1 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8987300904292398		[learning rate: 0.0067608]
	Learning Rate: 0.00676083
	LOSS [training: 0.8987300904292398 | validation: 0.5946945988840728]
	TIME [epoch: 15 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7473216838024285		[learning rate: 0.0067449]
	Learning Rate: 0.00674488
	LOSS [training: 0.7473216838024285 | validation: 0.5506857899288737]
	TIME [epoch: 15.1 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5465720311713611		[learning rate: 0.006729]
	Learning Rate: 0.00672897
	LOSS [training: 0.5465720311713611 | validation: 0.6160455131708533]
	TIME [epoch: 15 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6244608196766661		[learning rate: 0.0067131]
	Learning Rate: 0.0067131
	LOSS [training: 0.6244608196766661 | validation: 0.6352940280474476]
	TIME [epoch: 15 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6679264462125509		[learning rate: 0.0066973]
	Learning Rate: 0.00669726
	LOSS [training: 0.6679264462125509 | validation: 0.5811074511125668]
	TIME [epoch: 15 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6760526957203389		[learning rate: 0.0066815]
	Learning Rate: 0.00668147
	LOSS [training: 0.6760526957203389 | validation: 0.7207272495845611]
	TIME [epoch: 15 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5947102040414548		[learning rate: 0.0066657]
	Learning Rate: 0.00666571
	LOSS [training: 0.5947102040414548 | validation: 0.7423977571662502]
	TIME [epoch: 15 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7322598843995483		[learning rate: 0.00665]
	Learning Rate: 0.00664998
	LOSS [training: 0.7322598843995483 | validation: 0.6543795468444571]
	TIME [epoch: 15 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.790147202563971		[learning rate: 0.0066343]
	Learning Rate: 0.0066343
	LOSS [training: 0.790147202563971 | validation: 0.7739191473814524]
	TIME [epoch: 15 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.699916439699456		[learning rate: 0.0066186]
	Learning Rate: 0.00661865
	LOSS [training: 0.699916439699456 | validation: 0.5922947653875985]
	TIME [epoch: 15 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6886644027807012		[learning rate: 0.006603]
	Learning Rate: 0.00660304
	LOSS [training: 0.6886644027807012 | validation: 0.8143529210201916]
	TIME [epoch: 15 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7127461936632591		[learning rate: 0.0065875]
	Learning Rate: 0.00658746
	LOSS [training: 0.7127461936632591 | validation: 0.7248715879135738]
	TIME [epoch: 14.9 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6873414280386481		[learning rate: 0.0065719]
	Learning Rate: 0.00657192
	LOSS [training: 0.6873414280386481 | validation: 0.6690725715698054]
	TIME [epoch: 15 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7134990528964368		[learning rate: 0.0065564]
	Learning Rate: 0.00655642
	LOSS [training: 0.7134990528964368 | validation: 0.7258070808205506]
	TIME [epoch: 15 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6763967897218153		[learning rate: 0.006541]
	Learning Rate: 0.00654095
	LOSS [training: 0.6763967897218153 | validation: 0.6441889775494475]
	TIME [epoch: 15 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5947897739869632		[learning rate: 0.0065255]
	Learning Rate: 0.00652552
	LOSS [training: 0.5947897739869632 | validation: 0.8040378822162203]
	TIME [epoch: 14.9 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7166207340640285		[learning rate: 0.0065101]
	Learning Rate: 0.00651013
	LOSS [training: 0.7166207340640285 | validation: 0.8206756629602134]
	TIME [epoch: 14.9 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7451557809776603		[learning rate: 0.0064948]
	Learning Rate: 0.00649477
	LOSS [training: 0.7451557809776603 | validation: 0.6425633362188469]
	TIME [epoch: 15 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6369219851043294		[learning rate: 0.0064795]
	Learning Rate: 0.00647945
	LOSS [training: 0.6369219851043294 | validation: 0.5356483042828541]
	TIME [epoch: 14.9 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6831427283055511		[learning rate: 0.0064642]
	Learning Rate: 0.00646417
	LOSS [training: 0.6831427283055511 | validation: 0.8564132370892508]
	TIME [epoch: 14.9 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8536308677056544		[learning rate: 0.0064489]
	Learning Rate: 0.00644892
	LOSS [training: 0.8536308677056544 | validation: 0.7360908460841353]
	TIME [epoch: 15 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.700970790996145		[learning rate: 0.0064337]
	Learning Rate: 0.00643371
	LOSS [training: 0.700970790996145 | validation: 0.6357326265834305]
	TIME [epoch: 14.9 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6700860345781666		[learning rate: 0.0064185]
	Learning Rate: 0.00641853
	LOSS [training: 0.6700860345781666 | validation: 0.729222243180703]
	TIME [epoch: 15 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5860512708838286		[learning rate: 0.0064034]
	Learning Rate: 0.00640339
	LOSS [training: 0.5860512708838286 | validation: 0.6081660935958867]
	TIME [epoch: 15 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7656541740264645		[learning rate: 0.0063883]
	Learning Rate: 0.00638829
	LOSS [training: 0.7656541740264645 | validation: 0.7700850673105801]
	TIME [epoch: 15 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6908751471293278		[learning rate: 0.0063732]
	Learning Rate: 0.00637322
	LOSS [training: 0.6908751471293278 | validation: 0.6587099997889977]
	TIME [epoch: 15 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8493377417349088		[learning rate: 0.0063582]
	Learning Rate: 0.00635819
	LOSS [training: 0.8493377417349088 | validation: 0.9926594138977133]
	TIME [epoch: 15 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7665081510170089		[learning rate: 0.0063432]
	Learning Rate: 0.00634319
	LOSS [training: 0.7665081510170089 | validation: 0.7046586964951893]
	TIME [epoch: 14.9 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.600001346014751		[learning rate: 0.0063282]
	Learning Rate: 0.00632823
	LOSS [training: 0.600001346014751 | validation: 0.6597003438260799]
	TIME [epoch: 15 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6711839196927697		[learning rate: 0.0063133]
	Learning Rate: 0.0063133
	LOSS [training: 0.6711839196927697 | validation: 0.6219127981668191]
	TIME [epoch: 15 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.635391483936723		[learning rate: 0.0062984]
	Learning Rate: 0.00629841
	LOSS [training: 0.635391483936723 | validation: 0.8073442939125715]
	TIME [epoch: 15 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.683625760382813		[learning rate: 0.0062836]
	Learning Rate: 0.00628355
	LOSS [training: 0.683625760382813 | validation: 0.6139846131564484]
	TIME [epoch: 14.9 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6496455490945001		[learning rate: 0.0062687]
	Learning Rate: 0.00626873
	LOSS [training: 0.6496455490945001 | validation: 0.6301852854203391]
	TIME [epoch: 14.9 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6158752984679392		[learning rate: 0.0062539]
	Learning Rate: 0.00625394
	LOSS [training: 0.6158752984679392 | validation: 0.6000459993635922]
	TIME [epoch: 15 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6243653171164744		[learning rate: 0.0062392]
	Learning Rate: 0.00623919
	LOSS [training: 0.6243653171164744 | validation: 0.6542706282025847]
	TIME [epoch: 15 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6815585853142023		[learning rate: 0.0062245]
	Learning Rate: 0.00622447
	LOSS [training: 0.6815585853142023 | validation: 0.6153488931475711]
	TIME [epoch: 15 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5650318184252469		[learning rate: 0.0062098]
	Learning Rate: 0.00620979
	LOSS [training: 0.5650318184252469 | validation: 0.697989831848534]
	TIME [epoch: 15 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6562665470660858		[learning rate: 0.0061951]
	Learning Rate: 0.00619514
	LOSS [training: 0.6562665470660858 | validation: 0.6817041807490635]
	TIME [epoch: 15 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7116169853808286		[learning rate: 0.0061805]
	Learning Rate: 0.00618053
	LOSS [training: 0.7116169853808286 | validation: 0.7211213952988649]
	TIME [epoch: 15 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.733380840093713		[learning rate: 0.0061659]
	Learning Rate: 0.00616595
	LOSS [training: 0.733380840093713 | validation: 0.8222679233489343]
	TIME [epoch: 14.9 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6629009513362821		[learning rate: 0.0061514]
	Learning Rate: 0.00615141
	LOSS [training: 0.6629009513362821 | validation: 0.633974571610176]
	TIME [epoch: 14.9 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5627091749724877		[learning rate: 0.0061369]
	Learning Rate: 0.0061369
	LOSS [training: 0.5627091749724877 | validation: 1.0243804039783633]
	TIME [epoch: 15 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7056954094575344		[learning rate: 0.0061224]
	Learning Rate: 0.00612242
	LOSS [training: 0.7056954094575344 | validation: 0.6079423065846377]
	TIME [epoch: 15 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5825804597824555		[learning rate: 0.006108]
	Learning Rate: 0.00610798
	LOSS [training: 0.5825804597824555 | validation: 0.6009501626600873]
	TIME [epoch: 15 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6175498335404745		[learning rate: 0.0060936]
	Learning Rate: 0.00609357
	LOSS [training: 0.6175498335404745 | validation: 0.4898470983823747]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7079692469260723		[learning rate: 0.0060792]
	Learning Rate: 0.0060792
	LOSS [training: 0.7079692469260723 | validation: 0.662786776985376]
	TIME [epoch: 15 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6130726480268629		[learning rate: 0.0060649]
	Learning Rate: 0.00606486
	LOSS [training: 0.6130726480268629 | validation: 0.6050439413105457]
	TIME [epoch: 15.1 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5706775939188551		[learning rate: 0.0060505]
	Learning Rate: 0.00605055
	LOSS [training: 0.5706775939188551 | validation: 0.6061145916807076]
	TIME [epoch: 15 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5495857678191185		[learning rate: 0.0060363]
	Learning Rate: 0.00603628
	LOSS [training: 0.5495857678191185 | validation: 0.7831562016097312]
	TIME [epoch: 15 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6541989657933637		[learning rate: 0.006022]
	Learning Rate: 0.00602204
	LOSS [training: 0.6541989657933637 | validation: 0.5822047450639161]
	TIME [epoch: 15 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49530895616057613		[learning rate: 0.0060078]
	Learning Rate: 0.00600783
	LOSS [training: 0.49530895616057613 | validation: 0.61475379525624]
	TIME [epoch: 15 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5792458497199581		[learning rate: 0.0059937]
	Learning Rate: 0.00599366
	LOSS [training: 0.5792458497199581 | validation: 0.540898012498098]
	TIME [epoch: 15.1 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7047410534206447		[learning rate: 0.0059795]
	Learning Rate: 0.00597952
	LOSS [training: 0.7047410534206447 | validation: 0.5867930156362515]
	TIME [epoch: 15 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5383043124753428		[learning rate: 0.0059654]
	Learning Rate: 0.00596542
	LOSS [training: 0.5383043124753428 | validation: 0.6846049588631034]
	TIME [epoch: 15 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6014423388810128		[learning rate: 0.0059513]
	Learning Rate: 0.00595135
	LOSS [training: 0.6014423388810128 | validation: 0.7115492955720586]
	TIME [epoch: 15 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5731979374304292		[learning rate: 0.0059373]
	Learning Rate: 0.00593731
	LOSS [training: 0.5731979374304292 | validation: 0.5896545395292181]
	TIME [epoch: 14.9 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6006436979157654		[learning rate: 0.0059233]
	Learning Rate: 0.00592331
	LOSS [training: 0.6006436979157654 | validation: 0.5621937859194965]
	TIME [epoch: 15 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.523423348693258		[learning rate: 0.0059093]
	Learning Rate: 0.00590933
	LOSS [training: 0.523423348693258 | validation: 0.6105651982850391]
	TIME [epoch: 15.1 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6260881992263438		[learning rate: 0.0058954]
	Learning Rate: 0.00589539
	LOSS [training: 0.6260881992263438 | validation: 0.8517748967380834]
	TIME [epoch: 15 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6709992631693009		[learning rate: 0.0058815]
	Learning Rate: 0.00588149
	LOSS [training: 0.6709992631693009 | validation: 0.6931132831913674]
	TIME [epoch: 15.1 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5241205041751199		[learning rate: 0.0058676]
	Learning Rate: 0.00586761
	LOSS [training: 0.5241205041751199 | validation: 0.5883971200965148]
	TIME [epoch: 15 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6533690287795804		[learning rate: 0.0058538]
	Learning Rate: 0.00585377
	LOSS [training: 0.6533690287795804 | validation: 0.4882762166194075]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5657307701991233		[learning rate: 0.00584]
	Learning Rate: 0.00583996
	LOSS [training: 0.5657307701991233 | validation: 0.8002690670062191]
	TIME [epoch: 15.1 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6448582986118829		[learning rate: 0.0058262]
	Learning Rate: 0.00582619
	LOSS [training: 0.6448582986118829 | validation: 0.6132924372304317]
	TIME [epoch: 15 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6114469312702849		[learning rate: 0.0058124]
	Learning Rate: 0.00581245
	LOSS [training: 0.6114469312702849 | validation: 0.6467759366766574]
	TIME [epoch: 15 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5825885144628205		[learning rate: 0.0057987]
	Learning Rate: 0.00579874
	LOSS [training: 0.5825885144628205 | validation: 0.608408264351308]
	TIME [epoch: 15 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5739396180026981		[learning rate: 0.0057851]
	Learning Rate: 0.00578506
	LOSS [training: 0.5739396180026981 | validation: 0.5991520615734143]
	TIME [epoch: 15 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6325085432536521		[learning rate: 0.0057714]
	Learning Rate: 0.00577141
	LOSS [training: 0.6325085432536521 | validation: 0.6718063850920879]
	TIME [epoch: 15 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6071126166548997		[learning rate: 0.0057578]
	Learning Rate: 0.0057578
	LOSS [training: 0.6071126166548997 | validation: 0.5095369650248162]
	TIME [epoch: 15 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5653561069280807		[learning rate: 0.0057442]
	Learning Rate: 0.00574422
	LOSS [training: 0.5653561069280807 | validation: 0.6983895457572641]
	TIME [epoch: 15 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5476204797969554		[learning rate: 0.0057307]
	Learning Rate: 0.00573067
	LOSS [training: 0.5476204797969554 | validation: 0.8873628856383033]
	TIME [epoch: 15 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6460464641763375		[learning rate: 0.0057171]
	Learning Rate: 0.00571715
	LOSS [training: 0.6460464641763375 | validation: 0.5643682508212792]
	TIME [epoch: 14.9 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5273015002125914		[learning rate: 0.0057037]
	Learning Rate: 0.00570366
	LOSS [training: 0.5273015002125914 | validation: 0.5017893058148554]
	TIME [epoch: 15 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5194764397255814		[learning rate: 0.0056902]
	Learning Rate: 0.00569021
	LOSS [training: 0.5194764397255814 | validation: 0.6558513890994035]
	TIME [epoch: 15 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6644671070067715		[learning rate: 0.0056768]
	Learning Rate: 0.00567679
	LOSS [training: 0.6644671070067715 | validation: 0.7040881267912733]
	TIME [epoch: 15 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6077939438701101		[learning rate: 0.0056634]
	Learning Rate: 0.0056634
	LOSS [training: 0.6077939438701101 | validation: 0.5895791842410134]
	TIME [epoch: 15.1 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5939839459936658		[learning rate: 0.00565]
	Learning Rate: 0.00565004
	LOSS [training: 0.5939839459936658 | validation: 0.5471430446505254]
	TIME [epoch: 15 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48515001646689393		[learning rate: 0.0056367]
	Learning Rate: 0.00563671
	LOSS [training: 0.48515001646689393 | validation: 0.5455486783164818]
	TIME [epoch: 15 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.568917666309266		[learning rate: 0.0056234]
	Learning Rate: 0.00562341
	LOSS [training: 0.568917666309266 | validation: 0.5998712051901026]
	TIME [epoch: 15 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5360629453564476		[learning rate: 0.0056101]
	Learning Rate: 0.00561015
	LOSS [training: 0.5360629453564476 | validation: 0.5850411718968894]
	TIME [epoch: 15 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49865570120990904		[learning rate: 0.0055969]
	Learning Rate: 0.00559691
	LOSS [training: 0.49865570120990904 | validation: 0.7766797454435623]
	TIME [epoch: 15.1 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.647381621247947		[learning rate: 0.0055837]
	Learning Rate: 0.00558371
	LOSS [training: 0.647381621247947 | validation: 0.5162322160667188]
	TIME [epoch: 15 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4726903435452731		[learning rate: 0.0055705]
	Learning Rate: 0.00557054
	LOSS [training: 0.4726903435452731 | validation: 0.5030112182927562]
	TIME [epoch: 15 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5125595642102914		[learning rate: 0.0055574]
	Learning Rate: 0.0055574
	LOSS [training: 0.5125595642102914 | validation: 0.5550248816443722]
	TIME [epoch: 15 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5531549056320032		[learning rate: 0.0055443]
	Learning Rate: 0.00554429
	LOSS [training: 0.5531549056320032 | validation: 0.5411423411580456]
	TIME [epoch: 15 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4812536873524208		[learning rate: 0.0055312]
	Learning Rate: 0.00553121
	LOSS [training: 0.4812536873524208 | validation: 0.7058560431101144]
	TIME [epoch: 15.1 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6831019980950354		[learning rate: 0.0055182]
	Learning Rate: 0.00551817
	LOSS [training: 0.6831019980950354 | validation: 0.66239893410168]
	TIME [epoch: 15.1 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5280176487374887		[learning rate: 0.0055052]
	Learning Rate: 0.00550515
	LOSS [training: 0.5280176487374887 | validation: 0.5083883891905326]
	TIME [epoch: 15 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45519225556166265		[learning rate: 0.0054922]
	Learning Rate: 0.00549216
	LOSS [training: 0.45519225556166265 | validation: 0.556361344559047]
	TIME [epoch: 15.1 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6449450275288784		[learning rate: 0.0054792]
	Learning Rate: 0.00547921
	LOSS [training: 0.6449450275288784 | validation: 0.6344822092385209]
	TIME [epoch: 15 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5866249718504072		[learning rate: 0.0054663]
	Learning Rate: 0.00546629
	LOSS [training: 0.5866249718504072 | validation: 0.5681066709714117]
	TIME [epoch: 15 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5609274641521549		[learning rate: 0.0054534]
	Learning Rate: 0.00545339
	LOSS [training: 0.5609274641521549 | validation: 0.4937280987970969]
	TIME [epoch: 15.1 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6099476203418503		[learning rate: 0.0054405]
	Learning Rate: 0.00544053
	LOSS [training: 0.6099476203418503 | validation: 0.5840184859668731]
	TIME [epoch: 15.1 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5338088066147868		[learning rate: 0.0054277]
	Learning Rate: 0.00542769
	LOSS [training: 0.5338088066147868 | validation: 0.4963400200880797]
	TIME [epoch: 15.1 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5109870264884876		[learning rate: 0.0054149]
	Learning Rate: 0.00541489
	LOSS [training: 0.5109870264884876 | validation: 0.5890402449991894]
	TIME [epoch: 15.1 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5959773872900351		[learning rate: 0.0054021]
	Learning Rate: 0.00540212
	LOSS [training: 0.5959773872900351 | validation: 0.6361698076857762]
	TIME [epoch: 15 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5891161666744431		[learning rate: 0.0053894]
	Learning Rate: 0.00538938
	LOSS [training: 0.5891161666744431 | validation: 0.6704815865978946]
	TIME [epoch: 15 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4864545606962346		[learning rate: 0.0053767]
	Learning Rate: 0.00537666
	LOSS [training: 0.4864545606962346 | validation: 0.6109578232209429]
	TIME [epoch: 14.9 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5790890914942259		[learning rate: 0.005364]
	Learning Rate: 0.00536398
	LOSS [training: 0.5790890914942259 | validation: 0.5847442744714957]
	TIME [epoch: 15 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5073932913967303		[learning rate: 0.0053513]
	Learning Rate: 0.00535133
	LOSS [training: 0.5073932913967303 | validation: 0.5253545570391894]
	TIME [epoch: 15.1 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48264991327285733		[learning rate: 0.0053387]
	Learning Rate: 0.00533871
	LOSS [training: 0.48264991327285733 | validation: 0.5746288785453124]
	TIME [epoch: 15 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5458954114113648		[learning rate: 0.0053261]
	Learning Rate: 0.00532611
	LOSS [training: 0.5458954114113648 | validation: 0.6262117126124406]
	TIME [epoch: 15.1 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5407078086010688		[learning rate: 0.0053135]
	Learning Rate: 0.00531355
	LOSS [training: 0.5407078086010688 | validation: 0.5220829960898998]
	TIME [epoch: 15 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4315031820238934		[learning rate: 0.005301]
	Learning Rate: 0.00530101
	LOSS [training: 0.4315031820238934 | validation: 0.46932793945884727]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7612772515780069		[learning rate: 0.0052885]
	Learning Rate: 0.00528851
	LOSS [training: 0.7612772515780069 | validation: 0.6021481676150191]
	TIME [epoch: 15 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5547220534769824		[learning rate: 0.005276]
	Learning Rate: 0.00527604
	LOSS [training: 0.5547220534769824 | validation: 0.5448621854973407]
	TIME [epoch: 15 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5106694776195784		[learning rate: 0.0052636]
	Learning Rate: 0.00526359
	LOSS [training: 0.5106694776195784 | validation: 0.7759799458943794]
	TIME [epoch: 15.1 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5378071003263312		[learning rate: 0.0052512]
	Learning Rate: 0.00525117
	LOSS [training: 0.5378071003263312 | validation: 0.48103110693834733]
	TIME [epoch: 15.1 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43444360236678314		[learning rate: 0.0052388]
	Learning Rate: 0.00523879
	LOSS [training: 0.43444360236678314 | validation: 0.5454796341224009]
	TIME [epoch: 15 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48429586379188		[learning rate: 0.0052264]
	Learning Rate: 0.00522643
	LOSS [training: 0.48429586379188 | validation: 0.8322438129521851]
	TIME [epoch: 15 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5324364418812209		[learning rate: 0.0052141]
	Learning Rate: 0.0052141
	LOSS [training: 0.5324364418812209 | validation: 0.4233458097651031]
	TIME [epoch: 15.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5008339365034618		[learning rate: 0.0052018]
	Learning Rate: 0.0052018
	LOSS [training: 0.5008339365034618 | validation: 0.5250258125046198]
	TIME [epoch: 15 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.516439049361981		[learning rate: 0.0051895]
	Learning Rate: 0.00518953
	LOSS [training: 0.516439049361981 | validation: 0.45738690053020437]
	TIME [epoch: 15.1 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.458841812104269		[learning rate: 0.0051773]
	Learning Rate: 0.00517729
	LOSS [training: 0.458841812104269 | validation: 0.7767466119475845]
	TIME [epoch: 15 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5631002942759654		[learning rate: 0.0051651]
	Learning Rate: 0.00516508
	LOSS [training: 0.5631002942759654 | validation: 0.5140647644298559]
	TIME [epoch: 15 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4196479003586324		[learning rate: 0.0051529]
	Learning Rate: 0.00515289
	LOSS [training: 0.4196479003586324 | validation: 0.6981838886644027]
	TIME [epoch: 15.1 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5115767197116704		[learning rate: 0.0051407]
	Learning Rate: 0.00514074
	LOSS [training: 0.5115767197116704 | validation: 0.531594645667849]
	TIME [epoch: 15 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46490425212163233		[learning rate: 0.0051286]
	Learning Rate: 0.00512861
	LOSS [training: 0.46490425212163233 | validation: 0.6895518112828711]
	TIME [epoch: 15 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49634404558226275		[learning rate: 0.0051165]
	Learning Rate: 0.00511652
	LOSS [training: 0.49634404558226275 | validation: 0.4156842374267758]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48648597680012995		[learning rate: 0.0051044]
	Learning Rate: 0.00510445
	LOSS [training: 0.48648597680012995 | validation: 0.43317706047197]
	TIME [epoch: 15 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4851120162246141		[learning rate: 0.0050924]
	Learning Rate: 0.00509241
	LOSS [training: 0.4851120162246141 | validation: 0.5209550955258246]
	TIME [epoch: 15 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44750246119551623		[learning rate: 0.0050804]
	Learning Rate: 0.00508039
	LOSS [training: 0.44750246119551623 | validation: 0.43571801663531473]
	TIME [epoch: 15 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5233763144613572		[learning rate: 0.0050684]
	Learning Rate: 0.00506841
	LOSS [training: 0.5233763144613572 | validation: 0.6698141395844851]
	TIME [epoch: 15 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5987237526700375		[learning rate: 0.0050565]
	Learning Rate: 0.00505645
	LOSS [training: 0.5987237526700375 | validation: 0.4724088362569713]
	TIME [epoch: 15.1 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40250208083632383		[learning rate: 0.0050445]
	Learning Rate: 0.00504453
	LOSS [training: 0.40250208083632383 | validation: 0.504323912824743]
	TIME [epoch: 15.1 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5840379284350209		[learning rate: 0.0050326]
	Learning Rate: 0.00503263
	LOSS [training: 0.5840379284350209 | validation: 0.6506486221960439]
	TIME [epoch: 15 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6241812278700731		[learning rate: 0.0050208]
	Learning Rate: 0.00502076
	LOSS [training: 0.6241812278700731 | validation: 0.4787023456835347]
	TIME [epoch: 15.1 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5031691586494125		[learning rate: 0.0050089]
	Learning Rate: 0.00500891
	LOSS [training: 0.5031691586494125 | validation: 0.4716997952177462]
	TIME [epoch: 15 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4637202880958767		[learning rate: 0.0049971]
	Learning Rate: 0.0049971
	LOSS [training: 0.4637202880958767 | validation: 0.5010508371069364]
	TIME [epoch: 15 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4411791311882261		[learning rate: 0.0049853]
	Learning Rate: 0.00498531
	LOSS [training: 0.4411791311882261 | validation: 0.6174995373490315]
	TIME [epoch: 15.1 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6370439721992994		[learning rate: 0.0049736]
	Learning Rate: 0.00497355
	LOSS [training: 0.6370439721992994 | validation: 0.5688471663727674]
	TIME [epoch: 15 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5283918645852912		[learning rate: 0.0049618]
	Learning Rate: 0.00496182
	LOSS [training: 0.5283918645852912 | validation: 0.6238071394857349]
	TIME [epoch: 15 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6027951092056292		[learning rate: 0.0049501]
	Learning Rate: 0.00495012
	LOSS [training: 0.6027951092056292 | validation: 0.5498747709458152]
	TIME [epoch: 15.1 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5320989336331701		[learning rate: 0.0049384]
	Learning Rate: 0.00493844
	LOSS [training: 0.5320989336331701 | validation: 0.5571618312402195]
	TIME [epoch: 15 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43984622871472195		[learning rate: 0.0049268]
	Learning Rate: 0.00492679
	LOSS [training: 0.43984622871472195 | validation: 0.4671855860069305]
	TIME [epoch: 15 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6524666291057077		[learning rate: 0.0049152]
	Learning Rate: 0.00491517
	LOSS [training: 0.6524666291057077 | validation: 0.5319759049527648]
	TIME [epoch: 15 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5434184552295563		[learning rate: 0.0049036]
	Learning Rate: 0.00490357
	LOSS [training: 0.5434184552295563 | validation: 0.4892420107896164]
	TIME [epoch: 15 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48890879869099735		[learning rate: 0.004892]
	Learning Rate: 0.00489201
	LOSS [training: 0.48890879869099735 | validation: 0.7180818284534762]
	TIME [epoch: 15.1 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5456940544646119		[learning rate: 0.0048805]
	Learning Rate: 0.00488047
	LOSS [training: 0.5456940544646119 | validation: 0.5827629629660753]
	TIME [epoch: 15.1 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4296458986886819		[learning rate: 0.004869]
	Learning Rate: 0.00486896
	LOSS [training: 0.4296458986886819 | validation: 0.39908143184178935]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5843626273173536		[learning rate: 0.0048575]
	Learning Rate: 0.00485747
	LOSS [training: 0.5843626273173536 | validation: 0.8281440725355997]
	TIME [epoch: 15 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.65396811018222		[learning rate: 0.004846]
	Learning Rate: 0.00484601
	LOSS [training: 0.65396811018222 | validation: 0.5142266622189157]
	TIME [epoch: 15 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.436108998876002		[learning rate: 0.0048346]
	Learning Rate: 0.00483458
	LOSS [training: 0.436108998876002 | validation: 0.4894622639910925]
	TIME [epoch: 14.9 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4729415396391703		[learning rate: 0.0048232]
	Learning Rate: 0.00482318
	LOSS [training: 0.4729415396391703 | validation: 0.6479338322223285]
	TIME [epoch: 15 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48337078272195866		[learning rate: 0.0048118]
	Learning Rate: 0.0048118
	LOSS [training: 0.48337078272195866 | validation: 0.4228788708362513]
	TIME [epoch: 15 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48559561958725117		[learning rate: 0.0048005]
	Learning Rate: 0.00480045
	LOSS [training: 0.48559561958725117 | validation: 0.5071570005983408]
	TIME [epoch: 15 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4435894581468073		[learning rate: 0.0047891]
	Learning Rate: 0.00478913
	LOSS [training: 0.4435894581468073 | validation: 0.4207625065488651]
	TIME [epoch: 15 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4948415033959034		[learning rate: 0.0047778]
	Learning Rate: 0.00477783
	LOSS [training: 0.4948415033959034 | validation: 0.6242608224888448]
	TIME [epoch: 15 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43766147197510974		[learning rate: 0.0047666]
	Learning Rate: 0.00476656
	LOSS [training: 0.43766147197510974 | validation: 0.4736223009842442]
	TIME [epoch: 15 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5038092143850454		[learning rate: 0.0047553]
	Learning Rate: 0.00475532
	LOSS [training: 0.5038092143850454 | validation: 0.5354882615646517]
	TIME [epoch: 14.9 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47000778818809463		[learning rate: 0.0047441]
	Learning Rate: 0.0047441
	LOSS [training: 0.47000778818809463 | validation: 0.4419466383446371]
	TIME [epoch: 15 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.368769906777112		[learning rate: 0.0047329]
	Learning Rate: 0.00473291
	LOSS [training: 0.368769906777112 | validation: 0.41769032422926355]
	TIME [epoch: 15.1 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6130167716446349		[learning rate: 0.0047217]
	Learning Rate: 0.00472175
	LOSS [training: 0.6130167716446349 | validation: 0.7018390144570605]
	TIME [epoch: 15 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6152078460425459		[learning rate: 0.0047106]
	Learning Rate: 0.00471061
	LOSS [training: 0.6152078460425459 | validation: 0.49214525640798035]
	TIME [epoch: 15 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4244950158421725		[learning rate: 0.0046995]
	Learning Rate: 0.0046995
	LOSS [training: 0.4244950158421725 | validation: 0.7980648767421568]
	TIME [epoch: 15 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49060985725012396		[learning rate: 0.0046884]
	Learning Rate: 0.00468841
	LOSS [training: 0.49060985725012396 | validation: 0.5222733668517023]
	TIME [epoch: 15 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4119580283806531		[learning rate: 0.0046774]
	Learning Rate: 0.00467735
	LOSS [training: 0.4119580283806531 | validation: 0.5935248150137434]
	TIME [epoch: 15 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5057420190945506		[learning rate: 0.0046663]
	Learning Rate: 0.00466632
	LOSS [training: 0.5057420190945506 | validation: 0.47380215962547706]
	TIME [epoch: 15 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4603328034440649		[learning rate: 0.0046553]
	Learning Rate: 0.00465531
	LOSS [training: 0.4603328034440649 | validation: 0.4204990736304776]
	TIME [epoch: 15 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4461336077773126		[learning rate: 0.0046443]
	Learning Rate: 0.00464433
	LOSS [training: 0.4461336077773126 | validation: 0.5713074264298184]
	TIME [epoch: 15 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4579248828631904		[learning rate: 0.0046334]
	Learning Rate: 0.00463338
	LOSS [training: 0.4579248828631904 | validation: 0.4473237675691646]
	TIME [epoch: 15.1 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.371092330091943		[learning rate: 0.0046224]
	Learning Rate: 0.00462245
	LOSS [training: 0.371092330091943 | validation: 0.6162183867669305]
	TIME [epoch: 15 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5643830046371141		[learning rate: 0.0046115]
	Learning Rate: 0.00461154
	LOSS [training: 0.5643830046371141 | validation: 0.48670351780937693]
	TIME [epoch: 15.1 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4344027062278505		[learning rate: 0.0046007]
	Learning Rate: 0.00460066
	LOSS [training: 0.4344027062278505 | validation: 0.41280716858765976]
	TIME [epoch: 15.1 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5080699544069307		[learning rate: 0.0045898]
	Learning Rate: 0.00458981
	LOSS [training: 0.5080699544069307 | validation: 0.5686836248349614]
	TIME [epoch: 15 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5359806434882096		[learning rate: 0.004579]
	Learning Rate: 0.00457899
	LOSS [training: 0.5359806434882096 | validation: 0.48047675883996577]
	TIME [epoch: 15 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.408870934843522		[learning rate: 0.0045682]
	Learning Rate: 0.00456818
	LOSS [training: 0.408870934843522 | validation: 0.5101902255232084]
	TIME [epoch: 15 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.486525263037742		[learning rate: 0.0045574]
	Learning Rate: 0.00455741
	LOSS [training: 0.486525263037742 | validation: 0.4931440782227393]
	TIME [epoch: 15 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45076386301537535		[learning rate: 0.0045467]
	Learning Rate: 0.00454666
	LOSS [training: 0.45076386301537535 | validation: 0.5003403739416427]
	TIME [epoch: 15 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5195252170567177		[learning rate: 0.0045359]
	Learning Rate: 0.00453593
	LOSS [training: 0.5195252170567177 | validation: 0.48839125947374795]
	TIME [epoch: 15 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4180438564644382		[learning rate: 0.0045252]
	Learning Rate: 0.00452523
	LOSS [training: 0.4180438564644382 | validation: 0.4657507842034183]
	TIME [epoch: 15 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5316160656610731		[learning rate: 0.0045146]
	Learning Rate: 0.00451456
	LOSS [training: 0.5316160656610731 | validation: 0.5237631686768316]
	TIME [epoch: 15 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6170830860597586		[learning rate: 0.0045039]
	Learning Rate: 0.00450391
	LOSS [training: 0.6170830860597586 | validation: 0.5945577873023316]
	TIME [epoch: 14.9 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5246660415720329		[learning rate: 0.0044933]
	Learning Rate: 0.00449329
	LOSS [training: 0.5246660415720329 | validation: 0.45763820246220066]
	TIME [epoch: 15 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4464993691235555		[learning rate: 0.0044827]
	Learning Rate: 0.00448269
	LOSS [training: 0.4464993691235555 | validation: 0.5416514002691077]
	TIME [epoch: 15 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5971830185099269		[learning rate: 0.0044721]
	Learning Rate: 0.00447211
	LOSS [training: 0.5971830185099269 | validation: 0.5522091473321886]
	TIME [epoch: 15 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4953660357398462		[learning rate: 0.0044616]
	Learning Rate: 0.00446156
	LOSS [training: 0.4953660357398462 | validation: 0.5023735719897988]
	TIME [epoch: 15 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49613046123672877		[learning rate: 0.004451]
	Learning Rate: 0.00445104
	LOSS [training: 0.49613046123672877 | validation: 0.4985556902316852]
	TIME [epoch: 15 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4365845144416871		[learning rate: 0.0044405]
	Learning Rate: 0.00444054
	LOSS [training: 0.4365845144416871 | validation: 0.5645193313388919]
	TIME [epoch: 15 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5327518406800028		[learning rate: 0.0044301]
	Learning Rate: 0.00443007
	LOSS [training: 0.5327518406800028 | validation: 0.4310968146313017]
	TIME [epoch: 15 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4765562066514834		[learning rate: 0.0044196]
	Learning Rate: 0.00441962
	LOSS [training: 0.4765562066514834 | validation: 0.5848323162712799]
	TIME [epoch: 15 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5380866254973181		[learning rate: 0.0044092]
	Learning Rate: 0.00440919
	LOSS [training: 0.5380866254973181 | validation: 0.4612442003454519]
	TIME [epoch: 15 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4085185912406394		[learning rate: 0.0043988]
	Learning Rate: 0.00439879
	LOSS [training: 0.4085185912406394 | validation: 0.4331927891216496]
	TIME [epoch: 15 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46269588077466434		[learning rate: 0.0043884]
	Learning Rate: 0.00438841
	LOSS [training: 0.46269588077466434 | validation: 0.6391497174625786]
	TIME [epoch: 15 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7009828174807636		[learning rate: 0.0043781]
	Learning Rate: 0.00437806
	LOSS [training: 0.7009828174807636 | validation: 0.5526138365111501]
	TIME [epoch: 15 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.515640429972061		[learning rate: 0.0043677]
	Learning Rate: 0.00436774
	LOSS [training: 0.515640429972061 | validation: 0.46357838812761576]
	TIME [epoch: 15.1 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40876084518675415		[learning rate: 0.0043574]
	Learning Rate: 0.00435743
	LOSS [training: 0.40876084518675415 | validation: 0.4808490948768822]
	TIME [epoch: 15 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.424554548123255		[learning rate: 0.0043472]
	Learning Rate: 0.00434715
	LOSS [training: 0.424554548123255 | validation: 0.4589919338168926]
	TIME [epoch: 15 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3965200329198848		[learning rate: 0.0043369]
	Learning Rate: 0.0043369
	LOSS [training: 0.3965200329198848 | validation: 0.6046985200276471]
	TIME [epoch: 15.1 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4265020331946461		[learning rate: 0.0043267]
	Learning Rate: 0.00432667
	LOSS [training: 0.4265020331946461 | validation: 0.5050899599801603]
	TIME [epoch: 14.9 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44764629400403566		[learning rate: 0.0043165]
	Learning Rate: 0.00431646
	LOSS [training: 0.44764629400403566 | validation: 0.37784882765858063]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43208121959648627		[learning rate: 0.0043063]
	Learning Rate: 0.00430628
	LOSS [training: 0.43208121959648627 | validation: 0.7675023880027594]
	TIME [epoch: 14.9 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5043623463218343		[learning rate: 0.0042961]
	Learning Rate: 0.00429612
	LOSS [training: 0.5043623463218343 | validation: 0.4331384284767713]
	TIME [epoch: 15 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3877765686832998		[learning rate: 0.004286]
	Learning Rate: 0.00428599
	LOSS [training: 0.3877765686832998 | validation: 0.4075160354647741]
	TIME [epoch: 15 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3448600213114014		[learning rate: 0.0042759]
	Learning Rate: 0.00427588
	LOSS [training: 0.3448600213114014 | validation: 0.43726333829665465]
	TIME [epoch: 15 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48661701221763065		[learning rate: 0.0042658]
	Learning Rate: 0.0042658
	LOSS [training: 0.48661701221763065 | validation: 0.524099798607218]
	TIME [epoch: 15 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5480791055446076		[learning rate: 0.0042557]
	Learning Rate: 0.00425573
	LOSS [training: 0.5480791055446076 | validation: 0.5100940182288267]
	TIME [epoch: 15 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4098243469195411		[learning rate: 0.0042457]
	Learning Rate: 0.00424569
	LOSS [training: 0.4098243469195411 | validation: 0.43734073227149994]
	TIME [epoch: 15 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40669095213850914		[learning rate: 0.0042357]
	Learning Rate: 0.00423568
	LOSS [training: 0.40669095213850914 | validation: 0.4484920872814889]
	TIME [epoch: 15 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4979194251035919		[learning rate: 0.0042257]
	Learning Rate: 0.00422569
	LOSS [training: 0.4979194251035919 | validation: 0.42394059237716497]
	TIME [epoch: 15 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39773339636942806		[learning rate: 0.0042157]
	Learning Rate: 0.00421572
	LOSS [training: 0.39773339636942806 | validation: 0.4745321066990794]
	TIME [epoch: 15 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44163779817534676		[learning rate: 0.0042058]
	Learning Rate: 0.00420578
	LOSS [training: 0.44163779817534676 | validation: 0.6346053859396776]
	TIME [epoch: 15.1 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44919225765113624		[learning rate: 0.0041959]
	Learning Rate: 0.00419585
	LOSS [training: 0.44919225765113624 | validation: 0.4065863253657293]
	TIME [epoch: 15 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3554779009735259		[learning rate: 0.004186]
	Learning Rate: 0.00418596
	LOSS [training: 0.3554779009735259 | validation: 0.49308517110489924]
	TIME [epoch: 15 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42493018549944633		[learning rate: 0.0041761]
	Learning Rate: 0.00417608
	LOSS [training: 0.42493018549944633 | validation: 0.6049343376106486]
	TIME [epoch: 15 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4521278580996796		[learning rate: 0.0041662]
	Learning Rate: 0.00416623
	LOSS [training: 0.4521278580996796 | validation: 0.4507167895216688]
	TIME [epoch: 14.9 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.414177615098523		[learning rate: 0.0041564]
	Learning Rate: 0.00415641
	LOSS [training: 0.414177615098523 | validation: 0.35983101073748147]
	TIME [epoch: 15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38653356559887325		[learning rate: 0.0041466]
	Learning Rate: 0.0041466
	LOSS [training: 0.38653356559887325 | validation: 0.57032236933085]
	TIME [epoch: 15 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4853884212216814		[learning rate: 0.0041368]
	Learning Rate: 0.00413682
	LOSS [training: 0.4853884212216814 | validation: 0.488263859132639]
	TIME [epoch: 15 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40759538579124244		[learning rate: 0.0041271]
	Learning Rate: 0.00412706
	LOSS [training: 0.40759538579124244 | validation: 0.4672123650393549]
	TIME [epoch: 15 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4270574843986073		[learning rate: 0.0041173]
	Learning Rate: 0.00411733
	LOSS [training: 0.4270574843986073 | validation: 0.5164266079025575]
	TIME [epoch: 15.1 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40169305432626057		[learning rate: 0.0041076]
	Learning Rate: 0.00410762
	LOSS [training: 0.40169305432626057 | validation: 0.4359062146800309]
	TIME [epoch: 15 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49655119261486874		[learning rate: 0.0040979]
	Learning Rate: 0.00409793
	LOSS [training: 0.49655119261486874 | validation: 0.43740039215726445]
	TIME [epoch: 15 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39722162727507593		[learning rate: 0.0040883]
	Learning Rate: 0.00408826
	LOSS [training: 0.39722162727507593 | validation: 0.43329597703842493]
	TIME [epoch: 15.1 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48163006164798233		[learning rate: 0.0040786]
	Learning Rate: 0.00407862
	LOSS [training: 0.48163006164798233 | validation: 0.5277158669907683]
	TIME [epoch: 15 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4468240523735531		[learning rate: 0.004069]
	Learning Rate: 0.004069
	LOSS [training: 0.4468240523735531 | validation: 0.39185475664893304]
	TIME [epoch: 15 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42608464352240355		[learning rate: 0.0040594]
	Learning Rate: 0.0040594
	LOSS [training: 0.42608464352240355 | validation: 0.4328269738833932]
	TIME [epoch: 14.9 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4805394344202312		[learning rate: 0.0040498]
	Learning Rate: 0.00404982
	LOSS [training: 0.4805394344202312 | validation: 0.5641699966959928]
	TIME [epoch: 14.9 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4481408399159725		[learning rate: 0.0040403]
	Learning Rate: 0.00404027
	LOSS [training: 0.4481408399159725 | validation: 0.4463795146676453]
	TIME [epoch: 15 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5008511688689077		[learning rate: 0.0040307]
	Learning Rate: 0.00403074
	LOSS [training: 0.5008511688689077 | validation: 0.4418114912427098]
	TIME [epoch: 14.9 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4198783261078527		[learning rate: 0.0040212]
	Learning Rate: 0.00402123
	LOSS [training: 0.4198783261078527 | validation: 0.5242517746321248]
	TIME [epoch: 14.9 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.451871475531728		[learning rate: 0.0040117]
	Learning Rate: 0.00401175
	LOSS [training: 0.451871475531728 | validation: 0.42363927653423683]
	TIME [epoch: 15 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39803206790051265		[learning rate: 0.0040023]
	Learning Rate: 0.00400228
	LOSS [training: 0.39803206790051265 | validation: 0.3672572272270028]
	TIME [epoch: 14.9 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44231541194278934		[learning rate: 0.0039928]
	Learning Rate: 0.00399284
	LOSS [training: 0.44231541194278934 | validation: 0.45112091349658756]
	TIME [epoch: 14.9 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48157766821240805		[learning rate: 0.0039834]
	Learning Rate: 0.00398342
	LOSS [training: 0.48157766821240805 | validation: 0.5000144143542045]
	TIME [epoch: 15 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4212835822038396		[learning rate: 0.003974]
	Learning Rate: 0.00397403
	LOSS [training: 0.4212835822038396 | validation: 0.39382009925221156]
	TIME [epoch: 14.9 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34329445340960313		[learning rate: 0.0039647]
	Learning Rate: 0.00396465
	LOSS [training: 0.34329445340960313 | validation: 0.44306906146546476]
	TIME [epoch: 15 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4844984839136398		[learning rate: 0.0039553]
	Learning Rate: 0.0039553
	LOSS [training: 0.4844984839136398 | validation: 0.399658191112891]
	TIME [epoch: 15 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46236162542480186		[learning rate: 0.003946]
	Learning Rate: 0.00394597
	LOSS [training: 0.46236162542480186 | validation: 0.3955164560750131]
	TIME [epoch: 14.9 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3849164230290429		[learning rate: 0.0039367]
	Learning Rate: 0.00393666
	LOSS [training: 0.3849164230290429 | validation: 0.37861550895100626]
	TIME [epoch: 15 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38588128259254295		[learning rate: 0.0039274]
	Learning Rate: 0.00392738
	LOSS [training: 0.38588128259254295 | validation: 0.37182117934230446]
	TIME [epoch: 14.9 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4342656105864383		[learning rate: 0.0039181]
	Learning Rate: 0.00391811
	LOSS [training: 0.4342656105864383 | validation: 0.42002924437991673]
	TIME [epoch: 14.9 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3612360844878022		[learning rate: 0.0039089]
	Learning Rate: 0.00390887
	LOSS [training: 0.3612360844878022 | validation: 0.4362451684051488]
	TIME [epoch: 15 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42333398599984984		[learning rate: 0.0038996]
	Learning Rate: 0.00389965
	LOSS [training: 0.42333398599984984 | validation: 0.4434720449745315]
	TIME [epoch: 14.9 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40635266410921345		[learning rate: 0.0038905]
	Learning Rate: 0.00389045
	LOSS [training: 0.40635266410921345 | validation: 0.48999525807515754]
	TIME [epoch: 14.9 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42794439421346575		[learning rate: 0.0038813]
	Learning Rate: 0.00388127
	LOSS [training: 0.42794439421346575 | validation: 0.38434793597387307]
	TIME [epoch: 15 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.365295409397445		[learning rate: 0.0038721]
	Learning Rate: 0.00387212
	LOSS [training: 0.365295409397445 | validation: 0.43877059759713954]
	TIME [epoch: 14.9 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41970692277259264		[learning rate: 0.003863]
	Learning Rate: 0.00386299
	LOSS [training: 0.41970692277259264 | validation: 0.47317004052227407]
	TIME [epoch: 14.9 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4059829615327225		[learning rate: 0.0038539]
	Learning Rate: 0.00385387
	LOSS [training: 0.4059829615327225 | validation: 0.40893444581717875]
	TIME [epoch: 15 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4736040995105022		[learning rate: 0.0038448]
	Learning Rate: 0.00384478
	LOSS [training: 0.4736040995105022 | validation: 0.44148419368843655]
	TIME [epoch: 14.9 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.548937811807755		[learning rate: 0.0038357]
	Learning Rate: 0.00383571
	LOSS [training: 0.548937811807755 | validation: 0.47013179871989486]
	TIME [epoch: 14.9 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38330420843849156		[learning rate: 0.0038267]
	Learning Rate: 0.00382667
	LOSS [training: 0.38330420843849156 | validation: 0.4442118154160284]
	TIME [epoch: 15 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4128391551003229		[learning rate: 0.0038176]
	Learning Rate: 0.00381764
	LOSS [training: 0.4128391551003229 | validation: 0.40050856713829786]
	TIME [epoch: 14.9 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4813739477194838		[learning rate: 0.0038086]
	Learning Rate: 0.00380863
	LOSS [training: 0.4813739477194838 | validation: 0.46769857948038296]
	TIME [epoch: 15 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3842700512526516		[learning rate: 0.0037996]
	Learning Rate: 0.00379965
	LOSS [training: 0.3842700512526516 | validation: 0.3879309110094464]
	TIME [epoch: 14.9 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4642548630123921		[learning rate: 0.0037907]
	Learning Rate: 0.00379069
	LOSS [training: 0.4642548630123921 | validation: 0.4433193611189065]
	TIME [epoch: 14.9 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4176243199094534		[learning rate: 0.0037817]
	Learning Rate: 0.00378175
	LOSS [training: 0.4176243199094534 | validation: 0.4125232411997099]
	TIME [epoch: 15 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37223551183035863		[learning rate: 0.0037728]
	Learning Rate: 0.00377283
	LOSS [training: 0.37223551183035863 | validation: 0.3962423733986331]
	TIME [epoch: 14.9 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3434508865446358		[learning rate: 0.0037639]
	Learning Rate: 0.00376393
	LOSS [training: 0.3434508865446358 | validation: 0.4139439038157193]
	TIME [epoch: 14.9 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40336858220467886		[learning rate: 0.003755]
	Learning Rate: 0.00375505
	LOSS [training: 0.40336858220467886 | validation: 0.555502813080786]
	TIME [epoch: 15 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45607142347533136		[learning rate: 0.0037462]
	Learning Rate: 0.00374619
	LOSS [training: 0.45607142347533136 | validation: 0.4413906759746846]
	TIME [epoch: 14.9 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3745906202738536		[learning rate: 0.0037374]
	Learning Rate: 0.00373735
	LOSS [training: 0.3745906202738536 | validation: 0.3465272081177729]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4474324230309191		[learning rate: 0.0037285]
	Learning Rate: 0.00372854
	LOSS [training: 0.4474324230309191 | validation: 0.4187687658859058]
	TIME [epoch: 15 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3658238640152213		[learning rate: 0.0037197]
	Learning Rate: 0.00371974
	LOSS [training: 0.3658238640152213 | validation: 0.3817048078429309]
	TIME [epoch: 14.9 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43322583909272505		[learning rate: 0.003711]
	Learning Rate: 0.00371097
	LOSS [training: 0.43322583909272505 | validation: 0.49196137803161955]
	TIME [epoch: 15 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38217988580453477		[learning rate: 0.0037022]
	Learning Rate: 0.00370221
	LOSS [training: 0.38217988580453477 | validation: 0.39049776730662833]
	TIME [epoch: 15 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39441613397692576		[learning rate: 0.0036935]
	Learning Rate: 0.00369348
	LOSS [training: 0.39441613397692576 | validation: 0.4808452711231914]
	TIME [epoch: 14.9 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.421817454692776		[learning rate: 0.0036848]
	Learning Rate: 0.00368477
	LOSS [training: 0.421817454692776 | validation: 0.43406590999781913]
	TIME [epoch: 15 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3798088616736336		[learning rate: 0.0036761]
	Learning Rate: 0.00367608
	LOSS [training: 0.3798088616736336 | validation: 0.5878212475521097]
	TIME [epoch: 15 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4706202257705109		[learning rate: 0.0036674]
	Learning Rate: 0.00366741
	LOSS [training: 0.4706202257705109 | validation: 0.4095685229271918]
	TIME [epoch: 15 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34077449185728664		[learning rate: 0.0036588]
	Learning Rate: 0.00365875
	LOSS [training: 0.34077449185728664 | validation: 0.3544631152987604]
	TIME [epoch: 15 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4305429077498015		[learning rate: 0.0036501]
	Learning Rate: 0.00365012
	LOSS [training: 0.4305429077498015 | validation: 0.39763192002871384]
	TIME [epoch: 14.9 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4354668535190005		[learning rate: 0.0036415]
	Learning Rate: 0.00364151
	LOSS [training: 0.4354668535190005 | validation: 0.39200467269238437]
	TIME [epoch: 14.9 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3501268602857581		[learning rate: 0.0036329]
	Learning Rate: 0.00363292
	LOSS [training: 0.3501268602857581 | validation: 0.35012654093578877]
	TIME [epoch: 15 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4205414252713162		[learning rate: 0.0036244]
	Learning Rate: 0.00362436
	LOSS [training: 0.4205414252713162 | validation: 0.4250013275325101]
	TIME [epoch: 14.9 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3820359333630268		[learning rate: 0.0036158]
	Learning Rate: 0.00361581
	LOSS [training: 0.3820359333630268 | validation: 0.5150931513420615]
	TIME [epoch: 14.9 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45896226801517326		[learning rate: 0.0036073]
	Learning Rate: 0.00360728
	LOSS [training: 0.45896226801517326 | validation: 0.5553564665681063]
	TIME [epoch: 15 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4388936503119125		[learning rate: 0.0035988]
	Learning Rate: 0.00359877
	LOSS [training: 0.4388936503119125 | validation: 0.4080663888925643]
	TIME [epoch: 14.9 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41720852945258047		[learning rate: 0.0035903]
	Learning Rate: 0.00359028
	LOSS [training: 0.41720852945258047 | validation: 0.3820563919677117]
	TIME [epoch: 14.9 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3516754130714419		[learning rate: 0.0035818]
	Learning Rate: 0.00358181
	LOSS [training: 0.3516754130714419 | validation: 0.5412513457195747]
	TIME [epoch: 15 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5417046457719317		[learning rate: 0.0035734]
	Learning Rate: 0.00357336
	LOSS [training: 0.5417046457719317 | validation: 0.5287763382674537]
	TIME [epoch: 14.9 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44614116011254396		[learning rate: 0.0035649]
	Learning Rate: 0.00356493
	LOSS [training: 0.44614116011254396 | validation: 0.44764327246954866]
	TIME [epoch: 15 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3808168643987304		[learning rate: 0.0035565]
	Learning Rate: 0.00355652
	LOSS [training: 0.3808168643987304 | validation: 0.4242251872202025]
	TIME [epoch: 14.9 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3940022687294386		[learning rate: 0.0035481]
	Learning Rate: 0.00354813
	LOSS [training: 0.3940022687294386 | validation: 0.34014865770226976]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43918501121753517		[learning rate: 0.0035398]
	Learning Rate: 0.00353976
	LOSS [training: 0.43918501121753517 | validation: 0.3920004723711072]
	TIME [epoch: 15 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4416616140334335		[learning rate: 0.0035314]
	Learning Rate: 0.00353141
	LOSS [training: 0.4416616140334335 | validation: 0.43128872748721714]
	TIME [epoch: 15 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35926866068811936		[learning rate: 0.0035231]
	Learning Rate: 0.00352308
	LOSS [training: 0.35926866068811936 | validation: 0.49325332482072887]
	TIME [epoch: 15 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42692831950804905		[learning rate: 0.0035148]
	Learning Rate: 0.00351477
	LOSS [training: 0.42692831950804905 | validation: 0.43133242967711916]
	TIME [epoch: 15.1 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3688865375489171		[learning rate: 0.0035065]
	Learning Rate: 0.00350648
	LOSS [training: 0.3688865375489171 | validation: 0.3610277682453894]
	TIME [epoch: 15 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32314813328910164		[learning rate: 0.0034982]
	Learning Rate: 0.00349821
	LOSS [training: 0.32314813328910164 | validation: 0.388366630566655]
	TIME [epoch: 15.1 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4923207020723127		[learning rate: 0.00349]
	Learning Rate: 0.00348996
	LOSS [training: 0.4923207020723127 | validation: 0.40318301179495686]
	TIME [epoch: 15 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35954289325958155		[learning rate: 0.0034817]
	Learning Rate: 0.00348173
	LOSS [training: 0.35954289325958155 | validation: 0.34399830526192365]
	TIME [epoch: 14.9 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36725023667537116		[learning rate: 0.0034735]
	Learning Rate: 0.00347352
	LOSS [training: 0.36725023667537116 | validation: 0.44407323916750685]
	TIME [epoch: 15.1 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40162494694207523		[learning rate: 0.0034653]
	Learning Rate: 0.00346532
	LOSS [training: 0.40162494694207523 | validation: 0.3433450413757789]
	TIME [epoch: 15 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39596246848648015		[learning rate: 0.0034571]
	Learning Rate: 0.00345715
	LOSS [training: 0.39596246848648015 | validation: 0.4935350828514082]
	TIME [epoch: 15 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40977406858841214		[learning rate: 0.003449]
	Learning Rate: 0.00344899
	LOSS [training: 0.40977406858841214 | validation: 0.4198249169322096]
	TIME [epoch: 139 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40033037297672924		[learning rate: 0.0034409]
	Learning Rate: 0.00344086
	LOSS [training: 0.40033037297672924 | validation: 0.38834537925166557]
	TIME [epoch: 32.5 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43570424595773705		[learning rate: 0.0034327]
	Learning Rate: 0.00343274
	LOSS [training: 0.43570424595773705 | validation: 0.4749045978417567]
	TIME [epoch: 32.7 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3713277375516222		[learning rate: 0.0034246]
	Learning Rate: 0.00342464
	LOSS [training: 0.3713277375516222 | validation: 0.34888599421274796]
	TIME [epoch: 32.7 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3834188292995701		[learning rate: 0.0034166]
	Learning Rate: 0.00341657
	LOSS [training: 0.3834188292995701 | validation: 0.5093500763550528]
	TIME [epoch: 32.8 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4204007846212718		[learning rate: 0.0034085]
	Learning Rate: 0.00340851
	LOSS [training: 0.4204007846212718 | validation: 0.4025662606488274]
	TIME [epoch: 32.8 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3398380194889533		[learning rate: 0.0034005]
	Learning Rate: 0.00340047
	LOSS [training: 0.3398380194889533 | validation: 0.35831005054367304]
	TIME [epoch: 32.8 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40159759762483305		[learning rate: 0.0033924]
	Learning Rate: 0.00339244
	LOSS [training: 0.40159759762483305 | validation: 0.4085166006194625]
	TIME [epoch: 32.8 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3295436051364744		[learning rate: 0.0033844]
	Learning Rate: 0.00338444
	LOSS [training: 0.3295436051364744 | validation: 0.3235931791424599]
	TIME [epoch: 32.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41793888704530946		[learning rate: 0.0033765]
	Learning Rate: 0.00337646
	LOSS [training: 0.41793888704530946 | validation: 0.3283254241680328]
	TIME [epoch: 32.8 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3503160538386049		[learning rate: 0.0033685]
	Learning Rate: 0.0033685
	LOSS [training: 0.3503160538386049 | validation: 0.48501479741494413]
	TIME [epoch: 32.7 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4027985597446394		[learning rate: 0.0033605]
	Learning Rate: 0.00336055
	LOSS [training: 0.4027985597446394 | validation: 0.428526832512722]
	TIME [epoch: 32.7 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43155688169700934		[learning rate: 0.0033526]
	Learning Rate: 0.00335262
	LOSS [training: 0.43155688169700934 | validation: 0.3980619498007638]
	TIME [epoch: 32.8 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3258684285529956		[learning rate: 0.0033447]
	Learning Rate: 0.00334471
	LOSS [training: 0.3258684285529956 | validation: 0.3946909666524875]
	TIME [epoch: 32.8 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4461996217179329		[learning rate: 0.0033368]
	Learning Rate: 0.00333682
	LOSS [training: 0.4461996217179329 | validation: 0.34911784558264136]
	TIME [epoch: 32.7 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40739173660870814		[learning rate: 0.003329]
	Learning Rate: 0.00332895
	LOSS [training: 0.40739173660870814 | validation: 0.44727909816306133]
	TIME [epoch: 32.7 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36326391224386523		[learning rate: 0.0033211]
	Learning Rate: 0.0033211
	LOSS [training: 0.36326391224386523 | validation: 0.445891526152465]
	TIME [epoch: 32.7 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.405012870219109		[learning rate: 0.0033133]
	Learning Rate: 0.00331327
	LOSS [training: 0.405012870219109 | validation: 0.32239054727915084]
	TIME [epoch: 32.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44601833223028137		[learning rate: 0.0033055]
	Learning Rate: 0.00330545
	LOSS [training: 0.44601833223028137 | validation: 0.32606747154353]
	TIME [epoch: 32.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37415364006989915		[learning rate: 0.0032977]
	Learning Rate: 0.00329765
	LOSS [training: 0.37415364006989915 | validation: 0.4792360465022785]
	TIME [epoch: 32.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4082681795625872		[learning rate: 0.0032899]
	Learning Rate: 0.00328988
	LOSS [training: 0.4082681795625872 | validation: 0.3644927165345403]
	TIME [epoch: 32.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4708619918145852		[learning rate: 0.0032821]
	Learning Rate: 0.00328212
	LOSS [training: 0.4708619918145852 | validation: 0.5334427659224342]
	TIME [epoch: 32.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4211355383973021		[learning rate: 0.0032744]
	Learning Rate: 0.00327437
	LOSS [training: 0.4211355383973021 | validation: 0.40213403841380646]
	TIME [epoch: 32.8 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34305630851891883		[learning rate: 0.0032666]
	Learning Rate: 0.00326665
	LOSS [training: 0.34305630851891883 | validation: 0.626816400161583]
	TIME [epoch: 32.7 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4574836582719725		[learning rate: 0.0032589]
	Learning Rate: 0.00325894
	LOSS [training: 0.4574836582719725 | validation: 0.3548422192953988]
	TIME [epoch: 32.7 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3037279325846335		[learning rate: 0.0032513]
	Learning Rate: 0.00325126
	LOSS [training: 0.3037279325846335 | validation: 0.36737803262182245]
	TIME [epoch: 32.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48759503238165613		[learning rate: 0.0032436]
	Learning Rate: 0.00324359
	LOSS [training: 0.48759503238165613 | validation: 0.47607398998228595]
	TIME [epoch: 32.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3943211943863935		[learning rate: 0.0032359]
	Learning Rate: 0.00323594
	LOSS [training: 0.3943211943863935 | validation: 0.36798928490668503]
	TIME [epoch: 32.7 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34037954024609485		[learning rate: 0.0032283]
	Learning Rate: 0.0032283
	LOSS [training: 0.34037954024609485 | validation: 0.35559551592769684]
	TIME [epoch: 32.7 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3442754172663511		[learning rate: 0.0032207]
	Learning Rate: 0.00322069
	LOSS [training: 0.3442754172663511 | validation: 0.4414907082386037]
	TIME [epoch: 32.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4266883663581307		[learning rate: 0.0032131]
	Learning Rate: 0.00321309
	LOSS [training: 0.4266883663581307 | validation: 0.3952969698844573]
	TIME [epoch: 32.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3715279329408424		[learning rate: 0.0032055]
	Learning Rate: 0.00320551
	LOSS [training: 0.3715279329408424 | validation: 0.4035467863754524]
	TIME [epoch: 32.7 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3505698071643179		[learning rate: 0.003198]
	Learning Rate: 0.00319795
	LOSS [training: 0.3505698071643179 | validation: 0.371540932844543]
	TIME [epoch: 32.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3876818919048856		[learning rate: 0.0031904]
	Learning Rate: 0.00319041
	LOSS [training: 0.3876818919048856 | validation: 0.3637378218585546]
	TIME [epoch: 32.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3165743745696413		[learning rate: 0.0031829]
	Learning Rate: 0.00318288
	LOSS [training: 0.3165743745696413 | validation: 0.3680081294288707]
	TIME [epoch: 32.8 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47021082920665735		[learning rate: 0.0031754]
	Learning Rate: 0.00317537
	LOSS [training: 0.47021082920665735 | validation: 0.435633143044563]
	TIME [epoch: 32.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3512957468840616		[learning rate: 0.0031679]
	Learning Rate: 0.00316788
	LOSS [training: 0.3512957468840616 | validation: 0.31776218157727676]
	TIME [epoch: 32.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33456354480469336		[learning rate: 0.0031604]
	Learning Rate: 0.00316041
	LOSS [training: 0.33456354480469336 | validation: 0.46833033648956146]
	TIME [epoch: 32.8 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3670594703673124		[learning rate: 0.003153]
	Learning Rate: 0.00315296
	LOSS [training: 0.3670594703673124 | validation: 0.49896294040339406]
	TIME [epoch: 32.8 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4395288147860408		[learning rate: 0.0031455]
	Learning Rate: 0.00314552
	LOSS [training: 0.4395288147860408 | validation: 0.36897396691941864]
	TIME [epoch: 32.8 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3849125716648386		[learning rate: 0.0031381]
	Learning Rate: 0.0031381
	LOSS [training: 0.3849125716648386 | validation: 0.48441700408736943]
	TIME [epoch: 32.7 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4127416087229614		[learning rate: 0.0031307]
	Learning Rate: 0.0031307
	LOSS [training: 0.4127416087229614 | validation: 0.36453686128158447]
	TIME [epoch: 32.8 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33142792707715263		[learning rate: 0.0031233]
	Learning Rate: 0.00312331
	LOSS [training: 0.33142792707715263 | validation: 0.4796724495883084]
	TIME [epoch: 32.7 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41462875979494146		[learning rate: 0.0031159]
	Learning Rate: 0.00311594
	LOSS [training: 0.41462875979494146 | validation: 0.3826666429531815]
	TIME [epoch: 32.7 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3495041530177243		[learning rate: 0.0031086]
	Learning Rate: 0.0031086
	LOSS [training: 0.3495041530177243 | validation: 0.39213292111692755]
	TIME [epoch: 32.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3700204380903866		[learning rate: 0.0031013]
	Learning Rate: 0.00310126
	LOSS [training: 0.3700204380903866 | validation: 0.40083309598876316]
	TIME [epoch: 32.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36910988665529115		[learning rate: 0.0030939]
	Learning Rate: 0.00309395
	LOSS [training: 0.36910988665529115 | validation: 0.35794376039520714]
	TIME [epoch: 32.7 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.380475208934613		[learning rate: 0.0030866]
	Learning Rate: 0.00308665
	LOSS [training: 0.380475208934613 | validation: 0.3750615193557806]
	TIME [epoch: 32.7 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31100526931773737		[learning rate: 0.0030794]
	Learning Rate: 0.00307937
	LOSS [training: 0.31100526931773737 | validation: 0.29069839108288537]
	TIME [epoch: 32.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_549.pth
	Model improved!!!
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4394214593330741		[learning rate: 0.0030721]
	Learning Rate: 0.0030721
	LOSS [training: 0.4394214593330741 | validation: 0.5483203417529444]
	TIME [epoch: 32.7 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4874069822410669		[learning rate: 0.0030649]
	Learning Rate: 0.00306486
	LOSS [training: 0.4874069822410669 | validation: 0.38473976019969236]
	TIME [epoch: 32.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32237894533844336		[learning rate: 0.0030576]
	Learning Rate: 0.00305763
	LOSS [training: 0.32237894533844336 | validation: 0.31766165132918356]
	TIME [epoch: 32.8 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3641315085576532		[learning rate: 0.0030504]
	Learning Rate: 0.00305042
	LOSS [training: 0.3641315085576532 | validation: 0.4141240821252453]
	TIME [epoch: 32.7 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3230989395238667		[learning rate: 0.0030432]
	Learning Rate: 0.00304322
	LOSS [training: 0.3230989395238667 | validation: 0.45857351940580093]
	TIME [epoch: 32.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4400880086831068		[learning rate: 0.003036]
	Learning Rate: 0.00303604
	LOSS [training: 0.4400880086831068 | validation: 0.34674684084444735]
	TIME [epoch: 32.7 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34199715617793236		[learning rate: 0.0030289]
	Learning Rate: 0.00302888
	LOSS [training: 0.34199715617793236 | validation: 0.32976572543532706]
	TIME [epoch: 32.8 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34704131255525583		[learning rate: 0.0030217]
	Learning Rate: 0.00302174
	LOSS [training: 0.34704131255525583 | validation: 0.32123483266442965]
	TIME [epoch: 32.7 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40168121218833636		[learning rate: 0.0030146]
	Learning Rate: 0.00301461
	LOSS [training: 0.40168121218833636 | validation: 0.3980985306633941]
	TIME [epoch: 32.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37785876201935964		[learning rate: 0.0030075]
	Learning Rate: 0.0030075
	LOSS [training: 0.37785876201935964 | validation: 0.4468213388624095]
	TIME [epoch: 32.7 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4162861093084559		[learning rate: 0.0030004]
	Learning Rate: 0.0030004
	LOSS [training: 0.4162861093084559 | validation: 0.3752927799032354]
	TIME [epoch: 32.7 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31598362339624425		[learning rate: 0.0029933]
	Learning Rate: 0.00299332
	LOSS [training: 0.31598362339624425 | validation: 0.37998108507048456]
	TIME [epoch: 32.7 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39577417701964696		[learning rate: 0.0029863]
	Learning Rate: 0.00298626
	LOSS [training: 0.39577417701964696 | validation: 0.42484693551836095]
	TIME [epoch: 32.8 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3722023037151698		[learning rate: 0.0029792]
	Learning Rate: 0.00297922
	LOSS [training: 0.3722023037151698 | validation: 0.3600298764421641]
	TIME [epoch: 32.7 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31998001002123905		[learning rate: 0.0029722]
	Learning Rate: 0.00297219
	LOSS [training: 0.31998001002123905 | validation: 0.3794300154389385]
	TIME [epoch: 32.8 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3718120195143404		[learning rate: 0.0029652]
	Learning Rate: 0.00296518
	LOSS [training: 0.3718120195143404 | validation: 0.3626934817411691]
	TIME [epoch: 32.8 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3419122522592556		[learning rate: 0.0029582]
	Learning Rate: 0.00295819
	LOSS [training: 0.3419122522592556 | validation: 0.34918351473554066]
	TIME [epoch: 32.7 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38558632628308076		[learning rate: 0.0029512]
	Learning Rate: 0.00295121
	LOSS [training: 0.38558632628308076 | validation: 0.3605179946912984]
	TIME [epoch: 32.5 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36276642000498327		[learning rate: 0.0029442]
	Learning Rate: 0.00294425
	LOSS [training: 0.36276642000498327 | validation: 0.36074076631853025]
	TIME [epoch: 32.5 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3120240325769288		[learning rate: 0.0029373]
	Learning Rate: 0.0029373
	LOSS [training: 0.3120240325769288 | validation: 0.35737654969601185]
	TIME [epoch: 32.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3331101419270912		[learning rate: 0.0029304]
	Learning Rate: 0.00293037
	LOSS [training: 0.3331101419270912 | validation: 0.3587559284115091]
	TIME [epoch: 32.5 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39181770703986585		[learning rate: 0.0029235]
	Learning Rate: 0.00292346
	LOSS [training: 0.39181770703986585 | validation: 0.3816467454935264]
	TIME [epoch: 32.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31797837907080095		[learning rate: 0.0029166]
	Learning Rate: 0.00291657
	LOSS [training: 0.31797837907080095 | validation: 0.3008081746459363]
	TIME [epoch: 32.5 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3887900889795872		[learning rate: 0.0029097]
	Learning Rate: 0.00290969
	LOSS [training: 0.3887900889795872 | validation: 0.38544245527602183]
	TIME [epoch: 32.5 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31045684957250635		[learning rate: 0.0029028]
	Learning Rate: 0.00290282
	LOSS [training: 0.31045684957250635 | validation: 0.30159419444656943]
	TIME [epoch: 32.6 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4156955852448716		[learning rate: 0.002896]
	Learning Rate: 0.00289598
	LOSS [training: 0.4156955852448716 | validation: 0.46259180437075975]
	TIME [epoch: 32.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38192014716277023		[learning rate: 0.0028891]
	Learning Rate: 0.00288914
	LOSS [training: 0.38192014716277023 | validation: 0.39409515114427673]
	TIME [epoch: 32.7 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3574509614786246		[learning rate: 0.0028823]
	Learning Rate: 0.00288233
	LOSS [training: 0.3574509614786246 | validation: 0.4819172470814562]
	TIME [epoch: 32.7 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44404863990476484		[learning rate: 0.0028755]
	Learning Rate: 0.00287553
	LOSS [training: 0.44404863990476484 | validation: 0.41022335583436265]
	TIME [epoch: 32.7 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35787120810747175		[learning rate: 0.0028687]
	Learning Rate: 0.00286875
	LOSS [training: 0.35787120810747175 | validation: 0.3510151217927991]
	TIME [epoch: 32.6 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3119251508468633		[learning rate: 0.002862]
	Learning Rate: 0.00286198
	LOSS [training: 0.3119251508468633 | validation: 0.327189290996319]
	TIME [epoch: 32.7 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3553462065944969		[learning rate: 0.0028552]
	Learning Rate: 0.00285523
	LOSS [training: 0.3553462065944969 | validation: 0.3641743346685422]
	TIME [epoch: 32.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34543723681017613		[learning rate: 0.0028485]
	Learning Rate: 0.00284849
	LOSS [training: 0.34543723681017613 | validation: 0.3646645244521519]
	TIME [epoch: 32.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37886423038315054		[learning rate: 0.0028418]
	Learning Rate: 0.00284178
	LOSS [training: 0.37886423038315054 | validation: 0.49682753904429927]
	TIME [epoch: 32.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3910793679321205		[learning rate: 0.0028351]
	Learning Rate: 0.00283507
	LOSS [training: 0.3910793679321205 | validation: 0.35771753868466116]
	TIME [epoch: 32.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29632558675267684		[learning rate: 0.0028284]
	Learning Rate: 0.00282838
	LOSS [training: 0.29632558675267684 | validation: 0.34674980495071317]
	TIME [epoch: 32.5 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40922899537226354		[learning rate: 0.0028217]
	Learning Rate: 0.00282171
	LOSS [training: 0.40922899537226354 | validation: 0.4329328604609065]
	TIME [epoch: 32.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34653994601401883		[learning rate: 0.0028151]
	Learning Rate: 0.00281506
	LOSS [training: 0.34653994601401883 | validation: 0.34226361661342286]
	TIME [epoch: 32.5 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33004656827094825		[learning rate: 0.0028084]
	Learning Rate: 0.00280842
	LOSS [training: 0.33004656827094825 | validation: 0.3044243433778747]
	TIME [epoch: 32.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36588083364225443		[learning rate: 0.0028018]
	Learning Rate: 0.00280179
	LOSS [training: 0.36588083364225443 | validation: 0.3771484305818961]
	TIME [epoch: 32.6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33688979168919053		[learning rate: 0.0027952]
	Learning Rate: 0.00279518
	LOSS [training: 0.33688979168919053 | validation: 0.3030841663315301]
	TIME [epoch: 32.6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3249514453003261		[learning rate: 0.0027886]
	Learning Rate: 0.00278859
	LOSS [training: 0.3249514453003261 | validation: 0.33439828272442795]
	TIME [epoch: 32.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30439508712064156		[learning rate: 0.002782]
	Learning Rate: 0.00278201
	LOSS [training: 0.30439508712064156 | validation: 0.32184414448191623]
	TIME [epoch: 32.6 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33445212733797947		[learning rate: 0.0027754]
	Learning Rate: 0.00277545
	LOSS [training: 0.33445212733797947 | validation: 0.3281959867387216]
	TIME [epoch: 32.6 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3420094104243738		[learning rate: 0.0027689]
	Learning Rate: 0.0027689
	LOSS [training: 0.3420094104243738 | validation: 0.4641175592152419]
	TIME [epoch: 32.7 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39351728212039894		[learning rate: 0.0027624]
	Learning Rate: 0.00276237
	LOSS [training: 0.39351728212039894 | validation: 0.36789134691723085]
	TIME [epoch: 32.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32167350735491324		[learning rate: 0.0027559]
	Learning Rate: 0.00275586
	LOSS [training: 0.32167350735491324 | validation: 0.2970388807812914]
	TIME [epoch: 32.6 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36608682494597616		[learning rate: 0.0027494]
	Learning Rate: 0.00274936
	LOSS [training: 0.36608682494597616 | validation: 0.45975723519493294]
	TIME [epoch: 32.6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47154168445835454		[learning rate: 0.0027429]
	Learning Rate: 0.00274287
	LOSS [training: 0.47154168445835454 | validation: 0.379012462972923]
	TIME [epoch: 32.8 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3276475698089376		[learning rate: 0.0027364]
	Learning Rate: 0.0027364
	LOSS [training: 0.3276475698089376 | validation: 0.324919327471053]
	TIME [epoch: 32.6 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3588382736094063		[learning rate: 0.0027299]
	Learning Rate: 0.00272994
	LOSS [training: 0.3588382736094063 | validation: 0.338880826515811]
	TIME [epoch: 32.9 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3136605717304812		[learning rate: 0.0027235]
	Learning Rate: 0.00272351
	LOSS [training: 0.3136605717304812 | validation: 0.30259148727598295]
	TIME [epoch: 32.9 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3362805434246488		[learning rate: 0.0027171]
	Learning Rate: 0.00271708
	LOSS [training: 0.3362805434246488 | validation: 0.45415896197267236]
	TIME [epoch: 32.9 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44108417164844954		[learning rate: 0.0027107]
	Learning Rate: 0.00271067
	LOSS [training: 0.44108417164844954 | validation: 0.3584300200812303]
	TIME [epoch: 32.9 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2967084306390235		[learning rate: 0.0027043]
	Learning Rate: 0.00270428
	LOSS [training: 0.2967084306390235 | validation: 0.2938033492408729]
	TIME [epoch: 32.9 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3452966500253134		[learning rate: 0.0026979]
	Learning Rate: 0.0026979
	LOSS [training: 0.3452966500253134 | validation: 0.372712732449504]
	TIME [epoch: 32.8 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36618671046377665		[learning rate: 0.0026915]
	Learning Rate: 0.00269153
	LOSS [training: 0.36618671046377665 | validation: 0.3331165875713746]
	TIME [epoch: 32.8 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3519171216769373		[learning rate: 0.0026852]
	Learning Rate: 0.00268519
	LOSS [training: 0.3519171216769373 | validation: 0.3205746032562943]
	TIME [epoch: 32.7 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31780069000327105		[learning rate: 0.0026789]
	Learning Rate: 0.00267885
	LOSS [training: 0.31780069000327105 | validation: 0.3540009928206436]
	TIME [epoch: 32.7 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3428427313198314		[learning rate: 0.0026725]
	Learning Rate: 0.00267253
	LOSS [training: 0.3428427313198314 | validation: 0.3228557928532929]
	TIME [epoch: 32.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.301391260526387		[learning rate: 0.0026662]
	Learning Rate: 0.00266623
	LOSS [training: 0.301391260526387 | validation: 0.27112617066774236]
	TIME [epoch: 32.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3624612977261472		[learning rate: 0.0026599]
	Learning Rate: 0.00265994
	LOSS [training: 0.3624612977261472 | validation: 0.4117904534390884]
	TIME [epoch: 32.7 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3570169205456243		[learning rate: 0.0026537]
	Learning Rate: 0.00265367
	LOSS [training: 0.3570169205456243 | validation: 0.3717632376511658]
	TIME [epoch: 32.7 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31310930231953377		[learning rate: 0.0026474]
	Learning Rate: 0.00264741
	LOSS [training: 0.31310930231953377 | validation: 0.3171006388852917]
	TIME [epoch: 32.8 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3024168213276883		[learning rate: 0.0026412]
	Learning Rate: 0.00264116
	LOSS [training: 0.3024168213276883 | validation: 0.37100965173910677]
	TIME [epoch: 32.8 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35273511616485664		[learning rate: 0.0026349]
	Learning Rate: 0.00263493
	LOSS [training: 0.35273511616485664 | validation: 0.3340537581326943]
	TIME [epoch: 32.8 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33135837327850304		[learning rate: 0.0026287]
	Learning Rate: 0.00262872
	LOSS [training: 0.33135837327850304 | validation: 0.41327128225210963]
	TIME [epoch: 32.8 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3637797009941167		[learning rate: 0.0026225]
	Learning Rate: 0.00262251
	LOSS [training: 0.3637797009941167 | validation: 0.321898572533319]
	TIME [epoch: 32.8 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2789783648705263		[learning rate: 0.0026163]
	Learning Rate: 0.00261633
	LOSS [training: 0.2789783648705263 | validation: 0.3489414269884482]
	TIME [epoch: 32.8 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4259385214102379		[learning rate: 0.0026102]
	Learning Rate: 0.00261016
	LOSS [training: 0.4259385214102379 | validation: 0.3462659481751345]
	TIME [epoch: 32.8 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3319448386490705		[learning rate: 0.002604]
	Learning Rate: 0.002604
	LOSS [training: 0.3319448386490705 | validation: 0.3265777150022429]
	TIME [epoch: 32.7 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2716651394707856		[learning rate: 0.0025979]
	Learning Rate: 0.00259786
	LOSS [training: 0.2716651394707856 | validation: 0.34432506967033116]
	TIME [epoch: 32.8 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32854948362938613		[learning rate: 0.0025917]
	Learning Rate: 0.00259173
	LOSS [training: 0.32854948362938613 | validation: 0.29618088734178083]
	TIME [epoch: 32.7 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33724757703163233		[learning rate: 0.0025856]
	Learning Rate: 0.00258562
	LOSS [training: 0.33724757703163233 | validation: 0.3941682448628485]
	TIME [epoch: 32.7 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3724827360886105		[learning rate: 0.0025795]
	Learning Rate: 0.00257952
	LOSS [training: 0.3724827360886105 | validation: 0.35276439818229843]
	TIME [epoch: 32.7 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3145133916463312		[learning rate: 0.0025734]
	Learning Rate: 0.00257343
	LOSS [training: 0.3145133916463312 | validation: 0.2713967501779967]
	TIME [epoch: 32.7 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32506598188019264		[learning rate: 0.0025674]
	Learning Rate: 0.00256736
	LOSS [training: 0.32506598188019264 | validation: 0.46062272856624314]
	TIME [epoch: 32.8 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36878404254829245		[learning rate: 0.0025613]
	Learning Rate: 0.00256131
	LOSS [training: 0.36878404254829245 | validation: 0.3813755311605924]
	TIME [epoch: 32.7 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4009161060009066		[learning rate: 0.0025553]
	Learning Rate: 0.00255526
	LOSS [training: 0.4009161060009066 | validation: 0.3958933421505916]
	TIME [epoch: 32.7 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3177479519851724		[learning rate: 0.0025492]
	Learning Rate: 0.00254924
	LOSS [training: 0.3177479519851724 | validation: 0.3605802457572369]
	TIME [epoch: 32.8 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3340347931215909		[learning rate: 0.0025432]
	Learning Rate: 0.00254322
	LOSS [training: 0.3340347931215909 | validation: 0.3701473772893228]
	TIME [epoch: 32.8 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31899795907610184		[learning rate: 0.0025372]
	Learning Rate: 0.00253722
	LOSS [training: 0.31899795907610184 | validation: 0.2937974683182141]
	TIME [epoch: 32.8 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36164532188318527		[learning rate: 0.0025312]
	Learning Rate: 0.00253124
	LOSS [training: 0.36164532188318527 | validation: 0.5250481084961334]
	TIME [epoch: 32.9 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4737963642708952		[learning rate: 0.0025253]
	Learning Rate: 0.00252527
	LOSS [training: 0.4737963642708952 | validation: 0.3809386218046209]
	TIME [epoch: 32.8 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3090577099530276		[learning rate: 0.0025193]
	Learning Rate: 0.00251931
	LOSS [training: 0.3090577099530276 | validation: 0.3157037551990204]
	TIME [epoch: 32.8 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3205826708389067		[learning rate: 0.0025134]
	Learning Rate: 0.00251337
	LOSS [training: 0.3205826708389067 | validation: 0.3829559136578442]
	TIME [epoch: 32.7 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3792779786870324		[learning rate: 0.0025074]
	Learning Rate: 0.00250744
	LOSS [training: 0.3792779786870324 | validation: 0.3337415508145458]
	TIME [epoch: 32.8 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3031732035465337		[learning rate: 0.0025015]
	Learning Rate: 0.00250153
	LOSS [training: 0.3031732035465337 | validation: 0.27562660569422104]
	TIME [epoch: 32.9 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3175057853953529		[learning rate: 0.0024956]
	Learning Rate: 0.00249563
	LOSS [training: 0.3175057853953529 | validation: 0.3907761102020212]
	TIME [epoch: 32.8 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38241280492533214		[learning rate: 0.0024897]
	Learning Rate: 0.00248974
	LOSS [training: 0.38241280492533214 | validation: 0.4317200667094683]
	TIME [epoch: 32.9 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36341801391741907		[learning rate: 0.0024839]
	Learning Rate: 0.00248387
	LOSS [training: 0.36341801391741907 | validation: 0.33927041808919856]
	TIME [epoch: 32.8 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3143250671805546		[learning rate: 0.002478]
	Learning Rate: 0.00247801
	LOSS [training: 0.3143250671805546 | validation: 0.3306815522997102]
	TIME [epoch: 33 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.364535457276408		[learning rate: 0.0024722]
	Learning Rate: 0.00247216
	LOSS [training: 0.364535457276408 | validation: 0.38896936651482544]
	TIME [epoch: 32.9 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3061666224558863		[learning rate: 0.0024663]
	Learning Rate: 0.00246633
	LOSS [training: 0.3061666224558863 | validation: 0.2918879258022822]
	TIME [epoch: 32.8 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35580354847237217		[learning rate: 0.0024605]
	Learning Rate: 0.00246051
	LOSS [training: 0.35580354847237217 | validation: 0.3867755567312742]
	TIME [epoch: 32.8 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3269716936163714		[learning rate: 0.0024547]
	Learning Rate: 0.00245471
	LOSS [training: 0.3269716936163714 | validation: 0.28438731413279117]
	TIME [epoch: 32.7 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3867238499916346		[learning rate: 0.0024489]
	Learning Rate: 0.00244892
	LOSS [training: 0.3867238499916346 | validation: 0.4339171709111789]
	TIME [epoch: 33 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3463729403784314		[learning rate: 0.0024431]
	Learning Rate: 0.00244314
	LOSS [training: 0.3463729403784314 | validation: 0.33018853668782566]
	TIME [epoch: 32.9 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2938209454574571		[learning rate: 0.0024374]
	Learning Rate: 0.00243738
	LOSS [training: 0.2938209454574571 | validation: 0.28534375400802103]
	TIME [epoch: 32.9 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3320458681537247		[learning rate: 0.0024316]
	Learning Rate: 0.00243163
	LOSS [training: 0.3320458681537247 | validation: 0.336120348133197]
	TIME [epoch: 32.9 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29701076437963525		[learning rate: 0.0024259]
	Learning Rate: 0.00242589
	LOSS [training: 0.29701076437963525 | validation: 0.2811915231910682]
	TIME [epoch: 32.8 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25498988010471224		[learning rate: 0.0024202]
	Learning Rate: 0.00242017
	LOSS [training: 0.25498988010471224 | validation: 0.6757504256330673]
	TIME [epoch: 32.9 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46780866897793016		[learning rate: 0.0024145]
	Learning Rate: 0.00241446
	LOSS [training: 0.46780866897793016 | validation: 0.3609648169169465]
	TIME [epoch: 32.8 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29758900751101		[learning rate: 0.0024088]
	Learning Rate: 0.00240877
	LOSS [training: 0.29758900751101 | validation: 0.2967959850895606]
	TIME [epoch: 32.9 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2737594929063074		[learning rate: 0.0024031]
	Learning Rate: 0.00240309
	LOSS [training: 0.2737594929063074 | validation: 0.3254233844172366]
	TIME [epoch: 32.8 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30901618027282707		[learning rate: 0.0023974]
	Learning Rate: 0.00239742
	LOSS [training: 0.30901618027282707 | validation: 0.30360329132614716]
	TIME [epoch: 32.8 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30154584651836464		[learning rate: 0.0023918]
	Learning Rate: 0.00239176
	LOSS [training: 0.30154584651836464 | validation: 0.2934074654536178]
	TIME [epoch: 32.8 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3436141326103895		[learning rate: 0.0023861]
	Learning Rate: 0.00238612
	LOSS [training: 0.3436141326103895 | validation: 0.3074023311319132]
	TIME [epoch: 32.7 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3268318918508327		[learning rate: 0.0023805]
	Learning Rate: 0.00238049
	LOSS [training: 0.3268318918508327 | validation: 0.45287828594129803]
	TIME [epoch: 32.8 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38806824216741337		[learning rate: 0.0023749]
	Learning Rate: 0.00237488
	LOSS [training: 0.38806824216741337 | validation: 0.36561931290584]
	TIME [epoch: 32.8 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.299744754931477		[learning rate: 0.0023693]
	Learning Rate: 0.00236927
	LOSS [training: 0.299744754931477 | validation: 0.2915212359770501]
	TIME [epoch: 32.7 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2752713080638942		[learning rate: 0.0023637]
	Learning Rate: 0.00236369
	LOSS [training: 0.2752713080638942 | validation: 0.2717152222796922]
	TIME [epoch: 32.8 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38355326005193435		[learning rate: 0.0023581]
	Learning Rate: 0.00235811
	LOSS [training: 0.38355326005193435 | validation: 0.4232556034147509]
	TIME [epoch: 32.7 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3485933256085204		[learning rate: 0.0023525]
	Learning Rate: 0.00235255
	LOSS [training: 0.3485933256085204 | validation: 0.3040663563044831]
	TIME [epoch: 32.8 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33366141633569135		[learning rate: 0.002347]
	Learning Rate: 0.002347
	LOSS [training: 0.33366141633569135 | validation: 0.3487739282388612]
	TIME [epoch: 32.8 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33794575493382284		[learning rate: 0.0023415]
	Learning Rate: 0.00234146
	LOSS [training: 0.33794575493382284 | validation: 0.31035511669870963]
	TIME [epoch: 32.8 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2797616463575138		[learning rate: 0.0023359]
	Learning Rate: 0.00233594
	LOSS [training: 0.2797616463575138 | validation: 0.282869537053726]
	TIME [epoch: 32.7 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3016885619792901		[learning rate: 0.0023304]
	Learning Rate: 0.00233043
	LOSS [training: 0.3016885619792901 | validation: 0.3129555409459597]
	TIME [epoch: 32.9 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31719810922574215		[learning rate: 0.0023249]
	Learning Rate: 0.00232493
	LOSS [training: 0.31719810922574215 | validation: 0.35094794556900966]
	TIME [epoch: 32.9 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38681388057228483		[learning rate: 0.0023194]
	Learning Rate: 0.00231945
	LOSS [training: 0.38681388057228483 | validation: 0.33286547015468315]
	TIME [epoch: 32.8 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28152449332563634		[learning rate: 0.002314]
	Learning Rate: 0.00231398
	LOSS [training: 0.28152449332563634 | validation: 0.28137312502343803]
	TIME [epoch: 32.7 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31245005983548424		[learning rate: 0.0023085]
	Learning Rate: 0.00230852
	LOSS [training: 0.31245005983548424 | validation: 0.308394726566797]
	TIME [epoch: 32.9 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2742113200242992		[learning rate: 0.0023031]
	Learning Rate: 0.00230307
	LOSS [training: 0.2742113200242992 | validation: 0.3030336970860234]
	TIME [epoch: 32.8 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3400408626851344		[learning rate: 0.0022976]
	Learning Rate: 0.00229764
	LOSS [training: 0.3400408626851344 | validation: 0.3080559114668695]
	TIME [epoch: 32.7 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3056063881430513		[learning rate: 0.0022922]
	Learning Rate: 0.00229222
	LOSS [training: 0.3056063881430513 | validation: 0.4054935941384613]
	TIME [epoch: 32.8 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36360599901818924		[learning rate: 0.0022868]
	Learning Rate: 0.00228681
	LOSS [training: 0.36360599901818924 | validation: 0.30222264037798213]
	TIME [epoch: 32.8 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27449540877748535		[learning rate: 0.0022814]
	Learning Rate: 0.00228142
	LOSS [training: 0.27449540877748535 | validation: 0.3210503887596189]
	TIME [epoch: 32.8 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4055748726438994		[learning rate: 0.002276]
	Learning Rate: 0.00227604
	LOSS [training: 0.4055748726438994 | validation: 0.38878999622519045]
	TIME [epoch: 32.8 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3026199511264653		[learning rate: 0.0022707]
	Learning Rate: 0.00227067
	LOSS [training: 0.3026199511264653 | validation: 0.2764124277530398]
	TIME [epoch: 32.8 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31639843174444704		[learning rate: 0.0022653]
	Learning Rate: 0.00226531
	LOSS [training: 0.31639843174444704 | validation: 0.35325321279625]
	TIME [epoch: 32.8 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3156376751610191		[learning rate: 0.00226]
	Learning Rate: 0.00225997
	LOSS [training: 0.3156376751610191 | validation: 0.2864890755621696]
	TIME [epoch: 32.7 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2511399355905399		[learning rate: 0.0022546]
	Learning Rate: 0.00225464
	LOSS [training: 0.2511399355905399 | validation: 0.24761773308397653]
	TIME [epoch: 32.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_681.pth
	Model improved!!!
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34450303456059034		[learning rate: 0.0022493]
	Learning Rate: 0.00224932
	LOSS [training: 0.34450303456059034 | validation: 0.32645387176812446]
	TIME [epoch: 32.9 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28515316396556634		[learning rate: 0.002244]
	Learning Rate: 0.00224401
	LOSS [training: 0.28515316396556634 | validation: 0.3928974480074815]
	TIME [epoch: 32.8 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3854711583916345		[learning rate: 0.0022387]
	Learning Rate: 0.00223872
	LOSS [training: 0.3854711583916345 | validation: 0.3223918571455339]
	TIME [epoch: 32.8 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26885807215026747		[learning rate: 0.0022334]
	Learning Rate: 0.00223344
	LOSS [training: 0.26885807215026747 | validation: 0.25707268140349543]
	TIME [epoch: 32.7 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3611622298593606		[learning rate: 0.0022282]
	Learning Rate: 0.00222817
	LOSS [training: 0.3611622298593606 | validation: 0.4905344324998717]
	TIME [epoch: 32.9 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4601838398898156		[learning rate: 0.0022229]
	Learning Rate: 0.00222292
	LOSS [training: 0.4601838398898156 | validation: 0.452691810286833]
	TIME [epoch: 32.8 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3492362978953802		[learning rate: 0.0022177]
	Learning Rate: 0.00221767
	LOSS [training: 0.3492362978953802 | validation: 0.32936985518588235]
	TIME [epoch: 32.8 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.271870045102848		[learning rate: 0.0022124]
	Learning Rate: 0.00221244
	LOSS [training: 0.271870045102848 | validation: 0.2774406707439657]
	TIME [epoch: 32.8 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2788917679747935		[learning rate: 0.0022072]
	Learning Rate: 0.00220722
	LOSS [training: 0.2788917679747935 | validation: 0.3204295231416945]
	TIME [epoch: 32.9 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3127487612402515		[learning rate: 0.002202]
	Learning Rate: 0.00220202
	LOSS [training: 0.3127487612402515 | validation: 0.29762885373691483]
	TIME [epoch: 32.9 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3002425119171585		[learning rate: 0.0021968]
	Learning Rate: 0.00219682
	LOSS [training: 0.3002425119171585 | validation: 0.388628136801048]
	TIME [epoch: 32.8 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33729221646749935		[learning rate: 0.0021916]
	Learning Rate: 0.00219164
	LOSS [training: 0.33729221646749935 | validation: 0.27329218964770463]
	TIME [epoch: 32.9 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30425436526835337		[learning rate: 0.0021865]
	Learning Rate: 0.00218647
	LOSS [training: 0.30425436526835337 | validation: 0.35768140891108424]
	TIME [epoch: 32.8 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28671431519665047		[learning rate: 0.0021813]
	Learning Rate: 0.00218131
	LOSS [training: 0.28671431519665047 | validation: 0.2809076284674779]
	TIME [epoch: 32.7 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27372839373275226		[learning rate: 0.0021762]
	Learning Rate: 0.00217617
	LOSS [training: 0.27372839373275226 | validation: 0.3194594009398414]
	TIME [epoch: 32.9 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32504578400427564		[learning rate: 0.002171]
	Learning Rate: 0.00217103
	LOSS [training: 0.32504578400427564 | validation: 0.3387590918311923]
	TIME [epoch: 32.8 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35002493588308603		[learning rate: 0.0021659]
	Learning Rate: 0.00216591
	LOSS [training: 0.35002493588308603 | validation: 0.3083113130635905]
	TIME [epoch: 32.9 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29766554197756323		[learning rate: 0.0021608]
	Learning Rate: 0.0021608
	LOSS [training: 0.29766554197756323 | validation: 0.2998278039047012]
	TIME [epoch: 32.7 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.332162258363585		[learning rate: 0.0021557]
	Learning Rate: 0.00215571
	LOSS [training: 0.332162258363585 | validation: 0.2900048322370597]
	TIME [epoch: 32.8 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2990259124923123		[learning rate: 0.0021506]
	Learning Rate: 0.00215062
	LOSS [training: 0.2990259124923123 | validation: 0.2965227359563401]
	TIME [epoch: 32.8 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28208120922291535		[learning rate: 0.0021455]
	Learning Rate: 0.00214555
	LOSS [training: 0.28208120922291535 | validation: 0.27782532063797977]
	TIME [epoch: 32.5 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32035342133453076		[learning rate: 0.0021405]
	Learning Rate: 0.00214049
	LOSS [training: 0.32035342133453076 | validation: 0.313257994518077]
	TIME [epoch: 32.7 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2926004161562923		[learning rate: 0.0021354]
	Learning Rate: 0.00213544
	LOSS [training: 0.2926004161562923 | validation: 0.29952808033494926]
	TIME [epoch: 32.7 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28709102126293073		[learning rate: 0.0021304]
	Learning Rate: 0.0021304
	LOSS [training: 0.28709102126293073 | validation: 0.2908079934873363]
	TIME [epoch: 32.7 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3096331187042368		[learning rate: 0.0021254]
	Learning Rate: 0.00212538
	LOSS [training: 0.3096331187042368 | validation: 0.2860039172439008]
	TIME [epoch: 32.7 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.277431054904106		[learning rate: 0.0021204]
	Learning Rate: 0.00212036
	LOSS [training: 0.277431054904106 | validation: 0.2642084027542107]
	TIME [epoch: 32.8 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32572423568378617		[learning rate: 0.0021154]
	Learning Rate: 0.00211536
	LOSS [training: 0.32572423568378617 | validation: 0.33366119561617114]
	TIME [epoch: 32.8 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27838159859339695		[learning rate: 0.0021104]
	Learning Rate: 0.00211037
	LOSS [training: 0.27838159859339695 | validation: 0.33387758554253977]
	TIME [epoch: 32.7 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3746431236123412		[learning rate: 0.0021054]
	Learning Rate: 0.00210539
	LOSS [training: 0.3746431236123412 | validation: 0.3411000758179791]
	TIME [epoch: 32.8 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2764470645967469		[learning rate: 0.0021004]
	Learning Rate: 0.00210043
	LOSS [training: 0.2764470645967469 | validation: 0.27095489577834425]
	TIME [epoch: 32.6 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29846282762045867		[learning rate: 0.0020955]
	Learning Rate: 0.00209547
	LOSS [training: 0.29846282762045867 | validation: 0.2757001730027153]
	TIME [epoch: 32.8 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2516627747278293		[learning rate: 0.0020905]
	Learning Rate: 0.00209053
	LOSS [training: 0.2516627747278293 | validation: 0.26478470455457]
	TIME [epoch: 32.8 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30131785954864077		[learning rate: 0.0020856]
	Learning Rate: 0.0020856
	LOSS [training: 0.30131785954864077 | validation: 0.31557666871708356]
	TIME [epoch: 32.7 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3624431735850644		[learning rate: 0.0020807]
	Learning Rate: 0.00208068
	LOSS [training: 0.3624431735850644 | validation: 0.4096802747451497]
	TIME [epoch: 32.7 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32418569757195975		[learning rate: 0.0020758]
	Learning Rate: 0.00207577
	LOSS [training: 0.32418569757195975 | validation: 0.3169589658507897]
	TIME [epoch: 32.8 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30719893447078384		[learning rate: 0.0020709]
	Learning Rate: 0.00207087
	LOSS [training: 0.30719893447078384 | validation: 0.3560176431750045]
	TIME [epoch: 32.8 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3038945487208632		[learning rate: 0.002066]
	Learning Rate: 0.00206599
	LOSS [training: 0.3038945487208632 | validation: 0.27986168351451923]
	TIME [epoch: 32.7 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2871120051688383		[learning rate: 0.0020611]
	Learning Rate: 0.00206112
	LOSS [training: 0.2871120051688383 | validation: 0.28980055551272854]
	TIME [epoch: 32.7 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33182093249576383		[learning rate: 0.0020563]
	Learning Rate: 0.00205626
	LOSS [training: 0.33182093249576383 | validation: 0.32540358304975037]
	TIME [epoch: 32.8 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2952033050142149		[learning rate: 0.0020514]
	Learning Rate: 0.0020514
	LOSS [training: 0.2952033050142149 | validation: 0.2803302139725036]
	TIME [epoch: 32.8 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2967351013539205		[learning rate: 0.0020466]
	Learning Rate: 0.00204657
	LOSS [training: 0.2967351013539205 | validation: 0.27223521224884806]
	TIME [epoch: 32.7 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2891967283827711		[learning rate: 0.0020417]
	Learning Rate: 0.00204174
	LOSS [training: 0.2891967283827711 | validation: 0.2720293113066105]
	TIME [epoch: 32.8 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29314722071176536		[learning rate: 0.0020369]
	Learning Rate: 0.00203692
	LOSS [training: 0.29314722071176536 | validation: 0.26890378443635377]
	TIME [epoch: 32.6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2835179209378902		[learning rate: 0.0020321]
	Learning Rate: 0.00203212
	LOSS [training: 0.2835179209378902 | validation: 0.34488294916827333]
	TIME [epoch: 32.8 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3235830245300243		[learning rate: 0.0020273]
	Learning Rate: 0.00202732
	LOSS [training: 0.3235830245300243 | validation: 0.30092497211908364]
	TIME [epoch: 32.8 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2908043899706402		[learning rate: 0.0020225]
	Learning Rate: 0.00202254
	LOSS [training: 0.2908043899706402 | validation: 0.3005303804507094]
	TIME [epoch: 32.7 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2959366682819267		[learning rate: 0.0020178]
	Learning Rate: 0.00201777
	LOSS [training: 0.2959366682819267 | validation: 0.28543776681593236]
	TIME [epoch: 32.7 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2656874305784915		[learning rate: 0.002013]
	Learning Rate: 0.00201301
	LOSS [training: 0.2656874305784915 | validation: 0.37850761836909824]
	TIME [epoch: 32.7 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3678162331967265		[learning rate: 0.0020083]
	Learning Rate: 0.00200826
	LOSS [training: 0.3678162331967265 | validation: 0.34648298711987086]
	TIME [epoch: 32.7 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28530182642232504		[learning rate: 0.0020035]
	Learning Rate: 0.00200353
	LOSS [training: 0.28530182642232504 | validation: 0.27791706146272666]
	TIME [epoch: 32.6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2639487100426198		[learning rate: 0.0019988]
	Learning Rate: 0.0019988
	LOSS [training: 0.2639487100426198 | validation: 0.27997474076374884]
	TIME [epoch: 32.7 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31113654057971407		[learning rate: 0.0019941]
	Learning Rate: 0.00199408
	LOSS [training: 0.31113654057971407 | validation: 0.3151653307843102]
	TIME [epoch: 32.8 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31615560731376163		[learning rate: 0.0019894]
	Learning Rate: 0.00198938
	LOSS [training: 0.31615560731376163 | validation: 0.28101963912560535]
	TIME [epoch: 32.7 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2625410519936624		[learning rate: 0.0019847]
	Learning Rate: 0.00198469
	LOSS [training: 0.2625410519936624 | validation: 0.25173460040394274]
	TIME [epoch: 32.7 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.306698391973072		[learning rate: 0.00198]
	Learning Rate: 0.00198001
	LOSS [training: 0.306698391973072 | validation: 0.37736955209591505]
	TIME [epoch: 32.6 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3178693037469877		[learning rate: 0.0019753]
	Learning Rate: 0.00197534
	LOSS [training: 0.3178693037469877 | validation: 0.26864758069795547]
	TIME [epoch: 32.8 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2725523413790011		[learning rate: 0.0019707]
	Learning Rate: 0.00197068
	LOSS [training: 0.2725523413790011 | validation: 0.31350434895819235]
	TIME [epoch: 32.7 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3144911395405424		[learning rate: 0.001966]
	Learning Rate: 0.00196603
	LOSS [training: 0.3144911395405424 | validation: 0.27549239587034885]
	TIME [epoch: 32.7 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2462383272048349		[learning rate: 0.0019614]
	Learning Rate: 0.00196139
	LOSS [training: 0.2462383272048349 | validation: 0.2698254567566548]
	TIME [epoch: 32.7 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3467554949991538		[learning rate: 0.0019568]
	Learning Rate: 0.00195676
	LOSS [training: 0.3467554949991538 | validation: 0.40114953089276606]
	TIME [epoch: 32.8 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3187262877298673		[learning rate: 0.0019521]
	Learning Rate: 0.00195215
	LOSS [training: 0.3187262877298673 | validation: 0.2905691171333634]
	TIME [epoch: 32.7 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26849186498119204		[learning rate: 0.0019475]
	Learning Rate: 0.00194754
	LOSS [training: 0.26849186498119204 | validation: 0.27025142518015044]
	TIME [epoch: 32.6 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35047405054735703		[learning rate: 0.0019429]
	Learning Rate: 0.00194295
	LOSS [training: 0.35047405054735703 | validation: 0.28807056660591]
	TIME [epoch: 32.8 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26891329361161526		[learning rate: 0.0019384]
	Learning Rate: 0.00193837
	LOSS [training: 0.26891329361161526 | validation: 0.25321476216954664]
	TIME [epoch: 32.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2810467333169766		[learning rate: 0.0019338]
	Learning Rate: 0.00193379
	LOSS [training: 0.2810467333169766 | validation: 0.2905184587186194]
	TIME [epoch: 32.6 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29233175495810104		[learning rate: 0.0019292]
	Learning Rate: 0.00192923
	LOSS [training: 0.29233175495810104 | validation: 0.27988832205752157]
	TIME [epoch: 32.8 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26974818497725195		[learning rate: 0.0019247]
	Learning Rate: 0.00192468
	LOSS [training: 0.26974818497725195 | validation: 0.2638550756916618]
	TIME [epoch: 32.7 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28700990536633236		[learning rate: 0.0019201]
	Learning Rate: 0.00192014
	LOSS [training: 0.28700990536633236 | validation: 0.3475838852993276]
	TIME [epoch: 32.8 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3216637268815758		[learning rate: 0.0019156]
	Learning Rate: 0.00191561
	LOSS [training: 0.3216637268815758 | validation: 0.32057522525487037]
	TIME [epoch: 32.7 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2773536259940978		[learning rate: 0.0019111]
	Learning Rate: 0.00191109
	LOSS [training: 0.2773536259940978 | validation: 0.25320268225568465]
	TIME [epoch: 32.8 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2917825815076596		[learning rate: 0.0019066]
	Learning Rate: 0.00190659
	LOSS [training: 0.2917825815076596 | validation: 0.28207142870080304]
	TIME [epoch: 32.7 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2660267687394231		[learning rate: 0.0019021]
	Learning Rate: 0.00190209
	LOSS [training: 0.2660267687394231 | validation: 0.27588505248747486]
	TIME [epoch: 32.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28792882492765715		[learning rate: 0.0018976]
	Learning Rate: 0.0018976
	LOSS [training: 0.28792882492765715 | validation: 0.2865536654894153]
	TIME [epoch: 32.7 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33332341389102293		[learning rate: 0.0018931]
	Learning Rate: 0.00189313
	LOSS [training: 0.33332341389102293 | validation: 0.3448717459382753]
	TIME [epoch: 32.7 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.307723633284119		[learning rate: 0.0018887]
	Learning Rate: 0.00188866
	LOSS [training: 0.307723633284119 | validation: 0.2870181545262671]
	TIME [epoch: 32.7 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27107782030603395		[learning rate: 0.0018842]
	Learning Rate: 0.00188421
	LOSS [training: 0.27107782030603395 | validation: 0.2648551941647235]
	TIME [epoch: 32.7 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25159894440527086		[learning rate: 0.0018798]
	Learning Rate: 0.00187976
	LOSS [training: 0.25159894440527086 | validation: 0.2339716976142477]
	TIME [epoch: 32.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31327557489708663		[learning rate: 0.0018753]
	Learning Rate: 0.00187533
	LOSS [training: 0.31327557489708663 | validation: 0.320494096654776]
	TIME [epoch: 32.6 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3287201287843896		[learning rate: 0.0018709]
	Learning Rate: 0.0018709
	LOSS [training: 0.3287201287843896 | validation: 0.2862489502065384]
	TIME [epoch: 32.7 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2882018832067552		[learning rate: 0.0018665]
	Learning Rate: 0.00186649
	LOSS [training: 0.2882018832067552 | validation: 0.3344706907242617]
	TIME [epoch: 32.8 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2811302745067707		[learning rate: 0.0018621]
	Learning Rate: 0.00186209
	LOSS [training: 0.2811302745067707 | validation: 0.30874112788505403]
	TIME [epoch: 32.7 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2555999044274875		[learning rate: 0.0018577]
	Learning Rate: 0.00185769
	LOSS [training: 0.2555999044274875 | validation: 0.2780304677036663]
	TIME [epoch: 32.7 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3611193523785097		[learning rate: 0.0018533]
	Learning Rate: 0.00185331
	LOSS [training: 0.3611193523785097 | validation: 0.3939091831215314]
	TIME [epoch: 32.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.313802644710652		[learning rate: 0.0018489]
	Learning Rate: 0.00184894
	LOSS [training: 0.313802644710652 | validation: 0.30652671492116984]
	TIME [epoch: 32.7 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30749091020635627		[learning rate: 0.0018446]
	Learning Rate: 0.00184458
	LOSS [training: 0.30749091020635627 | validation: 0.36939374833740013]
	TIME [epoch: 32.8 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29147493723712714		[learning rate: 0.0018402]
	Learning Rate: 0.00184023
	LOSS [training: 0.29147493723712714 | validation: 0.27546699149426745]
	TIME [epoch: 32.8 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25385427241495884		[learning rate: 0.0018359]
	Learning Rate: 0.00183589
	LOSS [training: 0.25385427241495884 | validation: 0.28686492427981775]
	TIME [epoch: 32.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3241660725533685		[learning rate: 0.0018316]
	Learning Rate: 0.00183156
	LOSS [training: 0.3241660725533685 | validation: 0.2805023310456473]
	TIME [epoch: 32.7 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25820902920262406		[learning rate: 0.0018272]
	Learning Rate: 0.00182724
	LOSS [training: 0.25820902920262406 | validation: 0.2615142327931198]
	TIME [epoch: 32.8 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.287609589718716		[learning rate: 0.0018229]
	Learning Rate: 0.00182293
	LOSS [training: 0.287609589718716 | validation: 0.2988315149821782]
	TIME [epoch: 32.7 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27283801648665645		[learning rate: 0.0018186]
	Learning Rate: 0.00181863
	LOSS [training: 0.27283801648665645 | validation: 0.26765833935458994]
	TIME [epoch: 32.6 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24068617049007343		[learning rate: 0.0018143]
	Learning Rate: 0.00181434
	LOSS [training: 0.24068617049007343 | validation: 0.23180541286240267]
	TIME [epoch: 32.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_773.pth
	Model improved!!!
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32596285951236864		[learning rate: 0.0018101]
	Learning Rate: 0.00181006
	LOSS [training: 0.32596285951236864 | validation: 0.36278875165514146]
	TIME [epoch: 32.7 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3213889778806123		[learning rate: 0.0018058]
	Learning Rate: 0.00180579
	LOSS [training: 0.3213889778806123 | validation: 0.30650294036381664]
	TIME [epoch: 32.6 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2764579224042879		[learning rate: 0.0018015]
	Learning Rate: 0.00180153
	LOSS [training: 0.2764579224042879 | validation: 0.24881090149684276]
	TIME [epoch: 32.8 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2615645068124539		[learning rate: 0.0017973]
	Learning Rate: 0.00179728
	LOSS [training: 0.2615645068124539 | validation: 0.3344613474077869]
	TIME [epoch: 32.6 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2862508178538596		[learning rate: 0.001793]
	Learning Rate: 0.00179304
	LOSS [training: 0.2862508178538596 | validation: 0.23790726698889025]
	TIME [epoch: 32.6 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2820529507852595		[learning rate: 0.0017888]
	Learning Rate: 0.00178881
	LOSS [training: 0.2820529507852595 | validation: 0.34847450711860456]
	TIME [epoch: 32.8 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2683967304901491		[learning rate: 0.0017846]
	Learning Rate: 0.00178459
	LOSS [training: 0.2683967304901491 | validation: 0.25618709738021606]
	TIME [epoch: 32.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2863469858411716		[learning rate: 0.0017804]
	Learning Rate: 0.00178038
	LOSS [training: 0.2863469858411716 | validation: 0.3332089485423808]
	TIME [epoch: 32.7 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30753325530395065		[learning rate: 0.0017762]
	Learning Rate: 0.00177618
	LOSS [training: 0.30753325530395065 | validation: 0.2992720281911648]
	TIME [epoch: 32.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2744366008946397		[learning rate: 0.001772]
	Learning Rate: 0.00177199
	LOSS [training: 0.2744366008946397 | validation: 0.26989097141778545]
	TIME [epoch: 32.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27628965273882966		[learning rate: 0.0017678]
	Learning Rate: 0.00176781
	LOSS [training: 0.27628965273882966 | validation: 0.2820496230515559]
	TIME [epoch: 32.8 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3168914174107549		[learning rate: 0.0017636]
	Learning Rate: 0.00176364
	LOSS [training: 0.3168914174107549 | validation: 0.3134943198276879]
	TIME [epoch: 32.7 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2756707023271261		[learning rate: 0.0017595]
	Learning Rate: 0.00175948
	LOSS [training: 0.2756707023271261 | validation: 0.2863719490734761]
	TIME [epoch: 32.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2615293834588879		[learning rate: 0.0017553]
	Learning Rate: 0.00175533
	LOSS [training: 0.2615293834588879 | validation: 0.2668948781560696]
	TIME [epoch: 32.8 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29518743048961243		[learning rate: 0.0017512]
	Learning Rate: 0.00175119
	LOSS [training: 0.29518743048961243 | validation: 0.27196671573495534]
	TIME [epoch: 32.7 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28199453131010277		[learning rate: 0.0017471]
	Learning Rate: 0.00174706
	LOSS [training: 0.28199453131010277 | validation: 0.26325396481826363]
	TIME [epoch: 32.7 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2624117108351835		[learning rate: 0.0017429]
	Learning Rate: 0.00174294
	LOSS [training: 0.2624117108351835 | validation: 0.37868260796521314]
	TIME [epoch: 32.7 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33550284845610817		[learning rate: 0.0017388]
	Learning Rate: 0.00173883
	LOSS [training: 0.33550284845610817 | validation: 0.28127469493801827]
	TIME [epoch: 32.6 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2542059967411754		[learning rate: 0.0017347]
	Learning Rate: 0.00173473
	LOSS [training: 0.2542059967411754 | validation: 0.25712260500506345]
	TIME [epoch: 32.7 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2970565652033834		[learning rate: 0.0017306]
	Learning Rate: 0.00173063
	LOSS [training: 0.2970565652033834 | validation: 0.3171598125365792]
	TIME [epoch: 32.8 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2972850265847607		[learning rate: 0.0017266]
	Learning Rate: 0.00172655
	LOSS [training: 0.2972850265847607 | validation: 0.3184761493011945]
	TIME [epoch: 32.7 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2817825498744133		[learning rate: 0.0017225]
	Learning Rate: 0.00172248
	LOSS [training: 0.2817825498744133 | validation: 0.24948980070418836]
	TIME [epoch: 32.6 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2867155516465867		[learning rate: 0.0017184]
	Learning Rate: 0.00171842
	LOSS [training: 0.2867155516465867 | validation: 0.3589256734675904]
	TIME [epoch: 32.8 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3070470528976007		[learning rate: 0.0017144]
	Learning Rate: 0.00171436
	LOSS [training: 0.3070470528976007 | validation: 0.29368039884351405]
	TIME [epoch: 32.7 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2736807447404723		[learning rate: 0.0017103]
	Learning Rate: 0.00171032
	LOSS [training: 0.2736807447404723 | validation: 0.2558194745730463]
	TIME [epoch: 32.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25414996062818124		[learning rate: 0.0017063]
	Learning Rate: 0.00170628
	LOSS [training: 0.25414996062818124 | validation: 0.31105361291144473]
	TIME [epoch: 32.7 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3075305491468696		[learning rate: 0.0017023]
	Learning Rate: 0.00170226
	LOSS [training: 0.3075305491468696 | validation: 0.28921676843795796]
	TIME [epoch: 32.8 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27121861664877445		[learning rate: 0.0016982]
	Learning Rate: 0.00169824
	LOSS [training: 0.27121861664877445 | validation: 0.2724063961664308]
	TIME [epoch: 32.7 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2646893942950105		[learning rate: 0.0016942]
	Learning Rate: 0.00169424
	LOSS [training: 0.2646893942950105 | validation: 0.2562707314884647]
	TIME [epoch: 32.7 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27450830996420306		[learning rate: 0.0016902]
	Learning Rate: 0.00169024
	LOSS [training: 0.27450830996420306 | validation: 0.2750851814643802]
	TIME [epoch: 32.8 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2796993752305019		[learning rate: 0.0016863]
	Learning Rate: 0.00168625
	LOSS [training: 0.2796993752305019 | validation: 0.3118653628232444]
	TIME [epoch: 32.8 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27764573472022713		[learning rate: 0.0016823]
	Learning Rate: 0.00168228
	LOSS [training: 0.27764573472022713 | validation: 0.256826111328031]
	TIME [epoch: 32.9 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25594441271721047		[learning rate: 0.0016783]
	Learning Rate: 0.00167831
	LOSS [training: 0.25594441271721047 | validation: 0.3482390200932248]
	TIME [epoch: 32.8 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30697053732398544		[learning rate: 0.0016743]
	Learning Rate: 0.00167435
	LOSS [training: 0.30697053732398544 | validation: 0.3113357312742294]
	TIME [epoch: 32.7 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2845803653497144		[learning rate: 0.0016704]
	Learning Rate: 0.0016704
	LOSS [training: 0.2845803653497144 | validation: 0.2945980537259518]
	TIME [epoch: 32.8 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2559487744710568		[learning rate: 0.0016665]
	Learning Rate: 0.00166646
	LOSS [training: 0.2559487744710568 | validation: 0.32561784256830906]
	TIME [epoch: 32.8 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28419947025795284		[learning rate: 0.0016625]
	Learning Rate: 0.00166253
	LOSS [training: 0.28419947025795284 | validation: 0.26044996421408384]
	TIME [epoch: 32.7 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27048005893345184		[learning rate: 0.0016586]
	Learning Rate: 0.00165861
	LOSS [training: 0.27048005893345184 | validation: 0.25185989578232276]
	TIME [epoch: 32.7 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25557034842451937		[learning rate: 0.0016547]
	Learning Rate: 0.0016547
	LOSS [training: 0.25557034842451937 | validation: 0.31650055805734123]
	TIME [epoch: 32.7 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2910472148802073		[learning rate: 0.0016508]
	Learning Rate: 0.00165079
	LOSS [training: 0.2910472148802073 | validation: 0.2530929606501998]
	TIME [epoch: 32.8 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29266412044668777		[learning rate: 0.0016469]
	Learning Rate: 0.0016469
	LOSS [training: 0.29266412044668777 | validation: 0.2670229401249942]
	TIME [epoch: 32.7 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2523366395408817		[learning rate: 0.001643]
	Learning Rate: 0.00164301
	LOSS [training: 0.2523366395408817 | validation: 0.25793758223815144]
	TIME [epoch: 32.8 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.257829451715194		[learning rate: 0.0016391]
	Learning Rate: 0.00163914
	LOSS [training: 0.257829451715194 | validation: 0.2631977133972276]
	TIME [epoch: 32.7 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31473092238384653		[learning rate: 0.0016353]
	Learning Rate: 0.00163527
	LOSS [training: 0.31473092238384653 | validation: 0.2924887417572888]
	TIME [epoch: 32.7 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24557368856700673		[learning rate: 0.0016314]
	Learning Rate: 0.00163141
	LOSS [training: 0.24557368856700673 | validation: 0.2496519714935454]
	TIME [epoch: 32.8 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3088418408025634		[learning rate: 0.0016276]
	Learning Rate: 0.00162757
	LOSS [training: 0.3088418408025634 | validation: 0.33829968273799393]
	TIME [epoch: 32.8 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2933418646705131		[learning rate: 0.0016237]
	Learning Rate: 0.00162373
	LOSS [training: 0.2933418646705131 | validation: 0.25874318205638336]
	TIME [epoch: 32.7 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25845251138951175		[learning rate: 0.0016199]
	Learning Rate: 0.0016199
	LOSS [training: 0.25845251138951175 | validation: 0.4103573229253888]
	TIME [epoch: 32.8 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31351828330161047		[learning rate: 0.0016161]
	Learning Rate: 0.00161608
	LOSS [training: 0.31351828330161047 | validation: 0.24637852622277573]
	TIME [epoch: 32.8 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24048514799733073		[learning rate: 0.0016123]
	Learning Rate: 0.00161226
	LOSS [training: 0.24048514799733073 | validation: 0.22918582906542723]
	TIME [epoch: 32.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_823.pth
	Model improved!!!
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27890681334591977		[learning rate: 0.0016085]
	Learning Rate: 0.00160846
	LOSS [training: 0.27890681334591977 | validation: 0.276297227813829]
	TIME [epoch: 32.8 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28051389761480533		[learning rate: 0.0016047]
	Learning Rate: 0.00160467
	LOSS [training: 0.28051389761480533 | validation: 0.26361608984047574]
	TIME [epoch: 32.9 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26670906553284396		[learning rate: 0.0016009]
	Learning Rate: 0.00160088
	LOSS [training: 0.26670906553284396 | validation: 0.29770235598712175]
	TIME [epoch: 32.7 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3391619623251649		[learning rate: 0.0015971]
	Learning Rate: 0.0015971
	LOSS [training: 0.3391619623251649 | validation: 0.2820952480961604]
	TIME [epoch: 32.7 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25860742529823705		[learning rate: 0.0015933]
	Learning Rate: 0.00159334
	LOSS [training: 0.25860742529823705 | validation: 0.2544114057319712]
	TIME [epoch: 32.8 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2631448722856776		[learning rate: 0.0015896]
	Learning Rate: 0.00158958
	LOSS [training: 0.2631448722856776 | validation: 0.2546908453805341]
	TIME [epoch: 32.9 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2876165551459898		[learning rate: 0.0015858]
	Learning Rate: 0.00158583
	LOSS [training: 0.2876165551459898 | validation: 0.28023539864201574]
	TIME [epoch: 32.8 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2541285008630242		[learning rate: 0.0015821]
	Learning Rate: 0.00158209
	LOSS [training: 0.2541285008630242 | validation: 0.2446851983681988]
	TIME [epoch: 32.8 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3211749645188474		[learning rate: 0.0015784]
	Learning Rate: 0.00157836
	LOSS [training: 0.3211749645188474 | validation: 0.2548625376699773]
	TIME [epoch: 32.7 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2640581714513935		[learning rate: 0.0015746]
	Learning Rate: 0.00157463
	LOSS [training: 0.2640581714513935 | validation: 0.27088529058063593]
	TIME [epoch: 32.9 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2531534151381232		[learning rate: 0.0015709]
	Learning Rate: 0.00157092
	LOSS [training: 0.2531534151381232 | validation: 0.25111733816602017]
	TIME [epoch: 32.8 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29865154034467045		[learning rate: 0.0015672]
	Learning Rate: 0.00156721
	LOSS [training: 0.29865154034467045 | validation: 0.28184922798998924]
	TIME [epoch: 32.7 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.258797523989482		[learning rate: 0.0015635]
	Learning Rate: 0.00156352
	LOSS [training: 0.258797523989482 | validation: 0.2605777768889027]
	TIME [epoch: 32.8 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2497779847562478		[learning rate: 0.0015598]
	Learning Rate: 0.00155983
	LOSS [training: 0.2497779847562478 | validation: 0.24295982341172126]
	TIME [epoch: 32.8 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3199250685581204		[learning rate: 0.0015561]
	Learning Rate: 0.00155615
	LOSS [training: 0.3199250685581204 | validation: 0.3346136585355427]
	TIME [epoch: 32.8 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27976217495633027		[learning rate: 0.0015525]
	Learning Rate: 0.00155248
	LOSS [training: 0.27976217495633027 | validation: 0.2746918151419412]
	TIME [epoch: 32.8 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2809788592817465		[learning rate: 0.0015488]
	Learning Rate: 0.00154882
	LOSS [training: 0.2809788592817465 | validation: 0.31576626592225887]
	TIME [epoch: 32.7 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2826399733554477		[learning rate: 0.0015452]
	Learning Rate: 0.00154516
	LOSS [training: 0.2826399733554477 | validation: 0.2560026440401487]
	TIME [epoch: 32.8 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23472035672053382		[learning rate: 0.0015415]
	Learning Rate: 0.00154152
	LOSS [training: 0.23472035672053382 | validation: 0.22886808474425097]
	TIME [epoch: 32.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_842.pth
	Model improved!!!
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27857077211442155		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 0.27857077211442155 | validation: 0.3554112061374163]
	TIME [epoch: 32.8 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.280937303616062		[learning rate: 0.0015343]
	Learning Rate: 0.00153425
	LOSS [training: 0.280937303616062 | validation: 0.26338286431096103]
	TIME [epoch: 32.8 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24592325559462586		[learning rate: 0.0015306]
	Learning Rate: 0.00153064
	LOSS [training: 0.24592325559462586 | validation: 0.2305813012929775]
	TIME [epoch: 32.9 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2521837452490474		[learning rate: 0.001527]
	Learning Rate: 0.00152703
	LOSS [training: 0.2521837452490474 | validation: 0.3019411532754012]
	TIME [epoch: 32.8 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2912781308198569		[learning rate: 0.0015234]
	Learning Rate: 0.00152342
	LOSS [training: 0.2912781308198569 | validation: 0.2947624540481178]
	TIME [epoch: 32.7 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2627242659229968		[learning rate: 0.0015198]
	Learning Rate: 0.00151983
	LOSS [training: 0.2627242659229968 | validation: 0.24755954377418588]
	TIME [epoch: 32.9 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26360613297649715		[learning rate: 0.0015162]
	Learning Rate: 0.00151624
	LOSS [training: 0.26360613297649715 | validation: 0.2907996973098934]
	TIME [epoch: 32.9 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2867932169800303		[learning rate: 0.0015127]
	Learning Rate: 0.00151267
	LOSS [training: 0.2867932169800303 | validation: 0.2524772654797314]
	TIME [epoch: 32.9 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24053914950660907		[learning rate: 0.0015091]
	Learning Rate: 0.0015091
	LOSS [training: 0.24053914950660907 | validation: 0.2617970003189694]
	TIME [epoch: 32.8 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2919782176351625		[learning rate: 0.0015055]
	Learning Rate: 0.00150554
	LOSS [training: 0.2919782176351625 | validation: 0.26947355680224905]
	TIME [epoch: 32.8 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2654961915629725		[learning rate: 0.001502]
	Learning Rate: 0.00150199
	LOSS [training: 0.2654961915629725 | validation: 0.2601759267377344]
	TIME [epoch: 32.9 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2404860335898267		[learning rate: 0.0014984]
	Learning Rate: 0.00149845
	LOSS [training: 0.2404860335898267 | validation: 0.26341108535426583]
	TIME [epoch: 32.8 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3233800125878209		[learning rate: 0.0014949]
	Learning Rate: 0.00149491
	LOSS [training: 0.3233800125878209 | validation: 0.2813487503876616]
	TIME [epoch: 32.9 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26047608007001477		[learning rate: 0.0014914]
	Learning Rate: 0.00149139
	LOSS [training: 0.26047608007001477 | validation: 0.24147913553895428]
	TIME [epoch: 32.7 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27821880806780186		[learning rate: 0.0014879]
	Learning Rate: 0.00148787
	LOSS [training: 0.27821880806780186 | validation: 0.3229596044831311]
	TIME [epoch: 32.8 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2764076709466602		[learning rate: 0.0014844]
	Learning Rate: 0.00148436
	LOSS [training: 0.2764076709466602 | validation: 0.24902279577501418]
	TIME [epoch: 32.8 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2342507410400945		[learning rate: 0.0014809]
	Learning Rate: 0.00148086
	LOSS [training: 0.2342507410400945 | validation: 0.22441594898738454]
	TIME [epoch: 32.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_859.pth
	Model improved!!!
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2924257269025615		[learning rate: 0.0014774]
	Learning Rate: 0.00147736
	LOSS [training: 0.2924257269025615 | validation: 0.3477768571768415]
	TIME [epoch: 32.9 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29844196074713575		[learning rate: 0.0014739]
	Learning Rate: 0.00147388
	LOSS [training: 0.29844196074713575 | validation: 0.2569565302235446]
	TIME [epoch: 32.9 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25963546258623404		[learning rate: 0.0014704]
	Learning Rate: 0.0014704
	LOSS [training: 0.25963546258623404 | validation: 0.3114219162290105]
	TIME [epoch: 32.9 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27280391951961674		[learning rate: 0.0014669]
	Learning Rate: 0.00146693
	LOSS [training: 0.27280391951961674 | validation: 0.2424910580788587]
	TIME [epoch: 32.9 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22969223902887442		[learning rate: 0.0014635]
	Learning Rate: 0.00146347
	LOSS [training: 0.22969223902887442 | validation: 0.2629950007274237]
	TIME [epoch: 32.8 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29154590531310376		[learning rate: 0.00146]
	Learning Rate: 0.00146002
	LOSS [training: 0.29154590531310376 | validation: 0.2772051671253364]
	TIME [epoch: 32.9 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25636757606323285		[learning rate: 0.0014566]
	Learning Rate: 0.00145658
	LOSS [training: 0.25636757606323285 | validation: 0.23773650574311483]
	TIME [epoch: 32.9 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2595509859960061		[learning rate: 0.0014531]
	Learning Rate: 0.00145314
	LOSS [training: 0.2595509859960061 | validation: 0.29951135622180336]
	TIME [epoch: 32.9 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2544624336150436		[learning rate: 0.0014497]
	Learning Rate: 0.00144971
	LOSS [training: 0.2544624336150436 | validation: 0.24271001621472582]
	TIME [epoch: 32.9 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2614231955330059		[learning rate: 0.0014463]
	Learning Rate: 0.00144629
	LOSS [training: 0.2614231955330059 | validation: 0.258532911315538]
	TIME [epoch: 32.9 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2539896250213349		[learning rate: 0.0014429]
	Learning Rate: 0.00144288
	LOSS [training: 0.2539896250213349 | validation: 0.2882888347519712]
	TIME [epoch: 32.9 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3134626729111306		[learning rate: 0.0014395]
	Learning Rate: 0.00143948
	LOSS [training: 0.3134626729111306 | validation: 0.26526989544564133]
	TIME [epoch: 32.7 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2575963631230309		[learning rate: 0.0014361]
	Learning Rate: 0.00143608
	LOSS [training: 0.2575963631230309 | validation: 0.2622659202896178]
	TIME [epoch: 32.9 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23649803042022022		[learning rate: 0.0014327]
	Learning Rate: 0.0014327
	LOSS [training: 0.23649803042022022 | validation: 0.23329974409147272]
	TIME [epoch: 32.9 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2797620013587079		[learning rate: 0.0014293]
	Learning Rate: 0.00142932
	LOSS [training: 0.2797620013587079 | validation: 0.26573973298314885]
	TIME [epoch: 32.9 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2696406729783514		[learning rate: 0.0014259]
	Learning Rate: 0.00142594
	LOSS [training: 0.2696406729783514 | validation: 0.24531054996221202]
	TIME [epoch: 32.9 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2771594114326073		[learning rate: 0.0014226]
	Learning Rate: 0.00142258
	LOSS [training: 0.2771594114326073 | validation: 0.2880204176065854]
	TIME [epoch: 32.8 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26359773797453034		[learning rate: 0.0014192]
	Learning Rate: 0.00141923
	LOSS [training: 0.26359773797453034 | validation: 0.25231287325264407]
	TIME [epoch: 32.9 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2629552220039897		[learning rate: 0.0014159]
	Learning Rate: 0.00141588
	LOSS [training: 0.2629552220039897 | validation: 0.2958277961788157]
	TIME [epoch: 33 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26792382195433223		[learning rate: 0.0014125]
	Learning Rate: 0.00141254
	LOSS [training: 0.26792382195433223 | validation: 0.2506923728164757]
	TIME [epoch: 32.9 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24187480476651257		[learning rate: 0.0014092]
	Learning Rate: 0.00140921
	LOSS [training: 0.24187480476651257 | validation: 0.24800050095430953]
	TIME [epoch: 32.9 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23178120067878152		[learning rate: 0.0014059]
	Learning Rate: 0.00140588
	LOSS [training: 0.23178120067878152 | validation: 0.249754125672203]
	TIME [epoch: 32.9 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2943469099028567		[learning rate: 0.0014026]
	Learning Rate: 0.00140257
	LOSS [training: 0.2943469099028567 | validation: 0.2601499962814419]
	TIME [epoch: 32.9 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25959306404311716		[learning rate: 0.0013993]
	Learning Rate: 0.00139926
	LOSS [training: 0.25959306404311716 | validation: 0.2491715465856971]
	TIME [epoch: 33 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23443843065125575		[learning rate: 0.001396]
	Learning Rate: 0.00139596
	LOSS [training: 0.23443843065125575 | validation: 0.2545297144968274]
	TIME [epoch: 33 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3177586601977745		[learning rate: 0.0013927]
	Learning Rate: 0.00139266
	LOSS [training: 0.3177586601977745 | validation: 0.2953255837677643]
	TIME [epoch: 32.9 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27398987667693675		[learning rate: 0.0013894]
	Learning Rate: 0.00138938
	LOSS [training: 0.27398987667693675 | validation: 0.2548132283568743]
	TIME [epoch: 32.8 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23677182916771616		[learning rate: 0.0013861]
	Learning Rate: 0.0013861
	LOSS [training: 0.23677182916771616 | validation: 0.22685856510349395]
	TIME [epoch: 32.8 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26945494055903		[learning rate: 0.0013828]
	Learning Rate: 0.00138283
	LOSS [training: 0.26945494055903 | validation: 0.2399496667359724]
	TIME [epoch: 32.9 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23683825297567165		[learning rate: 0.0013796]
	Learning Rate: 0.00137957
	LOSS [training: 0.23683825297567165 | validation: 0.23059162489656537]
	TIME [epoch: 33 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.271180630545877		[learning rate: 0.0013763]
	Learning Rate: 0.00137632
	LOSS [training: 0.271180630545877 | validation: 0.3489066560169457]
	TIME [epoch: 32.9 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27927370061221823		[learning rate: 0.0013731]
	Learning Rate: 0.00137307
	LOSS [training: 0.27927370061221823 | validation: 0.2575544918978943]
	TIME [epoch: 32.9 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.245651417950855		[learning rate: 0.0013698]
	Learning Rate: 0.00136983
	LOSS [training: 0.245651417950855 | validation: 0.28164050023955234]
	TIME [epoch: 33 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27882384250002357		[learning rate: 0.0013666]
	Learning Rate: 0.0013666
	LOSS [training: 0.27882384250002357 | validation: 0.2606913959782489]
	TIME [epoch: 33 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2396054940176562		[learning rate: 0.0013634]
	Learning Rate: 0.00136338
	LOSS [training: 0.2396054940176562 | validation: 0.25168946071401366]
	TIME [epoch: 32.9 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2854125536333323		[learning rate: 0.0013602]
	Learning Rate: 0.00136016
	LOSS [training: 0.2854125536333323 | validation: 0.2551822613748512]
	TIME [epoch: 32.9 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23953960671986957		[learning rate: 0.001357]
	Learning Rate: 0.00135695
	LOSS [training: 0.23953960671986957 | validation: 0.24797522453723153]
	TIME [epoch: 33 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25189309717468467		[learning rate: 0.0013538]
	Learning Rate: 0.00135375
	LOSS [training: 0.25189309717468467 | validation: 0.2510908516390532]
	TIME [epoch: 32.9 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2463798814896222		[learning rate: 0.0013506]
	Learning Rate: 0.00135056
	LOSS [training: 0.2463798814896222 | validation: 0.22781570239507615]
	TIME [epoch: 32.9 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2550598506682519		[learning rate: 0.0013474]
	Learning Rate: 0.00134737
	LOSS [training: 0.2550598506682519 | validation: 0.2541123696068244]
	TIME [epoch: 32.9 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29129659919870393		[learning rate: 0.0013442]
	Learning Rate: 0.00134419
	LOSS [training: 0.29129659919870393 | validation: 0.26713180639263107]
	TIME [epoch: 32.9 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2787482331290826		[learning rate: 0.001341]
	Learning Rate: 0.00134102
	LOSS [training: 0.2787482331290826 | validation: 0.24473503336748614]
	TIME [epoch: 32.8 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23686594036784067		[learning rate: 0.0013379]
	Learning Rate: 0.00133786
	LOSS [training: 0.23686594036784067 | validation: 0.2513627006754294]
	TIME [epoch: 32.9 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29394032763118244		[learning rate: 0.0013347]
	Learning Rate: 0.0013347
	LOSS [training: 0.29394032763118244 | validation: 0.2507829517154223]
	TIME [epoch: 32.9 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23471440328152593		[learning rate: 0.0013316]
	Learning Rate: 0.00133155
	LOSS [training: 0.23471440328152593 | validation: 0.23759431925329405]
	TIME [epoch: 32.8 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26950744687684386		[learning rate: 0.0013284]
	Learning Rate: 0.00132841
	LOSS [training: 0.26950744687684386 | validation: 0.2736592044143068]
	TIME [epoch: 32.8 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27519115151636736		[learning rate: 0.0013253]
	Learning Rate: 0.00132528
	LOSS [training: 0.27519115151636736 | validation: 0.28495681399643874]
	TIME [epoch: 32.7 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25501382900862923		[learning rate: 0.0013222]
	Learning Rate: 0.00132215
	LOSS [training: 0.25501382900862923 | validation: 0.23256031934590712]
	TIME [epoch: 32.9 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23260003291660436		[learning rate: 0.001319]
	Learning Rate: 0.00131904
	LOSS [training: 0.23260003291660436 | validation: 0.2434338953429681]
	TIME [epoch: 33 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28228391262207675		[learning rate: 0.0013159]
	Learning Rate: 0.00131592
	LOSS [training: 0.28228391262207675 | validation: 0.2610643311354748]
	TIME [epoch: 32.8 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24447850040976699		[learning rate: 0.0013128]
	Learning Rate: 0.00131282
	LOSS [training: 0.24447850040976699 | validation: 0.2527782461869459]
	TIME [epoch: 32.9 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2694441070858919		[learning rate: 0.0013097]
	Learning Rate: 0.00130972
	LOSS [training: 0.2694441070858919 | validation: 0.2554276938092279]
	TIME [epoch: 32.9 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24923858145810512		[learning rate: 0.0013066]
	Learning Rate: 0.00130663
	LOSS [training: 0.24923858145810512 | validation: 0.23348841747769422]
	TIME [epoch: 32.9 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23359491901948556		[learning rate: 0.0013036]
	Learning Rate: 0.00130355
	LOSS [training: 0.23359491901948556 | validation: 0.2989689341505449]
	TIME [epoch: 32.9 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27333058944585137		[learning rate: 0.0013005]
	Learning Rate: 0.00130048
	LOSS [training: 0.27333058944585137 | validation: 0.24776074366572087]
	TIME [epoch: 32.8 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2347285919520024		[learning rate: 0.0012974]
	Learning Rate: 0.00129741
	LOSS [training: 0.2347285919520024 | validation: 0.24985225182237075]
	TIME [epoch: 33 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25548740702457534		[learning rate: 0.0012943]
	Learning Rate: 0.00129435
	LOSS [training: 0.25548740702457534 | validation: 0.24106234042058156]
	TIME [epoch: 32.9 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2590187215335105		[learning rate: 0.0012913]
	Learning Rate: 0.0012913
	LOSS [training: 0.2590187215335105 | validation: 0.24661398057544087]
	TIME [epoch: 32.9 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24539415632357675		[learning rate: 0.0012882]
	Learning Rate: 0.00128825
	LOSS [training: 0.24539415632357675 | validation: 0.23808151080137435]
	TIME [epoch: 33 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23279809017440392		[learning rate: 0.0012852]
	Learning Rate: 0.00128521
	LOSS [training: 0.23279809017440392 | validation: 0.22642381444385362]
	TIME [epoch: 33 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26733747614133485		[learning rate: 0.0012822]
	Learning Rate: 0.00128218
	LOSS [training: 0.26733747614133485 | validation: 0.2583940576912453]
	TIME [epoch: 32.9 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.268579862909452		[learning rate: 0.0012792]
	Learning Rate: 0.00127915
	LOSS [training: 0.268579862909452 | validation: 0.26750988320272867]
	TIME [epoch: 32.8 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2570914029151157		[learning rate: 0.0012761]
	Learning Rate: 0.00127614
	LOSS [training: 0.2570914029151157 | validation: 0.2619361037155667]
	TIME [epoch: 33 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.248354613172703		[learning rate: 0.0012731]
	Learning Rate: 0.00127313
	LOSS [training: 0.248354613172703 | validation: 0.27615823754064783]
	TIME [epoch: 32.9 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2782635688854288		[learning rate: 0.0012701]
	Learning Rate: 0.00127012
	LOSS [training: 0.2782635688854288 | validation: 0.24670342575746845]
	TIME [epoch: 32.9 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24559734980372427		[learning rate: 0.0012671]
	Learning Rate: 0.00126713
	LOSS [training: 0.24559734980372427 | validation: 0.267792797658849]
	TIME [epoch: 33 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2519360451925495		[learning rate: 0.0012641]
	Learning Rate: 0.00126414
	LOSS [training: 0.2519360451925495 | validation: 0.2385688690127104]
	TIME [epoch: 32.8 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24471987179803845		[learning rate: 0.0012612]
	Learning Rate: 0.00126116
	LOSS [training: 0.24471987179803845 | validation: 0.28156132667221667]
	TIME [epoch: 33 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2664924608982324		[learning rate: 0.0012582]
	Learning Rate: 0.00125818
	LOSS [training: 0.2664924608982324 | validation: 0.2581313988702165]
	TIME [epoch: 32.9 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2380276673516929		[learning rate: 0.0012552]
	Learning Rate: 0.00125521
	LOSS [training: 0.2380276673516929 | validation: 0.2545049522281057]
	TIME [epoch: 32.9 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2720988293644548		[learning rate: 0.0012523]
	Learning Rate: 0.00125225
	LOSS [training: 0.2720988293644548 | validation: 0.2501053246773094]
	TIME [epoch: 32.9 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23905093166781777		[learning rate: 0.0012493]
	Learning Rate: 0.0012493
	LOSS [training: 0.23905093166781777 | validation: 0.24350338913270847]
	TIME [epoch: 32.8 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2566695555134671		[learning rate: 0.0012464]
	Learning Rate: 0.00124635
	LOSS [training: 0.2566695555134671 | validation: 0.24324611306124266]
	TIME [epoch: 32.9 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23069737956344452		[learning rate: 0.0012434]
	Learning Rate: 0.00124341
	LOSS [training: 0.23069737956344452 | validation: 0.2238441948962813]
	TIME [epoch: 32.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_933.pth
	Model improved!!!
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2416290922587831		[learning rate: 0.0012405]
	Learning Rate: 0.00124048
	LOSS [training: 0.2416290922587831 | validation: 0.23682778629891243]
	TIME [epoch: 32.9 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2743969601204963		[learning rate: 0.0012376]
	Learning Rate: 0.00123755
	LOSS [training: 0.2743969601204963 | validation: 0.2712653073069423]
	TIME [epoch: 32.9 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25502924194658505		[learning rate: 0.0012346]
	Learning Rate: 0.00123463
	LOSS [training: 0.25502924194658505 | validation: 0.26757306477670273]
	TIME [epoch: 32.9 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.257535725026631		[learning rate: 0.0012317]
	Learning Rate: 0.00123172
	LOSS [training: 0.257535725026631 | validation: 0.2793948471367206]
	TIME [epoch: 32.9 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2675836055524272		[learning rate: 0.0012288]
	Learning Rate: 0.00122882
	LOSS [training: 0.2675836055524272 | validation: 0.24604740400863756]
	TIME [epoch: 32.9 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24197356487827965		[learning rate: 0.0012259]
	Learning Rate: 0.00122592
	LOSS [training: 0.24197356487827965 | validation: 0.24511704547219637]
	TIME [epoch: 32.9 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23996116288083982		[learning rate: 0.001223]
	Learning Rate: 0.00122303
	LOSS [training: 0.23996116288083982 | validation: 0.24606797031854438]
	TIME [epoch: 33 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2429119429267393		[learning rate: 0.0012201]
	Learning Rate: 0.00122014
	LOSS [training: 0.2429119429267393 | validation: 0.24028139253766917]
	TIME [epoch: 32.9 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25029768423481685		[learning rate: 0.0012173]
	Learning Rate: 0.00121726
	LOSS [training: 0.25029768423481685 | validation: 0.2494132010014764]
	TIME [epoch: 32.9 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25057015370160396		[learning rate: 0.0012144]
	Learning Rate: 0.00121439
	LOSS [training: 0.25057015370160396 | validation: 0.23496986911479373]
	TIME [epoch: 32.8 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2354077774195131		[learning rate: 0.0012115]
	Learning Rate: 0.00121153
	LOSS [training: 0.2354077774195131 | validation: 0.29869429336121206]
	TIME [epoch: 32.9 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2504666465282347		[learning rate: 0.0012087]
	Learning Rate: 0.00120867
	LOSS [training: 0.2504666465282347 | validation: 0.24677695744443484]
	TIME [epoch: 32.9 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2514303656853923		[learning rate: 0.0012058]
	Learning Rate: 0.00120582
	LOSS [training: 0.2514303656853923 | validation: 0.30547979002690073]
	TIME [epoch: 32.8 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2641637555241888		[learning rate: 0.001203]
	Learning Rate: 0.00120297
	LOSS [training: 0.2641637555241888 | validation: 0.27075493407572016]
	TIME [epoch: 32.9 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25886658384362515		[learning rate: 0.0012001]
	Learning Rate: 0.00120014
	LOSS [training: 0.25886658384362515 | validation: 0.2929433496725645]
	TIME [epoch: 33 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26162077428881225		[learning rate: 0.0011973]
	Learning Rate: 0.00119731
	LOSS [training: 0.26162077428881225 | validation: 0.23312261138995538]
	TIME [epoch: 32.9 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23934866086878703		[learning rate: 0.0011945]
	Learning Rate: 0.00119448
	LOSS [training: 0.23934866086878703 | validation: 0.23878255858657038]
	TIME [epoch: 32.8 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2369582783139034		[learning rate: 0.0011917]
	Learning Rate: 0.00119166
	LOSS [training: 0.2369582783139034 | validation: 0.23991324030845362]
	TIME [epoch: 32.9 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2922089968279885		[learning rate: 0.0011889]
	Learning Rate: 0.00118885
	LOSS [training: 0.2922089968279885 | validation: 0.2472233443747754]
	TIME [epoch: 32.8 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26071355516923517		[learning rate: 0.001186]
	Learning Rate: 0.00118605
	LOSS [training: 0.26071355516923517 | validation: 0.24002404244850195]
	TIME [epoch: 32.9 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26060663246930305		[learning rate: 0.0011833]
	Learning Rate: 0.00118325
	LOSS [training: 0.26060663246930305 | validation: 0.25077723344510094]
	TIME [epoch: 32.9 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23261621912533867		[learning rate: 0.0011805]
	Learning Rate: 0.00118046
	LOSS [training: 0.23261621912533867 | validation: 0.22650793915468284]
	TIME [epoch: 32.9 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25658482941891614		[learning rate: 0.0011777]
	Learning Rate: 0.00117768
	LOSS [training: 0.25658482941891614 | validation: 0.24937115900415233]
	TIME [epoch: 32.9 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23242891325767712		[learning rate: 0.0011749]
	Learning Rate: 0.0011749
	LOSS [training: 0.23242891325767712 | validation: 0.23178520360086885]
	TIME [epoch: 33 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23576841031552756		[learning rate: 0.0011721]
	Learning Rate: 0.00117213
	LOSS [training: 0.23576841031552756 | validation: 0.2286464548825647]
	TIME [epoch: 32.9 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26025213054243		[learning rate: 0.0011694]
	Learning Rate: 0.00116936
	LOSS [training: 0.26025213054243 | validation: 0.2565018629289365]
	TIME [epoch: 32.8 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24783239819575903		[learning rate: 0.0011666]
	Learning Rate: 0.0011666
	LOSS [training: 0.24783239819575903 | validation: 0.23257081373987576]
	TIME [epoch: 32.9 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26094492850313		[learning rate: 0.0011639]
	Learning Rate: 0.00116385
	LOSS [training: 0.26094492850313 | validation: 0.29340261070459356]
	TIME [epoch: 32.8 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2456363694836742		[learning rate: 0.0011611]
	Learning Rate: 0.00116111
	LOSS [training: 0.2456363694836742 | validation: 0.2224771594925478]
	TIME [epoch: 32.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_962.pth
	Model improved!!!
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2329574261432804		[learning rate: 0.0011584]
	Learning Rate: 0.00115837
	LOSS [training: 0.2329574261432804 | validation: 0.2476762261428582]
	TIME [epoch: 32.9 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2673955282753944		[learning rate: 0.0011556]
	Learning Rate: 0.00115563
	LOSS [training: 0.2673955282753944 | validation: 0.31088912840543675]
	TIME [epoch: 32.8 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27088871624497246		[learning rate: 0.0011529]
	Learning Rate: 0.00115291
	LOSS [training: 0.27088871624497246 | validation: 0.24934686072153053]
	TIME [epoch: 32.9 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24295392637674462		[learning rate: 0.0011502]
	Learning Rate: 0.00115019
	LOSS [training: 0.24295392637674462 | validation: 0.22687304028970973]
	TIME [epoch: 32.9 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24415808611282097		[learning rate: 0.0011475]
	Learning Rate: 0.00114748
	LOSS [training: 0.24415808611282097 | validation: 0.31440367710199685]
	TIME [epoch: 32.9 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27972539074877767		[learning rate: 0.0011448]
	Learning Rate: 0.00114477
	LOSS [training: 0.27972539074877767 | validation: 0.255400888421608]
	TIME [epoch: 32.9 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2362620087017474		[learning rate: 0.0011421]
	Learning Rate: 0.00114207
	LOSS [training: 0.2362620087017474 | validation: 0.2436810392820125]
	TIME [epoch: 32.9 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2332941748016054		[learning rate: 0.0011394]
	Learning Rate: 0.00113937
	LOSS [training: 0.2332941748016054 | validation: 0.22817150718453552]
	TIME [epoch: 32.9 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2615170470395568		[learning rate: 0.0011367]
	Learning Rate: 0.00113669
	LOSS [training: 0.2615170470395568 | validation: 0.2669987060366285]
	TIME [epoch: 32.8 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23605388402313693		[learning rate: 0.001134]
	Learning Rate: 0.00113401
	LOSS [training: 0.23605388402313693 | validation: 0.2352723542113369]
	TIME [epoch: 32.9 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2804971614309729		[learning rate: 0.0011313]
	Learning Rate: 0.00113133
	LOSS [training: 0.2804971614309729 | validation: 0.2860900603657685]
	TIME [epoch: 32.9 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24528698523973513		[learning rate: 0.0011287]
	Learning Rate: 0.00112866
	LOSS [training: 0.24528698523973513 | validation: 0.24174577708922562]
	TIME [epoch: 32.8 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2375136572975421		[learning rate: 0.001126]
	Learning Rate: 0.001126
	LOSS [training: 0.2375136572975421 | validation: 0.2202471849007018]
	TIME [epoch: 32.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_975.pth
	Model improved!!!
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24242409328110964		[learning rate: 0.0011233]
	Learning Rate: 0.00112334
	LOSS [training: 0.24242409328110964 | validation: 0.2532364562319753]
	TIME [epoch: 32.8 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24540206740623294		[learning rate: 0.0011207]
	Learning Rate: 0.00112069
	LOSS [training: 0.24540206740623294 | validation: 0.23996562029109478]
	TIME [epoch: 32.9 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24404719844673084		[learning rate: 0.0011181]
	Learning Rate: 0.00111805
	LOSS [training: 0.24404719844673084 | validation: 0.2760516767233992]
	TIME [epoch: 33 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27743329477364626		[learning rate: 0.0011154]
	Learning Rate: 0.00111541
	LOSS [training: 0.27743329477364626 | validation: 0.25599923410655695]
	TIME [epoch: 32.9 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2413501362786838		[learning rate: 0.0011128]
	Learning Rate: 0.00111278
	LOSS [training: 0.2413501362786838 | validation: 0.2292024362613009]
	TIME [epoch: 32.9 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23812601664635694		[learning rate: 0.0011102]
	Learning Rate: 0.00111016
	LOSS [training: 0.23812601664635694 | validation: 0.22525295963716202]
	TIME [epoch: 32.8 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24856303762438464		[learning rate: 0.0011075]
	Learning Rate: 0.00110754
	LOSS [training: 0.24856303762438464 | validation: 0.25609742294022253]
	TIME [epoch: 32.9 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2451297968511853		[learning rate: 0.0011049]
	Learning Rate: 0.00110493
	LOSS [training: 0.2451297968511853 | validation: 0.22575488727830947]
	TIME [epoch: 32.9 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23068466847819638		[learning rate: 0.0011023]
	Learning Rate: 0.00110232
	LOSS [training: 0.23068466847819638 | validation: 0.30921324841967307]
	TIME [epoch: 33 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29160351747847124		[learning rate: 0.0010997]
	Learning Rate: 0.00109972
	LOSS [training: 0.29160351747847124 | validation: 0.2561196265872299]
	TIME [epoch: 32.9 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24227860881032678		[learning rate: 0.0010971]
	Learning Rate: 0.00109713
	LOSS [training: 0.24227860881032678 | validation: 0.22479462194196903]
	TIME [epoch: 32.9 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22555401451518267		[learning rate: 0.0010945]
	Learning Rate: 0.00109454
	LOSS [training: 0.22555401451518267 | validation: 0.21589238800969285]
	TIME [epoch: 32.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_987.pth
	Model improved!!!
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22940892855324638		[learning rate: 0.001092]
	Learning Rate: 0.00109196
	LOSS [training: 0.22940892855324638 | validation: 0.2420224867486385]
	TIME [epoch: 32.9 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24456651705397953		[learning rate: 0.0010894]
	Learning Rate: 0.00108938
	LOSS [training: 0.24456651705397953 | validation: 0.2184170708535989]
	TIME [epoch: 33 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2739007064000611		[learning rate: 0.0010868]
	Learning Rate: 0.00108681
	LOSS [training: 0.2739007064000611 | validation: 0.26897826383806417]
	TIME [epoch: 32.9 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26121438858224727		[learning rate: 0.0010842]
	Learning Rate: 0.00108425
	LOSS [training: 0.26121438858224727 | validation: 0.22986271264578134]
	TIME [epoch: 32.9 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22906069728203626		[learning rate: 0.0010817]
	Learning Rate: 0.00108169
	LOSS [training: 0.22906069728203626 | validation: 0.22291488544808252]
	TIME [epoch: 33 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22980550587848447		[learning rate: 0.0010791]
	Learning Rate: 0.00107914
	LOSS [training: 0.22980550587848447 | validation: 0.2666700370210381]
	TIME [epoch: 32.9 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.264040414044238		[learning rate: 0.0010766]
	Learning Rate: 0.00107659
	LOSS [training: 0.264040414044238 | validation: 0.23737038242985053]
	TIME [epoch: 32.9 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23515715846834004		[learning rate: 0.0010741]
	Learning Rate: 0.00107405
	LOSS [training: 0.23515715846834004 | validation: 0.2718096348440091]
	TIME [epoch: 32.9 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24253975233592967		[learning rate: 0.0010715]
	Learning Rate: 0.00107152
	LOSS [training: 0.24253975233592967 | validation: 0.24842779010496496]
	TIME [epoch: 32.8 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23897315560658555		[learning rate: 0.001069]
	Learning Rate: 0.00106899
	LOSS [training: 0.23897315560658555 | validation: 0.24155149948150395]
	TIME [epoch: 32.8 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23395487831238998		[learning rate: 0.0010665]
	Learning Rate: 0.00106647
	LOSS [training: 0.23395487831238998 | validation: 0.2230575810891171]
	TIME [epoch: 32.9 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24038227974500992		[learning rate: 0.001064]
	Learning Rate: 0.00106395
	LOSS [training: 0.24038227974500992 | validation: 0.27980894086838426]
	TIME [epoch: 32.9 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2503460150628128		[learning rate: 0.0010614]
	Learning Rate: 0.00106144
	LOSS [training: 0.2503460150628128 | validation: 0.2996308019238572]
	TIME [epoch: 32.9 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28347194538777537		[learning rate: 0.0010589]
	Learning Rate: 0.00105894
	LOSS [training: 0.28347194538777537 | validation: 0.2400307084274626]
	TIME [epoch: 177 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2298162406776593		[learning rate: 0.0010564]
	Learning Rate: 0.00105644
	LOSS [training: 0.2298162406776593 | validation: 0.22073071863809823]
	TIME [epoch: 70.1 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22675387939826266		[learning rate: 0.001054]
	Learning Rate: 0.00105395
	LOSS [training: 0.22675387939826266 | validation: 0.22883027310968113]
	TIME [epoch: 70 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27815583881767686		[learning rate: 0.0010515]
	Learning Rate: 0.00105147
	LOSS [training: 0.27815583881767686 | validation: 0.25454123045987587]
	TIME [epoch: 70.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2479598616813488		[learning rate: 0.001049]
	Learning Rate: 0.00104898
	LOSS [training: 0.2479598616813488 | validation: 0.22997171726351745]
	TIME [epoch: 70.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2269539178130329		[learning rate: 0.0010465]
	Learning Rate: 0.00104651
	LOSS [training: 0.2269539178130329 | validation: 0.2217064808553713]
	TIME [epoch: 70.5 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24595851775314215		[learning rate: 0.001044]
	Learning Rate: 0.00104404
	LOSS [training: 0.24595851775314215 | validation: 0.2360552896814016]
	TIME [epoch: 70.5 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2605004948298998		[learning rate: 0.0010416]
	Learning Rate: 0.00104158
	LOSS [training: 0.2605004948298998 | validation: 0.2681285042583773]
	TIME [epoch: 70.5 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24206007997489948		[learning rate: 0.0010391]
	Learning Rate: 0.00103912
	LOSS [training: 0.24206007997489948 | validation: 0.2374426429153707]
	TIME [epoch: 70.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22911401419116026		[learning rate: 0.0010367]
	Learning Rate: 0.00103667
	LOSS [training: 0.22911401419116026 | validation: 0.22626343660695844]
	TIME [epoch: 70.6 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23810186903511743		[learning rate: 0.0010342]
	Learning Rate: 0.00103423
	LOSS [training: 0.23810186903511743 | validation: 0.2614307590864443]
	TIME [epoch: 70.3 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2600477051074598		[learning rate: 0.0010318]
	Learning Rate: 0.00103179
	LOSS [training: 0.2600477051074598 | validation: 0.25669764943315304]
	TIME [epoch: 70.2 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2563394850281855		[learning rate: 0.0010294]
	Learning Rate: 0.00102935
	LOSS [training: 0.2563394850281855 | validation: 0.24812606683988653]
	TIME [epoch: 70.2 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24473257529518827		[learning rate: 0.0010269]
	Learning Rate: 0.00102692
	LOSS [training: 0.24473257529518827 | validation: 0.2698179601459882]
	TIME [epoch: 70.1 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25577650431620086		[learning rate: 0.0010245]
	Learning Rate: 0.0010245
	LOSS [training: 0.25577650431620086 | validation: 0.24645961981103492]
	TIME [epoch: 70.1 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23173149771127804		[learning rate: 0.0010221]
	Learning Rate: 0.00102209
	LOSS [training: 0.23173149771127804 | validation: 0.23267201809323518]
	TIME [epoch: 70.1 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2268728299992162		[learning rate: 0.0010197]
	Learning Rate: 0.00101967
	LOSS [training: 0.2268728299992162 | validation: 0.2188093769010792]
	TIME [epoch: 70.1 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25178204265349885		[learning rate: 0.0010173]
	Learning Rate: 0.00101727
	LOSS [training: 0.25178204265349885 | validation: 0.2529283649655385]
	TIME [epoch: 70.1 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23825962726576927		[learning rate: 0.0010149]
	Learning Rate: 0.00101487
	LOSS [training: 0.23825962726576927 | validation: 0.2416336423003283]
	TIME [epoch: 70.2 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2315452392480456		[learning rate: 0.0010125]
	Learning Rate: 0.00101248
	LOSS [training: 0.2315452392480456 | validation: 0.2257015396532101]
	TIME [epoch: 70.1 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2368458492745149		[learning rate: 0.0010101]
	Learning Rate: 0.00101009
	LOSS [training: 0.2368458492745149 | validation: 0.22375038340137612]
	TIME [epoch: 70.3 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.234589512278145		[learning rate: 0.0010077]
	Learning Rate: 0.0010077
	LOSS [training: 0.234589512278145 | validation: 0.23007669171988288]
	TIME [epoch: 70.2 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23616992399128606		[learning rate: 0.0010053]
	Learning Rate: 0.00100533
	LOSS [training: 0.23616992399128606 | validation: 0.32051964119023785]
	TIME [epoch: 70 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2794879795079778		[learning rate: 0.001003]
	Learning Rate: 0.00100296
	LOSS [training: 0.2794879795079778 | validation: 0.2625886040984211]
	TIME [epoch: 69.9 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23730518851492713		[learning rate: 0.0010006]
	Learning Rate: 0.00100059
	LOSS [training: 0.23730518851492713 | validation: 0.22079259588500622]
	TIME [epoch: 70.1 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2775327438211708		[learning rate: 0.00099823]
	Learning Rate: 0.000998231
	LOSS [training: 0.2775327438211708 | validation: 0.26252972214557213]
	TIME [epoch: 70 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24754440963938254		[learning rate: 0.00099588]
	Learning Rate: 0.000995876
	LOSS [training: 0.24754440963938254 | validation: 0.24139964185207557]
	TIME [epoch: 70.1 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2284395472256332		[learning rate: 0.00099353]
	Learning Rate: 0.000993527
	LOSS [training: 0.2284395472256332 | validation: 0.22370479299261464]
	TIME [epoch: 70 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22919348702823522		[learning rate: 0.00099118]
	Learning Rate: 0.000991183
	LOSS [training: 0.22919348702823522 | validation: 0.2197772942151136]
	TIME [epoch: 70.1 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23184131765281296		[learning rate: 0.00098885]
	Learning Rate: 0.000988845
	LOSS [training: 0.23184131765281296 | validation: 0.2198478915579778]
	TIME [epoch: 70 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2444107945855987		[learning rate: 0.00098651]
	Learning Rate: 0.000986513
	LOSS [training: 0.2444107945855987 | validation: 0.2818974780161198]
	TIME [epoch: 70 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2627644539147049		[learning rate: 0.00098419]
	Learning Rate: 0.000984185
	LOSS [training: 0.2627644539147049 | validation: 0.28004043428317527]
	TIME [epoch: 70.1 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2538219729151522		[learning rate: 0.00098186]
	Learning Rate: 0.000981864
	LOSS [training: 0.2538219729151522 | validation: 0.23039387134978556]
	TIME [epoch: 70.1 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22592309317017667		[learning rate: 0.00097955]
	Learning Rate: 0.000979548
	LOSS [training: 0.22592309317017667 | validation: 0.2418409765119901]
	TIME [epoch: 70 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25365289027726245		[learning rate: 0.00097724]
	Learning Rate: 0.000977237
	LOSS [training: 0.25365289027726245 | validation: 0.23770920367112888]
	TIME [epoch: 70 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2421484948536135		[learning rate: 0.00097493]
	Learning Rate: 0.000974932
	LOSS [training: 0.2421484948536135 | validation: 0.24845180692241053]
	TIME [epoch: 70 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23693598904803628		[learning rate: 0.00097263]
	Learning Rate: 0.000972632
	LOSS [training: 0.23693598904803628 | validation: 0.2422390450338497]
	TIME [epoch: 70 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22881036272881866		[learning rate: 0.00097034]
	Learning Rate: 0.000970338
	LOSS [training: 0.22881036272881866 | validation: 0.22461699359976128]
	TIME [epoch: 70.1 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24835735608891032		[learning rate: 0.00096805]
	Learning Rate: 0.000968049
	LOSS [training: 0.24835735608891032 | validation: 0.2356263614820912]
	TIME [epoch: 70.1 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23689536852045615		[learning rate: 0.00096577]
	Learning Rate: 0.000965766
	LOSS [training: 0.23689536852045615 | validation: 0.2363393742192253]
	TIME [epoch: 70 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23225194033522506		[learning rate: 0.00096349]
	Learning Rate: 0.000963488
	LOSS [training: 0.23225194033522506 | validation: 0.24573107339643774]
	TIME [epoch: 70 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23754407740714611		[learning rate: 0.00096121]
	Learning Rate: 0.000961215
	LOSS [training: 0.23754407740714611 | validation: 0.24207975728934572]
	TIME [epoch: 70.2 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2579239486074038		[learning rate: 0.00095895]
	Learning Rate: 0.000958948
	LOSS [training: 0.2579239486074038 | validation: 0.22994019693130968]
	TIME [epoch: 69.9 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23204311872729513		[learning rate: 0.00095669]
	Learning Rate: 0.000956686
	LOSS [training: 0.23204311872729513 | validation: 0.2252967923342544]
	TIME [epoch: 70.1 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2255153849356524		[learning rate: 0.00095443]
	Learning Rate: 0.000954429
	LOSS [training: 0.2255153849356524 | validation: 0.22423133386743854]
	TIME [epoch: 70.1 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23755987050788874		[learning rate: 0.00095218]
	Learning Rate: 0.000952178
	LOSS [training: 0.23755987050788874 | validation: 0.2526225762942913]
	TIME [epoch: 70.1 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23089771252823765		[learning rate: 0.00094993]
	Learning Rate: 0.000949932
	LOSS [training: 0.23089771252823765 | validation: 0.22319951895396994]
	TIME [epoch: 70 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24111412826385767		[learning rate: 0.00094769]
	Learning Rate: 0.000947691
	LOSS [training: 0.24111412826385767 | validation: 0.23676181762569784]
	TIME [epoch: 70 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2427720477790258		[learning rate: 0.00094546]
	Learning Rate: 0.000945455
	LOSS [training: 0.2427720477790258 | validation: 0.23176385465776544]
	TIME [epoch: 70 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23259405028749708		[learning rate: 0.00094323]
	Learning Rate: 0.000943225
	LOSS [training: 0.23259405028749708 | validation: 0.22280511583099796]
	TIME [epoch: 70.1 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24067697229343482		[learning rate: 0.000941]
	Learning Rate: 0.000941
	LOSS [training: 0.24067697229343482 | validation: 0.22614494967587634]
	TIME [epoch: 70 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22324182972703355		[learning rate: 0.00093878]
	Learning Rate: 0.000938781
	LOSS [training: 0.22324182972703355 | validation: 0.23003364708583474]
	TIME [epoch: 70.1 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22888170278262254		[learning rate: 0.00093657]
	Learning Rate: 0.000936566
	LOSS [training: 0.22888170278262254 | validation: 0.23097892165810152]
	TIME [epoch: 70 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24826386049400656		[learning rate: 0.00093436]
	Learning Rate: 0.000934357
	LOSS [training: 0.24826386049400656 | validation: 0.23787194279794255]
	TIME [epoch: 70 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22812117429452808		[learning rate: 0.00093215]
	Learning Rate: 0.000932153
	LOSS [training: 0.22812117429452808 | validation: 0.23236588008442335]
	TIME [epoch: 70.1 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24284552951260108		[learning rate: 0.00092995]
	Learning Rate: 0.000929954
	LOSS [training: 0.24284552951260108 | validation: 0.24232656803171396]
	TIME [epoch: 70 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24716814720991256		[learning rate: 0.00092776]
	Learning Rate: 0.000927761
	LOSS [training: 0.24716814720991256 | validation: 0.2359593224130843]
	TIME [epoch: 70.1 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.239234988809854		[learning rate: 0.00092557]
	Learning Rate: 0.000925572
	LOSS [training: 0.239234988809854 | validation: 0.24346014033325492]
	TIME [epoch: 70 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2294760610290219		[learning rate: 0.00092339]
	Learning Rate: 0.000923389
	LOSS [training: 0.2294760610290219 | validation: 0.21554517910959942]
	TIME [epoch: 70.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_1059.pth
	Model improved!!!
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23108819715296097		[learning rate: 0.00092121]
	Learning Rate: 0.000921211
	LOSS [training: 0.23108819715296097 | validation: 0.22451193356087354]
	TIME [epoch: 70 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24013866935244074		[learning rate: 0.00091904]
	Learning Rate: 0.000919038
	LOSS [training: 0.24013866935244074 | validation: 0.2297860984261626]
	TIME [epoch: 70.1 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23482890381678723		[learning rate: 0.00091687]
	Learning Rate: 0.00091687
	LOSS [training: 0.23482890381678723 | validation: 0.23017955872953588]
	TIME [epoch: 70.1 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24370142053812477		[learning rate: 0.00091471]
	Learning Rate: 0.000914707
	LOSS [training: 0.24370142053812477 | validation: 0.2786842605115393]
	TIME [epoch: 70 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2540606636567579		[learning rate: 0.00091255]
	Learning Rate: 0.00091255
	LOSS [training: 0.2540606636567579 | validation: 0.22896774886687918]
	TIME [epoch: 70.2 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22393126871527802		[learning rate: 0.0009104]
	Learning Rate: 0.000910397
	LOSS [training: 0.22393126871527802 | validation: 0.23943757240036084]
	TIME [epoch: 70.3 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23913571398716038		[learning rate: 0.00090825]
	Learning Rate: 0.00090825
	LOSS [training: 0.23913571398716038 | validation: 0.22483651952477468]
	TIME [epoch: 70.5 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22761360131185565		[learning rate: 0.00090611]
	Learning Rate: 0.000906107
	LOSS [training: 0.22761360131185565 | validation: 0.21893902152741668]
	TIME [epoch: 70.3 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23142554812210692		[learning rate: 0.00090397]
	Learning Rate: 0.00090397
	LOSS [training: 0.23142554812210692 | validation: 0.25203879380710986]
	TIME [epoch: 70.3 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2462447541807702		[learning rate: 0.00090184]
	Learning Rate: 0.000901837
	LOSS [training: 0.2462447541807702 | validation: 0.24363923514907712]
	TIME [epoch: 70.5 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24104210772502263		[learning rate: 0.00089971]
	Learning Rate: 0.00089971
	LOSS [training: 0.24104210772502263 | validation: 0.25114030049861114]
	TIME [epoch: 70.2 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2527412940303384		[learning rate: 0.00089759]
	Learning Rate: 0.000897588
	LOSS [training: 0.2527412940303384 | validation: 0.230690767963926]
	TIME [epoch: 70.5 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22318515951289147		[learning rate: 0.00089547]
	Learning Rate: 0.00089547
	LOSS [training: 0.22318515951289147 | validation: 0.22348540762677188]
	TIME [epoch: 70.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2278306325299289		[learning rate: 0.00089336]
	Learning Rate: 0.000893358
	LOSS [training: 0.2278306325299289 | validation: 0.21775961802915347]
	TIME [epoch: 70.3 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24009894512240915		[learning rate: 0.00089125]
	Learning Rate: 0.000891251
	LOSS [training: 0.24009894512240915 | validation: 0.24531517219835497]
	TIME [epoch: 70.6 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2634102261789615		[learning rate: 0.00088915]
	Learning Rate: 0.000889149
	LOSS [training: 0.2634102261789615 | validation: 0.24054299353270564]
	TIME [epoch: 70.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2352411572341463		[learning rate: 0.00088705]
	Learning Rate: 0.000887051
	LOSS [training: 0.2352411572341463 | validation: 0.23162161588721175]
	TIME [epoch: 70.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2236640911530179		[learning rate: 0.00088496]
	Learning Rate: 0.000884959
	LOSS [training: 0.2236640911530179 | validation: 0.2101779281581055]
	TIME [epoch: 70.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_1077.pth
	Model improved!!!
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23047622677863178		[learning rate: 0.00088287]
	Learning Rate: 0.000882871
	LOSS [training: 0.23047622677863178 | validation: 0.2882458295640594]
	TIME [epoch: 70.4 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26931860747528863		[learning rate: 0.00088079]
	Learning Rate: 0.000880789
	LOSS [training: 0.26931860747528863 | validation: 0.28168794318743406]
	TIME [epoch: 70.3 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24811692133182042		[learning rate: 0.00087871]
	Learning Rate: 0.000878711
	LOSS [training: 0.24811692133182042 | validation: 0.22741120291240435]
	TIME [epoch: 70.4 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22566829520689502		[learning rate: 0.00087664]
	Learning Rate: 0.000876639
	LOSS [training: 0.22566829520689502 | validation: 0.2179077866037109]
	TIME [epoch: 70.2 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23852045052241633		[learning rate: 0.00087457]
	Learning Rate: 0.000874571
	LOSS [training: 0.23852045052241633 | validation: 0.22205632031764805]
	TIME [epoch: 70.4 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2444982813171031		[learning rate: 0.00087251]
	Learning Rate: 0.000872508
	LOSS [training: 0.2444982813171031 | validation: 0.22745111351408503]
	TIME [epoch: 70.5 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23238748735223205		[learning rate: 0.00087045]
	Learning Rate: 0.00087045
	LOSS [training: 0.23238748735223205 | validation: 0.22423607513807348]
	TIME [epoch: 70.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22999555461957413		[learning rate: 0.0008684]
	Learning Rate: 0.000868396
	LOSS [training: 0.22999555461957413 | validation: 0.2493109387007154]
	TIME [epoch: 70.5 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25178501053224184		[learning rate: 0.00086635]
	Learning Rate: 0.000866348
	LOSS [training: 0.25178501053224184 | validation: 0.28353743573898316]
	TIME [epoch: 70.4 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24746167376617104		[learning rate: 0.0008643]
	Learning Rate: 0.000864304
	LOSS [training: 0.24746167376617104 | validation: 0.23421396193327698]
	TIME [epoch: 70.2 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22698263474586586		[learning rate: 0.00086227]
	Learning Rate: 0.000862266
	LOSS [training: 0.22698263474586586 | validation: 0.22341841978988897]
	TIME [epoch: 70.4 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2313203754759558		[learning rate: 0.00086023]
	Learning Rate: 0.000860232
	LOSS [training: 0.2313203754759558 | validation: 0.2274941962487307]
	TIME [epoch: 70.2 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22750318268429645		[learning rate: 0.0008582]
	Learning Rate: 0.000858202
	LOSS [training: 0.22750318268429645 | validation: 0.29770213638973186]
	TIME [epoch: 70.4 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27113086076774534		[learning rate: 0.00085618]
	Learning Rate: 0.000856178
	LOSS [training: 0.27113086076774534 | validation: 0.23647536018991383]
	TIME [epoch: 70.2 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22949321705782938		[learning rate: 0.00085416]
	Learning Rate: 0.000854159
	LOSS [training: 0.22949321705782938 | validation: 0.22908433956682622]
	TIME [epoch: 70.6 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2308791358545125		[learning rate: 0.00085214]
	Learning Rate: 0.000852144
	LOSS [training: 0.2308791358545125 | validation: 0.24732476638329562]
	TIME [epoch: 70.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22987773353571195		[learning rate: 0.00085013]
	Learning Rate: 0.000850134
	LOSS [training: 0.22987773353571195 | validation: 0.22604599246620372]
	TIME [epoch: 70.5 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2282977594754869		[learning rate: 0.00084813]
	Learning Rate: 0.000848128
	LOSS [training: 0.2282977594754869 | validation: 0.2562303616912103]
	TIME [epoch: 70.4 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25230530792677974		[learning rate: 0.00084613]
	Learning Rate: 0.000846128
	LOSS [training: 0.25230530792677974 | validation: 0.22978476780466783]
	TIME [epoch: 70.4 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2287082060967009		[learning rate: 0.00084413]
	Learning Rate: 0.000844132
	LOSS [training: 0.2287082060967009 | validation: 0.2259134904303276]
	TIME [epoch: 70.5 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2280314758714886		[learning rate: 0.00084214]
	Learning Rate: 0.000842141
	LOSS [training: 0.2280314758714886 | validation: 0.23968184537842008]
	TIME [epoch: 70.4 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24534186873414726		[learning rate: 0.00084015]
	Learning Rate: 0.000840154
	LOSS [training: 0.24534186873414726 | validation: 0.22260413975703125]
	TIME [epoch: 70.3 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22721500827909452		[learning rate: 0.00083817]
	Learning Rate: 0.000838173
	LOSS [training: 0.22721500827909452 | validation: 0.21518885970714688]
	TIME [epoch: 70.4 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23642143261436893		[learning rate: 0.0008362]
	Learning Rate: 0.000836195
	LOSS [training: 0.23642143261436893 | validation: 0.24102018381202]
	TIME [epoch: 70.4 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26501156944985405		[learning rate: 0.00083422]
	Learning Rate: 0.000834223
	LOSS [training: 0.26501156944985405 | validation: 0.23733832529619986]
	TIME [epoch: 70.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23216120092442746		[learning rate: 0.00083226]
	Learning Rate: 0.000832255
	LOSS [training: 0.23216120092442746 | validation: 0.22702702913973233]
	TIME [epoch: 70.6 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.221866276738525		[learning rate: 0.00083029]
	Learning Rate: 0.000830292
	LOSS [training: 0.221866276738525 | validation: 0.21800627456553395]
	TIME [epoch: 70.4 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2308218405425565		[learning rate: 0.00082833]
	Learning Rate: 0.000828333
	LOSS [training: 0.2308218405425565 | validation: 0.2653749932304362]
	TIME [epoch: 70.2 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2431886388780915		[learning rate: 0.00082638]
	Learning Rate: 0.00082638
	LOSS [training: 0.2431886388780915 | validation: 0.22790412802472848]
	TIME [epoch: 70.5 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2342474746299928		[learning rate: 0.00082443]
	Learning Rate: 0.00082443
	LOSS [training: 0.2342474746299928 | validation: 0.23244567723901677]
	TIME [epoch: 70.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.232305402415521		[learning rate: 0.00082249]
	Learning Rate: 0.000822485
	LOSS [training: 0.232305402415521 | validation: 0.22905120982189486]
	TIME [epoch: 70.4 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23327257889758346		[learning rate: 0.00082055]
	Learning Rate: 0.000820545
	LOSS [training: 0.23327257889758346 | validation: 0.2164728347209437]
	TIME [epoch: 70.5 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2322494953370925		[learning rate: 0.00081861]
	Learning Rate: 0.00081861
	LOSS [training: 0.2322494953370925 | validation: 0.2242937152529162]
	TIME [epoch: 70.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23549428481930748		[learning rate: 0.00081668]
	Learning Rate: 0.000816679
	LOSS [training: 0.23549428481930748 | validation: 0.228752200174547]
	TIME [epoch: 70.3 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2336183592153226		[learning rate: 0.00081475]
	Learning Rate: 0.000814752
	LOSS [training: 0.2336183592153226 | validation: 0.226554878141024]
	TIME [epoch: 70.5 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22929138363608778		[learning rate: 0.00081283]
	Learning Rate: 0.000812831
	LOSS [training: 0.22929138363608778 | validation: 0.2338107315807214]
	TIME [epoch: 70.5 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2378646672942576		[learning rate: 0.00081091]
	Learning Rate: 0.000810913
	LOSS [training: 0.2378646672942576 | validation: 0.25412352545571815]
	TIME [epoch: 70.4 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24092456727380995		[learning rate: 0.000809]
	Learning Rate: 0.000809
	LOSS [training: 0.24092456727380995 | validation: 0.2531172941121243]
	TIME [epoch: 70.3 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23098406995089832		[learning rate: 0.00080709]
	Learning Rate: 0.000807092
	LOSS [training: 0.23098406995089832 | validation: 0.22979606902944388]
	TIME [epoch: 70.6 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2461386511874698		[learning rate: 0.00080519]
	Learning Rate: 0.000805188
	LOSS [training: 0.2461386511874698 | validation: 0.26135616669818196]
	TIME [epoch: 70.3 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24457781409081708		[learning rate: 0.00080329]
	Learning Rate: 0.000803289
	LOSS [training: 0.24457781409081708 | validation: 0.23919312743510446]
	TIME [epoch: 70.4 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22987320593683697		[learning rate: 0.00080139]
	Learning Rate: 0.000801394
	LOSS [training: 0.22987320593683697 | validation: 0.224758994579213]
	TIME [epoch: 70.5 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22315969488191617		[learning rate: 0.0007995]
	Learning Rate: 0.000799504
	LOSS [training: 0.22315969488191617 | validation: 0.22936295782473023]
	TIME [epoch: 70.2 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22987063288392998		[learning rate: 0.00079762]
	Learning Rate: 0.000797618
	LOSS [training: 0.22987063288392998 | validation: 0.25051791154130765]
	TIME [epoch: 69.9 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2311355260616107		[learning rate: 0.00079574]
	Learning Rate: 0.000795736
	LOSS [training: 0.2311355260616107 | validation: 0.22222643473628645]
	TIME [epoch: 69.9 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23432574292758315		[learning rate: 0.00079386]
	Learning Rate: 0.000793859
	LOSS [training: 0.23432574292758315 | validation: 0.24135064643760334]
	TIME [epoch: 70 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2313582355464308		[learning rate: 0.00079199]
	Learning Rate: 0.000791987
	LOSS [training: 0.2313582355464308 | validation: 0.23227944166668935]
	TIME [epoch: 70.1 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22503461934768867		[learning rate: 0.00079012]
	Learning Rate: 0.000790119
	LOSS [training: 0.22503461934768867 | validation: 0.21873113470597919]
	TIME [epoch: 70.1 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24031186023385967		[learning rate: 0.00078826]
	Learning Rate: 0.000788255
	LOSS [training: 0.24031186023385967 | validation: 0.22234236286470813]
	TIME [epoch: 70.1 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22476102424703084		[learning rate: 0.0007864]
	Learning Rate: 0.000786396
	LOSS [training: 0.22476102424703084 | validation: 0.231036180871308]
	TIME [epoch: 70.1 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22996995356375505		[learning rate: 0.00078454]
	Learning Rate: 0.000784541
	LOSS [training: 0.22996995356375505 | validation: 0.22276823841781895]
	TIME [epoch: 70.1 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23556926900620653		[learning rate: 0.00078269]
	Learning Rate: 0.00078269
	LOSS [training: 0.23556926900620653 | validation: 0.23988223198841335]
	TIME [epoch: 70 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2368748491396507		[learning rate: 0.00078084]
	Learning Rate: 0.000780844
	LOSS [training: 0.2368748491396507 | validation: 0.22722549453671867]
	TIME [epoch: 69.9 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22594264151705828		[learning rate: 0.000779]
	Learning Rate: 0.000779002
	LOSS [training: 0.22594264151705828 | validation: 0.22616272398949014]
	TIME [epoch: 70 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22188123778120666		[learning rate: 0.00077716]
	Learning Rate: 0.000777164
	LOSS [training: 0.22188123778120666 | validation: 0.21694233538046978]
	TIME [epoch: 70 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2538965182689947		[learning rate: 0.00077533]
	Learning Rate: 0.000775331
	LOSS [training: 0.2538965182689947 | validation: 0.2697953160483268]
	TIME [epoch: 70.2 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25105379349167156		[learning rate: 0.0007735]
	Learning Rate: 0.000773502
	LOSS [training: 0.25105379349167156 | validation: 0.2326113337208063]
	TIME [epoch: 70.5 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2298021800547143		[learning rate: 0.00077168]
	Learning Rate: 0.000771678
	LOSS [training: 0.2298021800547143 | validation: 0.22401827374336358]
	TIME [epoch: 70.6 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22346985133456795		[learning rate: 0.00076986]
	Learning Rate: 0.000769857
	LOSS [training: 0.22346985133456795 | validation: 0.22059672375099043]
	TIME [epoch: 69.8 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23676001551591436		[learning rate: 0.00076804]
	Learning Rate: 0.000768041
	LOSS [training: 0.23676001551591436 | validation: 0.25449888562183554]
	TIME [epoch: 70.1 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24380261916396428		[learning rate: 0.00076623]
	Learning Rate: 0.00076623
	LOSS [training: 0.24380261916396428 | validation: 0.23867505135639072]
	TIME [epoch: 70.2 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23124347291458866		[learning rate: 0.00076442]
	Learning Rate: 0.000764422
	LOSS [training: 0.23124347291458866 | validation: 0.2249280299696006]
	TIME [epoch: 70.5 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21990134882976706		[learning rate: 0.00076262]
	Learning Rate: 0.000762619
	LOSS [training: 0.21990134882976706 | validation: 0.22213856923414604]
	TIME [epoch: 70.4 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22465216686699568		[learning rate: 0.00076082]
	Learning Rate: 0.00076082
	LOSS [training: 0.22465216686699568 | validation: 0.23387279765371463]
	TIME [epoch: 70.4 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23778710813993564		[learning rate: 0.00075903]
	Learning Rate: 0.000759026
	LOSS [training: 0.23778710813993564 | validation: 0.22972343470298484]
	TIME [epoch: 70.3 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23840462681935087		[learning rate: 0.00075724]
	Learning Rate: 0.000757235
	LOSS [training: 0.23840462681935087 | validation: 0.22636943244881363]
	TIME [epoch: 70.5 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2298870675787532		[learning rate: 0.00075545]
	Learning Rate: 0.000755449
	LOSS [training: 0.2298870675787532 | validation: 0.2183177714844699]
	TIME [epoch: 70.4 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23205480234608156		[learning rate: 0.00075367]
	Learning Rate: 0.000753667
	LOSS [training: 0.23205480234608156 | validation: 0.22105515463474273]
	TIME [epoch: 70.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22812844022385256		[learning rate: 0.00075189]
	Learning Rate: 0.000751889
	LOSS [training: 0.22812844022385256 | validation: 0.2492477674523378]
	TIME [epoch: 70.5 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2352254879372867		[learning rate: 0.00075012]
	Learning Rate: 0.000750116
	LOSS [training: 0.2352254879372867 | validation: 0.21877605826152857]
	TIME [epoch: 70.7 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22430733518968765		[learning rate: 0.00074835]
	Learning Rate: 0.000748346
	LOSS [training: 0.22430733518968765 | validation: 0.23480839459451153]
	TIME [epoch: 70.6 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23382212210085856		[learning rate: 0.00074658]
	Learning Rate: 0.000746581
	LOSS [training: 0.23382212210085856 | validation: 0.21820157152367814]
	TIME [epoch: 70.7 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22522798827111504		[learning rate: 0.00074482]
	Learning Rate: 0.00074482
	LOSS [training: 0.22522798827111504 | validation: 0.23617002110591612]
	TIME [epoch: 70.6 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23655604806972877		[learning rate: 0.00074306]
	Learning Rate: 0.000743063
	LOSS [training: 0.23655604806972877 | validation: 0.21980998964090032]
	TIME [epoch: 70.7 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22657522753390497		[learning rate: 0.00074131]
	Learning Rate: 0.00074131
	LOSS [training: 0.22657522753390497 | validation: 0.24807741351224402]
	TIME [epoch: 70.5 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2316918882312894		[learning rate: 0.00073956]
	Learning Rate: 0.000739562
	LOSS [training: 0.2316918882312894 | validation: 0.2242043728403217]
	TIME [epoch: 70.6 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22280770380950027		[learning rate: 0.00073782]
	Learning Rate: 0.000737817
	LOSS [training: 0.22280770380950027 | validation: 0.2161842420858961]
	TIME [epoch: 70.6 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2324453225225378		[learning rate: 0.00073608]
	Learning Rate: 0.000736077
	LOSS [training: 0.2324453225225378 | validation: 0.2168825113821283]
	TIME [epoch: 70.6 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2231314126817635		[learning rate: 0.00073434]
	Learning Rate: 0.000734341
	LOSS [training: 0.2231314126817635 | validation: 0.2216225729139127]
	TIME [epoch: 70.7 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22575825670824434		[learning rate: 0.00073261]
	Learning Rate: 0.000732608
	LOSS [training: 0.22575825670824434 | validation: 0.23099155016967443]
	TIME [epoch: 70.6 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22555785261495212		[learning rate: 0.00073088]
	Learning Rate: 0.00073088
	LOSS [training: 0.22555785261495212 | validation: 0.22580856922012316]
	TIME [epoch: 70.6 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2231487788022991		[learning rate: 0.00072916]
	Learning Rate: 0.000729156
	LOSS [training: 0.2231487788022991 | validation: 0.23073827408997227]
	TIME [epoch: 70.6 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2602996889284087		[learning rate: 0.00072744]
	Learning Rate: 0.000727436
	LOSS [training: 0.2602996889284087 | validation: 0.24945965627932484]
	TIME [epoch: 70.5 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22972712827588332		[learning rate: 0.00072572]
	Learning Rate: 0.00072572
	LOSS [training: 0.22972712827588332 | validation: 0.23449592617133092]
	TIME [epoch: 70.6 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22537551779856566		[learning rate: 0.00072401]
	Learning Rate: 0.000724008
	LOSS [training: 0.22537551779856566 | validation: 0.22640539432866807]
	TIME [epoch: 70.5 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22327506268103464		[learning rate: 0.0007223]
	Learning Rate: 0.000722301
	LOSS [training: 0.22327506268103464 | validation: 0.2251669139255998]
	TIME [epoch: 70.5 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2461098664166549		[learning rate: 0.0007206]
	Learning Rate: 0.000720597
	LOSS [training: 0.2461098664166549 | validation: 0.2395898214932921]
	TIME [epoch: 70.6 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22326816397098068		[learning rate: 0.0007189]
	Learning Rate: 0.000718897
	LOSS [training: 0.22326816397098068 | validation: 0.21124468263668195]
	TIME [epoch: 70.5 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22322435161705426		[learning rate: 0.0007172]
	Learning Rate: 0.000717201
	LOSS [training: 0.22322435161705426 | validation: 0.2581646264102576]
	TIME [epoch: 70.5 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.249827790483326		[learning rate: 0.00071551]
	Learning Rate: 0.00071551
	LOSS [training: 0.249827790483326 | validation: 0.24538259522042516]
	TIME [epoch: 70.6 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23009391955918196		[learning rate: 0.00071382]
	Learning Rate: 0.000713822
	LOSS [training: 0.23009391955918196 | validation: 0.22218220477529693]
	TIME [epoch: 70.6 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2229592483844193		[learning rate: 0.00071214]
	Learning Rate: 0.000712138
	LOSS [training: 0.2229592483844193 | validation: 0.21678761310227507]
	TIME [epoch: 70.5 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22087133105418494		[learning rate: 0.00071046]
	Learning Rate: 0.000710458
	LOSS [training: 0.22087133105418494 | validation: 0.24280648505536273]
	TIME [epoch: 70.6 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23010614269830998		[learning rate: 0.00070878]
	Learning Rate: 0.000708782
	LOSS [training: 0.23010614269830998 | validation: 0.24821707075334107]
	TIME [epoch: 70.5 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24153970128750957		[learning rate: 0.00070711]
	Learning Rate: 0.00070711
	LOSS [training: 0.24153970128750957 | validation: 0.2395842912105808]
	TIME [epoch: 70.5 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24368740498897073		[learning rate: 0.00070544]
	Learning Rate: 0.000705443
	LOSS [training: 0.24368740498897073 | validation: 0.24585347207214003]
	TIME [epoch: 70.6 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22655727306993842		[learning rate: 0.00070378]
	Learning Rate: 0.000703778
	LOSS [training: 0.22655727306993842 | validation: 0.23689138145212993]
	TIME [epoch: 70.6 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22508783727702647		[learning rate: 0.00070212]
	Learning Rate: 0.000702118
	LOSS [training: 0.22508783727702647 | validation: 0.21401748473273496]
	TIME [epoch: 70.5 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23396230001577764		[learning rate: 0.00070046]
	Learning Rate: 0.000700462
	LOSS [training: 0.23396230001577764 | validation: 0.2584827694001903]
	TIME [epoch: 70.6 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24109210144396195		[learning rate: 0.00069881]
	Learning Rate: 0.00069881
	LOSS [training: 0.24109210144396195 | validation: 0.23367594340127332]
	TIME [epoch: 70.6 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22449457316801516		[learning rate: 0.00069716]
	Learning Rate: 0.000697161
	LOSS [training: 0.22449457316801516 | validation: 0.21654227035381746]
	TIME [epoch: 70.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd2_20240713_103448/states/model_phiq_1a_v_mmd2_1178.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 35537.616 seconds.
