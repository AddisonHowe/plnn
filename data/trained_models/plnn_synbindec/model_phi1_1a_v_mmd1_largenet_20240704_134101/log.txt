Args:
Namespace(name='model_phi1_1a_v_mmd1_largenet', outdir='out/model_training/model_phi1_1a_v_mmd1_largenet', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2778914654

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.225292175763059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.225292175763059 | validation: 4.900389408120334]
	TIME [epoch: 116 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.697997915156911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.697997915156911 | validation: 4.733110181194978]
	TIME [epoch: 9.06 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.419067973003433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.419067973003433 | validation: 4.497095848620772]
	TIME [epoch: 8.96 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.36356182817636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.36356182817636 | validation: 4.469491817238761]
	TIME [epoch: 9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.211356410150793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.211356410150793 | validation: 4.488399634342688]
	TIME [epoch: 9 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.117707499500784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.117707499500784 | validation: 4.226443553622129]
	TIME [epoch: 9.05 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.851661011514254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.851661011514254 | validation: 4.059148986718949]
	TIME [epoch: 8.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.724975821012289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.724975821012289 | validation: 3.76242743409722]
	TIME [epoch: 9.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.614567745343897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.614567745343897 | validation: 3.829782505221224]
	TIME [epoch: 9.06 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5518191318514383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5518191318514383 | validation: 3.5195242227322128]
	TIME [epoch: 9.05 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1544956395066435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1544956395066435 | validation: 2.9560319621329008]
	TIME [epoch: 9.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8751889125079035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8751889125079035 | validation: 2.9630311130413025]
	TIME [epoch: 9.01 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.670752478009073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.670752478009073 | validation: 2.755847697991115]
	TIME [epoch: 9.06 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.589405456464417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.589405456464417 | validation: 2.439453731806216]
	TIME [epoch: 9.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.399490526374268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.399490526374268 | validation: 2.336380023814008]
	TIME [epoch: 9.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.278691148725076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.278691148725076 | validation: 2.3054444387868562]
	TIME [epoch: 8.98 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2624771659902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2624771659902 | validation: 2.179281228812461]
	TIME [epoch: 9.04 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1743387764847606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1743387764847606 | validation: 2.0371692617859076]
	TIME [epoch: 9.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.042859835596098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.042859835596098 | validation: 2.4778372427849167]
	TIME [epoch: 9 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.136820731005311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.136820731005311 | validation: 1.966217387875885]
	TIME [epoch: 9.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0540470393255332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0540470393255332 | validation: 1.954484937514053]
	TIME [epoch: 9.07 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.417343309367025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.417343309367025 | validation: 2.7039707126600456]
	TIME [epoch: 9 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2805546038748945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2805546038748945 | validation: 2.0428361815676244]
	TIME [epoch: 8.99 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9245751882360018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9245751882360018 | validation: 1.9268842376126933]
	TIME [epoch: 9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8023263990507234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8023263990507234 | validation: 2.493741893350856]
	TIME [epoch: 9.06 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0658835590247038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0658835590247038 | validation: 1.830313662250021]
	TIME [epoch: 8.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7017296270819031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7017296270819031 | validation: 2.839888901957383]
	TIME [epoch: 8.98 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0438033977430665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0438033977430665 | validation: 1.6691358668617433]
	TIME [epoch: 8.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.872835895694465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.872835895694465 | validation: 1.767896013927254]
	TIME [epoch: 9.04 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.620712449171773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.620712449171773 | validation: 2.3638878031356594]
	TIME [epoch: 8.99 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9457880222777815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9457880222777815 | validation: 1.7512658193520123]
	TIME [epoch: 8.99 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5526978356021688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5526978356021688 | validation: 1.664485635263202]
	TIME [epoch: 8.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3371174266132675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3371174266132675 | validation: 1.8832793584701506]
	TIME [epoch: 9.03 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7350717898227184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7350717898227184 | validation: 1.611352260555301]
	TIME [epoch: 9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.917777508526544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.917777508526544 | validation: 2.0291308221007016]
	TIME [epoch: 8.98 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7322198087622318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7322198087622318 | validation: 1.4995796470923035]
	TIME [epoch: 8.98 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8310611584749181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8310611584749181 | validation: 2.1602433068399827]
	TIME [epoch: 9.05 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7678986998235664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7678986998235664 | validation: 1.4885164152889883]
	TIME [epoch: 9.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.540341183269013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.540341183269013 | validation: 2.1524020888241946]
	TIME [epoch: 8.99 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.728481228968854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.728481228968854 | validation: 1.8847710428339703]
	TIME [epoch: 9 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6311268647390562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6311268647390562 | validation: 1.2874796909223365]
	TIME [epoch: 9.05 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6288950730542253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6288950730542253 | validation: 1.3218577189393061]
	TIME [epoch: 8.99 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.210741950023285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.210741950023285 | validation: 2.594331731584184]
	TIME [epoch: 9.01 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.895583706919417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.895583706919417 | validation: 1.237648127554702]
	TIME [epoch: 8.98 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4585405907748932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4585405907748932 | validation: 1.1504948077172987]
	TIME [epoch: 9.04 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0870866804774335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0870866804774335 | validation: 2.265012545968814]
	TIME [epoch: 8.99 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4228183873845726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4228183873845726 | validation: 1.0451142683360424]
	TIME [epoch: 8.98 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3114013300064908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3114013300064908 | validation: 1.0698505116320896]
	TIME [epoch: 8.99 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4113442926589206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4113442926589206 | validation: 1.1877072507039164]
	TIME [epoch: 9.03 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.341632521936175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.341632521936175 | validation: 1.0398902740725702]
	TIME [epoch: 8.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1516133480249402		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.1516133480249402 | validation: 0.9914036143127543]
	TIME [epoch: 8.98 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1414621138000467		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.1414621138000467 | validation: 1.042480944906722]
	TIME [epoch: 8.99 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1798419398583841		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.1798419398583841 | validation: 0.9735926116723961]
	TIME [epoch: 9.05 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.082760072876673		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.082760072876673 | validation: 0.9891987189550826]
	TIME [epoch: 9 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1179980322873118		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.1179980322873118 | validation: 1.0013742170048823]
	TIME [epoch: 9 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8385978591400483		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.8385978591400483 | validation: 0.9073616610707739]
	TIME [epoch: 9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0702553351682114		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.0702553351682114 | validation: 0.8087372952611016]
	TIME [epoch: 9.05 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9396217578904216		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.9396217578904216 | validation: 0.9121468717394785]
	TIME [epoch: 9 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.020198983038064		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.020198983038064 | validation: 0.7887957663184542]
	TIME [epoch: 9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9132893753733883		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.9132893753733883 | validation: 0.8372310036352413]
	TIME [epoch: 9.01 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8693761546567541		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.8693761546567541 | validation: 0.7059314057569113]
	TIME [epoch: 9.06 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6122049722932877		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.6122049722932877 | validation: 0.654113480460689]
	TIME [epoch: 9.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0438921304750362		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 1.0438921304750362 | validation: 0.8728232661110162]
	TIME [epoch: 9.02 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8775822161434096		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.8775822161434096 | validation: 0.6581838323156433]
	TIME [epoch: 9.01 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7934461490546105		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.7934461490546105 | validation: 0.6243390818320923]
	TIME [epoch: 9.05 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6051380797235129		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.6051380797235129 | validation: 0.7224094425784875]
	TIME [epoch: 9.03 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8322815068063594		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.8322815068063594 | validation: 1.4545369045450172]
	TIME [epoch: 9.01 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9501257192007555		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.9501257192007555 | validation: 0.583878732668394]
	TIME [epoch: 9.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5433467976606182		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.5433467976606182 | validation: 0.4843167134085673]
	TIME [epoch: 9.06 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8120634989392526		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.8120634989392526 | validation: 0.7864444750291009]
	TIME [epoch: 9.01 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8445089768703794		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.8445089768703794 | validation: 0.5729007532823974]
	TIME [epoch: 8.99 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.78344673810965		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.78344673810965 | validation: 0.4815746006735964]
	TIME [epoch: 9.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5466751694035734		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.5466751694035734 | validation: 0.6944131275411547]
	TIME [epoch: 9.06 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.550897480947956		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.550897480947956 | validation: 0.45698574239524503]
	TIME [epoch: 9.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8408171686864281		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.8408171686864281 | validation: 0.5961105405458083]
	TIME [epoch: 9.01 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5392061039340375		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.5392061039340375 | validation: 0.5830959392544148]
	TIME [epoch: 9 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5041979321453297		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.5041979321453297 | validation: 0.4363922629233822]
	TIME [epoch: 9.06 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43488718610326726		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.43488718610326726 | validation: 0.5604696883618931]
	TIME [epoch: 9 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0230980773871896		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 1.0230980773871896 | validation: 1.0297580367530996]
	TIME [epoch: 8.99 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7579202948584955		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.7579202948584955 | validation: 0.3958642928941688]
	TIME [epoch: 8.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46933411262770963		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.46933411262770963 | validation: 0.40872798767363405]
	TIME [epoch: 9.04 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5751322757311627		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.5751322757311627 | validation: 0.6189031163688143]
	TIME [epoch: 9 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5689655980853505		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.5689655980853505 | validation: 0.49070461207060845]
	TIME [epoch: 8.99 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4950347063534666		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.4950347063534666 | validation: 0.4600552756339609]
	TIME [epoch: 8.99 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.432461380395032		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.432461380395032 | validation: 0.5024191417418513]
	TIME [epoch: 9.01 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.50986395739017		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.50986395739017 | validation: 0.4168604566112266]
	TIME [epoch: 9.01 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7577924528155998		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.7577924528155998 | validation: 0.5893850065985179]
	TIME [epoch: 8.98 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5714386457953343		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.5714386457953343 | validation: 0.3971368069259859]
	TIME [epoch: 8.99 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38043757260777017		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.38043757260777017 | validation: 0.3894338380920581]
	TIME [epoch: 9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4714648827581046		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.4714648827581046 | validation: 0.3733219417717855]
	TIME [epoch: 9.04 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5114578841883655		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.5114578841883655 | validation: 0.5443074549309839]
	TIME [epoch: 8.99 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6270442777292548		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.6270442777292548 | validation: 0.7329367301632183]
	TIME [epoch: 8.98 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5987742518487876		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.5987742518487876 | validation: 0.34407107571991813]
	TIME [epoch: 9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41028523643027137		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.41028523643027137 | validation: 0.442944893509703]
	TIME [epoch: 9.03 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5540621666310124		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.5540621666310124 | validation: 0.7471574606470528]
	TIME [epoch: 8.99 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5094600891297596		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.5094600891297596 | validation: 0.3908842645542469]
	TIME [epoch: 8.99 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36828950693642415		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.36828950693642415 | validation: 0.45964256710058726]
	TIME [epoch: 9 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5464426225308562		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.5464426225308562 | validation: 0.3669069519130306]
	TIME [epoch: 9.04 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48708236449863596		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.48708236449863596 | validation: 0.8263638895802272]
	TIME [epoch: 8.99 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.636648649630343		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.636648649630343 | validation: 0.5330763748650362]
	TIME [epoch: 8.99 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4498169691265694		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.4498169691265694 | validation: 0.44170838972202586]
	TIME [epoch: 8.99 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3936820337482095		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.3936820337482095 | validation: 0.3518950262871716]
	TIME [epoch: 9.04 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3818862241727937		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.3818862241727937 | validation: 1.1202667179542056]
	TIME [epoch: 8.99 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8376018866361186		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.8376018866361186 | validation: 0.4931970681874792]
	TIME [epoch: 8.99 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5316600212897085		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.5316600212897085 | validation: 0.4092753131855502]
	TIME [epoch: 8.99 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4536816483888612		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.4536816483888612 | validation: 0.3938381477746542]
	TIME [epoch: 9.04 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4206848740239221		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.4206848740239221 | validation: 0.41657666239679425]
	TIME [epoch: 9 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33730512813191166		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.33730512813191166 | validation: 0.33131032305242647]
	TIME [epoch: 8.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5492990297915261		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.5492990297915261 | validation: 0.4435543942766246]
	TIME [epoch: 8.99 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42387703087549955		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.42387703087549955 | validation: 0.348795607136027]
	TIME [epoch: 9.04 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3850542736400816		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.3850542736400816 | validation: 0.695998400140339]
	TIME [epoch: 9 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6372711131097022		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.6372711131097022 | validation: 0.5109676824584214]
	TIME [epoch: 8.99 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3993767623370683		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.3993767623370683 | validation: 0.3723571126754699]
	TIME [epoch: 8.99 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3377202201207423		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.3377202201207423 | validation: 0.35425218524671626]
	TIME [epoch: 9.03 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4179390253714236		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.4179390253714236 | validation: 0.37990636077362083]
	TIME [epoch: 9 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4450408769562297		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.4450408769562297 | validation: 0.9730131092599452]
	TIME [epoch: 8.99 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6729948421015086		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.6729948421015086 | validation: 0.5211912064732126]
	TIME [epoch: 8.99 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4196541767393605		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.4196541767393605 | validation: 0.3080466656547528]
	TIME [epoch: 9.04 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30321204262881857		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.30321204262881857 | validation: 0.2757574922297974]
	TIME [epoch: 9.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3669658627012215		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.3669658627012215 | validation: 0.3231431375880347]
	TIME [epoch: 8.97 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4837111371209672		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.4837111371209672 | validation: 0.4898938251523188]
	TIME [epoch: 8.97 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5118483086805335		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.5118483086805335 | validation: 0.5147978905667616]
	TIME [epoch: 9.01 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3739751301186547		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.3739751301186547 | validation: 0.3078562769266667]
	TIME [epoch: 8.98 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3011570749134335		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.3011570749134335 | validation: 0.41570100732255133]
	TIME [epoch: 8.97 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.543126106314164		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.543126106314164 | validation: 0.43831270616782925]
	TIME [epoch: 8.97 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3580694940032679		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.3580694940032679 | validation: 0.3520740694015918]
	TIME [epoch: 8.98 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34201736699619883		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.34201736699619883 | validation: 0.5865314372910401]
	TIME [epoch: 9.01 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4862053890261515		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.4862053890261515 | validation: 0.34924735106343807]
	TIME [epoch: 8.97 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35004420173355694		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.35004420173355694 | validation: 0.45794501843321894]
	TIME [epoch: 8.98 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4375017904073739		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.4375017904073739 | validation: 0.416430033279312]
	TIME [epoch: 8.99 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3386929962591086		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.3386929962591086 | validation: 0.3892439135013619]
	TIME [epoch: 9.01 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4229314460674934		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.4229314460674934 | validation: 0.5664675704552765]
	TIME [epoch: 8.96 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5006408046396534		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.5006408046396534 | validation: 0.3241701661552574]
	TIME [epoch: 8.97 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35638203868167856		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.35638203868167856 | validation: 0.33519941916421425]
	TIME [epoch: 8.96 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38458756807809097		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.38458756807809097 | validation: 0.260150193037079]
	TIME [epoch: 9.01 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5080570543114434		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.5080570543114434 | validation: 0.5057666440583762]
	TIME [epoch: 8.97 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39352251724875237		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.39352251724875237 | validation: 0.3118388812554924]
	TIME [epoch: 8.97 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29089514741064704		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.29089514741064704 | validation: 0.26082948802868033]
	TIME [epoch: 8.97 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.352889389097098		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.352889389097098 | validation: 0.3447516696135722]
	TIME [epoch: 9.03 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3876988710111011		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.3876988710111011 | validation: 0.5909563718545787]
	TIME [epoch: 8.98 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42259735304492746		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.42259735304492746 | validation: 0.3030113086721338]
	TIME [epoch: 8.98 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2762354440291209		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.2762354440291209 | validation: 0.3160518615051415]
	TIME [epoch: 8.97 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4085311990677589		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.4085311990677589 | validation: 0.6871851970865813]
	TIME [epoch: 9.03 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5033828267478819		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.5033828267478819 | validation: 0.42674736067327945]
	TIME [epoch: 8.99 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43925510049926403		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.43925510049926403 | validation: 0.37473654653723293]
	TIME [epoch: 8.97 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3714611399040238		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.3714611399040238 | validation: 0.2971286740541421]
	TIME [epoch: 8.97 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27952103170038584		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.27952103170038584 | validation: 0.4039087593624464]
	TIME [epoch: 9.02 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3906465281435466		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.3906465281435466 | validation: 0.26094149996930155]
	TIME [epoch: 8.98 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40079981817437316		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.40079981817437316 | validation: 0.41187019992987983]
	TIME [epoch: 8.98 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3268576665069227		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.3268576665069227 | validation: 0.2665233428090804]
	TIME [epoch: 8.98 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26919763539567265		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.26919763539567265 | validation: 0.660425132737591]
	TIME [epoch: 9.01 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5672694608110089		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.5672694608110089 | validation: 0.38078164792047703]
	TIME [epoch: 8.99 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35922354910153187		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.35922354910153187 | validation: 0.2691866140488483]
	TIME [epoch: 8.97 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30095560305220204		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.30095560305220204 | validation: 0.3249204652209524]
	TIME [epoch: 8.97 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28186093476598867		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.28186093476598867 | validation: 0.3527965376501836]
	TIME [epoch: 9 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3708261023657903		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.3708261023657903 | validation: 0.6555094979135592]
	TIME [epoch: 9.01 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46459357205998547		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.46459357205998547 | validation: 0.37783859509463835]
	TIME [epoch: 8.99 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3290298051562141		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.3290298051562141 | validation: 0.4056867551950631]
	TIME [epoch: 8.98 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39126044606382815		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.39126044606382815 | validation: 0.3130032251946464]
	TIME [epoch: 8.99 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37854922053266676		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.37854922053266676 | validation: 0.336535436496274]
	TIME [epoch: 9.03 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32440570198136465		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.32440570198136465 | validation: 0.30804477878895536]
	TIME [epoch: 8.97 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32221072301966114		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.32221072301966114 | validation: 0.28787558431441007]
	TIME [epoch: 8.99 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3360837357096552		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.3360837357096552 | validation: 0.4208221170880465]
	TIME [epoch: 8.98 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30625473721336804		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.30625473721336804 | validation: 0.27998372430000396]
	TIME [epoch: 9.03 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29061990242205976		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.29061990242205976 | validation: 0.34328112625943197]
	TIME [epoch: 8.99 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28969324347657943		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.28969324347657943 | validation: 0.33411914101166484]
	TIME [epoch: 8.98 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3148360784680585		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.3148360784680585 | validation: 0.4359508627459626]
	TIME [epoch: 8.98 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35834038457598905		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.35834038457598905 | validation: 0.2632535077857172]
	TIME [epoch: 9.04 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3563609340033519		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.3563609340033519 | validation: 0.3407170063424946]
	TIME [epoch: 8.98 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3507518571157683		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.3507518571157683 | validation: 0.2692935679241648]
	TIME [epoch: 8.98 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25223831275652747		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.25223831275652747 | validation: 0.3348066277960449]
	TIME [epoch: 8.98 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3636539391856811		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.3636539391856811 | validation: 0.262208437203018]
	TIME [epoch: 9.02 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3228454787853971		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.3228454787853971 | validation: 0.26292925683907925]
	TIME [epoch: 8.97 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2856978057049045		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.2856978057049045 | validation: 0.26141908511184786]
	TIME [epoch: 8.98 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24517048906423738		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.24517048906423738 | validation: 0.49186420406494147]
	TIME [epoch: 8.98 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3257525677779266		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.3257525677779266 | validation: 0.21918952952008933]
	TIME [epoch: 9.02 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26759770169170305		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.26759770169170305 | validation: 0.3035862514684584]
	TIME [epoch: 9.01 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28455421706023076		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.28455421706023076 | validation: 0.2710564360833069]
	TIME [epoch: 9.01 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26503375138720586		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.26503375138720586 | validation: 0.3037546132658489]
	TIME [epoch: 8.97 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31791252790007607		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.31791252790007607 | validation: 0.42917686701680613]
	TIME [epoch: 9.03 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38485327935860625		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.38485327935860625 | validation: 0.37421089628957027]
	TIME [epoch: 9.02 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3462549680449011		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.3462549680449011 | validation: 0.2625396968306025]
	TIME [epoch: 8.99 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3004401476272312		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.3004401476272312 | validation: 0.2767800690656177]
	TIME [epoch: 9 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2676611502070896		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.2676611502070896 | validation: 0.4010598300566002]
	TIME [epoch: 9.02 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28975439113714496		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.28975439113714496 | validation: 0.22870428904825113]
	TIME [epoch: 9.04 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32439587421109894		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.32439587421109894 | validation: 0.4383477763535343]
	TIME [epoch: 8.99 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35499666839676847		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.35499666839676847 | validation: 0.27188048309879254]
	TIME [epoch: 9 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24899323392964895		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.24899323392964895 | validation: 0.22176011772416024]
	TIME [epoch: 9 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25638182793059927		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.25638182793059927 | validation: 0.27004985746190213]
	TIME [epoch: 9.04 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2896417218338339		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.2896417218338339 | validation: 0.2123028052784645]
	TIME [epoch: 8.99 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2468246607409513		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.2468246607409513 | validation: 0.22920477383481513]
	TIME [epoch: 8.97 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2902315840784654		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.2902315840784654 | validation: 0.3026526290028787]
	TIME [epoch: 8.99 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25149956777401883		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.25149956777401883 | validation: 0.19038715685338803]
	TIME [epoch: 9.04 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30637345713849007		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.30637345713849007 | validation: 0.48347018145056614]
	TIME [epoch: 8.98 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34940033226016887		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.34940033226016887 | validation: 0.2449610096044776]
	TIME [epoch: 8.98 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23757987261331545		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.23757987261331545 | validation: 0.20098595315355935]
	TIME [epoch: 8.99 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2159496073623823		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.2159496073623823 | validation: 0.24541387314226104]
	TIME [epoch: 9.06 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3147715987225633		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.3147715987225633 | validation: 0.33440561237510935]
	TIME [epoch: 8.99 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24605296054373213		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.24605296054373213 | validation: 0.23948692887019662]
	TIME [epoch: 9 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2164000483753526		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.2164000483753526 | validation: 0.30931059139058326]
	TIME [epoch: 8.99 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3364217079764774		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.3364217079764774 | validation: 0.3595395644394358]
	TIME [epoch: 122 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32042313520122717		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.32042313520122717 | validation: 0.26542590733515814]
	TIME [epoch: 17.8 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2351649378578172		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.2351649378578172 | validation: 0.23379671621725717]
	TIME [epoch: 17.8 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2057280403219278		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.2057280403219278 | validation: 0.26093731671414416]
	TIME [epoch: 17.8 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31516967004786345		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.31516967004786345 | validation: 0.26892262093640174]
	TIME [epoch: 17.8 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2512835408543119		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.2512835408543119 | validation: 0.26248563951568904]
	TIME [epoch: 17.8 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22035557022147845		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.22035557022147845 | validation: 0.2558448341386499]
	TIME [epoch: 17.7 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23486014534815144		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.23486014534815144 | validation: 0.18962536001447988]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19773122920195219		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.19773122920195219 | validation: 0.17993657743100105]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19249012182333858		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.19249012182333858 | validation: 0.3023084581435641]
	TIME [epoch: 17.8 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3791014594634237		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.3791014594634237 | validation: 0.33516525750101495]
	TIME [epoch: 17.8 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27946015952699765		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.27946015952699765 | validation: 0.21402593155141741]
	TIME [epoch: 17.8 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23105248515667684		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.23105248515667684 | validation: 0.2730178232638008]
	TIME [epoch: 17.8 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24941449565178062		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.24941449565178062 | validation: 0.19074844962358223]
	TIME [epoch: 17.8 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28218054039616614		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.28218054039616614 | validation: 0.2433856103693446]
	TIME [epoch: 17.8 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20896409968238538		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.20896409968238538 | validation: 0.2819315447503689]
	TIME [epoch: 17.8 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2498473619520088		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.2498473619520088 | validation: 0.21983947365363124]
	TIME [epoch: 17.8 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2160654175516174		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.2160654175516174 | validation: 0.34614910729431436]
	TIME [epoch: 17.8 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23791498354423826		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.23791498354423826 | validation: 0.19141207407331645]
	TIME [epoch: 17.8 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2208945044506502		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.2208945044506502 | validation: 0.1777991691963195]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16776761854162928		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.16776761854162928 | validation: 0.4129252350912327]
	TIME [epoch: 17.8 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3211427887301085		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.3211427887301085 | validation: 0.242921971290966]
	TIME [epoch: 17.8 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24689899786293912		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.24689899786293912 | validation: 0.22183867164966836]
	TIME [epoch: 17.8 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22451401970354404		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.22451401970354404 | validation: 0.2664895763505084]
	TIME [epoch: 17.8 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20417912083321538		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.20417912083321538 | validation: 0.2522834735668575]
	TIME [epoch: 17.8 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21746453331177623		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.21746453331177623 | validation: 0.1706923144827044]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1669544446925857		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.1669544446925857 | validation: 0.27939510980276094]
	TIME [epoch: 17.8 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25961244833536934		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.25961244833536934 | validation: 0.3143933127711394]
	TIME [epoch: 17.8 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25488789419689917		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.25488789419689917 | validation: 0.2102897006190391]
	TIME [epoch: 17.9 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18502765890184714		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.18502765890184714 | validation: 0.15155627876827915]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18811546174290378		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.18811546174290378 | validation: 0.18571991195437315]
	TIME [epoch: 17.9 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17712000261049105		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.17712000261049105 | validation: 0.3381463177731745]
	TIME [epoch: 17.8 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2963152520675381		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.2963152520675381 | validation: 0.3213232141696526]
	TIME [epoch: 17.9 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21691109180932377		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.21691109180932377 | validation: 0.19912390987447048]
	TIME [epoch: 17.8 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1953309907308187		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.1953309907308187 | validation: 0.4691552103728742]
	TIME [epoch: 17.9 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24143771716826484		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.24143771716826484 | validation: 0.18764201543647763]
	TIME [epoch: 17.8 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1892865840583793		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.1892865840583793 | validation: 0.1919742176102487]
	TIME [epoch: 17.8 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16319085504294795		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.16319085504294795 | validation: 0.174551853562467]
	TIME [epoch: 17.8 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2470756634570688		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.2470756634570688 | validation: 0.3271044197965003]
	TIME [epoch: 17.8 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25121461171963233		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.25121461171963233 | validation: 0.2689628036135898]
	TIME [epoch: 17.9 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1955232686087098		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.1955232686087098 | validation: 0.1666236579894519]
	TIME [epoch: 17.8 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.237951762608546		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.237951762608546 | validation: 0.2652103653713593]
	TIME [epoch: 17.9 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2039760468800535		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.2039760468800535 | validation: 0.17714271552213967]
	TIME [epoch: 17.8 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1586102919154092		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.1586102919154092 | validation: 0.2541118024709569]
	TIME [epoch: 17.9 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23161111690854114		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.23161111690854114 | validation: 0.17152500718574704]
	TIME [epoch: 17.8 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21611289180826404		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.21611289180826404 | validation: 0.21636917236044495]
	TIME [epoch: 17.8 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22555276217242687		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.22555276217242687 | validation: 0.1528800052231767]
	TIME [epoch: 17.8 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1618890608853899		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.1618890608853899 | validation: 0.18271891577420563]
	TIME [epoch: 17.8 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1824463642148128		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.1824463642148128 | validation: 0.18239011557146825]
	TIME [epoch: 17.9 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1628731561195075		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.1628731561195075 | validation: 0.26373276193880474]
	TIME [epoch: 17.8 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23898867785448522		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.23898867785448522 | validation: 0.2852157141526347]
	TIME [epoch: 17.9 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21087106884659473		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.21087106884659473 | validation: 0.1470655397249703]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1565214669617682		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.1565214669617682 | validation: 0.16744404329635387]
	TIME [epoch: 17.9 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2407991184059803		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.2407991184059803 | validation: 0.31101999604262165]
	TIME [epoch: 17.8 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21141806877007288		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.21141806877007288 | validation: 0.226769314888641]
	TIME [epoch: 17.8 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18165042980878354		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.18165042980878354 | validation: 0.200427904575171]
	TIME [epoch: 17.8 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1780281003267141		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.1780281003267141 | validation: 0.1507566217404207]
	TIME [epoch: 17.8 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2075430521684033		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.2075430521684033 | validation: 0.14636445247075916]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20481000646133543		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.20481000646133543 | validation: 0.25049577483606233]
	TIME [epoch: 17.8 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16779618127058532		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.16779618127058532 | validation: 0.152286765046653]
	TIME [epoch: 17.8 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1383746577320478		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.1383746577320478 | validation: 0.24795644283182172]
	TIME [epoch: 17.8 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1985425085113367		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.1985425085113367 | validation: 0.2331002484156735]
	TIME [epoch: 17.9 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21315327705884968		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.21315327705884968 | validation: 0.22809759436725807]
	TIME [epoch: 17.8 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19358399963502063		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.19358399963502063 | validation: 0.217483714629942]
	TIME [epoch: 17.9 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16358656321583703		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.16358656321583703 | validation: 0.1774867211411561]
	TIME [epoch: 17.8 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18089667354242378		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.18089667354242378 | validation: 0.17377327922625244]
	TIME [epoch: 17.8 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16676555816996547		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.16676555816996547 | validation: 0.19972850866986608]
	TIME [epoch: 17.8 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17086903979782814		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.17086903979782814 | validation: 0.1815095861609279]
	TIME [epoch: 17.8 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20263072260665926		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.20263072260665926 | validation: 0.2320105761041019]
	TIME [epoch: 17.9 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18472654689606677		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.18472654689606677 | validation: 0.16927744667596417]
	TIME [epoch: 17.8 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14830694274941186		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.14830694274941186 | validation: 0.17518407608950354]
	TIME [epoch: 17.8 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13031795116480252		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.13031795116480252 | validation: 0.1369318571632244]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21602041923238835		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.21602041923238835 | validation: 0.22186295631103456]
	TIME [epoch: 17.9 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18921332070213442		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.18921332070213442 | validation: 0.22292757948620356]
	TIME [epoch: 17.8 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16595822761929996		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.16595822761929996 | validation: 0.15547487504349833]
	TIME [epoch: 17.9 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19350937122131212		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.19350937122131212 | validation: 0.20003243737025106]
	TIME [epoch: 17.8 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17476341116810407		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.17476341116810407 | validation: 0.13538565791088783]
	TIME [epoch: 17.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11685917553185511		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.11685917553185511 | validation: 0.13069471302174157]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2465296983829316		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.2465296983829316 | validation: 0.15492933101762685]
	TIME [epoch: 17.9 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18635882731229686		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.18635882731229686 | validation: 0.268031715284158]
	TIME [epoch: 17.9 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18611647845504742		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.18611647845504742 | validation: 0.12254317851644221]
	TIME [epoch: 17.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1357408564583926		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.1357408564583926 | validation: 0.11010850115818793]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10574186980741175		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.10574186980741175 | validation: 0.11001267058701183]
	TIME [epoch: 17.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.173966428089118		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.173966428089118 | validation: 0.31989999007025666]
	TIME [epoch: 17.8 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25944014369912427		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.25944014369912427 | validation: 0.17287338275828412]
	TIME [epoch: 17.9 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20776856462172189		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.20776856462172189 | validation: 0.17029984188228825]
	TIME [epoch: 17.9 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13789466832429842		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.13789466832429842 | validation: 0.20296512693164048]
	TIME [epoch: 17.9 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14608128671150356		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.14608128671150356 | validation: 0.16993924852196263]
	TIME [epoch: 17.9 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13740229535628462		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.13740229535628462 | validation: 0.13767132799442597]
	TIME [epoch: 17.8 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12825933346090784		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.12825933346090784 | validation: 0.25750787630514727]
	TIME [epoch: 17.9 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16559873992029192		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.16559873992029192 | validation: 0.10632040968798143]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21470111730603036		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.21470111730603036 | validation: 0.26367810427530186]
	TIME [epoch: 17.9 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16125331611571556		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.16125331611571556 | validation: 0.13691072723797088]
	TIME [epoch: 17.8 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13485500739021705		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.13485500739021705 | validation: 0.17305473779262265]
	TIME [epoch: 17.9 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16037396996720638		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.16037396996720638 | validation: 0.10873132176227553]
	TIME [epoch: 17.9 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1313066117400732		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.1313066117400732 | validation: 0.11439471130697658]
	TIME [epoch: 17.9 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11257016550674728		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.11257016550674728 | validation: 0.10381477196242964]
	TIME [epoch: 17.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19954977856824843		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.19954977856824843 | validation: 0.2307535952630145]
	TIME [epoch: 17.9 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13890268421517907		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.13890268421517907 | validation: 0.1157429063707132]
	TIME [epoch: 17.8 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12348603010828496		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.12348603010828496 | validation: 0.11180990153986076]
	TIME [epoch: 17.9 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0987310508860962		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.0987310508860962 | validation: 0.14103801154809156]
	TIME [epoch: 17.9 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13799324949918423		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.13799324949918423 | validation: 0.11254028577901778]
	TIME [epoch: 17.9 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1860886069515783		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.1860886069515783 | validation: 0.13672562489055176]
	TIME [epoch: 17.9 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16384620359814922		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.16384620359814922 | validation: 0.1363820779378601]
	TIME [epoch: 17.9 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1944010651397596		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.1944010651397596 | validation: 0.15536633981018375]
	TIME [epoch: 17.9 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11005890122883519		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.11005890122883519 | validation: 0.17640952126704051]
	TIME [epoch: 17.8 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.122478833986672		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.122478833986672 | validation: 0.09546417786303599]
	TIME [epoch: 17.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17962862337651167		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.17962862337651167 | validation: 0.14700494991924157]
	TIME [epoch: 17.8 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16652798249073159		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.16652798249073159 | validation: 0.28902551639366525]
	TIME [epoch: 17.9 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17495043665741264		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.17495043665741264 | validation: 0.11134905445750473]
	TIME [epoch: 17.8 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12427066108860411		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.12427066108860411 | validation: 0.13929388106154605]
	TIME [epoch: 17.9 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14811324208005253		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.14811324208005253 | validation: 0.10982239383735366]
	TIME [epoch: 17.8 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0929533402379151		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.0929533402379151 | validation: 0.09785778723430186]
	TIME [epoch: 17.9 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17139350901894598		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.17139350901894598 | validation: 0.19770914009970358]
	TIME [epoch: 17.9 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1215422424653893		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.1215422424653893 | validation: 0.1017810968094525]
	TIME [epoch: 17.9 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10763275664505763		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.10763275664505763 | validation: 0.1125418951165656]
	TIME [epoch: 17.9 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11252493878773788		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.11252493878773788 | validation: 0.12044695371620638]
	TIME [epoch: 17.8 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15843717397687174		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.15843717397687174 | validation: 0.20520705299926212]
	TIME [epoch: 17.9 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1314250669789392		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.1314250669789392 | validation: 0.16049872422561923]
	TIME [epoch: 17.8 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13433450874437589		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.13433450874437589 | validation: 0.11215800455381067]
	TIME [epoch: 17.9 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10311502400547		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.10311502400547 | validation: 0.10973163556101487]
	TIME [epoch: 17.8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15412700279172667		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.15412700279172667 | validation: 0.10166486302628377]
	TIME [epoch: 17.9 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2117281281114014		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.2117281281114014 | validation: 0.1299441901053131]
	TIME [epoch: 17.8 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13207417359506646		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.13207417359506646 | validation: 0.14776168736643444]
	TIME [epoch: 17.9 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11836267624838055		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.11836267624838055 | validation: 0.11136037089052728]
	TIME [epoch: 17.8 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1375988753996249		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.1375988753996249 | validation: 0.11425796397015214]
	TIME [epoch: 17.8 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08979836015136616		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.08979836015136616 | validation: 0.15200046173917753]
	TIME [epoch: 17.9 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13696837243493146		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.13696837243493146 | validation: 0.17230454841279857]
	TIME [epoch: 17.8 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1698269059227242		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.1698269059227242 | validation: 0.17713263878176422]
	TIME [epoch: 17.9 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10141452673811133		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.10141452673811133 | validation: 0.09106013635621382]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09948106864114903		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.09948106864114903 | validation: 0.09428252560581724]
	TIME [epoch: 17.9 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10224705340454479		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.10224705340454479 | validation: 0.1269649732887908]
	TIME [epoch: 17.8 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10325630935082172		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.10325630935082172 | validation: 0.17828236971864295]
	TIME [epoch: 17.9 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11689506740934243		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.11689506740934243 | validation: 0.08931476736165853]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10170997477105734		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.10170997477105734 | validation: 0.18008623855300832]
	TIME [epoch: 17.9 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16805449427695948		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.16805449427695948 | validation: 0.09587403699329902]
	TIME [epoch: 17.8 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14467207229806317		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.14467207229806317 | validation: 0.123455120379514]
	TIME [epoch: 17.8 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1278728961206175		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.1278728961206175 | validation: 0.1589976593197238]
	TIME [epoch: 17.8 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18558533709331954		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.18558533709331954 | validation: 0.1507863282349202]
	TIME [epoch: 17.8 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11125478199520118		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.11125478199520118 | validation: 0.08885978414635587]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08097928131234158		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.08097928131234158 | validation: 0.10047304630638876]
	TIME [epoch: 17.8 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09923380315081429		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.09923380315081429 | validation: 0.09397845020575624]
	TIME [epoch: 17.9 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0815545084381266		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.0815545084381266 | validation: 0.13999057200669315]
	TIME [epoch: 17.8 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13454963767779238		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.13454963767779238 | validation: 0.1396075491475216]
	TIME [epoch: 17.9 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08568632270573709		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.08568632270573709 | validation: 0.11166563964711679]
	TIME [epoch: 17.8 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10631847093534784		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.10631847093534784 | validation: 0.18179013549285603]
	TIME [epoch: 17.9 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14567359344721958		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.14567359344721958 | validation: 0.10651355903698581]
	TIME [epoch: 17.8 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10146400703333797		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.10146400703333797 | validation: 0.08061401784829875]
	TIME [epoch: 17.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07121613592954229		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.07121613592954229 | validation: 0.07011921033857091]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11520953034976322		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.11520953034976322 | validation: 0.07848323592271245]
	TIME [epoch: 17.9 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10046645909671573		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.10046645909671573 | validation: 0.08657821961025056]
	TIME [epoch: 17.8 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10741480727858194		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.10741480727858194 | validation: 0.22347444310890696]
	TIME [epoch: 17.9 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.162794304846845		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.162794304846845 | validation: 0.07627481878367796]
	TIME [epoch: 17.8 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09794192125664233		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.09794192125664233 | validation: 0.08764518748971312]
	TIME [epoch: 17.9 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06844654446347222		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.06844654446347222 | validation: 0.13599210575701348]
	TIME [epoch: 17.9 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09734826596756377		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.09734826596756377 | validation: 0.11470003990913873]
	TIME [epoch: 17.8 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07283168663851185		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.07283168663851185 | validation: 0.060435108754936906]
	TIME [epoch: 17.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060802164878818656		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.060802164878818656 | validation: 0.11674448682600339]
	TIME [epoch: 17.8 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15713162344383275		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.15713162344383275 | validation: 0.16704389172201006]
	TIME [epoch: 17.9 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12011188500086699		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.12011188500086699 | validation: 0.07667900821294016]
	TIME [epoch: 17.8 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06942730746815019		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.06942730746815019 | validation: 0.16302709582264802]
	TIME [epoch: 17.9 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12270398980888433		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.12270398980888433 | validation: 0.08796971517900785]
	TIME [epoch: 17.8 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0726026701888157		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.0726026701888157 | validation: 0.08328515374707142]
	TIME [epoch: 17.9 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08403022181792313		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.08403022181792313 | validation: 0.06602206797469967]
	TIME [epoch: 17.8 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06274980554920927		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.06274980554920927 | validation: 0.07130465883207715]
	TIME [epoch: 17.9 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09828676650702471		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.09828676650702471 | validation: 0.1310443149031857]
	TIME [epoch: 17.8 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11093325544557922		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.11093325544557922 | validation: 0.19600085779726062]
	TIME [epoch: 17.8 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11723402282249207		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.11723402282249207 | validation: 0.0883891568546234]
	TIME [epoch: 17.8 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08244235205367444		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.08244235205367444 | validation: 0.08752145165042208]
	TIME [epoch: 17.8 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09116068525658029		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.09116068525658029 | validation: 0.1148862315168527]
	TIME [epoch: 17.9 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09015884520630937		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.09015884520630937 | validation: 0.0775484401130219]
	TIME [epoch: 17.8 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060248979175259704		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.060248979175259704 | validation: 0.06917296567574206]
	TIME [epoch: 17.9 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07272841036086858		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.07272841036086858 | validation: 0.09432777518571919]
	TIME [epoch: 17.8 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09586972812791458		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.09586972812791458 | validation: 0.06843642785841769]
	TIME [epoch: 17.9 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07394158877869934		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.07394158877869934 | validation: 0.15934378612376393]
	TIME [epoch: 17.8 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09653724362412686		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.09653724362412686 | validation: 0.09120480831914343]
	TIME [epoch: 17.9 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08359437604160863		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.08359437604160863 | validation: 0.07754286959708209]
	TIME [epoch: 17.9 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0727686373269551		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.0727686373269551 | validation: 0.05742762779515733]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11084595985600623		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.11084595985600623 | validation: 0.16461957376563047]
	TIME [epoch: 17.8 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09395734077991899		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.09395734077991899 | validation: 0.06356501936529063]
	TIME [epoch: 17.8 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057120965194472725		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.057120965194472725 | validation: 0.05587145326116155]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05017168271777898		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.05017168271777898 | validation: 0.11791201404052148]
	TIME [epoch: 17.8 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08143417796345744		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.08143417796345744 | validation: 0.23583681368785187]
	TIME [epoch: 17.9 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10384543901212581		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.10384543901212581 | validation: 0.08448738996552686]
	TIME [epoch: 17.8 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05194086447560828		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.05194086447560828 | validation: 0.0873903209006961]
	TIME [epoch: 17.9 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0752789223781373		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.0752789223781373 | validation: 0.09158235767453773]
	TIME [epoch: 17.8 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07157485793980718		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.07157485793980718 | validation: 0.18551762339287448]
	TIME [epoch: 17.9 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10426623321512843		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.10426623321512843 | validation: 0.0778665611845242]
	TIME [epoch: 17.8 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07546136753609793		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.07546136753609793 | validation: 0.05659968617648116]
	TIME [epoch: 17.9 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052096876214267326		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.052096876214267326 | validation: 0.06563633381159954]
	TIME [epoch: 17.8 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08588841159528482		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.08588841159528482 | validation: 0.06479804869920312]
	TIME [epoch: 17.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05711566557419009		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.05711566557419009 | validation: 0.06980500644209742]
	TIME [epoch: 17.9 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055614337215466624		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.055614337215466624 | validation: 0.05086521530425189]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09383554695722998		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.09383554695722998 | validation: 0.13323172500567615]
	TIME [epoch: 17.9 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2532546362334829		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.2532546362334829 | validation: 0.07102305487462093]
	TIME [epoch: 17.8 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05341019587777321		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.05341019587777321 | validation: 0.05261846468803601]
	TIME [epoch: 17.9 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04394659065400839		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.04394659065400839 | validation: 0.049262245304565175]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07340017558341103		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.07340017558341103 | validation: 0.049577374515503876]
	TIME [epoch: 17.9 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05045874758672896		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.05045874758672896 | validation: 0.04487475281467386]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06561312273501362		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.06561312273501362 | validation: 0.11029306049431314]
	TIME [epoch: 17.9 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06427374377889959		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.06427374377889959 | validation: 0.061695468892804826]
	TIME [epoch: 17.8 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04501878895661045		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.04501878895661045 | validation: 0.062124318234260206]
	TIME [epoch: 17.9 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05469687779891706		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.05469687779891706 | validation: 0.14806747394253053]
	TIME [epoch: 17.8 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12349646117550314		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.12349646117550314 | validation: 0.0515021536978614]
	TIME [epoch: 17.9 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049009972722542		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.049009972722542 | validation: 0.05060910813381754]
	TIME [epoch: 17.8 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06372169741615713		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.06372169741615713 | validation: 0.1233365562999916]
	TIME [epoch: 17.8 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0660399881926346		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.0660399881926346 | validation: 0.06568018437266834]
	TIME [epoch: 17.8 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05933970309934012		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.05933970309934012 | validation: 0.05555179662147973]
	TIME [epoch: 17.9 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05446255206155168		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.05446255206155168 | validation: 0.04421250204001412]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04595107870888372		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.04595107870888372 | validation: 0.05599301884878996]
	TIME [epoch: 17.8 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09522362703865236		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.09522362703865236 | validation: 0.05817830424053451]
	TIME [epoch: 17.9 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05026262178784725		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.05026262178784725 | validation: 0.03275979186842003]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03721177194249294		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.03721177194249294 | validation: 0.03940532380253288]
	TIME [epoch: 17.9 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036433127103906		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.036433127103906 | validation: 0.08044504499392086]
	TIME [epoch: 17.8 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10589905329551481		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.10589905329551481 | validation: 0.06509436812978439]
	TIME [epoch: 17.9 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1025741405487538		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.1025741405487538 | validation: 0.07387827353203938]
	TIME [epoch: 17.8 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05224806072805015		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.05224806072805015 | validation: 0.0368121211849932]
	TIME [epoch: 17.9 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03978500956879979		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.03978500956879979 | validation: 0.04214019973093222]
	TIME [epoch: 17.8 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03739572845663118		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.03739572845663118 | validation: 0.04736296510153105]
	TIME [epoch: 17.9 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03836778641174084		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.03836778641174084 | validation: 0.08383737925929133]
	TIME [epoch: 17.8 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0523918343776961		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.0523918343776961 | validation: 0.07216560160998922]
	TIME [epoch: 17.9 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07568531585898733		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.07568531585898733 | validation: 0.05901477626867006]
	TIME [epoch: 17.8 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03603016818864078		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.03603016818864078 | validation: 0.0495001044707743]
	TIME [epoch: 17.8 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07252725115225989		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.07252725115225989 | validation: 0.05047789480898439]
	TIME [epoch: 17.8 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05414326852130407		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.05414326852130407 | validation: 0.03656079422742219]
	TIME [epoch: 17.8 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04954992233497712		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.04954992233497712 | validation: 0.03956145435425571]
	TIME [epoch: 17.8 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04690543354004498		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.04690543354004498 | validation: 0.05269988198719404]
	TIME [epoch: 17.8 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055240395800994525		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.055240395800994525 | validation: 0.11588759964397435]
	TIME [epoch: 17.8 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08241416185937167		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.08241416185937167 | validation: 0.06041227465544278]
	TIME [epoch: 17.8 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08023494117529184		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.08023494117529184 | validation: 0.07255597190194375]
	TIME [epoch: 17.8 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04899805383375148		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.04899805383375148 | validation: 0.050050020190179643]
	TIME [epoch: 17.8 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044699233383289134		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.044699233383289134 | validation: 0.03524413236477008]
	TIME [epoch: 17.8 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032662609193153815		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.032662609193153815 | validation: 0.05021614916352962]
	TIME [epoch: 17.8 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04860302274128051		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.04860302274128051 | validation: 0.042998097900283805]
	TIME [epoch: 17.7 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030150880050674442		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.030150880050674442 | validation: 0.04024855395348122]
	TIME [epoch: 17.7 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05413256598792572		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.05413256598792572 | validation: 0.04380143835550224]
	TIME [epoch: 17.6 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05649137964515172		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.05649137964515172 | validation: 0.0658138369769395]
	TIME [epoch: 17.8 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04971601634152304		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.04971601634152304 | validation: 0.060332570480726655]
	TIME [epoch: 17.7 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04443442743849779		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.04443442743849779 | validation: 0.03450760647348896]
	TIME [epoch: 17.8 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03511487879261192		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.03511487879261192 | validation: 0.041761307433106326]
	TIME [epoch: 17.7 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03938975906409273		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.03938975906409273 | validation: 0.046161772497113784]
	TIME [epoch: 17.8 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06718286109400358		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.06718286109400358 | validation: 0.06859865394668567]
	TIME [epoch: 17.8 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05049937295574815		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.05049937295574815 | validation: 0.04134230512547027]
	TIME [epoch: 17.8 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03445065170625652		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.03445065170625652 | validation: 0.12617455960559276]
	TIME [epoch: 17.8 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06895573034181476		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.06895573034181476 | validation: 0.044687831975123354]
	TIME [epoch: 17.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04608445248956192		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.04608445248956192 | validation: 0.0486020865749731]
	TIME [epoch: 17.8 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03839281045173006		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.03839281045173006 | validation: 0.050137446054586234]
	TIME [epoch: 17.8 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04455488013116771		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.04455488013116771 | validation: 0.02931962273503027]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042310627118856386		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.042310627118856386 | validation: 0.07522680144762854]
	TIME [epoch: 17.8 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03978736150576895		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.03978736150576895 | validation: 0.05257772523197149]
	TIME [epoch: 17.9 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042820752149993604		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.042820752149993604 | validation: 0.031694882253648596]
	TIME [epoch: 17.8 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026364014141862492		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.026364014141862492 | validation: 0.03405257279750181]
	TIME [epoch: 17.9 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03843786526085534		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.03843786526085534 | validation: 0.037934234107284186]
	TIME [epoch: 17.8 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027673573788710146		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.027673573788710146 | validation: 0.12122699685725682]
	TIME [epoch: 17.9 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21833073719195392		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.21833073719195392 | validation: 0.1363672912272049]
	TIME [epoch: 17.8 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10999923579144198		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.10999923579144198 | validation: 0.035705269918553946]
	TIME [epoch: 17.9 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034112172419467006		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.034112172419467006 | validation: 0.025258674276580004]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027615269376005516		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.027615269376005516 | validation: 0.026210867385787415]
	TIME [epoch: 17.8 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027108163861999993		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.027108163861999993 | validation: 0.06312420348308825]
	TIME [epoch: 17.8 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045876565159679375		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.045876565159679375 | validation: 0.027464319642967278]
	TIME [epoch: 17.8 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03755970332556004		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.03755970332556004 | validation: 0.03801556612693203]
	TIME [epoch: 17.8 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03172009370839153		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.03172009370839153 | validation: 0.1947886700519157]
	TIME [epoch: 17.8 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1621249535807999		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.1621249535807999 | validation: 0.19721185522039042]
	TIME [epoch: 17.8 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12896921045743565		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.12896921045743565 | validation: 0.05016467155998093]
	TIME [epoch: 17.8 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042187464524141786		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.042187464524141786 | validation: 0.05271744155423497]
	TIME [epoch: 17.9 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054887615737571636		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.054887615737571636 | validation: 0.049444915180750205]
	TIME [epoch: 17.8 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03443727565459497		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.03443727565459497 | validation: 0.031779350740679786]
	TIME [epoch: 17.9 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044000574106164474		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.044000574106164474 | validation: 0.06054311974383389]
	TIME [epoch: 17.8 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045791387275013695		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.045791387275013695 | validation: 0.025340677798508786]
	TIME [epoch: 17.9 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026544977975402895		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.026544977975402895 | validation: 0.07225305772775521]
	TIME [epoch: 17.8 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04737121444898868		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.04737121444898868 | validation: 0.035062958232445436]
	TIME [epoch: 17.8 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026419355052400784		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.026419355052400784 | validation: 0.029832488125382514]
	TIME [epoch: 17.8 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022011883094217637		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.022011883094217637 | validation: 0.024365728360767475]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_473.pth
	Model improved!!!
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030523941209849862		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.030523941209849862 | validation: 0.03558329623019217]
	TIME [epoch: 17.9 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02823736529468753		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.02823736529468753 | validation: 0.023061534368807042]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0263872700455886		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.0263872700455886 | validation: 0.06263442637423705]
	TIME [epoch: 17.9 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05119216644466733		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.05119216644466733 | validation: 0.039545204456995]
	TIME [epoch: 17.8 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023766787939834333		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.023766787939834333 | validation: 0.030730788302546913]
	TIME [epoch: 17.9 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020451593518546647		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.020451593518546647 | validation: 0.026203516728557745]
	TIME [epoch: 17.8 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03827516381628953		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.03827516381628953 | validation: 0.03645170700238664]
	TIME [epoch: 17.9 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06012723015841959		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.06012723015841959 | validation: 0.044840069229507826]
	TIME [epoch: 17.8 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03056448487708047		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.03056448487708047 | validation: 0.025468326877432553]
	TIME [epoch: 17.9 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021398491778057173		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.021398491778057173 | validation: 0.030540041004721546]
	TIME [epoch: 17.8 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021531403542940166		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.021531403542940166 | validation: 0.042386124194034785]
	TIME [epoch: 17.9 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046213439169418516		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.046213439169418516 | validation: 0.042144507902360635]
	TIME [epoch: 17.8 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03136335142799032		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.03136335142799032 | validation: 0.029591990394742057]
	TIME [epoch: 17.9 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02138052239640392		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.02138052239640392 | validation: 0.027361241465832114]
	TIME [epoch: 17.8 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04069362297517541		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.04069362297517541 | validation: 0.03691943627872962]
	TIME [epoch: 17.8 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036674498971526984		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.036674498971526984 | validation: 0.03673376091221926]
	TIME [epoch: 17.8 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022074815731163773		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.022074815731163773 | validation: 0.031871312635708804]
	TIME [epoch: 17.8 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02885197163053269		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.02885197163053269 | validation: 0.034801486921789956]
	TIME [epoch: 17.9 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028826091497374973		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.028826091497374973 | validation: 0.06740735495016778]
	TIME [epoch: 17.8 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0616847089478394		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.0616847089478394 | validation: 0.06523769392282747]
	TIME [epoch: 17.9 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03127925159537229		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.03127925159537229 | validation: 0.022271584680225793]
	TIME [epoch: 17.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018552628383176867		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.018552628383176867 | validation: 0.028722749705858787]
	TIME [epoch: 17.9 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045348622820480536		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.045348622820480536 | validation: 0.04986621947832738]
	TIME [epoch: 17.8 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031251539510146906		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.031251539510146906 | validation: 0.03398420975218718]
	TIME [epoch: 17.9 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028033074592985773		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.028033074592985773 | validation: 0.025006990674756466]
	TIME [epoch: 17.8 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0282322386932995		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.0282322386932995 | validation: 0.02166546342138783]
	TIME [epoch: 17.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_499.pth
	Model improved!!!
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02004551736536987		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.02004551736536987 | validation: 0.024876064820632537]
	TIME [epoch: 17.8 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023017591284985817		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.023017591284985817 | validation: 0.04511168275567697]
	TIME [epoch: 142 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05056580807142143		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.05056580807142143 | validation: 0.04348113690278088]
	TIME [epoch: 38.5 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039795495478130716		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.039795495478130716 | validation: 0.027078166080070822]
	TIME [epoch: 38.5 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03260417349761702		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.03260417349761702 | validation: 0.04411203175278894]
	TIME [epoch: 38.5 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023432481793735783		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.023432481793735783 | validation: 0.021750583049775056]
	TIME [epoch: 38.5 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017879238247671885		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.017879238247671885 | validation: 0.02320478283785636]
	TIME [epoch: 38.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037062494867802206		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.037062494867802206 | validation: 0.02843348076810224]
	TIME [epoch: 38.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02269834148480754		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.02269834148480754 | validation: 0.02546431339297872]
	TIME [epoch: 38.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01697416278600795		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.01697416278600795 | validation: 0.019781104380730316]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01683098417457623		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.01683098417457623 | validation: 0.029019254208731426]
	TIME [epoch: 38.5 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04787193162857847		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.04787193162857847 | validation: 0.03655892756094686]
	TIME [epoch: 38.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03350793602947778		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.03350793602947778 | validation: 0.021909296867927194]
	TIME [epoch: 38.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019050617405355734		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.019050617405355734 | validation: 0.044974203048980754]
	TIME [epoch: 38.5 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0223569110593615		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.0223569110593615 | validation: 0.019635102551552472]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020321171885328594		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.020321171885328594 | validation: 0.02368685957170965]
	TIME [epoch: 38.4 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04173090108652457		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.04173090108652457 | validation: 0.021896832948225295]
	TIME [epoch: 38.5 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023077775850892972		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.023077775850892972 | validation: 0.030732277409158737]
	TIME [epoch: 38.4 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027675037754655026		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.027675037754655026 | validation: 0.0324888496556836]
	TIME [epoch: 38.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022339045383420707		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.022339045383420707 | validation: 0.01927753871231071]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01645033230793009		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.01645033230793009 | validation: 0.02106584793495798]
	TIME [epoch: 38.5 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024633104696692713		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.024633104696692713 | validation: 0.029329070120985887]
	TIME [epoch: 38.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03160915551875913		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.03160915551875913 | validation: 0.03292670472174644]
	TIME [epoch: 38.5 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020830629438102613		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.020830629438102613 | validation: 0.027031260736659072]
	TIME [epoch: 38.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02508882983253526		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.02508882983253526 | validation: 0.01907715153195814]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020950490521576583		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.020950490521576583 | validation: 0.027289726268718666]
	TIME [epoch: 38.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020433449029248464		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.020433449029248464 | validation: 0.021962849749230143]
	TIME [epoch: 38.4 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019188011815277756		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.019188011815277756 | validation: 0.026073263624297228]
	TIME [epoch: 38.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04751824216704182		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.04751824216704182 | validation: 0.05441537968289181]
	TIME [epoch: 38.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024364924254654884		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.024364924254654884 | validation: 0.028007362297964276]
	TIME [epoch: 38.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01579362189934373		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.01579362189934373 | validation: 0.0202674983198963]
	TIME [epoch: 38.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015733075619536175		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.015733075619536175 | validation: 0.03381039350916511]
	TIME [epoch: 38.4 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031717388532525294		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.031717388532525294 | validation: 0.04163971659874966]
	TIME [epoch: 38.4 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029559195431249484		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.029559195431249484 | validation: 0.031051513782807613]
	TIME [epoch: 38.5 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01953804561810099		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.01953804561810099 | validation: 0.023118190087250072]
	TIME [epoch: 38.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022453405488706134		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.022453405488706134 | validation: 0.029331833580221504]
	TIME [epoch: 38.5 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03037973930871624		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.03037973930871624 | validation: 0.04321460771462665]
	TIME [epoch: 38.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02731004594391816		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.02731004594391816 | validation: 0.022115903786551608]
	TIME [epoch: 38.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018487921988172326		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.018487921988172326 | validation: 0.022239875295831567]
	TIME [epoch: 38.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018860724763497025		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.018860724763497025 | validation: 0.027312593184383352]
	TIME [epoch: 38.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018423085428701975		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.018423085428701975 | validation: 0.03330140397267203]
	TIME [epoch: 38.5 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030250921030628555		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.030250921030628555 | validation: 0.0802687056784071]
	TIME [epoch: 38.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040110787011472145		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.040110787011472145 | validation: 0.023820095765410547]
	TIME [epoch: 38.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017564656140678737		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.017564656140678737 | validation: 0.021417771624978554]
	TIME [epoch: 38.5 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021441057441633125		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.021441057441633125 | validation: 0.01614112201975]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017775285080900104		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.017775285080900104 | validation: 0.02431009142463278]
	TIME [epoch: 38.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02209933305362194		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.02209933305362194 | validation: 0.026652267391836203]
	TIME [epoch: 38.5 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024773195024217702		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.024773195024217702 | validation: 0.019597208906716963]
	TIME [epoch: 38.4 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01475263030339712		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.01475263030339712 | validation: 0.01870706678220833]
	TIME [epoch: 38.3 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016882820383531963		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.016882820383531963 | validation: 0.031124747729305118]
	TIME [epoch: 38.4 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038914044828852364		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.038914044828852364 | validation: 0.022703088977133995]
	TIME [epoch: 38.4 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01621197237934742		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.01621197237934742 | validation: 0.02006573672297938]
	TIME [epoch: 38.3 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015489106179117407		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.015489106179117407 | validation: 0.017204285035841847]
	TIME [epoch: 38.4 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019043127514775823		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.019043127514775823 | validation: 0.01508772224359287]
	TIME [epoch: 38.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01662251943708921		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.01662251943708921 | validation: 0.020721844595810272]
	TIME [epoch: 38.4 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029889029567398626		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.029889029567398626 | validation: 0.03220308753177507]
	TIME [epoch: 38.4 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0597441896411604		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.0597441896411604 | validation: 0.04002692247184923]
	TIME [epoch: 38.4 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023661754409544065		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.023661754409544065 | validation: 0.020173200131607588]
	TIME [epoch: 38.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015066177937867457		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.015066177937867457 | validation: 0.017718906179633738]
	TIME [epoch: 38.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013155596831748034		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.013155596831748034 | validation: 0.01692865310451307]
	TIME [epoch: 38.4 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019925133687638655		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.019925133687638655 | validation: 0.026552042279055537]
	TIME [epoch: 38.5 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017406358761854458		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.017406358761854458 | validation: 0.0180336703238497]
	TIME [epoch: 38.4 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016744922345431595		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.016744922345431595 | validation: 0.04111744278439591]
	TIME [epoch: 38.4 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028543164070079674		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.028543164070079674 | validation: 0.017415381137752557]
	TIME [epoch: 38.4 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016816141180523842		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.016816141180523842 | validation: 0.03370259354335297]
	TIME [epoch: 38.4 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022463534109179412		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.022463534109179412 | validation: 0.02170596676697778]
	TIME [epoch: 38.4 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018490596712492043		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.018490596712492043 | validation: 0.016388807619156294]
	TIME [epoch: 38.4 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02159714857768117		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.02159714857768117 | validation: 0.02204602192072422]
	TIME [epoch: 38.5 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020527832073366163		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.020527832073366163 | validation: 0.031074729443436734]
	TIME [epoch: 38.5 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01822639498264985		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.01822639498264985 | validation: 0.018147434540200943]
	TIME [epoch: 38.5 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015204838820061135		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.015204838820061135 | validation: 0.022976167444189555]
	TIME [epoch: 38.4 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01543927160632358		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.01543927160632358 | validation: 0.029862061515960235]
	TIME [epoch: 38.4 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02486850367524864		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.02486850367524864 | validation: 0.025292560237400126]
	TIME [epoch: 38.5 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021931366129173988		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.021931366129173988 | validation: 0.028583549572035044]
	TIME [epoch: 38.4 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019862281572905798		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.019862281572905798 | validation: 0.014116448389570352]
	TIME [epoch: 38.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_574.pth
	Model improved!!!
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012089179165678659		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.012089179165678659 | validation: 0.017221310497218225]
	TIME [epoch: 38.5 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026959140521690753		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.026959140521690753 | validation: 0.025618667432677558]
	TIME [epoch: 38.4 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03655793554272452		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.03655793554272452 | validation: 0.06325985888729255]
	TIME [epoch: 38.4 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03385536073724797		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.03385536073724797 | validation: 0.030201057831285315]
	TIME [epoch: 38.4 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015689741005826954		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.015689741005826954 | validation: 0.016117916609547776]
	TIME [epoch: 38.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012723298037375316		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.012723298037375316 | validation: 0.016066153301463267]
	TIME [epoch: 38.4 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011773128872581484		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.011773128872581484 | validation: 0.015512019314334949]
	TIME [epoch: 38.5 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022274420776951562		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.022274420776951562 | validation: 0.026245967968119316]
	TIME [epoch: 38.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02154466305134933		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.02154466305134933 | validation: 0.01619698349690038]
	TIME [epoch: 38.4 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013884372628594798		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.013884372628594798 | validation: 0.016328170287894576]
	TIME [epoch: 38.5 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01505889181805405		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.01505889181805405 | validation: 0.016908918176969955]
	TIME [epoch: 38.4 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01986871272854586		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.01986871272854586 | validation: 0.024739196242769722]
	TIME [epoch: 38.4 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01777788767435752		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.01777788767435752 | validation: 0.016349025981175488]
	TIME [epoch: 38.4 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01472325118729054		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.01472325118729054 | validation: 0.017259283266557245]
	TIME [epoch: 38.5 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013795704466116035		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.013795704466116035 | validation: 0.02482701015276737]
	TIME [epoch: 38.5 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025023882659072752		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.025023882659072752 | validation: 0.01549729815091302]
	TIME [epoch: 38.4 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01429784448955643		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.01429784448955643 | validation: 0.015928744391103394]
	TIME [epoch: 38.4 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012228520510453264		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.012228520510453264 | validation: 0.017960199313834278]
	TIME [epoch: 38.4 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02354254130771614		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.02354254130771614 | validation: 0.024135399556446422]
	TIME [epoch: 38.4 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013458980503348601		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.013458980503348601 | validation: 0.01787910577100789]
	TIME [epoch: 38.3 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016810495134752287		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.016810495134752287 | validation: 0.016726355936811925]
	TIME [epoch: 38.4 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011625543032939598		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.011625543032939598 | validation: 0.026471489095060263]
	TIME [epoch: 38.4 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02048241242295349		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.02048241242295349 | validation: 0.0245556856975706]
	TIME [epoch: 38.4 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013677359135659258		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.013677359135659258 | validation: 0.017212498616014676]
	TIME [epoch: 38.4 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024607810922099863		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.024607810922099863 | validation: 0.02163251377187156]
	TIME [epoch: 38.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02474557895688065		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.02474557895688065 | validation: 0.03246509212934223]
	TIME [epoch: 38.4 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016422387731787256		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.016422387731787256 | validation: 0.018538024247541134]
	TIME [epoch: 38.4 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016222746079992027		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.016222746079992027 | validation: 0.014419393892059339]
	TIME [epoch: 38.4 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012380560826338606		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.012380560826338606 | validation: 0.01841458694807918]
	TIME [epoch: 38.4 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017581527676357997		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.017581527676357997 | validation: 0.016706514196463747]
	TIME [epoch: 38.4 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020216719941499407		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.020216719941499407 | validation: 0.022756128823145216]
	TIME [epoch: 38.4 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018247957985756843		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.018247957985756843 | validation: 0.015985058443080825]
	TIME [epoch: 38.5 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012295278731864714		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.012295278731864714 | validation: 0.014753521246090272]
	TIME [epoch: 38.4 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012352541681564504		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.012352541681564504 | validation: 0.02773987292146244]
	TIME [epoch: 38.4 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02109929581284917		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.02109929581284917 | validation: 0.013991481672539153]
	TIME [epoch: 38.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013708631643358105		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.013708631643358105 | validation: 0.01643091828155784]
	TIME [epoch: 38.4 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013257384389495709		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.013257384389495709 | validation: 0.021307711712003758]
	TIME [epoch: 38.4 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015114629839024335		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.015114629839024335 | validation: 0.036667780318605805]
	TIME [epoch: 38.4 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022215440243743383		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.022215440243743383 | validation: 0.015303978291707002]
	TIME [epoch: 38.4 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013656703534006307		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.013656703534006307 | validation: 0.019090579133739453]
	TIME [epoch: 38.4 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01621121010368359		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.01621121010368359 | validation: 0.03447740735411651]
	TIME [epoch: 38.4 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024059553875978267		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.024059553875978267 | validation: 0.014450879674239714]
	TIME [epoch: 38.4 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012819226800274822		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.012819226800274822 | validation: 0.019464626040998224]
	TIME [epoch: 38.4 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011606363549953518		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.011606363549953518 | validation: 0.013242797390620367]
	TIME [epoch: 38.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_618.pth
	Model improved!!!
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011257836660070207		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.011257836660070207 | validation: 0.02333027971215984]
	TIME [epoch: 38.2 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020355447386567502		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.020355447386567502 | validation: 0.011877536532286098]
	TIME [epoch: 38.3 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015118934918621985		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.015118934918621985 | validation: 0.02250456055552537]
	TIME [epoch: 38.3 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01611798496192264		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.01611798496192264 | validation: 0.015300278545511569]
	TIME [epoch: 38.3 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012692538716970327		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.012692538716970327 | validation: 0.021176908040272023]
	TIME [epoch: 38.2 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013575549246585111		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.013575549246585111 | validation: 0.015148100113247017]
	TIME [epoch: 38.4 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026161178423411948		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.026161178423411948 | validation: 0.01656515288130568]
	TIME [epoch: 38.4 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012686606544817021		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.012686606544817021 | validation: 0.014887035079548593]
	TIME [epoch: 38.3 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022166870510427112		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.022166870510427112 | validation: 0.013862054992551307]
	TIME [epoch: 38.4 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013733226756000028		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.013733226756000028 | validation: 0.013252664845579229]
	TIME [epoch: 38.3 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010122745932494999		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.010122745932494999 | validation: 0.012946552687543942]
	TIME [epoch: 38.2 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01180964235465437		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.01180964235465437 | validation: 0.02509456142690834]
	TIME [epoch: 38.4 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014323474856158085		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.014323474856158085 | validation: 0.02403247713012182]
	TIME [epoch: 38.4 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021129105411426485		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.021129105411426485 | validation: 0.015851614295259]
	TIME [epoch: 38.2 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011095416547523159		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.011095416547523159 | validation: 0.0156520446836011]
	TIME [epoch: 38.4 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016450270916513108		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.016450270916513108 | validation: 0.013670427225729326]
	TIME [epoch: 38.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011300808756929221		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.011300808756929221 | validation: 0.014149519265508548]
	TIME [epoch: 38.5 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012637528065895132		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.012637528065895132 | validation: 0.016617306171056032]
	TIME [epoch: 38.5 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013196992883101093		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.013196992883101093 | validation: 0.01734107064794326]
	TIME [epoch: 38.4 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02100621482674675		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.02100621482674675 | validation: 0.01907890808279996]
	TIME [epoch: 38.4 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015516919044012499		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.015516919044012499 | validation: 0.013695279123908933]
	TIME [epoch: 38.5 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0141080298363763		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.0141080298363763 | validation: 0.024024972955959553]
	TIME [epoch: 38.4 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01520891001908899		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.01520891001908899 | validation: 0.013236439845801966]
	TIME [epoch: 38.4 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01038790217775685		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.01038790217775685 | validation: 0.014694711299585366]
	TIME [epoch: 38.4 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015043060035665804		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.015043060035665804 | validation: 0.013680936468549582]
	TIME [epoch: 38.5 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009862988118209433		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.009862988118209433 | validation: 0.010769260938379533]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_644.pth
	Model improved!!!
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01438286993116961		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.01438286993116961 | validation: 0.03120598588518428]
	TIME [epoch: 38.3 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01816825022993053		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.01816825022993053 | validation: 0.012776146412523764]
	TIME [epoch: 38.1 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01073998242734371		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.01073998242734371 | validation: 0.01649760175728577]
	TIME [epoch: 38.2 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021271584841188837		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.021271584841188837 | validation: 0.04856924162528234]
	TIME [epoch: 38.2 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0307644812430799		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.0307644812430799 | validation: 0.012745338528004645]
	TIME [epoch: 38.2 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012085547301146008		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.012085547301146008 | validation: 0.011781340879130974]
	TIME [epoch: 38.2 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00929957466294247		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.00929957466294247 | validation: 0.013418495683686919]
	TIME [epoch: 38.3 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015660041540861182		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.015660041540861182 | validation: 0.018571106394989004]
	TIME [epoch: 38.2 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011626898853800414		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.011626898853800414 | validation: 0.016622196265831758]
	TIME [epoch: 38.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010733828149099518		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.010733828149099518 | validation: 0.013195771289569645]
	TIME [epoch: 38.2 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013423663695743316		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.013423663695743316 | validation: 0.0181830680442024]
	TIME [epoch: 38.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014224901924245643		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.014224901924245643 | validation: 0.012046866632780824]
	TIME [epoch: 38.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011153031314633402		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.011153031314633402 | validation: 0.016561051079629092]
	TIME [epoch: 38.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010580882463729511		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.010580882463729511 | validation: 0.011739518410577132]
	TIME [epoch: 38.3 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012403657880981387		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.012403657880981387 | validation: 0.01413534216887493]
	TIME [epoch: 38.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013400007288867834		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.013400007288867834 | validation: 0.013318376715731453]
	TIME [epoch: 38.2 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012029817251933517		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.012029817251933517 | validation: 0.012736444287940536]
	TIME [epoch: 38.3 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011916152952508528		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.011916152952508528 | validation: 0.014031383954537496]
	TIME [epoch: 38.2 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010605254060520044		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.010605254060520044 | validation: 0.016048308892353823]
	TIME [epoch: 38.1 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015548100775109725		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.015548100775109725 | validation: 0.016950187629289442]
	TIME [epoch: 38.2 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011563125024930876		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.011563125024930876 | validation: 0.01495525297377234]
	TIME [epoch: 38.2 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010368574942834706		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.010368574942834706 | validation: 0.012872123340449413]
	TIME [epoch: 38.2 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0130124267727889		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.0130124267727889 | validation: 0.01311348456875044]
	TIME [epoch: 38.2 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01166118772293145		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.01166118772293145 | validation: 0.013261201806791828]
	TIME [epoch: 38.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01599126823648013		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.01599126823648013 | validation: 0.010758849745993372]
	TIME [epoch: 38.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009577745773399916		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.009577745773399916 | validation: 0.012009281732621764]
	TIME [epoch: 38.2 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009791830337971725		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.009791830337971725 | validation: 0.029221973914449362]
	TIME [epoch: 38.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015823888263337246		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.015823888263337246 | validation: 0.01879536759481562]
	TIME [epoch: 38.5 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011493571140543352		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.011493571140543352 | validation: 0.012448702625715912]
	TIME [epoch: 38.4 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01007564849909836		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.01007564849909836 | validation: 0.012579951047104174]
	TIME [epoch: 38.5 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017399720099667663		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.017399720099667663 | validation: 0.015204396595225284]
	TIME [epoch: 38.4 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01138142113357952		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.01138142113357952 | validation: 0.011268067912807104]
	TIME [epoch: 38.5 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010147865430209463		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.010147865430209463 | validation: 0.024235450994661536]
	TIME [epoch: 38.5 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015077267550091793		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.015077267550091793 | validation: 0.015761636266252335]
	TIME [epoch: 38.5 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010780056873362358		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.010780056873362358 | validation: 0.011777051531520762]
	TIME [epoch: 38.5 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009663306798509415		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.009663306798509415 | validation: 0.016727933669626757]
	TIME [epoch: 38.5 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013766058392106498		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.013766058392106498 | validation: 0.010089270266246338]
	TIME [epoch: 38.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_681.pth
	Model improved!!!
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008837307384634653		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.008837307384634653 | validation: 0.009580773757429061]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011489337571472788		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.011489337571472788 | validation: 0.012906861068909484]
	TIME [epoch: 38.5 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01012088438370263		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.01012088438370263 | validation: 0.012444731849231398]
	TIME [epoch: 38.5 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010126130015098406		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.010126130015098406 | validation: 0.013633482097311318]
	TIME [epoch: 38.5 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011899747724108432		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.011899747724108432 | validation: 0.015879262921917648]
	TIME [epoch: 38.5 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010926396419033209		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.010926396419033209 | validation: 0.012304028745876128]
	TIME [epoch: 38.5 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009138374961748113		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.009138374961748113 | validation: 0.023168764027751115]
	TIME [epoch: 38.5 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017139684378518098		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.017139684378518098 | validation: 0.01606147421534623]
	TIME [epoch: 38.5 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012190068806975329		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.012190068806975329 | validation: 0.013674349204841894]
	TIME [epoch: 38.5 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00849193639615918		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.00849193639615918 | validation: 0.01611177046343606]
	TIME [epoch: 38.5 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013334570647888611		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.013334570647888611 | validation: 0.01343102682461094]
	TIME [epoch: 38.5 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009096326786075545		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.009096326786075545 | validation: 0.010434257224385953]
	TIME [epoch: 38.5 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007868227120824764		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.007868227120824764 | validation: 0.012422542546095788]
	TIME [epoch: 38.5 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018013498544124887		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.018013498544124887 | validation: 0.01520294580427301]
	TIME [epoch: 38.5 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01001519009264699		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.01001519009264699 | validation: 0.012532402798935655]
	TIME [epoch: 38.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009196287885705028		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.009196287885705028 | validation: 0.011730011561639497]
	TIME [epoch: 38.5 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009175261752663592		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.009175261752663592 | validation: 0.012344748110606117]
	TIME [epoch: 38.5 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013808202622155475		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.013808202622155475 | validation: 0.01311241928912894]
	TIME [epoch: 38.5 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012365243476246617		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.012365243476246617 | validation: 0.010793135614860921]
	TIME [epoch: 38.5 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008604639973207696		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.008604639973207696 | validation: 0.021967554399169708]
	TIME [epoch: 38.5 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011676544965635296		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.011676544965635296 | validation: 0.009167804983460195]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_702.pth
	Model improved!!!
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008721923782388487		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.008721923782388487 | validation: 0.014574920713640725]
	TIME [epoch: 38.5 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009418646047434977		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.009418646047434977 | validation: 0.010130964049769249]
	TIME [epoch: 38.5 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009859991983652186		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.009859991983652186 | validation: 0.012838928032450462]
	TIME [epoch: 38.5 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0109649314893254		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.0109649314893254 | validation: 0.010294655065767584]
	TIME [epoch: 38.5 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009805972090017141		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.009805972090017141 | validation: 0.010021650083835588]
	TIME [epoch: 38.5 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008162687430378686		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.008162687430378686 | validation: 0.014005365650583475]
	TIME [epoch: 38.5 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013194180555301794		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.013194180555301794 | validation: 0.014334088245118056]
	TIME [epoch: 38.5 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012949265478858392		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.012949265478858392 | validation: 0.010229722097475837]
	TIME [epoch: 38.5 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008096869968683213		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.008096869968683213 | validation: 0.012295049775997652]
	TIME [epoch: 38.5 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00959682899302911		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.00959682899302911 | validation: 0.019004622872049674]
	TIME [epoch: 38.5 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010334554839664088		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.010334554839664088 | validation: 0.013815723172790397]
	TIME [epoch: 38.5 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009010876035451131		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.009010876035451131 | validation: 0.015513006425629427]
	TIME [epoch: 38.5 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012054115462389468		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.012054115462389468 | validation: 0.019299666534633296]
	TIME [epoch: 38.5 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010670080150695128		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.010670080150695128 | validation: 0.010374071918433888]
	TIME [epoch: 38.5 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007689751343888133		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.007689751343888133 | validation: 0.01151671440114754]
	TIME [epoch: 38.5 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010825534641195858		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.010825534641195858 | validation: 0.010980347296360393]
	TIME [epoch: 38.5 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011471294605479384		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.011471294605479384 | validation: 0.012208166593925533]
	TIME [epoch: 38.5 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009480631757413475		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.009480631757413475 | validation: 0.009322938176918504]
	TIME [epoch: 38.5 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0073622693693373735		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.0073622693693373735 | validation: 0.010945130883591775]
	TIME [epoch: 38.5 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010597520216443276		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.010597520216443276 | validation: 0.01122521795869079]
	TIME [epoch: 38.5 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008163620576151887		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.008163620576151887 | validation: 0.011603717018072408]
	TIME [epoch: 38.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008729581150796748		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.008729581150796748 | validation: 0.03136046729554231]
	TIME [epoch: 38.5 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017127147108366915		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.017127147108366915 | validation: 0.01406311902711627]
	TIME [epoch: 38.4 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009575327393438157		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.009575327393438157 | validation: 0.00998903765714588]
	TIME [epoch: 38.5 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0075225894476688236		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.0075225894476688236 | validation: 0.010504202808211871]
	TIME [epoch: 38.5 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007858442010499025		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.007858442010499025 | validation: 0.012071820792356797]
	TIME [epoch: 38.5 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010574497201002672		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.010574497201002672 | validation: 0.009677015430636489]
	TIME [epoch: 38.5 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007818894973322568		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.007818894973322568 | validation: 0.009241870416620284]
	TIME [epoch: 38.5 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076163343091110326		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.0076163343091110326 | validation: 0.012183940930772116]
	TIME [epoch: 38.5 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011767399314703064		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.011767399314703064 | validation: 0.01046542383015159]
	TIME [epoch: 38.5 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007875268162323154		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.007875268162323154 | validation: 0.011537458825199898]
	TIME [epoch: 38.4 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008044653340824465		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.008044653340824465 | validation: 0.014355134987836186]
	TIME [epoch: 38.5 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014858661267209788		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.014858661267209788 | validation: 0.013665180370471122]
	TIME [epoch: 38.5 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010763319893191719		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.010763319893191719 | validation: 0.013178641030851222]
	TIME [epoch: 38.5 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00792517447089306		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.00792517447089306 | validation: 0.00992596913914623]
	TIME [epoch: 38.5 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008548881788683614		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.008548881788683614 | validation: 0.010041123749456505]
	TIME [epoch: 38.5 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008545638694763148		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.008545638694763148 | validation: 0.01145981160966181]
	TIME [epoch: 38.5 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01048756357045844		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.01048756357045844 | validation: 0.011045087012019623]
	TIME [epoch: 38.5 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009317518122380092		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.009317518122380092 | validation: 0.009547901160463036]
	TIME [epoch: 38.5 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007531116069347459		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.007531116069347459 | validation: 0.009846822222705845]
	TIME [epoch: 38.5 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011882541003917902		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.011882541003917902 | validation: 0.009586331220509382]
	TIME [epoch: 38.5 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00877172037395581		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.00877172037395581 | validation: 0.01261236246344908]
	TIME [epoch: 38.5 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009798516940192816		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.009798516940192816 | validation: 0.010008817965600034]
	TIME [epoch: 38.5 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009345584575458305		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.009345584575458305 | validation: 0.014279948032521006]
	TIME [epoch: 38.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009103186923960555		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.009103186923960555 | validation: 0.011068089651911755]
	TIME [epoch: 38.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009887043624410457		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.009887043624410457 | validation: 0.008859444078529547]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007382185860038455		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.007382185860038455 | validation: 0.027625021713147298]
	TIME [epoch: 38.4 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015310532639669793		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.015310532639669793 | validation: 0.009579307353824618]
	TIME [epoch: 38.5 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008667280347769177		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.008667280347769177 | validation: 0.012243304921356184]
	TIME [epoch: 38.5 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00898349858113958		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.00898349858113958 | validation: 0.009029981179061806]
	TIME [epoch: 38.5 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006841985767427105		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.006841985767427105 | validation: 0.011998979411169983]
	TIME [epoch: 38.4 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016659251557159728		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.016659251557159728 | validation: 0.01426688770254396]
	TIME [epoch: 38.5 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010805088339654682		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.010805088339654682 | validation: 0.00759177997887742]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006891558676460137		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.006891558676460137 | validation: 0.00913535668999823]
	TIME [epoch: 38.5 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007218034998283303		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.007218034998283303 | validation: 0.010517807602856897]
	TIME [epoch: 38.5 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007355092029096783		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.007355092029096783 | validation: 0.009810095936267966]
	TIME [epoch: 38.5 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0067182453064664515		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.0067182453064664515 | validation: 0.009359218207441606]
	TIME [epoch: 38.5 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014077226616697486		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.014077226616697486 | validation: 0.0087241085885771]
	TIME [epoch: 38.5 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007691074054544597		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.007691074054544597 | validation: 0.009051469313999291]
	TIME [epoch: 38.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007378213659824243		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.007378213659824243 | validation: 0.007598709693239198]
	TIME [epoch: 38.5 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006414354370820125		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.006414354370820125 | validation: 0.008265703536081206]
	TIME [epoch: 38.5 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01027755166363365		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.01027755166363365 | validation: 0.01564202724119995]
	TIME [epoch: 38.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009174246543434099		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.009174246543434099 | validation: 0.008846371377742971]
	TIME [epoch: 38.5 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006928184503114479		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.006928184503114479 | validation: 0.008882142492436244]
	TIME [epoch: 38.4 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009173261739067549		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.009173261739067549 | validation: 0.009029874243396532]
	TIME [epoch: 38.5 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007787278045569005		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.007787278045569005 | validation: 0.010930287751876135]
	TIME [epoch: 38.5 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072352047656519085		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.0072352047656519085 | validation: 0.01115779713043334]
	TIME [epoch: 38.5 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007420764931435347		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.007420764931435347 | validation: 0.01068869734806568]
	TIME [epoch: 38.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009304185771000574		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.009304185771000574 | validation: 0.010279044714026152]
	TIME [epoch: 38.5 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00687164450773931		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.00687164450773931 | validation: 0.01137139356807022]
	TIME [epoch: 38.4 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00913844377992607		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.00913844377992607 | validation: 0.00912686652496185]
	TIME [epoch: 38.5 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008009660204415577		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.008009660204415577 | validation: 0.011372795166937844]
	TIME [epoch: 38.5 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00869344826477263		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.00869344826477263 | validation: 0.007202440049864131]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_775.pth
	Model improved!!!
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006888170669937205		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.006888170669937205 | validation: 0.010256156785770763]
	TIME [epoch: 38.4 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008688832018519823		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.008688832018519823 | validation: 0.009719537916486075]
	TIME [epoch: 38.4 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00731881235055434		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.00731881235055434 | validation: 0.012722994748945969]
	TIME [epoch: 38.4 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007836936839264001		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.007836936839264001 | validation: 0.009585178733966061]
	TIME [epoch: 38.4 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006195785694732928		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.006195785694732928 | validation: 0.010574420424925398]
	TIME [epoch: 38.4 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00998770818296755		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.00998770818296755 | validation: 0.012050244250056647]
	TIME [epoch: 38.4 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007956838231823067		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.007956838231823067 | validation: 0.007721876503008519]
	TIME [epoch: 38.4 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006858452464684958		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.006858452464684958 | validation: 0.009430030338964515]
	TIME [epoch: 38.4 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008228121271269016		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.008228121271269016 | validation: 0.01161995355266125]
	TIME [epoch: 38.4 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007895968640168132		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.007895968640168132 | validation: 0.010173188625393868]
	TIME [epoch: 38.4 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008752028194253983		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.008752028194253983 | validation: 0.012106653827805099]
	TIME [epoch: 38.4 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008111192944027942		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.008111192944027942 | validation: 0.009059162231705187]
	TIME [epoch: 38.4 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00653436970044279		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.00653436970044279 | validation: 0.010241281717399452]
	TIME [epoch: 38.4 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007121387171308567		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.007121387171308567 | validation: 0.013540886221734272]
	TIME [epoch: 38.4 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009647159375029114		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.009647159375029114 | validation: 0.008073488900180023]
	TIME [epoch: 38.4 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0069753545991190725		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.0069753545991190725 | validation: 0.009519157380478399]
	TIME [epoch: 38.4 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007324257074960859		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.007324257074960859 | validation: 0.01248570955246512]
	TIME [epoch: 38.4 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011773517235866685		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.011773517235866685 | validation: 0.007745669436216636]
	TIME [epoch: 38.4 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007032674108020795		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.007032674108020795 | validation: 0.008957637109530138]
	TIME [epoch: 38.4 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006815598386768613		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.006815598386768613 | validation: 0.008807653385844383]
	TIME [epoch: 38.4 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006840344730641054		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.006840344730641054 | validation: 0.0103892590514345]
	TIME [epoch: 38.4 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00789844030220071		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.00789844030220071 | validation: 0.0077311951967715]
	TIME [epoch: 38.4 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072680483891500135		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.0072680483891500135 | validation: 0.0077854690803850915]
	TIME [epoch: 38.4 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007183888426070576		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.007183888426070576 | validation: 0.007001762558353311]
	TIME [epoch: 38.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_799.pth
	Model improved!!!
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0065785589655584665		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.0065785589655584665 | validation: 0.009345939467511766]
	TIME [epoch: 38.4 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007538850085375278		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.007538850085375278 | validation: 0.008972768868756824]
	TIME [epoch: 38.4 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008779597037368546		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.008779597037368546 | validation: 0.00764188398353418]
	TIME [epoch: 38.4 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006044951113982777		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.006044951113982777 | validation: 0.008015187178426258]
	TIME [epoch: 38.4 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007004675074343554		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.007004675074343554 | validation: 0.007810545733698739]
	TIME [epoch: 38.4 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008948095190149897		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.008948095190149897 | validation: 0.011315988879257215]
	TIME [epoch: 38.4 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008002197804348203		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.008002197804348203 | validation: 0.006976598443140464]
	TIME [epoch: 38.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_806.pth
	Model improved!!!
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006015928584296462		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.006015928584296462 | validation: 0.007382697667752161]
	TIME [epoch: 38.4 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008210301844163881		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.008210301844163881 | validation: 0.007997041844431686]
	TIME [epoch: 38.4 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006329625307673567		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.006329625307673567 | validation: 0.008583098120563288]
	TIME [epoch: 38.4 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006050448869587226		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.006050448869587226 | validation: 0.010981288128273151]
	TIME [epoch: 38.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006897600772651278		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.006897600772651278 | validation: 0.009236949489720052]
	TIME [epoch: 38.4 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008811894836462487		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.008811894836462487 | validation: 0.008488594853222226]
	TIME [epoch: 38.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007238284798214092		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.007238284798214092 | validation: 0.007629662915559212]
	TIME [epoch: 38.4 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006777837436525434		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.006777837436525434 | validation: 0.007963468802305338]
	TIME [epoch: 38.4 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0059997160987517615		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.0059997160987517615 | validation: 0.010544947099349875]
	TIME [epoch: 38.4 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0069543254189850705		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.0069543254189850705 | validation: 0.007082932827872973]
	TIME [epoch: 38.4 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008264884294582955		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.008264884294582955 | validation: 0.010431002890243712]
	TIME [epoch: 38.4 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007610483755155365		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.007610483755155365 | validation: 0.007006172640418501]
	TIME [epoch: 38.4 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006316730999970038		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.006316730999970038 | validation: 0.007771313738499641]
	TIME [epoch: 38.4 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006344957479328588		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.006344957479328588 | validation: 0.008999037535377983]
	TIME [epoch: 38.4 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008657261715047992		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.008657261715047992 | validation: 0.012965416348649887]
	TIME [epoch: 38.4 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007604479565412859		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.007604479565412859 | validation: 0.007914647525167825]
	TIME [epoch: 38.4 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0067133789055826955		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.0067133789055826955 | validation: 0.018756732943656623]
	TIME [epoch: 38.4 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009012599541657592		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.009012599541657592 | validation: 0.008008200779651704]
	TIME [epoch: 38.4 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006902374638473156		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.006902374638473156 | validation: 0.008118943118957483]
	TIME [epoch: 38.4 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005611136943345236		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.005611136943345236 | validation: 0.006540511546902315]
	TIME [epoch: 38.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053160756289133025		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.0053160756289133025 | validation: 0.006830106865460608]
	TIME [epoch: 38.4 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0070378699225676885		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.0070378699225676885 | validation: 0.00729857411699217]
	TIME [epoch: 38.4 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006406414971428255		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.006406414971428255 | validation: 0.01123833455462515]
	TIME [epoch: 38.4 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00707501418030457		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.00707501418030457 | validation: 0.007173308513148264]
	TIME [epoch: 38.4 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005654629020159134		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.005654629020159134 | validation: 0.008376616893915986]
	TIME [epoch: 38.4 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006032325226031315		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.006032325226031315 | validation: 0.007328003074376591]
	TIME [epoch: 38.4 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00837074659500605		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.00837074659500605 | validation: 0.008599304579065697]
	TIME [epoch: 38.4 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007199186679975756		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.007199186679975756 | validation: 0.0073800770875715235]
	TIME [epoch: 38.4 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005413832417931367		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.005413832417931367 | validation: 0.006936848068113687]
	TIME [epoch: 38.4 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005598363665448127		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.005598363665448127 | validation: 0.008553634946682449]
	TIME [epoch: 38.4 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006598426266536348		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.006598426266536348 | validation: 0.008674148083530658]
	TIME [epoch: 38.4 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007223575254463292		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.007223575254463292 | validation: 0.0064992740393915085]
	TIME [epoch: 38.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_838.pth
	Model improved!!!
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00585541262243499		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.00585541262243499 | validation: 0.011864796966611534]
	TIME [epoch: 38.4 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006631673730837466		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.006631673730837466 | validation: 0.010336900326162909]
	TIME [epoch: 38.4 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00663712628970034		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.00663712628970034 | validation: 0.007816170751590072]
	TIME [epoch: 38.4 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054479132848894146		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.0054479132848894146 | validation: 0.01356871461780269]
	TIME [epoch: 38.4 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007956192050628353		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.007956192050628353 | validation: 0.006801585401101578]
	TIME [epoch: 38.4 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00564796773307286		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.00564796773307286 | validation: 0.00700016322913336]
	TIME [epoch: 38.4 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005377579543216585		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.005377579543216585 | validation: 0.0074301533851154076]
	TIME [epoch: 38.4 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006366491833353897		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.006366491833353897 | validation: 0.007726394523624391]
	TIME [epoch: 38.4 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005763315326260559		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.005763315326260559 | validation: 0.006613273858763511]
	TIME [epoch: 38.4 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0066368546858957675		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.0066368546858957675 | validation: 0.009863165025119075]
	TIME [epoch: 38.4 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006790553288884156		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.006790553288884156 | validation: 0.006710272381761552]
	TIME [epoch: 38.4 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009048788145917494		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.009048788145917494 | validation: 0.01199664635986472]
	TIME [epoch: 38.5 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007349064539163737		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.007349064539163737 | validation: 0.00689272212091555]
	TIME [epoch: 38.4 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005615814191221617		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.005615814191221617 | validation: 0.006074520716546498]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_852.pth
	Model improved!!!
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005717379238654721		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.005717379238654721 | validation: 0.009495884701117147]
	TIME [epoch: 38.5 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007004976241755353		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.007004976241755353 | validation: 0.0080844917825387]
	TIME [epoch: 38.4 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0063868210185646455		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.0063868210185646455 | validation: 0.006295090596350677]
	TIME [epoch: 38.5 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004988487232211834		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.004988487232211834 | validation: 0.008306191768992548]
	TIME [epoch: 38.4 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00792298526973402		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.00792298526973402 | validation: 0.006195787367740743]
	TIME [epoch: 38.5 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005460887219064094		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.005460887219064094 | validation: 0.006204242229186522]
	TIME [epoch: 38.4 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006303628117533336		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.006303628117533336 | validation: 0.006766432261038334]
	TIME [epoch: 38.4 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0062426952121405225		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.0062426952121405225 | validation: 0.006676412966568958]
	TIME [epoch: 38.4 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006569013416175256		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.006569013416175256 | validation: 0.0077220461160697525]
	TIME [epoch: 38.4 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005587595554050745		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.005587595554050745 | validation: 0.006316538011059436]
	TIME [epoch: 38.5 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00517301268383908		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.00517301268383908 | validation: 0.007638408942426616]
	TIME [epoch: 38.4 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005717997872356747		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.005717997872356747 | validation: 0.010732460764285811]
	TIME [epoch: 38.4 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007840251633871465		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.007840251633871465 | validation: 0.007685408416619735]
	TIME [epoch: 38.5 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006583118294043649		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.006583118294043649 | validation: 0.006210853411534625]
	TIME [epoch: 38.4 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005589939201406373		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.005589939201406373 | validation: 0.006153367145870766]
	TIME [epoch: 38.5 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005237017543620147		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.005237017543620147 | validation: 0.00816186160564455]
	TIME [epoch: 38.4 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005781240372535681		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.005781240372535681 | validation: 0.007528386564445143]
	TIME [epoch: 38.4 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005745133650875102		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.005745133650875102 | validation: 0.007999110635809173]
	TIME [epoch: 38.4 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007029255739439081		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.007029255739439081 | validation: 0.006515579338685171]
	TIME [epoch: 38.5 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005587919467414151		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.005587919467414151 | validation: 0.007527597954442344]
	TIME [epoch: 38.4 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006161153908020835		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.006161153908020835 | validation: 0.006698301509333888]
	TIME [epoch: 38.4 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005353796149058613		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.005353796149058613 | validation: 0.006391393454970484]
	TIME [epoch: 38.4 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005153391262147755		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.005153391262147755 | validation: 0.00665973300156643]
	TIME [epoch: 38.4 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005378342571357097		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.005378342571357097 | validation: 0.008153035657841024]
	TIME [epoch: 38.4 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005515416657960366		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.005515416657960366 | validation: 0.008019876759046236]
	TIME [epoch: 38.4 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005829519353631532		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.005829519353631532 | validation: 0.007665390647810005]
	TIME [epoch: 38.4 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072913202971542565		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.0072913202971542565 | validation: 0.006766300076314256]
	TIME [epoch: 38.4 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00540499510895703		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.00540499510895703 | validation: 0.006561473152230225]
	TIME [epoch: 38.4 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005227839890975019		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.005227839890975019 | validation: 0.008567807311954358]
	TIME [epoch: 38.4 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00523278903435986		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.00523278903435986 | validation: 0.007921115404956386]
	TIME [epoch: 38.4 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053806035846516945		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.0053806035846516945 | validation: 0.00650742569846127]
	TIME [epoch: 38.4 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005718270453519182		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.005718270453519182 | validation: 0.006490875681954089]
	TIME [epoch: 38.4 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005050369459814588		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.005050369459814588 | validation: 0.007161366409193071]
	TIME [epoch: 38.4 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006188986983461097		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.006188986983461097 | validation: 0.00593431540723615]
	TIME [epoch: 38.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_886.pth
	Model improved!!!
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048574465318312		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.0048574465318312 | validation: 0.006329381033196318]
	TIME [epoch: 38.4 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005798997288749197		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.005798997288749197 | validation: 0.009173840416510388]
	TIME [epoch: 38.4 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005960937616611458		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.005960937616611458 | validation: 0.0065537619516435495]
	TIME [epoch: 38.4 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006394717473626823		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.006394717473626823 | validation: 0.007183197583565169]
	TIME [epoch: 38.4 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053639299584548605		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.0053639299584548605 | validation: 0.007262879437444114]
	TIME [epoch: 38.4 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005186076614998187		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.005186076614998187 | validation: 0.005752124502183233]
	TIME [epoch: 38.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_892.pth
	Model improved!!!
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057628498170557325		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.0057628498170557325 | validation: 0.007568156648999482]
	TIME [epoch: 38.4 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005528018857516552		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.005528018857516552 | validation: 0.0064085818331007655]
	TIME [epoch: 38.4 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004641606572574358		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.004641606572574358 | validation: 0.006843217467877388]
	TIME [epoch: 38.4 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005605743561485852		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.005605743561485852 | validation: 0.0069912290139037695]
	TIME [epoch: 38.4 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005586582241781066		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.005586582241781066 | validation: 0.005409327133571776]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_897.pth
	Model improved!!!
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005589814254261746		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.005589814254261746 | validation: 0.009413493204149366]
	TIME [epoch: 38.4 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005643212234059808		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.005643212234059808 | validation: 0.006154604217074464]
	TIME [epoch: 38.4 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051765791990037065		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.0051765791990037065 | validation: 0.008149552541286516]
	TIME [epoch: 38.4 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004727298588688424		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.004727298588688424 | validation: 0.005894527829174528]
	TIME [epoch: 38.4 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005200132233627773		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.005200132233627773 | validation: 0.0066296641775382085]
	TIME [epoch: 38.5 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050879251353423745		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.0050879251353423745 | validation: 0.00847460167130228]
	TIME [epoch: 38.4 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009337083807457415		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.009337083807457415 | validation: 0.008615410747512358]
	TIME [epoch: 38.5 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00556614744346168		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.00556614744346168 | validation: 0.006907662795022586]
	TIME [epoch: 38.5 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0064228008457340394		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.0064228008457340394 | validation: 0.007737276121855942]
	TIME [epoch: 38.4 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005075017364851699		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.005075017364851699 | validation: 0.0057301148682239456]
	TIME [epoch: 38.5 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005709414633919061		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.005709414633919061 | validation: 0.004864325942688124]
	TIME [epoch: 38.4 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_908.pth
	Model improved!!!
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044799555153842135		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.0044799555153842135 | validation: 0.005957089681456895]
	TIME [epoch: 38.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005523895586362986		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.005523895586362986 | validation: 0.0070238569231622396]
	TIME [epoch: 38.5 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006093592868570036		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.006093592868570036 | validation: 0.007001286006320976]
	TIME [epoch: 38.5 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005202745976274628		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.005202745976274628 | validation: 0.005743174040019266]
	TIME [epoch: 38.5 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004660647815087698		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.004660647815087698 | validation: 0.005494093326647665]
	TIME [epoch: 38.5 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004932175206193129		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.004932175206193129 | validation: 0.007332564971123204]
	TIME [epoch: 38.5 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004885497870308345		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.004885497870308345 | validation: 0.0063215097775301664]
	TIME [epoch: 38.5 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005240157151225416		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.005240157151225416 | validation: 0.007436110152124767]
	TIME [epoch: 38.5 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004546871108013495		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.004546871108013495 | validation: 0.005100001998572692]
	TIME [epoch: 38.5 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005834892369900614		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.005834892369900614 | validation: 0.006140167816813505]
	TIME [epoch: 38.5 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044081590919441554		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.0044081590919441554 | validation: 0.006514604977431032]
	TIME [epoch: 38.5 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005200744076007057		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.005200744076007057 | validation: 0.0051144161294786415]
	TIME [epoch: 38.5 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005055887848795584		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.005055887848795584 | validation: 0.006459065882485411]
	TIME [epoch: 38.5 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004493159794927579		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.004493159794927579 | validation: 0.006608241608011381]
	TIME [epoch: 38.5 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006003955396304253		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.006003955396304253 | validation: 0.007727090927432428]
	TIME [epoch: 38.5 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005443931155667384		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.005443931155667384 | validation: 0.006604025065417292]
	TIME [epoch: 38.4 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048499844945499564		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.0048499844945499564 | validation: 0.006404924468194381]
	TIME [epoch: 38.4 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049774138577046755		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.0049774138577046755 | validation: 0.005525634766536059]
	TIME [epoch: 38.4 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004641630580162668		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.004641630580162668 | validation: 0.006056603755688911]
	TIME [epoch: 38.5 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0058026005803797735		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.0058026005803797735 | validation: 0.0068270636864792705]
	TIME [epoch: 38.5 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004467647188198897		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.004467647188198897 | validation: 0.00506665435020226]
	TIME [epoch: 38.5 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004166413868650855		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.004166413868650855 | validation: 0.006402889892732349]
	TIME [epoch: 38.4 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006370530102090082		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.006370530102090082 | validation: 0.007658219975133629]
	TIME [epoch: 38.5 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005196500260124098		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.005196500260124098 | validation: 0.005241890772624175]
	TIME [epoch: 38.4 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004390310904823724		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.004390310904823724 | validation: 0.006653387084757297]
	TIME [epoch: 38.5 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004848195908539662		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.004848195908539662 | validation: 0.0050961887684583705]
	TIME [epoch: 38.5 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004616211185667827		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.004616211185667827 | validation: 0.00603099147353771]
	TIME [epoch: 38.5 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005666510013152089		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.005666510013152089 | validation: 0.005726275877959999]
	TIME [epoch: 38.5 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004912653622131942		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.004912653622131942 | validation: 0.005567635834519758]
	TIME [epoch: 38.5 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004772981148815528		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.004772981148815528 | validation: 0.005708773564170591]
	TIME [epoch: 38.5 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056018147128600254		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.0056018147128600254 | validation: 0.0066211968876284215]
	TIME [epoch: 38.5 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005046902008519941		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.005046902008519941 | validation: 0.005453491184421364]
	TIME [epoch: 38.4 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004141759399320924		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.004141759399320924 | validation: 0.006354176773852488]
	TIME [epoch: 38.5 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005848552597910309		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.005848552597910309 | validation: 0.006023176372001828]
	TIME [epoch: 38.4 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005133785804955059		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.005133785804955059 | validation: 0.005889162636372886]
	TIME [epoch: 38.5 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004643688278632802		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.004643688278632802 | validation: 0.005271477145189339]
	TIME [epoch: 38.5 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004340511618706327		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.004340511618706327 | validation: 0.005181031561353157]
	TIME [epoch: 38.5 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004893708523932547		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.004893708523932547 | validation: 0.006275645661454933]
	TIME [epoch: 38.5 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004403530699560221		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.004403530699560221 | validation: 0.006801653893389797]
	TIME [epoch: 38.5 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004959117482229892		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.004959117482229892 | validation: 0.0062878328325449255]
	TIME [epoch: 38.5 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005454158892177784		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.005454158892177784 | validation: 0.007729234320245828]
	TIME [epoch: 38.5 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004557546052558583		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.004557546052558583 | validation: 0.005191783048381471]
	TIME [epoch: 38.5 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004170789054981008		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.004170789054981008 | validation: 0.005542841743580277]
	TIME [epoch: 38.5 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004144214016911955		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.004144214016911955 | validation: 0.004880951954033595]
	TIME [epoch: 38.5 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004749693952265576		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.004749693952265576 | validation: 0.006598470713935867]
	TIME [epoch: 38.5 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047001385829254505		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.0047001385829254505 | validation: 0.005281571595430926]
	TIME [epoch: 38.4 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004391153541406736		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.004391153541406736 | validation: 0.006213825238577747]
	TIME [epoch: 38.5 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004982286728278811		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.004982286728278811 | validation: 0.00681890174779292]
	TIME [epoch: 38.5 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005250122700000906		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.005250122700000906 | validation: 0.008403719494671051]
	TIME [epoch: 38.5 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005616560529018864		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.005616560529018864 | validation: 0.0053989630275243505]
	TIME [epoch: 38.5 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004297145510962495		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.004297145510962495 | validation: 0.005982318746794179]
	TIME [epoch: 38.5 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004340178221366698		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.004340178221366698 | validation: 0.005488668388309483]
	TIME [epoch: 38.4 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00524266222615319		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.00524266222615319 | validation: 0.00699590484306767]
	TIME [epoch: 38.5 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005022417160373543		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.005022417160373543 | validation: 0.005983332268067457]
	TIME [epoch: 38.5 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004496031086743098		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.004496031086743098 | validation: 0.005091212142684045]
	TIME [epoch: 38.5 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040159773339075965		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.0040159773339075965 | validation: 0.007864985668343928]
	TIME [epoch: 38.4 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00474371924899289		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.00474371924899289 | validation: 0.005097955906931278]
	TIME [epoch: 38.5 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043369228672245725		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.0043369228672245725 | validation: 0.006351145911352752]
	TIME [epoch: 38.5 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047073972546843946		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.0047073972546843946 | validation: 0.006012677230480575]
	TIME [epoch: 38.5 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0059898251582622365		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.0059898251582622365 | validation: 0.0064748748241390515]
	TIME [epoch: 38.5 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057577762857202175		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.0057577762857202175 | validation: 0.005924592856359052]
	TIME [epoch: 38.5 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040841362829775844		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.0040841362829775844 | validation: 0.005902989649215248]
	TIME [epoch: 38.5 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004192398087092529		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.004192398087092529 | validation: 0.0048684850738661065]
	TIME [epoch: 38.4 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037613306315135452		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.0037613306315135452 | validation: 0.006279973010677358]
	TIME [epoch: 38.5 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004440875210296442		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.004440875210296442 | validation: 0.005957448941349297]
	TIME [epoch: 38.5 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005068151930183972		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.005068151930183972 | validation: 0.004629259140972783]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_974.pth
	Model improved!!!
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004178069550724729		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.004178069550724729 | validation: 0.005504780384647851]
	TIME [epoch: 38.5 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004566963972863427		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.004566963972863427 | validation: 0.005260228290515617]
	TIME [epoch: 38.5 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045774927819334935		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.0045774927819334935 | validation: 0.006397583804362511]
	TIME [epoch: 38.5 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037317077927031084		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.0037317077927031084 | validation: 0.005600993844561441]
	TIME [epoch: 38.5 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047780063222732545		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.0047780063222732545 | validation: 0.005573253040211213]
	TIME [epoch: 38.5 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004297331775646806		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.004297331775646806 | validation: 0.0069584944109213264]
	TIME [epoch: 38.5 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047982985379396974		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.0047982985379396974 | validation: 0.005004707385617738]
	TIME [epoch: 38.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004747401072994014		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.004747401072994014 | validation: 0.006897900483330456]
	TIME [epoch: 38.5 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045286173352644945		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.0045286173352644945 | validation: 0.005275039456494412]
	TIME [epoch: 38.5 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004812958888977986		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.004812958888977986 | validation: 0.007104954862823167]
	TIME [epoch: 38.5 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049177252373162105		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.0049177252373162105 | validation: 0.005683102597803166]
	TIME [epoch: 38.5 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004140346422422251		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.004140346422422251 | validation: 0.004979734202913661]
	TIME [epoch: 38.5 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003950811560565103		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.003950811560565103 | validation: 0.007352500387607874]
	TIME [epoch: 38.5 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004754748486317204		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.004754748486317204 | validation: 0.006013486787544308]
	TIME [epoch: 38.5 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004702497291403989		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.004702497291403989 | validation: 0.005614726817918737]
	TIME [epoch: 38.5 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037841638003017885		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.0037841638003017885 | validation: 0.005028711756346912]
	TIME [epoch: 38.5 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004748225686172611		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.004748225686172611 | validation: 0.004927744509352542]
	TIME [epoch: 38.5 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042056743318039744		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.0042056743318039744 | validation: 0.0049557548850671556]
	TIME [epoch: 38.5 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004232946477266316		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.004232946477266316 | validation: 0.005437334053134972]
	TIME [epoch: 38.5 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040568999307924285		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.0040568999307924285 | validation: 0.005326671803350902]
	TIME [epoch: 38.5 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004202784494289291		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.004202784494289291 | validation: 0.004479657450116724]
	TIME [epoch: 38.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_995.pth
	Model improved!!!
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004561266132397502		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.004561266132397502 | validation: 0.005531714776809929]
	TIME [epoch: 38.5 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004307709456338994		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.004307709456338994 | validation: 0.00460928414326975]
	TIME [epoch: 38.5 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004027154922378055		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.004027154922378055 | validation: 0.005272568008795603]
	TIME [epoch: 38.5 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005048298886552236		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.005048298886552236 | validation: 0.0064646239782408695]
	TIME [epoch: 38.5 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004539261211164075		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.004539261211164075 | validation: 0.005760702851543055]
	TIME [epoch: 38.5 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003961438417656619		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.003961438417656619 | validation: 0.005038049760821306]
	TIME [epoch: 186 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037531870272811694		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.0037531870272811694 | validation: 0.006248482535915892]
	TIME [epoch: 82.1 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037422187413408267		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.0037422187413408267 | validation: 0.0060567275228749625]
	TIME [epoch: 82.1 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005018868047265616		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.005018868047265616 | validation: 0.006262830644781684]
	TIME [epoch: 82.1 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004204293433134829		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.004204293433134829 | validation: 0.004969480166567211]
	TIME [epoch: 82.2 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005351232245535766		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.005351232245535766 | validation: 0.006169534748357821]
	TIME [epoch: 82.2 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004399600430660226		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.004399600430660226 | validation: 0.0049713016133241]
	TIME [epoch: 82.1 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033996146023726756		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.0033996146023726756 | validation: 0.0046253618558970986]
	TIME [epoch: 82.1 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003829652460607465		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.003829652460607465 | validation: 0.004874399613484641]
	TIME [epoch: 82.1 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040092596938198066		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.0040092596938198066 | validation: 0.0044190525453609445]
	TIME [epoch: 82.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_1010.pth
	Model improved!!!
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003774058148790874		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.003774058148790874 | validation: 0.00581680190466558]
	TIME [epoch: 82.1 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00442829651232962		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.00442829651232962 | validation: 0.00793067350510251]
	TIME [epoch: 82.1 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00507883944174113		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.00507883944174113 | validation: 0.0057115994745424205]
	TIME [epoch: 82.1 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041770085491513505		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.0041770085491513505 | validation: 0.004898991884893557]
	TIME [epoch: 82.1 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003807485137079511		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.003807485137079511 | validation: 0.004468210699473517]
	TIME [epoch: 82.1 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004548383293417233		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.004548383293417233 | validation: 0.0051141266268842816]
	TIME [epoch: 82.1 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037624643190162024		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.0037624643190162024 | validation: 0.008195253467566109]
	TIME [epoch: 82.1 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004496548739842669		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.004496548739842669 | validation: 0.004006735197358229]
	TIME [epoch: 82.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_1018.pth
	Model improved!!!
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037182636892183355		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.0037182636892183355 | validation: 0.0044533641766630735]
	TIME [epoch: 82.1 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003827299480648588		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.003827299480648588 | validation: 0.004309154369534089]
	TIME [epoch: 82.2 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036997235467914865		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.0036997235467914865 | validation: 0.00555138237302975]
	TIME [epoch: 82.1 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004211816447534887		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.004211816447534887 | validation: 0.00676379164556046]
	TIME [epoch: 82.1 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039130146155647305		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.0039130146155647305 | validation: 0.004933514978123668]
	TIME [epoch: 82.1 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036185259453746675		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.0036185259453746675 | validation: 0.006447740523455954]
	TIME [epoch: 82.1 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045008904111946526		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.0045008904111946526 | validation: 0.005023713352315949]
	TIME [epoch: 82.1 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00399053256477652		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.00399053256477652 | validation: 0.006567993532665286]
	TIME [epoch: 82.1 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004200555802857752		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.004200555802857752 | validation: 0.004461340357245129]
	TIME [epoch: 82.1 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038736119226576993		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.0038736119226576993 | validation: 0.005818084617059292]
	TIME [epoch: 82.2 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003741790892611254		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.003741790892611254 | validation: 0.00495043569821413]
	TIME [epoch: 82.1 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003777553562120353		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.003777553562120353 | validation: 0.005131556596082982]
	TIME [epoch: 82.1 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004303987070458647		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.004303987070458647 | validation: 0.005479128511389194]
	TIME [epoch: 82.1 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039603997207623785		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.0039603997207623785 | validation: 0.005054341324027661]
	TIME [epoch: 82.2 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003986199730931384		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.003986199730931384 | validation: 0.004857800224253704]
	TIME [epoch: 82.1 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004302457537619801		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.004302457537619801 | validation: 0.004944637268180742]
	TIME [epoch: 82 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003775411699261565		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.003775411699261565 | validation: 0.004851901354949258]
	TIME [epoch: 82.1 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00351116523005378		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.00351116523005378 | validation: 0.00521791458307584]
	TIME [epoch: 82.1 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004023100583551646		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.004023100583551646 | validation: 0.005187479215857788]
	TIME [epoch: 82.1 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033908243991492663		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.0033908243991492663 | validation: 0.005746652425329308]
	TIME [epoch: 82.1 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003603623742968189		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.003603623742968189 | validation: 0.005837308848328849]
	TIME [epoch: 82.1 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004170246366275384		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.004170246366275384 | validation: 0.005144669899940186]
	TIME [epoch: 82.1 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003818133492122597		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.003818133492122597 | validation: 0.0048082312564747395]
	TIME [epoch: 82.1 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035674200888421315		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.0035674200888421315 | validation: 0.005418087937002022]
	TIME [epoch: 82.1 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004034115122475917		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.004034115122475917 | validation: 0.003770895290923593]
	TIME [epoch: 82.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_1043.pth
	Model improved!!!
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003654041724334133		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.003654041724334133 | validation: 0.005541259348056189]
	TIME [epoch: 82.1 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004283503626945557		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.004283503626945557 | validation: 0.004389071612550378]
	TIME [epoch: 82.1 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037250514166069417		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.0037250514166069417 | validation: 0.005699558028599993]
	TIME [epoch: 82.1 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004168198383463684		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.004168198383463684 | validation: 0.004901172974989901]
	TIME [epoch: 82.1 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003859694537901366		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.003859694537901366 | validation: 0.005011806237132942]
	TIME [epoch: 82.1 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003735552886128787		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.003735552886128787 | validation: 0.005141654406265783]
	TIME [epoch: 82.1 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034056094315889044		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.0034056094315889044 | validation: 0.004474508134530965]
	TIME [epoch: 82.1 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039515386538477955		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.0039515386538477955 | validation: 0.0038550198618957027]
	TIME [epoch: 82.1 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003924651502138207		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.003924651502138207 | validation: 0.0048988985318859654]
	TIME [epoch: 82 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003598118134056266		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.003598118134056266 | validation: 0.003929433876999371]
	TIME [epoch: 82.1 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003327305145465997		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.003327305145465997 | validation: 0.004586646110778299]
	TIME [epoch: 82.1 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004181134426021019		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.004181134426021019 | validation: 0.003615050251265761]
	TIME [epoch: 82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_1055.pth
	Model improved!!!
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006402407524768772		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.006402407524768772 | validation: 0.0068464305475285595]
	TIME [epoch: 82.1 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004467552236140554		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.004467552236140554 | validation: 0.005217399170539686]
	TIME [epoch: 82.1 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003671544130937127		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.003671544130937127 | validation: 0.004681922249207876]
	TIME [epoch: 82.1 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003677022412010455		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.003677022412010455 | validation: 0.005177131576459426]
	TIME [epoch: 82.1 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003634334697472176		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.003634334697472176 | validation: 0.004377721630336531]
	TIME [epoch: 82.1 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003849238392568794		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.003849238392568794 | validation: 0.004531906628801131]
	TIME [epoch: 82.1 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034470064676582804		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.0034470064676582804 | validation: 0.0050317886168158915]
	TIME [epoch: 82.1 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035202380887222546		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.0035202380887222546 | validation: 0.004122289818040831]
	TIME [epoch: 82.1 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003651226921072201		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.003651226921072201 | validation: 0.004403664662666264]
	TIME [epoch: 82.1 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003191461783985165		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.003191461783985165 | validation: 0.004932448524145293]
	TIME [epoch: 82.1 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003710767294486625		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.003710767294486625 | validation: 0.00412122982645677]
	TIME [epoch: 82.1 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043660122454725935		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.0043660122454725935 | validation: 0.005699573413025105]
	TIME [epoch: 82.1 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033668443762688276		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.0033668443762688276 | validation: 0.004914714759325056]
	TIME [epoch: 82.1 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003264312695363512		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.003264312695363512 | validation: 0.004989254078373142]
	TIME [epoch: 82.1 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034453911460135597		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.0034453911460135597 | validation: 0.0043910835342760544]
	TIME [epoch: 82.1 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034318940329706718		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.0034318940329706718 | validation: 0.004769851567564044]
	TIME [epoch: 82.1 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034241884327307404		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.0034241884327307404 | validation: 0.0038824035681547793]
	TIME [epoch: 82.1 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038396060284878993		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.0038396060284878993 | validation: 0.005554952698572622]
	TIME [epoch: 82.1 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004008171458231751		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.004008171458231751 | validation: 0.004268165464166235]
	TIME [epoch: 82.1 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004033824271224475		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.004033824271224475 | validation: 0.0037351235120078974]
	TIME [epoch: 82.1 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034295571728604787		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.0034295571728604787 | validation: 0.00409510840228831]
	TIME [epoch: 82.1 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034634231631326556		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.0034634231631326556 | validation: 0.005271447276883279]
	TIME [epoch: 82.1 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003256232505786519		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.003256232505786519 | validation: 0.00436744270978228]
	TIME [epoch: 82.1 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003034041145996764		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.003034041145996764 | validation: 0.004070497407286968]
	TIME [epoch: 82.1 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035539662651250655		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.0035539662651250655 | validation: 0.004499476860253144]
	TIME [epoch: 82.1 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003787165977614498		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.003787165977614498 | validation: 0.004191139133343133]
	TIME [epoch: 82.1 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00305832462791337		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.00305832462791337 | validation: 0.004228621653263709]
	TIME [epoch: 82.1 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033581089564481883		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.0033581089564481883 | validation: 0.004707985998794287]
	TIME [epoch: 82.1 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004015408497714677		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.004015408497714677 | validation: 0.00396397517000938]
	TIME [epoch: 82.1 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003551287223207316		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.003551287223207316 | validation: 0.004334366271399544]
	TIME [epoch: 82.1 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003239474584876692		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.003239474584876692 | validation: 0.005180561560116615]
	TIME [epoch: 82.1 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036896505054461962		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.0036896505054461962 | validation: 0.003923268997135575]
	TIME [epoch: 82.1 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034035114978636927		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.0034035114978636927 | validation: 0.0038671298414194874]
	TIME [epoch: 82.1 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003468993976032494		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.003468993976032494 | validation: 0.0053184202794018965]
	TIME [epoch: 82.1 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004102436101614511		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.004102436101614511 | validation: 0.004208573551960573]
	TIME [epoch: 82.1 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002887775906019305		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.002887775906019305 | validation: 0.005052912468288176]
	TIME [epoch: 82.1 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036584723889698844		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.0036584723889698844 | validation: 0.004566776815566122]
	TIME [epoch: 82.1 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003713373748624524		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.003713373748624524 | validation: 0.004084927945138138]
	TIME [epoch: 82.1 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036620796706490354		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.0036620796706490354 | validation: 0.004633549129503504]
	TIME [epoch: 82.1 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029975698466916736		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.0029975698466916736 | validation: 0.005662198155077092]
	TIME [epoch: 82.1 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003613505111826968		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.003613505111826968 | validation: 0.0035673631797226075]
	TIME [epoch: 82.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_1096.pth
	Model improved!!!
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003100440037023481		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.003100440037023481 | validation: 0.004626004323269081]
	TIME [epoch: 82.1 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031784589005216923		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.0031784589005216923 | validation: 0.003902164979312008]
	TIME [epoch: 82.1 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034329117117505837		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.0034329117117505837 | validation: 0.004717110203589852]
	TIME [epoch: 82 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003099107445929545		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.003099107445929545 | validation: 0.004876784561849179]
	TIME [epoch: 82.1 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003147631141903935		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.003147631141903935 | validation: 0.00399466037685278]
	TIME [epoch: 82.1 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004018621327606012		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.004018621327606012 | validation: 0.003943511504874523]
	TIME [epoch: 82.1 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030577908764716864		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.0030577908764716864 | validation: 0.0037261780896803956]
	TIME [epoch: 82.1 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038091504102701593		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.0038091504102701593 | validation: 0.004678550384395405]
	TIME [epoch: 82.1 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003268514052648411		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.003268514052648411 | validation: 0.004125377006843264]
	TIME [epoch: 82.1 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035502561248951406		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.0035502561248951406 | validation: 0.005140773243441495]
	TIME [epoch: 82.1 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033703061958751403		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.0033703061958751403 | validation: 0.003491902344180163]
	TIME [epoch: 82.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_1107.pth
	Model improved!!!
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003131714415865127		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.003131714415865127 | validation: 0.0047333387572221055]
	TIME [epoch: 82.2 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029765107555055196		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.0029765107555055196 | validation: 0.004762275254439634]
	TIME [epoch: 82.1 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003350489797261096		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.003350489797261096 | validation: 0.0034633253151479994]
	TIME [epoch: 82.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_1110.pth
	Model improved!!!
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003024189549710991		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.003024189549710991 | validation: 0.004906769523531624]
	TIME [epoch: 82.1 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034269301132473532		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.0034269301132473532 | validation: 0.004412029636716686]
	TIME [epoch: 82.1 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033553773605991574		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.0033553773605991574 | validation: 0.00417232843072926]
	TIME [epoch: 82.1 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003132808449889118		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.003132808449889118 | validation: 0.005398730485284729]
	TIME [epoch: 82.1 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00323104719707272		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.00323104719707272 | validation: 0.0037390987452710265]
	TIME [epoch: 82.1 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003413057863807794		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.003413057863807794 | validation: 0.004304946347809622]
	TIME [epoch: 82.1 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033761125259640845		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.0033761125259640845 | validation: 0.0038069593945278955]
	TIME [epoch: 82.1 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032220427333918954		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.0032220427333918954 | validation: 0.0033146188187217103]
	TIME [epoch: 82.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_1118.pth
	Model improved!!!
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031852762048804996		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.0031852762048804996 | validation: 0.004136760237175266]
	TIME [epoch: 82.1 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032243853379097754		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.0032243853379097754 | validation: 0.0040096297807686]
	TIME [epoch: 82.1 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032355462028561695		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.0032355462028561695 | validation: 0.005212885261225544]
	TIME [epoch: 82.2 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035366233837024483		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.0035366233837024483 | validation: 0.004711802085255832]
	TIME [epoch: 82.1 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032642463428450164		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.0032642463428450164 | validation: 0.0041049375859307565]
	TIME [epoch: 82.1 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030819447972532934		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.0030819447972532934 | validation: 0.00449134267048097]
	TIME [epoch: 82.2 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036223596756323525		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.0036223596756323525 | validation: 0.004295621973862953]
	TIME [epoch: 82.1 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003038484743471043		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.003038484743471043 | validation: 0.004797070918945072]
	TIME [epoch: 82.1 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034735836063621156		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.0034735836063621156 | validation: 0.0040535365089953085]
	TIME [epoch: 82.1 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003330857915866087		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.003330857915866087 | validation: 0.004354483565676933]
	TIME [epoch: 82.1 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003097440026584407		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.003097440026584407 | validation: 0.0033548021847862953]
	TIME [epoch: 82.2 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003195702778184415		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.003195702778184415 | validation: 0.004703475696935748]
	TIME [epoch: 82.1 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032576594893230348		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.0032576594893230348 | validation: 0.003495977167870164]
	TIME [epoch: 82.1 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003136787727320769		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.003136787727320769 | validation: 0.005206905546357122]
	TIME [epoch: 82.1 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003171906108284284		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.003171906108284284 | validation: 0.003845176804711087]
	TIME [epoch: 82.1 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032194115733135986		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.0032194115733135986 | validation: 0.0032839792252725066]
	TIME [epoch: 82.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_1134.pth
	Model improved!!!
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027612104832520547		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.0027612104832520547 | validation: 0.0038174336376998394]
	TIME [epoch: 82.1 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032485667143492426		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.0032485667143492426 | validation: 0.004568639258818934]
	TIME [epoch: 82.1 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034264549690199916		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.0034264549690199916 | validation: 0.004461320960122988]
	TIME [epoch: 82.1 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003082558719348324		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.003082558719348324 | validation: 0.003474454495163096]
	TIME [epoch: 82.1 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002756217954029955		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.002756217954029955 | validation: 0.0046345356735926005]
	TIME [epoch: 82.1 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002903300859286984		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.002903300859286984 | validation: 0.0044276836254925666]
	TIME [epoch: 82.1 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029258399289553783		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.0029258399289553783 | validation: 0.004024144271139358]
	TIME [epoch: 82.1 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002950535611331097		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.002950535611331097 | validation: 0.003839238375097596]
	TIME [epoch: 82.1 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035373494713121234		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.0035373494713121234 | validation: 0.003624363185401508]
	TIME [epoch: 82.1 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030693453745342728		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.0030693453745342728 | validation: 0.00506034246712919]
	TIME [epoch: 82.1 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031620603358145987		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.0031620603358145987 | validation: 0.0036181101146036065]
	TIME [epoch: 82.1 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031289664365411435		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.0031289664365411435 | validation: 0.004846256329781022]
	TIME [epoch: 82.1 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002895063553018668		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.002895063553018668 | validation: 0.004211249823249829]
	TIME [epoch: 82.1 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032003155844023213		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.0032003155844023213 | validation: 0.003939036523805956]
	TIME [epoch: 82.1 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029649475994373686		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.0029649475994373686 | validation: 0.0037899858232160995]
	TIME [epoch: 82.1 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030482797398365138		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.0030482797398365138 | validation: 0.0038242891216348127]
	TIME [epoch: 82.1 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003347494534905955		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.003347494534905955 | validation: 0.004769035263710057]
	TIME [epoch: 82.1 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002822567536077061		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.002822567536077061 | validation: 0.0033896400250439927]
	TIME [epoch: 82.1 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030664860458851084		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.0030664860458851084 | validation: 0.005080469326585265]
	TIME [epoch: 82.1 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030099530983604336		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.0030099530983604336 | validation: 0.0034205945743051797]
	TIME [epoch: 82.1 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003183481564271067		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.003183481564271067 | validation: 0.003625416557792577]
	TIME [epoch: 82.1 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029752531004291595		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.0029752531004291595 | validation: 0.00391440769274702]
	TIME [epoch: 82.1 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003087384345913517		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.003087384345913517 | validation: 0.003797317654534024]
	TIME [epoch: 82.1 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002848826130761973		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.002848826130761973 | validation: 0.004568182329206414]
	TIME [epoch: 82.1 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027727292311298256		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.0027727292311298256 | validation: 0.004306092731893181]
	TIME [epoch: 82.1 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028853791812387133		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.0028853791812387133 | validation: 0.004268582934265339]
	TIME [epoch: 82.1 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003243883086714857		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.003243883086714857 | validation: 0.00393020985667804]
	TIME [epoch: 82.1 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003171118934334402		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.003171118934334402 | validation: 0.004101087042212557]
	TIME [epoch: 82.1 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029373906159773974		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.0029373906159773974 | validation: 0.003825452201542852]
	TIME [epoch: 82.1 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002876607706975066		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.002876607706975066 | validation: 0.0034644705484751376]
	TIME [epoch: 82.1 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002838916449627796		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.002838916449627796 | validation: 0.00505858718707884]
	TIME [epoch: 82.1 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028815355776330718		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.0028815355776330718 | validation: 0.003652542421399403]
	TIME [epoch: 82.1 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002960687397053844		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.002960687397053844 | validation: 0.004149087808863042]
	TIME [epoch: 82.1 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036167781817324184		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.0036167781817324184 | validation: 0.004841930358701134]
	TIME [epoch: 82.1 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003150296784270285		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 0.003150296784270285 | validation: 0.0036447070334218595]
	TIME [epoch: 81.7 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028055534205054843		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.0028055534205054843 | validation: 0.0037300005043393496]
	TIME [epoch: 81.7 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002692347177312331		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.002692347177312331 | validation: 0.0035701639670383278]
	TIME [epoch: 81.8 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027427782995237664		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.0027427782995237664 | validation: 0.003531880400231497]
	TIME [epoch: 82 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024145089969437195		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.0024145089969437195 | validation: 0.003438824561435526]
	TIME [epoch: 82.1 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032157061430535473		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.0032157061430535473 | validation: 0.004099391663876335]
	TIME [epoch: 82.1 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027729519299648127		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.0027729519299648127 | validation: 0.004183052058358852]
	TIME [epoch: 82.1 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030555171631064925		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.0030555171631064925 | validation: 0.004437150931491683]
	TIME [epoch: 81.9 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025503469661242043		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.0025503469661242043 | validation: 0.0037635688482327606]
	TIME [epoch: 81.7 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042428259402356745		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.0042428259402356745 | validation: 0.004916612391571922]
	TIME [epoch: 82.1 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003790555111300805		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.003790555111300805 | validation: 0.0042660143602893524]
	TIME [epoch: 81.9 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029660470151103937		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.0029660470151103937 | validation: 0.003962800820394469]
	TIME [epoch: 81.8 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029560861319676114		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.0029560861319676114 | validation: 0.0036940852042440277]
	TIME [epoch: 81.9 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025102442403971345		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.0025102442403971345 | validation: 0.004486775307349353]
	TIME [epoch: 82.1 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027337891899187213		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.0027337891899187213 | validation: 0.003830163970747505]
	TIME [epoch: 82.1 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031706937027763536		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.0031706937027763536 | validation: 0.0038014190868945765]
	TIME [epoch: 82.1 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029185300527330075		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.0029185300527330075 | validation: 0.0036442028186317206]
	TIME [epoch: 82.1 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029219059513578035		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.0029219059513578035 | validation: 0.0035917645800991904]
	TIME [epoch: 82.1 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028481732712111562		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.0028481732712111562 | validation: 0.004249300836954618]
	TIME [epoch: 82.1 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029774825553843783		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.0029774825553843783 | validation: 0.005062178944537321]
	TIME [epoch: 82.1 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026271821694983204		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.0026271821694983204 | validation: 0.0038743938625370442]
	TIME [epoch: 82.1 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002760574181707719		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.002760574181707719 | validation: 0.004592517782111334]
	TIME [epoch: 82.2 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028691222255326073		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.0028691222255326073 | validation: 0.0038155204085013267]
	TIME [epoch: 82.1 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002867986066436132		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.002867986066436132 | validation: 0.003945910729086336]
	TIME [epoch: 82.1 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028695198118592054		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.0028695198118592054 | validation: 0.003111023303466104]
	TIME [epoch: 82.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_largenet_20240704_134101/states/model_phi1_1a_v_mmd1_largenet_1193.pth
	Model improved!!!
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00295455802363218		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.00295455802363218 | validation: 0.004467293636027787]
	TIME [epoch: 82.1 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030352345779228497		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.0030352345779228497 | validation: 0.003469600240020655]
	TIME [epoch: 82.1 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003416358021399996		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.003416358021399996 | validation: 0.003881006068684206]
	TIME [epoch: 82.1 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025494962423751296		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.0025494962423751296 | validation: 0.003301652522539121]
	TIME [epoch: 82.1 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027391688702259856		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.0027391688702259856 | validation: 0.003701212863748967]
	TIME [epoch: 82.1 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002761292704401619		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.002761292704401619 | validation: 0.0038253695286227848]
	TIME [epoch: 82.1 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029313231492706956		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.0029313231492706956 | validation: 0.004917070042122837]
	TIME [epoch: 82.1 sec]
EPOCH 1201/2000:
	Training over batches...
