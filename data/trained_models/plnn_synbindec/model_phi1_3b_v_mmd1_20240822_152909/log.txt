Args:
Namespace(name='model_phi1_3b_v_mmd1', outdir='out/model_training/model_phi1_3b_v_mmd1', training_data='data/training_data/data_phi1_3b/training', validation_data='data/training_data/data_phi1_3b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3290533256

Training model...

Saving initial model state to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.072053247862789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.072053247862789 | validation: 4.588463636844802]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.792864423433135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.792864423433135 | validation: 4.153455722653611]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.375925612942035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.375925612942035 | validation: 4.004582978763105]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.278716128407782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.278716128407782 | validation: 4.146974322379349]
	TIME [epoch: 1.74 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.371034011652681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.371034011652681 | validation: 3.9743067210070295]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.295378243353812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.295378243353812 | validation: 3.8233301074300408]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.185592850588397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.185592850588397 | validation: 3.569448509451715]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.975604078064504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.975604078064504 | validation: 3.585438127930656]
	TIME [epoch: 1.74 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9110788418228037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9110788418228037 | validation: 3.468160677725314]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8472688710110265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8472688710110265 | validation: 3.3286765222517065]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.79642716119095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.79642716119095 | validation: 3.301595601296416]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.728119828682326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.728119828682326 | validation: 3.1273025368770457]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6056213507048156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6056213507048156 | validation: 3.048123840319494]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5782768425408586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5782768425408586 | validation: 2.8821309509825648]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.398726323300073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.398726323300073 | validation: 3.334057121034667]
	TIME [epoch: 1.73 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.578140612499097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.578140612499097 | validation: 3.6695322514656628]
	TIME [epoch: 1.73 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.213266470745929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.213266470745929 | validation: 2.713353526476741]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2920239056139553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2920239056139553 | validation: 2.657814767185581]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.888592307621564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.888592307621564 | validation: 2.55758596025199]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3951462715742533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3951462715742533 | validation: 3.2447526886737665]
	TIME [epoch: 1.74 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.643356009736392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.643356009736392 | validation: 2.331330251513598]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.785819095912668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.785819095912668 | validation: 1.7095602536744936]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9765868082054303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9765868082054303 | validation: 1.8657752123369855]
	TIME [epoch: 1.73 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.624482064581785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.624482064581785 | validation: 1.4341173528518272]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3201850524640646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3201850524640646 | validation: 1.6519445698919513]
	TIME [epoch: 1.73 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7961274363945061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7961274363945061 | validation: 2.3673035280515276]
	TIME [epoch: 1.73 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1631062031813797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1631062031813797 | validation: 1.009615412548946]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0415417300391976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0415417300391976 | validation: 1.056504462137332]
	TIME [epoch: 1.74 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1648868136880643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1648868136880643 | validation: 1.2844049598715204]
	TIME [epoch: 1.74 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2295154758915556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2295154758915556 | validation: 1.0068405652686587]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.085334027237161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.085334027237161 | validation: 0.9206755457498013]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9462082804262194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9462082804262194 | validation: 0.8122605594381134]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8774817422542911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8774817422542911 | validation: 0.7871696172830174]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8591765670199242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8591765670199242 | validation: 0.7990783070165615]
	TIME [epoch: 1.73 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8461557033989985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8461557033989985 | validation: 0.8011124681362598]
	TIME [epoch: 1.73 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8581726736172592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8581726736172592 | validation: 0.8941744096398647]
	TIME [epoch: 1.73 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911840471778809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.911840471778809 | validation: 0.9854351311901782]
	TIME [epoch: 1.73 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0803147541856988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0803147541856988 | validation: 1.223278586270315]
	TIME [epoch: 1.73 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.159434445586805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.159434445586805 | validation: 0.948317898089194]
	TIME [epoch: 1.73 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.043815780093551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.043815780093551 | validation: 0.8207729697026855]
	TIME [epoch: 1.73 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8442132235695451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8442132235695451 | validation: 0.8192035196465014]
	TIME [epoch: 1.73 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320043839092659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8320043839092659 | validation: 0.8331093634601358]
	TIME [epoch: 1.73 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8759793794312382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8759793794312382 | validation: 0.9066607477590399]
	TIME [epoch: 1.73 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8968536373945363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8968536373945363 | validation: 0.8725648830941588]
	TIME [epoch: 1.73 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9302616490398858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9302616490398858 | validation: 0.8595786071046327]
	TIME [epoch: 1.73 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8663011961856754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8663011961856754 | validation: 0.8103673243791487]
	TIME [epoch: 1.74 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8288526531947353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8288526531947353 | validation: 0.7800296109282684]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7923885549544669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7923885549544669 | validation: 0.7626705495070722]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.774694605651585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.774694605651585 | validation: 0.7488098252145772]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7689659710357402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7689659710357402 | validation: 0.741263037313363]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7638668872290377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7638668872290377 | validation: 0.7490185893717085]
	TIME [epoch: 1.73 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7585095266244863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7585095266244863 | validation: 0.7445979164314595]
	TIME [epoch: 1.73 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7609728079271048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7609728079271048 | validation: 0.8235421157764735]
	TIME [epoch: 1.73 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8157857038195679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8157857038195679 | validation: 1.0748401674029666]
	TIME [epoch: 1.73 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2006004055481836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2006004055481836 | validation: 1.092189408770002]
	TIME [epoch: 1.73 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9937914198794557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9937914198794557 | validation: 0.8703017316924243]
	TIME [epoch: 1.73 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8451265076888046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8451265076888046 | validation: 0.7663083790007816]
	TIME [epoch: 1.73 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7554145496791906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7554145496791906 | validation: 0.7695169418814353]
	TIME [epoch: 1.73 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7584505856822271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7584505856822271 | validation: 0.7745183863606525]
	TIME [epoch: 1.73 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7711486878420147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7711486878420147 | validation: 0.809464209480381]
	TIME [epoch: 1.73 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949667363951215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7949667363951215 | validation: 0.8101090667476613]
	TIME [epoch: 1.73 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260801302449179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8260801302449179 | validation: 0.7741950145839948]
	TIME [epoch: 1.73 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7609506789608643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7609506789608643 | validation: 0.7457608827344216]
	TIME [epoch: 1.73 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7373415385670422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7373415385670422 | validation: 0.7576566869820662]
	TIME [epoch: 1.73 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7339753455388677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7339753455388677 | validation: 0.7374426038225553]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7313908006516948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7313908006516948 | validation: 0.8093091188999847]
	TIME [epoch: 1.73 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7655911783853694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7655911783853694 | validation: 0.8635077458897243]
	TIME [epoch: 1.73 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9276347293384539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9276347293384539 | validation: 1.0321780896815544]
	TIME [epoch: 1.74 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8886337551387218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8886337551387218 | validation: 0.7741029562713184]
	TIME [epoch: 1.73 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7502641265965281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7502641265965281 | validation: 0.7401221440830851]
	TIME [epoch: 1.73 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7465202373147255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7465202373147255 | validation: 0.8557650930679046]
	TIME [epoch: 1.73 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8038471321980813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8038471321980813 | validation: 0.7535336626252856]
	TIME [epoch: 1.73 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7520969163668391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7520969163668391 | validation: 0.7690877366047318]
	TIME [epoch: 1.73 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7385161479791161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7385161479791161 | validation: 0.7622868231487673]
	TIME [epoch: 1.73 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7281082913424973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7281082913424973 | validation: 0.7884230432467565]
	TIME [epoch: 1.73 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900160466062751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7900160466062751 | validation: 1.003775830313012]
	TIME [epoch: 1.73 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.872356537706676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.872356537706676 | validation: 0.9213666756702132]
	TIME [epoch: 1.73 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9651869649241078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9651869649241078 | validation: 0.7624566596119966]
	TIME [epoch: 1.73 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7341603496406627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7341603496406627 | validation: 0.8404312547296598]
	TIME [epoch: 1.73 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7843811258725787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7843811258725787 | validation: 0.7993905304028488]
	TIME [epoch: 1.73 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906187853163207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7906187853163207 | validation: 0.74129089699727]
	TIME [epoch: 1.73 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715773294141753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.715773294141753 | validation: 0.8050488452417395]
	TIME [epoch: 1.73 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7426187714456722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7426187714456722 | validation: 0.7640954066881414]
	TIME [epoch: 1.73 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7492190158803955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7492190158803955 | validation: 0.7766628768478961]
	TIME [epoch: 1.73 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7281165267612684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7281165267612684 | validation: 0.7481638888217113]
	TIME [epoch: 1.75 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7344226899619067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7344226899619067 | validation: 0.8250301683096208]
	TIME [epoch: 1.73 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7651410128140174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7651410128140174 | validation: 0.7829971609444247]
	TIME [epoch: 1.73 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7708527815036225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7708527815036225 | validation: 0.8178864144728991]
	TIME [epoch: 1.74 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7613520987314804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7613520987314804 | validation: 0.7400338428846358]
	TIME [epoch: 1.73 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7199035334058567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7199035334058567 | validation: 0.7852476332839647]
	TIME [epoch: 1.73 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7162735042143372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7162735042143372 | validation: 0.7623117851908484]
	TIME [epoch: 1.73 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7539961058351606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7539961058351606 | validation: 1.1258026503021665]
	TIME [epoch: 1.73 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9302929033849933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9302929033849933 | validation: 0.894665097399689]
	TIME [epoch: 1.73 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9632191914280377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9632191914280377 | validation: 0.7749026256390983]
	TIME [epoch: 1.73 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7334857264877073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7334857264877073 | validation: 0.8584165955026567]
	TIME [epoch: 1.73 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7902708600472874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7902708600472874 | validation: 0.83257619909085]
	TIME [epoch: 1.73 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8136886351482623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8136886351482623 | validation: 0.7481133538668638]
	TIME [epoch: 1.73 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7124878008334653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7124878008334653 | validation: 0.779730857652888]
	TIME [epoch: 1.73 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7329418548167602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7329418548167602 | validation: 0.7564523763255612]
	TIME [epoch: 1.73 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7366117669424145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7366117669424145 | validation: 0.7384398195912967]
	TIME [epoch: 1.73 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7151145473990395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7151145473990395 | validation: 0.8291012475561614]
	TIME [epoch: 1.73 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7417918492167579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7417918492167579 | validation: 0.7815514713534395]
	TIME [epoch: 1.73 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7700813415273402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7700813415273402 | validation: 0.9008692323896357]
	TIME [epoch: 1.73 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952492742364308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7952492742364308 | validation: 0.76694821440609]
	TIME [epoch: 1.74 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7434920751893435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7434920751893435 | validation: 0.8004135205878327]
	TIME [epoch: 1.74 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7331699804493271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7331699804493271 | validation: 0.7427391255511577]
	TIME [epoch: 1.74 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7178528958507938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7178528958507938 | validation: 0.7658061067625603]
	TIME [epoch: 1.74 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7160936179412799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7160936179412799 | validation: 0.7166634877530287]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7149203980503341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7149203980503341 | validation: 0.7934178456773424]
	TIME [epoch: 1.73 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7321412796430653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7321412796430653 | validation: 0.7840177530989134]
	TIME [epoch: 1.73 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7744325550704468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7744325550704468 | validation: 0.885006028544829]
	TIME [epoch: 1.73 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859895621919833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7859895621919833 | validation: 0.82956782976101]
	TIME [epoch: 1.73 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8307125210072864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8307125210072864 | validation: 0.783102751933588]
	TIME [epoch: 1.73 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7443519488121899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7443519488121899 | validation: 0.7433958318678517]
	TIME [epoch: 1.73 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7134227996008923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7134227996008923 | validation: 0.7357254736489458]
	TIME [epoch: 1.73 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7365424827190166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7365424827190166 | validation: 0.8790023320972999]
	TIME [epoch: 1.74 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7936834877440344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7936834877440344 | validation: 0.7595417370219717]
	TIME [epoch: 1.74 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524623477756159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7524623477756159 | validation: 0.788550743043051]
	TIME [epoch: 1.74 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7399751351051647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7399751351051647 | validation: 0.7443896548776473]
	TIME [epoch: 1.74 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.71653930369731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.71653930369731 | validation: 0.7963095144137494]
	TIME [epoch: 1.74 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7580749511929636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7580749511929636 | validation: 0.8209334849149243]
	TIME [epoch: 1.73 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7600644843709483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7600644843709483 | validation: 0.7892009798917599]
	TIME [epoch: 1.78 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.778451155277927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.778451155277927 | validation: 0.8080211104140468]
	TIME [epoch: 1.73 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.739862668205138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.739862668205138 | validation: 0.7427196819608977]
	TIME [epoch: 1.74 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7220138973008358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7220138973008358 | validation: 0.7654928922914312]
	TIME [epoch: 1.74 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7102307214610969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7102307214610969 | validation: 0.7260073436426272]
	TIME [epoch: 1.74 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7046325842388182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7046325842388182 | validation: 0.7909225210186103]
	TIME [epoch: 1.74 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7281173406763405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7281173406763405 | validation: 0.7756361179142687]
	TIME [epoch: 1.73 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7905892809762622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7905892809762622 | validation: 0.9495214701987609]
	TIME [epoch: 1.73 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8579840950928039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8579840950928039 | validation: 0.7491207362837671]
	TIME [epoch: 1.73 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7098346868085983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7098346868085983 | validation: 0.742104356851667]
	TIME [epoch: 1.73 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7154400737989443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7154400737989443 | validation: 0.7820588830200678]
	TIME [epoch: 1.73 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7348340721433823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7348340721433823 | validation: 0.7310506152130458]
	TIME [epoch: 1.73 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7183878350169287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7183878350169287 | validation: 0.7923430378016438]
	TIME [epoch: 1.73 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7295479048591075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7295479048591075 | validation: 0.7581705079551693]
	TIME [epoch: 1.73 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7326764957709793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7326764957709793 | validation: 0.92859290959666]
	TIME [epoch: 1.73 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8077571039688834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8077571039688834 | validation: 0.8657824035107954]
	TIME [epoch: 1.73 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8856037697001365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8856037697001365 | validation: 0.7436391199315051]
	TIME [epoch: 1.73 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7124585951121725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7124585951121725 | validation: 0.7751671354138494]
	TIME [epoch: 1.73 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.734196429917904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.734196429917904 | validation: 0.7989608673173705]
	TIME [epoch: 1.73 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7849620388033278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7849620388033278 | validation: 0.7331595961811126]
	TIME [epoch: 1.73 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001830219405774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001830219405774 | validation: 0.7588385736854527]
	TIME [epoch: 1.73 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7185522648915478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7185522648915478 | validation: 0.753787980167159]
	TIME [epoch: 1.73 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7304710952661333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7304710952661333 | validation: 0.7445288361968092]
	TIME [epoch: 1.73 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7012970773239897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7012970773239897 | validation: 0.7135272890022185]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7144088336256913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7144088336256913 | validation: 0.8221780523858175]
	TIME [epoch: 1.75 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7494488084247406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7494488084247406 | validation: 0.7919922348304302]
	TIME [epoch: 1.74 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7987452617003646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7987452617003646 | validation: 0.9880691724228019]
	TIME [epoch: 1.74 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8591790867968154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8591790867968154 | validation: 0.7445399892996409]
	TIME [epoch: 1.74 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7101230420519369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7101230420519369 | validation: 0.7643587204649869]
	TIME [epoch: 1.74 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7653814478401125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7653814478401125 | validation: 0.8691352042555498]
	TIME [epoch: 1.74 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7743250220442469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7743250220442469 | validation: 0.7313858056732836]
	TIME [epoch: 1.74 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6923919673098355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6923919673098355 | validation: 0.7252048908825781]
	TIME [epoch: 1.74 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7226916541851272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7226916541851272 | validation: 0.7865050601989623]
	TIME [epoch: 1.73 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7245395350454453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7245395350454453 | validation: 0.7308015146400846]
	TIME [epoch: 1.74 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6982243168023117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6982243168023117 | validation: 0.7380348484429975]
	TIME [epoch: 1.74 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7093598208853191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7093598208853191 | validation: 0.8443612022772318]
	TIME [epoch: 1.74 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7663377016636121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7663377016636121 | validation: 0.8381474967024691]
	TIME [epoch: 1.74 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8412740885240109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8412740885240109 | validation: 0.8103413228511376]
	TIME [epoch: 1.74 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7336551513325948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7336551513325948 | validation: 0.7217359055907585]
	TIME [epoch: 1.74 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6974694941563516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6974694941563516 | validation: 0.7556006335611064]
	TIME [epoch: 1.74 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7412400547039747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7412400547039747 | validation: 0.756734144716694]
	TIME [epoch: 1.74 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7159249901113088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7159249901113088 | validation: 0.7299615276755841]
	TIME [epoch: 1.74 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7034481591866663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7034481591866663 | validation: 0.7436717496355486]
	TIME [epoch: 1.75 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6960428450476895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6960428450476895 | validation: 0.7111088102703128]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7086192636142343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7086192636142343 | validation: 0.772053011199138]
	TIME [epoch: 1.75 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7394938878716623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7394938878716623 | validation: 0.7584655197158825]
	TIME [epoch: 1.74 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7324531191004156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7324531191004156 | validation: 0.778378110001606]
	TIME [epoch: 1.75 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7263918223409155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7263918223409155 | validation: 0.7200055811218669]
	TIME [epoch: 1.76 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7259967856621263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7259967856621263 | validation: 0.8612498890758818]
	TIME [epoch: 1.76 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7655580648202494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7655580648202494 | validation: 0.7531544793936925]
	TIME [epoch: 1.76 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7594338858313878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7594338858313878 | validation: 0.825532884712036]
	TIME [epoch: 1.76 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7477202247696687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7477202247696687 | validation: 0.7113811443683492]
	TIME [epoch: 1.76 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7056244642218462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7056244642218462 | validation: 0.7176439322440814]
	TIME [epoch: 1.75 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6879184363811678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6879184363811678 | validation: 0.7319595983254543]
	TIME [epoch: 1.76 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6914162757357868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6914162757357868 | validation: 0.7127761901662156]
	TIME [epoch: 1.77 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6921150976351875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6921150976351875 | validation: 0.7714487722722757]
	TIME [epoch: 1.78 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712490100011668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.712490100011668 | validation: 0.7244587125415693]
	TIME [epoch: 1.75 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7224024471466332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7224024471466332 | validation: 0.8525690393155259]
	TIME [epoch: 1.76 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7624791911794906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7624791911794906 | validation: 0.7258164720998801]
	TIME [epoch: 1.76 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.720167271164351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.720167271164351 | validation: 0.7734919859592871]
	TIME [epoch: 1.75 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7213087420435212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7213087420435212 | validation: 0.7793272700066423]
	TIME [epoch: 1.75 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449938334273734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7449938334273734 | validation: 0.7902722451978237]
	TIME [epoch: 1.77 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7659736819415336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7659736819415336 | validation: 0.7398347118024348]
	TIME [epoch: 1.75 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6915997389376287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6915997389376287 | validation: 0.7266369745253262]
	TIME [epoch: 1.76 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.691548367347549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.691548367347549 | validation: 0.7316155249334143]
	TIME [epoch: 1.75 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7063313161314798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7063313161314798 | validation: 0.7407194305983826]
	TIME [epoch: 1.75 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6975011783593553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6975011783593553 | validation: 0.7250385584378343]
	TIME [epoch: 1.76 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6938870986167724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6938870986167724 | validation: 0.7579783798120312]
	TIME [epoch: 1.75 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6999105616080897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6999105616080897 | validation: 0.7672682294541499]
	TIME [epoch: 1.75 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7422741870937789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7422741870937789 | validation: 0.9511658833733551]
	TIME [epoch: 1.75 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152626337687909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8152626337687909 | validation: 0.7242401857726167]
	TIME [epoch: 1.75 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7078526485373887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7078526485373887 | validation: 0.7569863016284301]
	TIME [epoch: 1.76 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7002129505416409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7002129505416409 | validation: 0.7568737588862179]
	TIME [epoch: 1.79 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305233103486346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7305233103486346 | validation: 0.8424402328822245]
	TIME [epoch: 1.75 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7759686425992465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7759686425992465 | validation: 0.7084425076449842]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7047790894284321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7047790894284321 | validation: 0.7714520889526901]
	TIME [epoch: 1.76 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7148030481867204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7148030481867204 | validation: 0.7011440111921059]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6979217908822244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6979217908822244 | validation: 0.7493723572626518]
	TIME [epoch: 1.75 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7028543765881411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7028543765881411 | validation: 0.7219470861991151]
	TIME [epoch: 1.75 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6910521626734595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6910521626734595 | validation: 0.7470947098072136]
	TIME [epoch: 30.6 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6940320275113233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6940320275113233 | validation: 0.700603258589228]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6845822693993767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6845822693993767 | validation: 0.7203148284870893]
	TIME [epoch: 3.47 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6705815998962907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6705815998962907 | validation: 0.7124603153557549]
	TIME [epoch: 3.46 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6854245548201889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6854245548201889 | validation: 0.8891369197641759]
	TIME [epoch: 3.47 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7938796880403043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7938796880403043 | validation: 0.9335362938829022]
	TIME [epoch: 3.46 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0129708793794816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0129708793794816 | validation: 0.8186418738354146]
	TIME [epoch: 3.45 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7475535358659096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7475535358659096 | validation: 0.7984484779892296]
	TIME [epoch: 3.46 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7362641959673297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7362641959673297 | validation: 0.7278883418287061]
	TIME [epoch: 3.46 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038158001141477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7038158001141477 | validation: 0.751360544752372]
	TIME [epoch: 3.46 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7049653251053277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7049653251053277 | validation: 0.7517972850171274]
	TIME [epoch: 3.47 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7472469859264951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7472469859264951 | validation: 0.7720635463485662]
	TIME [epoch: 3.47 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066301340941328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066301340941328 | validation: 0.718127619846653]
	TIME [epoch: 3.46 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.681762699610988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.681762699610988 | validation: 0.717547568141945]
	TIME [epoch: 3.46 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6732353857032868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6732353857032868 | validation: 0.7229101389180577]
	TIME [epoch: 3.47 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6767800196894248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6767800196894248 | validation: 0.7153999692068376]
	TIME [epoch: 3.48 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6671367141513577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6671367141513577 | validation: 0.7797740540739713]
	TIME [epoch: 3.47 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7009129819518782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7009129819518782 | validation: 0.7819234090227024]
	TIME [epoch: 3.47 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7727882074675472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7727882074675472 | validation: 0.8295605095677415]
	TIME [epoch: 3.46 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7221838103240499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7221838103240499 | validation: 0.7067605599370403]
	TIME [epoch: 3.46 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6596253302692433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6596253302692433 | validation: 0.7564881375069512]
	TIME [epoch: 3.45 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6721703045213485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6721703045213485 | validation: 0.9074639754050715]
	TIME [epoch: 3.46 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0039996984149564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0039996984149564 | validation: 0.7857754469614586]
	TIME [epoch: 3.46 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7267975073466967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7267975073466967 | validation: 0.8079255033599854]
	TIME [epoch: 3.45 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7380028083061848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7380028083061848 | validation: 0.7224448507362617]
	TIME [epoch: 3.45 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7045325034799033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7045325034799033 | validation: 0.7400971865403401]
	TIME [epoch: 3.45 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7049387654778844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7049387654778844 | validation: 0.7148575021921596]
	TIME [epoch: 3.46 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6892052964105224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6892052964105224 | validation: 0.7272825279101922]
	TIME [epoch: 3.45 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685229658016427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.685229658016427 | validation: 0.7711331108730596]
	TIME [epoch: 3.45 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7006550003785885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7006550003785885 | validation: 0.7174556304276616]
	TIME [epoch: 3.44 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7073509518607695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7073509518607695 | validation: 0.7642882878828092]
	TIME [epoch: 3.45 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6854602536431235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6854602536431235 | validation: 0.7170283095010749]
	TIME [epoch: 3.45 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6937732765360147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6937732765360147 | validation: 0.7265649956799054]
	TIME [epoch: 3.45 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6838175040180792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6838175040180792 | validation: 0.712782790747923]
	TIME [epoch: 3.45 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6778785041708159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6778785041708159 | validation: 0.7386585522633132]
	TIME [epoch: 3.44 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918632455618331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6918632455618331 | validation: 0.7216503751838007]
	TIME [epoch: 3.45 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7136375070953783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7136375070953783 | validation: 0.8024163988139588]
	TIME [epoch: 3.46 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7223658975339275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7223658975339275 | validation: 0.7160496210548171]
	TIME [epoch: 3.47 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6961794775229712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6961794775229712 | validation: 0.7785846203230213]
	TIME [epoch: 3.46 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6908415286957024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6908415286957024 | validation: 0.6954935394638329]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6841082080132946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6841082080132946 | validation: 0.8615491402350464]
	TIME [epoch: 3.45 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764348058946923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.764348058946923 | validation: 0.7223698354049333]
	TIME [epoch: 3.46 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7219537930832426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7219537930832426 | validation: 0.8352694916572532]
	TIME [epoch: 3.45 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6703762554177669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6703762554177669 | validation: 0.6952591592627267]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7297332801975708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7297332801975708 | validation: 0.6796552188243788]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6995980521683902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6995980521683902 | validation: 0.6892017957841459]
	TIME [epoch: 3.46 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6876272113980519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6876272113980519 | validation: 0.7068742826505122]
	TIME [epoch: 3.46 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6992766023544814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6992766023544814 | validation: 0.7017133068007289]
	TIME [epoch: 3.47 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.688736304358059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.688736304358059 | validation: 0.7014636424352096]
	TIME [epoch: 3.46 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6833825604047904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6833825604047904 | validation: 0.6916361567669025]
	TIME [epoch: 3.45 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6883279529962366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6883279529962366 | validation: 0.7123417773027323]
	TIME [epoch: 3.46 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.68840007077228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.68840007077228 | validation: 0.774118036101508]
	TIME [epoch: 3.45 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7146297826314815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7146297826314815 | validation: 0.787721916824771]
	TIME [epoch: 3.45 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7845385978979076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7845385978979076 | validation: 0.7996005096290184]
	TIME [epoch: 3.45 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7171899979842408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7171899979842408 | validation: 0.7354823962302612]
	TIME [epoch: 3.45 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6810193805785226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6810193805785226 | validation: 0.7156355505972147]
	TIME [epoch: 3.46 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6838724447969529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6838724447969529 | validation: 0.7397355382518773]
	TIME [epoch: 3.46 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6845063606295542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6845063606295542 | validation: 0.7177795151040963]
	TIME [epoch: 3.47 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6913112144516191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6913112144516191 | validation: 0.7878341449184991]
	TIME [epoch: 3.47 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7011314343419076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7011314343419076 | validation: 0.7277819870352571]
	TIME [epoch: 3.45 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.710869510410217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.710869510410217 | validation: 0.764401311297986]
	TIME [epoch: 3.46 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6900301457570955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6900301457570955 | validation: 0.7070129448374237]
	TIME [epoch: 3.45 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6785307822867056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6785307822867056 | validation: 0.748352525976887]
	TIME [epoch: 3.45 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931287779089478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6931287779089478 | validation: 0.71175226925335]
	TIME [epoch: 3.45 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7008910874005505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7008910874005505 | validation: 0.7933565671979872]
	TIME [epoch: 3.45 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7112174250576598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7112174250576598 | validation: 0.6879857099145963]
	TIME [epoch: 3.45 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6787386567555097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6787386567555097 | validation: 0.7743499388768029]
	TIME [epoch: 3.45 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772603713034012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6772603713034012 | validation: 0.6918962757836358]
	TIME [epoch: 3.47 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6448960857225342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6448960857225342 | validation: 3.343519770584559]
	TIME [epoch: 3.46 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.179797080748654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.179797080748654 | validation: 1.4672427330621791]
	TIME [epoch: 3.46 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2682035672906233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2682035672906233 | validation: 0.8643691577619367]
	TIME [epoch: 3.46 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9767318450111034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9767318450111034 | validation: 0.7986362292747899]
	TIME [epoch: 3.45 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7700640550689807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7700640550689807 | validation: 0.7965311986943819]
	TIME [epoch: 3.45 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7520757364765518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7520757364765518 | validation: 0.7340229123375316]
	TIME [epoch: 3.45 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7110182252346382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7110182252346382 | validation: 0.7635897607108388]
	TIME [epoch: 3.46 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7338516875785638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7338516875785638 | validation: 0.7437964697887526]
	TIME [epoch: 3.46 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7278065134874447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7278065134874447 | validation: 0.7887972406049825]
	TIME [epoch: 3.46 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7219565828239644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7219565828239644 | validation: 0.7032477337058001]
	TIME [epoch: 3.46 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7096671719884013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7096671719884013 | validation: 0.7594928048315732]
	TIME [epoch: 3.46 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7208812075303862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7208812075303862 | validation: 0.7344808332831152]
	TIME [epoch: 3.47 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7423954020951129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7423954020951129 | validation: 0.7495310210197645]
	TIME [epoch: 3.46 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6909654496496598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6909654496496598 | validation: 0.7547340084486205]
	TIME [epoch: 3.46 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7007672662035778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7007672662035778 | validation: 0.7192499777049595]
	TIME [epoch: 3.46 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7106553671868064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7106553671868064 | validation: 0.7652089918320155]
	TIME [epoch: 3.45 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6968217255728584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6968217255728584 | validation: 0.7214136412040356]
	TIME [epoch: 3.45 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6943971484067099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6943971484067099 | validation: 0.7560296598901924]
	TIME [epoch: 3.44 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699055206594441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.699055206594441 | validation: 0.7093372277668419]
	TIME [epoch: 3.43 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6985758352477215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6985758352477215 | validation: 0.7617458653062116]
	TIME [epoch: 3.44 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6917516627121674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6917516627121674 | validation: 0.7064172323346793]
	TIME [epoch: 3.45 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7006024243347918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7006024243347918 | validation: 0.7968035533015287]
	TIME [epoch: 3.44 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7053009479322665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7053009479322665 | validation: 0.7387083133839855]
	TIME [epoch: 3.43 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7399280987070188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7399280987070188 | validation: 0.7664144383055516]
	TIME [epoch: 3.44 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6975492485421081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6975492485421081 | validation: 0.7344726811883783]
	TIME [epoch: 3.46 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931344900087737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6931344900087737 | validation: 0.7567070959564989]
	TIME [epoch: 3.44 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6952112686623381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6952112686623381 | validation: 0.7261571969304647]
	TIME [epoch: 3.46 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6926257092197318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6926257092197318 | validation: 0.7290771981623501]
	TIME [epoch: 3.45 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6976868167425027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6976868167425027 | validation: 0.7341840584299342]
	TIME [epoch: 3.45 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6913668651776713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6913668651776713 | validation: 0.7249406366133949]
	TIME [epoch: 3.46 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6810540104006438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6810540104006438 | validation: 0.7563089118394284]
	TIME [epoch: 3.46 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6929526359136358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6929526359136358 | validation: 0.7522049655085001]
	TIME [epoch: 3.46 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7331034356552459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7331034356552459 | validation: 0.917302625127531]
	TIME [epoch: 3.47 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7822228635402617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7822228635402617 | validation: 0.7606202026092798]
	TIME [epoch: 3.46 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7393882161004046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7393882161004046 | validation: 0.7337526001360778]
	TIME [epoch: 3.46 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6898744329803509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6898744329803509 | validation: 0.7401952334663777]
	TIME [epoch: 3.46 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6998511238408868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6998511238408868 | validation: 0.7445740190121448]
	TIME [epoch: 3.46 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6872761085180489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6872761085180489 | validation: 0.7305275276346168]
	TIME [epoch: 3.47 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6896311051378947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6896311051378947 | validation: 0.7621444801745293]
	TIME [epoch: 3.46 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6936287344364989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6936287344364989 | validation: 0.7155271339817958]
	TIME [epoch: 3.46 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69434156668208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.69434156668208 | validation: 0.8098283922937416]
	TIME [epoch: 3.47 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.718801535306545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.718801535306545 | validation: 0.750008684859632]
	TIME [epoch: 3.46 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7312144292090722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7312144292090722 | validation: 0.8386027523667663]
	TIME [epoch: 3.45 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7279716821645345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7279716821645345 | validation: 0.726256575486923]
	TIME [epoch: 3.45 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6943770662430945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6943770662430945 | validation: 0.7115762560641923]
	TIME [epoch: 3.44 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6824275455945267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6824275455945267 | validation: 0.7486474390766221]
	TIME [epoch: 3.44 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6815984903042034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6815984903042034 | validation: 0.742640909501319]
	TIME [epoch: 3.44 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6954292009433232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6954292009433232 | validation: 0.7972688849654515]
	TIME [epoch: 3.45 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.735002304177411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.735002304177411 | validation: 0.7760098478304862]
	TIME [epoch: 3.44 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7212309556144811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7212309556144811 | validation: 0.7519736483026619]
	TIME [epoch: 3.45 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7183704911785981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7183704911785981 | validation: 0.7858413731854922]
	TIME [epoch: 3.46 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7060272172781231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7060272172781231 | validation: 0.7450484590249428]
	TIME [epoch: 3.47 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7027292338465807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7027292338465807 | validation: 0.7697003771077836]
	TIME [epoch: 3.47 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7020268194488624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7020268194488624 | validation: 0.7140499459305261]
	TIME [epoch: 3.44 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6821623606689949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6821623606689949 | validation: 0.7226135943138963]
	TIME [epoch: 3.45 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6692007403352287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6692007403352287 | validation: 0.709636186695022]
	TIME [epoch: 3.45 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.670556784542152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.670556784542152 | validation: 0.7477685157428424]
	TIME [epoch: 3.45 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7025311269406492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7025311269406492 | validation: 0.7441519657838894]
	TIME [epoch: 3.44 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7388351386597085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7388351386597085 | validation: 0.827420976324413]
	TIME [epoch: 3.45 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.727008629993612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.727008629993612 | validation: 0.6977676701771103]
	TIME [epoch: 3.44 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.686682101705885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.686682101705885 | validation: 0.7602097943587771]
	TIME [epoch: 3.44 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7052840285173596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7052840285173596 | validation: 0.7307864578901048]
	TIME [epoch: 3.44 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7131265320551685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7131265320551685 | validation: 0.7645655903852635]
	TIME [epoch: 3.46 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7075736407186637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7075736407186637 | validation: 0.7164852348175534]
	TIME [epoch: 3.44 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6749177369199546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6749177369199546 | validation: 0.7298644479511747]
	TIME [epoch: 3.44 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6838938740291617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6838938740291617 | validation: 0.7309691438761894]
	TIME [epoch: 3.44 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6899763812814749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6899763812814749 | validation: 0.7419068357617427]
	TIME [epoch: 3.44 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6388671230579348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6388671230579348 | validation: 0.9650059369380021]
	TIME [epoch: 3.44 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9645950872812256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9645950872812256 | validation: 0.7573282882302239]
	TIME [epoch: 3.45 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8160921471196622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8160921471196622 | validation: 0.9158756352803235]
	TIME [epoch: 3.48 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152501001393858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8152501001393858 | validation: 0.7893448468583268]
	TIME [epoch: 3.46 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7313825953588385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7313825953588385 | validation: 0.6861506516153829]
	TIME [epoch: 3.46 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7143114589611678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7143114589611678 | validation: 0.7436553425740905]
	TIME [epoch: 3.46 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066356648219199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066356648219199 | validation: 0.7011133293112639]
	TIME [epoch: 3.46 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.684429531634431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.684429531634431 | validation: 0.6954730740845134]
	TIME [epoch: 3.45 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6812651563863691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6812651563863691 | validation: 0.6973635610814258]
	TIME [epoch: 3.45 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6752247915755626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6752247915755626 | validation: 0.7332216293020245]
	TIME [epoch: 3.46 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.684993454917428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.684993454917428 | validation: 0.6930224980994032]
	TIME [epoch: 3.46 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6782651170819712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6782651170819712 | validation: 0.7071420656456575]
	TIME [epoch: 3.46 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6711132785244791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6711132785244791 | validation: 0.6921713047336788]
	TIME [epoch: 3.46 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.676887071906465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.676887071906465 | validation: 0.7144250377092528]
	TIME [epoch: 3.45 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6669236911417821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6669236911417821 | validation: 0.7124816862002037]
	TIME [epoch: 3.46 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.665756878079842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.665756878079842 | validation: 0.6907103206216564]
	TIME [epoch: 3.45 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6778099451363357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6778099451363357 | validation: 0.7668750943508457]
	TIME [epoch: 3.45 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001576354707993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001576354707993 | validation: 0.786365448422619]
	TIME [epoch: 3.44 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8315376805083683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8315376805083683 | validation: 0.8645196754607287]
	TIME [epoch: 3.45 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7523403237611598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7523403237611598 | validation: 0.7133701240802575]
	TIME [epoch: 3.45 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6682993188008222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6682993188008222 | validation: 0.6939576880534526]
	TIME [epoch: 3.45 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6840403529672391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6840403529672391 | validation: 0.7493364231235674]
	TIME [epoch: 3.45 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918444876328065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6918444876328065 | validation: 0.6976353596962512]
	TIME [epoch: 3.45 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6695910011793555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6695910011793555 | validation: 0.6947788673843703]
	TIME [epoch: 3.45 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6677339301187982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6677339301187982 | validation: 0.7266300501250552]
	TIME [epoch: 3.44 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6702803890927556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6702803890927556 | validation: 0.7087202595290449]
	TIME [epoch: 3.44 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6679111810146392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6679111810146392 | validation: 0.7179530095113401]
	TIME [epoch: 3.45 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6726006887638853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6726006887638853 | validation: 0.7125633683145054]
	TIME [epoch: 3.44 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6720622622350777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6720622622350777 | validation: 0.7515388964660303]
	TIME [epoch: 3.44 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.683310727998776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.683310727998776 | validation: 0.7179986329811782]
	TIME [epoch: 3.44 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719248906831936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.719248906831936 | validation: 0.8195893975736681]
	TIME [epoch: 3.43 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7136972584234144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7136972584234144 | validation: 0.6958229959521829]
	TIME [epoch: 3.44 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6658824864511175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6658824864511175 | validation: 0.7231198691119176]
	TIME [epoch: 3.46 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6552206086196217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6552206086196217 | validation: 0.7012651694287833]
	TIME [epoch: 3.45 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503910445801426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6503910445801426 | validation: 0.7270214342759167]
	TIME [epoch: 3.45 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6508439228757235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6508439228757235 | validation: 0.6784390811944591]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6498575889904348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6498575889904348 | validation: 1.1625727810165856]
	TIME [epoch: 3.45 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0250852380165656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0250852380165656 | validation: 0.676907989705752]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7185284110347196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7185284110347196 | validation: 0.6603458617848834]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6819118314916631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6819118314916631 | validation: 0.7075708117067498]
	TIME [epoch: 3.44 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6867884642847888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6867884642847888 | validation: 0.7104086190190686]
	TIME [epoch: 3.44 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781801162761858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6781801162761858 | validation: 0.665683063615894]
	TIME [epoch: 3.45 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6766112526421371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6766112526421371 | validation: 0.7498836900633578]
	TIME [epoch: 3.45 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6954596457577862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6954596457577862 | validation: 0.6944036685346213]
	TIME [epoch: 3.45 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6795165900971282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6795165900971282 | validation: 0.7007383782704188]
	TIME [epoch: 3.44 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6724743736117443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6724743736117443 | validation: 0.6905486967134373]
	TIME [epoch: 3.45 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6710162561936969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6710162561936969 | validation: 0.7498182248356486]
	TIME [epoch: 3.45 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6980328033452768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6980328033452768 | validation: 0.7110836671550717]
	TIME [epoch: 3.46 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7045568414261731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7045568414261731 | validation: 0.8086665053966411]
	TIME [epoch: 3.44 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7157875764346737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7157875764346737 | validation: 0.6893739531072377]
	TIME [epoch: 3.44 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6777031521592021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6777031521592021 | validation: 0.6930415121420072]
	TIME [epoch: 3.44 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6628825581446113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6628825581446113 | validation: 0.7538614466751534]
	TIME [epoch: 3.44 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6887500081569329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6887500081569329 | validation: 0.6997398776216417]
	TIME [epoch: 3.45 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6882007208513798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6882007208513798 | validation: 0.7359306022985106]
	TIME [epoch: 3.44 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6672660235765802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6672660235765802 | validation: 0.7000843557587955]
	TIME [epoch: 3.44 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6609356331762553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6609356331762553 | validation: 0.713063132627561]
	TIME [epoch: 3.46 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524742590587868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6524742590587868 | validation: 0.6845445481647721]
	TIME [epoch: 3.45 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6482913838609453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6482913838609453 | validation: 0.690451170841543]
	TIME [epoch: 3.45 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6532359796222673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6532359796222673 | validation: 0.6804045229608435]
	TIME [epoch: 3.45 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6596971588939292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6596971588939292 | validation: 0.7682480139332258]
	TIME [epoch: 3.45 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6740094676463372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6740094676463372 | validation: 0.6877771332803354]
	TIME [epoch: 3.46 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6701618727199095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6701618727199095 | validation: 1.076608714364033]
	TIME [epoch: 3.45 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0851850229870175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0851850229870175 | validation: 0.8344092343142262]
	TIME [epoch: 3.46 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8244519080931071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8244519080931071 | validation: 0.716990395702746]
	TIME [epoch: 3.45 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7047466052682196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7047466052682196 | validation: 0.7585585455054398]
	TIME [epoch: 3.46 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7068369491685793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7068369491685793 | validation: 0.7131295967068981]
	TIME [epoch: 3.45 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6719224196838467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6719224196838467 | validation: 0.6876883520078083]
	TIME [epoch: 3.47 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6790454338719257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6790454338719257 | validation: 0.6840836759051353]
	TIME [epoch: 3.47 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6626741869964562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6626741869964562 | validation: 0.6963472486975282]
	TIME [epoch: 3.48 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.668869267900383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.668869267900383 | validation: 0.7119610290187538]
	TIME [epoch: 3.46 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6692258723844302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6692258723844302 | validation: 0.7043382349891255]
	TIME [epoch: 3.46 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.659893982821937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.659893982821937 | validation: 0.7067837593892641]
	TIME [epoch: 3.46 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6734655758344162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6734655758344162 | validation: 0.7564187819938232]
	TIME [epoch: 3.45 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6669931867890424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6669931867890424 | validation: 0.6916592736120388]
	TIME [epoch: 3.47 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607771216686626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6607771216686626 | validation: 0.7214202248215188]
	TIME [epoch: 3.47 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6590672116506265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6590672116506265 | validation: 0.6719927854327642]
	TIME [epoch: 3.45 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6311020262332727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6311020262332727 | validation: 0.6617958509798276]
	TIME [epoch: 3.46 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6050756877686528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6050756877686528 | validation: 0.6784337194337015]
	TIME [epoch: 3.45 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6565188198907006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6565188198907006 | validation: 0.6916237806899878]
	TIME [epoch: 3.47 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439645061473327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7439645061473327 | validation: 0.7801412340889944]
	TIME [epoch: 3.45 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.738075516572066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.738075516572066 | validation: 0.6921429314082271]
	TIME [epoch: 3.45 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6659001039580638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6659001039580638 | validation: 0.6910578665958744]
	TIME [epoch: 3.45 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6902873851290111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6902873851290111 | validation: 0.676342202762177]
	TIME [epoch: 3.45 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6518873030308376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6518873030308376 | validation: 0.6956538074749288]
	TIME [epoch: 3.45 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6548647305776102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6548647305776102 | validation: 0.6622425707708081]
	TIME [epoch: 3.46 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.624138436033195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.624138436033195 | validation: 0.7284002657013051]
	TIME [epoch: 3.45 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.626031138593436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.626031138593436 | validation: 0.6507509978936142]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6007632710707579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6007632710707579 | validation: 1.7085197567973922]
	TIME [epoch: 3.46 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.405400448516296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.405400448516296 | validation: 0.756837361703462]
	TIME [epoch: 3.47 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7624882606146005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7624882606146005 | validation: 0.6494303781974797]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6358419353120207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6358419353120207 | validation: 0.7103663004629692]
	TIME [epoch: 3.46 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6802008663012474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6802008663012474 | validation: 0.6616755094812365]
	TIME [epoch: 3.45 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403257830731837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6403257830731837 | validation: 0.6402734012301246]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6457681460143361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6457681460143361 | validation: 0.6799031248509406]
	TIME [epoch: 3.46 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6463280391782763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6463280391782763 | validation: 0.6115280514189838]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6078109068663239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6078109068663239 | validation: 0.6183051666576116]
	TIME [epoch: 3.46 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5681707171417402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5681707171417402 | validation: 0.6200197400656844]
	TIME [epoch: 3.45 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5440850217821287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5440850217821287 | validation: 0.5896510469237358]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5288017230451613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5288017230451613 | validation: 0.6359527688022845]
	TIME [epoch: 3.48 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5313171603010628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5313171603010628 | validation: 0.6472178412675836]
	TIME [epoch: 3.45 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7236582961524646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7236582961524646 | validation: 0.7274579387674323]
	TIME [epoch: 3.45 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6910351431827931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6910351431827931 | validation: 0.6117451670636097]
	TIME [epoch: 3.46 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.626246736210076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.626246736210076 | validation: 0.5670570107279562]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5973179280635209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5973179280635209 | validation: 0.5786363823724406]
	TIME [epoch: 3.45 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.537606436165621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.537606436165621 | validation: 0.5832919318200775]
	TIME [epoch: 3.45 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5162147353581044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5162147353581044 | validation: 0.5962400103991079]
	TIME [epoch: 3.45 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4990216162269853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4990216162269853 | validation: 0.5438838734204311]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_442.pth
	Model improved!!!
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5036357347183451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5036357347183451 | validation: 0.5933933611415659]
	TIME [epoch: 3.46 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4899817245852435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4899817245852435 | validation: 0.5971055584437506]
	TIME [epoch: 3.48 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5916377787520797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5916377787520797 | validation: 0.8882973109759773]
	TIME [epoch: 3.45 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576028218904222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7576028218904222 | validation: 0.5207625654883465]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5534362714184691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5534362714184691 | validation: 0.5415921560128817]
	TIME [epoch: 3.45 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5720573376045822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5720573376045822 | validation: 0.5120059091871556]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5050973426548734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5050973426548734 | validation: 0.5057058361127253]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4669914057756192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4669914057756192 | validation: 0.4819552149417252]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_450.pth
	Model improved!!!
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4599897670289994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4599897670289994 | validation: 0.5089914107806657]
	TIME [epoch: 3.47 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4832460715837097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4832460715837097 | validation: 0.45246262854963554]
	TIME [epoch: 3.48 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4542300026244965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4542300026244965 | validation: 0.46392201377217257]
	TIME [epoch: 3.47 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43698469229096787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43698469229096787 | validation: 0.4517075341107957]
	TIME [epoch: 3.48 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44394754101624456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44394754101624456 | validation: 0.4675045585982092]
	TIME [epoch: 3.48 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4386474691114532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4386474691114532 | validation: 0.4649958209058147]
	TIME [epoch: 3.48 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4496437376439266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4496437376439266 | validation: 0.4626120526229891]
	TIME [epoch: 3.46 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46074104875281435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46074104875281435 | validation: 0.43471440108294335]
	TIME [epoch: 3.48 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.439178919295119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.439178919295119 | validation: 0.7824251732801099]
	TIME [epoch: 3.46 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5825050270504748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5825050270504748 | validation: 0.7451341069372215]
	TIME [epoch: 3.47 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6788132978747475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6788132978747475 | validation: 0.6808646647542128]
	TIME [epoch: 3.47 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6533641132204119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6533641132204119 | validation: 0.4761346724600092]
	TIME [epoch: 3.48 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4557999195773897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4557999195773897 | validation: 0.532297746033117]
	TIME [epoch: 3.49 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48470982404452073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48470982404452073 | validation: 0.41273276039214757]
	TIME [epoch: 3.48 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4261118222413093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4261118222413093 | validation: 0.4091064935658474]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_465.pth
	Model improved!!!
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4190111928633779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4190111928633779 | validation: 0.3884198139065085]
	TIME [epoch: 3.48 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3995078817020298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3995078817020298 | validation: 0.466144982337946]
	TIME [epoch: 3.46 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41073435713324047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41073435713324047 | validation: 0.4012646440422305]
	TIME [epoch: 3.47 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41903696089420794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41903696089420794 | validation: 0.38979837109124915]
	TIME [epoch: 3.48 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4049526365808781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4049526365808781 | validation: 0.41265597343237315]
	TIME [epoch: 3.48 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3879455712880175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3879455712880175 | validation: 0.40900264932806385]
	TIME [epoch: 3.48 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40281118821467227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40281118821467227 | validation: 0.42772111871500873]
	TIME [epoch: 3.49 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4422529214141677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4422529214141677 | validation: 0.5086252284918776]
	TIME [epoch: 3.5 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4722183686666703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4722183686666703 | validation: 0.40757285833917456]
	TIME [epoch: 3.48 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42250685638021657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42250685638021657 | validation: 0.38508350680251563]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39437462570633103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39437462570633103 | validation: 0.35317713480634955]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36922035051067814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36922035051067814 | validation: 0.35077091378325953]
	TIME [epoch: 3.49 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3527790586654379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3527790586654379 | validation: 0.39030979896569434]
	TIME [epoch: 3.48 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3721591833442774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3721591833442774 | validation: 0.49497021230737115]
	TIME [epoch: 3.49 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4632597717255261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4632597717255261 | validation: 0.39952719713573215]
	TIME [epoch: 3.49 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4137970986147802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4137970986147802 | validation: 0.5223362373417069]
	TIME [epoch: 3.49 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43992544187469196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43992544187469196 | validation: 0.4075498332976148]
	TIME [epoch: 3.5 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4282277129184678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4282277129184678 | validation: 0.4005065205595676]
	TIME [epoch: 3.5 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.401372364299822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.401372364299822 | validation: 0.43983534912357874]
	TIME [epoch: 3.48 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42562484592885513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42562484592885513 | validation: 0.404718463850353]
	TIME [epoch: 3.48 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3973479464939776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3973479464939776 | validation: 0.35984225027233957]
	TIME [epoch: 3.48 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35880455408072004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35880455408072004 | validation: 0.34097554181742573]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34245791852267277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34245791852267277 | validation: 0.5295494604211769]
	TIME [epoch: 3.46 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45946338047537083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45946338047537083 | validation: 0.6105841569073229]
	TIME [epoch: 3.49 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5286734877558481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5286734877558481 | validation: 0.6065813286931298]
	TIME [epoch: 3.48 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5558082558983093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5558082558983093 | validation: 0.35754118206736174]
	TIME [epoch: 3.47 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3912528842062766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3912528842062766 | validation: 0.43233443508687525]
	TIME [epoch: 3.48 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4348889895555595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4348889895555595 | validation: 0.3903448993156981]
	TIME [epoch: 3.5 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41913914448265943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41913914448265943 | validation: 0.38489730440837644]
	TIME [epoch: 3.49 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36918400732642453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36918400732642453 | validation: 0.39417269641364244]
	TIME [epoch: 3.48 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3721215518151517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3721215518151517 | validation: 0.3242831200340122]
	TIME [epoch: 3.49 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_496.pth
	Model improved!!!
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3498669976722688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3498669976722688 | validation: 0.3482540569075721]
	TIME [epoch: 3.47 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35715878803679607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35715878803679607 | validation: 0.32882637672915943]
	TIME [epoch: 3.49 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3491925468589122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3491925468589122 | validation: 0.30955005541960107]
	TIME [epoch: 3.49 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_499.pth
	Model improved!!!
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33531212347384076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33531212347384076 | validation: 0.32519247234247256]
	TIME [epoch: 3.49 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3298962744794611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3298962744794611 | validation: 0.35850920272404924]
	TIME [epoch: 36.3 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33700283277681636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33700283277681636 | validation: 0.35616241717052993]
	TIME [epoch: 7.64 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40452295828998147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40452295828998147 | validation: 0.4151005460653619]
	TIME [epoch: 7.63 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47163177446894394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47163177446894394 | validation: 0.49575108765737125]
	TIME [epoch: 7.65 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4463553915515016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4463553915515016 | validation: 0.6150427306155253]
	TIME [epoch: 7.63 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5388573370558509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5388573370558509 | validation: 0.4365070338320656]
	TIME [epoch: 7.64 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4658061529044636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4658061529044636 | validation: 0.33910156736705677]
	TIME [epoch: 7.62 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3411838235554896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3411838235554896 | validation: 0.39640830273321054]
	TIME [epoch: 7.63 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.366211955903393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.366211955903393 | validation: 0.2884114915873835]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.336562778909502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.336562778909502 | validation: 0.3237566251347103]
	TIME [epoch: 7.66 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3337844072207294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3337844072207294 | validation: 0.30050360357960343]
	TIME [epoch: 7.64 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3172245572951382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3172245572951382 | validation: 0.2963902703208729]
	TIME [epoch: 7.64 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30595041080124313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30595041080124313 | validation: 0.29209533429294304]
	TIME [epoch: 7.65 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30850450502587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30850450502587 | validation: 0.3746534450403443]
	TIME [epoch: 7.63 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3519794900278188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3519794900278188 | validation: 0.46956226053824807]
	TIME [epoch: 7.61 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4901458192421207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4901458192421207 | validation: 0.30819746374167745]
	TIME [epoch: 7.64 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3726031305148857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3726031305148857 | validation: 0.4367382467296091]
	TIME [epoch: 7.65 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41712454056996734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41712454056996734 | validation: 0.343566469165717]
	TIME [epoch: 7.66 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3880911702056517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3880911702056517 | validation: 0.28003362531724346]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3193050024451104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3193050024451104 | validation: 0.3956661925539193]
	TIME [epoch: 7.65 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35006621500904084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35006621500904084 | validation: 0.3111790837579516]
	TIME [epoch: 7.61 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3493718775854211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3493718775854211 | validation: 0.31361934560658533]
	TIME [epoch: 7.65 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33469262606975086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33469262606975086 | validation: 0.29274586014164344]
	TIME [epoch: 7.66 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2983500927909173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2983500927909173 | validation: 0.26816479257282844]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27396690871097085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27396690871097085 | validation: 0.29964218694454803]
	TIME [epoch: 7.62 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2945435901845127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2945435901845127 | validation: 0.4064946284082508]
	TIME [epoch: 7.64 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4592113470226941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4592113470226941 | validation: 0.2914682930082139]
	TIME [epoch: 7.62 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3669238057382937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3669238057382937 | validation: 0.4902769881949876]
	TIME [epoch: 7.66 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4432423734782106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4432423734782106 | validation: 0.3575337995207042]
	TIME [epoch: 7.64 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40431884236141685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40431884236141685 | validation: 0.3035537276544147]
	TIME [epoch: 7.66 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.368365533057615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.368365533057615 | validation: 0.318863477109685]
	TIME [epoch: 7.63 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3143156431952502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3143156431952502 | validation: 0.2962951741882263]
	TIME [epoch: 7.66 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3479545030756647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3479545030756647 | validation: 0.35101775901955246]
	TIME [epoch: 7.65 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3401273064697769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3401273064697769 | validation: 0.39874142804789126]
	TIME [epoch: 7.65 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4292024580514257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4292024580514257 | validation: 0.2920509787460929]
	TIME [epoch: 7.64 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3284126647510583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3284126647510583 | validation: 0.34761573260094386]
	TIME [epoch: 7.66 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32251262576750783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32251262576750783 | validation: 0.2891697075240009]
	TIME [epoch: 7.65 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32094499899643675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32094499899643675 | validation: 0.28270581146087936]
	TIME [epoch: 7.67 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28108005224492383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28108005224492383 | validation: 0.2530599606869303]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27098032323697674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27098032323697674 | validation: 0.3360119552592244]
	TIME [epoch: 7.65 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32284665069301904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32284665069301904 | validation: 0.6711354318587058]
	TIME [epoch: 7.62 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.587596118865534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.587596118865534 | validation: 0.3118438032855204]
	TIME [epoch: 7.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37343639544054213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37343639544054213 | validation: 0.545473352712461]
	TIME [epoch: 7.53 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5941979399714921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5941979399714921 | validation: 0.3184304387168136]
	TIME [epoch: 7.48 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3500630976908481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3500630976908481 | validation: 0.3082177293322641]
	TIME [epoch: 7.48 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3424181953402865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3424181953402865 | validation: 0.38589189542517544]
	TIME [epoch: 7.63 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3574179232984908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3574179232984908 | validation: 0.30868509352399826]
	TIME [epoch: 7.57 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32704819828889237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32704819828889237 | validation: 0.2766634282626695]
	TIME [epoch: 7.64 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3007974698254476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3007974698254476 | validation: 0.2939498156262671]
	TIME [epoch: 7.65 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2902907010956982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2902907010956982 | validation: 0.2616269362857875]
	TIME [epoch: 7.64 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25997606564703035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25997606564703035 | validation: 0.2548467015048306]
	TIME [epoch: 7.65 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2641316354342192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2641316354342192 | validation: 0.2689574061324605]
	TIME [epoch: 7.64 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26216476430544083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26216476430544083 | validation: 0.2241321191739706]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2710226910514626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2710226910514626 | validation: 0.3071340627844632]
	TIME [epoch: 7.65 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30759154292177693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30759154292177693 | validation: 0.454492480773637]
	TIME [epoch: 7.64 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4865934206856229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4865934206856229 | validation: 0.31369571177824834]
	TIME [epoch: 7.65 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38561954787831154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38561954787831154 | validation: 0.33843923863902176]
	TIME [epoch: 7.65 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3414552448541866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3414552448541866 | validation: 0.26672028228191985]
	TIME [epoch: 7.63 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29444865079473465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29444865079473465 | validation: 0.2631186169416038]
	TIME [epoch: 7.61 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28839623785488905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28839623785488905 | validation: 0.276999882390434]
	TIME [epoch: 7.61 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2698428335878532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2698428335878532 | validation: 0.22526125596733304]
	TIME [epoch: 7.63 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24242111865830995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24242111865830995 | validation: 0.24290247975751217]
	TIME [epoch: 7.59 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23940585281916987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23940585281916987 | validation: 0.24215936274942718]
	TIME [epoch: 7.62 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27873494460722104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27873494460722104 | validation: 0.4440857422233583]
	TIME [epoch: 7.62 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4505642740843653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4505642740843653 | validation: 0.48578064036009094]
	TIME [epoch: 7.62 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5039033545555428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5039033545555428 | validation: 0.2424569629849157]
	TIME [epoch: 7.63 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28184180554567445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28184180554567445 | validation: 0.38438578846209914]
	TIME [epoch: 7.64 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35567924578685156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35567924578685156 | validation: 0.2824730823087655]
	TIME [epoch: 7.63 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31798206916056954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31798206916056954 | validation: 0.2589018377351667]
	TIME [epoch: 7.62 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28724347141376383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28724347141376383 | validation: 0.28852901304330353]
	TIME [epoch: 7.63 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28275052824436403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28275052824436403 | validation: 0.24812735874395866]
	TIME [epoch: 7.65 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2755665020141366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2755665020141366 | validation: 0.23397362043847428]
	TIME [epoch: 7.62 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27875064371026387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27875064371026387 | validation: 0.28964852273517094]
	TIME [epoch: 7.64 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2925001140814885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2925001140814885 | validation: 0.23101392418062844]
	TIME [epoch: 7.62 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2720940590548072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2720940590548072 | validation: 0.23166196073990553]
	TIME [epoch: 7.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24591357097924388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24591357097924388 | validation: 0.2391969026173184]
	TIME [epoch: 7.64 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23901422811951464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23901422811951464 | validation: 0.22158056759129385]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22443428001835222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22443428001835222 | validation: 0.23908611998361265]
	TIME [epoch: 7.62 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23814869951510895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23814869951510895 | validation: 0.3051813993588658]
	TIME [epoch: 7.61 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3438549002873323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3438549002873323 | validation: 0.27597577880784135]
	TIME [epoch: 7.64 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2905825344682997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2905825344682997 | validation: 0.21103297638395999]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_581.pth
	Model improved!!!
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23600270002315993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23600270002315993 | validation: 0.20383367165561186]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_582.pth
	Model improved!!!
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21442906425020475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21442906425020475 | validation: 0.25681125652629555]
	TIME [epoch: 7.63 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3112192342965791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3112192342965791 | validation: 0.22069034740711468]
	TIME [epoch: 7.64 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23873022183063453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23873022183063453 | validation: 0.21088379703961158]
	TIME [epoch: 7.66 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21245999254052195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21245999254052195 | validation: 0.2069147846245495]
	TIME [epoch: 7.65 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22788349051442766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22788349051442766 | validation: 0.2995708297586848]
	TIME [epoch: 7.63 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34108813698492596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34108813698492596 | validation: 0.2315246877836885]
	TIME [epoch: 7.62 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2514242203531415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2514242203531415 | validation: 0.3114825058564238]
	TIME [epoch: 7.65 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31268253598600526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31268253598600526 | validation: 0.4254516899284269]
	TIME [epoch: 7.64 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43157775614497645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43157775614497645 | validation: 0.43641897152043524]
	TIME [epoch: 7.66 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4583594046541397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4583594046541397 | validation: 0.22688931459006448]
	TIME [epoch: 7.64 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3109585556505561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3109585556505561 | validation: 0.21144066214004634]
	TIME [epoch: 7.66 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27005860517128066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27005860517128066 | validation: 0.2452761221420818]
	TIME [epoch: 7.64 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23788162729217946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23788162729217946 | validation: 0.2512606555724443]
	TIME [epoch: 7.64 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23921396980979398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23921396980979398 | validation: 0.21754419402526504]
	TIME [epoch: 7.65 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22036513922379108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22036513922379108 | validation: 0.19836384504702242]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19350272176955918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19350272176955918 | validation: 0.20626041096533962]
	TIME [epoch: 7.59 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18353588661048154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18353588661048154 | validation: 0.19361999244641834]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_599.pth
	Model improved!!!
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1719871038628504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1719871038628504 | validation: 0.22724584375751902]
	TIME [epoch: 7.65 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24346951518477852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24346951518477852 | validation: 0.30604350446880163]
	TIME [epoch: 7.64 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34558286696144097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34558286696144097 | validation: 1.2133460910586686]
	TIME [epoch: 7.62 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8777174495447964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8777174495447964 | validation: 1.3573036776440455]
	TIME [epoch: 7.63 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0039972031399003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0039972031399003 | validation: 0.6721387307243377]
	TIME [epoch: 7.64 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4977632099947842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4977632099947842 | validation: 0.43981529462750424]
	TIME [epoch: 7.67 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4355162231824274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4355162231824274 | validation: 0.37543816206299874]
	TIME [epoch: 7.62 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.393809426198764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.393809426198764 | validation: 0.30712924184078744]
	TIME [epoch: 7.64 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33489014373246984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33489014373246984 | validation: 0.3214187619328883]
	TIME [epoch: 7.63 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28174485999552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28174485999552 | validation: 0.3727268693523321]
	TIME [epoch: 7.63 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28405325974210405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28405325974210405 | validation: 0.25604754933124524]
	TIME [epoch: 7.68 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24135544398354797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24135544398354797 | validation: 0.2280303089900312]
	TIME [epoch: 7.65 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2257050646309055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2257050646309055 | validation: 0.23643396021676027]
	TIME [epoch: 7.63 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2127984519475692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2127984519475692 | validation: 0.2087960028326525]
	TIME [epoch: 7.63 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20780445039953135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20780445039953135 | validation: 0.2122567646536347]
	TIME [epoch: 7.61 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1862318726906282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1862318726906282 | validation: 0.1871219841004177]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18819156563050307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18819156563050307 | validation: 0.18141806421154216]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_616.pth
	Model improved!!!
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17589136213340223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17589136213340223 | validation: 0.17677197675981288]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1885634051699319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1885634051699319 | validation: 0.20101173746819634]
	TIME [epoch: 7.62 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18186851816280508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18186851816280508 | validation: 0.18154675105654522]
	TIME [epoch: 7.63 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1923005618683354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1923005618683354 | validation: 0.2133947177789775]
	TIME [epoch: 7.63 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2277835313919438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2277835313919438 | validation: 0.21736323131676805]
	TIME [epoch: 7.63 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.239885523705887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.239885523705887 | validation: 0.16930497023433588]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1722421983670749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1722421983670749 | validation: 0.3443504518903745]
	TIME [epoch: 7.66 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36960554722372563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36960554722372563 | validation: 0.21863673634027903]
	TIME [epoch: 7.67 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21758931968816764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21758931968816764 | validation: 0.2045046606639338]
	TIME [epoch: 7.65 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19761782315766974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19761782315766974 | validation: 0.18490076566583064]
	TIME [epoch: 7.65 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1877145299101846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1877145299101846 | validation: 0.1918739441274841]
	TIME [epoch: 7.65 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1773487355969717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1773487355969717 | validation: 0.1832191178182354]
	TIME [epoch: 7.65 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17002803739720307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17002803739720307 | validation: 0.1753201472512964]
	TIME [epoch: 7.66 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16967758071514993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16967758071514993 | validation: 0.1804849549771414]
	TIME [epoch: 7.64 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18221818912902243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18221818912902243 | validation: 0.17362565378750996]
	TIME [epoch: 7.65 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16960240963466994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16960240963466994 | validation: 0.1786009062620868]
	TIME [epoch: 7.65 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1832373148076152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1832373148076152 | validation: 0.16924307228185387]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_633.pth
	Model improved!!!
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16551885004641415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16551885004641415 | validation: 0.21414945554674453]
	TIME [epoch: 7.67 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22723786224469378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22723786224469378 | validation: 0.17091921089722273]
	TIME [epoch: 7.65 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1710183951113619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1710183951113619 | validation: 0.2007246855394964]
	TIME [epoch: 7.64 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20713484025736315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20713484025736315 | validation: 0.1999180126987532]
	TIME [epoch: 7.64 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23414355972142986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23414355972142986 | validation: 0.22702588865114504]
	TIME [epoch: 7.65 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23242620361185395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23242620361185395 | validation: 0.20670277522520364]
	TIME [epoch: 7.66 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1754119015570727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1754119015570727 | validation: 0.19575998986932294]
	TIME [epoch: 7.62 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15792866357188298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15792866357188298 | validation: 0.1802542641983902]
	TIME [epoch: 7.63 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14882237453512118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14882237453512118 | validation: 0.15227509317454374]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1298274001620804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1298274001620804 | validation: 0.1556559468475452]
	TIME [epoch: 7.64 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13314607876421528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13314607876421528 | validation: 0.14582638058452196]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_644.pth
	Model improved!!!
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14975983789001154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14975983789001154 | validation: 0.3301317449503108]
	TIME [epoch: 7.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.347167160279501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.347167160279501 | validation: 0.2748381438241127]
	TIME [epoch: 7.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2652605391364087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2652605391364087 | validation: 0.1702576954793357]
	TIME [epoch: 7.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15232481433949394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15232481433949394 | validation: 0.13541047246544768]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1328425612060832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1328425612060832 | validation: 0.12799727382425966]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_649.pth
	Model improved!!!
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13277365902005564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13277365902005564 | validation: 0.2680565004509954]
	TIME [epoch: 7.61 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31819291539967576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31819291539967576 | validation: 0.16687768641611692]
	TIME [epoch: 7.59 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16304071330578054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16304071330578054 | validation: 0.14490464857418742]
	TIME [epoch: 7.62 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11850481323147406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11850481323147406 | validation: 0.14466668861148488]
	TIME [epoch: 7.63 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11955223309062682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11955223309062682 | validation: 0.3319044321352653]
	TIME [epoch: 7.62 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4134887506678895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4134887506678895 | validation: 0.3531517718782992]
	TIME [epoch: 7.62 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41688937190390407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41688937190390407 | validation: 0.20409949004695327]
	TIME [epoch: 7.61 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2506140310583372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2506140310583372 | validation: 0.31078965143510584]
	TIME [epoch: 7.63 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.317204084033165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.317204084033165 | validation: 0.2646303312558206]
	TIME [epoch: 7.64 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3108654839763342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3108654839763342 | validation: 0.28358604020839784]
	TIME [epoch: 7.62 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3047674136024174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3047674136024174 | validation: 0.2724485176984007]
	TIME [epoch: 7.62 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26857173602608136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26857173602608136 | validation: 0.2396455697809035]
	TIME [epoch: 7.61 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2312885962334896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2312885962334896 | validation: 0.23806941433575243]
	TIME [epoch: 7.63 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21355117344799296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21355117344799296 | validation: 0.2113495971129579]
	TIME [epoch: 7.63 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2014538034203291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2014538034203291 | validation: 0.2066920258639583]
	TIME [epoch: 7.61 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18344458468227245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18344458468227245 | validation: 0.20072761342424414]
	TIME [epoch: 7.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17051594346703178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17051594346703178 | validation: 0.21463133712493337]
	TIME [epoch: 7.62 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16374015531584743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16374015531584743 | validation: 0.2648305886551146]
	TIME [epoch: 7.62 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2724948046828705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2724948046828705 | validation: 0.32030148664514346]
	TIME [epoch: 7.63 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3013665269043464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3013665269043464 | validation: 0.18723816836887394]
	TIME [epoch: 7.61 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17443752670680912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17443752670680912 | validation: 0.15898271297052233]
	TIME [epoch: 7.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12686537216356922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12686537216356922 | validation: 0.13944294916169328]
	TIME [epoch: 7.6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10937980482844971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10937980482844971 | validation: 0.1426850941196784]
	TIME [epoch: 7.62 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10783242183903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10783242183903 | validation: 0.22856204104023084]
	TIME [epoch: 7.62 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23734196785156456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23734196785156456 | validation: 0.48008369481745705]
	TIME [epoch: 7.61 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48720318653636313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48720318653636313 | validation: 0.3914429495463473]
	TIME [epoch: 7.62 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42463060551505066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42463060551505066 | validation: 0.2192647514381866]
	TIME [epoch: 7.62 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25745583467116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25745583467116 | validation: 0.700695610694074]
	TIME [epoch: 7.63 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7806492651897733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7806492651897733 | validation: 0.42841585478831007]
	TIME [epoch: 7.63 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4120894145946277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4120894145946277 | validation: 0.5531761679767975]
	TIME [epoch: 7.61 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4455491802087937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4455491802087937 | validation: 0.44146646788747157]
	TIME [epoch: 7.61 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3802249650266737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3802249650266737 | validation: 0.3022773637318849]
	TIME [epoch: 7.62 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2863017092064926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2863017092064926 | validation: 0.24112739418752743]
	TIME [epoch: 7.63 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24106230315640603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24106230315640603 | validation: 0.21820067272483287]
	TIME [epoch: 7.63 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20950372151051283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20950372151051283 | validation: 0.21121549945679538]
	TIME [epoch: 7.62 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1789613219486229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1789613219486229 | validation: 0.20601380124736682]
	TIME [epoch: 7.62 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1669379486246184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1669379486246184 | validation: 0.1825624743119446]
	TIME [epoch: 7.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13515847509874251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13515847509874251 | validation: 0.19809568936516572]
	TIME [epoch: 7.63 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15056187811537947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15056187811537947 | validation: 0.2951146638292362]
	TIME [epoch: 7.62 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2865590128510891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2865590128510891 | validation: 0.5008749885227658]
	TIME [epoch: 7.63 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5022979380932047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5022979380932047 | validation: 0.284516800243924]
	TIME [epoch: 7.62 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2844643797327969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2844643797327969 | validation: 0.23280284728806358]
	TIME [epoch: 7.63 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20266358200503917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20266358200503917 | validation: 0.2045532430119585]
	TIME [epoch: 7.64 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16292833528867173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16292833528867173 | validation: 0.2097336056638362]
	TIME [epoch: 7.63 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1497142276030248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1497142276030248 | validation: 0.18035775267666038]
	TIME [epoch: 7.62 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11701477699089315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11701477699089315 | validation: 0.17495602408179303]
	TIME [epoch: 7.62 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12506832633724135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12506832633724135 | validation: 0.1792945670444471]
	TIME [epoch: 7.65 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1590893485297288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1590893485297288 | validation: 0.2887164666305985]
	TIME [epoch: 7.65 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27002264123281483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27002264123281483 | validation: 0.21976955995787817]
	TIME [epoch: 7.61 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19870049832247108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19870049832247108 | validation: 0.18542163791207075]
	TIME [epoch: 7.61 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13143664276679215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13143664276679215 | validation: 0.1611671266510316]
	TIME [epoch: 7.6 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10798347444644367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10798347444644367 | validation: 0.1673780030375932]
	TIME [epoch: 7.64 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1088422017487155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1088422017487155 | validation: 0.1392453601046102]
	TIME [epoch: 7.65 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09817782500938899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09817782500938899 | validation: 0.17012677568395645]
	TIME [epoch: 7.63 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12572954706251643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12572954706251643 | validation: 0.16570720527028635]
	TIME [epoch: 7.62 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1457053319132884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1457053319132884 | validation: 0.3928690536640362]
	TIME [epoch: 7.64 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39855295178867023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39855295178867023 | validation: 0.2716635436262546]
	TIME [epoch: 7.65 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30084610970809783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30084610970809783 | validation: 0.18742968875168972]
	TIME [epoch: 7.66 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1544330614365448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1544330614365448 | validation: 0.20840771856686816]
	TIME [epoch: 7.65 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16107840167334295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16107840167334295 | validation: 0.17871205040668248]
	TIME [epoch: 7.63 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1287748623937882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1287748623937882 | validation: 0.1556755275838576]
	TIME [epoch: 7.64 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09574788531205737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09574788531205737 | validation: 0.13679098700752165]
	TIME [epoch: 7.65 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08828263687322618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08828263687322618 | validation: 0.17152956315980905]
	TIME [epoch: 7.66 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327324328394465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12327324328394465 | validation: 0.5698555756995286]
	TIME [epoch: 7.64 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7031051044622274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7031051044622274 | validation: 0.4157857226868051]
	TIME [epoch: 7.64 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44044173661088737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44044173661088737 | validation: 0.2318501381284106]
	TIME [epoch: 7.62 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20284132714517825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20284132714517825 | validation: 0.23918573795124953]
	TIME [epoch: 7.66 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19478223794696167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19478223794696167 | validation: 0.24853353689879665]
	TIME [epoch: 7.65 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23412358249490128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23412358249490128 | validation: 0.2234717382468465]
	TIME [epoch: 7.64 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15994691469860262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15994691469860262 | validation: 0.2298061901068028]
	TIME [epoch: 7.62 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1605205595566995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1605205595566995 | validation: 0.2410434708652586]
	TIME [epoch: 7.63 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1919263002276441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1919263002276441 | validation: 0.19365568070305447]
	TIME [epoch: 7.63 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13514985939736876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13514985939736876 | validation: 0.1784429437568069]
	TIME [epoch: 7.63 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10755828104564394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10755828104564394 | validation: 0.14175567679839182]
	TIME [epoch: 7.62 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09280646982830694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09280646982830694 | validation: 0.146247644099638]
	TIME [epoch: 7.62 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0892919350625002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0892919350625002 | validation: 0.13934542378694567]
	TIME [epoch: 7.63 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09458342009572189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09458342009572189 | validation: 0.1419558529121153]
	TIME [epoch: 7.62 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10079370735309906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10079370735309906 | validation: 0.15924943720147106]
	TIME [epoch: 7.62 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13966835804568445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13966835804568445 | validation: 0.3365206515070884]
	TIME [epoch: 7.63 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.348394190648048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.348394190648048 | validation: 0.28420539734506417]
	TIME [epoch: 7.62 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2568866441120866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2568866441120866 | validation: 0.16943567950084348]
	TIME [epoch: 7.63 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12341050847881364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12341050847881364 | validation: 0.12287247646137403]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_731.pth
	Model improved!!!
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09851005586552748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09851005586552748 | validation: 0.127622920303698]
	TIME [epoch: 7.61 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08017616678802508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08017616678802508 | validation: 0.14308575754326378]
	TIME [epoch: 7.62 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08240488329571097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08240488329571097 | validation: 0.1832797161483959]
	TIME [epoch: 7.62 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20723197423429873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20723197423429873 | validation: 0.5264786734786101]
	TIME [epoch: 7.61 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5186793218945888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5186793218945888 | validation: 0.3593901711356533]
	TIME [epoch: 7.64 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4014948608017111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4014948608017111 | validation: 0.224252915466996]
	TIME [epoch: 7.62 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22873761847682644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22873761847682644 | validation: 0.2909453006129425]
	TIME [epoch: 7.63 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3132487204098185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3132487204098185 | validation: 0.23363563093825973]
	TIME [epoch: 7.62 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2434594104010266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2434594104010266 | validation: 0.21925998580908584]
	TIME [epoch: 7.64 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19351081164187156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19351081164187156 | validation: 0.211813657406266]
	TIME [epoch: 7.64 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1659334552168322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1659334552168322 | validation: 0.25506894432648364]
	TIME [epoch: 7.62 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22157774438640426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22157774438640426 | validation: 0.20281540057177208]
	TIME [epoch: 7.62 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1316717758418571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1316717758418571 | validation: 0.17477421720578457]
	TIME [epoch: 7.62 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11700513125252311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11700513125252311 | validation: 0.18106591767305089]
	TIME [epoch: 7.62 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11701224703085224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11701224703085224 | validation: 0.20666154762431407]
	TIME [epoch: 7.62 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17319634359518019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17319634359518019 | validation: 0.36889925026632675]
	TIME [epoch: 7.62 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35140809956095154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35140809956095154 | validation: 0.4270874551901991]
	TIME [epoch: 7.61 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5544175521811487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5544175521811487 | validation: 0.18058644923297004]
	TIME [epoch: 7.61 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12330650742955726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12330650742955726 | validation: 0.19409257385454695]
	TIME [epoch: 7.62 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14752241513735076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14752241513735076 | validation: 0.2863698671424242]
	TIME [epoch: 7.63 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2114118124446459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2114118124446459 | validation: 0.2420191926129167]
	TIME [epoch: 7.6 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.172120540735819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.172120540735819 | validation: 0.19274738193340527]
	TIME [epoch: 7.57 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14065642066736528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14065642066736528 | validation: 0.1744805067852415]
	TIME [epoch: 7.61 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11836380165661321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11836380165661321 | validation: 0.16205013028391277]
	TIME [epoch: 7.62 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10479745971065729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10479745971065729 | validation: 0.13734258297834884]
	TIME [epoch: 7.61 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09265935398409679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09265935398409679 | validation: 0.15050844583224274]
	TIME [epoch: 7.61 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09966926901714493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09966926901714493 | validation: 0.1551332796892112]
	TIME [epoch: 7.61 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10828040716297586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10828040716297586 | validation: 0.1680255032458741]
	TIME [epoch: 7.63 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12837518545762044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12837518545762044 | validation: 0.16382562964100233]
	TIME [epoch: 7.63 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1233109069597022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1233109069597022 | validation: 0.20992854562074836]
	TIME [epoch: 7.61 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1708183586202363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1708183586202363 | validation: 0.1710843719556666]
	TIME [epoch: 7.6 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15538121099680488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15538121099680488 | validation: 0.15040560279892215]
	TIME [epoch: 7.62 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10097826115505815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10097826115505815 | validation: 0.12537515646420658]
	TIME [epoch: 7.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0710046307499458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0710046307499458 | validation: 0.11772083030484125]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_765.pth
	Model improved!!!
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06703258432741924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06703258432741924 | validation: 0.11880020374754925]
	TIME [epoch: 7.64 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06908783574715881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06908783574715881 | validation: 0.1259577568884512]
	TIME [epoch: 7.65 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1276117765692083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1276117765692083 | validation: 0.8046426890775286]
	TIME [epoch: 7.65 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7434530751791448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7434530751791448 | validation: 1.0899174078240739]
	TIME [epoch: 7.65 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0994174597433817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0994174597433817 | validation: 1.205872857195795]
	TIME [epoch: 7.67 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9568577698601436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9568577698601436 | validation: 1.2302692439684966]
	TIME [epoch: 7.66 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9791440131557687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9791440131557687 | validation: 1.1669220215373313]
	TIME [epoch: 7.65 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9284598494445723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9284598494445723 | validation: 1.1594420698874617]
	TIME [epoch: 7.65 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9260022682850053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9260022682850053 | validation: 1.0841324407017803]
	TIME [epoch: 7.66 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9028972565272028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9028972565272028 | validation: 0.8629105563960249]
	TIME [epoch: 7.66 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840600190860564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7840600190860564 | validation: 0.7964464217173528]
	TIME [epoch: 7.65 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7367779738736837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7367779738736837 | validation: 0.6295435513696193]
	TIME [epoch: 7.65 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6413997243506229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6413997243506229 | validation: 0.48485280736127934]
	TIME [epoch: 7.66 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4894832168259584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4894832168259584 | validation: 0.361520741539612]
	TIME [epoch: 7.65 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.337841944788899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.337841944788899 | validation: 0.5347824494010657]
	TIME [epoch: 7.67 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45700957266570996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45700957266570996 | validation: 0.40315484168339755]
	TIME [epoch: 7.65 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3694664653121509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3694664653121509 | validation: 0.3055473794375792]
	TIME [epoch: 7.65 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30135987892798227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30135987892798227 | validation: 0.2152395862278429]
	TIME [epoch: 7.64 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2517518662902041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2517518662902041 | validation: 0.3779341685782889]
	TIME [epoch: 7.64 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35605100476468293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35605100476468293 | validation: 0.26062842041953016]
	TIME [epoch: 7.67 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24782203285830232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24782203285830232 | validation: 0.22653694356365214]
	TIME [epoch: 7.65 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25408844086909627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25408844086909627 | validation: 0.19887070482159702]
	TIME [epoch: 7.64 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22040106464899825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22040106464899825 | validation: 0.30852578271004827]
	TIME [epoch: 7.62 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35388155860147336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35388155860147336 | validation: 0.7662793818328008]
	TIME [epoch: 7.63 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.518480418579967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.518480418579967 | validation: 0.8904892777082252]
	TIME [epoch: 7.63 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5022482111998314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5022482111998314 | validation: 0.8434006223598595]
	TIME [epoch: 7.61 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47207890935250263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47207890935250263 | validation: 0.752466600242388]
	TIME [epoch: 7.61 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37215353237739784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37215353237739784 | validation: 0.4176813996234051]
	TIME [epoch: 7.62 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45274832869225584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45274832869225584 | validation: 0.4735263439464861]
	TIME [epoch: 7.64 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40463487701511724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40463487701511724 | validation: 0.38143451842927595]
	TIME [epoch: 7.61 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24714110828060526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24714110828060526 | validation: 0.43971169593015613]
	TIME [epoch: 7.62 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23636329560982575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23636329560982575 | validation: 0.37245604774668417]
	TIME [epoch: 7.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23555120348700484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23555120348700484 | validation: 0.15261417039769187]
	TIME [epoch: 7.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1434381544317974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1434381544317974 | validation: 0.17043273874418863]
	TIME [epoch: 7.6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13693457656558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13693457656558 | validation: 0.6957246930007083]
	TIME [epoch: 7.56 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.473556724058229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.473556724058229 | validation: 0.47805780387365576]
	TIME [epoch: 7.58 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4451429280162155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4451429280162155 | validation: 0.35811140288985777]
	TIME [epoch: 7.58 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3509128310111382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3509128310111382 | validation: 0.2669231745974331]
	TIME [epoch: 7.59 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2641695379750765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2641695379750765 | validation: 0.22830368450705132]
	TIME [epoch: 7.61 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23896125948832514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23896125948832514 | validation: 0.19547136338840715]
	TIME [epoch: 7.58 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2167692471355413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2167692471355413 | validation: 0.18315764549096844]
	TIME [epoch: 7.59 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17707967129939284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17707967129939284 | validation: 0.17510007735989042]
	TIME [epoch: 7.59 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16144828603964634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16144828603964634 | validation: 0.1783723506687118]
	TIME [epoch: 7.59 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15172314663912215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15172314663912215 | validation: 0.17327573928008547]
	TIME [epoch: 7.6 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14418479469193266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14418479469193266 | validation: 0.17693532560134947]
	TIME [epoch: 7.6 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.133209739722524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.133209739722524 | validation: 0.201456534574406]
	TIME [epoch: 7.59 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15447800217307667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15447800217307667 | validation: 0.2523474980819881]
	TIME [epoch: 7.6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20451092339615273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20451092339615273 | validation: 0.2769986544082076]
	TIME [epoch: 7.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2705889736426115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2705889736426115 | validation: 0.18412706067935375]
	TIME [epoch: 7.63 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13583357507621086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13583357507621086 | validation: 0.14988128977743984]
	TIME [epoch: 7.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11416706780588655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11416706780588655 | validation: 0.1481330006763571]
	TIME [epoch: 7.61 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09401282838143116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09401282838143116 | validation: 0.16141057282016355]
	TIME [epoch: 7.61 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12523860847709292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12523860847709292 | validation: 0.14130022525721989]
	TIME [epoch: 7.62 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08093813926169074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08093813926169074 | validation: 0.12551340395671282]
	TIME [epoch: 7.63 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07678870175173842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07678870175173842 | validation: 0.20952648269304694]
	TIME [epoch: 7.62 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.186032842725497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.186032842725497 | validation: 0.16478459761503467]
	TIME [epoch: 7.61 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10230127681747866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10230127681747866 | validation: 0.1694255558436012]
	TIME [epoch: 7.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12008645074654258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12008645074654258 | validation: 0.1625298589887804]
	TIME [epoch: 7.62 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14325599832182787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14325599832182787 | validation: 0.4105532274040211]
	TIME [epoch: 7.61 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3938390722111749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3938390722111749 | validation: 0.23219898551644672]
	TIME [epoch: 7.6 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19245232327230163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19245232327230163 | validation: 0.09922133837137981]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11132533963224596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11132533963224596 | validation: 0.19205164254438187]
	TIME [epoch: 7.61 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14963903566681483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14963903566681483 | validation: 0.18489200692032554]
	TIME [epoch: 7.65 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1768584490401048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1768584490401048 | validation: 0.31033712369027144]
	TIME [epoch: 7.63 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3166578057401535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3166578057401535 | validation: 0.25336550154925336]
	TIME [epoch: 7.63 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26178151781996545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26178151781996545 | validation: 0.19055574854132978]
	TIME [epoch: 7.63 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14772200511110897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14772200511110897 | validation: 0.16511316825001743]
	TIME [epoch: 7.64 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1181362665371222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1181362665371222 | validation: 0.13904336250540514]
	TIME [epoch: 7.6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09735117733807977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09735117733807977 | validation: 0.13802071899844026]
	TIME [epoch: 7.63 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0879651058294676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0879651058294676 | validation: 0.1381422203311175]
	TIME [epoch: 7.63 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08630429938496054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08630429938496054 | validation: 0.17848256572924423]
	TIME [epoch: 7.63 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21704608737911557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21704608737911557 | validation: 0.38409036585675105]
	TIME [epoch: 7.65 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38916356946055575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38916356946055575 | validation: 0.3141086303865303]
	TIME [epoch: 7.64 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34352959823875995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34352959823875995 | validation: 0.20783135569874678]
	TIME [epoch: 7.64 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20542764210236292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20542764210236292 | validation: 0.20043817928796717]
	TIME [epoch: 7.63 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1593423839793292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1593423839793292 | validation: 0.19452886125410004]
	TIME [epoch: 7.61 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14498058694456464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14498058694456464 | validation: 0.18098386818711068]
	TIME [epoch: 7.63 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13181386064976006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13181386064976006 | validation: 0.1718654896203473]
	TIME [epoch: 7.64 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10562624248714172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10562624248714172 | validation: 0.1620266764290691]
	TIME [epoch: 7.63 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09348847292170315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09348847292170315 | validation: 0.14495698430804546]
	TIME [epoch: 7.63 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08725582854888848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08725582854888848 | validation: 0.16321388368915835]
	TIME [epoch: 7.62 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12094449711815071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12094449711815071 | validation: 0.19341717740189593]
	TIME [epoch: 7.62 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16888447057696765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16888447057696765 | validation: 0.3166742155362779]
	TIME [epoch: 7.92 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3130392139412254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3130392139412254 | validation: 0.2032755639107295]
	TIME [epoch: 7.64 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1763032832270645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1763032832270645 | validation: 0.14005673618488088]
	TIME [epoch: 7.63 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09682164931673384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09682164931673384 | validation: 0.1358165590995092]
	TIME [epoch: 7.62 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09768873297829625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09768873297829625 | validation: 0.17873858090094968]
	TIME [epoch: 7.64 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13338863168560933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13338863168560933 | validation: 0.12718089905593952]
	TIME [epoch: 7.64 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13912985186977858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13912985186977858 | validation: 0.28766681199935523]
	TIME [epoch: 7.62 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25708116425518496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25708116425518496 | validation: 0.17814742619722035]
	TIME [epoch: 7.62 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15657112577466994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15657112577466994 | validation: 0.11395231208488749]
	TIME [epoch: 7.63 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09247407701274682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09247407701274682 | validation: 0.1408675612032287]
	TIME [epoch: 7.62 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10586271359410303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10586271359410303 | validation: 0.11456399982626311]
	TIME [epoch: 7.64 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09080817108484468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09080817108484468 | validation: 0.11860746107448189]
	TIME [epoch: 7.63 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08054983741702185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08054983741702185 | validation: 0.08711810290839578]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_860.pth
	Model improved!!!
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07176781016033162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07176781016033162 | validation: 0.14049776487104665]
	TIME [epoch: 7.61 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0942068102862264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0942068102862264 | validation: 0.11902745324654984]
	TIME [epoch: 7.64 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1177473946177755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1177473946177755 | validation: 0.4374868105928247]
	TIME [epoch: 7.61 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4282087696734479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4282087696734479 | validation: 0.3813732239885235]
	TIME [epoch: 7.61 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3718408495786572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3718408495786572 | validation: 0.6728742306379172]
	TIME [epoch: 7.61 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43462708784952403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43462708784952403 | validation: 0.5363404742353476]
	TIME [epoch: 7.62 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.358277742474878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.358277742474878 | validation: 0.3677804725336607]
	TIME [epoch: 7.63 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27988036015989554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27988036015989554 | validation: 0.22833326289935696]
	TIME [epoch: 7.63 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20286295971638602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20286295971638602 | validation: 0.1772915261967254]
	TIME [epoch: 7.59 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.138982805890383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.138982805890383 | validation: 0.11793175725527227]
	TIME [epoch: 7.61 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09713727834059238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09713727834059238 | validation: 0.10805737512444309]
	TIME [epoch: 7.59 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07647926825627892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07647926825627892 | validation: 0.09873684529026612]
	TIME [epoch: 7.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0692931392267663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0692931392267663 | validation: 0.21449131284050243]
	TIME [epoch: 7.58 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16940296966824905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16940296966824905 | validation: 0.21128405066348624]
	TIME [epoch: 7.57 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20578410818080617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20578410818080617 | validation: 0.3050213572465774]
	TIME [epoch: 7.58 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2904205373854655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2904205373854655 | validation: 0.1872948678043748]
	TIME [epoch: 7.59 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27084062733011294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27084062733011294 | validation: 0.21761502476582076]
	TIME [epoch: 7.6 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1709125240988078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1709125240988078 | validation: 0.23482783167316806]
	TIME [epoch: 7.54 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18389045780894425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18389045780894425 | validation: 0.23092241599642913]
	TIME [epoch: 7.53 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.174209998719441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.174209998719441 | validation: 0.17731160866326356]
	TIME [epoch: 7.53 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13958319244036813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13958319244036813 | validation: 0.14491411199660878]
	TIME [epoch: 7.57 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09462297965318679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09462297965318679 | validation: 0.12216158157750466]
	TIME [epoch: 7.58 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07297809557410843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07297809557410843 | validation: 0.11288746360538471]
	TIME [epoch: 7.55 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07815844160907767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07815844160907767 | validation: 0.1297640749243998]
	TIME [epoch: 7.54 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08975203039444356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08975203039444356 | validation: 0.1346722790716745]
	TIME [epoch: 7.54 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1071479752363286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1071479752363286 | validation: 0.15205846300444326]
	TIME [epoch: 7.53 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1205919336290351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1205919336290351 | validation: 0.1069340024455954]
	TIME [epoch: 7.54 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10126664815313408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10126664815313408 | validation: 0.17664536977226053]
	TIME [epoch: 7.54 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1395565739029968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1395565739029968 | validation: 0.17611247712075706]
	TIME [epoch: 7.53 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19850639293760877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19850639293760877 | validation: 0.32073399592649854]
	TIME [epoch: 7.54 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32210795391520075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32210795391520075 | validation: 0.2299668119008636]
	TIME [epoch: 7.58 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23795820938224987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23795820938224987 | validation: 0.23848050979120528]
	TIME [epoch: 7.55 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16936343532067322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16936343532067322 | validation: 0.3166145417899057]
	TIME [epoch: 7.53 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2919592025266139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2919592025266139 | validation: 0.2895399082997791]
	TIME [epoch: 7.55 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20267910343840861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20267910343840861 | validation: 0.18661951823964712]
	TIME [epoch: 7.61 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12625856549765133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12625856549765133 | validation: 0.12441811045638468]
	TIME [epoch: 7.52 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11363841812723592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11363841812723592 | validation: 0.12563446179034538]
	TIME [epoch: 7.54 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09026087042781204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09026087042781204 | validation: 0.195743483107215]
	TIME [epoch: 7.54 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12183262600176836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12183262600176836 | validation: 0.1390781099843903]
	TIME [epoch: 7.52 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0877769849510974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0877769849510974 | validation: 0.12235043177133936]
	TIME [epoch: 7.52 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08593266233991177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08593266233991177 | validation: 0.15034314810786947]
	TIME [epoch: 7.51 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08615355474234936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08615355474234936 | validation: 0.12032822479460964]
	TIME [epoch: 7.52 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08420277025924854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08420277025924854 | validation: 0.19512820220304825]
	TIME [epoch: 7.51 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12381369289496066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12381369289496066 | validation: 0.1250472765338931]
	TIME [epoch: 7.5 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08065219030729921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08065219030729921 | validation: 0.15024330916726036]
	TIME [epoch: 7.49 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08420767193559185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08420767193559185 | validation: 0.1156046427415146]
	TIME [epoch: 7.52 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10702549364100238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10702549364100238 | validation: 0.2493616680439093]
	TIME [epoch: 7.52 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20767333356954837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20767333356954837 | validation: 0.15266951233046885]
	TIME [epoch: 7.53 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17082218706541327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17082218706541327 | validation: 0.37385759297118243]
	TIME [epoch: 7.52 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3645059652981088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3645059652981088 | validation: 0.3275711829471321]
	TIME [epoch: 7.52 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3393210150403068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3393210150403068 | validation: 0.19947818123336336]
	TIME [epoch: 7.52 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21188199841674363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21188199841674363 | validation: 0.16906078161367488]
	TIME [epoch: 7.54 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11917252589908571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11917252589908571 | validation: 0.17310112892374835]
	TIME [epoch: 7.53 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11684992345635943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11684992345635943 | validation: 0.1584197856611212]
	TIME [epoch: 7.53 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09920303246867469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09920303246867469 | validation: 0.1446623317219409]
	TIME [epoch: 7.54 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08692717832669491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08692717832669491 | validation: 0.123606389556409]
	TIME [epoch: 7.51 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.072450607395934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.072450607395934 | validation: 0.1241161189458659]
	TIME [epoch: 7.54 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08227530667369568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08227530667369568 | validation: 0.1357881769580483]
	TIME [epoch: 7.52 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09792782241957802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09792782241957802 | validation: 0.3215294700840565]
	TIME [epoch: 7.52 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2881268122346202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2881268122346202 | validation: 0.2006817668544122]
	TIME [epoch: 7.52 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21064694802829953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21064694802829953 | validation: 0.09526132922592895]
	TIME [epoch: 7.54 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10434850307695785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10434850307695785 | validation: 0.09626841798355268]
	TIME [epoch: 7.52 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10792340974965794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10792340974965794 | validation: 0.18016987450562547]
	TIME [epoch: 7.53 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1644522549583818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1644522549583818 | validation: 0.16766759800583034]
	TIME [epoch: 7.52 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11219910146209715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11219910146209715 | validation: 0.13495817981495165]
	TIME [epoch: 7.53 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08594103657228562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08594103657228562 | validation: 0.12200877734973643]
	TIME [epoch: 7.54 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1099972500252402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1099972500252402 | validation: 0.21906153531116754]
	TIME [epoch: 7.53 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19875131825616094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19875131825616094 | validation: 0.19637733630389384]
	TIME [epoch: 7.52 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14551483424010841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14551483424010841 | validation: 0.10750097991507271]
	TIME [epoch: 7.53 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.068504494598658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.068504494598658 | validation: 0.08595878620212266]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_930.pth
	Model improved!!!
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08463068686689486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08463068686689486 | validation: 0.14723738838949108]
	TIME [epoch: 7.61 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10844087413336166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10844087413336166 | validation: 0.1700754117360814]
	TIME [epoch: 7.53 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11030367889647118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11030367889647118 | validation: 0.15690051188222692]
	TIME [epoch: 7.55 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10785179155946782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10785179155946782 | validation: 0.1339418211502513]
	TIME [epoch: 7.53 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12587035755054982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12587035755054982 | validation: 0.25315546234493624]
	TIME [epoch: 7.55 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25414563016514236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25414563016514236 | validation: 0.19263310710363696]
	TIME [epoch: 7.54 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18817794268663537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18817794268663537 | validation: 0.14826937342431543]
	TIME [epoch: 7.54 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08853151904323092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08853151904323092 | validation: 0.1325339807633634]
	TIME [epoch: 7.54 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08898352945176168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08898352945176168 | validation: 0.12125537532847981]
	TIME [epoch: 7.55 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0704534954880272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0704534954880272 | validation: 0.11691832436606642]
	TIME [epoch: 7.55 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07340255111095838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07340255111095838 | validation: 0.12410044220613298]
	TIME [epoch: 7.55 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08571653069091656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08571653069091656 | validation: 0.09955175657170104]
	TIME [epoch: 7.53 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09693373468636911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09693373468636911 | validation: 0.3394519067689419]
	TIME [epoch: 7.54 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34878490046279026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34878490046279026 | validation: 0.20398073906776837]
	TIME [epoch: 7.53 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15455521175487955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15455521175487955 | validation: 0.087140981977852]
	TIME [epoch: 7.52 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07565270354671577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07565270354671577 | validation: 0.09624205120202448]
	TIME [epoch: 7.55 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07677380581191416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07677380581191416 | validation: 0.12236902120257925]
	TIME [epoch: 7.53 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07297622104005563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07297622104005563 | validation: 0.08273726844935428]
	TIME [epoch: 7.57 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_948.pth
	Model improved!!!
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05503317065039829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05503317065039829 | validation: 0.17386387856483923]
	TIME [epoch: 7.56 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14980819408084453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14980819408084453 | validation: 0.1300578842760168]
	TIME [epoch: 7.54 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10932296779883291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10932296779883291 | validation: 0.13635995194207076]
	TIME [epoch: 7.54 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11093344366539203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11093344366539203 | validation: 0.07064799282465785]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_952.pth
	Model improved!!!
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08787200686815543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08787200686815543 | validation: 0.25740518749451846]
	TIME [epoch: 7.54 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2852730165365011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2852730165365011 | validation: 0.19197517445228293]
	TIME [epoch: 7.54 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17337975689090648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17337975689090648 | validation: 0.16326182919178658]
	TIME [epoch: 7.56 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11635955030922998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11635955030922998 | validation: 0.1961962616895547]
	TIME [epoch: 7.56 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11595588308900681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11595588308900681 | validation: 0.1252922111399723]
	TIME [epoch: 7.54 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07571946385945363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07571946385945363 | validation: 0.10835386013847428]
	TIME [epoch: 7.55 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060646348483290674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060646348483290674 | validation: 0.08640320537453804]
	TIME [epoch: 7.53 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0658295498565899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0658295498565899 | validation: 0.23002288940085408]
	TIME [epoch: 7.56 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2174235937658311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2174235937658311 | validation: 0.31410940582771624]
	TIME [epoch: 7.54 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42009615882402807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42009615882402807 | validation: 0.19050500871261522]
	TIME [epoch: 7.52 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1485729812602161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1485729812602161 | validation: 0.159956641117191]
	TIME [epoch: 7.52 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12025011357948977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12025011357948977 | validation: 0.17872836285559035]
	TIME [epoch: 7.54 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1121092817556109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1121092817556109 | validation: 0.15444594776990472]
	TIME [epoch: 7.55 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09993911515472771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09993911515472771 | validation: 0.11327133298696085]
	TIME [epoch: 7.57 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06285673411573203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06285673411573203 | validation: 0.1072165879893226]
	TIME [epoch: 7.53 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053525132864137154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053525132864137154 | validation: 0.08611202325805403]
	TIME [epoch: 7.53 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04861483243889655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04861483243889655 | validation: 0.06709948283724866]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_969.pth
	Model improved!!!
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04486056357190018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04486056357190018 | validation: 0.09271598417068659]
	TIME [epoch: 7.52 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06697204097123427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06697204097123427 | validation: 0.0823077867504105]
	TIME [epoch: 7.49 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09197562803875768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09197562803875768 | validation: 0.4072533993329184]
	TIME [epoch: 7.49 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47025475108549025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47025475108549025 | validation: 0.27725004764740335]
	TIME [epoch: 7.49 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3312075181520875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3312075181520875 | validation: 0.1349922587485792]
	TIME [epoch: 7.49 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16223940017572944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16223940017572944 | validation: 0.15514345514278857]
	TIME [epoch: 7.5 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11355064576523453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11355064576523453 | validation: 0.1475835073130868]
	TIME [epoch: 7.49 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10170885254497833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10170885254497833 | validation: 0.13384385266108187]
	TIME [epoch: 7.49 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08861376196162195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08861376196162195 | validation: 0.12975980872142343]
	TIME [epoch: 7.49 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06906087100079888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06906087100079888 | validation: 0.11931350582812178]
	TIME [epoch: 7.5 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0671506221938998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0671506221938998 | validation: 0.1514960145869979]
	TIME [epoch: 7.5 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10400016574152127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10400016574152127 | validation: 0.2013417448981091]
	TIME [epoch: 7.49 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2566539256561513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2566539256561513 | validation: 0.31407203977427867]
	TIME [epoch: 7.49 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3122211486717172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3122211486717172 | validation: 0.23379867722277936]
	TIME [epoch: 7.5 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1717585006411417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1717585006411417 | validation: 0.1511786315051024]
	TIME [epoch: 7.5 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10683687050912051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10683687050912051 | validation: 0.12917873966161736]
	TIME [epoch: 7.51 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09377217792022849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09377217792022849 | validation: 0.09689905058784966]
	TIME [epoch: 7.5 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060816713221765246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060816713221765246 | validation: 0.0873734737071643]
	TIME [epoch: 7.47 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05470388651965794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05470388651965794 | validation: 0.06795828272960659]
	TIME [epoch: 7.46 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04636951138666371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04636951138666371 | validation: 0.0823506827104248]
	TIME [epoch: 7.46 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05139284963362074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05139284963362074 | validation: 0.06034137570095857]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_990.pth
	Model improved!!!
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054469839250606285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054469839250606285 | validation: 0.23152857068882027]
	TIME [epoch: 7.47 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19334057235762434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19334057235762434 | validation: 0.1741239989003039]
	TIME [epoch: 7.46 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14420163080864057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14420163080864057 | validation: 0.20813621058095474]
	TIME [epoch: 7.46 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4197406760307415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4197406760307415 | validation: 0.177657627322539]
	TIME [epoch: 7.47 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16657990026178182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16657990026178182 | validation: 0.202588446553903]
	TIME [epoch: 7.48 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17713863712189873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17713863712189873 | validation: 0.14710605080331815]
	TIME [epoch: 7.46 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1032068087855648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1032068087855648 | validation: 0.1362641703060991]
	TIME [epoch: 7.47 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0846392779843901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0846392779843901 | validation: 0.11748808551423529]
	TIME [epoch: 7.46 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07483725331692584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07483725331692584 | validation: 0.11863366790222969]
	TIME [epoch: 7.48 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07265209360046222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07265209360046222 | validation: 0.09914963725900133]
	TIME [epoch: 7.47 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0754067638856823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0754067638856823 | validation: 0.10165858422553958]
	TIME [epoch: 41.8 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051743554001839255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051743554001839255 | validation: 0.0741830164762248]
	TIME [epoch: 16.1 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0481430874641708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0481430874641708 | validation: 0.0629926553339034]
	TIME [epoch: 16.1 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04917274206939766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04917274206939766 | validation: 0.07593630749663283]
	TIME [epoch: 16.1 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04973963503754281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04973963503754281 | validation: 0.09863128925106557]
	TIME [epoch: 16.1 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06992599136398156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06992599136398156 | validation: 0.31024944026615364]
	TIME [epoch: 16.1 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5261278560814853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5261278560814853 | validation: 0.265720538871224]
	TIME [epoch: 16.1 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25248981593312714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25248981593312714 | validation: 0.2562788818943639]
	TIME [epoch: 16.1 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2656556961404294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2656556961404294 | validation: 0.1569824828112968]
	TIME [epoch: 16.1 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09447498226454044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09447498226454044 | validation: 0.1602317247789008]
	TIME [epoch: 16.1 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10537058485510876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10537058485510876 | validation: 0.15459288391437642]
	TIME [epoch: 16.1 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09537811062745716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09537811062745716 | validation: 0.13907507857787782]
	TIME [epoch: 16.1 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08026290813444913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08026290813444913 | validation: 0.12461877291325596]
	TIME [epoch: 16.1 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05942495664552316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05942495664552316 | validation: 0.1055513696492057]
	TIME [epoch: 16.1 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05311370803153329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05311370803153329 | validation: 0.11812404676850508]
	TIME [epoch: 16.1 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06376982609306026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06376982609306026 | validation: 0.11764925013276249]
	TIME [epoch: 16.1 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059346222512330356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059346222512330356 | validation: 0.12169852643948631]
	TIME [epoch: 16.1 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07373553400928039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07373553400928039 | validation: 0.11421895076135949]
	TIME [epoch: 16.1 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08425428255470788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08425428255470788 | validation: 0.1724914711590318]
	TIME [epoch: 16.1 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12808979222062492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12808979222062492 | validation: 0.10522451488205471]
	TIME [epoch: 16.1 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11934142801415448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11934142801415448 | validation: 0.2204182415652028]
	TIME [epoch: 16.1 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17406083703684347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17406083703684347 | validation: 0.10800093554859853]
	TIME [epoch: 16 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06212404964598685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06212404964598685 | validation: 0.05818777038256415]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_1023.pth
	Model improved!!!
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04595098110011793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04595098110011793 | validation: 0.1171441404709066]
	TIME [epoch: 16.1 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07306745281225764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07306745281225764 | validation: 0.07340209028593726]
	TIME [epoch: 16.1 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05050805869124702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05050805869124702 | validation: 0.08047294702976099]
	TIME [epoch: 16.1 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04901976381230056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04901976381230056 | validation: 0.06178901135661329]
	TIME [epoch: 16.1 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048869970252511406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048869970252511406 | validation: 0.13045643947977312]
	TIME [epoch: 16.1 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08888098768249993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08888098768249993 | validation: 0.09846911673756459]
	TIME [epoch: 16.1 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13155098291178388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13155098291178388 | validation: 0.3240375900535575]
	TIME [epoch: 16.1 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3392559146323623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3392559146323623 | validation: 0.4447115457550802]
	TIME [epoch: 16.1 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4714394108599443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4714394108599443 | validation: 0.46704243123356115]
	TIME [epoch: 16.1 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4710656444866443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4710656444866443 | validation: 0.5053145150599695]
	TIME [epoch: 16.1 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4802670635798873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4802670635798873 | validation: 0.2983130661623212]
	TIME [epoch: 16.1 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4227175045616879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4227175045616879 | validation: 0.2498874004098088]
	TIME [epoch: 16.1 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3478141898759708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3478141898759708 | validation: 0.28259636433937446]
	TIME [epoch: 16.1 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27868352441592587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27868352441592587 | validation: 0.21863287900341996]
	TIME [epoch: 16.1 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22262136990178097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22262136990178097 | validation: 0.15995898842868594]
	TIME [epoch: 16.1 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21576480097124723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21576480097124723 | validation: 0.17069210153812586]
	TIME [epoch: 16.1 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18200165291766815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18200165291766815 | validation: 0.18323074341859957]
	TIME [epoch: 16.1 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18166397729507247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18166397729507247 | validation: 0.1894511313523697]
	TIME [epoch: 16 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17622642533748212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17622642533748212 | validation: 0.18181281253439008]
	TIME [epoch: 16 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16631018526495034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16631018526495034 | validation: 0.18167511424732663]
	TIME [epoch: 16 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17675379932619997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17675379932619997 | validation: 0.21571029202356395]
	TIME [epoch: 16.1 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29179497533736304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29179497533736304 | validation: 0.22670865393373216]
	TIME [epoch: 16 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20281040661711414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20281040661711414 | validation: 0.17871060015999263]
	TIME [epoch: 16.1 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13637395941878044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13637395941878044 | validation: 0.20299250489193038]
	TIME [epoch: 16 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1571648011214456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1571648011214456 | validation: 0.1396292712392504]
	TIME [epoch: 16.1 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11864951791349375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11864951791349375 | validation: 0.1638311519430513]
	TIME [epoch: 16.1 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14023677137688792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14023677137688792 | validation: 0.11811498093863824]
	TIME [epoch: 16.1 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07735520765657172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07735520765657172 | validation: 0.1261111653668608]
	TIME [epoch: 16.1 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09616028703015836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09616028703015836 | validation: 0.1033011219714932]
	TIME [epoch: 16.1 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0661355399840096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0661355399840096 | validation: 0.07803443855484848]
	TIME [epoch: 16 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058449838410305155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058449838410305155 | validation: 0.08483255422011016]
	TIME [epoch: 16.1 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04735011496240654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04735011496240654 | validation: 0.08450672859325975]
	TIME [epoch: 16.1 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07117471564665326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07117471564665326 | validation: 0.22398128543407428]
	TIME [epoch: 16.1 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1965043010607278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1965043010607278 | validation: 0.21848189148767425]
	TIME [epoch: 16 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.168140751233551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.168140751233551 | validation: 0.13465595817068873]
	TIME [epoch: 16.1 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11705426957554484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11705426957554484 | validation: 0.1263992440675231]
	TIME [epoch: 16 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1653380389797985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1653380389797985 | validation: 0.2173600751598179]
	TIME [epoch: 16 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.217059325307884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.217059325307884 | validation: 0.2069069384077377]
	TIME [epoch: 16 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15361974805792739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15361974805792739 | validation: 0.16165715872239367]
	TIME [epoch: 16.1 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10750897102667231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10750897102667231 | validation: 0.14070907100269445]
	TIME [epoch: 16 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08740510240745934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08740510240745934 | validation: 0.10513162574230127]
	TIME [epoch: 16.1 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06687523097804852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06687523097804852 | validation: 0.09119030736467154]
	TIME [epoch: 16 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05707495502156338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05707495502156338 | validation: 0.0688581107638965]
	TIME [epoch: 16 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05868596356795253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05868596356795253 | validation: 0.08632249442555662]
	TIME [epoch: 16 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06608443114364648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06608443114364648 | validation: 0.08701947572137642]
	TIME [epoch: 16 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08687341168068921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08687341168068921 | validation: 0.18211736051140873]
	TIME [epoch: 16.1 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1563902629420263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1563902629420263 | validation: 0.12305433107938546]
	TIME [epoch: 16 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13787716351219115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13787716351219115 | validation: 0.2529017979461094]
	TIME [epoch: 16 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22609698994117344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22609698994117344 | validation: 0.17341713089364064]
	TIME [epoch: 16 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12080224062671167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12080224062671167 | validation: 0.11419403332231251]
	TIME [epoch: 16.1 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11502207113029064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11502207113029064 | validation: 0.15944226533103137]
	TIME [epoch: 16.1 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13143959603718305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13143959603718305 | validation: 0.09325643681827966]
	TIME [epoch: 16.1 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07157397909993753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07157397909993753 | validation: 0.07169450979076313]
	TIME [epoch: 16.1 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06612338801890373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06612338801890373 | validation: 0.15237228300926564]
	TIME [epoch: 16 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11303445193541535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11303445193541535 | validation: 0.08625647055485643]
	TIME [epoch: 16 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08001581502502657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08001581502502657 | validation: 0.06427098980531323]
	TIME [epoch: 16 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06745581361391902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06745581361391902 | validation: 0.0618336275287634]
	TIME [epoch: 16 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07687002921790159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07687002921790159 | validation: 0.2288425551995971]
	TIME [epoch: 16.1 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19062904900215608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19062904900215608 | validation: 0.16055287771606586]
	TIME [epoch: 16.1 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292724346277013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1292724346277013 | validation: 0.1006515411155856]
	TIME [epoch: 16.1 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07125611774835053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07125611774835053 | validation: 0.1116112479630087]
	TIME [epoch: 16.1 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08856837938082678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08856837938082678 | validation: 0.13309040920324364]
	TIME [epoch: 16.2 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0924030607117456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0924030607117456 | validation: 0.10497254407509626]
	TIME [epoch: 16.1 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08422697665455987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08422697665455987 | validation: 0.1455725328136576]
	TIME [epoch: 16.1 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10885637465512966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10885637465512966 | validation: 0.09920911395882819]
	TIME [epoch: 16.1 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1298975415251535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1298975415251535 | validation: 0.22191217995415374]
	TIME [epoch: 16.1 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22519947047802014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22519947047802014 | validation: 0.1592634522448417]
	TIME [epoch: 16.1 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12048449660066166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12048449660066166 | validation: 0.10294312916543574]
	TIME [epoch: 16.1 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05378174886817046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05378174886817046 | validation: 0.08285624522229945]
	TIME [epoch: 16.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06624202225902014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06624202225902014 | validation: 0.09316941801058236]
	TIME [epoch: 16.1 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04962351766382717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04962351766382717 | validation: 0.10001385520656776]
	TIME [epoch: 20.8 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09458839597150645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09458839597150645 | validation: 0.0572534892098252]
	TIME [epoch: 16.1 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_1095.pth
	Model improved!!!
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07784782821818136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07784782821818136 | validation: 0.08731292302812504]
	TIME [epoch: 16.1 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07338937336486848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07338937336486848 | validation: 0.25819332894956043]
	TIME [epoch: 16 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3935462357279394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3935462357279394 | validation: 0.3283358534583804]
	TIME [epoch: 16 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3536394443832077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3536394443832077 | validation: 1.1163993720457988]
	TIME [epoch: 24.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8128555415364253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8128555415364253 | validation: 0.9893011852939702]
	TIME [epoch: 16.1 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7073902667698329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7073902667698329 | validation: 1.0855057833672366]
	TIME [epoch: 16.1 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8707812500728442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8707812500728442 | validation: 0.9582123553533155]
	TIME [epoch: 16.1 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6974473827208841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6974473827208841 | validation: 0.9861602170128221]
	TIME [epoch: 16.1 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6581099311146271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6581099311146271 | validation: 0.8532755838658916]
	TIME [epoch: 16.6 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5688591120886398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5688591120886398 | validation: 0.7220963240892916]
	TIME [epoch: 16.1 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.513153226459416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.513153226459416 | validation: 0.6415882517357012]
	TIME [epoch: 16.1 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4642181671081754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4642181671081754 | validation: 0.5131304948236659]
	TIME [epoch: 16 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42218222238805175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42218222238805175 | validation: 0.4189381224128189]
	TIME [epoch: 16.1 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35915978749442173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35915978749442173 | validation: 0.312989563106538]
	TIME [epoch: 22.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2968133967785611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2968133967785611 | validation: 0.27996276856614033]
	TIME [epoch: 16 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28177615831447056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28177615831447056 | validation: 0.2852667191737003]
	TIME [epoch: 16 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.274072893999549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.274072893999549 | validation: 0.2646806307763389]
	TIME [epoch: 16.6 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2611991722081611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2611991722081611 | validation: 0.27814554294183175]
	TIME [epoch: 16.1 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32925704627914243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32925704627914243 | validation: 0.20478293333488085]
	TIME [epoch: 16 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.276860328382319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.276860328382319 | validation: 0.18738368364937164]
	TIME [epoch: 16 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19888272788815195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19888272788815195 | validation: 0.21373094312423158]
	TIME [epoch: 24.5 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19562788044806367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19562788044806367 | validation: 0.22793706502228606]
	TIME [epoch: 16 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20724517430020192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20724517430020192 | validation: 0.23297687609047735]
	TIME [epoch: 16.1 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1838041950314833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1838041950314833 | validation: 0.2042603773959996]
	TIME [epoch: 16.1 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1670423513140365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1670423513140365 | validation: 0.1942148637338417]
	TIME [epoch: 16.1 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15403871428763938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15403871428763938 | validation: 0.19535483784096613]
	TIME [epoch: 16.1 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14993296116748867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14993296116748867 | validation: 0.21320456852166406]
	TIME [epoch: 16 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16862848301408767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16862848301408767 | validation: 0.1751355479313111]
	TIME [epoch: 24.5 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13272958542396293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13272958542396293 | validation: 0.21438218252369606]
	TIME [epoch: 16 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16969204519953177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16969204519953177 | validation: 0.17688484929869208]
	TIME [epoch: 16.1 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13681644768382398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13681644768382398 | validation: 0.7575583473259618]
	TIME [epoch: 16.1 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8832552082565659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8832552082565659 | validation: 0.1865174967778577]
	TIME [epoch: 16.1 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14940262252831232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14940262252831232 | validation: 0.22717523482078794]
	TIME [epoch: 16.1 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18665919692585853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18665919692585853 | validation: 0.22154370595994335]
	TIME [epoch: 16 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19393293595407401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19393293595407401 | validation: 0.19633618268123604]
	TIME [epoch: 16 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1651811650799852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1651811650799852 | validation: 0.15158430903165662]
	TIME [epoch: 22.9 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10950494967486034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10950494967486034 | validation: 0.15756103677229089]
	TIME [epoch: 16 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11142662706854102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11142662706854102 | validation: 0.15098664844499513]
	TIME [epoch: 16.1 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10158246508549973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10158246508549973 | validation: 0.15615298799599647]
	TIME [epoch: 16.4 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10821674023158614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10821674023158614 | validation: 0.1462243345468547]
	TIME [epoch: 16.1 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09679973943405404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09679973943405404 | validation: 0.14238199669714685]
	TIME [epoch: 16.1 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09389550549226452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09389550549226452 | validation: 0.21911217932852028]
	TIME [epoch: 16.1 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2928786119508694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2928786119508694 | validation: 0.20933805081417103]
	TIME [epoch: 16.1 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17553522585415166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17553522585415166 | validation: 0.21027116592955075]
	TIME [epoch: 16.1 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17996476200900538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17996476200900538 | validation: 0.2717850549292855]
	TIME [epoch: 16.1 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2583791881039074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2583791881039074 | validation: 0.23643826443442514]
	TIME [epoch: 16.1 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20564287609689957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20564287609689957 | validation: 0.17054901470712683]
	TIME [epoch: 16.1 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1421588864663112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1421588864663112 | validation: 0.1406629605994632]
	TIME [epoch: 16.1 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09525586893286575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09525586893286575 | validation: 0.14040711648161874]
	TIME [epoch: 16 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08856748468480895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08856748468480895 | validation: 0.13988599210082486]
	TIME [epoch: 16.1 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12201327080583202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12201327080583202 | validation: 0.1688448175607196]
	TIME [epoch: 16.1 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1403282995028061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1403282995028061 | validation: 0.16233944829324462]
	TIME [epoch: 16.1 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12420397577249716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12420397577249716 | validation: 0.1423238144543225]
	TIME [epoch: 16.1 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0968646700474496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0968646700474496 | validation: 0.1041126516583641]
	TIME [epoch: 16.1 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08978600360421794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08978600360421794 | validation: 0.1056475881247043]
	TIME [epoch: 16.1 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07726174049462779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07726174049462779 | validation: 0.09424928596900513]
	TIME [epoch: 16.1 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09941269062104241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09941269062104241 | validation: 0.15674357157849578]
	TIME [epoch: 16.1 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1123224531699198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1123224531699198 | validation: 0.13669068810444937]
	TIME [epoch: 16.1 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0976150435255352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0976150435255352 | validation: 0.10795536841341867]
	TIME [epoch: 16.1 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08188331653839463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08188331653839463 | validation: 0.09533882790423767]
	TIME [epoch: 16.1 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056313232664409074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056313232664409074 | validation: 0.09722741915688289]
	TIME [epoch: 16.1 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05747754850404606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05747754850404606 | validation: 0.07086055078328767]
	TIME [epoch: 16.1 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053546536905383774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053546536905383774 | validation: 0.15732929303932042]
	TIME [epoch: 16.1 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11968772772489505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11968772772489505 | validation: 0.1288218810206776]
	TIME [epoch: 16.1 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10133191673028791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10133191673028791 | validation: 0.12943566854612532]
	TIME [epoch: 16.1 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09667302512638296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09667302512638296 | validation: 0.08369788794332234]
	TIME [epoch: 16.1 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09523625729600894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09523625729600894 | validation: 0.24445238545591375]
	TIME [epoch: 16.1 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2507569321560044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2507569321560044 | validation: 0.22022103923963426]
	TIME [epoch: 16.1 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2360527584170601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2360527584170601 | validation: 0.1666482914178609]
	TIME [epoch: 16.1 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12053180547213417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12053180547213417 | validation: 0.13947679576991687]
	TIME [epoch: 16.1 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11314653584596653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11314653584596653 | validation: 0.11747603340679001]
	TIME [epoch: 16.1 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0802218117974814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0802218117974814 | validation: 0.10381414413658599]
	TIME [epoch: 16.1 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07919277203618515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07919277203618515 | validation: 0.10517332584366663]
	TIME [epoch: 16.1 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07137491913316088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07137491913316088 | validation: 0.10339733858718492]
	TIME [epoch: 16.1 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12380361423404386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12380361423404386 | validation: 0.22577159783247885]
	TIME [epoch: 16.1 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22330136581539656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22330136581539656 | validation: 0.17120151711449783]
	TIME [epoch: 16.1 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1427652143033559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1427652143033559 | validation: 0.11590043846203257]
	TIME [epoch: 16.1 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07479769328068514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07479769328068514 | validation: 0.13655954824467043]
	TIME [epoch: 16.1 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09852635139903931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09852635139903931 | validation: 0.12012185164361854]
	TIME [epoch: 16.1 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08903072518948392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08903072518948392 | validation: 0.09688381282135172]
	TIME [epoch: 16.1 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07237874529447956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07237874529447956 | validation: 0.07622295970735263]
	TIME [epoch: 16.1 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048253705847649764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048253705847649764 | validation: 0.07472974217278827]
	TIME [epoch: 16.1 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0483121224967062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0483121224967062 | validation: 0.06123089170413648]
	TIME [epoch: 16.1 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04766612398087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04766612398087 | validation: 0.09756916093086782]
	TIME [epoch: 16.1 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1215330140161635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1215330140161635 | validation: 0.3778504825240456]
	TIME [epoch: 16.1 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38979408113811276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38979408113811276 | validation: 0.2905555337492678]
	TIME [epoch: 16.1 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33283363340026684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33283363340026684 | validation: 0.19140940236757142]
	TIME [epoch: 16.1 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18642186927667168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18642186927667168 | validation: 0.2035693742140368]
	TIME [epoch: 16.1 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17924565076813867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17924565076813867 | validation: 0.18831525539126082]
	TIME [epoch: 16.1 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15716069758159373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15716069758159373 | validation: 0.1654290551123625]
	TIME [epoch: 16.1 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14730203338914144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14730203338914144 | validation: 0.15612816550092765]
	TIME [epoch: 16.1 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11176862176553798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11176862176553798 | validation: 0.13614764388693443]
	TIME [epoch: 16.1 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07888705846292987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07888705846292987 | validation: 0.13022565039483508]
	TIME [epoch: 16.1 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08417900118878822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08417900118878822 | validation: 0.11920557855328505]
	TIME [epoch: 16 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08474963298066907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08474963298066907 | validation: 0.17851061510259392]
	TIME [epoch: 16 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12454944265606217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12454944265606217 | validation: 0.29822528847684593]
	TIME [epoch: 16.1 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45573803110522476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45573803110522476 | validation: 0.257091373331519]
	TIME [epoch: 16.1 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22869353145709373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22869353145709373 | validation: 0.24087036855733854]
	TIME [epoch: 16.1 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21348316574420134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21348316574420134 | validation: 0.1870995388927088]
	TIME [epoch: 16.1 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13411776195467234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13411776195467234 | validation: 0.1404631244977062]
	TIME [epoch: 16.1 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09982118786110533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09982118786110533 | validation: 0.2831684211875155]
	TIME [epoch: 16.1 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_152909/states/model_phi1_3b_v_mmd1_1196.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 8615.917 seconds.
