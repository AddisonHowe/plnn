Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2911199954

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.003969510632994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.003969510632994 | validation: 6.824045065723199]
	TIME [epoch: 42.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.225348799981417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.225348799981417 | validation: 6.427193096179993]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.419877335410436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.419877335410436 | validation: 6.302390720606882]
	TIME [epoch: 0.919 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.950315746076865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.950315746076865 | validation: 6.490556836079519]
	TIME [epoch: 0.924 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.643163757866335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.643163757866335 | validation: 6.35449416117827]
	TIME [epoch: 0.92 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.491552498989779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.491552498989779 | validation: 6.030567250157571]
	TIME [epoch: 0.921 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.322841362377223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.322841362377223 | validation: 6.196401790849646]
	TIME [epoch: 0.924 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.308987692376611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.308987692376611 | validation: 6.0539796644825605]
	TIME [epoch: 0.922 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.149947519697151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.149947519697151 | validation: 6.029849593054137]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.096136356978268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.096136356978268 | validation: 6.02800287866708]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.018170240255767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.018170240255767 | validation: 5.873542082278567]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9772871494663145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9772871494663145 | validation: 6.044986743062405]
	TIME [epoch: 0.923 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9742067657734346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9742067657734346 | validation: 5.807081073704601]
	TIME [epoch: 0.919 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.011388716392024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.011388716392024 | validation: 6.050313452564622]
	TIME [epoch: 0.926 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.027008285226089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.027008285226089 | validation: 5.813514428116547]
	TIME [epoch: 0.933 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.788124166669482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.788124166669482 | validation: 5.74767043265592]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7752154147633665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7752154147633665 | validation: 5.901733108395]
	TIME [epoch: 0.932 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.817672467523374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.817672467523374 | validation: 5.73443486270461]
	TIME [epoch: 0.919 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7789145628335308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7789145628335308 | validation: 5.821867834734716]
	TIME [epoch: 0.924 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.757364583119417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.757364583119417 | validation: 5.693561024697928]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.68562096585082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.68562096585082 | validation: 5.748668451962158]
	TIME [epoch: 0.926 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6626002811995146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6626002811995146 | validation: 5.649878074503118]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6384050069940734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6384050069940734 | validation: 5.726610463205035]
	TIME [epoch: 0.919 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.646963120941648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.646963120941648 | validation: 5.623696878258483]
	TIME [epoch: 0.917 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.638679826332075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.638679826332075 | validation: 5.710099718085466]
	TIME [epoch: 0.919 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.662623509943868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.662623509943868 | validation: 5.613529970216614]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.591028496137721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.591028496137721 | validation: 5.612545740283593]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.559379395471058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.559379395471058 | validation: 5.578132100188]
	TIME [epoch: 0.922 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.527101694837262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.527101694837262 | validation: 5.559855577551382]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.515463522179991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.515463522179991 | validation: 5.523577760303529]
	TIME [epoch: 0.922 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5009982216735835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5009982216735835 | validation: 5.563458555998564]
	TIME [epoch: 0.923 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.521585489248948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.521585489248948 | validation: 5.477345064492733]
	TIME [epoch: 0.921 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4910270463884303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4910270463884303 | validation: 5.505231834278463]
	TIME [epoch: 0.924 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.490976997249051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.490976997249051 | validation: 5.448455925502111]
	TIME [epoch: 0.922 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4371873426607102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4371873426607102 | validation: 5.433899057243056]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4191229472646647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4191229472646647 | validation: 5.414618369900688]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.398186525155643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.398186525155643 | validation: 5.388043307616545]
	TIME [epoch: 0.921 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3962301390085976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3962301390085976 | validation: 5.3744346234497]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3777638145705584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3777638145705584 | validation: 5.345914516995097]
	TIME [epoch: 0.922 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.383148523340018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.383148523340018 | validation: 5.332167757531889]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3491888107522456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3491888107522456 | validation: 5.297647828496215]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3383554155686923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3383554155686923 | validation: 5.263703578163667]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.315266107453556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.315266107453556 | validation: 5.250191578083725]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2994658509527346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2994658509527346 | validation: 5.22159106161906]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.272718658394831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.272718658394831 | validation: 5.188685624155817]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.265458720379645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.265458720379645 | validation: 5.184622861845391]
	TIME [epoch: 0.92 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2494225997919264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2494225997919264 | validation: 5.141567431504464]
	TIME [epoch: 0.922 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.239992449252317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.239992449252317 | validation: 5.1492013249938955]
	TIME [epoch: 0.924 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2309294635834216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2309294635834216 | validation: 5.08065850489798]
	TIME [epoch: 0.916 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.228287447781537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.228287447781537 | validation: 5.109773206582939]
	TIME [epoch: 0.926 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2011997399537213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2011997399537213 | validation: 5.017439257690876]
	TIME [epoch: 0.922 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1887730804670027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1887730804670027 | validation: 5.044292918252697]
	TIME [epoch: 0.924 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1595034547390846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1595034547390846 | validation: 4.98888825640805]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1373055784889177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1373055784889177 | validation: 4.986088375075956]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1244478391889814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1244478391889814 | validation: 4.918927894822352]
	TIME [epoch: 0.918 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1100995015541435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1100995015541435 | validation: 4.952411592108556]
	TIME [epoch: 0.922 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.099271120638201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.099271120638201 | validation: 4.845419362644683]
	TIME [epoch: 0.922 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0857150496412467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0857150496412467 | validation: 4.868430249252132]
	TIME [epoch: 0.925 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.055882393691035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.055882393691035 | validation: 4.709296298458589]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.007483366711481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.007483366711481 | validation: 4.548712506611577]
	TIME [epoch: 0.921 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8540643353120774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8540643353120774 | validation: 4.245502433620629]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6672465158437695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6672465158437695 | validation: 4.116716269187526]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.491427182423794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.491427182423794 | validation: 3.747955077532285]
	TIME [epoch: 0.922 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.526714839074936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.526714839074936 | validation: 3.836790457738428]
	TIME [epoch: 0.918 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.436539425734598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.436539425734598 | validation: 2.9657655943879337]
	TIME [epoch: 0.916 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.205952119059464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.205952119059464 | validation: 2.28189374242517]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.800882335493618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.800882335493618 | validation: 2.0535808217434908]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5592318765133615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5592318765133615 | validation: 1.1454324897838724]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2716625852208152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2716625852208152 | validation: 1.7968459493921243]
	TIME [epoch: 0.922 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5098536122208237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5098536122208237 | validation: 0.9070374987742927]
	TIME [epoch: 0.915 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1295791673864568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1295791673864568 | validation: 1.0612182677906359]
	TIME [epoch: 0.925 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0515634957860482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0515634957860482 | validation: 0.983116629887372]
	TIME [epoch: 0.927 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0286644755231278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0286644755231278 | validation: 0.9034621181341053]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0086195360762775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0086195360762775 | validation: 1.0582888553769676]
	TIME [epoch: 0.926 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0017959109377383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0017959109377383 | validation: 0.888362719254157]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0017315401854838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0017315401854838 | validation: 1.285630976739757]
	TIME [epoch: 0.936 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0813256789044192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0813256789044192 | validation: 0.8412550356596429]
	TIME [epoch: 0.921 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.001538027936445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.001538027936445 | validation: 1.0639745638149352]
	TIME [epoch: 0.919 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9879539210804227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9879539210804227 | validation: 0.8400686549343306]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9535714923788072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9535714923788072 | validation: 0.954919901627315]
	TIME [epoch: 0.926 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9537025403718207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9537025403718207 | validation: 0.8370118943416162]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.950430347055976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.950430347055976 | validation: 1.0096228076029692]
	TIME [epoch: 0.925 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9579216414168973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9579216414168973 | validation: 0.8196280404209548]
	TIME [epoch: 0.92 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9621742738885283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9621742738885283 | validation: 1.0899038834645711]
	TIME [epoch: 0.921 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9900212323589946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9900212323589946 | validation: 0.8054835360681274]
	TIME [epoch: 0.922 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9358560913638043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9358560913638043 | validation: 0.9280228907826583]
	TIME [epoch: 0.924 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9264934692306838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9264934692306838 | validation: 0.8443981442965454]
	TIME [epoch: 0.921 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9231668152205927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9231668152205927 | validation: 0.9051734096580571]
	TIME [epoch: 0.926 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9439954636544806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9439954636544806 | validation: 0.9037994158493728]
	TIME [epoch: 0.92 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9712003786418191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9712003786418191 | validation: 0.9671692881128081]
	TIME [epoch: 0.919 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.063173107408027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.063173107408027 | validation: 0.9782051014302315]
	TIME [epoch: 0.919 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9497798862318044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9497798862318044 | validation: 0.6977979466723585]
	TIME [epoch: 0.919 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9910061573900246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9910061573900246 | validation: 1.2315312266614502]
	TIME [epoch: 0.922 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0091844925892184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0091844925892184 | validation: 0.7132239390735977]
	TIME [epoch: 0.924 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.940182533903662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.940182533903662 | validation: 0.845753657860866]
	TIME [epoch: 0.924 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8930730638085999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8930730638085999 | validation: 0.8409454292631318]
	TIME [epoch: 0.926 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8842843193043067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8842843193043067 | validation: 0.748331178414522]
	TIME [epoch: 0.922 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8904570194438876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8904570194438876 | validation: 0.9606718852883986]
	TIME [epoch: 0.92 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9000430602381718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9000430602381718 | validation: 0.7382979397850634]
	TIME [epoch: 0.921 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9075152349972844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9075152349972844 | validation: 1.01433446889869]
	TIME [epoch: 0.922 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9299962722086899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9299962722086899 | validation: 0.6806194546243918]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9419265931524512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9419265931524512 | validation: 1.052080031854809]
	TIME [epoch: 0.925 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9574400626841312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9574400626841312 | validation: 0.6998117355153058]
	TIME [epoch: 0.924 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0763363250947302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0763363250947302 | validation: 0.8870876786229807]
	TIME [epoch: 0.921 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9511572344431783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9511572344431783 | validation: 1.1630772867961927]
	TIME [epoch: 0.923 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.011599923134612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.011599923134612 | validation: 0.7573665550364634]
	TIME [epoch: 0.921 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8796559197788338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8796559197788338 | validation: 0.7574490244140671]
	TIME [epoch: 0.921 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8635321540429901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8635321540429901 | validation: 0.8435398426269991]
	TIME [epoch: 0.921 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8819338325018561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8819338325018561 | validation: 0.8293637741817546]
	TIME [epoch: 0.921 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8906661668149514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8906661668149514 | validation: 0.8416286169548528]
	TIME [epoch: 0.92 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9699878396264047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9699878396264047 | validation: 1.0345082411162456]
	TIME [epoch: 0.923 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0191768527521712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0191768527521712 | validation: 0.7622538916068522]
	TIME [epoch: 0.921 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0351473251964125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0351473251964125 | validation: 0.766964637411242]
	TIME [epoch: 0.921 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8568552797832026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8568552797832026 | validation: 0.9560904086378099]
	TIME [epoch: 0.92 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9265232548321999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9265232548321999 | validation: 0.798535145506887]
	TIME [epoch: 0.924 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9776025054398375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9776025054398375 | validation: 0.7903783241122376]
	TIME [epoch: 0.93 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672889522283432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8672889522283432 | validation: 1.0227910004270042]
	TIME [epoch: 0.92 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.926102111982466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.926102111982466 | validation: 0.6496083051441909]
	TIME [epoch: 0.92 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.019726843401306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.019726843401306 | validation: 1.034131643717479]
	TIME [epoch: 0.934 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9389501686857878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9389501686857878 | validation: 0.763908724907687]
	TIME [epoch: 0.928 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775056666685563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8775056666685563 | validation: 0.7233250737307076]
	TIME [epoch: 0.922 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8752466299066651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8752466299066651 | validation: 0.8918525061514198]
	TIME [epoch: 0.92 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8737637862398562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8737637862398562 | validation: 0.700255325088678]
	TIME [epoch: 0.925 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8994140674649904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8994140674649904 | validation: 0.8742047095924429]
	TIME [epoch: 0.921 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8928595996319453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8928595996319453 | validation: 0.734391371518284]
	TIME [epoch: 0.921 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9082413964657309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9082413964657309 | validation: 0.846633939865821]
	TIME [epoch: 0.921 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9013485784257815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9013485784257815 | validation: 1.0211100396908668]
	TIME [epoch: 0.926 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9584488154180661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9584488154180661 | validation: 0.7292353969741996]
	TIME [epoch: 0.923 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9143526348212041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9143526348212041 | validation: 0.9933827259832637]
	TIME [epoch: 0.922 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.910885663702494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.910885663702494 | validation: 0.6252412597027477]
	TIME [epoch: 0.921 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8805589835927228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8805589835927228 | validation: 0.890860168527723]
	TIME [epoch: 0.925 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8908488959735138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8908488959735138 | validation: 0.7212691702858889]
	TIME [epoch: 0.922 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9660562924286622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9660562924286622 | validation: 0.8752349191578666]
	TIME [epoch: 0.92 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8943696762947179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8943696762947179 | validation: 0.7374366047344602]
	TIME [epoch: 0.923 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8923742653150954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8923742653150954 | validation: 0.7702760020152657]
	TIME [epoch: 0.925 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8519065782602874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8519065782602874 | validation: 0.7696031338131043]
	TIME [epoch: 0.928 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8506324870775197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8506324870775197 | validation: 0.7591031783813444]
	TIME [epoch: 0.925 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8522050685574891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8522050685574891 | validation: 0.9857162585612582]
	TIME [epoch: 0.927 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9136365926014856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9136365926014856 | validation: 0.7499011219181095]
	TIME [epoch: 0.924 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9818470835006536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9818470835006536 | validation: 1.1548166463048362]
	TIME [epoch: 0.924 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9810947120293413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9810947120293413 | validation: 0.6565051829654396]
	TIME [epoch: 0.924 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8798709713761922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8798709713761922 | validation: 0.9922307247991458]
	TIME [epoch: 0.924 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.055649562953686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.055649562953686 | validation: 0.842030000796961]
	TIME [epoch: 0.926 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0826235869994714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0826235869994714 | validation: 0.7649729359460867]
	TIME [epoch: 0.949 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8440667708138375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8440667708138375 | validation: 0.9435595920272267]
	TIME [epoch: 0.926 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.982546139607772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.982546139607772 | validation: 0.7562297744442272]
	TIME [epoch: 0.926 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.938560598418223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.938560598418223 | validation: 0.7391902472599938]
	TIME [epoch: 0.924 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8267914060621502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8267914060621502 | validation: 0.794224368003934]
	TIME [epoch: 0.925 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8737284127000837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8737284127000837 | validation: 0.7769634307420156]
	TIME [epoch: 0.923 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8818484636204811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8818484636204811 | validation: 0.7657700286069066]
	TIME [epoch: 0.924 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8401702272014048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8401702272014048 | validation: 0.73604230078584]
	TIME [epoch: 0.925 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.824218266712953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.824218266712953 | validation: 0.6931782814205557]
	TIME [epoch: 0.925 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8242889610848276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8242889610848276 | validation: 0.75413126571819]
	TIME [epoch: 0.925 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8234585611329935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8234585611329935 | validation: 0.7086265382578545]
	TIME [epoch: 0.925 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8150937023363035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8150937023363035 | validation: 0.8032607991270915]
	TIME [epoch: 0.925 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8198594082328802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8198594082328802 | validation: 0.6015912089567741]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9776289369543782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9776289369543782 | validation: 1.5195186318063865]
	TIME [epoch: 0.922 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.309546444628425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.309546444628425 | validation: 0.7465141485142284]
	TIME [epoch: 0.924 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0966920897654582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0966920897654582 | validation: 0.7368886975376241]
	TIME [epoch: 0.933 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9881600151527016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9881600151527016 | validation: 1.0430902344794526]
	TIME [epoch: 0.923 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9434630415159722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9434630415159722 | validation: 0.7519328066873022]
	TIME [epoch: 0.922 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8515720143182991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8515720143182991 | validation: 0.6975401384054208]
	TIME [epoch: 0.925 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8811285896234725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8811285896234725 | validation: 0.8134688459308376]
	TIME [epoch: 0.934 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8446831404505251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8446831404505251 | validation: 0.7676670947090838]
	TIME [epoch: 0.923 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8194167485193222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8194167485193222 | validation: 0.648631264358777]
	TIME [epoch: 0.922 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8325120053305705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8325120053305705 | validation: 0.7465539853594456]
	TIME [epoch: 0.925 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8109705498210653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8109705498210653 | validation: 0.7049123612411385]
	TIME [epoch: 0.924 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8120951335324498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8120951335324498 | validation: 0.712048607884257]
	TIME [epoch: 0.923 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8327373541854365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8327373541854365 | validation: 0.7953407299297884]
	TIME [epoch: 0.923 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.88948803485433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.88948803485433 | validation: 0.92400617526663]
	TIME [epoch: 0.925 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9629236208983811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9629236208983811 | validation: 0.8139405027466154]
	TIME [epoch: 0.931 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0193294320437494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0193294320437494 | validation: 0.7215906818054592]
	TIME [epoch: 0.923 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8028369133818112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8028369133818112 | validation: 0.7872014456459238]
	TIME [epoch: 0.923 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8373084671317206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8373084671317206 | validation: 0.6898255180425663]
	TIME [epoch: 0.923 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9085972312804779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9085972312804779 | validation: 0.7762085598066945]
	TIME [epoch: 0.928 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8383521428028329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8383521428028329 | validation: 0.7393737160320795]
	TIME [epoch: 0.922 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8276715658418458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8276715658418458 | validation: 0.6833806168775889]
	TIME [epoch: 0.923 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8276744670437519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8276744670437519 | validation: 0.9319611871056632]
	TIME [epoch: 0.924 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.87197008949795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.87197008949795 | validation: 0.6400518023271644]
	TIME [epoch: 0.925 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8520653780463912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8520653780463912 | validation: 0.8547891002515066]
	TIME [epoch: 0.924 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8334668802425048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8334668802425048 | validation: 0.6049764911377719]
	TIME [epoch: 0.923 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8145635327650363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8145635327650363 | validation: 0.8188044077129805]
	TIME [epoch: 0.921 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.841480022791421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.841480022791421 | validation: 0.6332285420567016]
	TIME [epoch: 0.926 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0190992473264118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0190992473264118 | validation: 0.8812717159613049]
	TIME [epoch: 0.922 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9226409711408888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9226409711408888 | validation: 0.7005835308155098]
	TIME [epoch: 0.921 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8920377930319429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8920377930319429 | validation: 0.7329804300818672]
	TIME [epoch: 0.919 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.816917079451325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.816917079451325 | validation: 0.8653262951350952]
	TIME [epoch: 0.921 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8363061464663164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8363061464663164 | validation: 0.6630137475355373]
	TIME [epoch: 0.923 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8327778928104721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8327778928104721 | validation: 0.82378058053378]
	TIME [epoch: 0.922 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8196034995355426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8196034995355426 | validation: 0.7008489848188006]
	TIME [epoch: 0.923 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810819610737763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.810819610737763 | validation: 0.7069026208902023]
	TIME [epoch: 0.925 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8471105254614643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8471105254614643 | validation: 0.8528199329419778]
	TIME [epoch: 0.924 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9195829280253375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9195829280253375 | validation: 0.7200366264666975]
	TIME [epoch: 0.925 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0084960504796558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0084960504796558 | validation: 0.7232345649400824]
	TIME [epoch: 0.924 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7879131958624271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7879131958624271 | validation: 0.76519040535642]
	TIME [epoch: 0.926 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876115397703757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7876115397703757 | validation: 0.6009013103476701]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.842406363580886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.842406363580886 | validation: 0.8217180377550163]
	TIME [epoch: 0.918 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.814376217838213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.814376217838213 | validation: 0.6025063963813356]
	TIME [epoch: 0.914 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8361868959952162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8361868959952162 | validation: 0.8213440539216693]
	TIME [epoch: 0.915 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.815024609878227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.815024609878227 | validation: 0.5812371164541362]
	TIME [epoch: 0.915 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8450295080067859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8450295080067859 | validation: 0.8557253457537926]
	TIME [epoch: 41.9 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8327845640454561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8327845640454561 | validation: 0.6104490648933176]
	TIME [epoch: 1.83 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8244852193720373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8244852193720373 | validation: 0.8306778456593549]
	TIME [epoch: 1.81 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8046380999788514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8046380999788514 | validation: 0.669279163123818]
	TIME [epoch: 1.81 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7972159608981015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7972159608981015 | validation: 0.9177685063759367]
	TIME [epoch: 1.82 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8815572558983894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8815572558983894 | validation: 0.9623615485168167]
	TIME [epoch: 1.81 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.064961559351082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.064961559351082 | validation: 0.8024274867136714]
	TIME [epoch: 1.82 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9962404700935892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9962404700935892 | validation: 0.6619714625646188]
	TIME [epoch: 1.82 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7548721700385476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7548721700385476 | validation: 0.7844959481289232]
	TIME [epoch: 1.82 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8245862080378734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8245862080378734 | validation: 0.689487747048203]
	TIME [epoch: 1.83 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8603186887451298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8603186887451298 | validation: 0.67757390886866]
	TIME [epoch: 1.82 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.762991519411056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.762991519411056 | validation: 0.6745678200258745]
	TIME [epoch: 1.82 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524523372948473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7524523372948473 | validation: 0.6315876086356247]
	TIME [epoch: 1.81 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7399558017508089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7399558017508089 | validation: 0.6944787715448447]
	TIME [epoch: 1.81 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564394907309878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7564394907309878 | validation: 0.6633509476840316]
	TIME [epoch: 1.82 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260717209803279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8260717209803279 | validation: 0.8934803976915507]
	TIME [epoch: 1.82 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9798706068893512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9798706068893512 | validation: 0.8404820497935757]
	TIME [epoch: 1.81 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9459766242446971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9459766242446971 | validation: 0.6248324570570263]
	TIME [epoch: 1.87 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7472153349334416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7472153349334416 | validation: 0.7032512064122876]
	TIME [epoch: 1.81 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597021208736534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7597021208736534 | validation: 0.6205177086851772]
	TIME [epoch: 1.82 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861595047905369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7861595047905369 | validation: 0.7003789805230883]
	TIME [epoch: 1.82 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769329344924466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7769329344924466 | validation: 0.7459178581773578]
	TIME [epoch: 1.81 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7924084693793052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7924084693793052 | validation: 0.700278794070643]
	TIME [epoch: 1.81 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031598643703538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8031598643703538 | validation: 0.7341456114077858]
	TIME [epoch: 1.81 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7791401815063665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7791401815063665 | validation: 0.6066167498723544]
	TIME [epoch: 1.82 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7450014223597475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7450014223597475 | validation: 0.6414757609653369]
	TIME [epoch: 1.81 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7415165334818101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7415165334818101 | validation: 0.7314475571565116]
	TIME [epoch: 1.82 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966648269098713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7966648269098713 | validation: 0.5901997704246303]
	TIME [epoch: 1.81 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8277280439394966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8277280439394966 | validation: 0.6786411881042641]
	TIME [epoch: 1.81 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7553233674141726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7553233674141726 | validation: 0.6132006145001553]
	TIME [epoch: 1.82 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7172305311675602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7172305311675602 | validation: 0.6173040531420144]
	TIME [epoch: 1.81 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7183551965321532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7183551965321532 | validation: 0.7275491327436681]
	TIME [epoch: 1.81 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7437249657668409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7437249657668409 | validation: 0.617127050230999]
	TIME [epoch: 1.81 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7542479353738102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7542479353738102 | validation: 0.6624774896989415]
	TIME [epoch: 1.82 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7570026464621237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7570026464621237 | validation: 0.702950039589563]
	TIME [epoch: 1.82 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837681966594907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7837681966594907 | validation: 0.5191202207073506]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7081965133207689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7081965133207689 | validation: 0.6292386101378296]
	TIME [epoch: 1.81 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6994318870915882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6994318870915882 | validation: 0.6746649011718615]
	TIME [epoch: 1.8 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7107651100718125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7107651100718125 | validation: 0.502087474136787]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6964490506036393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6964490506036393 | validation: 0.6211815495385813]
	TIME [epoch: 1.8 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6868709708365751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6868709708365751 | validation: 0.595038685819452]
	TIME [epoch: 1.79 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6814842108971317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6814842108971317 | validation: 0.5481729682121694]
	TIME [epoch: 1.8 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6649135239931261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6649135239931261 | validation: 0.634751630388794]
	TIME [epoch: 1.81 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6688360715778605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6688360715778605 | validation: 0.5436243896980452]
	TIME [epoch: 1.81 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6706009801321865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6706009801321865 | validation: 0.6025083101331359]
	TIME [epoch: 1.81 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6771257052135906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6771257052135906 | validation: 0.6283828015362268]
	TIME [epoch: 1.8 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7214964454917271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7214964454917271 | validation: 0.7384877558322289]
	TIME [epoch: 1.81 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9299051132653958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9299051132653958 | validation: 0.7407071212500864]
	TIME [epoch: 1.82 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8433949987561162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8433949987561162 | validation: 0.5231480283448178]
	TIME [epoch: 1.81 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6659627001136567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6659627001136567 | validation: 0.7188651197863378]
	TIME [epoch: 1.81 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7156645901265472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7156645901265472 | validation: 0.6461723503340662]
	TIME [epoch: 1.8 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6790117761932549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6790117761932549 | validation: 0.48533113035668685]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6706040251529959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6706040251529959 | validation: 0.5971637878140581]
	TIME [epoch: 1.81 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6580042318100611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6580042318100611 | validation: 0.5986301743939143]
	TIME [epoch: 1.8 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7108319404928921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7108319404928921 | validation: 0.5902705485105316]
	TIME [epoch: 1.81 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7338745084209586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7338745084209586 | validation: 0.5326986873135348]
	TIME [epoch: 1.8 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.704440319572719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.704440319572719 | validation: 0.5689473040936631]
	TIME [epoch: 1.81 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.632637671179006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.632637671179006 | validation: 0.5389478421655995]
	TIME [epoch: 1.8 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6222011613894436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6222011613894436 | validation: 0.5102239982160615]
	TIME [epoch: 1.81 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6203746113051867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6203746113051867 | validation: 0.5531232229877694]
	TIME [epoch: 1.81 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293940584790352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6293940584790352 | validation: 0.5120425963979496]
	TIME [epoch: 1.81 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6321935649786187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6321935649786187 | validation: 0.5498489947831099]
	TIME [epoch: 1.8 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6377333437089449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6377333437089449 | validation: 0.5233239016958355]
	TIME [epoch: 1.8 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.670248091657368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.670248091657368 | validation: 0.5489874891116712]
	TIME [epoch: 1.81 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6390600901851944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6390600901851944 | validation: 0.4840632750493315]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.613925512054701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.613925512054701 | validation: 0.5259817792801835]
	TIME [epoch: 1.81 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6097297457644117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6097297457644117 | validation: 0.46566224788278054]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6083611944447813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6083611944447813 | validation: 0.5579251891794208]
	TIME [epoch: 1.81 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6095925335159581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6095925335159581 | validation: 0.5056836783674223]
	TIME [epoch: 1.8 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.643915495735143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.643915495735143 | validation: 0.5299786345622727]
	TIME [epoch: 1.8 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6041878383756076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6041878383756076 | validation: 0.4812586871175891]
	TIME [epoch: 1.81 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5904755151504668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5904755151504668 | validation: 0.4922510820971911]
	TIME [epoch: 1.8 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5851245408835263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5851245408835263 | validation: 0.5016285875380904]
	TIME [epoch: 1.8 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5955726131697534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5955726131697534 | validation: 0.48224842020111675]
	TIME [epoch: 1.8 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6043066033960925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6043066033960925 | validation: 0.6115832566170432]
	TIME [epoch: 1.81 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6250360654374025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6250360654374025 | validation: 0.5087618436395007]
	TIME [epoch: 1.8 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427916942961879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6427916942961879 | validation: 0.4347243688388073]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.552192587127062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.552192587127062 | validation: 0.49027763039894234]
	TIME [epoch: 1.81 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5447738736655734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5447738736655734 | validation: 0.44663795408564566]
	TIME [epoch: 1.81 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5433634274278681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5433634274278681 | validation: 0.5095064643632311]
	TIME [epoch: 1.81 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5749445605169572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5749445605169572 | validation: 0.4084253650622414]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.613843801641804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.613843801641804 | validation: 0.43197578411721205]
	TIME [epoch: 1.81 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5163974180481049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5163974180481049 | validation: 0.45184191601445645]
	TIME [epoch: 1.81 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5055478082502222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5055478082502222 | validation: 0.41367612028252493]
	TIME [epoch: 1.81 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5255454445045266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5255454445045266 | validation: 0.6549383138905429]
	TIME [epoch: 1.81 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6790450245313184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6790450245313184 | validation: 0.4216902447853621]
	TIME [epoch: 1.8 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6803690734981175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6803690734981175 | validation: 0.3845025230317545]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5529998987867561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5529998987867561 | validation: 0.5331682732539406]
	TIME [epoch: 1.82 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5606911387522683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5606911387522683 | validation: 0.46600298111663263]
	TIME [epoch: 1.81 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.501441486896301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.501441486896301 | validation: 0.42013255330932725]
	TIME [epoch: 1.81 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4754477701887373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4754477701887373 | validation: 0.45341226111699573]
	TIME [epoch: 1.81 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49344047165364885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49344047165364885 | validation: 0.42004486031728305]
	TIME [epoch: 1.82 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.540702636393733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.540702636393733 | validation: 0.5291528697551166]
	TIME [epoch: 1.81 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5360837882339664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5360837882339664 | validation: 0.4312494328625732]
	TIME [epoch: 1.81 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5739511245013122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5739511245013122 | validation: 0.36452266598483307]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4430888031830817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4430888031830817 | validation: 0.5127606054673495]
	TIME [epoch: 1.81 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48309001716703087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48309001716703087 | validation: 0.40289195089303487]
	TIME [epoch: 1.81 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4950712131095329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4950712131095329 | validation: 0.39550519258489353]
	TIME [epoch: 1.8 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4286325169660603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4286325169660603 | validation: 0.37312736330996826]
	TIME [epoch: 1.81 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3953962452048745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3953962452048745 | validation: 0.4489036544789567]
	TIME [epoch: 1.8 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3981673198595654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3981673198595654 | validation: 0.443137483800572]
	TIME [epoch: 1.81 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6036124861060058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6036124861060058 | validation: 0.4620578082777173]
	TIME [epoch: 1.8 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4581772739092159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4581772739092159 | validation: 0.47265791953454406]
	TIME [epoch: 1.8 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6401348146300994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6401348146300994 | validation: 0.4370190172432108]
	TIME [epoch: 1.8 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43006595429827454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43006595429827454 | validation: 0.5954385578566689]
	TIME [epoch: 1.81 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5248278117539834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5248278117539834 | validation: 0.42343106406632836]
	TIME [epoch: 1.8 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4674722867703539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4674722867703539 | validation: 0.35154926109074564]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39819768837112984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39819768837112984 | validation: 0.4463745827821387]
	TIME [epoch: 1.81 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40119373140139986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40119373140139986 | validation: 0.40839012983296624]
	TIME [epoch: 1.82 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49221710703137533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49221710703137533 | validation: 0.3735313323799053]
	TIME [epoch: 1.81 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3704348082985102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3704348082985102 | validation: 0.3418000190398]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36112778487073116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36112778487073116 | validation: 0.5427502822001843]
	TIME [epoch: 1.81 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46293535221026216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46293535221026216 | validation: 0.44705300786176994]
	TIME [epoch: 1.8 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6236811682980795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6236811682980795 | validation: 0.3052392462238376]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4784379210222744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4784379210222744 | validation: 0.4186105580658011]
	TIME [epoch: 1.81 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39211638660021136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39211638660021136 | validation: 0.4430597475809597]
	TIME [epoch: 1.81 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36423730320609676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36423730320609676 | validation: 0.32378157823540743]
	TIME [epoch: 1.81 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39016008841929106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39016008841929106 | validation: 0.41995161454415664]
	TIME [epoch: 1.8 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35835831866169615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35835831866169615 | validation: 0.37615261428582036]
	TIME [epoch: 1.81 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46300062574390427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46300062574390427 | validation: 0.3846353229550428]
	TIME [epoch: 1.8 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3203846810947663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3203846810947663 | validation: 0.3534180208692181]
	TIME [epoch: 1.8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3036367045526648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3036367045526648 | validation: 0.29507913847515715]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28534331957466325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28534331957466325 | validation: 0.4005211601860068]
	TIME [epoch: 1.81 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3307467108984816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3307467108984816 | validation: 0.45778774555985313]
	TIME [epoch: 1.81 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7134968641320332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7134968641320332 | validation: 0.3439441523125387]
	TIME [epoch: 1.81 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47984296236796214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47984296236796214 | validation: 0.44791901922264865]
	TIME [epoch: 1.81 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39678197626967293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39678197626967293 | validation: 0.4023753274926378]
	TIME [epoch: 1.81 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3368580387466099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3368580387466099 | validation: 0.2843446861066663]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3132919912863512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3132919912863512 | validation: 0.6006054406851774]
	TIME [epoch: 1.81 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5281221809958159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5281221809958159 | validation: 0.38406017949736015]
	TIME [epoch: 1.81 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5343120993632876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5343120993632876 | validation: 0.3909975539338899]
	TIME [epoch: 1.81 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4808943679470703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4808943679470703 | validation: 0.4096073488836205]
	TIME [epoch: 1.82 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4218483819192362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4218483819192362 | validation: 0.47470790432179694]
	TIME [epoch: 1.81 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3761838023880884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3761838023880884 | validation: 0.29588530255160056]
	TIME [epoch: 1.82 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30651741551738837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30651741551738837 | validation: 0.27811704609773635]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26357841270578214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26357841270578214 | validation: 0.32254239893063574]
	TIME [epoch: 1.81 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24173013472863075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24173013472863075 | validation: 0.26273887313948124]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34076062851757377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34076062851757377 | validation: 0.8688561659716719]
	TIME [epoch: 1.81 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8064818199294733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8064818199294733 | validation: 0.28999979500157064]
	TIME [epoch: 1.8 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35131488353416984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35131488353416984 | validation: 0.3211760657006839]
	TIME [epoch: 1.81 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26170532322455126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26170532322455126 | validation: 0.40621236985667564]
	TIME [epoch: 1.81 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30368261848698636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30368261848698636 | validation: 0.2926517684558528]
	TIME [epoch: 1.8 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3088988623284216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3088988623284216 | validation: 0.27475467683268046]
	TIME [epoch: 1.8 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23328792476767138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23328792476767138 | validation: 0.3241959496604376]
	TIME [epoch: 1.81 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2270635250304227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2270635250304227 | validation: 0.23227414347303732]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25533402369377056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25533402369377056 | validation: 0.6192578950051995]
	TIME [epoch: 1.81 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5311467647494733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5311467647494733 | validation: 0.3383891633427907]
	TIME [epoch: 1.81 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41543166785136976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41543166785136976 | validation: 0.2761271367911794]
	TIME [epoch: 1.81 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3049928553038349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3049928553038349 | validation: 0.43086482367464535]
	TIME [epoch: 1.81 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32355322164884626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32355322164884626 | validation: 0.28569173079980276]
	TIME [epoch: 2.16 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2689847813801486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2689847813801486 | validation: 0.28403631325436474]
	TIME [epoch: 1.81 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23678165144504565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23678165144504565 | validation: 0.2960668280443157]
	TIME [epoch: 1.8 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2137481168773271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2137481168773271 | validation: 0.2855999156806512]
	TIME [epoch: 1.81 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23311080350787933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23311080350787933 | validation: 0.32914789516771603]
	TIME [epoch: 1.8 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3746942758241819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3746942758241819 | validation: 0.8099512886374012]
	TIME [epoch: 1.81 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9047074228984155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9047074228984155 | validation: 0.7690122047679838]
	TIME [epoch: 1.82 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0624004170487313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0624004170487313 | validation: 1.1951965229159907]
	TIME [epoch: 1.81 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.132195792576398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.132195792576398 | validation: 0.6266498159319454]
	TIME [epoch: 1.82 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9318872428807171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9318872428807171 | validation: 0.5851267741769097]
	TIME [epoch: 1.81 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.63993877520264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.63993877520264 | validation: 0.6064127343765557]
	TIME [epoch: 1.81 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5970700672339857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5970700672339857 | validation: 0.593600650897664]
	TIME [epoch: 1.81 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6267570839459166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6267570839459166 | validation: 0.5398495755592507]
	TIME [epoch: 1.8 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5565447080878717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5565447080878717 | validation: 0.48604368566163353]
	TIME [epoch: 1.81 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5086211655320355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5086211655320355 | validation: 0.4513568687749243]
	TIME [epoch: 1.8 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5011863135799481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5011863135799481 | validation: 0.3977060914952338]
	TIME [epoch: 1.81 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40851313279230267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40851313279230267 | validation: 0.36928878010308086]
	TIME [epoch: 1.8 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34289838452914034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34289838452914034 | validation: 0.3930220282391147]
	TIME [epoch: 1.81 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30195639896503307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30195639896503307 | validation: 0.2793219940730435]
	TIME [epoch: 1.8 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2893042135669032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2893042135669032 | validation: 0.5021673621591093]
	TIME [epoch: 1.81 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41753306538080437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41753306538080437 | validation: 0.3240873399555282]
	TIME [epoch: 1.8 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4574107829554814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4574107829554814 | validation: 0.2479878198225657]
	TIME [epoch: 1.81 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31200258701841443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31200258701841443 | validation: 0.5941733346145325]
	TIME [epoch: 1.81 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5311364958070837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5311364958070837 | validation: 0.3245282240085625]
	TIME [epoch: 1.81 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27972125333882975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27972125333882975 | validation: 0.3121511151919451]
	TIME [epoch: 1.8 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3380052973666994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3380052973666994 | validation: 0.2801548997489134]
	TIME [epoch: 1.81 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23758429360467986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23758429360467986 | validation: 0.4099922441305032]
	TIME [epoch: 1.81 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27053518319121655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27053518319121655 | validation: 0.26944698108604825]
	TIME [epoch: 1.81 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25100918311658815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25100918311658815 | validation: 0.26591899108603495]
	TIME [epoch: 1.8 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17625547915736475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17625547915736475 | validation: 0.29195483012951867]
	TIME [epoch: 1.81 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19475309517615189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19475309517615189 | validation: 0.27954143390722724]
	TIME [epoch: 1.8 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3256636252347447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3256636252347447 | validation: 0.31236720345696317]
	TIME [epoch: 1.81 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2352646797108126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2352646797108126 | validation: 0.33173853799006275]
	TIME [epoch: 1.81 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2797407364021251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2797407364021251 | validation: 0.37405667279056765]
	TIME [epoch: 1.81 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2733380137302654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2733380137302654 | validation: 0.32306122852024494]
	TIME [epoch: 1.82 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1780441804569206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1780441804569206 | validation: 0.24062410133667453]
	TIME [epoch: 1.81 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16055904651936134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16055904651936134 | validation: 0.31168390398069423]
	TIME [epoch: 1.82 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21527175439558702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21527175439558702 | validation: 0.31031522377602655]
	TIME [epoch: 1.81 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5526696128320986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5526696128320986 | validation: 0.2188758150332087]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1998705362184002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1998705362184002 | validation: 0.32379904849025937]
	TIME [epoch: 1.81 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26165471081642905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26165471081642905 | validation: 0.19777305973206902]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26426115355358293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26426115355358293 | validation: 0.25296681416794675]
	TIME [epoch: 1.81 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1590258387430451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1590258387430451 | validation: 0.23778669767245822]
	TIME [epoch: 1.87 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1431774629624954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1431774629624954 | validation: 0.22520807431093026]
	TIME [epoch: 1.8 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15195914104927183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15195914104927183 | validation: 0.34997308844228225]
	TIME [epoch: 1.81 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24682249909930398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24682249909930398 | validation: 0.3778099106234356]
	TIME [epoch: 1.81 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5371815372447274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5371815372447274 | validation: 0.36018995638204104]
	TIME [epoch: 1.81 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3970169257868366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3970169257868366 | validation: 0.5314392950744905]
	TIME [epoch: 1.81 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4486009500205137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4486009500205137 | validation: 0.491609808177613]
	TIME [epoch: 1.81 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39122451208662673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39122451208662673 | validation: 0.2927641289889434]
	TIME [epoch: 1.81 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2666892627681764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2666892627681764 | validation: 0.26947695240169356]
	TIME [epoch: 1.81 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24153175487767634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24153175487767634 | validation: 0.23399324796980034]
	TIME [epoch: 1.81 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21472805482171603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21472805482171603 | validation: 0.2707892694152768]
	TIME [epoch: 1.81 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1858911279906279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1858911279906279 | validation: 0.34236362185398284]
	TIME [epoch: 1.81 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21378183633788453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21378183633788453 | validation: 0.2478812610066593]
	TIME [epoch: 1.81 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24159227392558058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24159227392558058 | validation: 0.28333118564499643]
	TIME [epoch: 1.81 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1688435140387584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1688435140387584 | validation: 0.22656154044230867]
	TIME [epoch: 1.81 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13617607755760155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13617607755760155 | validation: 0.2303297817333716]
	TIME [epoch: 1.82 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11627926074338367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11627926074338367 | validation: 0.21670453245965204]
	TIME [epoch: 1.82 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1120654983240413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1120654983240413 | validation: 0.19941646380300407]
	TIME [epoch: 1.81 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1450127377752982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1450127377752982 | validation: 0.3675149773075999]
	TIME [epoch: 1.81 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3139884937331508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3139884937331508 | validation: 0.33537874289775094]
	TIME [epoch: 1.81 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3474925607678041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3474925607678041 | validation: 0.3506882753779854]
	TIME [epoch: 1.81 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2016006521028334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2016006521028334 | validation: 0.27174091527978644]
	TIME [epoch: 1.81 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14426021212869042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14426021212869042 | validation: 0.251834003208663]
	TIME [epoch: 1.8 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13214732503830579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13214732503830579 | validation: 0.22364979742753166]
	TIME [epoch: 1.82 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1393535634853451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1393535634853451 | validation: 0.26490700863438693]
	TIME [epoch: 1.81 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.155551016962824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.155551016962824 | validation: 0.269160291850878]
	TIME [epoch: 1.81 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2659786297854939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2659786297854939 | validation: 0.28056338679528037]
	TIME [epoch: 1.81 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22613834659842597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22613834659842597 | validation: 0.35462433119565256]
	TIME [epoch: 1.81 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27166836355602786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27166836355602786 | validation: 0.2876240284912758]
	TIME [epoch: 1.81 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16324004438161357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16324004438161357 | validation: 0.31835412565190446]
	TIME [epoch: 1.81 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19362746221784663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19362746221784663 | validation: 0.22598404288854318]
	TIME [epoch: 1.81 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13615351673260892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13615351673260892 | validation: 0.2530735008860087]
	TIME [epoch: 1.81 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11719988457677047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11719988457677047 | validation: 0.20397070477902046]
	TIME [epoch: 1.81 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11210869179033629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11210869179033629 | validation: 0.22436249273874084]
	TIME [epoch: 1.81 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12402572324179781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12402572324179781 | validation: 0.21071418014549206]
	TIME [epoch: 1.81 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31429609787858587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31429609787858587 | validation: 0.5658547570912363]
	TIME [epoch: 1.81 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5312234437189403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5312234437189403 | validation: 0.3735224370732737]
	TIME [epoch: 1.81 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21605302968499523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21605302968499523 | validation: 0.30909021100040446]
	TIME [epoch: 1.81 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1985882662030606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1985882662030606 | validation: 0.3006484841604189]
	TIME [epoch: 1.81 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14329920161953288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14329920161953288 | validation: 0.2756113887360203]
	TIME [epoch: 1.81 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20603664205886318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20603664205886318 | validation: 0.20686360041686608]
	TIME [epoch: 1.81 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10848592759682264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10848592759682264 | validation: 0.18977430680093585]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11819002712450298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11819002712450298 | validation: 0.2457590355927336]
	TIME [epoch: 1.82 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13696166421424416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13696166421424416 | validation: 0.19071830831039047]
	TIME [epoch: 1.81 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1442377153420702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1442377153420702 | validation: 0.2912328664069494]
	TIME [epoch: 1.81 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19561779449026684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19561779449026684 | validation: 0.21635691938628848]
	TIME [epoch: 1.81 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17414942282557294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17414942282557294 | validation: 0.2774756617512245]
	TIME [epoch: 1.81 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1455479733544747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1455479733544747 | validation: 0.24251786315159665]
	TIME [epoch: 1.8 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11860548087330744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11860548087330744 | validation: 0.21374326253338222]
	TIME [epoch: 1.81 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13556160413670434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13556160413670434 | validation: 0.22266649852193432]
	TIME [epoch: 1.8 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14462449327334195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14462449327334195 | validation: 0.3263788471252176]
	TIME [epoch: 1.81 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2684728978958766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2684728978958766 | validation: 0.28390268138521824]
	TIME [epoch: 1.81 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1689076284587902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1689076284587902 | validation: 0.268107803613491]
	TIME [epoch: 1.81 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13135015583103568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13135015583103568 | validation: 0.2113104339348893]
	TIME [epoch: 1.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10281147943224948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10281147943224948 | validation: 0.19583301819903293]
	TIME [epoch: 1.81 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09227021541442378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09227021541442378 | validation: 0.15641594348009452]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16297687639293643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16297687639293643 | validation: 0.43665980370809443]
	TIME [epoch: 1.81 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42317151913411966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42317151913411966 | validation: 0.26736950536830467]
	TIME [epoch: 1.81 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15948140766771585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15948140766771585 | validation: 0.3184565784151956]
	TIME [epoch: 1.81 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2280063113484855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2280063113484855 | validation: 0.2745528342634819]
	TIME [epoch: 1.81 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1254918024615045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1254918024615045 | validation: 0.2725729454458067]
	TIME [epoch: 1.81 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12192156700949912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12192156700949912 | validation: 0.2188599928006089]
	TIME [epoch: 1.81 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10635017245057903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10635017245057903 | validation: 0.19502668315393945]
	TIME [epoch: 1.81 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11019316851671974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11019316851671974 | validation: 0.42108118179064435]
	TIME [epoch: 1.81 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38331479650476596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38331479650476596 | validation: 0.24576231277798175]
	TIME [epoch: 1.81 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14396702726175487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14396702726175487 | validation: 0.27397060963770054]
	TIME [epoch: 1.82 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17939879951575233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17939879951575233 | validation: 0.2219588412698551]
	TIME [epoch: 1.81 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13701308492175673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13701308492175673 | validation: 0.2080944739544635]
	TIME [epoch: 1.82 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12530036257388222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12530036257388222 | validation: 0.24289618699328588]
	TIME [epoch: 1.81 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13730396803977807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13730396803977807 | validation: 0.17216204554141779]
	TIME [epoch: 1.82 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16757750011116476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16757750011116476 | validation: 0.21068305839205348]
	TIME [epoch: 1.81 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12405714902810606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12405714902810606 | validation: 0.22522092494409837]
	TIME [epoch: 1.81 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14466499719126572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14466499719126572 | validation: 0.20466090108656143]
	TIME [epoch: 1.81 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14751008778635152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14751008778635152 | validation: 0.3082482462837395]
	TIME [epoch: 1.81 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20872426075292957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20872426075292957 | validation: 0.24885859412310338]
	TIME [epoch: 1.81 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12032659108573086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12032659108573086 | validation: 0.22411568421355224]
	TIME [epoch: 1.81 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08843984621465824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08843984621465824 | validation: 0.19634896584896128]
	TIME [epoch: 1.81 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07676328859412336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07676328859412336 | validation: 0.15401974187400388]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07297958568048184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07297958568048184 | validation: 0.17683294293674387]
	TIME [epoch: 1.81 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07532002141808275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07532002141808275 | validation: 0.1631935142856595]
	TIME [epoch: 1.81 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10107678231990092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10107678231990092 | validation: 0.27603247556946764]
	TIME [epoch: 1.81 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19659332052706213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19659332052706213 | validation: 0.3848204920522804]
	TIME [epoch: 1.81 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34246176996595906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34246176996595906 | validation: 0.2530943251699088]
	TIME [epoch: 1.81 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12036430617637905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12036430617637905 | validation: 0.2662162395637894]
	TIME [epoch: 1.81 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13768019873203105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13768019873203105 | validation: 0.176257340688319]
	TIME [epoch: 1.81 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1250625871857693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1250625871857693 | validation: 0.20109565698335438]
	TIME [epoch: 1.81 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08519517485706402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08519517485706402 | validation: 0.14755774194032534]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07506050278389408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07506050278389408 | validation: 0.1826531872046232]
	TIME [epoch: 1.81 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1254983516708654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1254983516708654 | validation: 0.21807675673835414]
	TIME [epoch: 1.81 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23386520477140413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23386520477140413 | validation: 0.2742088442829557]
	TIME [epoch: 1.81 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1550541398103679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1550541398103679 | validation: 0.2598556678455002]
	TIME [epoch: 1.81 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.170789333821278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.170789333821278 | validation: 0.2761270831300441]
	TIME [epoch: 1.81 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15944514885239472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15944514885239472 | validation: 0.17094716938873525]
	TIME [epoch: 1.82 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12628129005359268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12628129005359268 | validation: 0.2943485057795345]
	TIME [epoch: 1.8 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25636562625505976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25636562625505976 | validation: 0.22359075263623868]
	TIME [epoch: 1.81 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.136679439497233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.136679439497233 | validation: 0.2573871859880718]
	TIME [epoch: 1.81 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1350475423034136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1350475423034136 | validation: 0.2559704497203352]
	TIME [epoch: 1.81 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11135838032014547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11135838032014547 | validation: 0.20271303662591533]
	TIME [epoch: 1.81 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10499018014490506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10499018014490506 | validation: 0.19490326818746373]
	TIME [epoch: 1.81 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13001239430841507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13001239430841507 | validation: 0.4175682985767131]
	TIME [epoch: 1.81 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40812221172582713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40812221172582713 | validation: 0.2582083077412719]
	TIME [epoch: 1.81 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11588777355633752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11588777355633752 | validation: 0.3092412027692244]
	TIME [epoch: 1.81 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19162679570798646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19162679570798646 | validation: 0.28353589622186554]
	TIME [epoch: 1.81 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11028965508007847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11028965508007847 | validation: 0.28270163811564036]
	TIME [epoch: 1.81 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10145783796660107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10145783796660107 | validation: 0.22602143472481007]
	TIME [epoch: 1.81 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09513603605233907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09513603605233907 | validation: 0.16506885604084612]
	TIME [epoch: 1.81 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08492103446503776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08492103446503776 | validation: 0.17764501306683567]
	TIME [epoch: 1.81 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07879798274574072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07879798274574072 | validation: 0.14303126295796204]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_499.pth
	Model improved!!!
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0843471374520225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0843471374520225 | validation: 0.23035481453781623]
	TIME [epoch: 1.81 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17655434307090873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17655434307090873 | validation: 0.199845474602648]
	TIME [epoch: 43.9 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29884645828439327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29884645828439327 | validation: 0.37786264516876555]
	TIME [epoch: 3.61 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26854586642234873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26854586642234873 | validation: 0.33652991941817123]
	TIME [epoch: 3.58 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13372246879120214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13372246879120214 | validation: 0.2745598957917838]
	TIME [epoch: 3.59 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11494055655989165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11494055655989165 | validation: 0.2279033787959095]
	TIME [epoch: 3.56 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10329879026922392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10329879026922392 | validation: 0.27943519876481576]
	TIME [epoch: 3.59 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3064686821809153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3064686821809153 | validation: 0.21665225423059953]
	TIME [epoch: 3.58 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5117086147764245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5117086147764245 | validation: 0.48826215074611135]
	TIME [epoch: 3.59 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1366719408601502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1366719408601502 | validation: 0.3236349263102609]
	TIME [epoch: 3.59 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9529456249207013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9529456249207013 | validation: 0.3662255360093105]
	TIME [epoch: 3.58 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0060663664493306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0060663664493306 | validation: 0.37727503982054245]
	TIME [epoch: 3.58 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.083447181983617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.083447181983617 | validation: 0.48896795424534734]
	TIME [epoch: 3.58 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1197249674001615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1197249674001615 | validation: 0.325382173860124]
	TIME [epoch: 3.56 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0544547048438897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0544547048438897 | validation: 0.27595638645085546]
	TIME [epoch: 3.58 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9792088336036935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9792088336036935 | validation: 0.22945450515698518]
	TIME [epoch: 3.57 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9332319636041859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9332319636041859 | validation: 0.21303089091298688]
	TIME [epoch: 3.58 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8989122699123054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8989122699123054 | validation: 0.1952608309449571]
	TIME [epoch: 3.57 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8703095853750216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8703095853750216 | validation: 0.33078470790090053]
	TIME [epoch: 3.57 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9378710335364525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9378710335364525 | validation: 0.3526255774575329]
	TIME [epoch: 3.57 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7565336414771571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7565336414771571 | validation: 0.3802249357119198]
	TIME [epoch: 3.57 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6916803122116991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6916803122116991 | validation: 0.40860181936369244]
	TIME [epoch: 3.59 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6420817090966989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6420817090966989 | validation: 0.2450682985192092]
	TIME [epoch: 3.58 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6219960653059483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6219960653059483 | validation: 0.2819784944002495]
	TIME [epoch: 3.57 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6456446652833987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6456446652833987 | validation: 0.293891242977702]
	TIME [epoch: 3.58 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5437173447716108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5437173447716108 | validation: 0.2708399273043252]
	TIME [epoch: 3.59 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5086528484400994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5086528484400994 | validation: 0.21959596494907663]
	TIME [epoch: 3.59 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45856355955502426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45856355955502426 | validation: 0.2659435824622765]
	TIME [epoch: 3.58 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4100897764019844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4100897764019844 | validation: 0.22755278297246806]
	TIME [epoch: 3.59 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4635563994266576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4635563994266576 | validation: 0.1778812701279332]
	TIME [epoch: 3.58 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31204137197340565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31204137197340565 | validation: 0.18890262507570782]
	TIME [epoch: 3.58 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25257810764894634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25257810764894634 | validation: 0.16343658153284377]
	TIME [epoch: 3.58 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2288867346584555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2288867346584555 | validation: 0.1955025973805924]
	TIME [epoch: 3.59 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19117589106638494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19117589106638494 | validation: 0.19766704284970174]
	TIME [epoch: 3.59 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21011813515188615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21011813515188615 | validation: 0.1943559398973528]
	TIME [epoch: 3.58 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14762861394802573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14762861394802573 | validation: 0.15745794294456328]
	TIME [epoch: 3.58 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12236780473701542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12236780473701542 | validation: 0.23864576960858913]
	TIME [epoch: 3.58 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20290789540022588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20290789540022588 | validation: 0.20930770452468803]
	TIME [epoch: 3.58 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11413371891232649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11413371891232649 | validation: 0.1825375934214004]
	TIME [epoch: 3.59 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08554766093148093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08554766093148093 | validation: 0.17236475721220593]
	TIME [epoch: 3.58 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08350459925942325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08350459925942325 | validation: 0.15056255945306563]
	TIME [epoch: 3.58 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08328476497432809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08328476497432809 | validation: 0.14764342780759915]
	TIME [epoch: 3.57 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07911772017284896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07911772017284896 | validation: 0.1470512790801634]
	TIME [epoch: 3.58 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09090382642935486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09090382642935486 | validation: 0.18458181837841803]
	TIME [epoch: 3.58 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12380452147322697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12380452147322697 | validation: 0.1568626383849694]
	TIME [epoch: 3.57 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15270224571772104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15270224571772104 | validation: 0.1928060828424408]
	TIME [epoch: 3.58 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14285850747727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14285850747727 | validation: 0.1510607517285587]
	TIME [epoch: 3.59 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1199035410489867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1199035410489867 | validation: 0.12793233167511595]
	TIME [epoch: 3.58 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_547.pth
	Model improved!!!
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09122534397509081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09122534397509081 | validation: 0.1385365111982122]
	TIME [epoch: 3.59 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07336008099165504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07336008099165504 | validation: 0.11830262389082709]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_549.pth
	Model improved!!!
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08401832304884828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08401832304884828 | validation: 0.16700933506838284]
	TIME [epoch: 3.59 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0940501422079522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0940501422079522 | validation: 0.1520102690278736]
	TIME [epoch: 3.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11022973029473473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11022973029473473 | validation: 0.16323604763364652]
	TIME [epoch: 3.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10073436532415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10073436532415 | validation: 0.1988954301684296]
	TIME [epoch: 3.59 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12659597068727532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12659597068727532 | validation: 0.15129907020141065]
	TIME [epoch: 3.58 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13745100024377824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13745100024377824 | validation: 0.16445840623616148]
	TIME [epoch: 3.59 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10970548951021901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10970548951021901 | validation: 0.1198808920966799]
	TIME [epoch: 3.58 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08086153652620486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08086153652620486 | validation: 0.12175634012920543]
	TIME [epoch: 3.59 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0687648885732444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0687648885732444 | validation: 0.0960704967127003]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06030512201160694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06030512201160694 | validation: 0.10701598124006205]
	TIME [epoch: 3.59 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05620650149722838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05620650149722838 | validation: 0.08452740497673543]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_560.pth
	Model improved!!!
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05455184359426849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05455184359426849 | validation: 0.09874943156148792]
	TIME [epoch: 3.59 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05569937949604244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05569937949604244 | validation: 0.07724580171388194]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_562.pth
	Model improved!!!
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06553535948926138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06553535948926138 | validation: 0.13953049538761758]
	TIME [epoch: 3.59 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10164398532035225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10164398532035225 | validation: 0.24608242615254544]
	TIME [epoch: 3.59 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1929120041495888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1929120041495888 | validation: 0.22813138553435072]
	TIME [epoch: 3.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18552106830749943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18552106830749943 | validation: 0.12058957566780965]
	TIME [epoch: 3.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061133587123505036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061133587123505036 | validation: 0.1172611270117282]
	TIME [epoch: 3.59 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08712200977900773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08712200977900773 | validation: 0.15808380010476825]
	TIME [epoch: 3.59 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0844097359386619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0844097359386619 | validation: 0.16146972387046254]
	TIME [epoch: 3.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10913429231952329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10913429231952329 | validation: 0.25136471721126086]
	TIME [epoch: 3.59 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1816125246097154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1816125246097154 | validation: 0.17953557287147895]
	TIME [epoch: 3.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12034667339535263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12034667339535263 | validation: 0.1238866389812025]
	TIME [epoch: 3.58 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08336385296720217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08336385296720217 | validation: 0.12005775812473865]
	TIME [epoch: 3.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0633465838465313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0633465838465313 | validation: 0.11029692378565903]
	TIME [epoch: 3.66 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06905612846071138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06905612846071138 | validation: 0.12370515895822379]
	TIME [epoch: 3.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07534096137020041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07534096137020041 | validation: 0.08089556163755547]
	TIME [epoch: 3.59 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06497982252899609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06497982252899609 | validation: 0.0922865855353391]
	TIME [epoch: 3.59 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048396530722698304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048396530722698304 | validation: 0.06976060676375799]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_578.pth
	Model improved!!!
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042088771471182555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042088771471182555 | validation: 0.07838349601156448]
	TIME [epoch: 3.59 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04037580842540235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04037580842540235 | validation: 0.05712034423217896]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044336886125604796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044336886125604796 | validation: 0.12364611381913915]
	TIME [epoch: 3.57 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06911157646653075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06911157646653075 | validation: 0.317694169425136]
	TIME [epoch: 3.57 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22898875595476648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22898875595476648 | validation: 0.31497414555363923]
	TIME [epoch: 3.57 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34553017358527277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34553017358527277 | validation: 0.18259027830226987]
	TIME [epoch: 3.56 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12511350401687135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12511350401687135 | validation: 0.11720310611883922]
	TIME [epoch: 3.56 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07306344729755698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07306344729755698 | validation: 0.135383080054955]
	TIME [epoch: 3.56 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08122012822427095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08122012822427095 | validation: 0.12607784348879775]
	TIME [epoch: 3.56 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05755077853692752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05755077853692752 | validation: 0.11042439856785013]
	TIME [epoch: 3.56 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05397693307462774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05397693307462774 | validation: 0.09683944256670168]
	TIME [epoch: 3.56 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042450645941135276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042450645941135276 | validation: 0.08886334397377832]
	TIME [epoch: 3.57 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03730246644872698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03730246644872698 | validation: 0.08067465312744646]
	TIME [epoch: 3.57 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04016029550956041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04016029550956041 | validation: 0.09418310969749964]
	TIME [epoch: 3.57 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04478787793538344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04478787793538344 | validation: 0.07848838625146243]
	TIME [epoch: 3.58 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06667018096037239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06667018096037239 | validation: 0.15161888146295124]
	TIME [epoch: 3.56 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1309174170403718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1309174170403718 | validation: 0.14064234214306692]
	TIME [epoch: 3.57 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14362195720720183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14362195720720183 | validation: 0.22393988310930563]
	TIME [epoch: 3.56 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15723005223298403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15723005223298403 | validation: 0.20180504813932976]
	TIME [epoch: 3.57 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18705485603188582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18705485603188582 | validation: 0.1411160978220191]
	TIME [epoch: 3.58 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08377224059953452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08377224059953452 | validation: 0.16823023062414763]
	TIME [epoch: 3.58 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09907602997986288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09907602997986288 | validation: 0.10949507978073938]
	TIME [epoch: 3.58 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05088337045196084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05088337045196084 | validation: 0.10724166896880388]
	TIME [epoch: 3.59 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04694515860445291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04694515860445291 | validation: 0.0972910188972369]
	TIME [epoch: 3.59 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045515843896794736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045515843896794736 | validation: 0.09000604511085851]
	TIME [epoch: 3.58 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04226176001397896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04226176001397896 | validation: 0.10145682458449393]
	TIME [epoch: 3.59 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045764252353099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045764252353099 | validation: 0.091947952749846]
	TIME [epoch: 3.59 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0611897836817475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0611897836817475 | validation: 0.15382801541849075]
	TIME [epoch: 3.59 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09868749600790044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09868749600790044 | validation: 0.19573455883240934]
	TIME [epoch: 3.59 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16414616401901624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16414616401901624 | validation: 0.17464589292092503]
	TIME [epoch: 3.59 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1276988030357631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1276988030357631 | validation: 0.1179441143237494]
	TIME [epoch: 3.59 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07122888603073917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07122888603073917 | validation: 0.08413115993550889]
	TIME [epoch: 3.59 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03781527334520423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03781527334520423 | validation: 0.07405502713788877]
	TIME [epoch: 3.59 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03892467434863656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03892467434863656 | validation: 0.07574106856313012]
	TIME [epoch: 3.59 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040063342500309365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040063342500309365 | validation: 0.07851771013841007]
	TIME [epoch: 3.58 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054829046412566536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054829046412566536 | validation: 0.1301542930512755]
	TIME [epoch: 3.59 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10285048457543723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10285048457543723 | validation: 0.13357239085433667]
	TIME [epoch: 3.58 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13665429482850663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13665429482850663 | validation: 0.10775347075965158]
	TIME [epoch: 3.59 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07695709000717399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07695709000717399 | validation: 0.14689012319468706]
	TIME [epoch: 3.59 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10216278615122511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10216278615122511 | validation: 0.16753098267838476]
	TIME [epoch: 3.59 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12166814123686988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12166814123686988 | validation: 0.16712141787630685]
	TIME [epoch: 3.59 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08796481760607751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08796481760607751 | validation: 0.1282293067475153]
	TIME [epoch: 3.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06690402635844178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06690402635844178 | validation: 0.07972439386055322]
	TIME [epoch: 3.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04778353009817903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04778353009817903 | validation: 0.09119509282914082]
	TIME [epoch: 3.59 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03864433109594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03864433109594 | validation: 0.07096021960370741]
	TIME [epoch: 3.59 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041203317765889466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041203317765889466 | validation: 0.07260486162369151]
	TIME [epoch: 3.59 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03899892829089424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03899892829089424 | validation: 0.0627212077191062]
	TIME [epoch: 3.6 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04734139251077167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04734139251077167 | validation: 0.09064313753451853]
	TIME [epoch: 3.58 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06896801499805885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06896801499805885 | validation: 0.12183354096898857]
	TIME [epoch: 3.58 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12027472875448954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12027472875448954 | validation: 0.230314822820116]
	TIME [epoch: 3.58 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17388957362513047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17388957362513047 | validation: 0.182176787283722]
	TIME [epoch: 3.59 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375609992744035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1375609992744035 | validation: 0.1835805909095272]
	TIME [epoch: 3.58 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09802524376552006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09802524376552006 | validation: 0.13814393036242884]
	TIME [epoch: 3.58 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0775092045512112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0775092045512112 | validation: 0.0939212760353696]
	TIME [epoch: 3.58 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05000563138103914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05000563138103914 | validation: 0.1153229834653356]
	TIME [epoch: 3.58 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054035020605871814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054035020605871814 | validation: 0.06848128220268232]
	TIME [epoch: 3.59 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04802920398298612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04802920398298612 | validation: 0.0890143956578116]
	TIME [epoch: 3.6 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039699925127540205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039699925127540205 | validation: 0.07056129496559736]
	TIME [epoch: 3.59 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03792115101050929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03792115101050929 | validation: 0.07524514468420042]
	TIME [epoch: 3.58 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04575047379764138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04575047379764138 | validation: 0.06847872421319423]
	TIME [epoch: 3.58 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05915866610430638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05915866610430638 | validation: 0.09766680587280024]
	TIME [epoch: 3.57 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0772263907373467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0772263907373467 | validation: 0.0944043053382095]
	TIME [epoch: 3.58 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08735351265111174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08735351265111174 | validation: 0.15927158223344753]
	TIME [epoch: 3.58 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12756557232044155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12756557232044155 | validation: 0.20939926446627166]
	TIME [epoch: 3.58 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17297988500929357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17297988500929357 | validation: 0.24979596901882017]
	TIME [epoch: 3.58 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1689979262071197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1689979262071197 | validation: 0.14647479065915658]
	TIME [epoch: 3.59 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08775705790460382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08775705790460382 | validation: 0.12453525568797355]
	TIME [epoch: 3.58 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06455581338950368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06455581338950368 | validation: 0.1352666725684682]
	TIME [epoch: 3.58 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06065822712680628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06065822712680628 | validation: 0.08698041414711695]
	TIME [epoch: 3.58 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04219934059504649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04219934059504649 | validation: 0.08562592477506237]
	TIME [epoch: 3.59 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04176715736660435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04176715736660435 | validation: 0.0693304533036913]
	TIME [epoch: 3.59 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0375574838945414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0375574838945414 | validation: 0.06707687709997509]
	TIME [epoch: 3.58 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036654178030043644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036654178030043644 | validation: 0.0510048513504467]
	TIME [epoch: 3.58 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03618006078457917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03618006078457917 | validation: 0.07046484038934894]
	TIME [epoch: 3.59 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04490720515410278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04490720515410278 | validation: 0.07789421508543853]
	TIME [epoch: 3.58 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07702286057574824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07702286057574824 | validation: 0.1332427748355864]
	TIME [epoch: 3.59 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12462907266569374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12462907266569374 | validation: 0.1972501444530621]
	TIME [epoch: 3.58 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14741068888959322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14741068888959322 | validation: 0.17597104200728977]
	TIME [epoch: 3.58 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1197103043682236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1197103043682236 | validation: 0.12003307416288583]
	TIME [epoch: 3.59 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06885284705536494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06885284705536494 | validation: 0.08547272058241727]
	TIME [epoch: 3.59 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06355018616585983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06355018616585983 | validation: 0.09966978617926754]
	TIME [epoch: 3.59 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06071320342910012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06071320342910012 | validation: 0.07088278080308047]
	TIME [epoch: 3.59 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042443222073041846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042443222073041846 | validation: 0.06361341699570093]
	TIME [epoch: 3.58 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030014763771231554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030014763771231554 | validation: 0.05035842664584331]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029831634872739904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029831634872739904 | validation: 0.07729277966513973]
	TIME [epoch: 3.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029873257585304198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029873257585304198 | validation: 0.04701964592970971]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_664.pth
	Model improved!!!
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03426937983748502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03426937983748502 | validation: 0.0875808015995803]
	TIME [epoch: 3.58 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053470392948483346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053470392948483346 | validation: 0.18479885544081662]
	TIME [epoch: 3.59 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.141514667795273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.141514667795273 | validation: 0.23987150652543485]
	TIME [epoch: 3.59 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20137349427330126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20137349427330126 | validation: 0.1548977047506714]
	TIME [epoch: 3.58 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12371476867205002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12371476867205002 | validation: 0.09339068427816066]
	TIME [epoch: 3.59 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0872376410972695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0872376410972695 | validation: 0.102125793539778]
	TIME [epoch: 3.58 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04573453988965971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04573453988965971 | validation: 0.08372581016599322]
	TIME [epoch: 3.57 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042234922451430845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042234922451430845 | validation: 0.08399342254224562]
	TIME [epoch: 3.58 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043457886231408116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043457886231408116 | validation: 0.06662841454059051]
	TIME [epoch: 3.58 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03887580227857565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03887580227857565 | validation: 0.062026505047320546]
	TIME [epoch: 3.58 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03522968010686826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03522968010686826 | validation: 0.05978015807235218]
	TIME [epoch: 3.58 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03368636554884576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03368636554884576 | validation: 0.05383938179691277]
	TIME [epoch: 3.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03623935256944619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03623935256944619 | validation: 0.07015350463722422]
	TIME [epoch: 3.59 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056249178173568525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056249178173568525 | validation: 0.14507486291946475]
	TIME [epoch: 3.58 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11871200325048191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11871200325048191 | validation: 0.27742238877624487]
	TIME [epoch: 3.58 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20439908906157178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20439908906157178 | validation: 0.15445567039743355]
	TIME [epoch: 3.58 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1027437746394011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1027437746394011 | validation: 0.10385325828801101]
	TIME [epoch: 3.58 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0538726950543869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0538726950543869 | validation: 0.08792048380507361]
	TIME [epoch: 3.58 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08139355428710357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08139355428710357 | validation: 0.11755526870598383]
	TIME [epoch: 3.59 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06621309969683195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06621309969683195 | validation: 0.09800453928720085]
	TIME [epoch: 3.58 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04931888717447855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04931888717447855 | validation: 0.07225164099369333]
	TIME [epoch: 3.58 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036961670055533684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036961670055533684 | validation: 0.06662516221652486]
	TIME [epoch: 3.57 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03107137078853782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03107137078853782 | validation: 0.06948336623360328]
	TIME [epoch: 3.59 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030413707483809792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030413707483809792 | validation: 0.04223422451310293]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_688.pth
	Model improved!!!
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03207801688869864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03207801688869864 | validation: 0.06012714864294188]
	TIME [epoch: 3.6 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037444917742189915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037444917742189915 | validation: 0.040777497843415514]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_690.pth
	Model improved!!!
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053362932785508924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053362932785508924 | validation: 0.09063049526919405]
	TIME [epoch: 3.58 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07558250887877595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07558250887877595 | validation: 0.06159086326627403]
	TIME [epoch: 3.59 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08462325448559042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08462325448559042 | validation: 0.11422665963635176]
	TIME [epoch: 3.59 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06645243314016275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06645243314016275 | validation: 0.1813769564301138]
	TIME [epoch: 3.58 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12751077068926106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12751077068926106 | validation: 0.2823298001952842]
	TIME [epoch: 3.59 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22458458747015617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22458458747015617 | validation: 0.16878692538841622]
	TIME [epoch: 3.58 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12274145630504225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12274145630504225 | validation: 0.07319768019192087]
	TIME [epoch: 3.58 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03813189199317721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03813189199317721 | validation: 0.1280863261248876]
	TIME [epoch: 3.57 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05950893487585592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05950893487585592 | validation: 0.10137542752754776]
	TIME [epoch: 3.59 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04423201335495973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04423201335495973 | validation: 0.05471857448899378]
	TIME [epoch: 3.57 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036565504472824854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036565504472824854 | validation: 0.08562467994386277]
	TIME [epoch: 3.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033242126765205035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033242126765205035 | validation: 0.06451417335092106]
	TIME [epoch: 3.59 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03221411789157471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03221411789157471 | validation: 0.060736628673688155]
	TIME [epoch: 3.58 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04111242448335517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04111242448335517 | validation: 0.09047827985278138]
	TIME [epoch: 3.58 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06570180128531689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06570180128531689 | validation: 0.09160227734939831]
	TIME [epoch: 3.58 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07518673910630358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07518673910630358 | validation: 0.06583570113912487]
	TIME [epoch: 3.59 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052865968364384223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052865968364384223 | validation: 0.09625168799784539]
	TIME [epoch: 3.58 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05552252453061559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05552252453061559 | validation: 0.0933577717562315]
	TIME [epoch: 3.59 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08081479295925188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08081479295925188 | validation: 0.1367341204707852]
	TIME [epoch: 3.58 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09578115426194841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09578115426194841 | validation: 0.17525605159276791]
	TIME [epoch: 3.58 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11139230828515562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11139230828515562 | validation: 0.1300274094322713]
	TIME [epoch: 3.58 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07292843864237394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07292843864237394 | validation: 0.05905798272331533]
	TIME [epoch: 3.58 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03098199415975996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03098199415975996 | validation: 0.07290728875887408]
	TIME [epoch: 3.58 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031896190686068804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031896190686068804 | validation: 0.049427192426284176]
	TIME [epoch: 3.59 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036591879709269054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036591879709269054 | validation: 0.07424184371144572]
	TIME [epoch: 3.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0434075522747394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0434075522747394 | validation: 0.04406534842685398]
	TIME [epoch: 3.58 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0518082865175468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0518082865175468 | validation: 0.07892638871833839]
	TIME [epoch: 3.58 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05937896211632392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05937896211632392 | validation: 0.05513850469056558]
	TIME [epoch: 3.59 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0530245038706453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0530245038706453 | validation: 0.07334218848721173]
	TIME [epoch: 3.58 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04450308172027091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04450308172027091 | validation: 0.06714127861215083]
	TIME [epoch: 3.58 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05114909369386652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05114909369386652 | validation: 0.19466704974586324]
	TIME [epoch: 3.58 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13684413436406112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13684413436406112 | validation: 0.2746816534730406]
	TIME [epoch: 3.58 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19099961650070618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19099961650070618 | validation: 0.10818062577107193]
	TIME [epoch: 3.58 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07879379148000278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07879379148000278 | validation: 0.10265978053176782]
	TIME [epoch: 3.58 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043172309303059556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043172309303059556 | validation: 0.07725732072019345]
	TIME [epoch: 3.58 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03684720693212068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03684720693212068 | validation: 0.07534947032013468]
	TIME [epoch: 3.58 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032340989987531064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032340989987531064 | validation: 0.08180942993132978]
	TIME [epoch: 3.58 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03206466230635334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03206466230635334 | validation: 0.058964973015389326]
	TIME [epoch: 3.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035363133723970624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035363133723970624 | validation: 0.06719912595403722]
	TIME [epoch: 3.58 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0435546160657416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0435546160657416 | validation: 0.10550274575554884]
	TIME [epoch: 3.58 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.069638307064604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.069638307064604 | validation: 0.11755804517088855]
	TIME [epoch: 3.58 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10224584329614611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10224584329614611 | validation: 0.12469756572525204]
	TIME [epoch: 3.58 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08985619114705827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08985619114705827 | validation: 0.09981393456093407]
	TIME [epoch: 3.58 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08116272601093526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08116272601093526 | validation: 0.14936075238396257]
	TIME [epoch: 3.58 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08733038169954661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08733038169954661 | validation: 0.12905995056226477]
	TIME [epoch: 3.58 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09299372326186071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09299372326186071 | validation: 0.09468907609491617]
	TIME [epoch: 3.58 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07375790227820402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07375790227820402 | validation: 0.06454945299768133]
	TIME [epoch: 3.58 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035982438633415964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035982438633415964 | validation: 0.05985480643572734]
	TIME [epoch: 3.58 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026017616351162762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026017616351162762 | validation: 0.04847039139619298]
	TIME [epoch: 3.58 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02635040584078241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02635040584078241 | validation: 0.060107478828304]
	TIME [epoch: 3.58 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028163022496540253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028163022496540253 | validation: 0.036723090878991636]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_741.pth
	Model improved!!!
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027860752586923113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027860752586923113 | validation: 0.08128578507612332]
	TIME [epoch: 3.6 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03748080634624001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03748080634624001 | validation: 0.06889155381836687]
	TIME [epoch: 3.61 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06140445232265769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06140445232265769 | validation: 0.18077680867066503]
	TIME [epoch: 3.58 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13536547758946632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13536547758946632 | validation: 0.26502617904380205]
	TIME [epoch: 3.58 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21190141221898137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21190141221898137 | validation: 0.10386976487246663]
	TIME [epoch: 3.58 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06458318697633299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06458318697633299 | validation: 0.09985868574454183]
	TIME [epoch: 3.58 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046639777536058864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046639777536058864 | validation: 0.10108382322172747]
	TIME [epoch: 3.59 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052549712774316054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052549712774316054 | validation: 0.07263211367728431]
	TIME [epoch: 3.58 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055657253858525414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055657253858525414 | validation: 0.0816566545857612]
	TIME [epoch: 3.58 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047001558097453454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047001558097453454 | validation: 0.06217258583750767]
	TIME [epoch: 3.58 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04688606486827169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04688606486827169 | validation: 0.06766858757459747]
	TIME [epoch: 3.59 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0475243026742881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0475243026742881 | validation: 0.08668343996841428]
	TIME [epoch: 3.58 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040744684239496946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040744684239496946 | validation: 0.054873988272546116]
	TIME [epoch: 3.6 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0348123582394516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0348123582394516 | validation: 0.05608923087880108]
	TIME [epoch: 3.6 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030459536487495012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030459536487495012 | validation: 0.0480103181089914]
	TIME [epoch: 3.59 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039425604048170956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039425604048170956 | validation: 0.09151090511135296]
	TIME [epoch: 3.59 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056830491755364976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056830491755364976 | validation: 0.10166367891020066]
	TIME [epoch: 3.59 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09096588706127268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09096588706127268 | validation: 0.13657000812862752]
	TIME [epoch: 3.59 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10320571398857624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10320571398857624 | validation: 0.20607282263305865]
	TIME [epoch: 3.58 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14663374702642454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14663374702642454 | validation: 0.14929254366623626]
	TIME [epoch: 3.58 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07114821225831157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07114821225831157 | validation: 0.08688365579633682]
	TIME [epoch: 3.58 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04954959965331263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04954959965331263 | validation: 0.09344346231987644]
	TIME [epoch: 3.58 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06065924598110382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06065924598110382 | validation: 0.08298615498615153]
	TIME [epoch: 3.58 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05454084704144321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05454084704144321 | validation: 0.06592382726060274]
	TIME [epoch: 3.58 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03115402872954776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03115402872954776 | validation: 0.04049079852197162]
	TIME [epoch: 3.59 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024755441645642237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024755441645642237 | validation: 0.05478089020537638]
	TIME [epoch: 3.6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02215351543246671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02215351543246671 | validation: 0.0441581466643991]
	TIME [epoch: 3.59 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023924997680680522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023924997680680522 | validation: 0.043647787656452365]
	TIME [epoch: 3.58 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02706367866831885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02706367866831885 | validation: 0.060803657572792214]
	TIME [epoch: 3.59 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03385892813700123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03385892813700123 | validation: 0.055929150309542236]
	TIME [epoch: 3.58 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05194120507879913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05194120507879913 | validation: 0.07523075194559453]
	TIME [epoch: 3.59 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07137392266973025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07137392266973025 | validation: 0.06970054560840436]
	TIME [epoch: 3.58 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07273465591551888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07273465591551888 | validation: 0.16570527681180458]
	TIME [epoch: 3.59 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15839540894095921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15839540894095921 | validation: 0.18170110575345141]
	TIME [epoch: 3.58 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12199557185005108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12199557185005108 | validation: 0.12819098553905078]
	TIME [epoch: 3.58 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06881571728297066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06881571728297066 | validation: 0.09772447318954933]
	TIME [epoch: 3.58 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04129688802358798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04129688802358798 | validation: 0.06408415561974759]
	TIME [epoch: 3.58 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03462266745184575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03462266745184575 | validation: 0.08429449588039231]
	TIME [epoch: 3.58 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03829098236748644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03829098236748644 | validation: 0.08850561249235732]
	TIME [epoch: 3.59 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060714023964042935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060714023964042935 | validation: 0.13346493407779497]
	TIME [epoch: 3.59 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0875042202809691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0875042202809691 | validation: 0.09368763748087605]
	TIME [epoch: 3.58 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09102098210434095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09102098210434095 | validation: 0.12897332565090605]
	TIME [epoch: 3.58 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08682597509709482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08682597509709482 | validation: 0.09130954225723761]
	TIME [epoch: 3.58 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07325285176223868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07325285176223868 | validation: 0.0836462183565473]
	TIME [epoch: 3.58 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04350777749006765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04350777749006765 | validation: 0.07501485159987063]
	TIME [epoch: 3.58 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028989926135993032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028989926135993032 | validation: 0.049427535030671745]
	TIME [epoch: 3.59 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02495946051863231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02495946051863231 | validation: 0.04790343302406759]
	TIME [epoch: 3.58 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022065518604073516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022065518604073516 | validation: 0.05796650840452316]
	TIME [epoch: 3.59 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02286124230164815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02286124230164815 | validation: 0.0377545669610741]
	TIME [epoch: 3.58 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02616938929577084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02616938929577084 | validation: 0.09430714711630112]
	TIME [epoch: 3.58 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04123501679145023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04123501679145023 | validation: 0.10970917955552893]
	TIME [epoch: 3.58 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09736063292515518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09736063292515518 | validation: 0.20071875261420927]
	TIME [epoch: 3.59 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15445513418589407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15445513418589407 | validation: 0.1823442558584767]
	TIME [epoch: 3.59 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13856975496521012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13856975496521012 | validation: 0.07309111665445077]
	TIME [epoch: 3.58 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07152615932449626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07152615932449626 | validation: 0.08215201739085681]
	TIME [epoch: 3.58 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032671726637744314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032671726637744314 | validation: 0.09147691897860835]
	TIME [epoch: 3.58 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05149620215451601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05149620215451601 | validation: 0.09714413343389018]
	TIME [epoch: 3.58 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0563180974757897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0563180974757897 | validation: 0.06965540612294788]
	TIME [epoch: 3.58 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06188623257773388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06188623257773388 | validation: 0.10389267711502014]
	TIME [epoch: 3.58 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051474258667683674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051474258667683674 | validation: 0.034796237324186366]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_801.pth
	Model improved!!!
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03868092727391401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03868092727391401 | validation: 0.07975741084644147]
	TIME [epoch: 3.59 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03133678178632934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03133678178632934 | validation: 0.055700902573763936]
	TIME [epoch: 3.59 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03536663425437047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03536663425437047 | validation: 0.10453304278256136]
	TIME [epoch: 3.59 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05258269965972663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05258269965972663 | validation: 0.11345062233309022]
	TIME [epoch: 3.59 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09300266811545749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09300266811545749 | validation: 0.1419973661006071]
	TIME [epoch: 3.6 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0978743413405339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0978743413405339 | validation: 0.08523711850460819]
	TIME [epoch: 3.6 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06813389943023287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06813389943023287 | validation: 0.06905751920400116]
	TIME [epoch: 3.59 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028977862888038386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028977862888038386 | validation: 0.07537860435919197]
	TIME [epoch: 3.59 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02445369798744422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02445369798744422 | validation: 0.04129615611519932]
	TIME [epoch: 3.6 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026926315340668996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026926315340668996 | validation: 0.060986164373728606]
	TIME [epoch: 3.59 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025514786687813863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025514786687813863 | validation: 0.05990061171517483]
	TIME [epoch: 3.59 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0404916165722974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0404916165722974 | validation: 0.087157853372969]
	TIME [epoch: 3.59 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08028927617851983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08028927617851983 | validation: 0.0890337679859412]
	TIME [epoch: 3.59 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07932938754780663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07932938754780663 | validation: 0.07713238153784707]
	TIME [epoch: 3.59 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04258228399970948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04258228399970948 | validation: 0.04815837371033524]
	TIME [epoch: 3.58 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04555526480362192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04555526480362192 | validation: 0.1263438473187514]
	TIME [epoch: 3.59 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08340424176155294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08340424176155294 | validation: 0.10089530990552215]
	TIME [epoch: 3.59 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06789847392539826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06789847392539826 | validation: 0.17451129643747068]
	TIME [epoch: 3.6 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10411978081807872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10411978081807872 | validation: 0.12265690557154768]
	TIME [epoch: 3.59 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08608528460611668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08608528460611668 | validation: 0.08877937061114236]
	TIME [epoch: 3.59 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04267121207249221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04267121207249221 | validation: 0.0554112386060326]
	TIME [epoch: 3.59 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027683863744311487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027683863744311487 | validation: 0.06927395715285199]
	TIME [epoch: 3.59 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02243786451990821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02243786451990821 | validation: 0.04232414477141666]
	TIME [epoch: 3.59 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017443240235587604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017443240235587604 | validation: 0.04362154911485695]
	TIME [epoch: 3.58 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016497729112070536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016497729112070536 | validation: 0.055837077104112745]
	TIME [epoch: 3.59 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01999898575318601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01999898575318601 | validation: 0.02700102141399584]
	TIME [epoch: 3.58 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_827.pth
	Model improved!!!
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02764458963276462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02764458963276462 | validation: 0.12049591465448678]
	TIME [epoch: 3.59 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07442590458052374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07442590458052374 | validation: 0.21752647686512805]
	TIME [epoch: 3.59 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20338847418674882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20338847418674882 | validation: 0.21428278145354418]
	TIME [epoch: 3.59 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14408665619441366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14408665619441366 | validation: 0.14087919822361886]
	TIME [epoch: 3.59 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06466039143900833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06466039143900833 | validation: 0.06601417953702926]
	TIME [epoch: 3.6 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03902809407196491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03902809407196491 | validation: 0.09149771102785573]
	TIME [epoch: 3.6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03312047833955036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03312047833955036 | validation: 0.08918102989716725]
	TIME [epoch: 3.59 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03372813721481534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03372813721481534 | validation: 0.06619186345416074]
	TIME [epoch: 3.59 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029603019850638287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029603019850638287 | validation: 0.06005955450818971]
	TIME [epoch: 3.59 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031564828263368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031564828263368 | validation: 0.08178767390857787]
	TIME [epoch: 3.58 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04391656314644342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04391656314644342 | validation: 0.07764925787625034]
	TIME [epoch: 3.59 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05655095239518742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05655095239518742 | validation: 0.07091901709314706]
	TIME [epoch: 3.58 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06345625968528344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06345625968528344 | validation: 0.12865682476763596]
	TIME [epoch: 3.59 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09057048636580257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09057048636580257 | validation: 0.1368672978693733]
	TIME [epoch: 3.58 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12012157395285997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12012157395285997 | validation: 0.12373850531839595]
	TIME [epoch: 3.58 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07135443661139748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07135443661139748 | validation: 0.06807180039175768]
	TIME [epoch: 3.58 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03526262787053689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03526262787053689 | validation: 0.059974509462195735]
	TIME [epoch: 3.59 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03464035858363696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03464035858363696 | validation: 0.07935794280208358]
	TIME [epoch: 3.59 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03168988571514661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03168988571514661 | validation: 0.06309571932378959]
	TIME [epoch: 3.6 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02804874947907712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02804874947907712 | validation: 0.0469059410606191]
	TIME [epoch: 3.59 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0304729340828471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0304729340828471 | validation: 0.07178306621536394]
	TIME [epoch: 3.59 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03042590320900817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03042590320900817 | validation: 0.038741563324609735]
	TIME [epoch: 3.6 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02706574317834279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02706574317834279 | validation: 0.05256475038035945]
	TIME [epoch: 3.59 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02919456362319477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02919456362319477 | validation: 0.0636530384799608]
	TIME [epoch: 3.58 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03677472649773565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03677472649773565 | validation: 0.08201105619700329]
	TIME [epoch: 3.59 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0576249108180634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0576249108180634 | validation: 0.10400849625350253]
	TIME [epoch: 3.59 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09231700363805125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09231700363805125 | validation: 0.166536710719374]
	TIME [epoch: 3.59 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14723340990133385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14723340990133385 | validation: 0.2814735750331126]
	TIME [epoch: 3.59 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20888014011289235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20888014011289235 | validation: 0.0897274593304531]
	TIME [epoch: 3.59 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03563521369598818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03563521369598818 | validation: 0.11081881068449784]
	TIME [epoch: 3.58 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049644390041786654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049644390041786654 | validation: 0.0635035175132008]
	TIME [epoch: 3.59 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04076748235542427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04076748235542427 | validation: 0.06070582868646105]
	TIME [epoch: 3.6 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024926396677116953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024926396677116953 | validation: 0.07645949137739101]
	TIME [epoch: 3.59 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02273316259163546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02273316259163546 | validation: 0.06845653601438904]
	TIME [epoch: 3.58 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02360150856648719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02360150856648719 | validation: 0.04465642786341265]
	TIME [epoch: 3.59 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033887924637966194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033887924637966194 | validation: 0.0844915310480738]
	TIME [epoch: 3.59 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04708657703282717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04708657703282717 | validation: 0.057925924860446856]
	TIME [epoch: 3.59 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05685509129306686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05685509129306686 | validation: 0.0832086142903275]
	TIME [epoch: 3.59 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04476161054672611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04476161054672611 | validation: 0.058242591127265486]
	TIME [epoch: 3.58 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03388420396954696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03388420396954696 | validation: 0.0792358438444782]
	TIME [epoch: 3.58 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04278330073489701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04278330073489701 | validation: 0.0946722889327896]
	TIME [epoch: 3.59 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08329355809519555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08329355809519555 | validation: 0.17682800616964522]
	TIME [epoch: 3.58 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1305329813526111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1305329813526111 | validation: 0.1033108739332112]
	TIME [epoch: 3.58 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08001419222673363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08001419222673363 | validation: 0.06633936269118637]
	TIME [epoch: 3.59 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02611670024476449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02611670024476449 | validation: 0.07657654893892182]
	TIME [epoch: 3.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028531302075842185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028531302075842185 | validation: 0.048967251643081924]
	TIME [epoch: 3.58 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03861520361897342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03861520361897342 | validation: 0.0809929255516932]
	TIME [epoch: 3.58 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04079933704085942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04079933704085942 | validation: 0.06187553199180003]
	TIME [epoch: 3.58 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042901657084357885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042901657084357885 | validation: 0.06190621353706628]
	TIME [epoch: 3.58 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04115862180854988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04115862180854988 | validation: 0.040377013827368025]
	TIME [epoch: 3.59 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03233307136557714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03233307136557714 | validation: 0.0464914906410798]
	TIME [epoch: 3.58 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02483069673841201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02483069673841201 | validation: 0.03781172096363318]
	TIME [epoch: 3.58 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02946388617422165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02946388617422165 | validation: 0.12104803150375193]
	TIME [epoch: 3.58 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06405038279671346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06405038279671346 | validation: 0.18655659269211633]
	TIME [epoch: 3.58 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14567442270954786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14567442270954786 | validation: 0.14241777149165835]
	TIME [epoch: 3.59 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08793910405726545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08793910405726545 | validation: 0.09338883850398763]
	TIME [epoch: 3.58 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05565323719740523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05565323719740523 | validation: 0.06037233313940857]
	TIME [epoch: 3.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06246077431311229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06246077431311229 | validation: 0.07239899621080732]
	TIME [epoch: 3.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027836763427450596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027836763427450596 | validation: 0.0731442016386681]
	TIME [epoch: 3.59 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024808596431651102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024808596431651102 | validation: 0.04760929115819127]
	TIME [epoch: 3.58 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027867333837791643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027867333837791643 | validation: 0.06759025745593848]
	TIME [epoch: 3.59 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028116384838710217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028116384838710217 | validation: 0.07595805667376843]
	TIME [epoch: 3.58 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0419423277894679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0419423277894679 | validation: 0.061167416208526795]
	TIME [epoch: 3.58 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06250370975020282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06250370975020282 | validation: 0.10093517679948719]
	TIME [epoch: 3.57 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06369663738872682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06369663738872682 | validation: 0.07065389077976829]
	TIME [epoch: 3.58 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0631116905148345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0631116905148345 | validation: 0.13537803271731894]
	TIME [epoch: 3.57 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08635548402703357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08635548402703357 | validation: 0.11127259185238177]
	TIME [epoch: 3.58 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09452088628386558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09452088628386558 | validation: 0.08726475614865227]
	TIME [epoch: 3.58 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0535817052174027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0535817052174027 | validation: 0.044102973830769665]
	TIME [epoch: 3.58 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026442210187378263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026442210187378263 | validation: 0.04723889004234752]
	TIME [epoch: 3.57 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01963086822174162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01963086822174162 | validation: 0.0570520175731564]
	TIME [epoch: 3.6 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01968939973948413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01968939973948413 | validation: 0.03605287404344498]
	TIME [epoch: 3.6 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020503214341186623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020503214341186623 | validation: 0.04530353242103782]
	TIME [epoch: 3.59 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024345940749837338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024345940749837338 | validation: 0.04624159071997611]
	TIME [epoch: 3.58 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034561931350101355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034561931350101355 | validation: 0.08088899443461853]
	TIME [epoch: 3.57 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05565381444890028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05565381444890028 | validation: 0.11848323339059452]
	TIME [epoch: 3.58 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11295991648183197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11295991648183197 | validation: 0.18699641867297967]
	TIME [epoch: 3.58 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1602573379901078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1602573379901078 | validation: 0.06552847195148119]
	TIME [epoch: 3.58 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0477292316222239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0477292316222239 | validation: 0.05837480197775089]
	TIME [epoch: 3.58 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03649641219799129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03649641219799129 | validation: 0.09076272718740146]
	TIME [epoch: 3.59 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0373070682935305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0373070682935305 | validation: 0.07489067814676552]
	TIME [epoch: 3.58 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03139442735354156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03139442735354156 | validation: 0.050735344102602734]
	TIME [epoch: 3.59 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03984510274963772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03984510274963772 | validation: 0.07629830366132004]
	TIME [epoch: 3.58 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039586705549416956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039586705549416956 | validation: 0.04838652032667023]
	TIME [epoch: 3.58 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03556091427791778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03556091427791778 | validation: 0.042476839653908895]
	TIME [epoch: 3.59 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03615160821992072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03615160821992072 | validation: 0.09826307323841027]
	TIME [epoch: 3.58 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05065330366858843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05065330366858843 | validation: 0.07654972756754669]
	TIME [epoch: 3.59 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06709217148302267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06709217148302267 | validation: 0.12891353406339623]
	TIME [epoch: 3.58 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07360571397479312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07360571397479312 | validation: 0.0911267039609049]
	TIME [epoch: 3.58 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0743498881610457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0743498881610457 | validation: 0.08050951748912699]
	TIME [epoch: 3.58 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06498573433727478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06498573433727478 | validation: 0.06882930622514112]
	TIME [epoch: 3.58 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0352704032013867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0352704032013867 | validation: 0.0573962124433629]
	TIME [epoch: 3.59 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029276645866548334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029276645866548334 | validation: 0.045149898062953775]
	TIME [epoch: 3.59 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04390540432423226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04390540432423226 | validation: 0.07470512351335581]
	TIME [epoch: 3.58 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044089528415498354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044089528415498354 | validation: 0.02940790122920868]
	TIME [epoch: 3.58 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03099242425886507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03099242425886507 | validation: 0.06606065889159186]
	TIME [epoch: 3.58 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030639212360340937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030639212360340937 | validation: 0.057426744179631166]
	TIME [epoch: 3.59 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044930173678838656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044930173678838656 | validation: 0.1239298893549724]
	TIME [epoch: 3.59 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07325774505194105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07325774505194105 | validation: 0.10485690086966515]
	TIME [epoch: 3.58 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0897357864342862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0897357864342862 | validation: 0.11027228019637927]
	TIME [epoch: 3.57 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0622540394946604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0622540394946604 | validation: 0.04024975588568788]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_175457/states/model_phi1_4a_v_mmd1_928.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2474.838 seconds.
