Args:
Namespace(name='model_phiq_1a_v_mmd5', outdir='out/model_training/model_phiq_1a_v_mmd5', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.1, weight_decay=0.9, clip=1.0, lr_schedule='warmup_cosine_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2502511755

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_0.pth
EPOCH 1/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 4.7108328363731555		[learning rate: 0.01015]
	Learning Rate: 0.01015
	LOSS [training: 4.7108328363731555 | validation: 4.791796583018315]
	TIME [epoch: 162 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.676879337099972		[learning rate: 0.01035]
	Learning Rate: 0.01035
	LOSS [training: 4.676879337099972 | validation: 4.387201835659107]
	TIME [epoch: 9.84 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.537240988011714		[learning rate: 0.01055]
	Learning Rate: 0.01055
	LOSS [training: 4.537240988011714 | validation: 4.409618611703017]
	TIME [epoch: 9.72 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.411990042473452		[learning rate: 0.01075]
	Learning Rate: 0.01075
	LOSS [training: 4.411990042473452 | validation: 4.376761311510018]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.453589060662142		[learning rate: 0.01095]
	Learning Rate: 0.01095
	LOSS [training: 4.453589060662142 | validation: 4.603032309051956]
	TIME [epoch: 9.7 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4691257342137725		[learning rate: 0.01115]
	Learning Rate: 0.01115
	LOSS [training: 4.4691257342137725 | validation: 4.319545013047959]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2889766131764615		[learning rate: 0.01135]
	Learning Rate: 0.01135
	LOSS [training: 4.2889766131764615 | validation: 4.098134162669192]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.159246446911968		[learning rate: 0.01155]
	Learning Rate: 0.01155
	LOSS [training: 4.159246446911968 | validation: 3.985026597947285]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.088048608921207		[learning rate: 0.01175]
	Learning Rate: 0.01175
	LOSS [training: 4.088048608921207 | validation: 3.9187255977533555]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.049296517785862		[learning rate: 0.01195]
	Learning Rate: 0.01195
	LOSS [training: 4.049296517785862 | validation: 3.828922911231719]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9254575584583615		[learning rate: 0.01215]
	Learning Rate: 0.01215
	LOSS [training: 3.9254575584583615 | validation: 3.740008512811267]
	TIME [epoch: 9.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7845913711344803		[learning rate: 0.01235]
	Learning Rate: 0.01235
	LOSS [training: 3.7845913711344803 | validation: 3.8411438355541447]
	TIME [epoch: 9.73 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7704280841058213		[learning rate: 0.01255]
	Learning Rate: 0.01255
	LOSS [training: 3.7704280841058213 | validation: 3.728983836391534]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.635160175740702		[learning rate: 0.01275]
	Learning Rate: 0.01275
	LOSS [training: 3.635160175740702 | validation: 3.49545545659278]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4793708041880294		[learning rate: 0.01295]
	Learning Rate: 0.01295
	LOSS [training: 3.4793708041880294 | validation: 3.49661046368924]
	TIME [epoch: 9.74 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.438824617597374		[learning rate: 0.01315]
	Learning Rate: 0.01315
	LOSS [training: 3.438824617597374 | validation: 3.423376720517608]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2530390642341986		[learning rate: 0.01335]
	Learning Rate: 0.01335
	LOSS [training: 3.2530390642341986 | validation: 3.4974405084019793]
	TIME [epoch: 9.72 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2531753292541685		[learning rate: 0.01355]
	Learning Rate: 0.01355
	LOSS [training: 3.2531753292541685 | validation: 3.201447497963142]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.033204969310251		[learning rate: 0.01375]
	Learning Rate: 0.01375
	LOSS [training: 3.033204969310251 | validation: 3.0678636588096238]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9912918500029217		[learning rate: 0.01395]
	Learning Rate: 0.01395
	LOSS [training: 2.9912918500029217 | validation: 3.027297497422402]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.921540029845075		[learning rate: 0.01415]
	Learning Rate: 0.01415
	LOSS [training: 2.921540029845075 | validation: 2.934559295808712]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8162881843178083		[learning rate: 0.01435]
	Learning Rate: 0.01435
	LOSS [training: 2.8162881843178083 | validation: 2.8405986563568915]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7338508003558024		[learning rate: 0.01455]
	Learning Rate: 0.01455
	LOSS [training: 2.7338508003558024 | validation: 2.7736980257435375]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.655061404935004		[learning rate: 0.01475]
	Learning Rate: 0.01475
	LOSS [training: 2.655061404935004 | validation: 2.8125711295777927]
	TIME [epoch: 9.71 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5708413297828967		[learning rate: 0.01495]
	Learning Rate: 0.01495
	LOSS [training: 2.5708413297828967 | validation: 2.6332731010324615]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4670543805566147		[learning rate: 0.01515]
	Learning Rate: 0.01515
	LOSS [training: 2.4670543805566147 | validation: 2.5456833384822066]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3928077513775583		[learning rate: 0.01535]
	Learning Rate: 0.01535
	LOSS [training: 2.3928077513775583 | validation: 2.286966496460761]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.224563493995422		[learning rate: 0.01555]
	Learning Rate: 0.01555
	LOSS [training: 2.224563493995422 | validation: 2.239313737636523]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.185959459122317		[learning rate: 0.01575]
	Learning Rate: 0.01575
	LOSS [training: 2.185959459122317 | validation: 2.165045549707118]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.034665206120991		[learning rate: 0.01595]
	Learning Rate: 0.01595
	LOSS [training: 2.034665206120991 | validation: 2.168942825082606]
	TIME [epoch: 9.75 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0280728492176436		[learning rate: 0.01615]
	Learning Rate: 0.01615
	LOSS [training: 2.0280728492176436 | validation: 1.9883979172020227]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9677960232322456		[learning rate: 0.01635]
	Learning Rate: 0.01635
	LOSS [training: 1.9677960232322456 | validation: 1.9660784172427683]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.879273778979789		[learning rate: 0.01655]
	Learning Rate: 0.01655
	LOSS [training: 1.879273778979789 | validation: 1.97184719334438]
	TIME [epoch: 9.72 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8709307608994123		[learning rate: 0.01675]
	Learning Rate: 0.01675
	LOSS [training: 1.8709307608994123 | validation: 1.9132193822798373]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8609101806265584		[learning rate: 0.01695]
	Learning Rate: 0.01695
	LOSS [training: 1.8609101806265584 | validation: 1.8512757912595235]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.789759438196541		[learning rate: 0.01715]
	Learning Rate: 0.01715
	LOSS [training: 1.789759438196541 | validation: 1.8096411960238323]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.753921501467605		[learning rate: 0.01735]
	Learning Rate: 0.01735
	LOSS [training: 1.753921501467605 | validation: 1.7907682338933157]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7206334878402028		[learning rate: 0.01755]
	Learning Rate: 0.01755
	LOSS [training: 1.7206334878402028 | validation: 1.7552858151501278]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7111110389283055		[learning rate: 0.01775]
	Learning Rate: 0.01775
	LOSS [training: 1.7111110389283055 | validation: 1.7196796683194062]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6760519680142578		[learning rate: 0.01795]
	Learning Rate: 0.01795
	LOSS [training: 1.6760519680142578 | validation: 1.646623428885171]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6243492929711556		[learning rate: 0.01815]
	Learning Rate: 0.01815
	LOSS [training: 1.6243492929711556 | validation: 1.5830089211142946]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6128738061659442		[learning rate: 0.01835]
	Learning Rate: 0.01835
	LOSS [training: 1.6128738061659442 | validation: 1.6220760710945468]
	TIME [epoch: 9.71 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6171975604278912		[learning rate: 0.01855]
	Learning Rate: 0.01855
	LOSS [training: 1.6171975604278912 | validation: 1.5788009831394112]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5729420566503118		[learning rate: 0.01875]
	Learning Rate: 0.01875
	LOSS [training: 1.5729420566503118 | validation: 1.536080584612244]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.545899681379829		[learning rate: 0.01895]
	Learning Rate: 0.01895
	LOSS [training: 1.545899681379829 | validation: 1.5165940601777306]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5158856779872198		[learning rate: 0.01915]
	Learning Rate: 0.01915
	LOSS [training: 1.5158856779872198 | validation: 1.5153681672723591]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5094099489229935		[learning rate: 0.01935]
	Learning Rate: 0.01935
	LOSS [training: 1.5094099489229935 | validation: 1.5152910125197667]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.480799554048323		[learning rate: 0.01955]
	Learning Rate: 0.01955
	LOSS [training: 1.480799554048323 | validation: 1.4825021611532803]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.464844811422421		[learning rate: 0.01975]
	Learning Rate: 0.01975
	LOSS [training: 1.464844811422421 | validation: 1.4699903876877474]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4549906139704736		[learning rate: 0.01995]
	Learning Rate: 0.01995
	LOSS [training: 1.4549906139704736 | validation: 1.415906639356497]
	TIME [epoch: 9.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4393201658425672		[learning rate: 0.02]
	Learning Rate: 0.02
	LOSS [training: 1.4393201658425672 | validation: 1.4029921433330401]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4326154645218026		[learning rate: 0.02]
	Learning Rate: 0.02
	LOSS [training: 1.4326154645218026 | validation: 1.398805849786548]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3874328070949964		[learning rate: 0.02]
	Learning Rate: 0.0199999
	LOSS [training: 1.3874328070949964 | validation: 1.3719351935778095]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.385191294372381		[learning rate: 0.02]
	Learning Rate: 0.0199998
	LOSS [training: 1.385191294372381 | validation: 1.3865046258783496]
	TIME [epoch: 9.71 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.360021176977419		[learning rate: 0.02]
	Learning Rate: 0.0199997
	LOSS [training: 1.360021176977419 | validation: 1.3466724444312879]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3556314199211617		[learning rate: 0.02]
	Learning Rate: 0.0199995
	LOSS [training: 1.3556314199211617 | validation: 1.338276398185837]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3293790276661461		[learning rate: 0.019999]
	Learning Rate: 0.0199994
	LOSS [training: 1.3293790276661461 | validation: 1.3392573940824084]
	TIME [epoch: 9.75 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.325973562067765		[learning rate: 0.019999]
	Learning Rate: 0.0199992
	LOSS [training: 1.325973562067765 | validation: 1.3107652239590104]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3062681927111086		[learning rate: 0.019999]
	Learning Rate: 0.019999
	LOSS [training: 1.3062681927111086 | validation: 1.2989668925961713]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.317570678376779		[learning rate: 0.019999]
	Learning Rate: 0.0199987
	LOSS [training: 1.317570678376779 | validation: 1.2899195729254218]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.306951069476019		[learning rate: 0.019998]
	Learning Rate: 0.0199984
	LOSS [training: 1.306951069476019 | validation: 1.2967398301177537]
	TIME [epoch: 9.75 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2954874259970213		[learning rate: 0.019998]
	Learning Rate: 0.0199981
	LOSS [training: 1.2954874259970213 | validation: 1.2806405222564006]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2926706245239146		[learning rate: 0.019998]
	Learning Rate: 0.0199978
	LOSS [training: 1.2926706245239146 | validation: 1.2966105376450732]
	TIME [epoch: 9.7 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2903992550134689		[learning rate: 0.019997]
	Learning Rate: 0.0199974
	LOSS [training: 1.2903992550134689 | validation: 1.2991590265204076]
	TIME [epoch: 9.69 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2830195686706851		[learning rate: 0.019997]
	Learning Rate: 0.019997
	LOSS [training: 1.2830195686706851 | validation: 1.2739312256801534]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2760027816131703		[learning rate: 0.019997]
	Learning Rate: 0.0199966
	LOSS [training: 1.2760027816131703 | validation: 1.2617043379582396]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.275947265050504		[learning rate: 0.019996]
	Learning Rate: 0.0199962
	LOSS [training: 1.275947265050504 | validation: 1.2910902539285698]
	TIME [epoch: 9.71 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2651042384826394		[learning rate: 0.019996]
	Learning Rate: 0.0199957
	LOSS [training: 1.2651042384826394 | validation: 1.258584333055072]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2533443238101536		[learning rate: 0.019995]
	Learning Rate: 0.0199952
	LOSS [training: 1.2533443238101536 | validation: 1.2939571979525832]
	TIME [epoch: 9.73 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2657001016886147		[learning rate: 0.019995]
	Learning Rate: 0.0199947
	LOSS [training: 1.2657001016886147 | validation: 1.2363851552083935]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2502166152508996		[learning rate: 0.019994]
	Learning Rate: 0.0199941
	LOSS [training: 1.2502166152508996 | validation: 1.2359185113406281]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2455166525545351		[learning rate: 0.019994]
	Learning Rate: 0.0199935
	LOSS [training: 1.2455166525545351 | validation: 1.2181929563842044]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.236360177994663		[learning rate: 0.019993]
	Learning Rate: 0.0199929
	LOSS [training: 1.236360177994663 | validation: 1.2164609848959085]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2454466425263748		[learning rate: 0.019992]
	Learning Rate: 0.0199923
	LOSS [training: 1.2454466425263748 | validation: 1.2593688998956083]
	TIME [epoch: 9.72 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2347279247075293		[learning rate: 0.019992]
	Learning Rate: 0.0199916
	LOSS [training: 1.2347279247075293 | validation: 1.2837694580488157]
	TIME [epoch: 9.77 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.240870042846644		[learning rate: 0.019991]
	Learning Rate: 0.0199909
	LOSS [training: 1.240870042846644 | validation: 1.1935347047859048]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2046228895357884		[learning rate: 0.01999]
	Learning Rate: 0.0199902
	LOSS [training: 1.2046228895357884 | validation: 1.2831180793673298]
	TIME [epoch: 9.72 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2531051724398257		[learning rate: 0.019989]
	Learning Rate: 0.0199895
	LOSS [training: 1.2531051724398257 | validation: 1.1952536881236229]
	TIME [epoch: 9.72 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2088081428363526		[learning rate: 0.019989]
	Learning Rate: 0.0199887
	LOSS [training: 1.2088081428363526 | validation: 1.2601317282479423]
	TIME [epoch: 9.71 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2115581666020727		[learning rate: 0.019988]
	Learning Rate: 0.0199879
	LOSS [training: 1.2115581666020727 | validation: 1.1672223954861782]
	TIME [epoch: 9.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.215484974058974		[learning rate: 0.019987]
	Learning Rate: 0.0199871
	LOSS [training: 1.215484974058974 | validation: 1.200964684019263]
	TIME [epoch: 9.72 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.185250971709954		[learning rate: 0.019986]
	Learning Rate: 0.0199862
	LOSS [training: 1.185250971709954 | validation: 1.1365095561601293]
	TIME [epoch: 9.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2235429368830504		[learning rate: 0.019985]
	Learning Rate: 0.0199853
	LOSS [training: 1.2235429368830504 | validation: 1.2741517673010616]
	TIME [epoch: 9.73 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.263224202706014		[learning rate: 0.019984]
	Learning Rate: 0.0199844
	LOSS [training: 1.263224202706014 | validation: 1.1930730932806175]
	TIME [epoch: 9.73 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.180211833266526		[learning rate: 0.019984]
	Learning Rate: 0.0199835
	LOSS [training: 1.180211833266526 | validation: 1.166272578984195]
	TIME [epoch: 9.75 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1527130561108736		[learning rate: 0.019983]
	Learning Rate: 0.0199825
	LOSS [training: 1.1527130561108736 | validation: 1.2141102737156582]
	TIME [epoch: 9.73 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2206848045588394		[learning rate: 0.019982]
	Learning Rate: 0.0199816
	LOSS [training: 1.2206848045588394 | validation: 1.1626456126761204]
	TIME [epoch: 9.72 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1517147335788893		[learning rate: 0.019981]
	Learning Rate: 0.0199805
	LOSS [training: 1.1517147335788893 | validation: 1.1297016383114755]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2177130160659853		[learning rate: 0.019979]
	Learning Rate: 0.0199795
	LOSS [training: 1.2177130160659853 | validation: 1.18525120506616]
	TIME [epoch: 9.75 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1576836115069873		[learning rate: 0.019978]
	Learning Rate: 0.0199784
	LOSS [training: 1.1576836115069873 | validation: 1.1837325754906796]
	TIME [epoch: 9.73 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1739572441808617		[learning rate: 0.019977]
	Learning Rate: 0.0199773
	LOSS [training: 1.1739572441808617 | validation: 1.1202604796918683]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1632153748258318		[learning rate: 0.019976]
	Learning Rate: 0.0199762
	LOSS [training: 1.1632153748258318 | validation: 1.1567397211900243]
	TIME [epoch: 9.71 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.125935056723388		[learning rate: 0.019975]
	Learning Rate: 0.019975
	LOSS [training: 1.125935056723388 | validation: 1.1151188328753339]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2487696794329626		[learning rate: 0.019974]
	Learning Rate: 0.0199739
	LOSS [training: 1.2487696794329626 | validation: 1.1662486886585626]
	TIME [epoch: 10.2 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.157179387399414		[learning rate: 0.019973]
	Learning Rate: 0.0199727
	LOSS [training: 1.157179387399414 | validation: 1.1857665709676304]
	TIME [epoch: 9.72 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1437283300625725		[learning rate: 0.019971]
	Learning Rate: 0.0199714
	LOSS [training: 1.1437283300625725 | validation: 1.1135733138506687]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.105382375336308		[learning rate: 0.01997]
	Learning Rate: 0.0199702
	LOSS [training: 1.105382375336308 | validation: 1.2766868775267561]
	TIME [epoch: 9.71 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1748588355794136		[learning rate: 0.019969]
	Learning Rate: 0.0199689
	LOSS [training: 1.1748588355794136 | validation: 1.129920297430862]
	TIME [epoch: 9.71 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0967299469807554		[learning rate: 0.019968]
	Learning Rate: 0.0199675
	LOSS [training: 1.0967299469807554 | validation: 1.1440868062791933]
	TIME [epoch: 9.76 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1574741980230518		[learning rate: 0.019966]
	Learning Rate: 0.0199662
	LOSS [training: 1.1574741980230518 | validation: 1.0475009424703403]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1044933785447277		[learning rate: 0.019965]
	Learning Rate: 0.0199648
	LOSS [training: 1.1044933785447277 | validation: 1.1365097082394442]
	TIME [epoch: 9.72 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0974544961507642		[learning rate: 0.019963]
	Learning Rate: 0.0199634
	LOSS [training: 1.0974544961507642 | validation: 1.1301773203353735]
	TIME [epoch: 9.71 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1828186094005864		[learning rate: 0.019962]
	Learning Rate: 0.019962
	LOSS [training: 1.1828186094005864 | validation: 1.125300486710226]
	TIME [epoch: 9.72 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0898150819937593		[learning rate: 0.019961]
	Learning Rate: 0.0199606
	LOSS [training: 1.0898150819937593 | validation: 1.1028303563867272]
	TIME [epoch: 9.74 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0913001426556634		[learning rate: 0.019959]
	Learning Rate: 0.0199591
	LOSS [training: 1.0913001426556634 | validation: 0.9891861400372832]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0513073976101017		[learning rate: 0.019958]
	Learning Rate: 0.0199576
	LOSS [training: 1.0513073976101017 | validation: 1.047456191804079]
	TIME [epoch: 9.7 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0934945010487023		[learning rate: 0.019956]
	Learning Rate: 0.019956
	LOSS [training: 1.0934945010487023 | validation: 0.9947200476257374]
	TIME [epoch: 9.7 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1242548773386392		[learning rate: 0.019954]
	Learning Rate: 0.0199545
	LOSS [training: 1.1242548773386392 | validation: 1.072722810515327]
	TIME [epoch: 9.74 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0739228299834445		[learning rate: 0.019953]
	Learning Rate: 0.0199529
	LOSS [training: 1.0739228299834445 | validation: 1.1487135443598957]
	TIME [epoch: 9.72 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0878947672439485		[learning rate: 0.019951]
	Learning Rate: 0.0199513
	LOSS [training: 1.0878947672439485 | validation: 0.9967622245205688]
	TIME [epoch: 9.7 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.051709096690915		[learning rate: 0.01995]
	Learning Rate: 0.0199496
	LOSS [training: 1.051709096690915 | validation: 0.9938582707011676]
	TIME [epoch: 9.7 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0492039984397101		[learning rate: 0.019948]
	Learning Rate: 0.0199479
	LOSS [training: 1.0492039984397101 | validation: 0.9328223782816532]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.175490123403564		[learning rate: 0.019946]
	Learning Rate: 0.0199462
	LOSS [training: 1.175490123403564 | validation: 1.108616445404616]
	TIME [epoch: 9.75 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.072133182747442		[learning rate: 0.019945]
	Learning Rate: 0.0199445
	LOSS [training: 1.072133182747442 | validation: 1.0022910841299428]
	TIME [epoch: 9.71 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0395246482835307		[learning rate: 0.019943]
	Learning Rate: 0.0199428
	LOSS [training: 1.0395246482835307 | validation: 0.9904898228965165]
	TIME [epoch: 9.7 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0278881218077454		[learning rate: 0.019941]
	Learning Rate: 0.019941
	LOSS [training: 1.0278881218077454 | validation: 1.0692072003930548]
	TIME [epoch: 9.7 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0537024539459383		[learning rate: 0.019939]
	Learning Rate: 0.0199392
	LOSS [training: 1.0537024539459383 | validation: 0.9127092163822614]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0358440906812323		[learning rate: 0.019937]
	Learning Rate: 0.0199374
	LOSS [training: 1.0358440906812323 | validation: 1.0887821692388227]
	TIME [epoch: 9.76 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0795528903832223		[learning rate: 0.019935]
	Learning Rate: 0.0199355
	LOSS [training: 1.0795528903832223 | validation: 1.084106402922954]
	TIME [epoch: 9.7 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0479254564159086		[learning rate: 0.019934]
	Learning Rate: 0.0199336
	LOSS [training: 1.0479254564159086 | validation: 0.9734547504740919]
	TIME [epoch: 9.7 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0355606280576342		[learning rate: 0.019932]
	Learning Rate: 0.0199317
	LOSS [training: 1.0355606280576342 | validation: 0.9292332913640257]
	TIME [epoch: 9.7 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9853675192977196		[learning rate: 0.01993]
	Learning Rate: 0.0199297
	LOSS [training: 0.9853675192977196 | validation: 0.9093541523404645]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0279547398197266		[learning rate: 0.019928]
	Learning Rate: 0.0199278
	LOSS [training: 1.0279547398197266 | validation: 1.0273829455268113]
	TIME [epoch: 9.79 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0156878980570467		[learning rate: 0.019926]
	Learning Rate: 0.0199258
	LOSS [training: 1.0156878980570467 | validation: 1.0475799204636056]
	TIME [epoch: 9.72 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0276280418209742		[learning rate: 0.019924]
	Learning Rate: 0.0199238
	LOSS [training: 1.0276280418209742 | validation: 0.98380110812712]
	TIME [epoch: 9.7 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0213312251511033		[learning rate: 0.019922]
	Learning Rate: 0.0199217
	LOSS [training: 1.0213312251511033 | validation: 0.9806533970773386]
	TIME [epoch: 9.7 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9930167651524131		[learning rate: 0.01992]
	Learning Rate: 0.0199196
	LOSS [training: 0.9930167651524131 | validation: 1.0705859770800488]
	TIME [epoch: 9.72 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0262222949386064		[learning rate: 0.019918]
	Learning Rate: 0.0199175
	LOSS [training: 1.0262222949386064 | validation: 1.0800657618570506]
	TIME [epoch: 9.74 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0783800266859054		[learning rate: 0.019915]
	Learning Rate: 0.0199154
	LOSS [training: 1.0783800266859054 | validation: 0.9439468699946407]
	TIME [epoch: 9.7 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9733198961982994		[learning rate: 0.019913]
	Learning Rate: 0.0199132
	LOSS [training: 0.9733198961982994 | validation: 1.085566281270383]
	TIME [epoch: 9.7 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0126932351883742		[learning rate: 0.019911]
	Learning Rate: 0.019911
	LOSS [training: 1.0126932351883742 | validation: 0.9464762281810719]
	TIME [epoch: 9.7 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9820723367558002		[learning rate: 0.019909]
	Learning Rate: 0.0199088
	LOSS [training: 0.9820723367558002 | validation: 0.9206092108285082]
	TIME [epoch: 9.74 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9285794399180346		[learning rate: 0.019907]
	Learning Rate: 0.0199066
	LOSS [training: 0.9285794399180346 | validation: 1.1965474929402002]
	TIME [epoch: 9.72 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0805710918332747		[learning rate: 0.019904]
	Learning Rate: 0.0199043
	LOSS [training: 1.0805710918332747 | validation: 0.8837450856390796]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9458041885487734		[learning rate: 0.019902]
	Learning Rate: 0.019902
	LOSS [training: 0.9458041885487734 | validation: 0.9450980566424491]
	TIME [epoch: 9.73 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9892828452806088		[learning rate: 0.0199]
	Learning Rate: 0.0198997
	LOSS [training: 0.9892828452806088 | validation: 0.9430114188699177]
	TIME [epoch: 9.71 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9356961700683537		[learning rate: 0.019897]
	Learning Rate: 0.0198974
	LOSS [training: 0.9356961700683537 | validation: 1.1762104045981179]
	TIME [epoch: 9.76 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9990630045541666		[learning rate: 0.019895]
	Learning Rate: 0.019895
	LOSS [training: 0.9990630045541666 | validation: 0.9114662024109077]
	TIME [epoch: 9.72 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.97689453314697		[learning rate: 0.019893]
	Learning Rate: 0.0198926
	LOSS [training: 0.97689453314697 | validation: 0.9014228253342754]
	TIME [epoch: 9.71 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9469505001723422		[learning rate: 0.01989]
	Learning Rate: 0.0198901
	LOSS [training: 0.9469505001723422 | validation: 0.8637226632451194]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8939537771660944		[learning rate: 0.019888]
	Learning Rate: 0.0198877
	LOSS [training: 0.8939537771660944 | validation: 1.3123929190930268]
	TIME [epoch: 9.71 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0365624090164494		[learning rate: 0.019885]
	Learning Rate: 0.0198852
	LOSS [training: 1.0365624090164494 | validation: 0.972094787554334]
	TIME [epoch: 9.76 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9570918031600688		[learning rate: 0.019883]
	Learning Rate: 0.0198827
	LOSS [training: 0.9570918031600688 | validation: 0.8615416211768853]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9975358490320347		[learning rate: 0.01988]
	Learning Rate: 0.0198802
	LOSS [training: 0.9975358490320347 | validation: 0.9241819710076769]
	TIME [epoch: 9.7 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8924140982881872		[learning rate: 0.019878]
	Learning Rate: 0.0198776
	LOSS [training: 0.8924140982881872 | validation: 0.937968895293397]
	TIME [epoch: 9.69 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9830340707117311		[learning rate: 0.019875]
	Learning Rate: 0.019875
	LOSS [training: 0.9830340707117311 | validation: 0.9209495123872518]
	TIME [epoch: 9.73 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9110138279671767		[learning rate: 0.019872]
	Learning Rate: 0.0198724
	LOSS [training: 0.9110138279671767 | validation: 1.0309122616933828]
	TIME [epoch: 9.76 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.974559333648866		[learning rate: 0.01987]
	Learning Rate: 0.0198697
	LOSS [training: 0.974559333648866 | validation: 0.8299255513942659]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8876495681172714		[learning rate: 0.019867]
	Learning Rate: 0.0198671
	LOSS [training: 0.8876495681172714 | validation: 0.9212926225201569]
	TIME [epoch: 9.73 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8961030401071387		[learning rate: 0.019864]
	Learning Rate: 0.0198644
	LOSS [training: 0.8961030401071387 | validation: 0.9785863796987999]
	TIME [epoch: 9.72 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9609868775903871		[learning rate: 0.019862]
	Learning Rate: 0.0198616
	LOSS [training: 0.9609868775903871 | validation: 0.8601866875505395]
	TIME [epoch: 9.77 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9186606006780481		[learning rate: 0.019859]
	Learning Rate: 0.0198589
	LOSS [training: 0.9186606006780481 | validation: 0.9481825851211476]
	TIME [epoch: 9.75 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9104400887534598		[learning rate: 0.019856]
	Learning Rate: 0.0198561
	LOSS [training: 0.9104400887534598 | validation: 0.8231209270581652]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8807184234368002		[learning rate: 0.019853]
	Learning Rate: 0.0198533
	LOSS [training: 0.8807184234368002 | validation: 0.8878029342181886]
	TIME [epoch: 9.72 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9413440707911097		[learning rate: 0.01985]
	Learning Rate: 0.0198505
	LOSS [training: 0.9413440707911097 | validation: 0.9428661867349409]
	TIME [epoch: 9.72 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9252643800801286		[learning rate: 0.019848]
	Learning Rate: 0.0198476
	LOSS [training: 0.9252643800801286 | validation: 0.8420100843216272]
	TIME [epoch: 9.77 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8668114374060849		[learning rate: 0.019845]
	Learning Rate: 0.0198447
	LOSS [training: 0.8668114374060849 | validation: 0.7848127088739789]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8964045128616657		[learning rate: 0.019842]
	Learning Rate: 0.0198418
	LOSS [training: 0.8964045128616657 | validation: 0.9410837663870255]
	TIME [epoch: 9.73 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8833250901243277		[learning rate: 0.019839]
	Learning Rate: 0.0198388
	LOSS [training: 0.8833250901243277 | validation: 0.7444134892582657]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0211830133612336		[learning rate: 0.019836]
	Learning Rate: 0.0198359
	LOSS [training: 1.0211830133612336 | validation: 0.9151570724366385]
	TIME [epoch: 9.73 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8964868261079817		[learning rate: 0.019833]
	Learning Rate: 0.0198329
	LOSS [training: 0.8964868261079817 | validation: 0.8690591757520215]
	TIME [epoch: 9.79 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9364513542191919		[learning rate: 0.01983]
	Learning Rate: 0.0198299
	LOSS [training: 0.9364513542191919 | validation: 0.8883576514188487]
	TIME [epoch: 9.73 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8642996523734435		[learning rate: 0.019827]
	Learning Rate: 0.0198268
	LOSS [training: 0.8642996523734435 | validation: 0.7594839086432758]
	TIME [epoch: 9.73 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9144443626458895		[learning rate: 0.019824]
	Learning Rate: 0.0198237
	LOSS [training: 0.9144443626458895 | validation: 0.8715692623787598]
	TIME [epoch: 9.74 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9128159225002166		[learning rate: 0.019821]
	Learning Rate: 0.0198206
	LOSS [training: 0.9128159225002166 | validation: 0.8256103077170296]
	TIME [epoch: 9.78 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.838561436330021		[learning rate: 0.019818]
	Learning Rate: 0.0198175
	LOSS [training: 0.838561436330021 | validation: 0.9655408900440546]
	TIME [epoch: 9.77 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9146340354140133		[learning rate: 0.019814]
	Learning Rate: 0.0198143
	LOSS [training: 0.9146340354140133 | validation: 0.8064905192262338]
	TIME [epoch: 9.73 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8362366734750135		[learning rate: 0.019811]
	Learning Rate: 0.0198112
	LOSS [training: 0.8362366734750135 | validation: 1.0116278187619494]
	TIME [epoch: 9.73 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9577351391145812		[learning rate: 0.019808]
	Learning Rate: 0.0198079
	LOSS [training: 0.9577351391145812 | validation: 0.8247983840484214]
	TIME [epoch: 9.72 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8001276305218123		[learning rate: 0.019805]
	Learning Rate: 0.0198047
	LOSS [training: 0.8001276305218123 | validation: 0.9601430257466266]
	TIME [epoch: 9.79 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9115648699219838		[learning rate: 0.019801]
	Learning Rate: 0.0198014
	LOSS [training: 0.9115648699219838 | validation: 0.7574324442170245]
	TIME [epoch: 9.76 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8495646281414657		[learning rate: 0.019798]
	Learning Rate: 0.0197982
	LOSS [training: 0.8495646281414657 | validation: 0.9216023952434255]
	TIME [epoch: 9.73 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8573257335999127		[learning rate: 0.019795]
	Learning Rate: 0.0197948
	LOSS [training: 0.8573257335999127 | validation: 0.8636354550211931]
	TIME [epoch: 9.73 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8445995211774122		[learning rate: 0.019791]
	Learning Rate: 0.0197915
	LOSS [training: 0.8445995211774122 | validation: 0.7694155963700675]
	TIME [epoch: 9.73 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.818527562474872		[learning rate: 0.019788]
	Learning Rate: 0.0197881
	LOSS [training: 0.818527562474872 | validation: 0.8164915777055826]
	TIME [epoch: 9.76 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9581013499874224		[learning rate: 0.019785]
	Learning Rate: 0.0197847
	LOSS [training: 0.9581013499874224 | validation: 0.8412307632485075]
	TIME [epoch: 9.73 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8403191047902261		[learning rate: 0.019781]
	Learning Rate: 0.0197813
	LOSS [training: 0.8403191047902261 | validation: 0.8306060731960876]
	TIME [epoch: 9.72 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8055923009168058		[learning rate: 0.019778]
	Learning Rate: 0.0197778
	LOSS [training: 0.8055923009168058 | validation: 0.91611102905143]
	TIME [epoch: 9.71 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8760190946582451		[learning rate: 0.019774]
	Learning Rate: 0.0197744
	LOSS [training: 0.8760190946582451 | validation: 0.7818221352183803]
	TIME [epoch: 9.72 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7760222048847076		[learning rate: 0.019771]
	Learning Rate: 0.0197709
	LOSS [training: 0.7760222048847076 | validation: 0.8791933998539812]
	TIME [epoch: 9.77 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.884090646616952		[learning rate: 0.019767]
	Learning Rate: 0.0197673
	LOSS [training: 0.884090646616952 | validation: 0.7683426825246837]
	TIME [epoch: 9.72 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7579301226317818		[learning rate: 0.019764]
	Learning Rate: 0.0197638
	LOSS [training: 0.7579301226317818 | validation: 1.1957247757681913]
	TIME [epoch: 9.72 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0233992116655795		[learning rate: 0.01976]
	Learning Rate: 0.0197602
	LOSS [training: 1.0233992116655795 | validation: 0.7840722462240094]
	TIME [epoch: 9.72 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8609352965765997		[learning rate: 0.019757]
	Learning Rate: 0.0197566
	LOSS [training: 0.8609352965765997 | validation: 0.8922889797194458]
	TIME [epoch: 9.72 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.80657970621819		[learning rate: 0.019753]
	Learning Rate: 0.0197529
	LOSS [training: 0.80657970621819 | validation: 0.77123715730823]
	TIME [epoch: 9.77 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8232234198217618		[learning rate: 0.019749]
	Learning Rate: 0.0197493
	LOSS [training: 0.8232234198217618 | validation: 0.6903212374984644]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8586252163712517		[learning rate: 0.019746]
	Learning Rate: 0.0197456
	LOSS [training: 0.8586252163712517 | validation: 0.7499559304205811]
	TIME [epoch: 9.72 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7894257846654336		[learning rate: 0.019742]
	Learning Rate: 0.0197419
	LOSS [training: 0.7894257846654336 | validation: 0.8336398647815506]
	TIME [epoch: 9.72 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.768671585514222		[learning rate: 0.019738]
	Learning Rate: 0.0197381
	LOSS [training: 0.768671585514222 | validation: 0.9646364160420344]
	TIME [epoch: 9.73 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.880122538655812		[learning rate: 0.019734]
	Learning Rate: 0.0197343
	LOSS [training: 0.880122538655812 | validation: 0.7041915707245251]
	TIME [epoch: 9.76 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.814610837731006		[learning rate: 0.019731]
	Learning Rate: 0.0197305
	LOSS [training: 0.814610837731006 | validation: 0.7730473964347686]
	TIME [epoch: 9.72 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7795221357554473		[learning rate: 0.019727]
	Learning Rate: 0.0197267
	LOSS [training: 0.7795221357554473 | validation: 0.9036008516065498]
	TIME [epoch: 9.72 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7836948594676418		[learning rate: 0.019723]
	Learning Rate: 0.0197229
	LOSS [training: 0.7836948594676418 | validation: 0.6739887339810114]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7995656230614259		[learning rate: 0.019719]
	Learning Rate: 0.019719
	LOSS [training: 0.7995656230614259 | validation: 0.9290435990498216]
	TIME [epoch: 9.75 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.874326322049518		[learning rate: 0.019715]
	Learning Rate: 0.0197151
	LOSS [training: 0.874326322049518 | validation: 0.7160842386018262]
	TIME [epoch: 9.74 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.768954996103822		[learning rate: 0.019711]
	Learning Rate: 0.0197112
	LOSS [training: 0.768954996103822 | validation: 0.8230011976209455]
	TIME [epoch: 9.72 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.754519824483306		[learning rate: 0.019707]
	Learning Rate: 0.0197072
	LOSS [training: 0.754519824483306 | validation: 0.6879265290277035]
	TIME [epoch: 9.72 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7590230614860508		[learning rate: 0.019703]
	Learning Rate: 0.0197032
	LOSS [training: 0.7590230614860508 | validation: 0.8669698952654292]
	TIME [epoch: 9.72 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8202161191793497		[learning rate: 0.019699]
	Learning Rate: 0.0196992
	LOSS [training: 0.8202161191793497 | validation: 0.9470623799182494]
	TIME [epoch: 9.77 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7737878456793293		[learning rate: 0.019695]
	Learning Rate: 0.0196952
	LOSS [training: 0.7737878456793293 | validation: 0.7038908061325412]
	TIME [epoch: 9.75 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.777306722636621		[learning rate: 0.019691]
	Learning Rate: 0.0196911
	LOSS [training: 0.777306722636621 | validation: 0.6742346314172365]
	TIME [epoch: 118 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7656702656065077		[learning rate: 0.019687]
	Learning Rate: 0.019687
	LOSS [training: 0.7656702656065077 | validation: 0.7416152226365118]
	TIME [epoch: 19.3 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7753172197332352		[learning rate: 0.019683]
	Learning Rate: 0.0196829
	LOSS [training: 0.7753172197332352 | validation: 0.7446422635825413]
	TIME [epoch: 19.2 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7521963798408547		[learning rate: 0.019679]
	Learning Rate: 0.0196788
	LOSS [training: 0.7521963798408547 | validation: 0.664847165482918]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7472998036865415		[learning rate: 0.019675]
	Learning Rate: 0.0196746
	LOSS [training: 0.7472998036865415 | validation: 0.6427830571964848]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7703715644934511		[learning rate: 0.01967]
	Learning Rate: 0.0196704
	LOSS [training: 0.7703715644934511 | validation: 0.7182755163894239]
	TIME [epoch: 19.3 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7092710335621192		[learning rate: 0.019666]
	Learning Rate: 0.0196662
	LOSS [training: 0.7092710335621192 | validation: 0.8823946849013806]
	TIME [epoch: 19.2 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7513049812481252		[learning rate: 0.019662]
	Learning Rate: 0.0196619
	LOSS [training: 0.7513049812481252 | validation: 0.8412872081330471]
	TIME [epoch: 19.2 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7759746748106301		[learning rate: 0.019658]
	Learning Rate: 0.0196576
	LOSS [training: 0.7759746748106301 | validation: 0.673138399398516]
	TIME [epoch: 19.3 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8042186950243844		[learning rate: 0.019653]
	Learning Rate: 0.0196533
	LOSS [training: 0.8042186950243844 | validation: 0.6609825150502495]
	TIME [epoch: 19.2 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7330818816309725		[learning rate: 0.019649]
	Learning Rate: 0.019649
	LOSS [training: 0.7330818816309725 | validation: 0.8424686877158891]
	TIME [epoch: 19.3 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7562138593702687		[learning rate: 0.019645]
	Learning Rate: 0.0196447
	LOSS [training: 0.7562138593702687 | validation: 0.7969698100715021]
	TIME [epoch: 19.2 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7648847158448459		[learning rate: 0.01964]
	Learning Rate: 0.0196403
	LOSS [training: 0.7648847158448459 | validation: 0.7435948370335214]
	TIME [epoch: 19.2 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7652551419221107		[learning rate: 0.019636]
	Learning Rate: 0.0196359
	LOSS [training: 0.7652551419221107 | validation: 0.7454700171301033]
	TIME [epoch: 19.3 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7651361881621244		[learning rate: 0.019631]
	Learning Rate: 0.0196314
	LOSS [training: 0.7651361881621244 | validation: 0.6692908525082406]
	TIME [epoch: 19.2 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7226914769788801		[learning rate: 0.019627]
	Learning Rate: 0.019627
	LOSS [training: 0.7226914769788801 | validation: 0.6798987422779246]
	TIME [epoch: 19.3 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7178369790867865		[learning rate: 0.019622]
	Learning Rate: 0.0196225
	LOSS [training: 0.7178369790867865 | validation: 0.6786202708443656]
	TIME [epoch: 19.2 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7529803566293852		[learning rate: 0.019618]
	Learning Rate: 0.019618
	LOSS [training: 0.7529803566293852 | validation: 0.7128624362003572]
	TIME [epoch: 19.2 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7499088352796778		[learning rate: 0.019613]
	Learning Rate: 0.0196134
	LOSS [training: 0.7499088352796778 | validation: 0.7042873797421728]
	TIME [epoch: 19.3 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7066609217998269		[learning rate: 0.019609]
	Learning Rate: 0.0196089
	LOSS [training: 0.7066609217998269 | validation: 0.7972983966754462]
	TIME [epoch: 19.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7851613070599084		[learning rate: 0.019604]
	Learning Rate: 0.0196043
	LOSS [training: 0.7851613070599084 | validation: 0.805464213174749]
	TIME [epoch: 19.3 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7363762253242097		[learning rate: 0.0196]
	Learning Rate: 0.0195997
	LOSS [training: 0.7363762253242097 | validation: 0.6378353025179875]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.768615746390946		[learning rate: 0.019595]
	Learning Rate: 0.019595
	LOSS [training: 0.768615746390946 | validation: 0.6571735139494409]
	TIME [epoch: 19.2 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7181224605674886		[learning rate: 0.01959]
	Learning Rate: 0.0195904
	LOSS [training: 0.7181224605674886 | validation: 0.761359327229761]
	TIME [epoch: 19.3 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7029526169639889		[learning rate: 0.019586]
	Learning Rate: 0.0195857
	LOSS [training: 0.7029526169639889 | validation: 0.9776669504315993]
	TIME [epoch: 19.2 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8265951121015905		[learning rate: 0.019581]
	Learning Rate: 0.0195809
	LOSS [training: 0.8265951121015905 | validation: 0.6292093786948297]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8009233669880602		[learning rate: 0.019576]
	Learning Rate: 0.0195762
	LOSS [training: 0.8009233669880602 | validation: 0.759672451709035]
	TIME [epoch: 19.3 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7291010687889987		[learning rate: 0.019571]
	Learning Rate: 0.0195714
	LOSS [training: 0.7291010687889987 | validation: 0.6577085637021342]
	TIME [epoch: 19.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7132516336009485		[learning rate: 0.019567]
	Learning Rate: 0.0195666
	LOSS [training: 0.7132516336009485 | validation: 0.7641354715136985]
	TIME [epoch: 19.3 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7309726503603307		[learning rate: 0.019562]
	Learning Rate: 0.0195618
	LOSS [training: 0.7309726503603307 | validation: 0.7083527253529047]
	TIME [epoch: 19.2 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6635096286720923		[learning rate: 0.019557]
	Learning Rate: 0.0195569
	LOSS [training: 0.6635096286720923 | validation: 0.8029874171659886]
	TIME [epoch: 19.3 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6958939441755044		[learning rate: 0.019552]
	Learning Rate: 0.0195521
	LOSS [training: 0.6958939441755044 | validation: 0.6397919281082738]
	TIME [epoch: 19.2 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6794401614720145		[learning rate: 0.019547]
	Learning Rate: 0.0195472
	LOSS [training: 0.6794401614720145 | validation: 0.7122869026110377]
	TIME [epoch: 19.2 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7633905418215188		[learning rate: 0.019542]
	Learning Rate: 0.0195422
	LOSS [training: 0.7633905418215188 | validation: 0.6610473596952438]
	TIME [epoch: 19.3 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6799506583590174		[learning rate: 0.019537]
	Learning Rate: 0.0195373
	LOSS [training: 0.6799506583590174 | validation: 0.9842925528004539]
	TIME [epoch: 19.2 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7584466965913865		[learning rate: 0.019532]
	Learning Rate: 0.0195323
	LOSS [training: 0.7584466965913865 | validation: 0.6268681185229125]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.643987929406326		[learning rate: 0.019527]
	Learning Rate: 0.0195273
	LOSS [training: 0.643987929406326 | validation: 0.6616128568281988]
	TIME [epoch: 19.3 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7721368397325896		[learning rate: 0.019522]
	Learning Rate: 0.0195222
	LOSS [training: 0.7721368397325896 | validation: 0.6407503123826921]
	TIME [epoch: 19.2 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6685627485843539		[learning rate: 0.019517]
	Learning Rate: 0.0195172
	LOSS [training: 0.6685627485843539 | validation: 0.8579681948003373]
	TIME [epoch: 19.3 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7725936405373645		[learning rate: 0.019512]
	Learning Rate: 0.0195121
	LOSS [training: 0.7725936405373645 | validation: 0.6911894460982202]
	TIME [epoch: 19.2 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6418691692521967		[learning rate: 0.019507]
	Learning Rate: 0.019507
	LOSS [training: 0.6418691692521967 | validation: 0.7315172156925298]
	TIME [epoch: 19.3 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6292029818143429		[learning rate: 0.019502]
	Learning Rate: 0.0195018
	LOSS [training: 0.6292029818143429 | validation: 0.9349970721381019]
	TIME [epoch: 19.2 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7201885652906991		[learning rate: 0.019497]
	Learning Rate: 0.0194967
	LOSS [training: 0.7201885652906991 | validation: 0.7990596125530929]
	TIME [epoch: 19.2 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6890168544823305		[learning rate: 0.019491]
	Learning Rate: 0.0194915
	LOSS [training: 0.6890168544823305 | validation: 0.6459030842729828]
	TIME [epoch: 19.3 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6678550076657171		[learning rate: 0.019486]
	Learning Rate: 0.0194863
	LOSS [training: 0.6678550076657171 | validation: 0.6102667466552854]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6429956126359164		[learning rate: 0.019481]
	Learning Rate: 0.019481
	LOSS [training: 0.6429956126359164 | validation: 0.7048933833175903]
	TIME [epoch: 19.3 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7488773855103964		[learning rate: 0.019476]
	Learning Rate: 0.0194757
	LOSS [training: 0.7488773855103964 | validation: 0.6767877272351093]
	TIME [epoch: 19.2 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6484419607640036		[learning rate: 0.01947]
	Learning Rate: 0.0194705
	LOSS [training: 0.6484419607640036 | validation: 0.7716691215577701]
	TIME [epoch: 19.2 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7468192853238449		[learning rate: 0.019465]
	Learning Rate: 0.0194651
	LOSS [training: 0.7468192853238449 | validation: 0.7412818852286762]
	TIME [epoch: 19.3 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6301198188419835		[learning rate: 0.01946]
	Learning Rate: 0.0194598
	LOSS [training: 0.6301198188419835 | validation: 0.7001406611053648]
	TIME [epoch: 19.2 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6805816232640693		[learning rate: 0.019454]
	Learning Rate: 0.0194544
	LOSS [training: 0.6805816232640693 | validation: 1.166497055568413]
	TIME [epoch: 19.3 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8361351985999168		[learning rate: 0.019449]
	Learning Rate: 0.019449
	LOSS [training: 0.8361351985999168 | validation: 0.7281196583770948]
	TIME [epoch: 19.2 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6769682975198634		[learning rate: 0.019444]
	Learning Rate: 0.0194436
	LOSS [training: 0.6769682975198634 | validation: 0.6769679764222185]
	TIME [epoch: 19.2 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6851361073607514		[learning rate: 0.019438]
	Learning Rate: 0.0194381
	LOSS [training: 0.6851361073607514 | validation: 0.6306297279366713]
	TIME [epoch: 19.3 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6901238727587775		[learning rate: 0.019433]
	Learning Rate: 0.0194327
	LOSS [training: 0.6901238727587775 | validation: 0.679791602409155]
	TIME [epoch: 19.2 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6873868985608749		[learning rate: 0.019427]
	Learning Rate: 0.0194272
	LOSS [training: 0.6873868985608749 | validation: 0.5977366959535131]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6379415750100565		[learning rate: 0.019422]
	Learning Rate: 0.0194216
	LOSS [training: 0.6379415750100565 | validation: 0.6348051957740981]
	TIME [epoch: 19.3 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5940211784547045		[learning rate: 0.019416]
	Learning Rate: 0.0194161
	LOSS [training: 0.5940211784547045 | validation: 0.92845540148656]
	TIME [epoch: 19.2 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7622211248673656		[learning rate: 0.019411]
	Learning Rate: 0.0194105
	LOSS [training: 0.7622211248673656 | validation: 0.6218522832440598]
	TIME [epoch: 19.3 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6396105795149322		[learning rate: 0.019405]
	Learning Rate: 0.0194049
	LOSS [training: 0.6396105795149322 | validation: 0.590694613342673]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7059736310774105		[learning rate: 0.019399]
	Learning Rate: 0.0193993
	LOSS [training: 0.7059736310774105 | validation: 0.6665537365387018]
	TIME [epoch: 19.3 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6641760856785838		[learning rate: 0.019394]
	Learning Rate: 0.0193936
	LOSS [training: 0.6641760856785838 | validation: 0.7904424036889288]
	TIME [epoch: 19.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6380632976379675		[learning rate: 0.019388]
	Learning Rate: 0.0193879
	LOSS [training: 0.6380632976379675 | validation: 0.7594479621592365]
	TIME [epoch: 19.2 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.636876420279167		[learning rate: 0.019382]
	Learning Rate: 0.0193822
	LOSS [training: 0.636876420279167 | validation: 0.8004206896820396]
	TIME [epoch: 19.2 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5921860562437973		[learning rate: 0.019376]
	Learning Rate: 0.0193765
	LOSS [training: 0.5921860562437973 | validation: 0.8094679837740324]
	TIME [epoch: 19.2 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.753139678774305		[learning rate: 0.019371]
	Learning Rate: 0.0193707
	LOSS [training: 0.753139678774305 | validation: 0.6083188834423623]
	TIME [epoch: 19.3 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6462098315005333		[learning rate: 0.019365]
	Learning Rate: 0.0193649
	LOSS [training: 0.6462098315005333 | validation: 0.7795776325880976]
	TIME [epoch: 19.2 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6321035590183759		[learning rate: 0.019359]
	Learning Rate: 0.0193591
	LOSS [training: 0.6321035590183759 | validation: 0.9794221331782607]
	TIME [epoch: 19.2 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7072397025534207		[learning rate: 0.019353]
	Learning Rate: 0.0193533
	LOSS [training: 0.7072397025534207 | validation: 0.6360880897530312]
	TIME [epoch: 19.3 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6278770634069759		[learning rate: 0.019347]
	Learning Rate: 0.0193474
	LOSS [training: 0.6278770634069759 | validation: 0.5619715261332479]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6201754484136621		[learning rate: 0.019342]
	Learning Rate: 0.0193416
	LOSS [training: 0.6201754484136621 | validation: 0.6680158713564054]
	TIME [epoch: 19.2 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6756547647941413		[learning rate: 0.019336]
	Learning Rate: 0.0193356
	LOSS [training: 0.6756547647941413 | validation: 0.5639220611192335]
	TIME [epoch: 19.2 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5354917879249367		[learning rate: 0.01933]
	Learning Rate: 0.0193297
	LOSS [training: 0.5354917879249367 | validation: 1.2282758322912706]
	TIME [epoch: 19.2 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7542381514352963		[learning rate: 0.019324]
	Learning Rate: 0.0193237
	LOSS [training: 0.7542381514352963 | validation: 0.6496392614938492]
	TIME [epoch: 19.3 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6840780122375663		[learning rate: 0.019318]
	Learning Rate: 0.0193178
	LOSS [training: 0.6840780122375663 | validation: 0.5250968516095257]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6034175599781533		[learning rate: 0.019312]
	Learning Rate: 0.0193117
	LOSS [training: 0.6034175599781533 | validation: 0.5625017480275366]
	TIME [epoch: 19.3 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6538250582132892		[learning rate: 0.019306]
	Learning Rate: 0.0193057
	LOSS [training: 0.6538250582132892 | validation: 0.571459604914486]
	TIME [epoch: 19.2 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6246033960742629		[learning rate: 0.0193]
	Learning Rate: 0.0192996
	LOSS [training: 0.6246033960742629 | validation: 0.6299402793212686]
	TIME [epoch: 19.2 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6048245046548963		[learning rate: 0.019294]
	Learning Rate: 0.0192935
	LOSS [training: 0.6048245046548963 | validation: 0.640168696949529]
	TIME [epoch: 19.3 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6338869850683243		[learning rate: 0.019287]
	Learning Rate: 0.0192874
	LOSS [training: 0.6338869850683243 | validation: 0.6485619626653005]
	TIME [epoch: 19.2 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6484890494404575		[learning rate: 0.019281]
	Learning Rate: 0.0192813
	LOSS [training: 0.6484890494404575 | validation: 0.6800978821972908]
	TIME [epoch: 19.3 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6206331647466125		[learning rate: 0.019275]
	Learning Rate: 0.0192751
	LOSS [training: 0.6206331647466125 | validation: 0.6374972546347182]
	TIME [epoch: 19.2 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5667999705757546		[learning rate: 0.019269]
	Learning Rate: 0.0192689
	LOSS [training: 0.5667999705757546 | validation: 0.6264115603721814]
	TIME [epoch: 19.2 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7421397993975805		[learning rate: 0.019263]
	Learning Rate: 0.0192627
	LOSS [training: 0.7421397993975805 | validation: 0.5735998654929735]
	TIME [epoch: 19.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6203261103012403		[learning rate: 0.019256]
	Learning Rate: 0.0192565
	LOSS [training: 0.6203261103012403 | validation: 0.5528890004102973]
	TIME [epoch: 19.2 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5999737730002638		[learning rate: 0.01925]
	Learning Rate: 0.0192502
	LOSS [training: 0.5999737730002638 | validation: 0.7479550055628263]
	TIME [epoch: 19.3 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6329016331033052		[learning rate: 0.019244]
	Learning Rate: 0.0192439
	LOSS [training: 0.6329016331033052 | validation: 0.5477253695109802]
	TIME [epoch: 19.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.595730283602956		[learning rate: 0.019238]
	Learning Rate: 0.0192376
	LOSS [training: 0.595730283602956 | validation: 0.6836207439228322]
	TIME [epoch: 19.3 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6312950250231834		[learning rate: 0.019231]
	Learning Rate: 0.0192313
	LOSS [training: 0.6312950250231834 | validation: 0.5119751123208718]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.584119120013304		[learning rate: 0.019225]
	Learning Rate: 0.0192249
	LOSS [training: 0.584119120013304 | validation: 0.7604998153705081]
	TIME [epoch: 19.2 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6638588100411913		[learning rate: 0.019218]
	Learning Rate: 0.0192185
	LOSS [training: 0.6638588100411913 | validation: 0.8384540979450079]
	TIME [epoch: 19.3 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6525817703834227		[learning rate: 0.019212]
	Learning Rate: 0.0192121
	LOSS [training: 0.6525817703834227 | validation: 0.6017404600229724]
	TIME [epoch: 19.3 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6231648799907173		[learning rate: 0.019206]
	Learning Rate: 0.0192056
	LOSS [training: 0.6231648799907173 | validation: 0.7475731243613555]
	TIME [epoch: 19.3 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5914792764977429		[learning rate: 0.019199]
	Learning Rate: 0.0191992
	LOSS [training: 0.5914792764977429 | validation: 0.6293297251490799]
	TIME [epoch: 19.3 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6361229305989146		[learning rate: 0.019193]
	Learning Rate: 0.0191927
	LOSS [training: 0.6361229305989146 | validation: 0.6011522790213871]
	TIME [epoch: 19.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6675952359728161		[learning rate: 0.019186]
	Learning Rate: 0.0191861
	LOSS [training: 0.6675952359728161 | validation: 0.600512467156819]
	TIME [epoch: 19.3 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5813205628926786		[learning rate: 0.01918]
	Learning Rate: 0.0191796
	LOSS [training: 0.5813205628926786 | validation: 0.695523843200925]
	TIME [epoch: 19.2 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5901726518703645		[learning rate: 0.019173]
	Learning Rate: 0.019173
	LOSS [training: 0.5901726518703645 | validation: 0.6102913024629937]
	TIME [epoch: 19.3 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7073825563938377		[learning rate: 0.019166]
	Learning Rate: 0.0191664
	LOSS [training: 0.7073825563938377 | validation: 0.6314874302411018]
	TIME [epoch: 19.3 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5499172380734506		[learning rate: 0.01916]
	Learning Rate: 0.0191598
	LOSS [training: 0.5499172380734506 | validation: 0.8108883562941105]
	TIME [epoch: 19.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6827386748065194		[learning rate: 0.019153]
	Learning Rate: 0.0191532
	LOSS [training: 0.6827386748065194 | validation: 0.6391142513163758]
	TIME [epoch: 19.3 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6608940707423892		[learning rate: 0.019146]
	Learning Rate: 0.0191465
	LOSS [training: 0.6608940707423892 | validation: 0.5219796120942912]
	TIME [epoch: 19.3 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5946190299736739		[learning rate: 0.01914]
	Learning Rate: 0.0191398
	LOSS [training: 0.5946190299736739 | validation: 0.5928262648511257]
	TIME [epoch: 19.2 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6164038605381272		[learning rate: 0.019133]
	Learning Rate: 0.0191331
	LOSS [training: 0.6164038605381272 | validation: 0.6277852053074071]
	TIME [epoch: 19.3 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6123112052236206		[learning rate: 0.019126]
	Learning Rate: 0.0191263
	LOSS [training: 0.6123112052236206 | validation: 0.6712190227119204]
	TIME [epoch: 19.2 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5931677235585138		[learning rate: 0.01912]
	Learning Rate: 0.0191196
	LOSS [training: 0.5931677235585138 | validation: 0.49803186015632395]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6258546076194412		[learning rate: 0.019113]
	Learning Rate: 0.0191128
	LOSS [training: 0.6258546076194412 | validation: 0.5298576837933466]
	TIME [epoch: 19.3 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6128168666370855		[learning rate: 0.019106]
	Learning Rate: 0.019106
	LOSS [training: 0.6128168666370855 | validation: 0.5729752582937835]
	TIME [epoch: 19.2 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5650904857532542		[learning rate: 0.019099]
	Learning Rate: 0.0190991
	LOSS [training: 0.5650904857532542 | validation: 0.720870988428323]
	TIME [epoch: 19.3 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.657433430206688		[learning rate: 0.019092]
	Learning Rate: 0.0190922
	LOSS [training: 0.657433430206688 | validation: 0.6354605947677534]
	TIME [epoch: 19.2 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5391725227993136		[learning rate: 0.019085]
	Learning Rate: 0.0190853
	LOSS [training: 0.5391725227993136 | validation: 0.8644377518069161]
	TIME [epoch: 19.3 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6452459284136037		[learning rate: 0.019078]
	Learning Rate: 0.0190784
	LOSS [training: 0.6452459284136037 | validation: 0.7633423068688359]
	TIME [epoch: 19.2 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5959868842332362		[learning rate: 0.019071]
	Learning Rate: 0.0190715
	LOSS [training: 0.5959868842332362 | validation: 0.7694384895758579]
	TIME [epoch: 19.2 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.573322621264129		[learning rate: 0.019065]
	Learning Rate: 0.0190645
	LOSS [training: 0.573322621264129 | validation: 0.7599798837577469]
	TIME [epoch: 19.3 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6263313348428551		[learning rate: 0.019058]
	Learning Rate: 0.0190575
	LOSS [training: 0.6263313348428551 | validation: 0.5654573601302355]
	TIME [epoch: 19.2 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6298142644254485		[learning rate: 0.019051]
	Learning Rate: 0.0190505
	LOSS [training: 0.6298142644254485 | validation: 0.49297345244628976]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5826153739386208		[learning rate: 0.019043]
	Learning Rate: 0.0190435
	LOSS [training: 0.5826153739386208 | validation: 0.6763621971057643]
	TIME [epoch: 19.2 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5930100789013384		[learning rate: 0.019036]
	Learning Rate: 0.0190364
	LOSS [training: 0.5930100789013384 | validation: 0.5433486665925105]
	TIME [epoch: 19.2 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6016884814532032		[learning rate: 0.019029]
	Learning Rate: 0.0190293
	LOSS [training: 0.6016884814532032 | validation: 0.5575598617251754]
	TIME [epoch: 19.3 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6297716817701084		[learning rate: 0.019022]
	Learning Rate: 0.0190222
	LOSS [training: 0.6297716817701084 | validation: 0.5645090733150053]
	TIME [epoch: 19.2 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5590846114989996		[learning rate: 0.019015]
	Learning Rate: 0.019015
	LOSS [training: 0.5590846114989996 | validation: 0.6837599671921022]
	TIME [epoch: 19.3 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5823787643087421		[learning rate: 0.019008]
	Learning Rate: 0.0190079
	LOSS [training: 0.5823787643087421 | validation: 0.6632500161701553]
	TIME [epoch: 19.2 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5939216436841772		[learning rate: 0.019001]
	Learning Rate: 0.0190007
	LOSS [training: 0.5939216436841772 | validation: 0.5930869555982189]
	TIME [epoch: 19.2 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6679301925724015		[learning rate: 0.018993]
	Learning Rate: 0.0189935
	LOSS [training: 0.6679301925724015 | validation: 0.6536061469061352]
	TIME [epoch: 19.3 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5556694250324052		[learning rate: 0.018986]
	Learning Rate: 0.0189862
	LOSS [training: 0.5556694250324052 | validation: 0.6804509009280608]
	TIME [epoch: 19.2 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5799686589860875		[learning rate: 0.018979]
	Learning Rate: 0.018979
	LOSS [training: 0.5799686589860875 | validation: 0.7475110407423398]
	TIME [epoch: 19.3 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5730477679130805		[learning rate: 0.018972]
	Learning Rate: 0.0189717
	LOSS [training: 0.5730477679130805 | validation: 0.5755501730065558]
	TIME [epoch: 19.2 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6695803561299664		[learning rate: 0.018964]
	Learning Rate: 0.0189644
	LOSS [training: 0.6695803561299664 | validation: 0.5954880535379661]
	TIME [epoch: 19.2 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5454490333169614		[learning rate: 0.018957]
	Learning Rate: 0.018957
	LOSS [training: 0.5454490333169614 | validation: 0.6467218550055704]
	TIME [epoch: 19.3 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5762869932817793		[learning rate: 0.01895]
	Learning Rate: 0.0189497
	LOSS [training: 0.5762869932817793 | validation: 0.5568061946304632]
	TIME [epoch: 19.2 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5713853532356143		[learning rate: 0.018942]
	Learning Rate: 0.0189423
	LOSS [training: 0.5713853532356143 | validation: 0.5947230086302406]
	TIME [epoch: 19.3 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5519642453062473		[learning rate: 0.018935]
	Learning Rate: 0.0189349
	LOSS [training: 0.5519642453062473 | validation: 0.5878766824277823]
	TIME [epoch: 19.2 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6964527329764224		[learning rate: 0.018927]
	Learning Rate: 0.0189274
	LOSS [training: 0.6964527329764224 | validation: 1.045456033918576]
	TIME [epoch: 19.2 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7482999813179034		[learning rate: 0.01892]
	Learning Rate: 0.01892
	LOSS [training: 0.7482999813179034 | validation: 0.5487007010202981]
	TIME [epoch: 19.3 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.469395226048286		[learning rate: 0.018912]
	Learning Rate: 0.0189125
	LOSS [training: 0.469395226048286 | validation: 0.7435941962159085]
	TIME [epoch: 19.2 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6646309231252718		[learning rate: 0.018905]
	Learning Rate: 0.018905
	LOSS [training: 0.6646309231252718 | validation: 0.7085377338339258]
	TIME [epoch: 19.3 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5317813411783578		[learning rate: 0.018897]
	Learning Rate: 0.0188974
	LOSS [training: 0.5317813411783578 | validation: 0.9319860190849822]
	TIME [epoch: 19.2 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6356402419306955		[learning rate: 0.01889]
	Learning Rate: 0.0188899
	LOSS [training: 0.6356402419306955 | validation: 0.5197432953796923]
	TIME [epoch: 19.2 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.58614736543708		[learning rate: 0.018882]
	Learning Rate: 0.0188823
	LOSS [training: 0.58614736543708 | validation: 0.5316126397425645]
	TIME [epoch: 19.3 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5187927944067524		[learning rate: 0.018875]
	Learning Rate: 0.0188747
	LOSS [training: 0.5187927944067524 | validation: 0.6832506381249017]
	TIME [epoch: 19.2 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6255294147625953		[learning rate: 0.018867]
	Learning Rate: 0.0188671
	LOSS [training: 0.6255294147625953 | validation: 0.5252938731876962]
	TIME [epoch: 19.3 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5580591191204665		[learning rate: 0.018859]
	Learning Rate: 0.0188594
	LOSS [training: 0.5580591191204665 | validation: 0.5055642582153101]
	TIME [epoch: 19.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5661452089243251		[learning rate: 0.018852]
	Learning Rate: 0.0188517
	LOSS [training: 0.5661452089243251 | validation: 0.5212784982549648]
	TIME [epoch: 19.2 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.540857543812308		[learning rate: 0.018844]
	Learning Rate: 0.018844
	LOSS [training: 0.540857543812308 | validation: 0.8411864034361789]
	TIME [epoch: 19.3 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6354407043845272		[learning rate: 0.018836]
	Learning Rate: 0.0188363
	LOSS [training: 0.6354407043845272 | validation: 0.48533378106739883]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5511111533499409		[learning rate: 0.018829]
	Learning Rate: 0.0188286
	LOSS [training: 0.5511111533499409 | validation: 0.5306878076517377]
	TIME [epoch: 19.3 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5598512095360213		[learning rate: 0.018821]
	Learning Rate: 0.0188208
	LOSS [training: 0.5598512095360213 | validation: 0.49928306367212116]
	TIME [epoch: 19.2 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6093580406917332		[learning rate: 0.018813]
	Learning Rate: 0.018813
	LOSS [training: 0.6093580406917332 | validation: 0.4959675401948995]
	TIME [epoch: 19.2 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.507822200002092		[learning rate: 0.018805]
	Learning Rate: 0.0188052
	LOSS [training: 0.507822200002092 | validation: 0.6445686970793869]
	TIME [epoch: 19.3 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5865314262258277		[learning rate: 0.018797]
	Learning Rate: 0.0187973
	LOSS [training: 0.5865314262258277 | validation: 0.5657624861051413]
	TIME [epoch: 19.2 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5761928257143784		[learning rate: 0.018789]
	Learning Rate: 0.0187894
	LOSS [training: 0.5761928257143784 | validation: 0.49022132495531795]
	TIME [epoch: 19.3 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6428744472916528		[learning rate: 0.018782]
	Learning Rate: 0.0187815
	LOSS [training: 0.6428744472916528 | validation: 0.5437133623239899]
	TIME [epoch: 19.2 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5128884081610897		[learning rate: 0.018774]
	Learning Rate: 0.0187736
	LOSS [training: 0.5128884081610897 | validation: 0.5513303336069451]
	TIME [epoch: 19.2 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5467397083298052		[learning rate: 0.018766]
	Learning Rate: 0.0187657
	LOSS [training: 0.5467397083298052 | validation: 0.7146949990166227]
	TIME [epoch: 19.3 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5634949759056732		[learning rate: 0.018758]
	Learning Rate: 0.0187577
	LOSS [training: 0.5634949759056732 | validation: 0.5025777596016]
	TIME [epoch: 19.2 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6130577775344868		[learning rate: 0.01875]
	Learning Rate: 0.0187497
	LOSS [training: 0.6130577775344868 | validation: 0.6902294026852549]
	TIME [epoch: 19.3 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5213164194999331		[learning rate: 0.018742]
	Learning Rate: 0.0187417
	LOSS [training: 0.5213164194999331 | validation: 0.64773655143685]
	TIME [epoch: 19.2 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5771205640933781		[learning rate: 0.018734]
	Learning Rate: 0.0187337
	LOSS [training: 0.5771205640933781 | validation: 0.5420483570589103]
	TIME [epoch: 19.2 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5609704305889233		[learning rate: 0.018726]
	Learning Rate: 0.0187256
	LOSS [training: 0.5609704305889233 | validation: 0.520299394352014]
	TIME [epoch: 19.3 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6103403404648774		[learning rate: 0.018718]
	Learning Rate: 0.0187175
	LOSS [training: 0.6103403404648774 | validation: 0.5146780039783057]
	TIME [epoch: 19.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5354367096895398		[learning rate: 0.018709]
	Learning Rate: 0.0187094
	LOSS [training: 0.5354367096895398 | validation: 0.5982179505999056]
	TIME [epoch: 19.3 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5420817349145128		[learning rate: 0.018701]
	Learning Rate: 0.0187013
	LOSS [training: 0.5420817349145128 | validation: 0.6160176890502492]
	TIME [epoch: 19.2 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5556994264094941		[learning rate: 0.018693]
	Learning Rate: 0.0186931
	LOSS [training: 0.5556994264094941 | validation: 0.7752044071212696]
	TIME [epoch: 19.2 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.546563905473604		[learning rate: 0.018685]
	Learning Rate: 0.0186849
	LOSS [training: 0.546563905473604 | validation: 0.5628699127561676]
	TIME [epoch: 19.3 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5774896960552747		[learning rate: 0.018677]
	Learning Rate: 0.0186767
	LOSS [training: 0.5774896960552747 | validation: 0.5199692349680163]
	TIME [epoch: 19.2 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5179996308979772		[learning rate: 0.018668]
	Learning Rate: 0.0186685
	LOSS [training: 0.5179996308979772 | validation: 0.6839430003832943]
	TIME [epoch: 19.3 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5167295182146846		[learning rate: 0.01866]
	Learning Rate: 0.0186602
	LOSS [training: 0.5167295182146846 | validation: 0.4902220451485875]
	TIME [epoch: 19.2 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6163286576333722		[learning rate: 0.018652]
	Learning Rate: 0.018652
	LOSS [training: 0.6163286576333722 | validation: 0.5747926373361784]
	TIME [epoch: 19.2 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5616088235606826		[learning rate: 0.018644]
	Learning Rate: 0.0186437
	LOSS [training: 0.5616088235606826 | validation: 0.5628082856435352]
	TIME [epoch: 19.3 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49946172432072944		[learning rate: 0.018635]
	Learning Rate: 0.0186353
	LOSS [training: 0.49946172432072944 | validation: 0.5986367034376404]
	TIME [epoch: 19.2 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5610462315940103		[learning rate: 0.018627]
	Learning Rate: 0.018627
	LOSS [training: 0.5610462315940103 | validation: 0.6260749716289868]
	TIME [epoch: 19.3 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5629763734913709		[learning rate: 0.018619]
	Learning Rate: 0.0186186
	LOSS [training: 0.5629763734913709 | validation: 0.4928262138301759]
	TIME [epoch: 19.2 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5022896478452519		[learning rate: 0.01861]
	Learning Rate: 0.0186102
	LOSS [training: 0.5022896478452519 | validation: 0.5586829310683501]
	TIME [epoch: 19.2 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.618665902806655		[learning rate: 0.018602]
	Learning Rate: 0.0186018
	LOSS [training: 0.618665902806655 | validation: 0.6006040502869511]
	TIME [epoch: 19.3 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5088360862575321		[learning rate: 0.018593]
	Learning Rate: 0.0185934
	LOSS [training: 0.5088360862575321 | validation: 0.5104110860109355]
	TIME [epoch: 19.2 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5686719741047153		[learning rate: 0.018585]
	Learning Rate: 0.0185849
	LOSS [training: 0.5686719741047153 | validation: 0.5308015010513515]
	TIME [epoch: 19.3 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5271995907368734		[learning rate: 0.018576]
	Learning Rate: 0.0185764
	LOSS [training: 0.5271995907368734 | validation: 0.6305917850033254]
	TIME [epoch: 19.2 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5704568464248023		[learning rate: 0.018568]
	Learning Rate: 0.0185679
	LOSS [training: 0.5704568464248023 | validation: 0.5612908792672151]
	TIME [epoch: 19.2 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5457581797726535		[learning rate: 0.018559]
	Learning Rate: 0.0185594
	LOSS [training: 0.5457581797726535 | validation: 0.46048685815851986]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5452415733119081		[learning rate: 0.018551]
	Learning Rate: 0.0185508
	LOSS [training: 0.5452415733119081 | validation: 0.47391292979679345]
	TIME [epoch: 19.2 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5201570473831574		[learning rate: 0.018542]
	Learning Rate: 0.0185422
	LOSS [training: 0.5201570473831574 | validation: 0.6005416646309956]
	TIME [epoch: 19.2 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5569914282221547		[learning rate: 0.018534]
	Learning Rate: 0.0185336
	LOSS [training: 0.5569914282221547 | validation: 0.47039976336739153]
	TIME [epoch: 19.2 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.521680621699116		[learning rate: 0.018525]
	Learning Rate: 0.018525
	LOSS [training: 0.521680621699116 | validation: 0.7386186836090629]
	TIME [epoch: 19.2 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5508727611834087		[learning rate: 0.018516]
	Learning Rate: 0.0185163
	LOSS [training: 0.5508727611834087 | validation: 0.6785363364879371]
	TIME [epoch: 19.3 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5477539894007012		[learning rate: 0.018508]
	Learning Rate: 0.0185077
	LOSS [training: 0.5477539894007012 | validation: 0.6751262271010172]
	TIME [epoch: 19.2 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5538442514987759		[learning rate: 0.018499]
	Learning Rate: 0.018499
	LOSS [training: 0.5538442514987759 | validation: 0.527949555762235]
	TIME [epoch: 19.3 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5530849031144421		[learning rate: 0.01849]
	Learning Rate: 0.0184902
	LOSS [training: 0.5530849031144421 | validation: 0.5728389887280094]
	TIME [epoch: 19.2 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5371305107042271		[learning rate: 0.018481]
	Learning Rate: 0.0184815
	LOSS [training: 0.5371305107042271 | validation: 0.5128252989145788]
	TIME [epoch: 19.2 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.562259788773464		[learning rate: 0.018473]
	Learning Rate: 0.0184727
	LOSS [training: 0.562259788773464 | validation: 0.4684581603241038]
	TIME [epoch: 19.3 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4884885649093602		[learning rate: 0.018464]
	Learning Rate: 0.0184639
	LOSS [training: 0.4884885649093602 | validation: 0.62746753452642]
	TIME [epoch: 19.2 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.514095644137141		[learning rate: 0.018455]
	Learning Rate: 0.0184551
	LOSS [training: 0.514095644137141 | validation: 0.699980748805112]
	TIME [epoch: 19.3 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5396082757991668		[learning rate: 0.018446]
	Learning Rate: 0.0184463
	LOSS [training: 0.5396082757991668 | validation: 0.49607124295892446]
	TIME [epoch: 19.2 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5081274137746725		[learning rate: 0.018437]
	Learning Rate: 0.0184374
	LOSS [training: 0.5081274137746725 | validation: 0.5621771501140711]
	TIME [epoch: 19.2 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.522926732854758		[learning rate: 0.018429]
	Learning Rate: 0.0184285
	LOSS [training: 0.522926732854758 | validation: 0.710408487976336]
	TIME [epoch: 19.3 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5558528706789041		[learning rate: 0.01842]
	Learning Rate: 0.0184196
	LOSS [training: 0.5558528706789041 | validation: 0.516011165884985]
	TIME [epoch: 19.2 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5562366325188683		[learning rate: 0.018411]
	Learning Rate: 0.0184107
	LOSS [training: 0.5562366325188683 | validation: 0.5048999075245248]
	TIME [epoch: 19.3 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5187558331072833		[learning rate: 0.018402]
	Learning Rate: 0.0184017
	LOSS [training: 0.5187558331072833 | validation: 0.5976565429446653]
	TIME [epoch: 19.2 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5440569553285735		[learning rate: 0.018393]
	Learning Rate: 0.0183928
	LOSS [training: 0.5440569553285735 | validation: 0.5201082243627436]
	TIME [epoch: 19.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4903686314627622		[learning rate: 0.018384]
	Learning Rate: 0.0183838
	LOSS [training: 0.4903686314627622 | validation: 0.674496296339443]
	TIME [epoch: 19.3 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5134336191423532		[learning rate: 0.018375]
	Learning Rate: 0.0183747
	LOSS [training: 0.5134336191423532 | validation: 0.8644672137071228]
	TIME [epoch: 19.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5848283532730013		[learning rate: 0.018366]
	Learning Rate: 0.0183657
	LOSS [training: 0.5848283532730013 | validation: 0.47208554948785786]
	TIME [epoch: 19.2 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.517941337451814		[learning rate: 0.018357]
	Learning Rate: 0.0183566
	LOSS [training: 0.517941337451814 | validation: 0.5054168321698634]
	TIME [epoch: 19.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5650821268748893		[learning rate: 0.018348]
	Learning Rate: 0.0183475
	LOSS [training: 0.5650821268748893 | validation: 0.6206384525079489]
	TIME [epoch: 19.2 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5348998779793387		[learning rate: 0.018338]
	Learning Rate: 0.0183384
	LOSS [training: 0.5348998779793387 | validation: 0.548935584540875]
	TIME [epoch: 19.3 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5109042313983774		[learning rate: 0.018329]
	Learning Rate: 0.0183293
	LOSS [training: 0.5109042313983774 | validation: 0.51646454905112]
	TIME [epoch: 19.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5341315162029916		[learning rate: 0.01832]
	Learning Rate: 0.0183201
	LOSS [training: 0.5341315162029916 | validation: 0.5045340986055314]
	TIME [epoch: 19.3 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5062366166380667		[learning rate: 0.018311]
	Learning Rate: 0.0183109
	LOSS [training: 0.5062366166380667 | validation: 0.544681812379573]
	TIME [epoch: 19.2 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5050670246057711		[learning rate: 0.018302]
	Learning Rate: 0.0183017
	LOSS [training: 0.5050670246057711 | validation: 0.5838134479380381]
	TIME [epoch: 19.2 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5516731559691672		[learning rate: 0.018293]
	Learning Rate: 0.0182925
	LOSS [training: 0.5516731559691672 | validation: 0.5103772770259869]
	TIME [epoch: 19.3 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4368612981796943		[learning rate: 0.018283]
	Learning Rate: 0.0182833
	LOSS [training: 0.4368612981796943 | validation: 0.4568270351845505]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.555182388476216		[learning rate: 0.018274]
	Learning Rate: 0.018274
	LOSS [training: 0.555182388476216 | validation: 0.6411431365080933]
	TIME [epoch: 19.3 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.571539782392932		[learning rate: 0.018265]
	Learning Rate: 0.0182647
	LOSS [training: 0.571539782392932 | validation: 0.5703103123356531]
	TIME [epoch: 19.2 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5094216007878372		[learning rate: 0.018255]
	Learning Rate: 0.0182554
	LOSS [training: 0.5094216007878372 | validation: 0.5437436954681718]
	TIME [epoch: 19.2 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5858050714320341		[learning rate: 0.018246]
	Learning Rate: 0.018246
	LOSS [training: 0.5858050714320341 | validation: 0.48381594692101243]
	TIME [epoch: 19.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4989725924857301		[learning rate: 0.018237]
	Learning Rate: 0.0182367
	LOSS [training: 0.4989725924857301 | validation: 0.4649631493864654]
	TIME [epoch: 19.2 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5114324912769493		[learning rate: 0.018227]
	Learning Rate: 0.0182273
	LOSS [training: 0.5114324912769493 | validation: 0.47644492759618096]
	TIME [epoch: 19.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5162266391581724		[learning rate: 0.018218]
	Learning Rate: 0.0182179
	LOSS [training: 0.5162266391581724 | validation: 0.47340051073202094]
	TIME [epoch: 19.2 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4980071075218389		[learning rate: 0.018208]
	Learning Rate: 0.0182085
	LOSS [training: 0.4980071075218389 | validation: 0.4937105685594949]
	TIME [epoch: 19.2 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5318655393606124		[learning rate: 0.018199]
	Learning Rate: 0.018199
	LOSS [training: 0.5318655393606124 | validation: 0.5001304679218553]
	TIME [epoch: 19.3 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5105892614583498		[learning rate: 0.01819]
	Learning Rate: 0.0181895
	LOSS [training: 0.5105892614583498 | validation: 0.4323192837379821]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4372970170156486		[learning rate: 0.01818]
	Learning Rate: 0.01818
	LOSS [training: 0.4372970170156486 | validation: 0.5347676288007673]
	TIME [epoch: 19.2 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6317588462156571		[learning rate: 0.018171]
	Learning Rate: 0.0181705
	LOSS [training: 0.6317588462156571 | validation: 0.614808600641038]
	TIME [epoch: 19.2 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4761019592335217		[learning rate: 0.018161]
	Learning Rate: 0.018161
	LOSS [training: 0.4761019592335217 | validation: 0.5405775602815742]
	TIME [epoch: 19.2 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.516679837992387		[learning rate: 0.018151]
	Learning Rate: 0.0181514
	LOSS [training: 0.516679837992387 | validation: 0.5667780572939254]
	TIME [epoch: 19.3 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5292375577104755		[learning rate: 0.018142]
	Learning Rate: 0.0181418
	LOSS [training: 0.5292375577104755 | validation: 0.538941608481542]
	TIME [epoch: 19.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5234886619780554		[learning rate: 0.018132]
	Learning Rate: 0.0181322
	LOSS [training: 0.5234886619780554 | validation: 0.501214409575752]
	TIME [epoch: 19.3 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49329671722884827		[learning rate: 0.018123]
	Learning Rate: 0.0181226
	LOSS [training: 0.49329671722884827 | validation: 0.4851646348247276]
	TIME [epoch: 19.2 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5163351920628603		[learning rate: 0.018113]
	Learning Rate: 0.0181129
	LOSS [training: 0.5163351920628603 | validation: 0.482606061381577]
	TIME [epoch: 19.3 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5366305178202518		[learning rate: 0.018103]
	Learning Rate: 0.0181032
	LOSS [training: 0.5366305178202518 | validation: 0.48894914506770293]
	TIME [epoch: 19.3 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5322886316380914		[learning rate: 0.018094]
	Learning Rate: 0.0180936
	LOSS [training: 0.5322886316380914 | validation: 0.5652246615536001]
	TIME [epoch: 19.2 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5018385452751966		[learning rate: 0.018084]
	Learning Rate: 0.0180838
	LOSS [training: 0.5018385452751966 | validation: 0.49981806253354066]
	TIME [epoch: 19.3 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45601415963298264		[learning rate: 0.018074]
	Learning Rate: 0.0180741
	LOSS [training: 0.45601415963298264 | validation: 0.5283568564198533]
	TIME [epoch: 19.2 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6184870091395834		[learning rate: 0.018064]
	Learning Rate: 0.0180643
	LOSS [training: 0.6184870091395834 | validation: 0.4685270405104539]
	TIME [epoch: 19.2 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47212961338965054		[learning rate: 0.018055]
	Learning Rate: 0.0180545
	LOSS [training: 0.47212961338965054 | validation: 0.5189712989804776]
	TIME [epoch: 19.3 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45565377997833245		[learning rate: 0.018045]
	Learning Rate: 0.0180447
	LOSS [training: 0.45565377997833245 | validation: 0.6363334868075219]
	TIME [epoch: 19.2 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5610402993046656		[learning rate: 0.018035]
	Learning Rate: 0.0180349
	LOSS [training: 0.5610402993046656 | validation: 0.5036471553626976]
	TIME [epoch: 19.2 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4981808400547443		[learning rate: 0.018025]
	Learning Rate: 0.0180251
	LOSS [training: 0.4981808400547443 | validation: 0.48127169006586745]
	TIME [epoch: 19.2 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.514767405333621		[learning rate: 0.018015]
	Learning Rate: 0.0180152
	LOSS [training: 0.514767405333621 | validation: 0.6411707727300004]
	TIME [epoch: 19.2 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5042672816996604		[learning rate: 0.018005]
	Learning Rate: 0.0180053
	LOSS [training: 0.5042672816996604 | validation: 0.6021624676591746]
	TIME [epoch: 19.2 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5125421280839348		[learning rate: 0.017995]
	Learning Rate: 0.0179954
	LOSS [training: 0.5125421280839348 | validation: 0.5511028734539635]
	TIME [epoch: 19.2 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5056202685441774		[learning rate: 0.017985]
	Learning Rate: 0.0179854
	LOSS [training: 0.5056202685441774 | validation: 0.47909631177909046]
	TIME [epoch: 19.2 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4996978385981141		[learning rate: 0.017975]
	Learning Rate: 0.0179755
	LOSS [training: 0.4996978385981141 | validation: 0.4765932463473374]
	TIME [epoch: 19.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44620622100196355		[learning rate: 0.017965]
	Learning Rate: 0.0179655
	LOSS [training: 0.44620622100196355 | validation: 0.6587082517236946]
	TIME [epoch: 19.2 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.547228808582252		[learning rate: 0.017955]
	Learning Rate: 0.0179555
	LOSS [training: 0.547228808582252 | validation: 0.4561693045224584]
	TIME [epoch: 19.2 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5574797127405866		[learning rate: 0.017945]
	Learning Rate: 0.0179455
	LOSS [training: 0.5574797127405866 | validation: 0.5075663131541479]
	TIME [epoch: 19.1 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47389846336450986		[learning rate: 0.017935]
	Learning Rate: 0.0179354
	LOSS [training: 0.47389846336450986 | validation: 0.5457047586153585]
	TIME [epoch: 19.2 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4728450397260302		[learning rate: 0.017925]
	Learning Rate: 0.0179253
	LOSS [training: 0.4728450397260302 | validation: 0.47765650792314096]
	TIME [epoch: 19.2 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6042435617783601		[learning rate: 0.017915]
	Learning Rate: 0.0179152
	LOSS [training: 0.6042435617783601 | validation: 0.5809360823871703]
	TIME [epoch: 19.2 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5175271505343274		[learning rate: 0.017905]
	Learning Rate: 0.0179051
	LOSS [training: 0.5175271505343274 | validation: 0.5770197543327729]
	TIME [epoch: 19.2 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5036141101401289		[learning rate: 0.017895]
	Learning Rate: 0.017895
	LOSS [training: 0.5036141101401289 | validation: 0.6411519031065134]
	TIME [epoch: 19.2 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4796189889589826		[learning rate: 0.017885]
	Learning Rate: 0.0178848
	LOSS [training: 0.4796189889589826 | validation: 0.5409946122971288]
	TIME [epoch: 19.2 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4773885369460421		[learning rate: 0.017875]
	Learning Rate: 0.0178747
	LOSS [training: 0.4773885369460421 | validation: 0.74243433922319]
	TIME [epoch: 19.2 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5316013217289544		[learning rate: 0.017864]
	Learning Rate: 0.0178645
	LOSS [training: 0.5316013217289544 | validation: 0.5067464542638548]
	TIME [epoch: 19.2 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5338817126771106		[learning rate: 0.017854]
	Learning Rate: 0.0178542
	LOSS [training: 0.5338817126771106 | validation: 0.4518124370055401]
	TIME [epoch: 19.2 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4381885434395401		[learning rate: 0.017844]
	Learning Rate: 0.017844
	LOSS [training: 0.4381885434395401 | validation: 0.5600868804565857]
	TIME [epoch: 19.2 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.578072678460909		[learning rate: 0.017834]
	Learning Rate: 0.0178337
	LOSS [training: 0.578072678460909 | validation: 0.6029896469136033]
	TIME [epoch: 19.2 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41869717560978315		[learning rate: 0.017823]
	Learning Rate: 0.0178235
	LOSS [training: 0.41869717560978315 | validation: 0.398414008605788]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4802516793755176		[learning rate: 0.017813]
	Learning Rate: 0.0178131
	LOSS [training: 0.4802516793755176 | validation: 0.7015449724175644]
	TIME [epoch: 19.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5626218403543853		[learning rate: 0.017803]
	Learning Rate: 0.0178028
	LOSS [training: 0.5626218403543853 | validation: 0.4945682307452488]
	TIME [epoch: 19.3 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5467138889532945		[learning rate: 0.017792]
	Learning Rate: 0.0177925
	LOSS [training: 0.5467138889532945 | validation: 0.4493579992138583]
	TIME [epoch: 19.2 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4682030093118328		[learning rate: 0.017782]
	Learning Rate: 0.0177821
	LOSS [training: 0.4682030093118328 | validation: 0.541387365435418]
	TIME [epoch: 19.3 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5160887469817473		[learning rate: 0.017772]
	Learning Rate: 0.0177717
	LOSS [training: 0.5160887469817473 | validation: 0.45138926885201347]
	TIME [epoch: 19.3 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45105371363957475		[learning rate: 0.017761]
	Learning Rate: 0.0177613
	LOSS [training: 0.45105371363957475 | validation: 0.5973508158106253]
	TIME [epoch: 19.2 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47941852686512376		[learning rate: 0.017751]
	Learning Rate: 0.0177509
	LOSS [training: 0.47941852686512376 | validation: 0.9116642481138109]
	TIME [epoch: 19.3 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6221295608027451		[learning rate: 0.01774]
	Learning Rate: 0.0177404
	LOSS [training: 0.6221295608027451 | validation: 0.4621052740191674]
	TIME [epoch: 19.2 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48007712273828146		[learning rate: 0.01773]
	Learning Rate: 0.0177299
	LOSS [training: 0.48007712273828146 | validation: 0.4949296966690976]
	TIME [epoch: 19.2 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4858333553865094		[learning rate: 0.017719]
	Learning Rate: 0.0177194
	LOSS [training: 0.4858333553865094 | validation: 0.46526021743086377]
	TIME [epoch: 19.2 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49064796852628		[learning rate: 0.017709]
	Learning Rate: 0.0177089
	LOSS [training: 0.49064796852628 | validation: 0.6481954555497951]
	TIME [epoch: 19.2 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5732841688293121		[learning rate: 0.017698]
	Learning Rate: 0.0176984
	LOSS [training: 0.5732841688293121 | validation: 0.4566996751377995]
	TIME [epoch: 19.3 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4934453716992441		[learning rate: 0.017688]
	Learning Rate: 0.0176878
	LOSS [training: 0.4934453716992441 | validation: 0.4630764950338605]
	TIME [epoch: 19.2 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5426755139175137		[learning rate: 0.017677]
	Learning Rate: 0.0176772
	LOSS [training: 0.5426755139175137 | validation: 0.46203211207581746]
	TIME [epoch: 19.2 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4914557328345166		[learning rate: 0.017667]
	Learning Rate: 0.0176666
	LOSS [training: 0.4914557328345166 | validation: 0.7047201266060599]
	TIME [epoch: 19.2 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4801908379145543		[learning rate: 0.017656]
	Learning Rate: 0.017656
	LOSS [training: 0.4801908379145543 | validation: 0.548556424986729]
	TIME [epoch: 19.2 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5325440034476284		[learning rate: 0.017645]
	Learning Rate: 0.0176454
	LOSS [training: 0.5325440034476284 | validation: 0.4850250005672795]
	TIME [epoch: 19.3 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41926609989314473		[learning rate: 0.017635]
	Learning Rate: 0.0176347
	LOSS [training: 0.41926609989314473 | validation: 0.6946094069818984]
	TIME [epoch: 19.2 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5811542393076401		[learning rate: 0.017624]
	Learning Rate: 0.017624
	LOSS [training: 0.5811542393076401 | validation: 0.41165044360661296]
	TIME [epoch: 19.2 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47507233566305873		[learning rate: 0.017613]
	Learning Rate: 0.0176133
	LOSS [training: 0.47507233566305873 | validation: 0.5703315215265385]
	TIME [epoch: 19.2 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.504882901915082		[learning rate: 0.017603]
	Learning Rate: 0.0176026
	LOSS [training: 0.504882901915082 | validation: 0.46735878279688403]
	TIME [epoch: 19.2 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.513468846307932		[learning rate: 0.017592]
	Learning Rate: 0.0175918
	LOSS [training: 0.513468846307932 | validation: 0.5501057242713338]
	TIME [epoch: 19.3 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5147018501710636		[learning rate: 0.017581]
	Learning Rate: 0.0175811
	LOSS [training: 0.5147018501710636 | validation: 0.48000566777894293]
	TIME [epoch: 19.2 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40605033369570087		[learning rate: 0.01757]
	Learning Rate: 0.0175703
	LOSS [training: 0.40605033369570087 | validation: 0.526032445816637]
	TIME [epoch: 19.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5829032911647414		[learning rate: 0.017559]
	Learning Rate: 0.0175595
	LOSS [training: 0.5829032911647414 | validation: 0.46620163811986914]
	TIME [epoch: 19.2 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42957045550085515		[learning rate: 0.017549]
	Learning Rate: 0.0175486
	LOSS [training: 0.42957045550085515 | validation: 0.40841846973157225]
	TIME [epoch: 19.2 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4807265919881035		[learning rate: 0.017538]
	Learning Rate: 0.0175378
	LOSS [training: 0.4807265919881035 | validation: 0.42388300675163904]
	TIME [epoch: 19.3 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4777395510094706		[learning rate: 0.017527]
	Learning Rate: 0.0175269
	LOSS [training: 0.4777395510094706 | validation: 0.42191087610780775]
	TIME [epoch: 19.2 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.531288865219959		[learning rate: 0.017516]
	Learning Rate: 0.017516
	LOSS [training: 0.531288865219959 | validation: 0.5556184542428668]
	TIME [epoch: 19.2 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5006981980663643		[learning rate: 0.017505]
	Learning Rate: 0.0175051
	LOSS [training: 0.5006981980663643 | validation: 0.4727359543431811]
	TIME [epoch: 19.2 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45821638698542677		[learning rate: 0.017494]
	Learning Rate: 0.0174942
	LOSS [training: 0.45821638698542677 | validation: 0.5814058671136548]
	TIME [epoch: 19.2 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49546061798205704		[learning rate: 0.017483]
	Learning Rate: 0.0174832
	LOSS [training: 0.49546061798205704 | validation: 0.46374426765325405]
	TIME [epoch: 19.3 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47984078716062156		[learning rate: 0.017472]
	Learning Rate: 0.0174722
	LOSS [training: 0.47984078716062156 | validation: 0.5948023656331949]
	TIME [epoch: 19.2 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5216790183319931		[learning rate: 0.017461]
	Learning Rate: 0.0174612
	LOSS [training: 0.5216790183319931 | validation: 0.46648766709503453]
	TIME [epoch: 19.2 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4922813216860833		[learning rate: 0.01745]
	Learning Rate: 0.0174502
	LOSS [training: 0.4922813216860833 | validation: 0.5646424626755722]
	TIME [epoch: 19.2 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4731912316841862		[learning rate: 0.017439]
	Learning Rate: 0.0174392
	LOSS [training: 0.4731912316841862 | validation: 0.7331129342292295]
	TIME [epoch: 19.2 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5010301105089574		[learning rate: 0.017428]
	Learning Rate: 0.0174281
	LOSS [training: 0.5010301105089574 | validation: 0.4584179527285895]
	TIME [epoch: 19.2 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46762949075937366		[learning rate: 0.017417]
	Learning Rate: 0.017417
	LOSS [training: 0.46762949075937366 | validation: 0.5781007983542564]
	TIME [epoch: 19.2 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5016823155605405		[learning rate: 0.017406]
	Learning Rate: 0.0174059
	LOSS [training: 0.5016823155605405 | validation: 0.5226362557522862]
	TIME [epoch: 19.2 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4832079788998873		[learning rate: 0.017395]
	Learning Rate: 0.0173948
	LOSS [training: 0.4832079788998873 | validation: 0.4010275777427483]
	TIME [epoch: 19.2 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47932478551995017		[learning rate: 0.017384]
	Learning Rate: 0.0173837
	LOSS [training: 0.47932478551995017 | validation: 0.47641464778114884]
	TIME [epoch: 19.2 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44017642993787437		[learning rate: 0.017373]
	Learning Rate: 0.0173725
	LOSS [training: 0.44017642993787437 | validation: 0.4498408561027536]
	TIME [epoch: 19.3 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5486515341381075		[learning rate: 0.017361]
	Learning Rate: 0.0173614
	LOSS [training: 0.5486515341381075 | validation: 0.46866486301919086]
	TIME [epoch: 19.2 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4485843647111818		[learning rate: 0.01735]
	Learning Rate: 0.0173502
	LOSS [training: 0.4485843647111818 | validation: 0.6724766240333929]
	TIME [epoch: 142 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4945552585576843		[learning rate: 0.017339]
	Learning Rate: 0.0173389
	LOSS [training: 0.4945552585576843 | validation: 0.4595782394554384]
	TIME [epoch: 41.4 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4748730203294014		[learning rate: 0.017328]
	Learning Rate: 0.0173277
	LOSS [training: 0.4748730203294014 | validation: 0.4667691522706241]
	TIME [epoch: 41.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46132062710165284		[learning rate: 0.017316]
	Learning Rate: 0.0173164
	LOSS [training: 0.46132062710165284 | validation: 0.5246104800355731]
	TIME [epoch: 41.3 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48164144358825		[learning rate: 0.017305]
	Learning Rate: 0.0173052
	LOSS [training: 0.48164144358825 | validation: 0.4414960085626133]
	TIME [epoch: 41.3 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43013163325129355		[learning rate: 0.017294]
	Learning Rate: 0.0172939
	LOSS [training: 0.43013163325129355 | validation: 0.778087079171863]
	TIME [epoch: 41.2 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5697369846990062		[learning rate: 0.017283]
	Learning Rate: 0.0172826
	LOSS [training: 0.5697369846990062 | validation: 0.5313954808940856]
	TIME [epoch: 41.2 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5783186754971257		[learning rate: 0.017271]
	Learning Rate: 0.0172712
	LOSS [training: 0.5783186754971257 | validation: 0.5581483351121472]
	TIME [epoch: 41.3 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4300743804776751		[learning rate: 0.01726]
	Learning Rate: 0.0172599
	LOSS [training: 0.4300743804776751 | validation: 0.42484268654840895]
	TIME [epoch: 41.3 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.460405978274373		[learning rate: 0.017248]
	Learning Rate: 0.0172485
	LOSS [training: 0.460405978274373 | validation: 0.45738300170670765]
	TIME [epoch: 41.2 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48393305586849455		[learning rate: 0.017237]
	Learning Rate: 0.0172371
	LOSS [training: 0.48393305586849455 | validation: 0.511655537263167]
	TIME [epoch: 41.2 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5752052956135048		[learning rate: 0.017226]
	Learning Rate: 0.0172257
	LOSS [training: 0.5752052956135048 | validation: 0.6416450015001245]
	TIME [epoch: 41.2 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47873484980962794		[learning rate: 0.017214]
	Learning Rate: 0.0172142
	LOSS [training: 0.47873484980962794 | validation: 0.4173159791404608]
	TIME [epoch: 41.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4407355538504472		[learning rate: 0.017203]
	Learning Rate: 0.0172028
	LOSS [training: 0.4407355538504472 | validation: 0.43597540350317937]
	TIME [epoch: 41.2 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4777509929082637		[learning rate: 0.017191]
	Learning Rate: 0.0171913
	LOSS [training: 0.4777509929082637 | validation: 0.5336317302820955]
	TIME [epoch: 41.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4481243599782727		[learning rate: 0.01718]
	Learning Rate: 0.0171798
	LOSS [training: 0.4481243599782727 | validation: 0.7137172175517801]
	TIME [epoch: 41.3 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5060739792923528		[learning rate: 0.017168]
	Learning Rate: 0.0171683
	LOSS [training: 0.5060739792923528 | validation: 0.42806358287260377]
	TIME [epoch: 41.3 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4919345978044606		[learning rate: 0.017157]
	Learning Rate: 0.0171567
	LOSS [training: 0.4919345978044606 | validation: 0.8497002025525264]
	TIME [epoch: 41.2 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49835913133088194		[learning rate: 0.017145]
	Learning Rate: 0.0171452
	LOSS [training: 0.49835913133088194 | validation: 0.4778486691140424]
	TIME [epoch: 41.2 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46611080084420375		[learning rate: 0.017134]
	Learning Rate: 0.0171336
	LOSS [training: 0.46611080084420375 | validation: 0.47022457018976554]
	TIME [epoch: 41.2 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47360837154176855		[learning rate: 0.017122]
	Learning Rate: 0.017122
	LOSS [training: 0.47360837154176855 | validation: 0.500946289134759]
	TIME [epoch: 41.3 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5084695759505764		[learning rate: 0.01711]
	Learning Rate: 0.0171104
	LOSS [training: 0.5084695759505764 | validation: 0.5407197379985352]
	TIME [epoch: 41.3 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4914037177139675		[learning rate: 0.017099]
	Learning Rate: 0.0170988
	LOSS [training: 0.4914037177139675 | validation: 0.4429710368130259]
	TIME [epoch: 41.3 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5101423096343721		[learning rate: 0.017087]
	Learning Rate: 0.0170871
	LOSS [training: 0.5101423096343721 | validation: 0.4572665661395326]
	TIME [epoch: 41.3 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4210430185767025		[learning rate: 0.017075]
	Learning Rate: 0.0170755
	LOSS [training: 0.4210430185767025 | validation: 0.5747690822685771]
	TIME [epoch: 41.2 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47760248544088796		[learning rate: 0.017064]
	Learning Rate: 0.0170638
	LOSS [training: 0.47760248544088796 | validation: 0.4752732517467376]
	TIME [epoch: 41.2 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5165649152567204		[learning rate: 0.017052]
	Learning Rate: 0.0170521
	LOSS [training: 0.5165649152567204 | validation: 0.4647920023140077]
	TIME [epoch: 41.2 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44784397723928115		[learning rate: 0.01704]
	Learning Rate: 0.0170403
	LOSS [training: 0.44784397723928115 | validation: 0.5363895712146474]
	TIME [epoch: 41.2 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46258338744187977		[learning rate: 0.017029]
	Learning Rate: 0.0170286
	LOSS [training: 0.46258338744187977 | validation: 0.5408883240254706]
	TIME [epoch: 41.2 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46825667180052655		[learning rate: 0.017017]
	Learning Rate: 0.0170168
	LOSS [training: 0.46825667180052655 | validation: 0.42973124226600634]
	TIME [epoch: 41.2 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4438760670655876		[learning rate: 0.017005]
	Learning Rate: 0.017005
	LOSS [training: 0.4438760670655876 | validation: 0.5186927940029685]
	TIME [epoch: 41.2 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47366941211980945		[learning rate: 0.016993]
	Learning Rate: 0.0169932
	LOSS [training: 0.47366941211980945 | validation: 0.4608772582337989]
	TIME [epoch: 41.2 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49958576780591296		[learning rate: 0.016981]
	Learning Rate: 0.0169814
	LOSS [training: 0.49958576780591296 | validation: 0.5487432720494001]
	TIME [epoch: 41.2 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5054792064902249		[learning rate: 0.01697]
	Learning Rate: 0.0169695
	LOSS [training: 0.5054792064902249 | validation: 0.44555822614135043]
	TIME [epoch: 41.2 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40845089288492165		[learning rate: 0.016958]
	Learning Rate: 0.0169577
	LOSS [training: 0.40845089288492165 | validation: 0.476266333057219]
	TIME [epoch: 41.3 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5576725889757981		[learning rate: 0.016946]
	Learning Rate: 0.0169458
	LOSS [training: 0.5576725889757981 | validation: 0.5370461341970223]
	TIME [epoch: 41.3 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.432080864009209		[learning rate: 0.016934]
	Learning Rate: 0.0169339
	LOSS [training: 0.432080864009209 | validation: 0.6319366511286351]
	TIME [epoch: 41.3 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4562857705663342		[learning rate: 0.016922]
	Learning Rate: 0.016922
	LOSS [training: 0.4562857705663342 | validation: 0.4332820670258036]
	TIME [epoch: 41.3 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4848086414775336		[learning rate: 0.01691]
	Learning Rate: 0.01691
	LOSS [training: 0.4848086414775336 | validation: 0.4154008532596492]
	TIME [epoch: 41.2 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4719169222626509		[learning rate: 0.016898]
	Learning Rate: 0.0168981
	LOSS [training: 0.4719169222626509 | validation: 0.561507668987159]
	TIME [epoch: 41.2 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44792665831948497		[learning rate: 0.016886]
	Learning Rate: 0.0168861
	LOSS [training: 0.44792665831948497 | validation: 0.5808605890627745]
	TIME [epoch: 41.3 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46781139340621847		[learning rate: 0.016874]
	Learning Rate: 0.0168741
	LOSS [training: 0.46781139340621847 | validation: 0.4990387278943395]
	TIME [epoch: 41.3 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4620276264125979		[learning rate: 0.016862]
	Learning Rate: 0.0168621
	LOSS [training: 0.4620276264125979 | validation: 0.6275304634764641]
	TIME [epoch: 41.3 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4660163384427246		[learning rate: 0.01685]
	Learning Rate: 0.0168501
	LOSS [training: 0.4660163384427246 | validation: 0.4391968178646266]
	TIME [epoch: 41.3 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49062978025500836		[learning rate: 0.016838]
	Learning Rate: 0.016838
	LOSS [training: 0.49062978025500836 | validation: 0.4348211709472095]
	TIME [epoch: 41.2 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4331783343552671		[learning rate: 0.016826]
	Learning Rate: 0.0168259
	LOSS [training: 0.4331783343552671 | validation: 0.5071069462686733]
	TIME [epoch: 41.2 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.465851984550995		[learning rate: 0.016814]
	Learning Rate: 0.0168138
	LOSS [training: 0.465851984550995 | validation: 0.5202367806307456]
	TIME [epoch: 41.3 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47150662267994004		[learning rate: 0.016802]
	Learning Rate: 0.0168017
	LOSS [training: 0.47150662267994004 | validation: 0.49486752321003746]
	TIME [epoch: 41.3 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44770285192697823		[learning rate: 0.01679]
	Learning Rate: 0.0167896
	LOSS [training: 0.44770285192697823 | validation: 0.5238536472702626]
	TIME [epoch: 41.3 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4728606908606575		[learning rate: 0.016777]
	Learning Rate: 0.0167775
	LOSS [training: 0.4728606908606575 | validation: 0.4396003775934657]
	TIME [epoch: 41.2 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4640916522805756		[learning rate: 0.016765]
	Learning Rate: 0.0167653
	LOSS [training: 0.4640916522805756 | validation: 0.5917349494274456]
	TIME [epoch: 41.2 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44618267363914194		[learning rate: 0.016753]
	Learning Rate: 0.0167531
	LOSS [training: 0.44618267363914194 | validation: 0.5424644038480761]
	TIME [epoch: 41.2 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5841183459276309		[learning rate: 0.016741]
	Learning Rate: 0.0167409
	LOSS [training: 0.5841183459276309 | validation: 0.5063919534441562]
	TIME [epoch: 41.2 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4793132356658141		[learning rate: 0.016729]
	Learning Rate: 0.0167287
	LOSS [training: 0.4793132356658141 | validation: 0.4929923476286513]
	TIME [epoch: 41.3 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4245799930697788		[learning rate: 0.016716]
	Learning Rate: 0.0167165
	LOSS [training: 0.4245799930697788 | validation: 0.4187648501575336]
	TIME [epoch: 41.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44818180640208366		[learning rate: 0.016704]
	Learning Rate: 0.0167042
	LOSS [training: 0.44818180640208366 | validation: 0.46629627405585966]
	TIME [epoch: 41.2 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4447216625645067		[learning rate: 0.016692]
	Learning Rate: 0.0166919
	LOSS [training: 0.4447216625645067 | validation: 0.4619269328305191]
	TIME [epoch: 41.2 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5079685386847244		[learning rate: 0.01668]
	Learning Rate: 0.0166796
	LOSS [training: 0.5079685386847244 | validation: 0.417854062264453]
	TIME [epoch: 41.2 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40649789832883015		[learning rate: 0.016667]
	Learning Rate: 0.0166673
	LOSS [training: 0.40649789832883015 | validation: 0.39403771675755617]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_559.pth
	Model improved!!!
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4653074889212		[learning rate: 0.016655]
	Learning Rate: 0.016655
	LOSS [training: 0.4653074889212 | validation: 0.43841337171780437]
	TIME [epoch: 41.3 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4248856239855719		[learning rate: 0.016643]
	Learning Rate: 0.0166427
	LOSS [training: 0.4248856239855719 | validation: 0.4711746312397608]
	TIME [epoch: 41.3 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49850449270118274		[learning rate: 0.01663]
	Learning Rate: 0.0166303
	LOSS [training: 0.49850449270118274 | validation: 0.4223124618659565]
	TIME [epoch: 41.3 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43574834817138497		[learning rate: 0.016618]
	Learning Rate: 0.0166179
	LOSS [training: 0.43574834817138497 | validation: 0.5188364105714519]
	TIME [epoch: 41.3 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44835707224324256		[learning rate: 0.016606]
	Learning Rate: 0.0166055
	LOSS [training: 0.44835707224324256 | validation: 0.4180441233134189]
	TIME [epoch: 41.2 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48761429047875193		[learning rate: 0.016593]
	Learning Rate: 0.0165931
	LOSS [training: 0.48761429047875193 | validation: 0.4962862633523788]
	TIME [epoch: 41.2 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4543216820004203		[learning rate: 0.016581]
	Learning Rate: 0.0165807
	LOSS [training: 0.4543216820004203 | validation: 0.4239164390157979]
	TIME [epoch: 41.2 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43837553345391356		[learning rate: 0.016568]
	Learning Rate: 0.0165682
	LOSS [training: 0.43837553345391356 | validation: 0.4870318702376147]
	TIME [epoch: 41.3 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4728280764401872		[learning rate: 0.016556]
	Learning Rate: 0.0165557
	LOSS [training: 0.4728280764401872 | validation: 0.5207634431342324]
	TIME [epoch: 41.3 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43099057138762015		[learning rate: 0.016543]
	Learning Rate: 0.0165432
	LOSS [training: 0.43099057138762015 | validation: 0.4587713017264978]
	TIME [epoch: 41.3 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47445381185662555		[learning rate: 0.016531]
	Learning Rate: 0.0165307
	LOSS [training: 0.47445381185662555 | validation: 0.4655394751024411]
	TIME [epoch: 41.3 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4796542086019118		[learning rate: 0.016518]
	Learning Rate: 0.0165182
	LOSS [training: 0.4796542086019118 | validation: 0.5083991207373515]
	TIME [epoch: 41.3 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4130585918719988		[learning rate: 0.016506]
	Learning Rate: 0.0165057
	LOSS [training: 0.4130585918719988 | validation: 0.463182744219796]
	TIME [epoch: 41.3 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4542743956803597		[learning rate: 0.016493]
	Learning Rate: 0.0164931
	LOSS [training: 0.4542743956803597 | validation: 0.4304124025091661]
	TIME [epoch: 41.3 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4649504048699467		[learning rate: 0.016481]
	Learning Rate: 0.0164805
	LOSS [training: 0.4649504048699467 | validation: 0.45204704591541284]
	TIME [epoch: 41.3 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4194630973478014		[learning rate: 0.016468]
	Learning Rate: 0.0164679
	LOSS [training: 0.4194630973478014 | validation: 0.6662480799642537]
	TIME [epoch: 41.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4415961384430636		[learning rate: 0.016455]
	Learning Rate: 0.0164553
	LOSS [training: 0.4415961384430636 | validation: 0.43192206335665156]
	TIME [epoch: 41.3 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5266155830004775		[learning rate: 0.016443]
	Learning Rate: 0.0164427
	LOSS [training: 0.5266155830004775 | validation: 0.5093213687604502]
	TIME [epoch: 41.3 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37666728512653613		[learning rate: 0.01643]
	Learning Rate: 0.01643
	LOSS [training: 0.37666728512653613 | validation: 0.3626228094312429]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_578.pth
	Model improved!!!
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5359313445041178		[learning rate: 0.016417]
	Learning Rate: 0.0164173
	LOSS [training: 0.5359313445041178 | validation: 0.5638639580518945]
	TIME [epoch: 41.3 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4505515988430488		[learning rate: 0.016405]
	Learning Rate: 0.0164047
	LOSS [training: 0.4505515988430488 | validation: 0.5090058750822797]
	TIME [epoch: 41.3 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46079179684216726		[learning rate: 0.016392]
	Learning Rate: 0.016392
	LOSS [training: 0.46079179684216726 | validation: 0.4364411618493178]
	TIME [epoch: 41.2 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4491515910792001		[learning rate: 0.016379]
	Learning Rate: 0.0163792
	LOSS [training: 0.4491515910792001 | validation: 0.5384027754527517]
	TIME [epoch: 41.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4311511604485614		[learning rate: 0.016366]
	Learning Rate: 0.0163665
	LOSS [training: 0.4311511604485614 | validation: 0.49699109419806936]
	TIME [epoch: 41.2 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4560308950143423		[learning rate: 0.016354]
	Learning Rate: 0.0163537
	LOSS [training: 0.4560308950143423 | validation: 0.4354097565690912]
	TIME [epoch: 41.2 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45049287466882687		[learning rate: 0.016341]
	Learning Rate: 0.016341
	LOSS [training: 0.45049287466882687 | validation: 0.6600863514926947]
	TIME [epoch: 41.2 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4969255478526496		[learning rate: 0.016328]
	Learning Rate: 0.0163282
	LOSS [training: 0.4969255478526496 | validation: 0.39033951911981213]
	TIME [epoch: 41.2 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4138995416536413		[learning rate: 0.016315]
	Learning Rate: 0.0163154
	LOSS [training: 0.4138995416536413 | validation: 0.5765030993388476]
	TIME [epoch: 41.2 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42807819566232125		[learning rate: 0.016303]
	Learning Rate: 0.0163025
	LOSS [training: 0.42807819566232125 | validation: 0.49877202921202357]
	TIME [epoch: 41.2 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5464563548051908		[learning rate: 0.01629]
	Learning Rate: 0.0162897
	LOSS [training: 0.5464563548051908 | validation: 0.5355839117804077]
	TIME [epoch: 41.2 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4431688099208925		[learning rate: 0.016277]
	Learning Rate: 0.0162768
	LOSS [training: 0.4431688099208925 | validation: 0.4308076747630639]
	TIME [epoch: 41.2 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4637428855656292		[learning rate: 0.016264]
	Learning Rate: 0.016264
	LOSS [training: 0.4637428855656292 | validation: 0.45477434649218373]
	TIME [epoch: 41.2 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3996818791524821		[learning rate: 0.016251]
	Learning Rate: 0.0162511
	LOSS [training: 0.3996818791524821 | validation: 0.4299010921608356]
	TIME [epoch: 41.2 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4762969521701673		[learning rate: 0.016238]
	Learning Rate: 0.0162382
	LOSS [training: 0.4762969521701673 | validation: 0.43482011070448634]
	TIME [epoch: 41.3 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44765297211683375		[learning rate: 0.016225]
	Learning Rate: 0.0162252
	LOSS [training: 0.44765297211683375 | validation: 0.4722970631494836]
	TIME [epoch: 41.3 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4133349709776605		[learning rate: 0.016212]
	Learning Rate: 0.0162123
	LOSS [training: 0.4133349709776605 | validation: 0.3886506103421066]
	TIME [epoch: 41.2 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44118662502200157		[learning rate: 0.016199]
	Learning Rate: 0.0161993
	LOSS [training: 0.44118662502200157 | validation: 0.41747821817495273]
	TIME [epoch: 41.3 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5623088446986545		[learning rate: 0.016186]
	Learning Rate: 0.0161864
	LOSS [training: 0.5623088446986545 | validation: 0.5300234533155098]
	TIME [epoch: 41.2 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47938660829510094		[learning rate: 0.016173]
	Learning Rate: 0.0161734
	LOSS [training: 0.47938660829510094 | validation: 0.42423820946208657]
	TIME [epoch: 41.2 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4315008831544914		[learning rate: 0.01616]
	Learning Rate: 0.0161603
	LOSS [training: 0.4315008831544914 | validation: 0.40917624861796253]
	TIME [epoch: 41.2 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4054536701161229		[learning rate: 0.016147]
	Learning Rate: 0.0161473
	LOSS [training: 0.4054536701161229 | validation: 0.5108634911317119]
	TIME [epoch: 41.2 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4715827222077621		[learning rate: 0.016134]
	Learning Rate: 0.0161343
	LOSS [training: 0.4715827222077621 | validation: 0.4814186498123406]
	TIME [epoch: 41.3 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41361267582194494		[learning rate: 0.016121]
	Learning Rate: 0.0161212
	LOSS [training: 0.41361267582194494 | validation: 0.49620040458526393]
	TIME [epoch: 41.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5423272007623262		[learning rate: 0.016108]
	Learning Rate: 0.0161081
	LOSS [training: 0.5423272007623262 | validation: 0.42515033205326114]
	TIME [epoch: 41.2 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39814252123874344		[learning rate: 0.016095]
	Learning Rate: 0.016095
	LOSS [training: 0.39814252123874344 | validation: 0.47921621982588636]
	TIME [epoch: 41.2 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4485227895946834		[learning rate: 0.016082]
	Learning Rate: 0.0160819
	LOSS [training: 0.4485227895946834 | validation: 0.4750263356963975]
	TIME [epoch: 41.2 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43290957762747695		[learning rate: 0.016069]
	Learning Rate: 0.0160688
	LOSS [training: 0.43290957762747695 | validation: 0.5300665715650398]
	TIME [epoch: 41.2 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4401414607957178		[learning rate: 0.016056]
	Learning Rate: 0.0160556
	LOSS [training: 0.4401414607957178 | validation: 0.4802828768836135]
	TIME [epoch: 41.3 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43349624309340595		[learning rate: 0.016042]
	Learning Rate: 0.0160425
	LOSS [training: 0.43349624309340595 | validation: 0.44024968018880006]
	TIME [epoch: 41.2 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46879231800984267		[learning rate: 0.016029]
	Learning Rate: 0.0160293
	LOSS [training: 0.46879231800984267 | validation: 0.4614978405828809]
	TIME [epoch: 41.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4294231754986254		[learning rate: 0.016016]
	Learning Rate: 0.0160161
	LOSS [training: 0.4294231754986254 | validation: 0.45672315823989856]
	TIME [epoch: 41.2 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42080057363042483		[learning rate: 0.016003]
	Learning Rate: 0.0160029
	LOSS [training: 0.42080057363042483 | validation: 0.578053095899558]
	TIME [epoch: 41.2 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5275981453178731		[learning rate: 0.01599]
	Learning Rate: 0.0159897
	LOSS [training: 0.5275981453178731 | validation: 0.48844319100505873]
	TIME [epoch: 41.2 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41864994357643837		[learning rate: 0.015976]
	Learning Rate: 0.0159764
	LOSS [training: 0.41864994357643837 | validation: 0.41749679867526623]
	TIME [epoch: 41.2 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.421011602244327		[learning rate: 0.015963]
	Learning Rate: 0.0159632
	LOSS [training: 0.421011602244327 | validation: 0.6894878368387065]
	TIME [epoch: 41.2 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5453135357461439		[learning rate: 0.01595]
	Learning Rate: 0.0159499
	LOSS [training: 0.5453135357461439 | validation: 0.38027173528037506]
	TIME [epoch: 41.2 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4340101014988408		[learning rate: 0.015937]
	Learning Rate: 0.0159366
	LOSS [training: 0.4340101014988408 | validation: 0.4266909441702089]
	TIME [epoch: 41.2 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42244073147661554		[learning rate: 0.015923]
	Learning Rate: 0.0159233
	LOSS [training: 0.42244073147661554 | validation: 0.4296760311750537]
	TIME [epoch: 41.2 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4116972108575814		[learning rate: 0.01591]
	Learning Rate: 0.01591
	LOSS [training: 0.4116972108575814 | validation: 0.428139514602909]
	TIME [epoch: 41.3 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44361793944797323		[learning rate: 0.015897]
	Learning Rate: 0.0158966
	LOSS [training: 0.44361793944797323 | validation: 0.5980689093927902]
	TIME [epoch: 41.2 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4621542668215072		[learning rate: 0.015883]
	Learning Rate: 0.0158833
	LOSS [training: 0.4621542668215072 | validation: 0.3660314565243914]
	TIME [epoch: 41.2 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4596659874261987		[learning rate: 0.01587]
	Learning Rate: 0.0158699
	LOSS [training: 0.4596659874261987 | validation: 0.5138269328804472]
	TIME [epoch: 41.2 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42581676927201484		[learning rate: 0.015856]
	Learning Rate: 0.0158565
	LOSS [training: 0.42581676927201484 | validation: 0.377945245560349]
	TIME [epoch: 41.2 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41278926571566804		[learning rate: 0.015843]
	Learning Rate: 0.0158431
	LOSS [training: 0.41278926571566804 | validation: 0.4756516348759633]
	TIME [epoch: 41.2 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4266639701440061		[learning rate: 0.01583]
	Learning Rate: 0.0158297
	LOSS [training: 0.4266639701440061 | validation: 0.48433321232358245]
	TIME [epoch: 41.2 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46030888254168395		[learning rate: 0.015816]
	Learning Rate: 0.0158162
	LOSS [training: 0.46030888254168395 | validation: 0.39198147872579103]
	TIME [epoch: 41.2 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4287084989463158		[learning rate: 0.015803]
	Learning Rate: 0.0158028
	LOSS [training: 0.4287084989463158 | validation: 0.42045155185464034]
	TIME [epoch: 41.2 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45630521201879565		[learning rate: 0.015789]
	Learning Rate: 0.0157893
	LOSS [training: 0.45630521201879565 | validation: 0.40549478740384926]
	TIME [epoch: 41.2 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4176563078210664		[learning rate: 0.015776]
	Learning Rate: 0.0157758
	LOSS [training: 0.4176563078210664 | validation: 0.5470396927739092]
	TIME [epoch: 41.2 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4254500873715932		[learning rate: 0.015762]
	Learning Rate: 0.0157623
	LOSS [training: 0.4254500873715932 | validation: 0.5412959609065959]
	TIME [epoch: 41.2 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47878536719405307		[learning rate: 0.015749]
	Learning Rate: 0.0157488
	LOSS [training: 0.47878536719405307 | validation: 0.39642882534547946]
	TIME [epoch: 41.2 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4027716025885211		[learning rate: 0.015735]
	Learning Rate: 0.0157353
	LOSS [training: 0.4027716025885211 | validation: 0.536591625840594]
	TIME [epoch: 41.3 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4452602166618548		[learning rate: 0.015722]
	Learning Rate: 0.0157217
	LOSS [training: 0.4452602166618548 | validation: 0.4207741410581761]
	TIME [epoch: 41.3 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4131010225035733		[learning rate: 0.015708]
	Learning Rate: 0.0157082
	LOSS [training: 0.4131010225035733 | validation: 0.4495663143271049]
	TIME [epoch: 41.2 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45095149850829364		[learning rate: 0.015695]
	Learning Rate: 0.0156946
	LOSS [training: 0.45095149850829364 | validation: 0.5992992845675311]
	TIME [epoch: 41.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4889093210925762		[learning rate: 0.015681]
	Learning Rate: 0.015681
	LOSS [training: 0.4889093210925762 | validation: 0.4189137859159116]
	TIME [epoch: 41.2 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42297355377391094		[learning rate: 0.015667]
	Learning Rate: 0.0156674
	LOSS [training: 0.42297355377391094 | validation: 0.45675765260738377]
	TIME [epoch: 41.2 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43158373431113073		[learning rate: 0.015654]
	Learning Rate: 0.0156537
	LOSS [training: 0.43158373431113073 | validation: 0.47514703938602443]
	TIME [epoch: 41.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3914696968579562		[learning rate: 0.01564]
	Learning Rate: 0.0156401
	LOSS [training: 0.3914696968579562 | validation: 0.4542238786840076]
	TIME [epoch: 41.2 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5520723038222344		[learning rate: 0.015626]
	Learning Rate: 0.0156264
	LOSS [training: 0.5520723038222344 | validation: 0.43445486094091]
	TIME [epoch: 41.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4194597037905722		[learning rate: 0.015613]
	Learning Rate: 0.0156128
	LOSS [training: 0.4194597037905722 | validation: 0.47705660796425925]
	TIME [epoch: 41.2 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5104583321412167		[learning rate: 0.015599]
	Learning Rate: 0.0155991
	LOSS [training: 0.5104583321412167 | validation: 0.48780443996188744]
	TIME [epoch: 41.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39459347637034425		[learning rate: 0.015585]
	Learning Rate: 0.0155854
	LOSS [training: 0.39459347637034425 | validation: 0.576658097235581]
	TIME [epoch: 41.2 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5041552339059528		[learning rate: 0.015572]
	Learning Rate: 0.0155717
	LOSS [training: 0.5041552339059528 | validation: 0.43494234955453376]
	TIME [epoch: 41.2 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4082312955045013		[learning rate: 0.015558]
	Learning Rate: 0.0155579
	LOSS [training: 0.4082312955045013 | validation: 0.4411574364084129]
	TIME [epoch: 41.2 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39608447840926697		[learning rate: 0.015544]
	Learning Rate: 0.0155442
	LOSS [training: 0.39608447840926697 | validation: 0.39234470371448116]
	TIME [epoch: 41.2 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47477291502940355		[learning rate: 0.01553]
	Learning Rate: 0.0155304
	LOSS [training: 0.47477291502940355 | validation: 0.39818044487075843]
	TIME [epoch: 41.2 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3747983034161716		[learning rate: 0.015517]
	Learning Rate: 0.0155166
	LOSS [training: 0.3747983034161716 | validation: 0.4063355787171954]
	TIME [epoch: 41.3 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40900627233533543		[learning rate: 0.015503]
	Learning Rate: 0.0155028
	LOSS [training: 0.40900627233533543 | validation: 0.5054041778890209]
	TIME [epoch: 41.2 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4870027811849954		[learning rate: 0.015489]
	Learning Rate: 0.015489
	LOSS [training: 0.4870027811849954 | validation: 0.38028795165562734]
	TIME [epoch: 41.2 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3903151433645883		[learning rate: 0.015475]
	Learning Rate: 0.0154752
	LOSS [training: 0.3903151433645883 | validation: 0.4048141274003477]
	TIME [epoch: 41.2 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43287273801710807		[learning rate: 0.015461]
	Learning Rate: 0.0154614
	LOSS [training: 0.43287273801710807 | validation: 0.5271958596292915]
	TIME [epoch: 41.2 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5097083831927832		[learning rate: 0.015448]
	Learning Rate: 0.0154475
	LOSS [training: 0.5097083831927832 | validation: 0.44958542912404764]
	TIME [epoch: 41.3 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39517451339912696		[learning rate: 0.015434]
	Learning Rate: 0.0154336
	LOSS [training: 0.39517451339912696 | validation: 0.4012021034468335]
	TIME [epoch: 41.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4838153214235553		[learning rate: 0.01542]
	Learning Rate: 0.0154198
	LOSS [training: 0.4838153214235553 | validation: 0.57257443561122]
	TIME [epoch: 41.2 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4231322860442645		[learning rate: 0.015406]
	Learning Rate: 0.0154059
	LOSS [training: 0.4231322860442645 | validation: 0.6510716453966805]
	TIME [epoch: 41.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5049609977766751		[learning rate: 0.015392]
	Learning Rate: 0.0153919
	LOSS [training: 0.5049609977766751 | validation: 0.475938729814545]
	TIME [epoch: 41.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4253458042640008		[learning rate: 0.015378]
	Learning Rate: 0.015378
	LOSS [training: 0.4253458042640008 | validation: 0.4841283543707402]
	TIME [epoch: 41.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43896981569139254		[learning rate: 0.015364]
	Learning Rate: 0.0153641
	LOSS [training: 0.43896981569139254 | validation: 0.45497053890481365]
	TIME [epoch: 41.2 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4389872278786		[learning rate: 0.01535]
	Learning Rate: 0.0153501
	LOSS [training: 0.4389872278786 | validation: 0.4713060498962346]
	TIME [epoch: 41.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47198116634517595		[learning rate: 0.015336]
	Learning Rate: 0.0153361
	LOSS [training: 0.47198116634517595 | validation: 0.5353602772352187]
	TIME [epoch: 41.3 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4408128612476213		[learning rate: 0.015322]
	Learning Rate: 0.0153222
	LOSS [training: 0.4408128612476213 | validation: 0.3620787780301906]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4170192584271946		[learning rate: 0.015308]
	Learning Rate: 0.0153082
	LOSS [training: 0.4170192584271946 | validation: 0.4498717323348163]
	TIME [epoch: 41.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45646765149429264		[learning rate: 0.015294]
	Learning Rate: 0.0152941
	LOSS [training: 0.45646765149429264 | validation: 0.4503085668558284]
	TIME [epoch: 41.3 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4278992094831253		[learning rate: 0.01528]
	Learning Rate: 0.0152801
	LOSS [training: 0.4278992094831253 | validation: 0.5945421507816538]
	TIME [epoch: 41.3 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5300086171147951		[learning rate: 0.015266]
	Learning Rate: 0.0152661
	LOSS [training: 0.5300086171147951 | validation: 0.4128800460492945]
	TIME [epoch: 41.3 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3997434329802817		[learning rate: 0.015252]
	Learning Rate: 0.015252
	LOSS [training: 0.3997434329802817 | validation: 0.4178707318594038]
	TIME [epoch: 41.2 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.418930743521848		[learning rate: 0.015238]
	Learning Rate: 0.0152379
	LOSS [training: 0.418930743521848 | validation: 0.4583498515915773]
	TIME [epoch: 41.2 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3972660376129647		[learning rate: 0.015224]
	Learning Rate: 0.0152238
	LOSS [training: 0.3972660376129647 | validation: 0.40703001209510725]
	TIME [epoch: 41.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4312739452131129		[learning rate: 0.01521]
	Learning Rate: 0.0152097
	LOSS [training: 0.4312739452131129 | validation: 0.4140307871734199]
	TIME [epoch: 41.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39812030941479987		[learning rate: 0.015196]
	Learning Rate: 0.0151956
	LOSS [training: 0.39812030941479987 | validation: 0.42908303852928864]
	TIME [epoch: 41.2 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5346798627387944		[learning rate: 0.015182]
	Learning Rate: 0.0151815
	LOSS [training: 0.5346798627387944 | validation: 0.6578453941482572]
	TIME [epoch: 41.2 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44012484113886563		[learning rate: 0.015167]
	Learning Rate: 0.0151674
	LOSS [training: 0.44012484113886563 | validation: 0.4903330866388034]
	TIME [epoch: 41.3 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40366019558158805		[learning rate: 0.015153]
	Learning Rate: 0.0151532
	LOSS [training: 0.40366019558158805 | validation: 0.4350205689912702]
	TIME [epoch: 41.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46061832463636176		[learning rate: 0.015139]
	Learning Rate: 0.015139
	LOSS [training: 0.46061832463636176 | validation: 0.4527420401966349]
	TIME [epoch: 41.2 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4679584769706855		[learning rate: 0.015125]
	Learning Rate: 0.0151248
	LOSS [training: 0.4679584769706855 | validation: 0.3950819204277274]
	TIME [epoch: 41.2 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4352065924696637		[learning rate: 0.015111]
	Learning Rate: 0.0151106
	LOSS [training: 0.4352065924696637 | validation: 0.46655559701591487]
	TIME [epoch: 41.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4359941645091153		[learning rate: 0.015096]
	Learning Rate: 0.0150964
	LOSS [training: 0.4359941645091153 | validation: 0.47416627768478437]
	TIME [epoch: 41.2 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3746780846516375		[learning rate: 0.015082]
	Learning Rate: 0.0150822
	LOSS [training: 0.3746780846516375 | validation: 0.4135932453802377]
	TIME [epoch: 41.3 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5240353406776894		[learning rate: 0.015068]
	Learning Rate: 0.015068
	LOSS [training: 0.5240353406776894 | validation: 0.3803002936216539]
	TIME [epoch: 41.2 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.404865599080767		[learning rate: 0.015054]
	Learning Rate: 0.0150537
	LOSS [training: 0.404865599080767 | validation: 0.4781227209994935]
	TIME [epoch: 41.2 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.436492515827736		[learning rate: 0.015039]
	Learning Rate: 0.0150394
	LOSS [training: 0.436492515827736 | validation: 0.4905070059800347]
	TIME [epoch: 41.2 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44571419161854375		[learning rate: 0.015025]
	Learning Rate: 0.0150251
	LOSS [training: 0.44571419161854375 | validation: 0.42534336488343205]
	TIME [epoch: 41.2 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4202202392338099		[learning rate: 0.015011]
	Learning Rate: 0.0150108
	LOSS [training: 0.4202202392338099 | validation: 0.4549963561162156]
	TIME [epoch: 41.2 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4573465295088687		[learning rate: 0.014997]
	Learning Rate: 0.0149965
	LOSS [training: 0.4573465295088687 | validation: 0.3985080287993944]
	TIME [epoch: 41.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3699836981661288		[learning rate: 0.014982]
	Learning Rate: 0.0149822
	LOSS [training: 0.3699836981661288 | validation: 0.42939843530909316]
	TIME [epoch: 41.3 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42204931058188		[learning rate: 0.014968]
	Learning Rate: 0.0149679
	LOSS [training: 0.42204931058188 | validation: 0.5027619296366448]
	TIME [epoch: 41.3 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4507706785876153		[learning rate: 0.014954]
	Learning Rate: 0.0149535
	LOSS [training: 0.4507706785876153 | validation: 0.5044780155797739]
	TIME [epoch: 41.2 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4419265598883342		[learning rate: 0.014939]
	Learning Rate: 0.0149392
	LOSS [training: 0.4419265598883342 | validation: 0.383224915980688]
	TIME [epoch: 41.2 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4335107180081309		[learning rate: 0.014925]
	Learning Rate: 0.0149248
	LOSS [training: 0.4335107180081309 | validation: 0.5201175052927766]
	TIME [epoch: 41.2 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4890161840872482		[learning rate: 0.01491]
	Learning Rate: 0.0149104
	LOSS [training: 0.4890161840872482 | validation: 0.41741360882332534]
	TIME [epoch: 41.3 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36052669001163734		[learning rate: 0.014896]
	Learning Rate: 0.014896
	LOSS [training: 0.36052669001163734 | validation: 0.4179690675612425]
	TIME [epoch: 41.3 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4214390031219909		[learning rate: 0.014882]
	Learning Rate: 0.0148816
	LOSS [training: 0.4214390031219909 | validation: 0.470344955724226]
	TIME [epoch: 41.3 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42826154873800804		[learning rate: 0.014867]
	Learning Rate: 0.0148671
	LOSS [training: 0.42826154873800804 | validation: 0.4194187809714788]
	TIME [epoch: 41.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45425368172502417		[learning rate: 0.014853]
	Learning Rate: 0.0148527
	LOSS [training: 0.45425368172502417 | validation: 0.40046009042937325]
	TIME [epoch: 41.2 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40827925273487176		[learning rate: 0.014838]
	Learning Rate: 0.0148382
	LOSS [training: 0.40827925273487176 | validation: 0.36738632804041715]
	TIME [epoch: 41.2 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38691507677618386		[learning rate: 0.014824]
	Learning Rate: 0.0148237
	LOSS [training: 0.38691507677618386 | validation: 0.3730574614208768]
	TIME [epoch: 41.3 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4106120000547516		[learning rate: 0.014809]
	Learning Rate: 0.0148093
	LOSS [training: 0.4106120000547516 | validation: 0.5515527023153521]
	TIME [epoch: 41.2 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45462940017881137		[learning rate: 0.014795]
	Learning Rate: 0.0147948
	LOSS [training: 0.45462940017881137 | validation: 0.4613917572530297]
	TIME [epoch: 41.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46763875525087223		[learning rate: 0.01478]
	Learning Rate: 0.0147803
	LOSS [training: 0.46763875525087223 | validation: 0.5497421149232463]
	TIME [epoch: 41.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4114029872849525		[learning rate: 0.014766]
	Learning Rate: 0.0147657
	LOSS [training: 0.4114029872849525 | validation: 0.40788445820398567]
	TIME [epoch: 41.2 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40199749039287436		[learning rate: 0.014751]
	Learning Rate: 0.0147512
	LOSS [training: 0.40199749039287436 | validation: 0.40630787502510013]
	TIME [epoch: 41.3 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42799823694595457		[learning rate: 0.014737]
	Learning Rate: 0.0147366
	LOSS [training: 0.42799823694595457 | validation: 0.4158606925543056]
	TIME [epoch: 41.3 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38072274317231347		[learning rate: 0.014722]
	Learning Rate: 0.0147221
	LOSS [training: 0.38072274317231347 | validation: 0.4475666157894502]
	TIME [epoch: 41.3 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37842728531769393		[learning rate: 0.014707]
	Learning Rate: 0.0147075
	LOSS [training: 0.37842728531769393 | validation: 0.4362860842371381]
	TIME [epoch: 41.2 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46984436029231735		[learning rate: 0.014693]
	Learning Rate: 0.0146929
	LOSS [training: 0.46984436029231735 | validation: 0.3978283014236228]
	TIME [epoch: 41.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3859992724080754		[learning rate: 0.014678]
	Learning Rate: 0.0146783
	LOSS [training: 0.3859992724080754 | validation: 0.4820825423910098]
	TIME [epoch: 41.2 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4112024215927905		[learning rate: 0.014664]
	Learning Rate: 0.0146637
	LOSS [training: 0.4112024215927905 | validation: 0.4618660473187206]
	TIME [epoch: 41.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.416419707824394		[learning rate: 0.014649]
	Learning Rate: 0.0146491
	LOSS [training: 0.416419707824394 | validation: 0.4031879255498854]
	TIME [epoch: 41.2 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42913464510261545		[learning rate: 0.014634]
	Learning Rate: 0.0146344
	LOSS [training: 0.42913464510261545 | validation: 0.4695181745043691]
	TIME [epoch: 41.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4141311117590116		[learning rate: 0.01462]
	Learning Rate: 0.0146198
	LOSS [training: 0.4141311117590116 | validation: 0.4730680382752455]
	TIME [epoch: 41.3 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38522625754021217		[learning rate: 0.014605]
	Learning Rate: 0.0146051
	LOSS [training: 0.38522625754021217 | validation: 0.43037063017550536]
	TIME [epoch: 41.2 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46497120293105265		[learning rate: 0.01459]
	Learning Rate: 0.0145904
	LOSS [training: 0.46497120293105265 | validation: 0.4079180685231628]
	TIME [epoch: 41.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39930865204640686		[learning rate: 0.014576]
	Learning Rate: 0.0145757
	LOSS [training: 0.39930865204640686 | validation: 0.44756704391206603]
	TIME [epoch: 41.2 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4201530556502831		[learning rate: 0.014561]
	Learning Rate: 0.014561
	LOSS [training: 0.4201530556502831 | validation: 0.4602781204299024]
	TIME [epoch: 41.2 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4725148035058991		[learning rate: 0.014546]
	Learning Rate: 0.0145463
	LOSS [training: 0.4725148035058991 | validation: 0.3891032196276024]
	TIME [epoch: 41.2 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3987104834669063		[learning rate: 0.014532]
	Learning Rate: 0.0145316
	LOSS [training: 0.3987104834669063 | validation: 0.44478969128565715]
	TIME [epoch: 41.2 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38652627041473764		[learning rate: 0.014517]
	Learning Rate: 0.0145168
	LOSS [training: 0.38652627041473764 | validation: 0.48532861789859527]
	TIME [epoch: 41.2 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.446869715558091		[learning rate: 0.014502]
	Learning Rate: 0.0145021
	LOSS [training: 0.446869715558091 | validation: 0.4907347960285444]
	TIME [epoch: 41.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.408269719235524		[learning rate: 0.014487]
	Learning Rate: 0.0144873
	LOSS [training: 0.408269719235524 | validation: 0.4080126970091633]
	TIME [epoch: 41.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38338876732016336		[learning rate: 0.014473]
	Learning Rate: 0.0144726
	LOSS [training: 0.38338876732016336 | validation: 0.5455677151049751]
	TIME [epoch: 41.2 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5079358709789566		[learning rate: 0.014458]
	Learning Rate: 0.0144578
	LOSS [training: 0.5079358709789566 | validation: 0.39320314314694527]
	TIME [epoch: 41.2 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35071334403204446		[learning rate: 0.014443]
	Learning Rate: 0.014443
	LOSS [training: 0.35071334403204446 | validation: 0.3915581705497585]
	TIME [epoch: 41.2 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4287089955935683		[learning rate: 0.014428]
	Learning Rate: 0.0144281
	LOSS [training: 0.4287089955935683 | validation: 0.41425336985290684]
	TIME [epoch: 41.2 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40515312536218606		[learning rate: 0.014413]
	Learning Rate: 0.0144133
	LOSS [training: 0.40515312536218606 | validation: 0.5842844461060251]
	TIME [epoch: 41.2 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41889979087521934		[learning rate: 0.014398]
	Learning Rate: 0.0143985
	LOSS [training: 0.41889979087521934 | validation: 0.39965722063663756]
	TIME [epoch: 41.2 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37592684998169185		[learning rate: 0.014384]
	Learning Rate: 0.0143836
	LOSS [training: 0.37592684998169185 | validation: 0.4567768699301842]
	TIME [epoch: 41.2 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4121209152678369		[learning rate: 0.014369]
	Learning Rate: 0.0143688
	LOSS [training: 0.4121209152678369 | validation: 0.38283593744935357]
	TIME [epoch: 41.2 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3971803058348021		[learning rate: 0.014354]
	Learning Rate: 0.0143539
	LOSS [training: 0.3971803058348021 | validation: 0.4364101009213138]
	TIME [epoch: 41.3 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4641628670324867		[learning rate: 0.014339]
	Learning Rate: 0.014339
	LOSS [training: 0.4641628670324867 | validation: 0.3949379754207787]
	TIME [epoch: 41.2 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4021519781645207		[learning rate: 0.014324]
	Learning Rate: 0.0143241
	LOSS [training: 0.4021519781645207 | validation: 0.39721742835661045]
	TIME [epoch: 41.2 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36235858848704344		[learning rate: 0.014309]
	Learning Rate: 0.0143092
	LOSS [training: 0.36235858848704344 | validation: 0.5657154442774486]
	TIME [epoch: 41.2 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43590775000554904		[learning rate: 0.014294]
	Learning Rate: 0.0142943
	LOSS [training: 0.43590775000554904 | validation: 0.48453107440888343]
	TIME [epoch: 41.3 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38842059750977465		[learning rate: 0.014279]
	Learning Rate: 0.0142793
	LOSS [training: 0.38842059750977465 | validation: 0.45055026656874986]
	TIME [epoch: 41.2 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45449650445297485		[learning rate: 0.014264]
	Learning Rate: 0.0142644
	LOSS [training: 0.45449650445297485 | validation: 0.3810583716682856]
	TIME [epoch: 41.2 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3516500076431487		[learning rate: 0.014249]
	Learning Rate: 0.0142494
	LOSS [training: 0.3516500076431487 | validation: 0.3881137000826546]
	TIME [epoch: 41.2 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4545599182081466		[learning rate: 0.014234]
	Learning Rate: 0.0142345
	LOSS [training: 0.4545599182081466 | validation: 0.49655518826441014]
	TIME [epoch: 41.2 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4585038946413209		[learning rate: 0.014219]
	Learning Rate: 0.0142195
	LOSS [training: 0.4585038946413209 | validation: 0.39464050490118896]
	TIME [epoch: 41.2 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37557842249893225		[learning rate: 0.014205]
	Learning Rate: 0.0142045
	LOSS [training: 0.37557842249893225 | validation: 0.36266273337076005]
	TIME [epoch: 41.3 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35983487122578917		[learning rate: 0.01419]
	Learning Rate: 0.0141895
	LOSS [training: 0.35983487122578917 | validation: 0.5877264089027061]
	TIME [epoch: 41.2 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4931351518491256		[learning rate: 0.014174]
	Learning Rate: 0.0141745
	LOSS [training: 0.4931351518491256 | validation: 0.37741919568634785]
	TIME [epoch: 41.2 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3529494486213598		[learning rate: 0.014159]
	Learning Rate: 0.0141595
	LOSS [training: 0.3529494486213598 | validation: 0.3860970872312153]
	TIME [epoch: 41.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40522564360931257		[learning rate: 0.014144]
	Learning Rate: 0.0141444
	LOSS [training: 0.40522564360931257 | validation: 0.3899873492062713]
	TIME [epoch: 41.2 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36057447938781134		[learning rate: 0.014129]
	Learning Rate: 0.0141294
	LOSS [training: 0.36057447938781134 | validation: 0.422571909848765]
	TIME [epoch: 41.3 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4375785225273639		[learning rate: 0.014114]
	Learning Rate: 0.0141143
	LOSS [training: 0.4375785225273639 | validation: 0.7043443726147751]
	TIME [epoch: 41.2 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4535628122876733		[learning rate: 0.014099]
	Learning Rate: 0.0140992
	LOSS [training: 0.4535628122876733 | validation: 0.36421719109224393]
	TIME [epoch: 41.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3778324360666893		[learning rate: 0.014084]
	Learning Rate: 0.0140842
	LOSS [training: 0.3778324360666893 | validation: 0.3955348515009609]
	TIME [epoch: 41.2 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4012510496048655		[learning rate: 0.014069]
	Learning Rate: 0.0140691
	LOSS [training: 0.4012510496048655 | validation: 0.4239577627422914]
	TIME [epoch: 41.3 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4079308353435173		[learning rate: 0.014054]
	Learning Rate: 0.014054
	LOSS [training: 0.4079308353435173 | validation: 0.3932236743865291]
	TIME [epoch: 41.2 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4062810907677264		[learning rate: 0.014039]
	Learning Rate: 0.0140389
	LOSS [training: 0.4062810907677264 | validation: 0.38581083135301775]
	TIME [epoch: 41.3 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3807606055273292		[learning rate: 0.014024]
	Learning Rate: 0.0140237
	LOSS [training: 0.3807606055273292 | validation: 0.41858310054969905]
	TIME [epoch: 41.2 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41897868398836885		[learning rate: 0.014009]
	Learning Rate: 0.0140086
	LOSS [training: 0.41897868398836885 | validation: 0.411626660141708]
	TIME [epoch: 41.3 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3789268326032318		[learning rate: 0.013993]
	Learning Rate: 0.0139934
	LOSS [training: 0.3789268326032318 | validation: 0.4430094985646074]
	TIME [epoch: 41.1 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3964496691116661		[learning rate: 0.013978]
	Learning Rate: 0.0139783
	LOSS [training: 0.3964496691116661 | validation: 0.5935076405533182]
	TIME [epoch: 41.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7635423050000409		[learning rate: 0.013963]
	Learning Rate: 0.0139631
	LOSS [training: 0.7635423050000409 | validation: 1.4794326365204173]
	TIME [epoch: 41.2 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6944961524552078		[learning rate: 0.013948]
	Learning Rate: 0.0139479
	LOSS [training: 0.6944961524552078 | validation: 0.3808647104432601]
	TIME [epoch: 41.2 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34573143608915113		[learning rate: 0.013933]
	Learning Rate: 0.0139327
	LOSS [training: 0.34573143608915113 | validation: 0.36241671060781305]
	TIME [epoch: 41.3 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3579605483897408		[learning rate: 0.013918]
	Learning Rate: 0.0139175
	LOSS [training: 0.3579605483897408 | validation: 0.9020166429339314]
	TIME [epoch: 41.2 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4951742565462439		[learning rate: 0.013902]
	Learning Rate: 0.0139023
	LOSS [training: 0.4951742565462439 | validation: 0.36802481436357826]
	TIME [epoch: 41.3 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36343554773535847		[learning rate: 0.013887]
	Learning Rate: 0.0138871
	LOSS [training: 0.36343554773535847 | validation: 0.46322839113883285]
	TIME [epoch: 41.2 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4357935724147773		[learning rate: 0.013872]
	Learning Rate: 0.0138719
	LOSS [training: 0.4357935724147773 | validation: 0.40956894558003965]
	TIME [epoch: 41.2 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3884991523814877		[learning rate: 0.013857]
	Learning Rate: 0.0138566
	LOSS [training: 0.3884991523814877 | validation: 0.40514728128450794]
	TIME [epoch: 41.2 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38106721422605583		[learning rate: 0.013841]
	Learning Rate: 0.0138414
	LOSS [training: 0.38106721422605583 | validation: 0.414074278963116]
	TIME [epoch: 41.2 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37412734300688033		[learning rate: 0.013826]
	Learning Rate: 0.0138261
	LOSS [training: 0.37412734300688033 | validation: 0.42167296837002766]
	TIME [epoch: 41.2 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49305105381603576		[learning rate: 0.013811]
	Learning Rate: 0.0138108
	LOSS [training: 0.49305105381603576 | validation: 0.37217490127022]
	TIME [epoch: 41.2 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3339973095309673		[learning rate: 0.013796]
	Learning Rate: 0.0137955
	LOSS [training: 0.3339973095309673 | validation: 0.4582616032161524]
	TIME [epoch: 41.2 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42966869975518807		[learning rate: 0.01378]
	Learning Rate: 0.0137802
	LOSS [training: 0.42966869975518807 | validation: 0.38670953638714334]
	TIME [epoch: 41.2 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4168592681049331		[learning rate: 0.013765]
	Learning Rate: 0.0137649
	LOSS [training: 0.4168592681049331 | validation: 0.37752066695205133]
	TIME [epoch: 41.2 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3639026044069195		[learning rate: 0.01375]
	Learning Rate: 0.0137496
	LOSS [training: 0.3639026044069195 | validation: 0.458322942701881]
	TIME [epoch: 41.2 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4194187888288567		[learning rate: 0.013734]
	Learning Rate: 0.0137343
	LOSS [training: 0.4194187888288567 | validation: 0.4425355784552071]
	TIME [epoch: 41.3 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35090890006393827		[learning rate: 0.013719]
	Learning Rate: 0.013719
	LOSS [training: 0.35090890006393827 | validation: 0.397858189804128]
	TIME [epoch: 41.2 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40779919572506484		[learning rate: 0.013704]
	Learning Rate: 0.0137036
	LOSS [training: 0.40779919572506484 | validation: 0.38810251428560183]
	TIME [epoch: 41.2 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4736138015834084		[learning rate: 0.013688]
	Learning Rate: 0.0136882
	LOSS [training: 0.4736138015834084 | validation: 0.5108172769173062]
	TIME [epoch: 41.2 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3843001603041038		[learning rate: 0.013673]
	Learning Rate: 0.0136729
	LOSS [training: 0.3843001603041038 | validation: 0.36063246443913777]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_773.pth
	Model improved!!!
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.373605356004786		[learning rate: 0.013657]
	Learning Rate: 0.0136575
	LOSS [training: 0.373605356004786 | validation: 0.4120623866850393]
	TIME [epoch: 41.2 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40096966073642976		[learning rate: 0.013642]
	Learning Rate: 0.0136421
	LOSS [training: 0.40096966073642976 | validation: 0.3906539111492341]
	TIME [epoch: 41.2 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3805898013936353		[learning rate: 0.013627]
	Learning Rate: 0.0136267
	LOSS [training: 0.3805898013936353 | validation: 0.5885436205064598]
	TIME [epoch: 41.2 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4177479354228842		[learning rate: 0.013611]
	Learning Rate: 0.0136113
	LOSS [training: 0.4177479354228842 | validation: 0.3478704949400476]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_777.pth
	Model improved!!!
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39723998021143664		[learning rate: 0.013596]
	Learning Rate: 0.0135959
	LOSS [training: 0.39723998021143664 | validation: 0.4194369341630049]
	TIME [epoch: 41.2 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41762637077034276		[learning rate: 0.01358]
	Learning Rate: 0.0135805
	LOSS [training: 0.41762637077034276 | validation: 0.47718699579067125]
	TIME [epoch: 41.2 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4055851545118768		[learning rate: 0.013565]
	Learning Rate: 0.013565
	LOSS [training: 0.4055851545118768 | validation: 0.3878175267833719]
	TIME [epoch: 41.2 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34844850040218384		[learning rate: 0.01355]
	Learning Rate: 0.0135496
	LOSS [training: 0.34844850040218384 | validation: 0.3826526751722043]
	TIME [epoch: 41.2 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42071364925211796		[learning rate: 0.013534]
	Learning Rate: 0.0135341
	LOSS [training: 0.42071364925211796 | validation: 0.5596200894127836]
	TIME [epoch: 41.3 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4217136750113796		[learning rate: 0.013519]
	Learning Rate: 0.0135186
	LOSS [training: 0.4217136750113796 | validation: 0.3675112764729228]
	TIME [epoch: 41.3 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3805342810032682		[learning rate: 0.013503]
	Learning Rate: 0.0135032
	LOSS [training: 0.3805342810032682 | validation: 0.3894602410294211]
	TIME [epoch: 41.2 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3948822413658691		[learning rate: 0.013488]
	Learning Rate: 0.0134877
	LOSS [training: 0.3948822413658691 | validation: 0.4678715562712201]
	TIME [epoch: 41.2 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3643937602334611		[learning rate: 0.013472]
	Learning Rate: 0.0134722
	LOSS [training: 0.3643937602334611 | validation: 0.4418381754362878]
	TIME [epoch: 41.2 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4608677707905849		[learning rate: 0.013457]
	Learning Rate: 0.0134567
	LOSS [training: 0.4608677707905849 | validation: 0.39415293146319613]
	TIME [epoch: 41.2 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.505007245406733		[learning rate: 0.013441]
	Learning Rate: 0.0134412
	LOSS [training: 0.505007245406733 | validation: 0.4080719510209201]
	TIME [epoch: 41.2 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38860044884783695		[learning rate: 0.013426]
	Learning Rate: 0.0134256
	LOSS [training: 0.38860044884783695 | validation: 0.3371703134074507]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3312021508393804		[learning rate: 0.01341]
	Learning Rate: 0.0134101
	LOSS [training: 0.3312021508393804 | validation: 0.3809951178795905]
	TIME [epoch: 41.2 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40980528608067185		[learning rate: 0.013395]
	Learning Rate: 0.0133946
	LOSS [training: 0.40980528608067185 | validation: 0.522167276272051]
	TIME [epoch: 41.2 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4316556540704606		[learning rate: 0.013379]
	Learning Rate: 0.013379
	LOSS [training: 0.4316556540704606 | validation: 0.35947767847347395]
	TIME [epoch: 41.3 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37533919313248926		[learning rate: 0.013363]
	Learning Rate: 0.0133635
	LOSS [training: 0.37533919313248926 | validation: 0.3571671966156761]
	TIME [epoch: 41.2 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3967636143340778		[learning rate: 0.013348]
	Learning Rate: 0.0133479
	LOSS [training: 0.3967636143340778 | validation: 0.41818580634171165]
	TIME [epoch: 41.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39068659532446853		[learning rate: 0.013332]
	Learning Rate: 0.0133323
	LOSS [training: 0.39068659532446853 | validation: 0.4014944027761148]
	TIME [epoch: 41.2 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3787866698104442		[learning rate: 0.013317]
	Learning Rate: 0.0133167
	LOSS [training: 0.3787866698104442 | validation: 0.43571730995533425]
	TIME [epoch: 41.3 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40900377494866613		[learning rate: 0.013301]
	Learning Rate: 0.0133011
	LOSS [training: 0.40900377494866613 | validation: 0.43691279550791795]
	TIME [epoch: 41.2 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37789998138911485		[learning rate: 0.013286]
	Learning Rate: 0.0132855
	LOSS [training: 0.37789998138911485 | validation: 0.413876896447358]
	TIME [epoch: 41.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3920291035858511		[learning rate: 0.01327]
	Learning Rate: 0.0132699
	LOSS [training: 0.3920291035858511 | validation: 0.49230983310193766]
	TIME [epoch: 41.2 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44529006530985554		[learning rate: 0.013254]
	Learning Rate: 0.0132543
	LOSS [training: 0.44529006530985554 | validation: 0.40124455203540194]
	TIME [epoch: 41.2 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38955710456295267		[learning rate: 0.013239]
	Learning Rate: 0.0132386
	LOSS [training: 0.38955710456295267 | validation: 0.48287457481728613]
	TIME [epoch: 41.2 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3790819192466365		[learning rate: 0.013223]
	Learning Rate: 0.013223
	LOSS [training: 0.3790819192466365 | validation: 0.33241536227329727]
	TIME [epoch: 41.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_802.pth
	Model improved!!!
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3639635648946709		[learning rate: 0.013207]
	Learning Rate: 0.0132074
	LOSS [training: 0.3639635648946709 | validation: 0.52707427016882]
	TIME [epoch: 41.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38730111253167665		[learning rate: 0.013192]
	Learning Rate: 0.0131917
	LOSS [training: 0.38730111253167665 | validation: 0.38525529907478784]
	TIME [epoch: 41.3 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3829701078282003		[learning rate: 0.013176]
	Learning Rate: 0.013176
	LOSS [training: 0.3829701078282003 | validation: 0.4568649551980485]
	TIME [epoch: 41.2 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36755790644363623		[learning rate: 0.01316]
	Learning Rate: 0.0131603
	LOSS [training: 0.36755790644363623 | validation: 0.3690755320621131]
	TIME [epoch: 41.2 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.459207410424041		[learning rate: 0.013145]
	Learning Rate: 0.0131447
	LOSS [training: 0.459207410424041 | validation: 0.4164565047796903]
	TIME [epoch: 41.3 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3635522235008294		[learning rate: 0.013129]
	Learning Rate: 0.013129
	LOSS [training: 0.3635522235008294 | validation: 0.3854412659552393]
	TIME [epoch: 41.3 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39167132611736155		[learning rate: 0.013113]
	Learning Rate: 0.0131133
	LOSS [training: 0.39167132611736155 | validation: 0.3519036491827954]
	TIME [epoch: 41.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36200291525699		[learning rate: 0.013098]
	Learning Rate: 0.0130976
	LOSS [training: 0.36200291525699 | validation: 0.38800243075720925]
	TIME [epoch: 41.2 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3960744937346765		[learning rate: 0.013082]
	Learning Rate: 0.0130818
	LOSS [training: 0.3960744937346765 | validation: 0.3517538848941824]
	TIME [epoch: 41.2 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34606959689247513		[learning rate: 0.013066]
	Learning Rate: 0.0130661
	LOSS [training: 0.34606959689247513 | validation: 0.3658289660681342]
	TIME [epoch: 41.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4162982780659007		[learning rate: 0.01305]
	Learning Rate: 0.0130504
	LOSS [training: 0.4162982780659007 | validation: 0.3996216537843759]
	TIME [epoch: 41.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4221600888444471		[learning rate: 0.013035]
	Learning Rate: 0.0130346
	LOSS [training: 0.4221600888444471 | validation: 0.36663632032545534]
	TIME [epoch: 41.2 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35163372666433335		[learning rate: 0.013019]
	Learning Rate: 0.0130189
	LOSS [training: 0.35163372666433335 | validation: 0.4000488703509574]
	TIME [epoch: 41.2 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39474512468368145		[learning rate: 0.013003]
	Learning Rate: 0.0130031
	LOSS [training: 0.39474512468368145 | validation: 0.37924247233307273]
	TIME [epoch: 41.2 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40269096645129054		[learning rate: 0.012987]
	Learning Rate: 0.0129873
	LOSS [training: 0.40269096645129054 | validation: 0.40129917661506187]
	TIME [epoch: 41.3 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.350588319260884		[learning rate: 0.012972]
	Learning Rate: 0.0129716
	LOSS [training: 0.350588319260884 | validation: 0.3359614914139619]
	TIME [epoch: 41.2 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3983253583685624		[learning rate: 0.012956]
	Learning Rate: 0.0129558
	LOSS [training: 0.3983253583685624 | validation: 0.38238378019958674]
	TIME [epoch: 41.2 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.388847873454474		[learning rate: 0.01294]
	Learning Rate: 0.01294
	LOSS [training: 0.388847873454474 | validation: 0.3436755275995832]
	TIME [epoch: 41.2 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.602491757441052		[learning rate: 0.012924]
	Learning Rate: 0.0129242
	LOSS [training: 0.602491757441052 | validation: 0.3685341386842832]
	TIME [epoch: 41.2 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33812416259303907		[learning rate: 0.012908]
	Learning Rate: 0.0129084
	LOSS [training: 0.33812416259303907 | validation: 0.3638988251208376]
	TIME [epoch: 41.2 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3325725645825462		[learning rate: 0.012893]
	Learning Rate: 0.0128926
	LOSS [training: 0.3325725645825462 | validation: 0.2937955425301424]
	TIME [epoch: 41.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_823.pth
	Model improved!!!
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3434409019119855		[learning rate: 0.012877]
	Learning Rate: 0.0128767
	LOSS [training: 0.3434409019119855 | validation: 0.3086603024136935]
	TIME [epoch: 41.3 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29336457843042557		[learning rate: 0.012861]
	Learning Rate: 0.0128609
	LOSS [training: 0.29336457843042557 | validation: 0.295533274413922]
	TIME [epoch: 41.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3329245068482246		[learning rate: 0.012845]
	Learning Rate: 0.0128451
	LOSS [training: 0.3329245068482246 | validation: 0.38601239356761996]
	TIME [epoch: 41.3 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34558163995742774		[learning rate: 0.012829]
	Learning Rate: 0.0128292
	LOSS [training: 0.34558163995742774 | validation: 0.3098737579838611]
	TIME [epoch: 41.3 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3608780337734113		[learning rate: 0.012813]
	Learning Rate: 0.0128133
	LOSS [training: 0.3608780337734113 | validation: 0.32252055265244395]
	TIME [epoch: 41.3 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.368375734666527		[learning rate: 0.012797]
	Learning Rate: 0.0127975
	LOSS [training: 0.368375734666527 | validation: 0.33931667001133775]
	TIME [epoch: 41.3 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31027655127977843		[learning rate: 0.012782]
	Learning Rate: 0.0127816
	LOSS [training: 0.31027655127977843 | validation: 0.31359275548413656]
	TIME [epoch: 41.3 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39637966069126696		[learning rate: 0.012766]
	Learning Rate: 0.0127657
	LOSS [training: 0.39637966069126696 | validation: 0.3728147319887317]
	TIME [epoch: 41.2 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36499654081917504		[learning rate: 0.01275]
	Learning Rate: 0.0127498
	LOSS [training: 0.36499654081917504 | validation: 0.33050724617930427]
	TIME [epoch: 41.3 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35120411410219976		[learning rate: 0.012734]
	Learning Rate: 0.0127339
	LOSS [training: 0.35120411410219976 | validation: 0.4245938278889859]
	TIME [epoch: 41.3 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4266532202126456		[learning rate: 0.012718]
	Learning Rate: 0.012718
	LOSS [training: 0.4266532202126456 | validation: 0.38018680883667366]
	TIME [epoch: 41.2 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3590484481467912		[learning rate: 0.012702]
	Learning Rate: 0.0127021
	LOSS [training: 0.3590484481467912 | validation: 0.38442954109687566]
	TIME [epoch: 41.3 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3361469932629087		[learning rate: 0.012686]
	Learning Rate: 0.0126862
	LOSS [training: 0.3361469932629087 | validation: 0.322301446276099]
	TIME [epoch: 41.2 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35584158892924356		[learning rate: 0.01267]
	Learning Rate: 0.0126703
	LOSS [training: 0.35584158892924356 | validation: 0.3396942773556764]
	TIME [epoch: 41.3 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3444410902591415		[learning rate: 0.012654]
	Learning Rate: 0.0126544
	LOSS [training: 0.3444410902591415 | validation: 0.4319234590168103]
	TIME [epoch: 41.2 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.449867486186032		[learning rate: 0.012638]
	Learning Rate: 0.0126384
	LOSS [training: 0.449867486186032 | validation: 0.36748850454726]
	TIME [epoch: 41.3 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34010537053341955		[learning rate: 0.012622]
	Learning Rate: 0.0126225
	LOSS [training: 0.34010537053341955 | validation: 0.3547885297321839]
	TIME [epoch: 41.2 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37198028916350284		[learning rate: 0.012607]
	Learning Rate: 0.0126065
	LOSS [training: 0.37198028916350284 | validation: 0.4076961692729282]
	TIME [epoch: 41.3 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38907530760646847		[learning rate: 0.012591]
	Learning Rate: 0.0125906
	LOSS [training: 0.38907530760646847 | validation: 0.37279094810478036]
	TIME [epoch: 41.2 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33922235951746155		[learning rate: 0.012575]
	Learning Rate: 0.0125746
	LOSS [training: 0.33922235951746155 | validation: 0.43765661920964805]
	TIME [epoch: 41.3 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43065326817766303		[learning rate: 0.012559]
	Learning Rate: 0.0125586
	LOSS [training: 0.43065326817766303 | validation: 0.35990320310468926]
	TIME [epoch: 41.3 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35753807562448675		[learning rate: 0.012543]
	Learning Rate: 0.0125426
	LOSS [training: 0.35753807562448675 | validation: 0.43114545359372036]
	TIME [epoch: 41.2 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39488980521381634		[learning rate: 0.012527]
	Learning Rate: 0.0125267
	LOSS [training: 0.39488980521381634 | validation: 0.37967290498225936]
	TIME [epoch: 41.3 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34774701159153465		[learning rate: 0.012511]
	Learning Rate: 0.0125107
	LOSS [training: 0.34774701159153465 | validation: 0.41103512572725037]
	TIME [epoch: 41.3 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41511394213334396		[learning rate: 0.012495]
	Learning Rate: 0.0124947
	LOSS [training: 0.41511394213334396 | validation: 0.47347097894820483]
	TIME [epoch: 41.2 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6708961118829071		[learning rate: 0.012479]
	Learning Rate: 0.0124786
	LOSS [training: 0.6708961118829071 | validation: 0.4081484925471749]
	TIME [epoch: 41.3 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36383920118742646		[learning rate: 0.012463]
	Learning Rate: 0.0124626
	LOSS [training: 0.36383920118742646 | validation: 0.6221997293364985]
	TIME [epoch: 41.2 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4976877290081875		[learning rate: 0.012447]
	Learning Rate: 0.0124466
	LOSS [training: 0.4976877290081875 | validation: 0.3708777486600461]
	TIME [epoch: 41.3 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38128413660688576		[learning rate: 0.012431]
	Learning Rate: 0.0124306
	LOSS [training: 0.38128413660688576 | validation: 0.37317034925098175]
	TIME [epoch: 41.3 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34686152035075524		[learning rate: 0.012415]
	Learning Rate: 0.0124145
	LOSS [training: 0.34686152035075524 | validation: 0.3916465688918691]
	TIME [epoch: 41.2 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3949022148300098		[learning rate: 0.012399]
	Learning Rate: 0.0123985
	LOSS [training: 0.3949022148300098 | validation: 0.40293989851640755]
	TIME [epoch: 41.3 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3760069356177087		[learning rate: 0.012382]
	Learning Rate: 0.0123825
	LOSS [training: 0.3760069356177087 | validation: 0.41972745327495736]
	TIME [epoch: 41.2 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3916615927179876		[learning rate: 0.012366]
	Learning Rate: 0.0123664
	LOSS [training: 0.3916615927179876 | validation: 0.4530618819858143]
	TIME [epoch: 41.2 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36300259703911236		[learning rate: 0.01235]
	Learning Rate: 0.0123503
	LOSS [training: 0.36300259703911236 | validation: 0.39512885722093705]
	TIME [epoch: 41.2 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3786422410696545		[learning rate: 0.012334]
	Learning Rate: 0.0123343
	LOSS [training: 0.3786422410696545 | validation: 0.3185442007118039]
	TIME [epoch: 41.3 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41325547510438915		[learning rate: 0.012318]
	Learning Rate: 0.0123182
	LOSS [training: 0.41325547510438915 | validation: 0.36853872193487197]
	TIME [epoch: 41.3 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3604292864975628		[learning rate: 0.012302]
	Learning Rate: 0.0123021
	LOSS [training: 0.3604292864975628 | validation: 0.3625989832732208]
	TIME [epoch: 41.2 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38673948783540546		[learning rate: 0.012286]
	Learning Rate: 0.012286
	LOSS [training: 0.38673948783540546 | validation: 0.40313290122296563]
	TIME [epoch: 41.2 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3778221365023132		[learning rate: 0.01227]
	Learning Rate: 0.0122699
	LOSS [training: 0.3778221365023132 | validation: 0.38211434654104237]
	TIME [epoch: 41.3 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3857779262505767		[learning rate: 0.012254]
	Learning Rate: 0.0122538
	LOSS [training: 0.3857779262505767 | validation: 0.3936071560912116]
	TIME [epoch: 41.3 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41673663537108296		[learning rate: 0.012238]
	Learning Rate: 0.0122377
	LOSS [training: 0.41673663537108296 | validation: 0.4379169360267096]
	TIME [epoch: 41.3 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4058323192533561		[learning rate: 0.012222]
	Learning Rate: 0.0122216
	LOSS [training: 0.4058323192533561 | validation: 0.3410050738216401]
	TIME [epoch: 41.3 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36980336891835075		[learning rate: 0.012205]
	Learning Rate: 0.0122055
	LOSS [training: 0.36980336891835075 | validation: 0.4144888134922463]
	TIME [epoch: 41.2 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36393220204853804		[learning rate: 0.012189]
	Learning Rate: 0.0121894
	LOSS [training: 0.36393220204853804 | validation: 0.35289509814253817]
	TIME [epoch: 41.3 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47431799328837143		[learning rate: 0.012173]
	Learning Rate: 0.0121732
	LOSS [training: 0.47431799328837143 | validation: 0.8878947265982301]
	TIME [epoch: 41.2 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6721486422456427		[learning rate: 0.012157]
	Learning Rate: 0.0121571
	LOSS [training: 0.6721486422456427 | validation: 0.7045129424429191]
	TIME [epoch: 41.3 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5244872610433072		[learning rate: 0.012141]
	Learning Rate: 0.012141
	LOSS [training: 0.5244872610433072 | validation: 0.4102655903437604]
	TIME [epoch: 41.3 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3748458297392866		[learning rate: 0.012125]
	Learning Rate: 0.0121248
	LOSS [training: 0.3748458297392866 | validation: 0.3560138440185466]
	TIME [epoch: 41.3 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37494897916187053		[learning rate: 0.012109]
	Learning Rate: 0.0121087
	LOSS [training: 0.37494897916187053 | validation: 0.3521941414805869]
	TIME [epoch: 41.3 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.335251113601496		[learning rate: 0.012093]
	Learning Rate: 0.0120925
	LOSS [training: 0.335251113601496 | validation: 0.4188328173308795]
	TIME [epoch: 41.2 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38838663441090404		[learning rate: 0.012076]
	Learning Rate: 0.0120763
	LOSS [training: 0.38838663441090404 | validation: 0.35621242703704836]
	TIME [epoch: 41.3 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3551744459618017		[learning rate: 0.01206]
	Learning Rate: 0.0120602
	LOSS [training: 0.3551744459618017 | validation: 0.32842116723576137]
	TIME [epoch: 41.3 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3768670687046657		[learning rate: 0.012044]
	Learning Rate: 0.012044
	LOSS [training: 0.3768670687046657 | validation: 0.34263246507639455]
	TIME [epoch: 41.3 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35017959185828984		[learning rate: 0.012028]
	Learning Rate: 0.0120278
	LOSS [training: 0.35017959185828984 | validation: 0.48180421509799665]
	TIME [epoch: 41.2 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36062272269027174		[learning rate: 0.012012]
	Learning Rate: 0.0120116
	LOSS [training: 0.36062272269027174 | validation: 0.3481032624802088]
	TIME [epoch: 41.2 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4347849128530577		[learning rate: 0.011995]
	Learning Rate: 0.0119954
	LOSS [training: 0.4347849128530577 | validation: 0.38738810508595817]
	TIME [epoch: 41.3 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3690063461631372		[learning rate: 0.011979]
	Learning Rate: 0.0119792
	LOSS [training: 0.3690063461631372 | validation: 0.43301398045896555]
	TIME [epoch: 41.3 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38821689395303		[learning rate: 0.011963]
	Learning Rate: 0.011963
	LOSS [training: 0.38821689395303 | validation: 0.3255611879796017]
	TIME [epoch: 41.3 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3457578849755746		[learning rate: 0.011947]
	Learning Rate: 0.0119468
	LOSS [training: 0.3457578849755746 | validation: 0.4207458415388053]
	TIME [epoch: 41.3 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4185078046140358		[learning rate: 0.011931]
	Learning Rate: 0.0119306
	LOSS [training: 0.4185078046140358 | validation: 0.3584253357793813]
	TIME [epoch: 41.3 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3486646027647223		[learning rate: 0.011914]
	Learning Rate: 0.0119144
	LOSS [training: 0.3486646027647223 | validation: 0.38348748884941186]
	TIME [epoch: 41.3 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4197607076741637		[learning rate: 0.011898]
	Learning Rate: 0.0118982
	LOSS [training: 0.4197607076741637 | validation: 0.3669053713456637]
	TIME [epoch: 41.3 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33473820594982434		[learning rate: 0.011882]
	Learning Rate: 0.0118819
	LOSS [training: 0.33473820594982434 | validation: 0.42510415712021865]
	TIME [epoch: 41.2 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37774988392967257		[learning rate: 0.011866]
	Learning Rate: 0.0118657
	LOSS [training: 0.37774988392967257 | validation: 0.41918412602678945]
	TIME [epoch: 41.3 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4223680902793845		[learning rate: 0.011849]
	Learning Rate: 0.0118495
	LOSS [training: 0.4223680902793845 | validation: 0.3888821269314089]
	TIME [epoch: 41.2 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3947302808203941		[learning rate: 0.011833]
	Learning Rate: 0.0118332
	LOSS [training: 0.3947302808203941 | validation: 0.3106170830870796]
	TIME [epoch: 41.2 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3835145851601087		[learning rate: 0.011817]
	Learning Rate: 0.011817
	LOSS [training: 0.3835145851601087 | validation: 0.4241100750182756]
	TIME [epoch: 41.2 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34490241270816263		[learning rate: 0.011801]
	Learning Rate: 0.0118007
	LOSS [training: 0.34490241270816263 | validation: 0.37271760338902565]
	TIME [epoch: 41.2 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42033730429880095		[learning rate: 0.011784]
	Learning Rate: 0.0117844
	LOSS [training: 0.42033730429880095 | validation: 0.4908468378830346]
	TIME [epoch: 41.2 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4336531350623532		[learning rate: 0.011768]
	Learning Rate: 0.0117682
	LOSS [training: 0.4336531350623532 | validation: 0.3611574808036646]
	TIME [epoch: 41.2 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3394691907951366		[learning rate: 0.011752]
	Learning Rate: 0.0117519
	LOSS [training: 0.3394691907951366 | validation: 0.35281577160976124]
	TIME [epoch: 41.3 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4224666992441284		[learning rate: 0.011736]
	Learning Rate: 0.0117356
	LOSS [training: 0.4224666992441284 | validation: 0.37862605807203303]
	TIME [epoch: 41.3 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37396025083736		[learning rate: 0.011719]
	Learning Rate: 0.0117194
	LOSS [training: 0.37396025083736 | validation: 0.36436970246509237]
	TIME [epoch: 41.3 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3786679208369599		[learning rate: 0.011703]
	Learning Rate: 0.0117031
	LOSS [training: 0.3786679208369599 | validation: 0.3756078464539367]
	TIME [epoch: 41.3 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3686154222049436		[learning rate: 0.011687]
	Learning Rate: 0.0116868
	LOSS [training: 0.3686154222049436 | validation: 0.4180458511020746]
	TIME [epoch: 41.3 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39268199365995304		[learning rate: 0.01167]
	Learning Rate: 0.0116705
	LOSS [training: 0.39268199365995304 | validation: 0.33758054620401956]
	TIME [epoch: 41.4 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3463332259553008		[learning rate: 0.011654]
	Learning Rate: 0.0116542
	LOSS [training: 0.3463332259553008 | validation: 0.3691309889420755]
	TIME [epoch: 41.3 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38393746149472757		[learning rate: 0.011638]
	Learning Rate: 0.0116379
	LOSS [training: 0.38393746149472757 | validation: 0.3654803851169755]
	TIME [epoch: 41.2 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4072665351315721		[learning rate: 0.011622]
	Learning Rate: 0.0116216
	LOSS [training: 0.4072665351315721 | validation: 0.4286003131591042]
	TIME [epoch: 41.2 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.352805752063062		[learning rate: 0.011605]
	Learning Rate: 0.0116053
	LOSS [training: 0.352805752063062 | validation: 0.37220324556648976]
	TIME [epoch: 41.2 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35984817555822207		[learning rate: 0.011589]
	Learning Rate: 0.011589
	LOSS [training: 0.35984817555822207 | validation: 0.38330016157484964]
	TIME [epoch: 41.2 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3629134144934464		[learning rate: 0.011573]
	Learning Rate: 0.0115726
	LOSS [training: 0.3629134144934464 | validation: 0.41628345058679594]
	TIME [epoch: 41.2 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41282897914469446		[learning rate: 0.011556]
	Learning Rate: 0.0115563
	LOSS [training: 0.41282897914469446 | validation: 1.2117742146953443]
	TIME [epoch: 41.2 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2783212237197388		[learning rate: 0.01154]
	Learning Rate: 0.01154
	LOSS [training: 1.2783212237197388 | validation: 1.1157630803891005]
	TIME [epoch: 41.3 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7371716216707024		[learning rate: 0.011524]
	Learning Rate: 0.0115237
	LOSS [training: 0.7371716216707024 | validation: 0.49271960482988386]
	TIME [epoch: 41.3 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4028615150145244		[learning rate: 0.011507]
	Learning Rate: 0.0115073
	LOSS [training: 0.4028615150145244 | validation: 0.38685485941359404]
	TIME [epoch: 41.3 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34317578071576504		[learning rate: 0.011491]
	Learning Rate: 0.011491
	LOSS [training: 0.34317578071576504 | validation: 0.32309599845180326]
	TIME [epoch: 41.2 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3335233115409971		[learning rate: 0.011475]
	Learning Rate: 0.0114746
	LOSS [training: 0.3335233115409971 | validation: 0.32566472597770535]
	TIME [epoch: 41.2 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32413612219759236		[learning rate: 0.011458]
	Learning Rate: 0.0114583
	LOSS [training: 0.32413612219759236 | validation: 0.3276542076540897]
	TIME [epoch: 41.2 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3353345965456125		[learning rate: 0.011442]
	Learning Rate: 0.0114419
	LOSS [training: 0.3353345965456125 | validation: 0.32582619775505606]
	TIME [epoch: 41.2 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3242408505849309		[learning rate: 0.011426]
	Learning Rate: 0.0114256
	LOSS [training: 0.3242408505849309 | validation: 0.3078401968876182]
	TIME [epoch: 41.2 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32756702255309794		[learning rate: 0.011409]
	Learning Rate: 0.0114092
	LOSS [training: 0.32756702255309794 | validation: 0.35690506055555493]
	TIME [epoch: 41.2 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35019412898333435		[learning rate: 0.011393]
	Learning Rate: 0.0113929
	LOSS [training: 0.35019412898333435 | validation: 0.34281571079553863]
	TIME [epoch: 41.2 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32503904052374893		[learning rate: 0.011376]
	Learning Rate: 0.0113765
	LOSS [training: 0.32503904052374893 | validation: 0.31987941000060544]
	TIME [epoch: 41.2 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38686305313535374		[learning rate: 0.01136]
	Learning Rate: 0.0113601
	LOSS [training: 0.38686305313535374 | validation: 0.3294530677807088]
	TIME [epoch: 41.2 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3105511786101859		[learning rate: 0.011344]
	Learning Rate: 0.0113437
	LOSS [training: 0.3105511786101859 | validation: 0.3721908739343768]
	TIME [epoch: 41.2 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35965757318686187		[learning rate: 0.011327]
	Learning Rate: 0.0113274
	LOSS [training: 0.35965757318686187 | validation: 0.35370091731322695]
	TIME [epoch: 41.2 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3741443238227312		[learning rate: 0.011311]
	Learning Rate: 0.011311
	LOSS [training: 0.3741443238227312 | validation: 0.3923873554651409]
	TIME [epoch: 41.2 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37210518464862685		[learning rate: 0.011295]
	Learning Rate: 0.0112946
	LOSS [training: 0.37210518464862685 | validation: 0.32585918577659806]
	TIME [epoch: 41.2 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3195488376761033		[learning rate: 0.011278]
	Learning Rate: 0.0112782
	LOSS [training: 0.3195488376761033 | validation: 0.40593312673511783]
	TIME [epoch: 41.2 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4755459509753596		[learning rate: 0.011262]
	Learning Rate: 0.0112618
	LOSS [training: 0.4755459509753596 | validation: 0.42294685198329207]
	TIME [epoch: 41.3 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35415264123711776		[learning rate: 0.011245]
	Learning Rate: 0.0112454
	LOSS [training: 0.35415264123711776 | validation: 0.3258159782339158]
	TIME [epoch: 41.2 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3048847469594913		[learning rate: 0.011229]
	Learning Rate: 0.011229
	LOSS [training: 0.3048847469594913 | validation: 0.3286358039736298]
	TIME [epoch: 41.3 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39142774063012203		[learning rate: 0.011213]
	Learning Rate: 0.0112126
	LOSS [training: 0.39142774063012203 | validation: 0.3289485521375139]
	TIME [epoch: 41.3 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3614660074576332		[learning rate: 0.011196]
	Learning Rate: 0.0111962
	LOSS [training: 0.3614660074576332 | validation: 0.3611367307793196]
	TIME [epoch: 41.2 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3710396956222659		[learning rate: 0.01118]
	Learning Rate: 0.0111798
	LOSS [training: 0.3710396956222659 | validation: 0.377652393864795]
	TIME [epoch: 41.2 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3513196513235968		[learning rate: 0.011163]
	Learning Rate: 0.0111634
	LOSS [training: 0.3513196513235968 | validation: 0.34293606073145844]
	TIME [epoch: 41.3 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34013655414673016		[learning rate: 0.011147]
	Learning Rate: 0.011147
	LOSS [training: 0.34013655414673016 | validation: 0.4063233391784283]
	TIME [epoch: 41.2 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39215090183275436		[learning rate: 0.011131]
	Learning Rate: 0.0111305
	LOSS [training: 0.39215090183275436 | validation: 0.35537781670469426]
	TIME [epoch: 41.3 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34368866513937113		[learning rate: 0.011114]
	Learning Rate: 0.0111141
	LOSS [training: 0.34368866513937113 | validation: 0.35137432911526234]
	TIME [epoch: 41.2 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40075200697457014		[learning rate: 0.011098]
	Learning Rate: 0.0110977
	LOSS [training: 0.40075200697457014 | validation: 0.3313185573535777]
	TIME [epoch: 41.3 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3217374356509463		[learning rate: 0.011081]
	Learning Rate: 0.0110813
	LOSS [training: 0.3217374356509463 | validation: 0.36994049972897425]
	TIME [epoch: 41.2 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4035287915857736		[learning rate: 0.011065]
	Learning Rate: 0.0110648
	LOSS [training: 0.4035287915857736 | validation: 0.36913212196134293]
	TIME [epoch: 41.2 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.348388098452704		[learning rate: 0.011048]
	Learning Rate: 0.0110484
	LOSS [training: 0.348388098452704 | validation: 0.3445245472667009]
	TIME [epoch: 41.3 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4858283671584698		[learning rate: 0.011032]
	Learning Rate: 0.011032
	LOSS [training: 0.4858283671584698 | validation: 0.4418105188769099]
	TIME [epoch: 41.3 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3493168285674749		[learning rate: 0.011016]
	Learning Rate: 0.0110155
	LOSS [training: 0.3493168285674749 | validation: 0.317995141785157]
	TIME [epoch: 41.2 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32367452338479785		[learning rate: 0.010999]
	Learning Rate: 0.0109991
	LOSS [training: 0.32367452338479785 | validation: 0.41984313897415715]
	TIME [epoch: 41.2 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38001643915197936		[learning rate: 0.010983]
	Learning Rate: 0.0109826
	LOSS [training: 0.38001643915197936 | validation: 0.33689137553845094]
	TIME [epoch: 41.2 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3804522440325709		[learning rate: 0.010966]
	Learning Rate: 0.0109662
	LOSS [training: 0.3804522440325709 | validation: 0.35237021570112304]
	TIME [epoch: 41.2 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32711446265983635		[learning rate: 0.01095]
	Learning Rate: 0.0109497
	LOSS [training: 0.32711446265983635 | validation: 0.4259732068874462]
	TIME [epoch: 41.3 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3381075223052846		[learning rate: 0.010933]
	Learning Rate: 0.0109333
	LOSS [training: 0.3381075223052846 | validation: 0.34607789730358696]
	TIME [epoch: 41.3 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37180627140965605		[learning rate: 0.010917]
	Learning Rate: 0.0109168
	LOSS [training: 0.37180627140965605 | validation: 0.4041854203561003]
	TIME [epoch: 41.2 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3901772618403124		[learning rate: 0.0109]
	Learning Rate: 0.0109004
	LOSS [training: 0.3901772618403124 | validation: 0.3364259059297018]
	TIME [epoch: 41.2 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47697037545640586		[learning rate: 0.010884]
	Learning Rate: 0.0108839
	LOSS [training: 0.47697037545640586 | validation: 0.3213079211362554]
	TIME [epoch: 41.2 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37764647263669465		[learning rate: 0.010867]
	Learning Rate: 0.0108674
	LOSS [training: 0.37764647263669465 | validation: 0.3254765383879187]
	TIME [epoch: 41.2 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.327144799600364		[learning rate: 0.010851]
	Learning Rate: 0.010851
	LOSS [training: 0.327144799600364 | validation: 0.3324168110857634]
	TIME [epoch: 41.2 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3256623042785246		[learning rate: 0.010834]
	Learning Rate: 0.0108345
	LOSS [training: 0.3256623042785246 | validation: 0.33317894825202754]
	TIME [epoch: 41.2 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3360880877716862		[learning rate: 0.010818]
	Learning Rate: 0.010818
	LOSS [training: 0.3360880877716862 | validation: 0.3451003419574955]
	TIME [epoch: 41.2 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3451144510989161		[learning rate: 0.010802]
	Learning Rate: 0.0108016
	LOSS [training: 0.3451144510989161 | validation: 0.3177843181500656]
	TIME [epoch: 41.2 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35488758065934456		[learning rate: 0.010785]
	Learning Rate: 0.0107851
	LOSS [training: 0.35488758065934456 | validation: 0.4506142410156831]
	TIME [epoch: 41.2 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3954494314495358		[learning rate: 0.010769]
	Learning Rate: 0.0107686
	LOSS [training: 0.3954494314495358 | validation: 0.31709866234086126]
	TIME [epoch: 41.2 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32537821112739707		[learning rate: 0.010752]
	Learning Rate: 0.0107521
	LOSS [training: 0.32537821112739707 | validation: 0.33503556488878594]
	TIME [epoch: 41.3 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34517863868604903		[learning rate: 0.010736]
	Learning Rate: 0.0107356
	LOSS [training: 0.34517863868604903 | validation: 0.354374989698787]
	TIME [epoch: 41.2 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3933797620040383		[learning rate: 0.010719]
	Learning Rate: 0.0107192
	LOSS [training: 0.3933797620040383 | validation: 0.4096570087448531]
	TIME [epoch: 41.2 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3642881608172817		[learning rate: 0.010703]
	Learning Rate: 0.0107027
	LOSS [training: 0.3642881608172817 | validation: 0.3118899012863944]
	TIME [epoch: 41.2 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.333137797878695		[learning rate: 0.010686]
	Learning Rate: 0.0106862
	LOSS [training: 0.333137797878695 | validation: 0.33822261289992894]
	TIME [epoch: 41.2 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3402491601976513		[learning rate: 0.01067]
	Learning Rate: 0.0106697
	LOSS [training: 0.3402491601976513 | validation: 0.33207061755439476]
	TIME [epoch: 41.2 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39543490183371016		[learning rate: 0.010653]
	Learning Rate: 0.0106532
	LOSS [training: 0.39543490183371016 | validation: 0.38578246928588333]
	TIME [epoch: 41.2 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3504832558095449		[learning rate: 0.010637]
	Learning Rate: 0.0106367
	LOSS [training: 0.3504832558095449 | validation: 0.36070393152451097]
	TIME [epoch: 41.3 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35387874441564937		[learning rate: 0.01062]
	Learning Rate: 0.0106202
	LOSS [training: 0.35387874441564937 | validation: 0.33971869835735813]
	TIME [epoch: 41.2 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33634755522137166		[learning rate: 0.010604]
	Learning Rate: 0.0106037
	LOSS [training: 0.33634755522137166 | validation: 0.7985983607976708]
	TIME [epoch: 41.2 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5337945372460315		[learning rate: 0.010587]
	Learning Rate: 0.0105872
	LOSS [training: 0.5337945372460315 | validation: 0.4742888998522292]
	TIME [epoch: 41.2 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4376395278552494		[learning rate: 0.010571]
	Learning Rate: 0.0105707
	LOSS [training: 0.4376395278552494 | validation: 1.196451182812055]
	TIME [epoch: 41.2 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5565967306522226		[learning rate: 0.010554]
	Learning Rate: 0.0105542
	LOSS [training: 0.5565967306522226 | validation: 0.3648008707337045]
	TIME [epoch: 41.2 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32501344143133504		[learning rate: 0.010538]
	Learning Rate: 0.0105377
	LOSS [training: 0.32501344143133504 | validation: 0.37345909076074923]
	TIME [epoch: 41.3 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3411639460350516		[learning rate: 0.010521]
	Learning Rate: 0.0105212
	LOSS [training: 0.3411639460350516 | validation: 0.3708295585724507]
	TIME [epoch: 41.2 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34415788204561404		[learning rate: 0.010505]
	Learning Rate: 0.0105047
	LOSS [training: 0.34415788204561404 | validation: 0.3564934524719619]
	TIME [epoch: 41.2 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33852470611300395		[learning rate: 0.010488]
	Learning Rate: 0.0104882
	LOSS [training: 0.33852470611300395 | validation: 0.3554225344485428]
	TIME [epoch: 41.2 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32802310000795315		[learning rate: 0.010472]
	Learning Rate: 0.0104717
	LOSS [training: 0.32802310000795315 | validation: 0.3696548185793434]
	TIME [epoch: 41.2 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35420653203567054		[learning rate: 0.010455]
	Learning Rate: 0.0104552
	LOSS [training: 0.35420653203567054 | validation: 0.3485006376467138]
	TIME [epoch: 41.2 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3954932850847141		[learning rate: 0.010439]
	Learning Rate: 0.0104387
	LOSS [training: 0.3954932850847141 | validation: 0.3573316775601287]
	TIME [epoch: 41.2 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3357566939328878		[learning rate: 0.010422]
	Learning Rate: 0.0104222
	LOSS [training: 0.3357566939328878 | validation: 0.3524078615720274]
	TIME [epoch: 41.2 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35759342316635223		[learning rate: 0.010406]
	Learning Rate: 0.0104057
	LOSS [training: 0.35759342316635223 | validation: 0.3449014680761935]
	TIME [epoch: 41.2 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34331366312577577		[learning rate: 0.010389]
	Learning Rate: 0.0103891
	LOSS [training: 0.34331366312577577 | validation: 0.4062003524675224]
	TIME [epoch: 41.2 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36690039854265144		[learning rate: 0.010373]
	Learning Rate: 0.0103726
	LOSS [training: 0.36690039854265144 | validation: 0.3193829445919837]
	TIME [epoch: 41.2 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3364405805552597		[learning rate: 0.010356]
	Learning Rate: 0.0103561
	LOSS [training: 0.3364405805552597 | validation: 0.37943661734047907]
	TIME [epoch: 41.2 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3587303663872818		[learning rate: 0.01034]
	Learning Rate: 0.0103396
	LOSS [training: 0.3587303663872818 | validation: 0.32334902492553386]
	TIME [epoch: 41.2 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3153993103152959		[learning rate: 0.010323]
	Learning Rate: 0.0103231
	LOSS [training: 0.3153993103152959 | validation: 0.3099892981713839]
	TIME [epoch: 41.3 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3448587174251744		[learning rate: 0.010307]
	Learning Rate: 0.0103066
	LOSS [training: 0.3448587174251744 | validation: 0.421537329459821]
	TIME [epoch: 41.2 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3670919482227628		[learning rate: 0.01029]
	Learning Rate: 0.01029
	LOSS [training: 0.3670919482227628 | validation: 0.43311494563775166]
	TIME [epoch: 41.2 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3868863076257102		[learning rate: 0.010274]
	Learning Rate: 0.0102735
	LOSS [training: 0.3868863076257102 | validation: 0.3292215994622244]
	TIME [epoch: 41.1 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.321025934388898		[learning rate: 0.010257]
	Learning Rate: 0.010257
	LOSS [training: 0.321025934388898 | validation: 0.2880881084604676]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_985.pth
	Model improved!!!
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3491561995766723		[learning rate: 0.01024]
	Learning Rate: 0.0102405
	LOSS [training: 0.3491561995766723 | validation: 0.44958547353103717]
	TIME [epoch: 41.3 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38370927283884126		[learning rate: 0.010224]
	Learning Rate: 0.010224
	LOSS [training: 0.38370927283884126 | validation: 0.3180570709190107]
	TIME [epoch: 41.3 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3553272326420141		[learning rate: 0.010207]
	Learning Rate: 0.0102074
	LOSS [training: 0.3553272326420141 | validation: 0.36264325087782967]
	TIME [epoch: 41.2 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3514126363992237		[learning rate: 0.010191]
	Learning Rate: 0.0101909
	LOSS [training: 0.3514126363992237 | validation: 0.3410391668388141]
	TIME [epoch: 41.2 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3131595941483484		[learning rate: 0.010174]
	Learning Rate: 0.0101744
	LOSS [training: 0.3131595941483484 | validation: 0.4640106347134119]
	TIME [epoch: 41.2 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3679971397988925		[learning rate: 0.010158]
	Learning Rate: 0.0101579
	LOSS [training: 0.3679971397988925 | validation: 0.35797503212451365]
	TIME [epoch: 41.2 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34640709980503104		[learning rate: 0.010141]
	Learning Rate: 0.0101413
	LOSS [training: 0.34640709980503104 | validation: 0.3825612686673773]
	TIME [epoch: 41.2 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.401636250546182		[learning rate: 0.010125]
	Learning Rate: 0.0101248
	LOSS [training: 0.401636250546182 | validation: 0.3683878336976464]
	TIME [epoch: 41.3 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5161963560216851		[learning rate: 0.010108]
	Learning Rate: 0.0101083
	LOSS [training: 0.5161963560216851 | validation: 1.3274261979065758]
	TIME [epoch: 41.2 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.320366278577136		[learning rate: 0.010092]
	Learning Rate: 0.0100918
	LOSS [training: 1.320366278577136 | validation: 1.510858519696931]
	TIME [epoch: 41.2 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3462999473863242		[learning rate: 0.010075]
	Learning Rate: 0.0100752
	LOSS [training: 1.3462999473863242 | validation: 1.2689284693125562]
	TIME [epoch: 41.2 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0641885315415276		[learning rate: 0.010059]
	Learning Rate: 0.0100587
	LOSS [training: 1.0641885315415276 | validation: 0.9530794196665378]
	TIME [epoch: 41.2 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8787679513935106		[learning rate: 0.010042]
	Learning Rate: 0.0100422
	LOSS [training: 0.8787679513935106 | validation: 0.8175631637283239]
	TIME [epoch: 41.2 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.703340930887748		[learning rate: 0.010026]
	Learning Rate: 0.0100257
	LOSS [training: 0.703340930887748 | validation: 0.5748979849815654]
	TIME [epoch: 41.2 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.515482840650943		[learning rate: 0.010009]
	Learning Rate: 0.0100091
	LOSS [training: 0.515482840650943 | validation: 0.5097059607671516]
	TIME [epoch: 41.2 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44748149268750403		[learning rate: 0.0099926]
	Learning Rate: 0.00999261
	LOSS [training: 0.44748149268750403 | validation: 0.46793339454677896]
	TIME [epoch: 186 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4129873052466754		[learning rate: 0.0099761]
	Learning Rate: 0.00997608
	LOSS [training: 0.4129873052466754 | validation: 0.4171711729870922]
	TIME [epoch: 87.8 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5353881903875264		[learning rate: 0.0099596]
	Learning Rate: 0.00995955
	LOSS [training: 0.5353881903875264 | validation: 0.4995935880889303]
	TIME [epoch: 87.8 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46978274443600154		[learning rate: 0.009943]
	Learning Rate: 0.00994303
	LOSS [training: 0.46978274443600154 | validation: 0.46854461329224384]
	TIME [epoch: 87.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4808945644747073		[learning rate: 0.0099265]
	Learning Rate: 0.0099265
	LOSS [training: 0.4808945644747073 | validation: 0.9444014444026102]
	TIME [epoch: 87.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8770288685810965		[learning rate: 0.00991]
	Learning Rate: 0.00990997
	LOSS [training: 0.8770288685810965 | validation: 0.7981779683957084]
	TIME [epoch: 87.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5802820738679028		[learning rate: 0.0098934]
	Learning Rate: 0.00989345
	LOSS [training: 0.5802820738679028 | validation: 0.5266872397572648]
	TIME [epoch: 87.7 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4454522191167715		[learning rate: 0.0098769]
	Learning Rate: 0.00987692
	LOSS [training: 0.4454522191167715 | validation: 0.480378973365344]
	TIME [epoch: 87.7 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39980556550174656		[learning rate: 0.0098604]
	Learning Rate: 0.0098604
	LOSS [training: 0.39980556550174656 | validation: 0.3935248775759519]
	TIME [epoch: 87.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4063323800329832		[learning rate: 0.0098439]
	Learning Rate: 0.00984387
	LOSS [training: 0.4063323800329832 | validation: 0.403733876058196]
	TIME [epoch: 87.7 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37231662417402833		[learning rate: 0.0098274]
	Learning Rate: 0.00982735
	LOSS [training: 0.37231662417402833 | validation: 0.5732480019769479]
	TIME [epoch: 87.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40407001734836934		[learning rate: 0.0098108]
	Learning Rate: 0.00981083
	LOSS [training: 0.40407001734836934 | validation: 0.4034601755010616]
	TIME [epoch: 87.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3661434004868439		[learning rate: 0.0097943]
	Learning Rate: 0.0097943
	LOSS [training: 0.3661434004868439 | validation: 0.3681671143595835]
	TIME [epoch: 87.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.361971423322238		[learning rate: 0.0097778]
	Learning Rate: 0.00977778
	LOSS [training: 0.361971423322238 | validation: 0.4042306682364001]
	TIME [epoch: 87.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3394885233652002		[learning rate: 0.0097613]
	Learning Rate: 0.00976126
	LOSS [training: 0.3394885233652002 | validation: 0.4106655118250251]
	TIME [epoch: 87.6 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37379651655077595		[learning rate: 0.0097447]
	Learning Rate: 0.00974474
	LOSS [training: 0.37379651655077595 | validation: 0.4177061420131154]
	TIME [epoch: 87.6 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34707528896636497		[learning rate: 0.0097282]
	Learning Rate: 0.00972822
	LOSS [training: 0.34707528896636497 | validation: 0.3253163784831785]
	TIME [epoch: 87.7 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34528101020415647		[learning rate: 0.0097117]
	Learning Rate: 0.0097117
	LOSS [training: 0.34528101020415647 | validation: 0.41615463555202487]
	TIME [epoch: 87.6 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36327483160315555		[learning rate: 0.0096952]
	Learning Rate: 0.00969518
	LOSS [training: 0.36327483160315555 | validation: 0.3095737070793976]
	TIME [epoch: 87.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3219892031177132		[learning rate: 0.0096787]
	Learning Rate: 0.00967866
	LOSS [training: 0.3219892031177132 | validation: 0.49687640264213]
	TIME [epoch: 87.7 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3951760217300516		[learning rate: 0.0096621]
	Learning Rate: 0.00966214
	LOSS [training: 0.3951760217300516 | validation: 0.35666607797326655]
	TIME [epoch: 87.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3371869756930743		[learning rate: 0.0096456]
	Learning Rate: 0.00964563
	LOSS [training: 0.3371869756930743 | validation: 0.4149405267541116]
	TIME [epoch: 87.7 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.335022871155218		[learning rate: 0.0096291]
	Learning Rate: 0.00962911
	LOSS [training: 0.335022871155218 | validation: 0.3247787611711489]
	TIME [epoch: 87.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33354603730856713		[learning rate: 0.0096126]
	Learning Rate: 0.0096126
	LOSS [training: 0.33354603730856713 | validation: 0.43034729554084167]
	TIME [epoch: 87.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3592089155835007		[learning rate: 0.0095961]
	Learning Rate: 0.00959609
	LOSS [training: 0.3592089155835007 | validation: 0.2987569234399689]
	TIME [epoch: 87.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30290271837981586		[learning rate: 0.0095796]
	Learning Rate: 0.00957957
	LOSS [training: 0.30290271837981586 | validation: 0.41158316506824033]
	TIME [epoch: 87.9 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3605858780112777		[learning rate: 0.0095631]
	Learning Rate: 0.00956306
	LOSS [training: 0.3605858780112777 | validation: 0.348597142789455]
	TIME [epoch: 88.1 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3553714402079958		[learning rate: 0.0095466]
	Learning Rate: 0.00954655
	LOSS [training: 0.3553714402079958 | validation: 0.32905407479800186]
	TIME [epoch: 88 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34331401963853403		[learning rate: 0.00953]
	Learning Rate: 0.00953004
	LOSS [training: 0.34331401963853403 | validation: 0.356221721477314]
	TIME [epoch: 88 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33915748216830865		[learning rate: 0.0095135]
	Learning Rate: 0.00951354
	LOSS [training: 0.33915748216830865 | validation: 0.30375138834734017]
	TIME [epoch: 88 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33590070377683456		[learning rate: 0.009497]
	Learning Rate: 0.00949703
	LOSS [training: 0.33590070377683456 | validation: 0.4879038597213634]
	TIME [epoch: 88 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3299012103526478		[learning rate: 0.0094805]
	Learning Rate: 0.00948053
	LOSS [training: 0.3299012103526478 | validation: 0.3428061466157572]
	TIME [epoch: 88 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3792649216730104		[learning rate: 0.009464]
	Learning Rate: 0.00946402
	LOSS [training: 0.3792649216730104 | validation: 0.3530549144306211]
	TIME [epoch: 87.9 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3361820227531669		[learning rate: 0.0094475]
	Learning Rate: 0.00944752
	LOSS [training: 0.3361820227531669 | validation: 0.3365194856352288]
	TIME [epoch: 87.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33713105330435106		[learning rate: 0.009431]
	Learning Rate: 0.00943102
	LOSS [training: 0.33713105330435106 | validation: 0.3330861503789687]
	TIME [epoch: 87.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3238026377184195		[learning rate: 0.0094145]
	Learning Rate: 0.00941452
	LOSS [training: 0.3238026377184195 | validation: 0.358418900208935]
	TIME [epoch: 87.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3458521242032582		[learning rate: 0.009398]
	Learning Rate: 0.00939803
	LOSS [training: 0.3458521242032582 | validation: 0.40152562288461724]
	TIME [epoch: 87.8 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32422299778054847		[learning rate: 0.0093815]
	Learning Rate: 0.00938153
	LOSS [training: 0.32422299778054847 | validation: 0.3061515274948446]
	TIME [epoch: 88.1 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3544748414096198		[learning rate: 0.009365]
	Learning Rate: 0.00936504
	LOSS [training: 0.3544748414096198 | validation: 0.3565551067977638]
	TIME [epoch: 88 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33823138795194196		[learning rate: 0.0093485]
	Learning Rate: 0.00934855
	LOSS [training: 0.33823138795194196 | validation: 0.3303252090202812]
	TIME [epoch: 88.2 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3360892800308653		[learning rate: 0.0093321]
	Learning Rate: 0.00933206
	LOSS [training: 0.3360892800308653 | validation: 0.32645528209843405]
	TIME [epoch: 88 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33133485928817874		[learning rate: 0.0093156]
	Learning Rate: 0.00931557
	LOSS [training: 0.33133485928817874 | validation: 0.2983281095224945]
	TIME [epoch: 88.2 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3859552918297614		[learning rate: 0.0092991]
	Learning Rate: 0.00929908
	LOSS [training: 0.3859552918297614 | validation: 0.31665269389202777]
	TIME [epoch: 88 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3130591520048218		[learning rate: 0.0092826]
	Learning Rate: 0.0092826
	LOSS [training: 0.3130591520048218 | validation: 0.3327878849302083]
	TIME [epoch: 88.2 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.356678136568091		[learning rate: 0.0092661]
	Learning Rate: 0.00926612
	LOSS [training: 0.356678136568091 | validation: 0.3647433888202989]
	TIME [epoch: 88 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32296627466254724		[learning rate: 0.0092496]
	Learning Rate: 0.00924964
	LOSS [training: 0.32296627466254724 | validation: 0.3282298660876366]
	TIME [epoch: 88.1 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32453854159482975		[learning rate: 0.0092332]
	Learning Rate: 0.00923316
	LOSS [training: 0.32453854159482975 | validation: 0.42913697060698963]
	TIME [epoch: 88.2 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3867425879554478		[learning rate: 0.0092167]
	Learning Rate: 0.00921668
	LOSS [training: 0.3867425879554478 | validation: 0.33537757098690435]
	TIME [epoch: 88.2 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30700940741666516		[learning rate: 0.0092002]
	Learning Rate: 0.00920021
	LOSS [training: 0.30700940741666516 | validation: 0.31428495502764975]
	TIME [epoch: 88.1 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32446134193747106		[learning rate: 0.0091837]
	Learning Rate: 0.00918374
	LOSS [training: 0.32446134193747106 | validation: 0.383431866429088]
	TIME [epoch: 87.8 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3572484151719708		[learning rate: 0.0091673]
	Learning Rate: 0.00916727
	LOSS [training: 0.3572484151719708 | validation: 0.30346410376092964]
	TIME [epoch: 87.8 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33662968959722683		[learning rate: 0.0091508]
	Learning Rate: 0.0091508
	LOSS [training: 0.33662968959722683 | validation: 0.35210211272801595]
	TIME [epoch: 87.8 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35369260348695225		[learning rate: 0.0091343]
	Learning Rate: 0.00913434
	LOSS [training: 0.35369260348695225 | validation: 0.31193109675783315]
	TIME [epoch: 87.8 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2998515334899304		[learning rate: 0.0091179]
	Learning Rate: 0.00911787
	LOSS [training: 0.2998515334899304 | validation: 0.33049041469922785]
	TIME [epoch: 87.8 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37645798210356934		[learning rate: 0.0091014]
	Learning Rate: 0.00910141
	LOSS [training: 0.37645798210356934 | validation: 0.34404075798698175]
	TIME [epoch: 87.8 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31793596978420163		[learning rate: 0.009085]
	Learning Rate: 0.00908496
	LOSS [training: 0.31793596978420163 | validation: 0.3305839991135097]
	TIME [epoch: 87.8 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32295332695423334		[learning rate: 0.0090685]
	Learning Rate: 0.0090685
	LOSS [training: 0.32295332695423334 | validation: 0.3811474364305857]
	TIME [epoch: 87.8 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34762965039207994		[learning rate: 0.009052]
	Learning Rate: 0.00905205
	LOSS [training: 0.34762965039207994 | validation: 0.34010369144948777]
	TIME [epoch: 87.8 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2942000890047396		[learning rate: 0.0090356]
	Learning Rate: 0.0090356
	LOSS [training: 0.2942000890047396 | validation: 0.3319872081338968]
	TIME [epoch: 87.8 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3632210477864537		[learning rate: 0.0090192]
	Learning Rate: 0.00901915
	LOSS [training: 0.3632210477864537 | validation: 0.33512914588182496]
	TIME [epoch: 87.8 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3067054299770805		[learning rate: 0.0090027]
	Learning Rate: 0.00900271
	LOSS [training: 0.3067054299770805 | validation: 0.3585722197833987]
	TIME [epoch: 87.8 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32019651916780567		[learning rate: 0.0089863]
	Learning Rate: 0.00898627
	LOSS [training: 0.32019651916780567 | validation: 0.33235468927599227]
	TIME [epoch: 87.8 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36629094924953814		[learning rate: 0.0089698]
	Learning Rate: 0.00896983
	LOSS [training: 0.36629094924953814 | validation: 0.32226434765486456]
	TIME [epoch: 87.8 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3051356229062553		[learning rate: 0.0089534]
	Learning Rate: 0.00895339
	LOSS [training: 0.3051356229062553 | validation: 0.389491771247538]
	TIME [epoch: 87.8 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3828258162734547		[learning rate: 0.008937]
	Learning Rate: 0.00893696
	LOSS [training: 0.3828258162734547 | validation: 0.3368842137275403]
	TIME [epoch: 87.8 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32238914794532414		[learning rate: 0.0089205]
	Learning Rate: 0.00892053
	LOSS [training: 0.32238914794532414 | validation: 0.3493438450088534]
	TIME [epoch: 87.8 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3376994859179506		[learning rate: 0.0089041]
	Learning Rate: 0.0089041
	LOSS [training: 0.3376994859179506 | validation: 0.3708458410588723]
	TIME [epoch: 87.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32265958626002944		[learning rate: 0.0088877]
	Learning Rate: 0.00888767
	LOSS [training: 0.32265958626002944 | validation: 0.33750743033889924]
	TIME [epoch: 87.8 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3468047128881458		[learning rate: 0.0088713]
	Learning Rate: 0.00887125
	LOSS [training: 0.3468047128881458 | validation: 0.30559933447817755]
	TIME [epoch: 87.8 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3122168078348383		[learning rate: 0.0088548]
	Learning Rate: 0.00885484
	LOSS [training: 0.3122168078348383 | validation: 0.3006430707787109]
	TIME [epoch: 87.8 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3594830752945895		[learning rate: 0.0088384]
	Learning Rate: 0.00883842
	LOSS [training: 0.3594830752945895 | validation: 0.4202887153734327]
	TIME [epoch: 87.8 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35223541361336036		[learning rate: 0.008822]
	Learning Rate: 0.00882201
	LOSS [training: 0.35223541361336036 | validation: 0.30770893422078915]
	TIME [epoch: 87.8 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2984360764367944		[learning rate: 0.0088056]
	Learning Rate: 0.0088056
	LOSS [training: 0.2984360764367944 | validation: 0.3395713763151619]
	TIME [epoch: 87.8 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32121589451516996		[learning rate: 0.0087892]
	Learning Rate: 0.00878919
	LOSS [training: 0.32121589451516996 | validation: 0.3346934400469773]
	TIME [epoch: 87.8 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3384991342616029		[learning rate: 0.0087728]
	Learning Rate: 0.00877279
	LOSS [training: 0.3384991342616029 | validation: 0.32889491242606167]
	TIME [epoch: 87.8 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30882002775770073		[learning rate: 0.0087564]
	Learning Rate: 0.00875639
	LOSS [training: 0.30882002775770073 | validation: 0.3227118119841328]
	TIME [epoch: 87.7 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35977027350057095		[learning rate: 0.00874]
	Learning Rate: 0.00874
	LOSS [training: 0.35977027350057095 | validation: 0.29841044338590333]
	TIME [epoch: 87.7 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3062447829947259		[learning rate: 0.0087236]
	Learning Rate: 0.00872361
	LOSS [training: 0.3062447829947259 | validation: 0.3224387021989975]
	TIME [epoch: 87.8 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33040295383631046		[learning rate: 0.0087072]
	Learning Rate: 0.00870722
	LOSS [training: 0.33040295383631046 | validation: 0.39328117941445834]
	TIME [epoch: 87.8 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35735487560225976		[learning rate: 0.0086908]
	Learning Rate: 0.00869083
	LOSS [training: 0.35735487560225976 | validation: 0.32826317115315623]
	TIME [epoch: 87.8 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3035028202767911		[learning rate: 0.0086745]
	Learning Rate: 0.00867445
	LOSS [training: 0.3035028202767911 | validation: 0.35582146726821184]
	TIME [epoch: 87.8 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3193576007545991		[learning rate: 0.0086581]
	Learning Rate: 0.00865807
	LOSS [training: 0.3193576007545991 | validation: 0.2884762789408738]
	TIME [epoch: 87.7 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47771139882201935		[learning rate: 0.0086417]
	Learning Rate: 0.0086417
	LOSS [training: 0.47771139882201935 | validation: 0.38986183534061486]
	TIME [epoch: 87.7 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.329360226104656		[learning rate: 0.0086253]
	Learning Rate: 0.00862533
	LOSS [training: 0.329360226104656 | validation: 0.32497373547956165]
	TIME [epoch: 87.7 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31363122257517195		[learning rate: 0.008609]
	Learning Rate: 0.00860896
	LOSS [training: 0.31363122257517195 | validation: 0.29154809626834843]
	TIME [epoch: 87.7 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31146935718597557		[learning rate: 0.0085926]
	Learning Rate: 0.0085926
	LOSS [training: 0.31146935718597557 | validation: 0.30107610429527276]
	TIME [epoch: 87.7 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30070917558937216		[learning rate: 0.0085762]
	Learning Rate: 0.00857624
	LOSS [training: 0.30070917558937216 | validation: 0.30004479525412353]
	TIME [epoch: 87.7 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3299568044122601		[learning rate: 0.0085599]
	Learning Rate: 0.00855989
	LOSS [training: 0.3299568044122601 | validation: 0.3558538396202615]
	TIME [epoch: 87.7 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3214601301095824		[learning rate: 0.0085435]
	Learning Rate: 0.00854354
	LOSS [training: 0.3214601301095824 | validation: 0.3054596416677599]
	TIME [epoch: 87.8 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35726575828970114		[learning rate: 0.0085272]
	Learning Rate: 0.00852719
	LOSS [training: 0.35726575828970114 | validation: 0.33731809388529377]
	TIME [epoch: 87.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3056565106254495		[learning rate: 0.0085108]
	Learning Rate: 0.00851085
	LOSS [training: 0.3056565106254495 | validation: 0.29232073023651706]
	TIME [epoch: 87.7 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.336634972782258		[learning rate: 0.0084945]
	Learning Rate: 0.00849451
	LOSS [training: 0.336634972782258 | validation: 0.314032191539925]
	TIME [epoch: 87.7 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29234697187070857		[learning rate: 0.0084782]
	Learning Rate: 0.00847817
	LOSS [training: 0.29234697187070857 | validation: 0.3069199541809189]
	TIME [epoch: 87.7 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3392642628721691		[learning rate: 0.0084618]
	Learning Rate: 0.00846184
	LOSS [training: 0.3392642628721691 | validation: 0.31076150819643394]
	TIME [epoch: 87.7 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32922265162840936		[learning rate: 0.0084455]
	Learning Rate: 0.00844552
	LOSS [training: 0.32922265162840936 | validation: 0.36584999821716374]
	TIME [epoch: 87.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34861662563798934		[learning rate: 0.0084292]
	Learning Rate: 0.0084292
	LOSS [training: 0.34861662563798934 | validation: 0.30067085237133045]
	TIME [epoch: 87.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3185816206706993		[learning rate: 0.0084129]
	Learning Rate: 0.00841288
	LOSS [training: 0.3185816206706993 | validation: 0.34084812430589334]
	TIME [epoch: 87.7 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31205218746983393		[learning rate: 0.0083966]
	Learning Rate: 0.00839657
	LOSS [training: 0.31205218746983393 | validation: 0.3763855187709491]
	TIME [epoch: 87.7 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34663961628830553		[learning rate: 0.0083803]
	Learning Rate: 0.00838026
	LOSS [training: 0.34663961628830553 | validation: 0.30615866275977965]
	TIME [epoch: 87.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3228702789981171		[learning rate: 0.008364]
	Learning Rate: 0.00836395
	LOSS [training: 0.3228702789981171 | validation: 0.3295746986357468]
	TIME [epoch: 87.7 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31449095932918697		[learning rate: 0.0083477]
	Learning Rate: 0.00834765
	LOSS [training: 0.31449095932918697 | validation: 0.31187939276459975]
	TIME [epoch: 87.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3098372376361408		[learning rate: 0.0083314]
	Learning Rate: 0.00833136
	LOSS [training: 0.3098372376361408 | validation: 0.29387193463555483]
	TIME [epoch: 87.6 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33300564882923867		[learning rate: 0.0083151]
	Learning Rate: 0.00831507
	LOSS [training: 0.33300564882923867 | validation: 0.4229195394442431]
	TIME [epoch: 87.7 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3198167288220388		[learning rate: 0.0082988]
	Learning Rate: 0.00829878
	LOSS [training: 0.3198167288220388 | validation: 0.3281832687321355]
	TIME [epoch: 87.7 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35466331778331583		[learning rate: 0.0082825]
	Learning Rate: 0.0082825
	LOSS [training: 0.35466331778331583 | validation: 0.33215334055724]
	TIME [epoch: 87.6 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29286477835645963		[learning rate: 0.0082662]
	Learning Rate: 0.00826622
	LOSS [training: 0.29286477835645963 | validation: 0.3302208866684988]
	TIME [epoch: 87.7 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32666443737499146		[learning rate: 0.0082499]
	Learning Rate: 0.00824995
	LOSS [training: 0.32666443737499146 | validation: 0.30244913698012443]
	TIME [epoch: 87.6 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3500294658149043		[learning rate: 0.0082337]
	Learning Rate: 0.00823368
	LOSS [training: 0.3500294658149043 | validation: 0.3464795466682347]
	TIME [epoch: 87.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3162687519092231		[learning rate: 0.0082174]
	Learning Rate: 0.00821742
	LOSS [training: 0.3162687519092231 | validation: 0.2921661402585591]
	TIME [epoch: 87.6 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28861732281590446		[learning rate: 0.0082012]
	Learning Rate: 0.00820116
	LOSS [training: 0.28861732281590446 | validation: 0.35513248028729394]
	TIME [epoch: 87.7 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35354636350022095		[learning rate: 0.0081849]
	Learning Rate: 0.00818491
	LOSS [training: 0.35354636350022095 | validation: 0.3191280711272988]
	TIME [epoch: 87.6 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30495280340921266		[learning rate: 0.0081687]
	Learning Rate: 0.00816866
	LOSS [training: 0.30495280340921266 | validation: 0.2808572784181943]
	TIME [epoch: 87.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_1112.pth
	Model improved!!!
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31769609725561		[learning rate: 0.0081524]
	Learning Rate: 0.00815242
	LOSS [training: 0.31769609725561 | validation: 0.342334813085235]
	TIME [epoch: 87.7 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32002231845420026		[learning rate: 0.0081362]
	Learning Rate: 0.00813618
	LOSS [training: 0.32002231845420026 | validation: 0.3654833223758088]
	TIME [epoch: 87.6 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3364532760351215		[learning rate: 0.00812]
	Learning Rate: 0.00811995
	LOSS [training: 0.3364532760351215 | validation: 0.32411839152264355]
	TIME [epoch: 87.6 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32270637509284394		[learning rate: 0.0081037]
	Learning Rate: 0.00810372
	LOSS [training: 0.32270637509284394 | validation: 0.3587707224333505]
	TIME [epoch: 87.7 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3220737806086403		[learning rate: 0.0080875]
	Learning Rate: 0.0080875
	LOSS [training: 0.3220737806086403 | validation: 0.29930891527945536]
	TIME [epoch: 87.6 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3076603780268043		[learning rate: 0.0080713]
	Learning Rate: 0.00807128
	LOSS [training: 0.3076603780268043 | validation: 0.3306065082098707]
	TIME [epoch: 87.6 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32849680331951325		[learning rate: 0.0080551]
	Learning Rate: 0.00805507
	LOSS [training: 0.32849680331951325 | validation: 0.30168916992931993]
	TIME [epoch: 87.6 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3008085891484784		[learning rate: 0.0080389]
	Learning Rate: 0.00803887
	LOSS [training: 0.3008085891484784 | validation: 0.30272123653786975]
	TIME [epoch: 87.6 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30239809512999577		[learning rate: 0.0080227]
	Learning Rate: 0.00802267
	LOSS [training: 0.30239809512999577 | validation: 0.32094401397519534]
	TIME [epoch: 87.6 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32297540945354164		[learning rate: 0.0080065]
	Learning Rate: 0.00800647
	LOSS [training: 0.32297540945354164 | validation: 0.3534138450172749]
	TIME [epoch: 87.7 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32171173628260474		[learning rate: 0.0079903]
	Learning Rate: 0.00799028
	LOSS [training: 0.32171173628260474 | validation: 0.3046958398663142]
	TIME [epoch: 87.6 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3181647168766514		[learning rate: 0.0079741]
	Learning Rate: 0.0079741
	LOSS [training: 0.3181647168766514 | validation: 0.34274115074848865]
	TIME [epoch: 87.6 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3394479215492794		[learning rate: 0.0079579]
	Learning Rate: 0.00795792
	LOSS [training: 0.3394479215492794 | validation: 0.29432810029665823]
	TIME [epoch: 87.7 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.297382513182062		[learning rate: 0.0079417]
	Learning Rate: 0.00794174
	LOSS [training: 0.297382513182062 | validation: 0.3162367404833323]
	TIME [epoch: 87.7 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3272732793528557		[learning rate: 0.0079256]
	Learning Rate: 0.00792558
	LOSS [training: 0.3272732793528557 | validation: 0.3244289209116711]
	TIME [epoch: 87.6 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31544286024034224		[learning rate: 0.0079094]
	Learning Rate: 0.00790941
	LOSS [training: 0.31544286024034224 | validation: 0.30560767061126354]
	TIME [epoch: 87.7 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3343764623672506		[learning rate: 0.0078933]
	Learning Rate: 0.00789326
	LOSS [training: 0.3343764623672506 | validation: 0.30626362680958596]
	TIME [epoch: 87.6 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3056496725211726		[learning rate: 0.0078771]
	Learning Rate: 0.00787711
	LOSS [training: 0.3056496725211726 | validation: 0.3456673579818499]
	TIME [epoch: 87.7 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31813111305457775		[learning rate: 0.007861]
	Learning Rate: 0.00786096
	LOSS [training: 0.31813111305457775 | validation: 0.29846868066554105]
	TIME [epoch: 87.6 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3029034145285276		[learning rate: 0.0078448]
	Learning Rate: 0.00784482
	LOSS [training: 0.3029034145285276 | validation: 0.3857079806505647]
	TIME [epoch: 87.6 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34047210006106376		[learning rate: 0.0078287]
	Learning Rate: 0.00782869
	LOSS [training: 0.34047210006106376 | validation: 0.29209126605053276]
	TIME [epoch: 87.6 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28988462671544774		[learning rate: 0.0078126]
	Learning Rate: 0.00781256
	LOSS [training: 0.28988462671544774 | validation: 0.33556754160532115]
	TIME [epoch: 87.7 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36883456460675557		[learning rate: 0.0077964]
	Learning Rate: 0.00779644
	LOSS [training: 0.36883456460675557 | validation: 0.3103420930727139]
	TIME [epoch: 87.7 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2961279025558432		[learning rate: 0.0077803]
	Learning Rate: 0.00778033
	LOSS [training: 0.2961279025558432 | validation: 0.30369731441618386]
	TIME [epoch: 87.6 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32724227012285056		[learning rate: 0.0077642]
	Learning Rate: 0.00776422
	LOSS [training: 0.32724227012285056 | validation: 0.3256159357401735]
	TIME [epoch: 87.7 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31399969399798705		[learning rate: 0.0077481]
	Learning Rate: 0.00774812
	LOSS [training: 0.31399969399798705 | validation: 0.2987528029805782]
	TIME [epoch: 87.6 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30276013172769756		[learning rate: 0.007732]
	Learning Rate: 0.00773202
	LOSS [training: 0.30276013172769756 | validation: 0.30349089659044237]
	TIME [epoch: 87.7 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2985751708436424		[learning rate: 0.0077159]
	Learning Rate: 0.00771593
	LOSS [training: 0.2985751708436424 | validation: 0.2891561617311572]
	TIME [epoch: 87.7 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3185821010974448		[learning rate: 0.0076998]
	Learning Rate: 0.00769985
	LOSS [training: 0.3185821010974448 | validation: 0.2969854646830111]
	TIME [epoch: 87.7 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3275835016537893		[learning rate: 0.0076838]
	Learning Rate: 0.00768377
	LOSS [training: 0.3275835016537893 | validation: 0.31299612013666644]
	TIME [epoch: 87.6 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3082523619575773		[learning rate: 0.0076677]
	Learning Rate: 0.0076677
	LOSS [training: 0.3082523619575773 | validation: 0.27445861991529885]
	TIME [epoch: 87.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd5_20240713_103448/states/model_phiq_1a_v_mmd5_1143.pth
	Model improved!!!
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33477140443206915		[learning rate: 0.0076516]
	Learning Rate: 0.00765163
	LOSS [training: 0.33477140443206915 | validation: 0.3904871076199352]
	TIME [epoch: 87.7 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32448242067482425		[learning rate: 0.0076356]
	Learning Rate: 0.00763557
	LOSS [training: 0.32448242067482425 | validation: 0.2999423349499516]
	TIME [epoch: 87.7 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2791425598388135		[learning rate: 0.0076195]
	Learning Rate: 0.00761952
	LOSS [training: 0.2791425598388135 | validation: 0.30318003417967687]
	TIME [epoch: 87.6 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3586215213435635		[learning rate: 0.0076035]
	Learning Rate: 0.00760347
	LOSS [training: 0.3586215213435635 | validation: 0.3104987159601228]
	TIME [epoch: 87.7 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3030309809508671		[learning rate: 0.0075874]
	Learning Rate: 0.00758744
	LOSS [training: 0.3030309809508671 | validation: 0.31571525096007996]
	TIME [epoch: 87.7 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29755713211515655		[learning rate: 0.0075714]
	Learning Rate: 0.0075714
	LOSS [training: 0.29755713211515655 | validation: 0.37114578659002184]
	TIME [epoch: 87.7 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3245670077940997		[learning rate: 0.0075554]
	Learning Rate: 0.00755538
	LOSS [training: 0.3245670077940997 | validation: 0.2901313095368517]
	TIME [epoch: 87.7 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29193250098210083		[learning rate: 0.0075394]
	Learning Rate: 0.00753936
	LOSS [training: 0.29193250098210083 | validation: 0.3048716992277829]
	TIME [epoch: 87.6 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3224316587825898		[learning rate: 0.0075233]
	Learning Rate: 0.00752335
	LOSS [training: 0.3224316587825898 | validation: 0.3117226724907642]
	TIME [epoch: 87.6 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29958152863319754		[learning rate: 0.0075073]
	Learning Rate: 0.00750734
	LOSS [training: 0.29958152863319754 | validation: 0.34727339833851556]
	TIME [epoch: 87.6 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3463868900629813		[learning rate: 0.0074913]
	Learning Rate: 0.00749134
	LOSS [training: 0.3463868900629813 | validation: 0.29224537126959904]
	TIME [epoch: 87.6 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2775709021687489		[learning rate: 0.0074754]
	Learning Rate: 0.00747535
	LOSS [training: 0.2775709021687489 | validation: 0.28671142179381137]
	TIME [epoch: 87.7 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33535127489080824		[learning rate: 0.0074594]
	Learning Rate: 0.00745937
	LOSS [training: 0.33535127489080824 | validation: 0.424738104696445]
	TIME [epoch: 87.6 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33727757366822464		[learning rate: 0.0074434]
	Learning Rate: 0.00744339
	LOSS [training: 0.33727757366822464 | validation: 0.30328646239149737]
	TIME [epoch: 87.7 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3049493939743575		[learning rate: 0.0074274]
	Learning Rate: 0.00742742
	LOSS [training: 0.3049493939743575 | validation: 0.27786816891503974]
	TIME [epoch: 87.6 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29502461169454947		[learning rate: 0.0074115]
	Learning Rate: 0.00741145
	LOSS [training: 0.29502461169454947 | validation: 0.3069766606487522]
	TIME [epoch: 87.6 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3078410896820982		[learning rate: 0.0073955]
	Learning Rate: 0.0073955
	LOSS [training: 0.3078410896820982 | validation: 0.35118946770332693]
	TIME [epoch: 87.6 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31727681643546846		[learning rate: 0.0073795]
	Learning Rate: 0.00737955
	LOSS [training: 0.31727681643546846 | validation: 0.28969754732416686]
	TIME [epoch: 87.6 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29496519599973625		[learning rate: 0.0073636]
	Learning Rate: 0.0073636
	LOSS [training: 0.29496519599973625 | validation: 0.2810644197525393]
	TIME [epoch: 87.6 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31318413821232216		[learning rate: 0.0073477]
	Learning Rate: 0.00734767
	LOSS [training: 0.31318413821232216 | validation: 0.3218926783323358]
	TIME [epoch: 87.7 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31012559789176597		[learning rate: 0.0073317]
	Learning Rate: 0.00733174
	LOSS [training: 0.31012559789176597 | validation: 0.30071074841938883]
	TIME [epoch: 87.7 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30687986233382347		[learning rate: 0.0073158]
	Learning Rate: 0.00731582
	LOSS [training: 0.30687986233382347 | validation: 0.43282400326968107]
	TIME [epoch: 87.7 sec]
EPOCH 1166/2000:
	Training over batches...
