Args:
Namespace(name='model_phi1_2b_v_mmd1', outdir='out/model_training/model_phi1_2b_v_mmd1', training_data='data/training_data/data_phi1_2b/training', validation_data='data/training_data/data_phi1_2b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3697872853

Training model...

Saving initial model state to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.612256425987928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.612256425987928 | validation: 5.6754190879138084]
	TIME [epoch: 104 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.604582074096399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.604582074096399 | validation: 4.296934092405795]
	TIME [epoch: 3.35 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.180942981558615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.180942981558615 | validation: 3.5775107626404075]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.6170403087837064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6170403087837064 | validation: 3.349891750237731]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.405909580999486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.405909580999486 | validation: 3.3462427403677086]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.304399839604467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.304399839604467 | validation: 3.4453730469506505]
	TIME [epoch: 3.28 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.2751997712738627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2751997712738627 | validation: 3.767838241482089]
	TIME [epoch: 3.27 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.3362391408894627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3362391408894627 | validation: 3.1908591531660147]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.1063485492617176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1063485492617176 | validation: 3.2081211142880792]
	TIME [epoch: 3.27 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.9770407261751473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9770407261751473 | validation: 3.4092816765401235]
	TIME [epoch: 3.27 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.0746077015884623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0746077015884623 | validation: 3.0587580144969553]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.8346453813063732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8346453813063732 | validation: 3.0316531445931827]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.8040095207109017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8040095207109017 | validation: 2.9274148366044477]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.695612797143835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.695612797143835 | validation: 2.9051739486276476]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.639726322990515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.639726322990515 | validation: 2.911335938484022]
	TIME [epoch: 3.27 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.5942995580946553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5942995580946553 | validation: 2.807507550202828]
	TIME [epoch: 3.29 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.6629486751217613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6629486751217613 | validation: 2.862050901759111]
	TIME [epoch: 3.29 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.722963460342216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.722963460342216 | validation: 2.689598993637537]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.501153791003823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.501153791003823 | validation: 2.739442745696816]
	TIME [epoch: 3.28 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.472400935379082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.472400935379082 | validation: 2.931825798453457]
	TIME [epoch: 3.27 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.565950417635664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.565950417635664 | validation: 2.517170626582771]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.280521511678172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.280521511678172 | validation: 2.8393130539831137]
	TIME [epoch: 3.27 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.373311233067829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.373311233067829 | validation: 2.7234986219295436]
	TIME [epoch: 3.26 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.4131967605406515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4131967605406515 | validation: 2.4797182580810517]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.1497716915364053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1497716915364053 | validation: 2.625857068374877]
	TIME [epoch: 3.27 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.186768778186126		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.186768778186126 | validation: 2.243913561723451]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.0415004427764534		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.0415004427764534 | validation: 2.131900672876774]
	TIME [epoch: 3.31 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9940260167227726		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.9940260167227726 | validation: 2.237600037561922]
	TIME [epoch: 3.28 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.109581945567221		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.109581945567221 | validation: 2.2981034488101124]
	TIME [epoch: 3.27 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.1061749733308037		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.1061749733308037 | validation: 2.036165384422502]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9215095003370102		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.9215095003370102 | validation: 1.9723917182442983]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8661469804349244		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.8661469804349244 | validation: 1.9817288159250535]
	TIME [epoch: 3.27 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7687926107709124		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.7687926107709124 | validation: 2.065270005863766]
	TIME [epoch: 3.27 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8203239507720697		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.8203239507720697 | validation: 1.895280073407378]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.756560471606873		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.756560471606873 | validation: 1.8312867427289554]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7333392953080031		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.7333392953080031 | validation: 2.046198189461172]
	TIME [epoch: 3.28 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7335603060258944		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.7335603060258944 | validation: 1.6803163576012876]
	TIME [epoch: 3.29 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.71942786148467		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.71942786148467 | validation: 1.7106249049242834]
	TIME [epoch: 3.29 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7633605487346014		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.7633605487346014 | validation: 2.1801422943652677]
	TIME [epoch: 3.27 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8131694178365256		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.8131694178365256 | validation: 1.6655468466496288]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.63220416782405		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.63220416782405 | validation: 1.9208577247032634]
	TIME [epoch: 3.27 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5979829517857365		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.5979829517857365 | validation: 1.7856477425803534]
	TIME [epoch: 3.26 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.723231658967078		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.723231658967078 | validation: 1.6348095824722422]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6330910805256118		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.6330910805256118 | validation: 1.7704424599662645]
	TIME [epoch: 3.26 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.555494750550488		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.555494750550488 | validation: 1.6306951220777102]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4964548766626995		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.4964548766626995 | validation: 1.7578233421544063]
	TIME [epoch: 3.27 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.552031017992523		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.552031017992523 | validation: 1.6144319859385143]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4798024763258424		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.4798024763258424 | validation: 1.5307723709272771]
	TIME [epoch: 3.3 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5522783858297862		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.5522783858297862 | validation: 1.6137096078736197]
	TIME [epoch: 3.28 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6824878494185733		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.6824878494185733 | validation: 1.7210408672791269]
	TIME [epoch: 3.27 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.534605806807186		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.534605806807186 | validation: 1.6008447975778404]
	TIME [epoch: 3.27 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5858939270972061		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.5858939270972061 | validation: 1.4957432073417385]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4518036209532907		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.4518036209532907 | validation: 1.6263768794018483]
	TIME [epoch: 3.27 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4581121520051057		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.4581121520051057 | validation: 1.4947001310588697]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4096465602824502		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.4096465602824502 | validation: 1.4340350587212942]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6040806877009626		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.6040806877009626 | validation: 1.5963524249123866]
	TIME [epoch: 3.27 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6486055712140941		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.6486055712140941 | validation: 1.7631982625632145]
	TIME [epoch: 3.27 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4405713257222756		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.4405713257222756 | validation: 1.5571530390554096]
	TIME [epoch: 3.28 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3310584577836204		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.3310584577836204 | validation: 1.5719590913396106]
	TIME [epoch: 3.29 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5381421377486433		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.5381421377486433 | validation: 1.4689162250279182]
	TIME [epoch: 3.27 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3140820046293336		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.3140820046293336 | validation: 1.4534232653289283]
	TIME [epoch: 3.27 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3585228256065367		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.3585228256065367 | validation: 1.394381592852922]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3272630141431996		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.3272630141431996 | validation: 1.5719839640124456]
	TIME [epoch: 3.27 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5291464889191828		[learning rate: 0.0076127]
	Learning Rate: 0.00761269
	LOSS [training: 1.5291464889191828 | validation: 1.7683622687377658]
	TIME [epoch: 3.27 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4421921704734484		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.4421921704734484 | validation: 1.5496941395917148]
	TIME [epoch: 3.27 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3801727466225877		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.3801727466225877 | validation: 1.6497026354291568]
	TIME [epoch: 3.27 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4548036419022812		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.4548036419022812 | validation: 1.4033334652297198]
	TIME [epoch: 3.27 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4700193753302824		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.4700193753302824 | validation: 1.3551550807283068]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3343960627951306		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.3343960627951306 | validation: 1.5120734933234194]
	TIME [epoch: 3.29 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.250011900321015		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.250011900321015 | validation: 1.5420280985084418]
	TIME [epoch: 3.28 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4343565188061156		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.4343565188061156 | validation: 1.3905299633516404]
	TIME [epoch: 3.27 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.254486250654159		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.254486250654159 | validation: 1.5431841922803766]
	TIME [epoch: 3.27 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2498713870229485		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.2498713870229485 | validation: 1.437136774791024]
	TIME [epoch: 3.27 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4567025189478513		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.4567025189478513 | validation: 1.5364475508425357]
	TIME [epoch: 3.26 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2851823269296903		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.2851823269296903 | validation: 1.248063966433503]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2655584546346015		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 1.2655584546346015 | validation: 1.285900622489117]
	TIME [epoch: 3.27 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2897293214085028		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.2897293214085028 | validation: 1.1983729156093996]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1639981270739663		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.1639981270739663 | validation: 1.2001420402563285]
	TIME [epoch: 3.27 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2570689599253242		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.2570689599253242 | validation: 1.5405628677695657]
	TIME [epoch: 3.26 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5499994125357188		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.5499994125357188 | validation: 1.815930545253544]
	TIME [epoch: 3.3 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3930073529964702		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 1.3930073529964702 | validation: 1.2250350889987356]
	TIME [epoch: 3.29 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1654690105628136		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.1654690105628136 | validation: 1.2632021563684173]
	TIME [epoch: 3.27 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1117082081352665		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.1117082081352665 | validation: 1.290700376430923]
	TIME [epoch: 3.27 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1006002941317354		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.1006002941317354 | validation: 1.2325254087552162]
	TIME [epoch: 3.27 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.055750238125078		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.055750238125078 | validation: 1.6104695697631897]
	TIME [epoch: 3.26 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.554018666953025		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.554018666953025 | validation: 1.3006172588425153]
	TIME [epoch: 3.27 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4374081890441899		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.4374081890441899 | validation: 1.1909052377775706]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1584509426763332		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.1584509426763332 | validation: 1.280551484915529]
	TIME [epoch: 3.27 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.137848445308485		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.137848445308485 | validation: 1.1201786179942919]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0810571527550752		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.0810571527550752 | validation: 1.1510782724428237]
	TIME [epoch: 3.28 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0729910461057268		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.0729910461057268 | validation: 1.1829734896217303]
	TIME [epoch: 3.3 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0217363319004629		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.0217363319004629 | validation: 1.2281915431722692]
	TIME [epoch: 3.29 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1055395538660784		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.1055395538660784 | validation: 1.609352523144984]
	TIME [epoch: 3.27 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4331510323569412		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.4331510323569412 | validation: 1.2804845404745846]
	TIME [epoch: 3.27 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0346215057530408		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.0346215057530408 | validation: 1.1174182772186723]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0202281963228788		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.0202281963228788 | validation: 1.0958443669819393]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.184758967738922		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.184758967738922 | validation: 1.1220911085172154]
	TIME [epoch: 3.27 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1486747557352575		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.1486747557352575 | validation: 1.0916937982829567]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0607606093603237		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.0607606093603237 | validation: 1.0947008213455245]
	TIME [epoch: 3.27 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9717917908205145		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.9717917908205145 | validation: 1.1662426484516424]
	TIME [epoch: 3.27 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.118188689247092		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.118188689247092 | validation: 1.0844669452140796]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0989722138970273		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.0989722138970273 | validation: 2.041256563059116]
	TIME [epoch: 3.29 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.646996987753274		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.646996987753274 | validation: 1.073879117072702]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2581414881112567		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.2581414881112567 | validation: 1.0850935659442356]
	TIME [epoch: 3.27 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0456646060022523		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.0456646060022523 | validation: 1.210055247083309]
	TIME [epoch: 3.27 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9781661237589931		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.9781661237589931 | validation: 1.071964691512494]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.010366829498916		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.010366829498916 | validation: 1.0062061489144305]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9748790533567524		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.9748790533567524 | validation: 1.1631034665005016]
	TIME [epoch: 3.27 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0741703673368264		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.0741703673368264 | validation: 1.699914335801453]
	TIME [epoch: 3.27 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1137851582499962		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.1137851582499962 | validation: 1.0106512205963494]
	TIME [epoch: 3.26 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.022630715155073		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.022630715155073 | validation: 0.9984578717354478]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0693342591961306		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.0693342591961306 | validation: 0.9909436608284463]
	TIME [epoch: 3.3 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9269658484243273		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.9269658484243273 | validation: 1.063230680552765]
	TIME [epoch: 3.29 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9204192094824971		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.9204192094824971 | validation: 1.1613617940259753]
	TIME [epoch: 3.27 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.044337036698745		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.044337036698745 | validation: 1.0771719047114232]
	TIME [epoch: 3.27 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.076551780401711		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.076551780401711 | validation: 1.5513989545518538]
	TIME [epoch: 3.27 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1851008057659569		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.1851008057659569 | validation: 1.0202678782436228]
	TIME [epoch: 3.27 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.944185129308742		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.944185129308742 | validation: 0.9106487242713153]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9326085399876621		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.9326085399876621 | validation: 0.9295151887689036]
	TIME [epoch: 3.28 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.933937568992294		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.933937568992294 | validation: 0.9713351542146115]
	TIME [epoch: 3.27 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0141957235931667		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 1.0141957235931667 | validation: 0.9357519249358479]
	TIME [epoch: 3.27 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8378596041453399		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.8378596041453399 | validation: 0.9586870120893511]
	TIME [epoch: 3.28 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9230838033913349		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.9230838033913349 | validation: 0.9456790727628115]
	TIME [epoch: 3.3 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8605370992080756		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.8605370992080756 | validation: 0.9500685182867546]
	TIME [epoch: 3.29 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1026285484211247		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.1026285484211247 | validation: 1.0895963104245345]
	TIME [epoch: 3.27 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1818176834784868		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.1818176834784868 | validation: 1.3935312125501298]
	TIME [epoch: 3.28 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.950085143763472		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.950085143763472 | validation: 0.9669635742835005]
	TIME [epoch: 3.27 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8214070874842042		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.8214070874842042 | validation: 1.1243197664773104]
	TIME [epoch: 3.29 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.834334060469217		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.834334060469217 | validation: 0.8541562676150035]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0294870796809972		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.0294870796809972 | validation: 0.8633046058202885]
	TIME [epoch: 3.27 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.910059968677239		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.910059968677239 | validation: 1.2052580665340724]
	TIME [epoch: 3.27 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8997514737830379		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.8997514737830379 | validation: 0.9100203923082465]
	TIME [epoch: 3.27 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8102821315274809		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.8102821315274809 | validation: 0.9125950774302396]
	TIME [epoch: 3.27 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9117080319303345		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.9117080319303345 | validation: 0.8203153407476059]
	TIME [epoch: 3.29 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7982816193443951		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.7982816193443951 | validation: 0.8862460109690176]
	TIME [epoch: 3.31 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7437141618404397		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.7437141618404397 | validation: 0.8137446189198777]
	TIME [epoch: 3.58 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7475348662206722		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.7475348662206722 | validation: 0.8161032791814726]
	TIME [epoch: 3.26 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9344230332174068		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.9344230332174068 | validation: 0.848512772812504]
	TIME [epoch: 3.25 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7514470291781978		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.7514470291781978 | validation: 0.8413092698363225]
	TIME [epoch: 3.26 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7116962712047004		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.7116962712047004 | validation: 0.8226317660773882]
	TIME [epoch: 3.25 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8612032531050127		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.8612032531050127 | validation: 0.8600534212940038]
	TIME [epoch: 3.26 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0280564596248696		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.0280564596248696 | validation: 1.0989985940968559]
	TIME [epoch: 3.26 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7769390207876747		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.7769390207876747 | validation: 1.430549440371603]
	TIME [epoch: 3.26 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9153254169053187		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.9153254169053187 | validation: 0.8835610398093598]
	TIME [epoch: 3.26 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7451457124463717		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.7451457124463717 | validation: 0.7244630037724646]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7652340867888512		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.7652340867888512 | validation: 0.9402352846748813]
	TIME [epoch: 3.29 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8120380063844521		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.8120380063844521 | validation: 1.1420581801496084]
	TIME [epoch: 3.27 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8905985960395333		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.8905985960395333 | validation: 0.9600963683560302]
	TIME [epoch: 3.26 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6917468601339904		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.6917468601339904 | validation: 0.9591465953700298]
	TIME [epoch: 3.27 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6713658778461695		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.6713658778461695 | validation: 0.8211433371801207]
	TIME [epoch: 3.27 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6992813956517698		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.6992813956517698 | validation: 0.916283409182491]
	TIME [epoch: 3.27 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9335345439330248		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.9335345439330248 | validation: 1.08194871921461]
	TIME [epoch: 3.26 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.812300180702731		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.812300180702731 | validation: 1.150434917867937]
	TIME [epoch: 3.26 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7883916861453608		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.7883916861453608 | validation: 1.0531535184603964]
	TIME [epoch: 3.27 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6936789224286046		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.6936789224286046 | validation: 0.783073801561418]
	TIME [epoch: 3.26 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7219971880255827		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.7219971880255827 | validation: 0.8359714698012891]
	TIME [epoch: 3.27 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6561087012491699		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.6561087012491699 | validation: 1.4073152548671546]
	TIME [epoch: 3.29 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9149619950132335		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.9149619950132335 | validation: 0.8398208802967795]
	TIME [epoch: 3.27 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6841164151623202		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.6841164151623202 | validation: 0.6325031418393925]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6792110322275406		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.6792110322275406 | validation: 1.1313312281945194]
	TIME [epoch: 3.26 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7624549127775624		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.7624549127775624 | validation: 0.8486911895224684]
	TIME [epoch: 3.26 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5937309365464587		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.5937309365464587 | validation: 1.0539523963660757]
	TIME [epoch: 3.26 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7606218786228525		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.7606218786228525 | validation: 0.92797797565173]
	TIME [epoch: 3.26 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6095882711696169		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.6095882711696169 | validation: 0.7586783203303008]
	TIME [epoch: 3.27 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5906404191244798		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.5906404191244798 | validation: 0.8049250137911328]
	TIME [epoch: 3.27 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6104573655997947		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.6104573655997947 | validation: 1.310712892271343]
	TIME [epoch: 3.27 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8067561957364433		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.8067561957364433 | validation: 0.7573153238870215]
	TIME [epoch: 3.27 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5711847777223984		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.5711847777223984 | validation: 0.8390859281275767]
	TIME [epoch: 3.3 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6233661364115958		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.6233661364115958 | validation: 1.0180122976920887]
	TIME [epoch: 3.27 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6800746114373503		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.6800746114373503 | validation: 0.9648246561493519]
	TIME [epoch: 3.27 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5557194156194579		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.5557194156194579 | validation: 0.7895574818665045]
	TIME [epoch: 3.27 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5775025893600663		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.5775025893600663 | validation: 1.3742654318043035]
	TIME [epoch: 3.27 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7867153360193109		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.7867153360193109 | validation: 0.6232099880081255]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5934425281947038		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.5934425281947038 | validation: 0.6343341330568575]
	TIME [epoch: 3.27 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.55306052086566		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.55306052086566 | validation: 0.5251584003700439]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6592218704598639		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.6592218704598639 | validation: 0.5284165654876926]
	TIME [epoch: 3.27 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4962141591784758		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.4962141591784758 | validation: 1.0647136765478877]
	TIME [epoch: 3.27 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5583499169755846		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.5583499169755846 | validation: 0.6015729995788355]
	TIME [epoch: 3.28 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.49123116092821556		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.49123116092821556 | validation: 0.623209969135714]
	TIME [epoch: 3.3 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6749811502461536		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.6749811502461536 | validation: 1.194782320154718]
	TIME [epoch: 3.27 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5989924205329068		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.5989924205329068 | validation: 1.0656074562904303]
	TIME [epoch: 3.27 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5721549698886526		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.5721549698886526 | validation: 0.5361269952541753]
	TIME [epoch: 3.27 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6027073518034411		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.6027073518034411 | validation: 0.636548258706439]
	TIME [epoch: 3.27 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5024867921990936		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.5024867921990936 | validation: 1.204703600000227]
	TIME [epoch: 3.27 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.621883540004099		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.621883540004099 | validation: 0.734679275154891]
	TIME [epoch: 3.27 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4382562359134402		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.4382562359134402 | validation: 0.647689671603428]
	TIME [epoch: 3.26 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4157775783521591		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.4157775783521591 | validation: 0.48234329736666964]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5798497033101		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.5798497033101 | validation: 0.50980868786791]
	TIME [epoch: 3.27 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5966439927641094		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.5966439927641094 | validation: 0.6630318880315214]
	TIME [epoch: 3.28 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4031033345173614		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.4031033345173614 | validation: 0.7193883959581827]
	TIME [epoch: 3.29 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4597512278149567		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.4597512278149567 | validation: 1.0699257981975918]
	TIME [epoch: 3.28 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.517499387969791		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.517499387969791 | validation: 0.6317525496597938]
	TIME [epoch: 3.27 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5950576982691551		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.5950576982691551 | validation: 0.42677855377649865]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5062459588530331		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.5062459588530331 | validation: 0.533309991830238]
	TIME [epoch: 3.27 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3700813105316137		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.3700813105316137 | validation: 0.8358130366795802]
	TIME [epoch: 3.28 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5069201524802742		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.5069201524802742 | validation: 0.9157376377419122]
	TIME [epoch: 3.28 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.48838373789640976		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.48838373789640976 | validation: 0.7789258062840666]
	TIME [epoch: 3.28 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.39699308944346845		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.39699308944346845 | validation: 0.4251357441824271]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38767570211809854		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.38767570211809854 | validation: 0.40392072740591356]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.39127063673227397		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.39127063673227397 | validation: 0.517839536519279]
	TIME [epoch: 3.27 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.49819608332244303		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.49819608332244303 | validation: 0.41842533262113274]
	TIME [epoch: 107 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.34299359020199305		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.34299359020199305 | validation: 0.3662083665556314]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4966840220262807		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.4966840220262807 | validation: 0.38289333006642823]
	TIME [epoch: 6.42 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.43380144340676585		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.43380144340676585 | validation: 0.5409508320322233]
	TIME [epoch: 6.42 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4510449792288906		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.4510449792288906 | validation: 0.8948847119658074]
	TIME [epoch: 6.4 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.42681600514209317		[learning rate: 0.0027837]
	Learning Rate: 0.00278366
	LOSS [training: 0.42681600514209317 | validation: 0.5684583062542141]
	TIME [epoch: 6.4 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4027107189845347		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.4027107189845347 | validation: 0.4089745264581063]
	TIME [epoch: 6.43 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30534472925227973		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.30534472925227973 | validation: 0.48819725363476185]
	TIME [epoch: 6.44 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.34496745094481407		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.34496745094481407 | validation: 0.3806433518075472]
	TIME [epoch: 6.41 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.48143240590716185		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.48143240590716185 | validation: 0.6365966260810563]
	TIME [epoch: 6.41 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6984829869547919		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.6984829869547919 | validation: 1.5606785963975696]
	TIME [epoch: 6.41 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6683342951825559		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.6683342951825559 | validation: 0.4666361485282641]
	TIME [epoch: 6.42 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4441520583263083		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.4441520583263083 | validation: 0.608598870550975]
	TIME [epoch: 6.43 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3407818169270305		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.3407818169270305 | validation: 0.37606843162814896]
	TIME [epoch: 6.44 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3292271742028078		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.3292271742028078 | validation: 0.3342247963436387]
	TIME [epoch: 6.39 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2926555567762368		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.2926555567762368 | validation: 0.5887918544654244]
	TIME [epoch: 6.42 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31832461587231076		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.31832461587231076 | validation: 0.7768887521897292]
	TIME [epoch: 6.41 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4368256828268527		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.4368256828268527 | validation: 0.7352087525857893]
	TIME [epoch: 6.42 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3434096472467195		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.3434096472467195 | validation: 0.3216725221781831]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27942372453680353		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.27942372453680353 | validation: 0.30504333326442373]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2948827998993657		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.2948827998993657 | validation: 0.32264883339611733]
	TIME [epoch: 6.41 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3680166312545231		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.3680166312545231 | validation: 0.4761085428795239]
	TIME [epoch: 6.41 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45046231687334704		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.45046231687334704 | validation: 0.47907943582484647]
	TIME [epoch: 6.41 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3260813472078045		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.3260813472078045 | validation: 0.9446703667130047]
	TIME [epoch: 6.41 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.41462024572814393		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.41462024572814393 | validation: 0.4916547187928849]
	TIME [epoch: 6.47 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29850576619411623		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.29850576619411623 | validation: 0.7041348159857714]
	TIME [epoch: 6.43 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3271032008536051		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.3271032008536051 | validation: 0.5758964468636709]
	TIME [epoch: 6.4 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3633954345216416		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.3633954345216416 | validation: 0.4115738628895483]
	TIME [epoch: 6.41 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.278822643586591		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.278822643586591 | validation: 0.2865468420205685]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27332768599091734		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.27332768599091734 | validation: 0.2835332964604617]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.34386736258123707		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.34386736258123707 | validation: 0.38594290977277546]
	TIME [epoch: 6.47 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5042320627623367		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.5042320627623367 | validation: 0.30407732253811265]
	TIME [epoch: 6.45 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3150680715183378		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.3150680715183378 | validation: 0.32026434775520934]
	TIME [epoch: 6.44 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23235712262286873		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.23235712262286873 | validation: 0.41833772229862715]
	TIME [epoch: 6.42 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24954191665226955		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.24954191665226955 | validation: 0.6716425637517247]
	TIME [epoch: 6.44 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3230640087218059		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.3230640087218059 | validation: 0.7102761939651202]
	TIME [epoch: 6.44 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.49141073231694277		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.49141073231694277 | validation: 0.6600579982753102]
	TIME [epoch: 6.47 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33327713687450894		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.33327713687450894 | validation: 0.5789421452040511]
	TIME [epoch: 6.44 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33214974167433353		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.33214974167433353 | validation: 0.28109306178718946]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3097425415236198		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.3097425415236198 | validation: 0.27851900167416604]
	TIME [epoch: 6.42 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.342778299206539		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.342778299206539 | validation: 0.46730082035821435]
	TIME [epoch: 6.4 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3484376249692973		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.3484376249692973 | validation: 0.95504771326154]
	TIME [epoch: 6.43 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.36522694640866027		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.36522694640866027 | validation: 0.3890377543763388]
	TIME [epoch: 6.44 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3669758330465033		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.3669758330465033 | validation: 0.4015660829420797]
	TIME [epoch: 6.41 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31854845542774823		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.31854845542774823 | validation: 0.33020493127984135]
	TIME [epoch: 6.42 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22406218714685505		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.22406218714685505 | validation: 0.34121920684776197]
	TIME [epoch: 6.41 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21179638383378002		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.21179638383378002 | validation: 0.828361924816271]
	TIME [epoch: 6.41 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.37655139400895443		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.37655139400895443 | validation: 0.7843069305391246]
	TIME [epoch: 6.43 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3251662386280634		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.3251662386280634 | validation: 0.34596692570115306]
	TIME [epoch: 6.44 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.409841954622968		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.409841954622968 | validation: 0.28825708665254296]
	TIME [epoch: 6.42 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24769216421938928		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.24769216421938928 | validation: 0.4377451706883954]
	TIME [epoch: 6.42 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2801314091933427		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.2801314091933427 | validation: 0.5422130336766803]
	TIME [epoch: 6.41 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2489932556828285		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.2489932556828285 | validation: 0.25347722376141507]
	TIME [epoch: 6.42 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19772869780114644		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.19772869780114644 | validation: 0.27002628840324566]
	TIME [epoch: 6.44 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.237073846013681		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.237073846013681 | validation: 0.3030966674248283]
	TIME [epoch: 6.44 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24661214700625575		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.24661214700625575 | validation: 0.2829599007328709]
	TIME [epoch: 6.42 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2901362732678436		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.2901362732678436 | validation: 0.23862405654490326]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3756934658340324		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.3756934658340324 | validation: 0.25271897507806756]
	TIME [epoch: 6.4 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22351111328142464		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.22351111328142464 | validation: 0.4485064995240631]
	TIME [epoch: 6.41 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.274916164726914		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.274916164726914 | validation: 0.44831686200828536]
	TIME [epoch: 6.41 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23061968638635894		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.23061968638635894 | validation: 0.2709014144427145]
	TIME [epoch: 6.44 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3451458199585311		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.3451458199585311 | validation: 0.2190478007099569]
	TIME [epoch: 6.42 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22511728742871284		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.22511728742871284 | validation: 0.8807854165865199]
	TIME [epoch: 6.42 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3227939249755896		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.3227939249755896 | validation: 0.45967971559999743]
	TIME [epoch: 6.41 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22737650042761043		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.22737650042761043 | validation: 0.30668428602611203]
	TIME [epoch: 6.42 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1979372245560942		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.1979372245560942 | validation: 0.2703212045529975]
	TIME [epoch: 6.43 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17319122265730535		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.17319122265730535 | validation: 0.22555155360479345]
	TIME [epoch: 6.42 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20183225030202		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.20183225030202 | validation: 0.5867525987281699]
	TIME [epoch: 6.4 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38771055471763183		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.38771055471763183 | validation: 0.23729845976635364]
	TIME [epoch: 6.41 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.262889891865661		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.262889891865661 | validation: 0.349742504701892]
	TIME [epoch: 6.41 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.37467649605211234		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.37467649605211234 | validation: 0.33643989927580525]
	TIME [epoch: 6.41 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2550928364475872		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.2550928364475872 | validation: 0.5034634963173831]
	TIME [epoch: 6.44 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2350298212057121		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.2350298212057121 | validation: 0.30291875393786616]
	TIME [epoch: 6.46 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1991898118219009		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.1991898118219009 | validation: 0.32337979607990286]
	TIME [epoch: 6.41 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3136868676878214		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.3136868676878214 | validation: 0.38749854901006703]
	TIME [epoch: 6.41 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24333958292489982		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.24333958292489982 | validation: 0.3222168914141676]
	TIME [epoch: 6.42 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21520965791422375		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.21520965791422375 | validation: 0.5352978789269454]
	TIME [epoch: 6.41 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24011725552152619		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.24011725552152619 | validation: 0.2974230486980518]
	TIME [epoch: 6.44 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1668825893922551		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.1668825893922551 | validation: 0.2972596147907297]
	TIME [epoch: 6.43 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15858658037749662		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.15858658037749662 | validation: 0.28662097607974896]
	TIME [epoch: 6.4 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19359747597512744		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.19359747597512744 | validation: 0.4990183460190501]
	TIME [epoch: 6.42 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2544030918672946		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.2544030918672946 | validation: 0.45263072292131207]
	TIME [epoch: 6.4 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1914995453938508		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.1914995453938508 | validation: 0.22287944876041174]
	TIME [epoch: 6.41 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16310659647346273		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.16310659647346273 | validation: 0.20760684011110764]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1801843904513084		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.1801843904513084 | validation: 0.6469532931611258]
	TIME [epoch: 6.43 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2573535159149748		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.2573535159149748 | validation: 0.5136126718779208]
	TIME [epoch: 6.42 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22999421967398392		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.22999421967398392 | validation: 0.4014592583040571]
	TIME [epoch: 6.41 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19925345330366817		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.19925345330366817 | validation: 0.4028828913734537]
	TIME [epoch: 6.41 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21803380092395636		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.21803380092395636 | validation: 0.39229285632690475]
	TIME [epoch: 6.42 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24568645134818484		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.24568645134818484 | validation: 0.24784375172520107]
	TIME [epoch: 6.44 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31200525282440383		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.31200525282440383 | validation: 0.23574907219000593]
	TIME [epoch: 6.45 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22344753738502632		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.22344753738502632 | validation: 0.7298654236031213]
	TIME [epoch: 6.4 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2300531868646318		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.2300531868646318 | validation: 0.24274863255134194]
	TIME [epoch: 6.41 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21959209190250562		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.21959209190250562 | validation: 0.270763981987068]
	TIME [epoch: 6.41 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27020425236857953		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.27020425236857953 | validation: 0.4023369768562467]
	TIME [epoch: 6.42 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20486397510094242		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.20486397510094242 | validation: 0.18731017097419775]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.169346007894617		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.169346007894617 | validation: 0.18658748335866257]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15196919210198867		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.15196919210198867 | validation: 0.3521977493927282]
	TIME [epoch: 6.44 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.171505070035527		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.171505070035527 | validation: 0.31660255166087425]
	TIME [epoch: 6.45 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2048743360841629		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.2048743360841629 | validation: 0.521241622691693]
	TIME [epoch: 6.45 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18776201824051936		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.18776201824051936 | validation: 0.2376369782883618]
	TIME [epoch: 6.46 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29009202668648015		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.29009202668648015 | validation: 0.18593301217488276]
	TIME [epoch: 6.49 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15468775763148399		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.15468775763148399 | validation: 0.4285369670447125]
	TIME [epoch: 6.42 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1966340312540198		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.1966340312540198 | validation: 0.312968414988562]
	TIME [epoch: 6.42 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14192744100790675		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.14192744100790675 | validation: 0.17340620372080623]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17586849111205957		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.17586849111205957 | validation: 0.2086049195098899]
	TIME [epoch: 6.45 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2456184335193896		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.2456184335193896 | validation: 0.23111395532822254]
	TIME [epoch: 6.46 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1932246059778134		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.1932246059778134 | validation: 0.5094484761479604]
	TIME [epoch: 6.46 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2625059831854618		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.2625059831854618 | validation: 0.2548131487240552]
	TIME [epoch: 6.44 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15792641440949723		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.15792641440949723 | validation: 0.19422036450804023]
	TIME [epoch: 6.43 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1654866156677545		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.1654866156677545 | validation: 0.2008743962976987]
	TIME [epoch: 6.44 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15225403009356814		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.15225403009356814 | validation: 0.38187324883581086]
	TIME [epoch: 6.43 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15102673646144543		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.15102673646144543 | validation: 0.1588039513183478]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16465121343267858		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.16465121343267858 | validation: 0.22455343614593964]
	TIME [epoch: 6.48 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21245713907498281		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.21245713907498281 | validation: 0.166904094934613]
	TIME [epoch: 6.45 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2980992886002752		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.2980992886002752 | validation: 0.1703832685241026]
	TIME [epoch: 6.46 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20157705539603393		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.20157705539603393 | validation: 0.26375383889118265]
	TIME [epoch: 6.43 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1690472977475455		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.1690472977475455 | validation: 0.2423406413543075]
	TIME [epoch: 6.45 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21691807487391943		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.21691807487391943 | validation: 0.46592290673085995]
	TIME [epoch: 6.47 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21933739887789808		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.21933739887789808 | validation: 0.3377616312034323]
	TIME [epoch: 6.48 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16915953523221264		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.16915953523221264 | validation: 0.3225118171419081]
	TIME [epoch: 6.43 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15458055134410448		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.15458055134410448 | validation: 0.2078295076563122]
	TIME [epoch: 6.44 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1487552934268663		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.1487552934268663 | validation: 0.4218139024468502]
	TIME [epoch: 6.44 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16465340377297139		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.16465340377297139 | validation: 0.17059802294830734]
	TIME [epoch: 6.45 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11719031213015416		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.11719031213015416 | validation: 0.17341361491194981]
	TIME [epoch: 6.47 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12325829233549049		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.12325829233549049 | validation: 0.19244292995318882]
	TIME [epoch: 6.49 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1893356836345647		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.1893356836345647 | validation: 0.1805915001810787]
	TIME [epoch: 6.44 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18514236073074242		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.18514236073074242 | validation: 0.16556370420261232]
	TIME [epoch: 6.46 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12773129784592885		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.12773129784592885 | validation: 0.2900635996991401]
	TIME [epoch: 6.44 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13790018580938654		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.13790018580938654 | validation: 0.32147924575010034]
	TIME [epoch: 6.47 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21822190433290448		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.21822190433290448 | validation: 0.5529543369515608]
	TIME [epoch: 6.49 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18815744860315003		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.18815744860315003 | validation: 0.1934839611252922]
	TIME [epoch: 6.47 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30610187239375597		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.30610187239375597 | validation: 0.35438474277416443]
	TIME [epoch: 6.45 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21247778717043536		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.21247778717043536 | validation: 0.3180614847273637]
	TIME [epoch: 6.47 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16814962445314657		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.16814962445314657 | validation: 0.18216648336635688]
	TIME [epoch: 6.45 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12085425916427668		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.12085425916427668 | validation: 0.15289267043790586]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11397877899179751		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.11397877899179751 | validation: 0.14697015439793054]
	TIME [epoch: 6.49 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11103414601565142		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.11103414601565142 | validation: 0.14872608907686657]
	TIME [epoch: 6.47 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10975254824458551		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.10975254824458551 | validation: 0.14619684054590745]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13248325541378167		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.13248325541378167 | validation: 0.3539400930158776]
	TIME [epoch: 6.42 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22824009518791422		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.22824009518791422 | validation: 0.24406650531136093]
	TIME [epoch: 6.42 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18471806540084268		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.18471806540084268 | validation: 0.19954532689784504]
	TIME [epoch: 6.42 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11907133662785306		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.11907133662785306 | validation: 0.1935050526893628]
	TIME [epoch: 6.46 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13882803333482224		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.13882803333482224 | validation: 0.24233364406688718]
	TIME [epoch: 6.43 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16297972790491133		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.16297972790491133 | validation: 0.3858394098565251]
	TIME [epoch: 6.44 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15916555530370222		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.15916555530370222 | validation: 0.13961905559663457]
	TIME [epoch: 6.52 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10813647738594828		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.10813647738594828 | validation: 0.15328620416962657]
	TIME [epoch: 6.43 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11807117826883229		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.11807117826883229 | validation: 0.2668202006036807]
	TIME [epoch: 6.44 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23536913358196554		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.23536913358196554 | validation: 0.27755065467415835]
	TIME [epoch: 6.46 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1561665172298854		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.1561665172298854 | validation: 0.9174582488718892]
	TIME [epoch: 6.45 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7610127433765987		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.7610127433765987 | validation: 0.5484912204959169]
	TIME [epoch: 6.42 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2683362358768577		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.2683362358768577 | validation: 0.29519237642490675]
	TIME [epoch: 6.42 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18274066571120567		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.18274066571120567 | validation: 0.15680087953357008]
	TIME [epoch: 6.45 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12167086779692521		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.12167086779692521 | validation: 0.17031918995654383]
	TIME [epoch: 6.4 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18283623872502305		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.18283623872502305 | validation: 0.1354990730251728]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10818394439204948		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.10818394439204948 | validation: 0.2643519742498541]
	TIME [epoch: 6.47 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14338999949844736		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.14338999949844736 | validation: 0.15028724621348832]
	TIME [epoch: 6.41 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1349010582647408		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.1349010582647408 | validation: 0.32098816542448755]
	TIME [epoch: 6.43 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15014038360835408		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.15014038360835408 | validation: 0.190156904649341]
	TIME [epoch: 6.41 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11411482105406769		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.11411482105406769 | validation: 0.14662891410997347]
	TIME [epoch: 6.42 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10554887241933358		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.10554887241933358 | validation: 0.12720277811822864]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1575680941796463		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.1575680941796463 | validation: 0.1863898197766002]
	TIME [epoch: 6.49 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17266769011788868		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.17266769011788868 | validation: 0.13490561343313098]
	TIME [epoch: 6.42 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10752302638013204		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.10752302638013204 | validation: 0.13555140243574487]
	TIME [epoch: 6.43 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10260387530472456		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.10260387530472456 | validation: 0.13868058731504787]
	TIME [epoch: 6.42 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11674292081370738		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.11674292081370738 | validation: 0.3520015517523846]
	TIME [epoch: 6.42 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1613382206284647		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.1613382206284647 | validation: 0.16191941906108412]
	TIME [epoch: 6.42 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10412160717630597		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.10412160717630597 | validation: 0.12203043281321622]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10150262745219704		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.10150262745219704 | validation: 0.18543109051088]
	TIME [epoch: 6.45 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1483802081130841		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.1483802081130841 | validation: 0.21491207090218112]
	TIME [epoch: 6.45 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14334773059876094		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.14334773059876094 | validation: 0.23556964452471993]
	TIME [epoch: 6.44 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12465885479602265		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.12465885479602265 | validation: 0.5247368948043917]
	TIME [epoch: 6.43 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32880547322007003		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.32880547322007003 | validation: 0.12997972286267598]
	TIME [epoch: 6.41 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12480049648732232		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.12480049648732232 | validation: 0.14684722260440763]
	TIME [epoch: 6.45 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11282374109797541		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.11282374109797541 | validation: 0.2117470302261865]
	TIME [epoch: 6.47 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10523143220647072		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.10523143220647072 | validation: 0.13345108348117227]
	TIME [epoch: 6.42 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14209294512734974		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.14209294512734974 | validation: 0.1421805807758731]
	TIME [epoch: 6.43 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11469660125394719		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.11469660125394719 | validation: 0.3723234073538863]
	TIME [epoch: 6.44 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13318974150685037		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.13318974150685037 | validation: 0.15698346585292372]
	TIME [epoch: 6.44 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13659486973663904		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.13659486973663904 | validation: 0.12461106105484523]
	TIME [epoch: 6.47 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09914898639759931		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.09914898639759931 | validation: 0.1178851825471477]
	TIME [epoch: 6.49 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10638584240670629		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.10638584240670629 | validation: 0.16288010561494293]
	TIME [epoch: 6.43 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13117448483729077		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.13117448483729077 | validation: 1.0895837589279276]
	TIME [epoch: 6.44 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.430822830482931		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.430822830482931 | validation: 0.14103241585949722]
	TIME [epoch: 6.43 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1472422853254624		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.1472422853254624 | validation: 0.1253196293892839]
	TIME [epoch: 6.44 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10333513642114972		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.10333513642114972 | validation: 0.1188084839620124]
	TIME [epoch: 6.45 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09624399216501789		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.09624399216501789 | validation: 0.12764685469208945]
	TIME [epoch: 6.48 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10301411606112158		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.10301411606112158 | validation: 0.18467158778323664]
	TIME [epoch: 6.43 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12171161003069432		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.12171161003069432 | validation: 0.1992219880254504]
	TIME [epoch: 6.43 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11538295111044562		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.11538295111044562 | validation: 0.1581565454133557]
	TIME [epoch: 6.44 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09874425749078627		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.09874425749078627 | validation: 0.15939085352002186]
	TIME [epoch: 6.43 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09601652508162661		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.09601652508162661 | validation: 0.16212949020034242]
	TIME [epoch: 6.45 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12672156266531062		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.12672156266531062 | validation: 0.1439022234936565]
	TIME [epoch: 6.46 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14226385309347553		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.14226385309347553 | validation: 0.19495624673442746]
	TIME [epoch: 6.44 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1033491911804967		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.1033491911804967 | validation: 0.11209445503231508]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09479326468887433		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.09479326468887433 | validation: 0.13177298708053078]
	TIME [epoch: 6.43 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09786504177685483		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.09786504177685483 | validation: 0.24799530519692828]
	TIME [epoch: 6.44 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16604992574599098		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.16604992574599098 | validation: 0.1501140705544401]
	TIME [epoch: 6.45 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09764942090176643		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.09764942090176643 | validation: 0.10895823536722274]
	TIME [epoch: 6.51 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09752174234936017		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.09752174234936017 | validation: 0.1398591432986027]
	TIME [epoch: 6.44 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10440425036037382		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.10440425036037382 | validation: 0.22291572001740462]
	TIME [epoch: 6.44 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12123829194789373		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.12123829194789373 | validation: 0.14706181016096126]
	TIME [epoch: 6.45 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1091381015179489		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.1091381015179489 | validation: 0.11645099432276217]
	TIME [epoch: 6.43 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09552836842121337		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.09552836842121337 | validation: 0.27441302083445324]
	TIME [epoch: 6.47 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24029492470341596		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.24029492470341596 | validation: 0.2887648784295362]
	TIME [epoch: 6.46 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19545552610594671		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.19545552610594671 | validation: 0.26812166915279445]
	TIME [epoch: 6.46 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14265107012296713		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.14265107012296713 | validation: 0.15286501981671885]
	TIME [epoch: 6.44 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10151470701554471		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.10151470701554471 | validation: 0.12671758762726615]
	TIME [epoch: 6.45 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17232070408848743		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.17232070408848743 | validation: 0.23623044734473628]
	TIME [epoch: 6.44 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1298211826261727		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.1298211826261727 | validation: 0.1274207645967366]
	TIME [epoch: 6.46 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10415430420134122		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.10415430420134122 | validation: 0.23004875448413792]
	TIME [epoch: 6.48 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1633192170160461		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.1633192170160461 | validation: 0.21583076967047798]
	TIME [epoch: 6.47 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11607864471417767		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.11607864471417767 | validation: 0.10117328648617424]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08703890874596254		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.08703890874596254 | validation: 0.12983356754914263]
	TIME [epoch: 6.45 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0873766013173431		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.0873766013173431 | validation: 0.09906575188556513]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1002781476461162		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.1002781476461162 | validation: 0.11733138032874463]
	TIME [epoch: 6.46 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09931472064862648		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.09931472064862648 | validation: 0.16221800334739256]
	TIME [epoch: 6.48 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10821682599355055		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.10821682599355055 | validation: 0.15636252309756551]
	TIME [epoch: 6.46 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09175911705366696		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.09175911705366696 | validation: 0.12073198026599395]
	TIME [epoch: 6.45 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10877234865966434		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.10877234865966434 | validation: 0.11333063056565303]
	TIME [epoch: 6.45 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10553933142361145		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.10553933142361145 | validation: 0.12428949814907386]
	TIME [epoch: 6.45 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08689343547133335		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.08689343547133335 | validation: 0.1392897163671076]
	TIME [epoch: 6.45 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10359097644364776		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.10359097644364776 | validation: 0.12316083859033072]
	TIME [epoch: 6.49 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08499484507308427		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.08499484507308427 | validation: 0.10022107248783914]
	TIME [epoch: 6.47 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09414141786883956		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.09414141786883956 | validation: 0.322719542858837]
	TIME [epoch: 6.44 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15703512910497963		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.15703512910497963 | validation: 0.1218381064618258]
	TIME [epoch: 6.45 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11827405371897434		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.11827405371897434 | validation: 0.11542860210647864]
	TIME [epoch: 6.44 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09300648139197826		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.09300648139197826 | validation: 0.1352754857695477]
	TIME [epoch: 6.46 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.100797948277729		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.100797948277729 | validation: 0.12256344461486668]
	TIME [epoch: 6.46 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08962936958427914		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.08962936958427914 | validation: 0.14530290245620053]
	TIME [epoch: 6.48 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09980732969541697		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.09980732969541697 | validation: 0.10715256298650076]
	TIME [epoch: 6.45 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08178574892081648		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.08178574892081648 | validation: 0.13265767478577745]
	TIME [epoch: 6.45 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11214667110391403		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.11214667110391403 | validation: 0.2474050494053003]
	TIME [epoch: 6.47 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11194291524730618		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.11194291524730618 | validation: 0.09972218891112555]
	TIME [epoch: 6.45 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08688740450586455		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.08688740450586455 | validation: 0.1010816235391774]
	TIME [epoch: 6.47 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09080602514302488		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.09080602514302488 | validation: 0.21706761206847103]
	TIME [epoch: 6.48 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10388136652609795		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.10388136652609795 | validation: 0.0949751686488125]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09144083833190358		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.09144083833190358 | validation: 0.27784127009132753]
	TIME [epoch: 6.45 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22970976156988973		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.22970976156988973 | validation: 0.19020139011811021]
	TIME [epoch: 6.47 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10479965613935638		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.10479965613935638 | validation: 0.10985096965532337]
	TIME [epoch: 6.46 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09440736368428125		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.09440736368428125 | validation: 0.13943114476362337]
	TIME [epoch: 6.5 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0956476314864696		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.0956476314864696 | validation: 0.09255618972731809]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_442.pth
	Model improved!!!
EPOCH 443/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0797608641243932		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.0797608641243932 | validation: 0.09104803368827127]
	TIME [epoch: 6.42 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10602211462542567		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.10602211462542567 | validation: 0.1282562863313176]
	TIME [epoch: 6.43 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08591339125604479		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.08591339125604479 | validation: 0.10815855024559573]
	TIME [epoch: 6.42 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.080089503363626		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.080089503363626 | validation: 0.09948030978368183]
	TIME [epoch: 6.44 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0802531218361951		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.0802531218361951 | validation: 0.1234148897434551]
	TIME [epoch: 6.45 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08950083225012986		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.08950083225012986 | validation: 0.09756877957794707]
	TIME [epoch: 6.45 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0878910753164297		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.0878910753164297 | validation: 0.10416114698404931]
	TIME [epoch: 6.44 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08028031713262969		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.08028031713262969 | validation: 0.22080289668181852]
	TIME [epoch: 6.42 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10814791711863744		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.10814791711863744 | validation: 0.08803787845635523]
	TIME [epoch: 6.42 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07394628031532005		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.07394628031532005 | validation: 0.11567411256206683]
	TIME [epoch: 6.43 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09028842077042479		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.09028842077042479 | validation: 0.15270624258318424]
	TIME [epoch: 6.46 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10114917273011745		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.10114917273011745 | validation: 0.09427135916150019]
	TIME [epoch: 6.44 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08842919928328968		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.08842919928328968 | validation: 0.09573977592433508]
	TIME [epoch: 6.43 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07642669344652142		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.07642669344652142 | validation: 0.08747164690023279]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08578420729921313		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.08578420729921313 | validation: 0.1019947476446219]
	TIME [epoch: 6.43 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09845701264078066		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.09845701264078066 | validation: 0.09710850972064677]
	TIME [epoch: 6.43 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08134854886416185		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.08134854886416185 | validation: 0.13025510799904513]
	TIME [epoch: 6.43 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0947431373556459		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.0947431373556459 | validation: 0.09455810087225591]
	TIME [epoch: 6.45 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08502922328780721		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.08502922328780721 | validation: 0.10452468936963236]
	TIME [epoch: 6.43 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08352982094115097		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.08352982094115097 | validation: 0.12230043484230313]
	TIME [epoch: 6.44 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10114574521670117		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.10114574521670117 | validation: 0.10363822861247177]
	TIME [epoch: 6.41 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08248461032409435		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.08248461032409435 | validation: 0.20503047526819349]
	TIME [epoch: 6.41 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11386612588314414		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.11386612588314414 | validation: 0.097573363093711]
	TIME [epoch: 6.43 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09124933652588987		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.09124933652588987 | validation: 0.2435769873637229]
	TIME [epoch: 6.45 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18537316113536517		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.18537316113536517 | validation: 0.1317539888420898]
	TIME [epoch: 6.44 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08581235246962876		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.08581235246962876 | validation: 0.20971300568891735]
	TIME [epoch: 6.41 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10617572654273114		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.10617572654273114 | validation: 0.0856402903395439]
	TIME [epoch: 6.42 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07950155407787121		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.07950155407787121 | validation: 0.09030011594964026]
	TIME [epoch: 6.43 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07554672582594328		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.07554672582594328 | validation: 0.08179264944600247]
	TIME [epoch: 6.42 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07397911414116054		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.07397911414116054 | validation: 0.08292492512162156]
	TIME [epoch: 6.47 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07882888874096097		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.07882888874096097 | validation: 0.0893287382033224]
	TIME [epoch: 6.45 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08325043367106041		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.08325043367106041 | validation: 0.09012626517173952]
	TIME [epoch: 6.43 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07947487112259642		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.07947487112259642 | validation: 0.13216141541053192]
	TIME [epoch: 6.43 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07951152879663662		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.07951152879663662 | validation: 0.08232233984980245]
	TIME [epoch: 6.43 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0753862863702642		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.0753862863702642 | validation: 0.08994688282620736]
	TIME [epoch: 6.43 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07582605531572864		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.07582605531572864 | validation: 0.09519474229553954]
	TIME [epoch: 6.49 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0808077025193907		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.0808077025193907 | validation: 0.1232344905456133]
	TIME [epoch: 6.45 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09209580513202295		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.09209580513202295 | validation: 0.09788176739943236]
	TIME [epoch: 6.44 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07294026142807154		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.07294026142807154 | validation: 0.09277957580054777]
	TIME [epoch: 6.44 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08364228829061679		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.08364228829061679 | validation: 0.10150710105160841]
	TIME [epoch: 6.44 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10927657087588385		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.10927657087588385 | validation: 0.1067473571241741]
	TIME [epoch: 6.43 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08300253439145916		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.08300253439145916 | validation: 0.09297488061608652]
	TIME [epoch: 6.45 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08249068648857644		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.08249068648857644 | validation: 0.11255073545403334]
	TIME [epoch: 6.45 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07998337264092673		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.07998337264092673 | validation: 0.0897598540678568]
	TIME [epoch: 6.44 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07298611356207488		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.07298611356207488 | validation: 0.09338183161296298]
	TIME [epoch: 6.44 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13181990801270868		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.13181990801270868 | validation: 0.08047476403676318]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09277200141221585		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.09277200141221585 | validation: 0.08089886580127517]
	TIME [epoch: 6.45 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07357996952104903		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.07357996952104903 | validation: 0.0862038107959994]
	TIME [epoch: 6.49 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07325146611844227		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.07325146611844227 | validation: 0.09528267603844989]
	TIME [epoch: 6.48 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07908007045589005		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.07908007045589005 | validation: 0.08230222193698501]
	TIME [epoch: 6.45 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07721771489875394		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.07721771489875394 | validation: 0.07902114020737044]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07330296168774676		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.07330296168774676 | validation: 0.09252612642027197]
	TIME [epoch: 6.44 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.071948731292312		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.071948731292312 | validation: 0.08645325743807304]
	TIME [epoch: 6.46 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07180917249261774		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.07180917249261774 | validation: 0.08733533005853394]
	TIME [epoch: 6.48 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08353769780001899		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.08353769780001899 | validation: 0.08293600717290261]
	TIME [epoch: 6.47 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07442792256519264		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.07442792256519264 | validation: 0.12750368740456416]
	TIME [epoch: 6.45 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08085985506033845		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.08085985506033845 | validation: 0.09542275403988354]
	TIME [epoch: 6.45 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08591937897162169		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.08591937897162169 | validation: 0.08123311561995446]
	TIME [epoch: 6.47 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07151147750580436		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.07151147750580436 | validation: 0.11462251845068205]
	TIME [epoch: 114 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1063547695363632		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.1063547695363632 | validation: 0.08236606332998413]
	TIME [epoch: 14 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07738499776407655		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.07738499776407655 | validation: 0.08129273698543248]
	TIME [epoch: 14.1 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07251013055606273		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.07251013055606273 | validation: 0.10725645942795868]
	TIME [epoch: 14.1 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07508568981563893		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.07508568981563893 | validation: 0.08732321602623253]
	TIME [epoch: 14.1 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07673475210841685		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.07673475210841685 | validation: 0.09591201903325472]
	TIME [epoch: 14.1 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.074424172517535		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.074424172517535 | validation: 0.08123674229346678]
	TIME [epoch: 14.1 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06841836597922171		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.06841836597922171 | validation: 0.07965369459124337]
	TIME [epoch: 14.1 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06973722015858821		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.06973722015858821 | validation: 0.08303145806270538]
	TIME [epoch: 14.1 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07184470348894836		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.07184470348894836 | validation: 0.10480171533860526]
	TIME [epoch: 14.1 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07639229289450883		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.07639229289450883 | validation: 0.21273886865643876]
	TIME [epoch: 14.1 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1323778351863194		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.1323778351863194 | validation: 0.07987567834738393]
	TIME [epoch: 14.1 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08007982097286936		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.08007982097286936 | validation: 0.08359966418666978]
	TIME [epoch: 14.1 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07607963076936111		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.07607963076936111 | validation: 0.08621843169613813]
	TIME [epoch: 14.1 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07551274855141486		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.07551274855141486 | validation: 0.09162742205713481]
	TIME [epoch: 14.1 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07395588525216054		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.07395588525216054 | validation: 0.08638597879000649]
	TIME [epoch: 14.1 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06885543544398559		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.06885543544398559 | validation: 0.07948376332842988]
	TIME [epoch: 14.1 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0689377688432713		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.0689377688432713 | validation: 0.07669269720627006]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08046068714400592		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.08046068714400592 | validation: 0.15461849549878504]
	TIME [epoch: 14.1 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08482060826904429		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.08482060826904429 | validation: 0.07780874187493857]
	TIME [epoch: 14.1 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06743533138218841		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.06743533138218841 | validation: 0.0786751414977066]
	TIME [epoch: 14.1 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06695478225161952		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.06695478225161952 | validation: 0.07930399490563803]
	TIME [epoch: 14.1 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06784846850801379		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.06784846850801379 | validation: 0.07460129388394843]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06929703310488111		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.06929703310488111 | validation: 0.07539101129796268]
	TIME [epoch: 14.1 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06906982812349274		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.06906982812349274 | validation: 0.12744883001530116]
	TIME [epoch: 14.1 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07810967764349724		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.07810967764349724 | validation: 0.07707484390793222]
	TIME [epoch: 14.1 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06755380818395595		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.06755380818395595 | validation: 0.08232191286266133]
	TIME [epoch: 14.1 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06708857491870975		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.06708857491870975 | validation: 0.0793902605824951]
	TIME [epoch: 14.1 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07412708886609656		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.07412708886609656 | validation: 0.07491625450223074]
	TIME [epoch: 14.1 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0745371372344019		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.0745371372344019 | validation: 0.11844640328921169]
	TIME [epoch: 14.1 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08191828895274181		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.08191828895274181 | validation: 0.071959873122671]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0675013190374567		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.0675013190374567 | validation: 0.07950265103913151]
	TIME [epoch: 14.1 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07039633834452699		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.07039633834452699 | validation: 0.07384289631029307]
	TIME [epoch: 14.1 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15146796627738993		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.15146796627738993 | validation: 0.16001486142617205]
	TIME [epoch: 14.1 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12002529355499444		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.12002529355499444 | validation: 0.10156856763472187]
	TIME [epoch: 14.1 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07129199409161663		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.07129199409161663 | validation: 0.06831309321107004]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07051192751792719		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.07051192751792719 | validation: 0.0733196519522358]
	TIME [epoch: 14.1 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06564215746019104		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.06564215746019104 | validation: 0.07098206171501464]
	TIME [epoch: 14.1 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06392113947062729		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.06392113947062729 | validation: 0.06924964783996335]
	TIME [epoch: 14.1 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06792001919315319		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.06792001919315319 | validation: 0.07201325261678522]
	TIME [epoch: 14.1 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06387847732538268		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.06387847732538268 | validation: 0.07381210253341962]
	TIME [epoch: 14.1 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06748979712334721		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.06748979712334721 | validation: 0.07702942701156903]
	TIME [epoch: 14.1 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06446003009648618		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.06446003009648618 | validation: 0.0856130199932359]
	TIME [epoch: 14.1 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06601305147137262		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.06601305147137262 | validation: 0.07454571216631868]
	TIME [epoch: 14.1 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06796632265126083		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.06796632265126083 | validation: 0.0925851826334006]
	TIME [epoch: 14.1 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06912857829573747		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.06912857829573747 | validation: 0.086737852630484]
	TIME [epoch: 14.1 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07138427390415605		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.07138427390415605 | validation: 0.0842097484832749]
	TIME [epoch: 14.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0638361657590841		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.0638361657590841 | validation: 0.0691350470687342]
	TIME [epoch: 14.1 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06499739522816317		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.06499739522816317 | validation: 0.06876868753682334]
	TIME [epoch: 14.1 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06617293007065628		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.06617293007065628 | validation: 0.07475190687327914]
	TIME [epoch: 14.1 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06447667508625923		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.06447667508625923 | validation: 0.08956196153378587]
	TIME [epoch: 14.1 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06999302772694606		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.06999302772694606 | validation: 0.07811330391604294]
	TIME [epoch: 14 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06540273790514695		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.06540273790514695 | validation: 0.08170017526430642]
	TIME [epoch: 14.1 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0722422588441123		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.0722422588441123 | validation: 0.0823846845880925]
	TIME [epoch: 14.1 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06356317972023547		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.06356317972023547 | validation: 0.07022649532057355]
	TIME [epoch: 14.1 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07258222215874771		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.07258222215874771 | validation: 0.09402868352996128]
	TIME [epoch: 14.1 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08067280823408865		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.08067280823408865 | validation: 0.09694464510365702]
	TIME [epoch: 14.1 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07248877841132217		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.07248877841132217 | validation: 0.07932913472126585]
	TIME [epoch: 14.1 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08208887977931009		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.08208887977931009 | validation: 0.08218575292908598]
	TIME [epoch: 14.1 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07533713310359161		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.07533713310359161 | validation: 0.10653090314211336]
	TIME [epoch: 14.1 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07771967399600654		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.07771967399600654 | validation: 0.06875631192052506]
	TIME [epoch: 14.1 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06430632783803804		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.06430632783803804 | validation: 0.07542471726465133]
	TIME [epoch: 14.1 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06224829292230545		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.06224829292230545 | validation: 0.06954928365394808]
	TIME [epoch: 14.1 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06677241986809856		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.06677241986809856 | validation: 0.07445163932870563]
	TIME [epoch: 14.1 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07001294319699194		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.07001294319699194 | validation: 0.08358974683648453]
	TIME [epoch: 14.1 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06786763798083054		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.06786763798083054 | validation: 0.06690854601335021]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06420207732438865		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.06420207732438865 | validation: 0.07013577151324735]
	TIME [epoch: 14.1 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06118750181093126		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.06118750181093126 | validation: 0.06478434495519061]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061778084118556786		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.061778084118556786 | validation: 0.06654170048336194]
	TIME [epoch: 14.1 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06169520579310986		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.06169520579310986 | validation: 0.06714554695868363]
	TIME [epoch: 14.1 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06310115684034182		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.06310115684034182 | validation: 0.06708044582260808]
	TIME [epoch: 14.1 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06238236503028115		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.06238236503028115 | validation: 0.07262895921669146]
	TIME [epoch: 14.1 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06395579080476255		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.06395579080476255 | validation: 0.0675859410336567]
	TIME [epoch: 14.1 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06514191230429764		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.06514191230429764 | validation: 0.10366621319535547]
	TIME [epoch: 14.1 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09692431511524052		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.09692431511524052 | validation: 0.11029570877908922]
	TIME [epoch: 14.1 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07036460883273707		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.07036460883273707 | validation: 0.07573139498365239]
	TIME [epoch: 14.1 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07090242630155183		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.07090242630155183 | validation: 0.07644180606203588]
	TIME [epoch: 14.1 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06352171918858755		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.06352171918858755 | validation: 0.0661028365394974]
	TIME [epoch: 14.1 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06341818650596101		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.06341818650596101 | validation: 0.06769631024534895]
	TIME [epoch: 14.1 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0820962497759098		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.0820962497759098 | validation: 0.12835605786945078]
	TIME [epoch: 14.1 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08596207388366486		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.08596207388366486 | validation: 0.07195594795726011]
	TIME [epoch: 14.1 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06864514024790772		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.06864514024790772 | validation: 0.08620238810209849]
	TIME [epoch: 14.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0738322329642922		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.0738322329642922 | validation: 0.07083888338122273]
	TIME [epoch: 14.1 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06266952189773567		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.06266952189773567 | validation: 0.06617832656258281]
	TIME [epoch: 14.1 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06163996933577946		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.06163996933577946 | validation: 0.06691408371477625]
	TIME [epoch: 14.1 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061139180239818874		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.061139180239818874 | validation: 0.06587382770308495]
	TIME [epoch: 14 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07717096577423005		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.07717096577423005 | validation: 0.18574298209199436]
	TIME [epoch: 14.1 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1103092539833487		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.1103092539833487 | validation: 0.09113594066662144]
	TIME [epoch: 14.1 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0631544620414886		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.0631544620414886 | validation: 0.061616836661618934]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06424546223472899		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.06424546223472899 | validation: 0.06641366715209827]
	TIME [epoch: 14.1 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0618958106028648		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.0618958106028648 | validation: 0.06649709864916338]
	TIME [epoch: 14.2 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06196008540891626		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.06196008540891626 | validation: 0.06810491171366205]
	TIME [epoch: 14.1 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08859876316053952		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.08859876316053952 | validation: 0.1222996365162597]
	TIME [epoch: 14.1 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10500746262624379		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.10500746262624379 | validation: 0.06977985774333088]
	TIME [epoch: 14.1 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06362207856667548		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.06362207856667548 | validation: 0.07386680014168066]
	TIME [epoch: 14.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0624494530220361		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.0624494530220361 | validation: 0.0694280982556845]
	TIME [epoch: 14.1 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06164864536000796		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.06164864536000796 | validation: 0.06787602976548483]
	TIME [epoch: 14.1 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05802129008540972		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.05802129008540972 | validation: 0.06789669894549617]
	TIME [epoch: 14.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06397071719494533		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.06397071719494533 | validation: 0.06453126319086744]
	TIME [epoch: 14.1 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06287785166333813		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.06287785166333813 | validation: 0.06815966499425634]
	TIME [epoch: 14.1 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06048545635777795		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.06048545635777795 | validation: 0.06469705900369103]
	TIME [epoch: 14.1 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05902599603953892		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.05902599603953892 | validation: 0.0683790194651404]
	TIME [epoch: 14.2 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05978712370707068		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.05978712370707068 | validation: 0.0713635118306899]
	TIME [epoch: 14.1 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06477798265708273		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.06477798265708273 | validation: 0.0653828122594398]
	TIME [epoch: 14.1 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06610743349331095		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.06610743349331095 | validation: 0.06468328309219151]
	TIME [epoch: 14.2 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.062421526103128844		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.062421526103128844 | validation: 0.06276848214123437]
	TIME [epoch: 14.1 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059783901769414616		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.059783901769414616 | validation: 0.06812611390418791]
	TIME [epoch: 14.2 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061907003865367724		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.061907003865367724 | validation: 0.06838551640959517]
	TIME [epoch: 14.2 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08225044219207509		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.08225044219207509 | validation: 0.06682410110341999]
	TIME [epoch: 14.1 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06324713531943904		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.06324713531943904 | validation: 0.07419768472082881]
	TIME [epoch: 14.1 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06089647396090365		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.06089647396090365 | validation: 0.06431058641492685]
	TIME [epoch: 14.2 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057847353199025606		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.057847353199025606 | validation: 0.06573105491548414]
	TIME [epoch: 14.1 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06201259587780658		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.06201259587780658 | validation: 0.06271303886948626]
	TIME [epoch: 14.2 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05994026822393726		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.05994026822393726 | validation: 0.062491786954390664]
	TIME [epoch: 14.2 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06188553573417295		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.06188553573417295 | validation: 0.05998540792980975]
	TIME [epoch: 14.2 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06519518318268526		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.06519518318268526 | validation: 0.07386954321650317]
	TIME [epoch: 14.1 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061993576894537564		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.061993576894537564 | validation: 0.06159869159328322]
	TIME [epoch: 14.1 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06701662834190582		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.06701662834190582 | validation: 0.07700008266157854]
	TIME [epoch: 14.1 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06246922028562562		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.06246922028562562 | validation: 0.06953378662335873]
	TIME [epoch: 14.1 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06625510343951808		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.06625510343951808 | validation: 0.07358110080845268]
	TIME [epoch: 14.1 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06411245206849592		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.06411245206849592 | validation: 0.06549078802869623]
	TIME [epoch: 14.1 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06123323259554805		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.06123323259554805 | validation: 0.07103861920803815]
	TIME [epoch: 14.1 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06154519404496784		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.06154519404496784 | validation: 0.06464031414467075]
	TIME [epoch: 14.1 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05914938505881799		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.05914938505881799 | validation: 0.08352304951426853]
	TIME [epoch: 14.1 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06507223436698789		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.06507223436698789 | validation: 0.06506969782310511]
	TIME [epoch: 14.1 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07749655655934687		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.07749655655934687 | validation: 0.14215216581360696]
	TIME [epoch: 14.1 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09028532353843333		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.09028532353843333 | validation: 0.09241307175718211]
	TIME [epoch: 14.1 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06577304281334183		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.06577304281334183 | validation: 0.06337257292580148]
	TIME [epoch: 14.1 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06071172625638221		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.06071172625638221 | validation: 0.06812123143299319]
	TIME [epoch: 14.1 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0629474494799148		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.0629474494799148 | validation: 0.0668027390249816]
	TIME [epoch: 14.1 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06357480483713776		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.06357480483713776 | validation: 0.06810098665151894]
	TIME [epoch: 14.1 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05977686707827294		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.05977686707827294 | validation: 0.0605234039077603]
	TIME [epoch: 14.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06101787866602695		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.06101787866602695 | validation: 0.06775445223301627]
	TIME [epoch: 14.1 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05967753983001583		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.05967753983001583 | validation: 0.06857355707001021]
	TIME [epoch: 14.1 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05774563505500263		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.05774563505500263 | validation: 0.06645767440615169]
	TIME [epoch: 14.1 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05795152375730542		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.05795152375730542 | validation: 0.06579684587967159]
	TIME [epoch: 14.1 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060319734596184865		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.060319734596184865 | validation: 0.059641824991351934]
	TIME [epoch: 14.2 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_637.pth
	Model improved!!!
EPOCH 638/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056781089064050855		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.056781089064050855 | validation: 0.06705976570415731]
	TIME [epoch: 14.1 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06160272648210951		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.06160272648210951 | validation: 0.06413810568103077]
	TIME [epoch: 14.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05942662757366123		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.05942662757366123 | validation: 0.07176147994899143]
	TIME [epoch: 14.2 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06018739006641802		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.06018739006641802 | validation: 0.06411779127369643]
	TIME [epoch: 14.1 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058280443584685315		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.058280443584685315 | validation: 0.06241516045496123]
	TIME [epoch: 14.2 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06304143672032957		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.06304143672032957 | validation: 0.06261650462164861]
	TIME [epoch: 14.1 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05868092105700197		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.05868092105700197 | validation: 0.07023956387476674]
	TIME [epoch: 14.2 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060760863075935596		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.060760863075935596 | validation: 0.06399881716174947]
	TIME [epoch: 14.2 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061607180480069634		[learning rate: 0.00012324]
	Learning Rate: 0.000123245
	LOSS [training: 0.061607180480069634 | validation: 0.06651598355928724]
	TIME [epoch: 14.1 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06409909847636078		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.06409909847636078 | validation: 0.07259654913693006]
	TIME [epoch: 14.2 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059611751294012805		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.059611751294012805 | validation: 0.05979892758720567]
	TIME [epoch: 14.2 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05887717762831579		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.05887717762831579 | validation: 0.06455726021697575]
	TIME [epoch: 14.1 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0589220335647175		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.0589220335647175 | validation: 0.0649282183711039]
	TIME [epoch: 14.2 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06268261714423516		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.06268261714423516 | validation: 0.06677919017210251]
	TIME [epoch: 14.2 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0616060851033783		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.0616060851033783 | validation: 0.0611135446170179]
	TIME [epoch: 14.1 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06125730427318003		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.06125730427318003 | validation: 0.06504760558076297]
	TIME [epoch: 14.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058850654904154934		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.058850654904154934 | validation: 0.06937080294496603]
	TIME [epoch: 14.1 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05987089438279879		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.05987089438279879 | validation: 0.06525469922798358]
	TIME [epoch: 14.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05878021966194652		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.05878021966194652 | validation: 0.06149708042616773]
	TIME [epoch: 14.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059597355119782064		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.059597355119782064 | validation: 0.06769204910680715]
	TIME [epoch: 14.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057115064609811306		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.057115064609811306 | validation: 0.06507830135861703]
	TIME [epoch: 14.1 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06024537202950082		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.06024537202950082 | validation: 0.06499498382286749]
	TIME [epoch: 14.2 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05905855892975784		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.05905855892975784 | validation: 0.06344193243709859]
	TIME [epoch: 14.1 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06121729759568403		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.06121729759568403 | validation: 0.06177032286639055]
	TIME [epoch: 14.1 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06052136375435733		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.06052136375435733 | validation: 0.0580591772324445]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061052352847574		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.061052352847574 | validation: 0.07053898380749894]
	TIME [epoch: 14.1 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05681027932518104		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.05681027932518104 | validation: 0.06242430180623193]
	TIME [epoch: 14.2 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05908207215273624		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.05908207215273624 | validation: 0.062381835150427146]
	TIME [epoch: 14.2 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06138891806296051		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.06138891806296051 | validation: 0.06882179142274623]
	TIME [epoch: 14.2 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05696573582309271		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.05696573582309271 | validation: 0.060694960544128956]
	TIME [epoch: 14.1 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05629575066910662		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.05629575066910662 | validation: 0.06023989040851074]
	TIME [epoch: 14.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05958052452450506		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.05958052452450506 | validation: 0.06502734675176487]
	TIME [epoch: 14.1 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057739579250663184		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.057739579250663184 | validation: 0.06819737396010277]
	TIME [epoch: 14.2 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056790730894698355		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.056790730894698355 | validation: 0.06287951637402903]
	TIME [epoch: 14.2 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05915692492769563		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.05915692492769563 | validation: 0.062384547770264635]
	TIME [epoch: 14.1 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05684270595822339		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.05684270595822339 | validation: 0.060746409636595666]
	TIME [epoch: 14.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060047761049020966		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.060047761049020966 | validation: 0.06710461129669298]
	TIME [epoch: 14.1 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060465260325481586		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.060465260325481586 | validation: 0.060125558781787415]
	TIME [epoch: 14.1 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05915093301942177		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.05915093301942177 | validation: 0.07316472930613448]
	TIME [epoch: 14.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06340766548363393		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.06340766548363393 | validation: 0.06468792667108614]
	TIME [epoch: 14.2 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058924272950627674		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.058924272950627674 | validation: 0.06768283623179147]
	TIME [epoch: 14.2 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05915109953043992		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.05915109953043992 | validation: 0.058552034425577194]
	TIME [epoch: 14.2 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05608799719340673		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.05608799719340673 | validation: 0.059379846417178694]
	TIME [epoch: 14.1 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055132088211922295		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.055132088211922295 | validation: 0.0585302380403039]
	TIME [epoch: 14.1 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054166129629753194		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.054166129629753194 | validation: 0.06144264947222427]
	TIME [epoch: 14.2 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0575159005180029		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.0575159005180029 | validation: 0.05864285038217059]
	TIME [epoch: 14.1 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05795136051239477		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.05795136051239477 | validation: 0.06014571445543948]
	TIME [epoch: 14.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05697166973664698		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.05697166973664698 | validation: 0.062366508170749235]
	TIME [epoch: 14.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0571951307431261		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.0571951307431261 | validation: 0.05756621670882481]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_686.pth
	Model improved!!!
EPOCH 687/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058570369335195646		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.058570369335195646 | validation: 0.06134330075348964]
	TIME [epoch: 14.1 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059817736833172555		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.059817736833172555 | validation: 0.06632534277702043]
	TIME [epoch: 14.1 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05522235853773602		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.05522235853773602 | validation: 0.06517054410384902]
	TIME [epoch: 14.1 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05729088209208069		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.05729088209208069 | validation: 0.06238253082432352]
	TIME [epoch: 14.1 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056126930347122614		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.056126930347122614 | validation: 0.0610209692398109]
	TIME [epoch: 14.1 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05486695267738874		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.05486695267738874 | validation: 0.06085501393981181]
	TIME [epoch: 14.1 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05635484040291421		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.05635484040291421 | validation: 0.058089449913101214]
	TIME [epoch: 14.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05693407205513686		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.05693407205513686 | validation: 0.06017924186216429]
	TIME [epoch: 14.1 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05889356997800853		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.05889356997800853 | validation: 0.061326663013567755]
	TIME [epoch: 14.1 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06022593625014942		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.06022593625014942 | validation: 0.05647476170510031]
	TIME [epoch: 14.2 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_696.pth
	Model improved!!!
EPOCH 697/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05891889621184802		[learning rate: 8.5871e-05]
	Learning Rate: 8.58709e-05
	LOSS [training: 0.05891889621184802 | validation: 0.06073613356501733]
	TIME [epoch: 14.1 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05813892176616835		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.05813892176616835 | validation: 0.05800832055200744]
	TIME [epoch: 14.2 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055793056792303536		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.055793056792303536 | validation: 0.06145978444193461]
	TIME [epoch: 14.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05594596099976963		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.05594596099976963 | validation: 0.05832988139253964]
	TIME [epoch: 14.1 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056438233112365906		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.056438233112365906 | validation: 0.05834467448744969]
	TIME [epoch: 14.1 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056583213388278236		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.056583213388278236 | validation: 0.057623962456831206]
	TIME [epoch: 14.1 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059376964388791406		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.059376964388791406 | validation: 0.060640842041435876]
	TIME [epoch: 14.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05772425585268322		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.05772425585268322 | validation: 0.06008693725405137]
	TIME [epoch: 14.2 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057688427977682075		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.057688427977682075 | validation: 0.06124608433884959]
	TIME [epoch: 14.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05510793166293537		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.05510793166293537 | validation: 0.058642913410357725]
	TIME [epoch: 14.1 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05605824205724585		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.05605824205724585 | validation: 0.05928854283656429]
	TIME [epoch: 14.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0564453940987516		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.0564453940987516 | validation: 0.05507072344312791]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_708.pth
	Model improved!!!
EPOCH 709/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05497904262301488		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.05497904262301488 | validation: 0.0562337881990581]
	TIME [epoch: 14.1 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05320968715802548		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.05320968715802548 | validation: 0.0601132691851667]
	TIME [epoch: 14.1 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05931308217049129		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.05931308217049129 | validation: 0.059789440200889714]
	TIME [epoch: 14.1 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05603349573209636		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.05603349573209636 | validation: 0.055492162450939075]
	TIME [epoch: 14.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053826680688858944		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.053826680688858944 | validation: 0.06256799021000953]
	TIME [epoch: 14.2 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05653493951585703		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.05653493951585703 | validation: 0.061335710150009805]
	TIME [epoch: 14.2 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05501605439636366		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.05501605439636366 | validation: 0.059810026196291846]
	TIME [epoch: 14.1 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05391207386688564		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.05391207386688564 | validation: 0.056946430645783974]
	TIME [epoch: 14.8 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05504890690447811		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.05504890690447811 | validation: 0.05954177316426208]
	TIME [epoch: 14 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05562069635445874		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.05562069635445874 | validation: 0.06060039535427195]
	TIME [epoch: 14.1 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055936708721870745		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.055936708721870745 | validation: 0.05700624602970446]
	TIME [epoch: 14.1 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05586257895719411		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.05586257895719411 | validation: 0.056475670923905624]
	TIME [epoch: 14 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054718692844311384		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.054718692844311384 | validation: 0.055980661904299005]
	TIME [epoch: 14.1 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05836864019773415		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.05836864019773415 | validation: 0.0629747350556276]
	TIME [epoch: 14 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05577607014863316		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.05577607014863316 | validation: 0.05833323157073013]
	TIME [epoch: 14 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05558695260758055		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.05558695260758055 | validation: 0.05754737362290929]
	TIME [epoch: 14.1 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056050496499016084		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.056050496499016084 | validation: 0.05919390288493642]
	TIME [epoch: 14 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053856319614512455		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.053856319614512455 | validation: 0.058561700346959004]
	TIME [epoch: 14 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056348407539158235		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.056348407539158235 | validation: 0.0605662843696146]
	TIME [epoch: 14.1 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05280318740749802		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.05280318740749802 | validation: 0.06084846924106544]
	TIME [epoch: 14 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05438867200261484		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.05438867200261484 | validation: 0.05889473146143416]
	TIME [epoch: 14 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054781522846757615		[learning rate: 6.7968e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.054781522846757615 | validation: 0.056701293729569946]
	TIME [epoch: 14.1 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05387808554295761		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.05387808554295761 | validation: 0.055434209938527636]
	TIME [epoch: 14 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05418247559278136		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.05418247559278136 | validation: 0.05570087633349985]
	TIME [epoch: 14 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056928758964919335		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.056928758964919335 | validation: 0.052922656927048645]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_733.pth
	Model improved!!!
EPOCH 734/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05280678486207173		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.05280678486207173 | validation: 0.06219726082735658]
	TIME [epoch: 14 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05601004163308117		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.05601004163308117 | validation: 0.07024571388437328]
	TIME [epoch: 14 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056647315167904624		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.056647315167904624 | validation: 0.056291256042567045]
	TIME [epoch: 14.1 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053147778063986795		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.053147778063986795 | validation: 0.056066562663569845]
	TIME [epoch: 14 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05662164801611145		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.05662164801611145 | validation: 0.0594436962002382]
	TIME [epoch: 14.1 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05278662376254043		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.05278662376254043 | validation: 0.05965369390138317]
	TIME [epoch: 14.1 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054148021191005466		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.054148021191005466 | validation: 0.06113112845231803]
	TIME [epoch: 14.1 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05467895670636071		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.05467895670636071 | validation: 0.061395757633474046]
	TIME [epoch: 14.1 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05342434960993009		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.05342434960993009 | validation: 0.053700190872230706]
	TIME [epoch: 14 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05476566373504281		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.05476566373504281 | validation: 0.06150782762140954]
	TIME [epoch: 14 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055972738478987726		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.055972738478987726 | validation: 0.056626411106287494]
	TIME [epoch: 14.1 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052966179989827625		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.052966179989827625 | validation: 0.05776217751331724]
	TIME [epoch: 14.1 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057017072485702106		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.057017072485702106 | validation: 0.054564869001151296]
	TIME [epoch: 14.1 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05334817059116721		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.05334817059116721 | validation: 0.06289138911573111]
	TIME [epoch: 14.1 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05674519038957125		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.05674519038957125 | validation: 0.0592484913300779]
	TIME [epoch: 14.1 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05406437509809511		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.05406437509809511 | validation: 0.057928400013526817]
	TIME [epoch: 14.1 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0549508521358381		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.0549508521358381 | validation: 0.05714482438133196]
	TIME [epoch: 14.1 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05241519371619352		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.05241519371619352 | validation: 0.05748770920608248]
	TIME [epoch: 14.1 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05445435669206232		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.05445435669206232 | validation: 0.05456253435478528]
	TIME [epoch: 14.1 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05346151130656544		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.05346151130656544 | validation: 0.057801408120647726]
	TIME [epoch: 14.1 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056250456476590646		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.056250456476590646 | validation: 0.06017477855869489]
	TIME [epoch: 14.1 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053235489882143786		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.053235489882143786 | validation: 0.05392055654298466]
	TIME [epoch: 14 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053324636170225766		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.053324636170225766 | validation: 0.05907791077204291]
	TIME [epoch: 14.1 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05475085028071208		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.05475085028071208 | validation: 0.05787028303312224]
	TIME [epoch: 14 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055213397648751814		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.055213397648751814 | validation: 0.05516765455614943]
	TIME [epoch: 14.1 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05200567071000313		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.05200567071000313 | validation: 0.055628311871231566]
	TIME [epoch: 14.1 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055850047485711846		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.055850047485711846 | validation: 0.055113789075009106]
	TIME [epoch: 14.1 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05416312936002779		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.05416312936002779 | validation: 0.05660526316849024]
	TIME [epoch: 14.1 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05335200551253396		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.05335200551253396 | validation: 0.056290718288259156]
	TIME [epoch: 14.1 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05268990999628827		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.05268990999628827 | validation: 0.058755110319586815]
	TIME [epoch: 14.1 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05247933350200834		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.05247933350200834 | validation: 0.05516829182870078]
	TIME [epoch: 14.1 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05238894334633179		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.05238894334633179 | validation: 0.05834635520370493]
	TIME [epoch: 14.1 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056401313193418894		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.056401313193418894 | validation: 0.05808969853643206]
	TIME [epoch: 14 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05239207371649179		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.05239207371649179 | validation: 0.055353369824801296]
	TIME [epoch: 14.1 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0539730717718936		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.0539730717718936 | validation: 0.05844275083261145]
	TIME [epoch: 14 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05220789241863971		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.05220789241863971 | validation: 0.05690751794909073]
	TIME [epoch: 14 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05529532845460203		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.05529532845460203 | validation: 0.059456921494098584]
	TIME [epoch: 14.1 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053611148727634846		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.053611148727634846 | validation: 0.057538796893676775]
	TIME [epoch: 14.1 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056033575996298365		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.056033575996298365 | validation: 0.0600192459584639]
	TIME [epoch: 14.1 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053365500987139144		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.053365500987139144 | validation: 0.0564154426497148]
	TIME [epoch: 14.1 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0538685334088948		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.0538685334088948 | validation: 0.056524448832430046]
	TIME [epoch: 14.1 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051738276837688224		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.051738276837688224 | validation: 0.056907345633504774]
	TIME [epoch: 14.1 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05650728366768595		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.05650728366768595 | validation: 0.05794992942758065]
	TIME [epoch: 14.1 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05128113835287999		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.05128113835287999 | validation: 0.05444518505487389]
	TIME [epoch: 14 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05425191911097668		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.05425191911097668 | validation: 0.05716203234176312]
	TIME [epoch: 14.1 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05258856964545172		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.05258856964545172 | validation: 0.05753805654644965]
	TIME [epoch: 14.1 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05378079391347264		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.05378079391347264 | validation: 0.056320594971394966]
	TIME [epoch: 14.1 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05245610550420321		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.05245610550420321 | validation: 0.05795243443560272]
	TIME [epoch: 14.1 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054990128483902834		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.054990128483902834 | validation: 0.05706603000260832]
	TIME [epoch: 14 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0554223097923949		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.0554223097923949 | validation: 0.0562092440883639]
	TIME [epoch: 14 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05098631096895673		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.05098631096895673 | validation: 0.05589535303677289]
	TIME [epoch: 14.1 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05541763151237282		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.05541763151237282 | validation: 0.05780315304207387]
	TIME [epoch: 14 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0536072542682431		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.0536072542682431 | validation: 0.05414166956956108]
	TIME [epoch: 14.1 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0539704276629078		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.0539704276629078 | validation: 0.05362379607102596]
	TIME [epoch: 14.1 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05179168789180581		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.05179168789180581 | validation: 0.06197302290344665]
	TIME [epoch: 14.1 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05995007508332503		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.05995007508332503 | validation: 0.0590992146072067]
	TIME [epoch: 14.1 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05298717081542063		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.05298717081542063 | validation: 0.05612613861074561]
	TIME [epoch: 14.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05065901153438923		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.05065901153438923 | validation: 0.055257625794183945]
	TIME [epoch: 14.1 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05302653085834304		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.05302653085834304 | validation: 0.05513630173834926]
	TIME [epoch: 14 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05357716002336596		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.05357716002336596 | validation: 0.053980438456815785]
	TIME [epoch: 14.1 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05325524638715029		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.05325524638715029 | validation: 0.05617464576219836]
	TIME [epoch: 14 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05263780440650322		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.05263780440650322 | validation: 0.055307328835078044]
	TIME [epoch: 14.1 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052878877028825357		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.052878877028825357 | validation: 0.0576537843926305]
	TIME [epoch: 14.1 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05284067242080741		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.05284067242080741 | validation: 0.05601458342126025]
	TIME [epoch: 14 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05318582782171723		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.05318582782171723 | validation: 0.05381517560494431]
	TIME [epoch: 14.1 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049839692611825755		[learning rate: 4.1687e-05]
	Learning Rate: 4.16869e-05
	LOSS [training: 0.049839692611825755 | validation: 0.05630886495053389]
	TIME [epoch: 14.1 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05271145116641298		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.05271145116641298 | validation: 0.05378409293552187]
	TIME [epoch: 14.1 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052394962007445055		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.052394962007445055 | validation: 0.0505907767015388]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_801.pth
	Model improved!!!
EPOCH 802/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052557695863326606		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.052557695863326606 | validation: 0.056806225227765864]
	TIME [epoch: 14.1 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053917532483906405		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.053917532483906405 | validation: 0.052181792111317196]
	TIME [epoch: 14.1 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05359736156531408		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.05359736156531408 | validation: 0.054410605222901254]
	TIME [epoch: 14.1 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05245234257925794		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.05245234257925794 | validation: 0.051336577357421656]
	TIME [epoch: 14 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053972787729554134		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.053972787729554134 | validation: 0.055631437954473964]
	TIME [epoch: 14 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05268306982493916		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.05268306982493916 | validation: 0.054069786039604995]
	TIME [epoch: 14.1 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052628314702807835		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.052628314702807835 | validation: 0.05227575599191275]
	TIME [epoch: 14 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054767117040138744		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.054767117040138744 | validation: 0.05526712357960331]
	TIME [epoch: 14.1 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05093480656603873		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.05093480656603873 | validation: 0.05849810939921536]
	TIME [epoch: 14.1 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05270525672039076		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.05270525672039076 | validation: 0.05473935499861537]
	TIME [epoch: 14.1 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052465072560030426		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.052465072560030426 | validation: 0.0530571608202465]
	TIME [epoch: 14 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051345393519973664		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.051345393519973664 | validation: 0.05210783104523986]
	TIME [epoch: 14.1 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053705191346206264		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.053705191346206264 | validation: 0.058009148445010254]
	TIME [epoch: 14.1 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0533719225334896		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.0533719225334896 | validation: 0.05565048485893398]
	TIME [epoch: 14.1 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05216315130996299		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.05216315130996299 | validation: 0.057753755749999705]
	TIME [epoch: 14.1 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054343122906433686		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.054343122906433686 | validation: 0.054393618468287365]
	TIME [epoch: 14 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054857433672222226		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.054857433672222226 | validation: 0.055822921743561786]
	TIME [epoch: 14.1 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05124686146592601		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.05124686146592601 | validation: 0.05531700661679324]
	TIME [epoch: 14.1 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053506083329039715		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.053506083329039715 | validation: 0.057562183129278056]
	TIME [epoch: 14 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05266406514158434		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.05266406514158434 | validation: 0.0565110169263918]
	TIME [epoch: 14.1 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05247792401347251		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.05247792401347251 | validation: 0.05508567846263072]
	TIME [epoch: 14.1 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05651829231559383		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.05651829231559383 | validation: 0.05439319315019814]
	TIME [epoch: 14.1 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05162793653599493		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.05162793653599493 | validation: 0.05431475808734007]
	TIME [epoch: 14.1 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05211794377616613		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.05211794377616613 | validation: 0.054934357719865705]
	TIME [epoch: 14.1 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05034648531673028		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.05034648531673028 | validation: 0.05454292795723168]
	TIME [epoch: 14.1 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05069424532579375		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.05069424532579375 | validation: 0.05399982960600369]
	TIME [epoch: 14.1 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05024594481984498		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.05024594481984498 | validation: 0.05319207069749987]
	TIME [epoch: 14.1 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05317741351360651		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.05317741351360651 | validation: 0.05357116414366769]
	TIME [epoch: 14.1 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05038541137744504		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.05038541137744504 | validation: 0.05696515913638809]
	TIME [epoch: 14.1 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05166693743051422		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.05166693743051422 | validation: 0.05199924799396789]
	TIME [epoch: 14.1 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0514054493569602		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.0514054493569602 | validation: 0.0552868306609303]
	TIME [epoch: 14.1 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05189728836084101		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.05189728836084101 | validation: 0.05272719134762535]
	TIME [epoch: 14.1 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05089069565271259		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.05089069565271259 | validation: 0.05448329217031529]
	TIME [epoch: 14.1 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05156454760842413		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.05156454760842413 | validation: 0.05602678659425461]
	TIME [epoch: 14.1 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05109446130741323		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.05109446130741323 | validation: 0.05362627189568639]
	TIME [epoch: 14.1 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05141064993275257		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.05141064993275257 | validation: 0.052697831257169425]
	TIME [epoch: 14.1 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053428169611116696		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.053428169611116696 | validation: 0.05246536709901333]
	TIME [epoch: 14.1 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05415561534418325		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.05415561534418325 | validation: 0.05336037704107915]
	TIME [epoch: 14.1 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05393257738226365		[learning rate: 3.1178e-05]
	Learning Rate: 3.11778e-05
	LOSS [training: 0.05393257738226365 | validation: 0.05488872625924275]
	TIME [epoch: 14.1 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05149780717066604		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.05149780717066604 | validation: 0.054786548468945984]
	TIME [epoch: 14.1 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052214212958343165		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.052214212958343165 | validation: 0.053431819668677785]
	TIME [epoch: 14.1 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051932282054652515		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.051932282054652515 | validation: 0.05657972397081073]
	TIME [epoch: 14.1 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049723193466580703		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.049723193466580703 | validation: 0.052141312780726916]
	TIME [epoch: 14.1 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05390307101742424		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.05390307101742424 | validation: 0.054460459207330225]
	TIME [epoch: 14.1 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0512475853598513		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.0512475853598513 | validation: 0.05667903045188092]
	TIME [epoch: 14.1 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050630334442275195		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.050630334442275195 | validation: 0.05819318365038904]
	TIME [epoch: 14.1 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051629906343437684		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.051629906343437684 | validation: 0.05197139296141056]
	TIME [epoch: 14.1 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052141794750084326		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.052141794750084326 | validation: 0.053191993157988685]
	TIME [epoch: 14.1 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05233549131209697		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.05233549131209697 | validation: 0.05522680010948536]
	TIME [epoch: 14.1 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05158041241112776		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.05158041241112776 | validation: 0.05569714788625162]
	TIME [epoch: 14.1 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05203987691222295		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.05203987691222295 | validation: 0.05394355686976865]
	TIME [epoch: 14.1 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053420177186606584		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.053420177186606584 | validation: 0.052557095933022685]
	TIME [epoch: 14.1 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05189757990544455		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.05189757990544455 | validation: 0.05282194166052139]
	TIME [epoch: 14.1 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05253663471960929		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.05253663471960929 | validation: 0.05188916555571115]
	TIME [epoch: 14.1 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05094482538621435		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.05094482538621435 | validation: 0.054031727326889434]
	TIME [epoch: 14.1 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05139760030746397		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.05139760030746397 | validation: 0.05757264300434291]
	TIME [epoch: 14.1 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04971644399964981		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.04971644399964981 | validation: 0.0540645208697544]
	TIME [epoch: 14.1 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05338916769222818		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.05338916769222818 | validation: 0.054791234636713426]
	TIME [epoch: 14.1 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0522917992931557		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.0522917992931557 | validation: 0.05079929996381554]
	TIME [epoch: 14.1 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05213994005177841		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.05213994005177841 | validation: 0.05731729646129629]
	TIME [epoch: 14.1 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05107844273030926		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.05107844273030926 | validation: 0.05458900582691324]
	TIME [epoch: 14.1 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04994462256346916		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.04994462256346916 | validation: 0.053052587545718656]
	TIME [epoch: 14 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05092942622759154		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.05092942622759154 | validation: 0.05291580567680187]
	TIME [epoch: 14.1 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050637332544098385		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.050637332544098385 | validation: 0.05346355699703224]
	TIME [epoch: 14.1 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04994346427368799		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.04994346427368799 | validation: 0.05641867442725084]
	TIME [epoch: 14.1 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05307480223756815		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.05307480223756815 | validation: 0.053033772889055975]
	TIME [epoch: 14.1 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05056314154166511		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.05056314154166511 | validation: 0.052959119601935736]
	TIME [epoch: 14.1 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051125954808825064		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.051125954808825064 | validation: 0.05681324350528103]
	TIME [epoch: 14.1 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04901721433159431		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.04901721433159431 | validation: 0.05284308819585253]
	TIME [epoch: 14.2 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05150291722947791		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.05150291722947791 | validation: 0.049648533125715044]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_871.pth
	Model improved!!!
EPOCH 872/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04952975980847267		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.04952975980847267 | validation: 0.04968486708468991]
	TIME [epoch: 14.1 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05205977369655883		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.05205977369655883 | validation: 0.052332534234075245]
	TIME [epoch: 14.1 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04953944776867748		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.04953944776867748 | validation: 0.052424676640382]
	TIME [epoch: 14.1 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050062159266124895		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.050062159266124895 | validation: 0.05097743985994227]
	TIME [epoch: 14.1 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050331263621245834		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.050331263621245834 | validation: 0.05326706140838893]
	TIME [epoch: 14.1 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05284454526102516		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.05284454526102516 | validation: 0.0526925690844304]
	TIME [epoch: 14.1 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05220501225588332		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.05220501225588332 | validation: 0.050834703083371485]
	TIME [epoch: 14.1 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04987831239451396		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.04987831239451396 | validation: 0.05251438804526862]
	TIME [epoch: 14.1 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050754017276709404		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.050754017276709404 | validation: 0.050566527803838725]
	TIME [epoch: 14.1 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052557351545263975		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.052557351545263975 | validation: 0.052679360437430114]
	TIME [epoch: 14.1 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05107268887033162		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.05107268887033162 | validation: 0.056881603403027364]
	TIME [epoch: 14.1 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04993801686217322		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.04993801686217322 | validation: 0.055268421488837774]
	TIME [epoch: 14 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050435342870540883		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.050435342870540883 | validation: 0.05308487514173852]
	TIME [epoch: 14.1 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05054017970868028		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.05054017970868028 | validation: 0.05230929425737413]
	TIME [epoch: 14.1 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051800904469608625		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.051800904469608625 | validation: 0.056113965465503936]
	TIME [epoch: 14.1 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05163309162425434		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.05163309162425434 | validation: 0.058849142441932206]
	TIME [epoch: 14.1 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05213537185883079		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.05213537185883079 | validation: 0.05397573784925887]
	TIME [epoch: 14.1 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04977224326151217		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.04977224326151217 | validation: 0.053350791676912396]
	TIME [epoch: 14.1 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051914597234017104		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.051914597234017104 | validation: 0.050986436874508136]
	TIME [epoch: 14.1 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04845163467262927		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.04845163467262927 | validation: 0.052204161481646]
	TIME [epoch: 14.1 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05035523476914104		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.05035523476914104 | validation: 0.05070460706719078]
	TIME [epoch: 14.1 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05208918139131403		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.05208918139131403 | validation: 0.05479729442909325]
	TIME [epoch: 14.1 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051909053760996055		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.051909053760996055 | validation: 0.05158900881144643]
	TIME [epoch: 14.1 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05132433409884618		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.05132433409884618 | validation: 0.05526871958059372]
	TIME [epoch: 14.1 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05124325043360632		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.05124325043360632 | validation: 0.05250432727815742]
	TIME [epoch: 14.1 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050502577381742214		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.050502577381742214 | validation: 0.0538565365579454]
	TIME [epoch: 14.1 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049769258165068775		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.049769258165068775 | validation: 0.05734122891355029]
	TIME [epoch: 14.1 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05096234930598371		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.05096234930598371 | validation: 0.05761640336522278]
	TIME [epoch: 14.1 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05213949362435216		[learning rate: 2.0381e-05]
	Learning Rate: 2.03813e-05
	LOSS [training: 0.05213949362435216 | validation: 0.05249548254507111]
	TIME [epoch: 14.1 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053567644613033774		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.053567644613033774 | validation: 0.052345830559980794]
	TIME [epoch: 14.2 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05062303359358392		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.05062303359358392 | validation: 0.052978621261963704]
	TIME [epoch: 14.1 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05086580124553478		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.05086580124553478 | validation: 0.05227024426439042]
	TIME [epoch: 14.1 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04924022566960118		[learning rate: 1.9812e-05]
	Learning Rate: 1.98118e-05
	LOSS [training: 0.04924022566960118 | validation: 0.04892174725541688]
	TIME [epoch: 14.2 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_904.pth
	Model improved!!!
EPOCH 905/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04963381582938424		[learning rate: 1.9672e-05]
	Learning Rate: 1.96719e-05
	LOSS [training: 0.04963381582938424 | validation: 0.05427742045992959]
	TIME [epoch: 14.1 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051568451909822385		[learning rate: 1.9533e-05]
	Learning Rate: 1.9533e-05
	LOSS [training: 0.051568451909822385 | validation: 0.05185463555631136]
	TIME [epoch: 14.1 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04994731938525948		[learning rate: 1.9395e-05]
	Learning Rate: 1.93951e-05
	LOSS [training: 0.04994731938525948 | validation: 0.051824856196102914]
	TIME [epoch: 14.1 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04977472705038892		[learning rate: 1.9258e-05]
	Learning Rate: 1.92582e-05
	LOSS [training: 0.04977472705038892 | validation: 0.0525469488216239]
	TIME [epoch: 14.1 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05235235885415934		[learning rate: 1.9122e-05]
	Learning Rate: 1.91222e-05
	LOSS [training: 0.05235235885415934 | validation: 0.05269622276379879]
	TIME [epoch: 14.1 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054827002354426946		[learning rate: 1.8987e-05]
	Learning Rate: 1.89872e-05
	LOSS [training: 0.054827002354426946 | validation: 0.05386428248175837]
	TIME [epoch: 14.1 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05190027059204346		[learning rate: 1.8853e-05]
	Learning Rate: 1.88532e-05
	LOSS [training: 0.05190027059204346 | validation: 0.05453468217301993]
	TIME [epoch: 14.1 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0504783146830747		[learning rate: 1.872e-05]
	Learning Rate: 1.87201e-05
	LOSS [training: 0.0504783146830747 | validation: 0.051187437452050066]
	TIME [epoch: 14.1 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05299486157191037		[learning rate: 1.8588e-05]
	Learning Rate: 1.85879e-05
	LOSS [training: 0.05299486157191037 | validation: 0.0532815707232561]
	TIME [epoch: 14.1 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05080743022228903		[learning rate: 1.8457e-05]
	Learning Rate: 1.84567e-05
	LOSS [training: 0.05080743022228903 | validation: 0.05270311778896184]
	TIME [epoch: 14.1 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049935706357104065		[learning rate: 1.8326e-05]
	Learning Rate: 1.83264e-05
	LOSS [training: 0.049935706357104065 | validation: 0.04980163488120388]
	TIME [epoch: 14.1 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05408712032072449		[learning rate: 1.8197e-05]
	Learning Rate: 1.8197e-05
	LOSS [training: 0.05408712032072449 | validation: 0.05382838324319899]
	TIME [epoch: 14.1 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05233179457522215		[learning rate: 1.8069e-05]
	Learning Rate: 1.80685e-05
	LOSS [training: 0.05233179457522215 | validation: 0.05345435647926522]
	TIME [epoch: 14.1 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05027249284559572		[learning rate: 1.7941e-05]
	Learning Rate: 1.7941e-05
	LOSS [training: 0.05027249284559572 | validation: 0.051540740532947264]
	TIME [epoch: 14.1 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047037313097387276		[learning rate: 1.7814e-05]
	Learning Rate: 1.78143e-05
	LOSS [training: 0.047037313097387276 | validation: 0.05362031746452907]
	TIME [epoch: 14.1 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04945601386494282		[learning rate: 1.7689e-05]
	Learning Rate: 1.76886e-05
	LOSS [training: 0.04945601386494282 | validation: 0.051502073631056036]
	TIME [epoch: 14.1 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05029261580753112		[learning rate: 1.7564e-05]
	Learning Rate: 1.75637e-05
	LOSS [training: 0.05029261580753112 | validation: 0.052049071947533936]
	TIME [epoch: 14.1 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052609205309454274		[learning rate: 1.744e-05]
	Learning Rate: 1.74397e-05
	LOSS [training: 0.052609205309454274 | validation: 0.05117574020064977]
	TIME [epoch: 14.1 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05039099024037125		[learning rate: 1.7317e-05]
	Learning Rate: 1.73166e-05
	LOSS [training: 0.05039099024037125 | validation: 0.05358792123463907]
	TIME [epoch: 14.1 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049945102013371025		[learning rate: 1.7194e-05]
	Learning Rate: 1.71943e-05
	LOSS [training: 0.049945102013371025 | validation: 0.050374922934064675]
	TIME [epoch: 14.1 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049975429541254404		[learning rate: 1.7073e-05]
	Learning Rate: 1.70729e-05
	LOSS [training: 0.049975429541254404 | validation: 0.05502241516433342]
	TIME [epoch: 14.1 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05063278216866994		[learning rate: 1.6952e-05]
	Learning Rate: 1.69524e-05
	LOSS [training: 0.05063278216866994 | validation: 0.053862148483895124]
	TIME [epoch: 14.1 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05034002877671745		[learning rate: 1.6833e-05]
	Learning Rate: 1.68327e-05
	LOSS [training: 0.05034002877671745 | validation: 0.054075377779603556]
	TIME [epoch: 14.1 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05114186289308732		[learning rate: 1.6714e-05]
	Learning Rate: 1.67139e-05
	LOSS [training: 0.05114186289308732 | validation: 0.05433474172221932]
	TIME [epoch: 14.1 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052837366408001914		[learning rate: 1.6596e-05]
	Learning Rate: 1.65959e-05
	LOSS [training: 0.052837366408001914 | validation: 0.05326543464116798]
	TIME [epoch: 14.1 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05092195341426068		[learning rate: 1.6479e-05]
	Learning Rate: 1.64787e-05
	LOSS [training: 0.05092195341426068 | validation: 0.051392436226697806]
	TIME [epoch: 14.1 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05078324242519986		[learning rate: 1.6362e-05]
	Learning Rate: 1.63624e-05
	LOSS [training: 0.05078324242519986 | validation: 0.05117849839989357]
	TIME [epoch: 14.1 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05192012071396637		[learning rate: 1.6247e-05]
	Learning Rate: 1.62469e-05
	LOSS [training: 0.05192012071396637 | validation: 0.04987136777662865]
	TIME [epoch: 14.1 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05091769966012262		[learning rate: 1.6132e-05]
	Learning Rate: 1.61322e-05
	LOSS [training: 0.05091769966012262 | validation: 0.052339273829055954]
	TIME [epoch: 14.1 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05192749872864634		[learning rate: 1.6018e-05]
	Learning Rate: 1.60183e-05
	LOSS [training: 0.05192749872864634 | validation: 0.05074081581659019]
	TIME [epoch: 14.1 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05023491803373127		[learning rate: 1.5905e-05]
	Learning Rate: 1.59052e-05
	LOSS [training: 0.05023491803373127 | validation: 0.05106766504904197]
	TIME [epoch: 14.1 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05069618475217983		[learning rate: 1.5793e-05]
	Learning Rate: 1.57929e-05
	LOSS [training: 0.05069618475217983 | validation: 0.05221214671203951]
	TIME [epoch: 14.1 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0495435119433109		[learning rate: 1.5681e-05]
	Learning Rate: 1.56814e-05
	LOSS [training: 0.0495435119433109 | validation: 0.050828079149610544]
	TIME [epoch: 14.1 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05129550405632102		[learning rate: 1.5571e-05]
	Learning Rate: 1.55707e-05
	LOSS [training: 0.05129550405632102 | validation: 0.05437011195711418]
	TIME [epoch: 14.1 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049245284245469166		[learning rate: 1.5461e-05]
	Learning Rate: 1.54608e-05
	LOSS [training: 0.049245284245469166 | validation: 0.052309216793270545]
	TIME [epoch: 14.1 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052276123499957736		[learning rate: 1.5352e-05]
	Learning Rate: 1.53516e-05
	LOSS [training: 0.052276123499957736 | validation: 0.05204195207070189]
	TIME [epoch: 14.1 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051217128499414705		[learning rate: 1.5243e-05]
	Learning Rate: 1.52432e-05
	LOSS [training: 0.051217128499414705 | validation: 0.05080289681985728]
	TIME [epoch: 14.1 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051643735393351625		[learning rate: 1.5136e-05]
	Learning Rate: 1.51356e-05
	LOSS [training: 0.051643735393351625 | validation: 0.05518676385850382]
	TIME [epoch: 14.1 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05392367067962914		[learning rate: 1.5029e-05]
	Learning Rate: 1.50288e-05
	LOSS [training: 0.05392367067962914 | validation: 0.057887426225650254]
	TIME [epoch: 14.1 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05186570014244799		[learning rate: 1.4923e-05]
	Learning Rate: 1.49227e-05
	LOSS [training: 0.05186570014244799 | validation: 0.056734634995603585]
	TIME [epoch: 14.1 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049248917067370106		[learning rate: 1.4817e-05]
	Learning Rate: 1.48173e-05
	LOSS [training: 0.049248917067370106 | validation: 0.05693648381470312]
	TIME [epoch: 14.1 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05055501308234006		[learning rate: 1.4713e-05]
	Learning Rate: 1.47127e-05
	LOSS [training: 0.05055501308234006 | validation: 0.05243188786153237]
	TIME [epoch: 14.1 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05021199377336592		[learning rate: 1.4609e-05]
	Learning Rate: 1.46088e-05
	LOSS [training: 0.05021199377336592 | validation: 0.054346977606299175]
	TIME [epoch: 14.1 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05183601021679565		[learning rate: 1.4506e-05]
	Learning Rate: 1.45057e-05
	LOSS [training: 0.05183601021679565 | validation: 0.04956611439066946]
	TIME [epoch: 14.1 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048699075222284546		[learning rate: 1.4403e-05]
	Learning Rate: 1.44033e-05
	LOSS [training: 0.048699075222284546 | validation: 0.05167090637070442]
	TIME [epoch: 14.1 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049576118729948424		[learning rate: 1.4302e-05]
	Learning Rate: 1.43016e-05
	LOSS [training: 0.049576118729948424 | validation: 0.048439533087907834]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_950.pth
	Model improved!!!
EPOCH 951/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05040002895569076		[learning rate: 1.4201e-05]
	Learning Rate: 1.42006e-05
	LOSS [training: 0.05040002895569076 | validation: 0.05315259972817152]
	TIME [epoch: 14.1 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05231531652482015		[learning rate: 1.41e-05]
	Learning Rate: 1.41004e-05
	LOSS [training: 0.05231531652482015 | validation: 0.05045873381606003]
	TIME [epoch: 14.1 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04989887946121614		[learning rate: 1.4001e-05]
	Learning Rate: 1.40008e-05
	LOSS [training: 0.04989887946121614 | validation: 0.05290359621209728]
	TIME [epoch: 14.2 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05121389721564003		[learning rate: 1.3902e-05]
	Learning Rate: 1.3902e-05
	LOSS [training: 0.05121389721564003 | validation: 0.05115798205968095]
	TIME [epoch: 14.1 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04967565053732301		[learning rate: 1.3804e-05]
	Learning Rate: 1.38038e-05
	LOSS [training: 0.04967565053732301 | validation: 0.053780977939637334]
	TIME [epoch: 14.1 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0497842020535836		[learning rate: 1.3706e-05]
	Learning Rate: 1.37064e-05
	LOSS [training: 0.0497842020535836 | validation: 0.050053235769331475]
	TIME [epoch: 14.1 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04829839041743732		[learning rate: 1.361e-05]
	Learning Rate: 1.36096e-05
	LOSS [training: 0.04829839041743732 | validation: 0.05520478812804091]
	TIME [epoch: 14.1 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05239554961927115		[learning rate: 1.3514e-05]
	Learning Rate: 1.35135e-05
	LOSS [training: 0.05239554961927115 | validation: 0.053198040764943826]
	TIME [epoch: 14.1 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0481898924627637		[learning rate: 1.3418e-05]
	Learning Rate: 1.34181e-05
	LOSS [training: 0.0481898924627637 | validation: 0.05534703377729538]
	TIME [epoch: 14.1 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04809530936450717		[learning rate: 1.3323e-05]
	Learning Rate: 1.33234e-05
	LOSS [training: 0.04809530936450717 | validation: 0.055826702008506396]
	TIME [epoch: 14.1 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05053912917957619		[learning rate: 1.3229e-05]
	Learning Rate: 1.32294e-05
	LOSS [training: 0.05053912917957619 | validation: 0.05449272269244043]
	TIME [epoch: 14.2 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05084223808701381		[learning rate: 1.3136e-05]
	Learning Rate: 1.3136e-05
	LOSS [training: 0.05084223808701381 | validation: 0.05380614121504867]
	TIME [epoch: 14.1 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05135208002382664		[learning rate: 1.3043e-05]
	Learning Rate: 1.30432e-05
	LOSS [training: 0.05135208002382664 | validation: 0.05166262227244041]
	TIME [epoch: 14.1 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05074587583160778		[learning rate: 1.2951e-05]
	Learning Rate: 1.29511e-05
	LOSS [training: 0.05074587583160778 | validation: 0.052593247590817606]
	TIME [epoch: 14.2 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04991057850092176		[learning rate: 1.286e-05]
	Learning Rate: 1.28597e-05
	LOSS [training: 0.04991057850092176 | validation: 0.05461913304062205]
	TIME [epoch: 14.1 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05216525246821419		[learning rate: 1.2769e-05]
	Learning Rate: 1.27689e-05
	LOSS [training: 0.05216525246821419 | validation: 0.053069351584519614]
	TIME [epoch: 14.1 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0495253181263568		[learning rate: 1.2679e-05]
	Learning Rate: 1.26788e-05
	LOSS [training: 0.0495253181263568 | validation: 0.05190042225608105]
	TIME [epoch: 14.2 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04811025134057762		[learning rate: 1.2589e-05]
	Learning Rate: 1.25893e-05
	LOSS [training: 0.04811025134057762 | validation: 0.053703707617188526]
	TIME [epoch: 14.1 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0520220610082271		[learning rate: 1.25e-05]
	Learning Rate: 1.25004e-05
	LOSS [training: 0.0520220610082271 | validation: 0.051661153291499354]
	TIME [epoch: 14.1 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050290326462821465		[learning rate: 1.2412e-05]
	Learning Rate: 1.24121e-05
	LOSS [training: 0.050290326462821465 | validation: 0.049346551002221656]
	TIME [epoch: 14.2 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04905086740954165		[learning rate: 1.2325e-05]
	Learning Rate: 1.23245e-05
	LOSS [training: 0.04905086740954165 | validation: 0.05097678621266158]
	TIME [epoch: 14.1 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04882525293844826		[learning rate: 1.2237e-05]
	Learning Rate: 1.22375e-05
	LOSS [training: 0.04882525293844826 | validation: 0.05167914090871977]
	TIME [epoch: 14.1 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05007292972414153		[learning rate: 1.2151e-05]
	Learning Rate: 1.21511e-05
	LOSS [training: 0.05007292972414153 | validation: 0.05449639199187373]
	TIME [epoch: 14.2 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051139216808947466		[learning rate: 1.2065e-05]
	Learning Rate: 1.20653e-05
	LOSS [training: 0.051139216808947466 | validation: 0.05526779337778375]
	TIME [epoch: 14.1 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05146145670914133		[learning rate: 1.198e-05]
	Learning Rate: 1.19801e-05
	LOSS [training: 0.05146145670914133 | validation: 0.05552810088675908]
	TIME [epoch: 14.2 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051900791420314146		[learning rate: 1.1896e-05]
	Learning Rate: 1.18956e-05
	LOSS [training: 0.051900791420314146 | validation: 0.0518354651838043]
	TIME [epoch: 14.1 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05199954152453869		[learning rate: 1.1812e-05]
	Learning Rate: 1.18116e-05
	LOSS [training: 0.05199954152453869 | validation: 0.050629814059987704]
	TIME [epoch: 14.1 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05231283828509112		[learning rate: 1.1728e-05]
	Learning Rate: 1.17282e-05
	LOSS [training: 0.05231283828509112 | validation: 0.057651183839894364]
	TIME [epoch: 14.1 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052240853522774994		[learning rate: 1.1645e-05]
	Learning Rate: 1.16454e-05
	LOSS [training: 0.052240853522774994 | validation: 0.052530770063625915]
	TIME [epoch: 14.1 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050309665346832504		[learning rate: 1.1563e-05]
	Learning Rate: 1.15632e-05
	LOSS [training: 0.050309665346832504 | validation: 0.05349832049745815]
	TIME [epoch: 14.1 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0520262792778644		[learning rate: 1.1482e-05]
	Learning Rate: 1.14815e-05
	LOSS [training: 0.0520262792778644 | validation: 0.05557858432164299]
	TIME [epoch: 14.1 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05073497152940677		[learning rate: 1.14e-05]
	Learning Rate: 1.14005e-05
	LOSS [training: 0.05073497152940677 | validation: 0.05220711098999097]
	TIME [epoch: 14.1 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0503569298434019		[learning rate: 1.132e-05]
	Learning Rate: 1.132e-05
	LOSS [training: 0.0503569298434019 | validation: 0.051813563949971476]
	TIME [epoch: 14.1 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04877198516457277		[learning rate: 1.124e-05]
	Learning Rate: 1.12401e-05
	LOSS [training: 0.04877198516457277 | validation: 0.05304716115375172]
	TIME [epoch: 14.2 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05013783218968638		[learning rate: 1.1161e-05]
	Learning Rate: 1.11607e-05
	LOSS [training: 0.05013783218968638 | validation: 0.05431473988027385]
	TIME [epoch: 14.1 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05191960232797759		[learning rate: 1.1082e-05]
	Learning Rate: 1.10819e-05
	LOSS [training: 0.05191960232797759 | validation: 0.053327827971624435]
	TIME [epoch: 14.2 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051700920039532985		[learning rate: 1.1004e-05]
	Learning Rate: 1.10037e-05
	LOSS [training: 0.051700920039532985 | validation: 0.05134867762005357]
	TIME [epoch: 14.2 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05024856232763812		[learning rate: 1.0926e-05]
	Learning Rate: 1.0926e-05
	LOSS [training: 0.05024856232763812 | validation: 0.05266076576663358]
	TIME [epoch: 14.1 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04985821145047687		[learning rate: 1.0849e-05]
	Learning Rate: 1.08489e-05
	LOSS [training: 0.04985821145047687 | validation: 0.05338858984818149]
	TIME [epoch: 14.1 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049025361855460345		[learning rate: 1.0772e-05]
	Learning Rate: 1.07723e-05
	LOSS [training: 0.049025361855460345 | validation: 0.05117523447840788]
	TIME [epoch: 14.2 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05052849390730623		[learning rate: 1.0696e-05]
	Learning Rate: 1.06962e-05
	LOSS [training: 0.05052849390730623 | validation: 0.05254579918785747]
	TIME [epoch: 14.1 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051348102058938465		[learning rate: 1.0621e-05]
	Learning Rate: 1.06207e-05
	LOSS [training: 0.051348102058938465 | validation: 0.05183288049593422]
	TIME [epoch: 14.1 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05005896535474989		[learning rate: 1.0546e-05]
	Learning Rate: 1.05457e-05
	LOSS [training: 0.05005896535474989 | validation: 0.05262665799547686]
	TIME [epoch: 14.1 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0504350642384346		[learning rate: 1.0471e-05]
	Learning Rate: 1.04713e-05
	LOSS [training: 0.0504350642384346 | validation: 0.05149669138765814]
	TIME [epoch: 14.1 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051559699838884906		[learning rate: 1.0397e-05]
	Learning Rate: 1.03974e-05
	LOSS [training: 0.051559699838884906 | validation: 0.052187285634054184]
	TIME [epoch: 14.2 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05009703315128426		[learning rate: 1.0324e-05]
	Learning Rate: 1.0324e-05
	LOSS [training: 0.05009703315128426 | validation: 0.05334821429868615]
	TIME [epoch: 14.1 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051149325726594384		[learning rate: 1.0251e-05]
	Learning Rate: 1.02511e-05
	LOSS [training: 0.051149325726594384 | validation: 0.051007661551959196]
	TIME [epoch: 14.2 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04764866859628321		[learning rate: 1.0179e-05]
	Learning Rate: 1.01787e-05
	LOSS [training: 0.04764866859628321 | validation: 0.048495303175470544]
	TIME [epoch: 14.2 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05162277469799749		[learning rate: 1.0107e-05]
	Learning Rate: 1.01068e-05
	LOSS [training: 0.05162277469799749 | validation: 0.05305090725705655]
	TIME [epoch: 14.1 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04858980054629783		[learning rate: 1.0035e-05]
	Learning Rate: 1.00355e-05
	LOSS [training: 0.04858980054629783 | validation: 0.05253491973531386]
	TIME [epoch: 14.1 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0513567108472097		[learning rate: 9.9646e-06]
	Learning Rate: 9.96464e-06
	LOSS [training: 0.0513567108472097 | validation: 0.050257339745166885]
	TIME [epoch: 131 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05149324248632524		[learning rate: 9.8943e-06]
	Learning Rate: 9.89429e-06
	LOSS [training: 0.05149324248632524 | validation: 0.05294802237324996]
	TIME [epoch: 30.2 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04966487278911961		[learning rate: 9.8244e-06]
	Learning Rate: 9.82444e-06
	LOSS [training: 0.04966487278911961 | validation: 0.05194935306952691]
	TIME [epoch: 30.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0499293974569374		[learning rate: 9.7551e-06]
	Learning Rate: 9.75508e-06
	LOSS [training: 0.0499293974569374 | validation: 0.05455314184406578]
	TIME [epoch: 30.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050216451136835455		[learning rate: 9.6862e-06]
	Learning Rate: 9.68621e-06
	LOSS [training: 0.050216451136835455 | validation: 0.04950975090405609]
	TIME [epoch: 30.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04878576829711294		[learning rate: 9.6178e-06]
	Learning Rate: 9.61783e-06
	LOSS [training: 0.04878576829711294 | validation: 0.05300143006510192]
	TIME [epoch: 30.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053903216853543316		[learning rate: 9.5499e-06]
	Learning Rate: 9.54993e-06
	LOSS [training: 0.053903216853543316 | validation: 0.05077141187501293]
	TIME [epoch: 30.3 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05010280385356202		[learning rate: 9.4825e-06]
	Learning Rate: 9.48251e-06
	LOSS [training: 0.05010280385356202 | validation: 0.047760923065497764]
	TIME [epoch: 30.3 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_1008.pth
	Model improved!!!
EPOCH 1009/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049465041047474095		[learning rate: 9.4156e-06]
	Learning Rate: 9.41556e-06
	LOSS [training: 0.049465041047474095 | validation: 0.055446631951196285]
	TIME [epoch: 30.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04973429340027474		[learning rate: 9.3491e-06]
	Learning Rate: 9.34909e-06
	LOSS [training: 0.04973429340027474 | validation: 0.0542616380838725]
	TIME [epoch: 30.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04991720681598988		[learning rate: 9.2831e-06]
	Learning Rate: 9.28308e-06
	LOSS [training: 0.04991720681598988 | validation: 0.05102527839422828]
	TIME [epoch: 30.3 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04951773025174605		[learning rate: 9.2176e-06]
	Learning Rate: 9.21755e-06
	LOSS [training: 0.04951773025174605 | validation: 0.05062724458281769]
	TIME [epoch: 30.3 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05077049435353763		[learning rate: 9.1525e-06]
	Learning Rate: 9.15248e-06
	LOSS [training: 0.05077049435353763 | validation: 0.05304710428283104]
	TIME [epoch: 30.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05122406440570494		[learning rate: 9.0879e-06]
	Learning Rate: 9.08786e-06
	LOSS [training: 0.05122406440570494 | validation: 0.05308040229672048]
	TIME [epoch: 30.3 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05086520990984361		[learning rate: 9.0237e-06]
	Learning Rate: 9.0237e-06
	LOSS [training: 0.05086520990984361 | validation: 0.0503025630627709]
	TIME [epoch: 30.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04867010397612215		[learning rate: 8.96e-06]
	Learning Rate: 8.96e-06
	LOSS [training: 0.04867010397612215 | validation: 0.05206891194319463]
	TIME [epoch: 30.3 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05055400839532956		[learning rate: 8.8967e-06]
	Learning Rate: 8.89674e-06
	LOSS [training: 0.05055400839532956 | validation: 0.0509895187671775]
	TIME [epoch: 30.3 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04784181080143218		[learning rate: 8.8339e-06]
	Learning Rate: 8.83393e-06
	LOSS [training: 0.04784181080143218 | validation: 0.051847691731527215]
	TIME [epoch: 30.4 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05087331878859075		[learning rate: 8.7716e-06]
	Learning Rate: 8.77157e-06
	LOSS [training: 0.05087331878859075 | validation: 0.05435241476393955]
	TIME [epoch: 30.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048786150226307876		[learning rate: 8.7096e-06]
	Learning Rate: 8.70964e-06
	LOSS [training: 0.048786150226307876 | validation: 0.05014030843848688]
	TIME [epoch: 30.3 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04824895453966772		[learning rate: 8.6481e-06]
	Learning Rate: 8.64815e-06
	LOSS [training: 0.04824895453966772 | validation: 0.05664089130967385]
	TIME [epoch: 30.3 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05069790054737046		[learning rate: 8.5871e-06]
	Learning Rate: 8.5871e-06
	LOSS [training: 0.05069790054737046 | validation: 0.052411525118928506]
	TIME [epoch: 30.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04962208177004135		[learning rate: 8.5265e-06]
	Learning Rate: 8.52647e-06
	LOSS [training: 0.04962208177004135 | validation: 0.051592497377168284]
	TIME [epoch: 30.4 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0490074456295501		[learning rate: 8.4663e-06]
	Learning Rate: 8.46627e-06
	LOSS [training: 0.0490074456295501 | validation: 0.04876872460726807]
	TIME [epoch: 30.4 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050998742669250474		[learning rate: 8.4065e-06]
	Learning Rate: 8.40651e-06
	LOSS [training: 0.050998742669250474 | validation: 0.05427468558720249]
	TIME [epoch: 30.4 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04988702808850362		[learning rate: 8.3472e-06]
	Learning Rate: 8.34716e-06
	LOSS [training: 0.04988702808850362 | validation: 0.05413298392307149]
	TIME [epoch: 30.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048258334124081786		[learning rate: 8.2882e-06]
	Learning Rate: 8.28823e-06
	LOSS [training: 0.048258334124081786 | validation: 0.0531534991790982]
	TIME [epoch: 30.4 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05037652544864515		[learning rate: 8.2297e-06]
	Learning Rate: 8.22972e-06
	LOSS [training: 0.05037652544864515 | validation: 0.05136904845309348]
	TIME [epoch: 30.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048685650727461546		[learning rate: 8.1716e-06]
	Learning Rate: 8.17162e-06
	LOSS [training: 0.048685650727461546 | validation: 0.052827172302095694]
	TIME [epoch: 30.4 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048312094963769835		[learning rate: 8.1139e-06]
	Learning Rate: 8.11392e-06
	LOSS [training: 0.048312094963769835 | validation: 0.05534291603414654]
	TIME [epoch: 30.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05073473167342604		[learning rate: 8.0566e-06]
	Learning Rate: 8.05664e-06
	LOSS [training: 0.05073473167342604 | validation: 0.04967865904361024]
	TIME [epoch: 30.4 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05010772713735768		[learning rate: 7.9998e-06]
	Learning Rate: 7.99976e-06
	LOSS [training: 0.05010772713735768 | validation: 0.05269898772314677]
	TIME [epoch: 30.3 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04905616279763634		[learning rate: 7.9433e-06]
	Learning Rate: 7.94328e-06
	LOSS [training: 0.04905616279763634 | validation: 0.05114556391275528]
	TIME [epoch: 30.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048321829035270894		[learning rate: 7.8872e-06]
	Learning Rate: 7.8872e-06
	LOSS [training: 0.048321829035270894 | validation: 0.05339766283189811]
	TIME [epoch: 30.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049971605317363624		[learning rate: 7.8315e-06]
	Learning Rate: 7.83153e-06
	LOSS [training: 0.049971605317363624 | validation: 0.050967537555772525]
	TIME [epoch: 30.2 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0494326385620579		[learning rate: 7.7762e-06]
	Learning Rate: 7.77624e-06
	LOSS [training: 0.0494326385620579 | validation: 0.04846082252970295]
	TIME [epoch: 30.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05162483424454055		[learning rate: 7.7213e-06]
	Learning Rate: 7.72133e-06
	LOSS [training: 0.05162483424454055 | validation: 0.05157291221142595]
	TIME [epoch: 30.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050263379644302573		[learning rate: 7.6668e-06]
	Learning Rate: 7.66683e-06
	LOSS [training: 0.050263379644302573 | validation: 0.051344464587344256]
	TIME [epoch: 30.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051957266815621406		[learning rate: 7.6127e-06]
	Learning Rate: 7.6127e-06
	LOSS [training: 0.051957266815621406 | validation: 0.05333556637902648]
	TIME [epoch: 30.2 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04887133823803098		[learning rate: 7.559e-06]
	Learning Rate: 7.55895e-06
	LOSS [training: 0.04887133823803098 | validation: 0.05326709027432819]
	TIME [epoch: 30.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050288256119930146		[learning rate: 7.5056e-06]
	Learning Rate: 7.50559e-06
	LOSS [training: 0.050288256119930146 | validation: 0.05184866023515886]
	TIME [epoch: 30.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05301291548221669		[learning rate: 7.4526e-06]
	Learning Rate: 7.4526e-06
	LOSS [training: 0.05301291548221669 | validation: 0.04870564578634027]
	TIME [epoch: 30.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0488437867199709		[learning rate: 7.4e-06]
	Learning Rate: 7.39998e-06
	LOSS [training: 0.0488437867199709 | validation: 0.05200960080120814]
	TIME [epoch: 30.3 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04969078870606203		[learning rate: 7.3477e-06]
	Learning Rate: 7.34774e-06
	LOSS [training: 0.04969078870606203 | validation: 0.04981660066113146]
	TIME [epoch: 30.4 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051264803162544864		[learning rate: 7.2959e-06]
	Learning Rate: 7.29587e-06
	LOSS [training: 0.051264803162544864 | validation: 0.05003191915387331]
	TIME [epoch: 30.2 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05037368725201952		[learning rate: 7.2444e-06]
	Learning Rate: 7.24436e-06
	LOSS [training: 0.05037368725201952 | validation: 0.05194966147298177]
	TIME [epoch: 30.4 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04985594546465662		[learning rate: 7.1932e-06]
	Learning Rate: 7.19322e-06
	LOSS [training: 0.04985594546465662 | validation: 0.052499375551767063]
	TIME [epoch: 30.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04660574438561464		[learning rate: 7.1424e-06]
	Learning Rate: 7.14244e-06
	LOSS [training: 0.04660574438561464 | validation: 0.05000403972844108]
	TIME [epoch: 30.4 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05010002430249001		[learning rate: 7.092e-06]
	Learning Rate: 7.09201e-06
	LOSS [training: 0.05010002430249001 | validation: 0.05383733925934447]
	TIME [epoch: 30.4 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049440851221150106		[learning rate: 7.0419e-06]
	Learning Rate: 7.04194e-06
	LOSS [training: 0.049440851221150106 | validation: 0.051613338074045806]
	TIME [epoch: 30.4 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05114140224756178		[learning rate: 6.9922e-06]
	Learning Rate: 6.99222e-06
	LOSS [training: 0.05114140224756178 | validation: 0.051120127632508464]
	TIME [epoch: 30.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05210601780734231		[learning rate: 6.9429e-06]
	Learning Rate: 6.94286e-06
	LOSS [training: 0.05210601780734231 | validation: 0.051414727690057095]
	TIME [epoch: 30.3 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0482900916903952		[learning rate: 6.8938e-06]
	Learning Rate: 6.89385e-06
	LOSS [training: 0.0482900916903952 | validation: 0.05188439811992338]
	TIME [epoch: 30.4 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05190409670428125		[learning rate: 6.8452e-06]
	Learning Rate: 6.84518e-06
	LOSS [training: 0.05190409670428125 | validation: 0.05137969640234542]
	TIME [epoch: 30.3 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050038531437874297		[learning rate: 6.7969e-06]
	Learning Rate: 6.79685e-06
	LOSS [training: 0.050038531437874297 | validation: 0.05276409453440625]
	TIME [epoch: 30.4 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04984688004478176		[learning rate: 6.7489e-06]
	Learning Rate: 6.74887e-06
	LOSS [training: 0.04984688004478176 | validation: 0.05613237439381017]
	TIME [epoch: 30.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05015392350188795		[learning rate: 6.7012e-06]
	Learning Rate: 6.70122e-06
	LOSS [training: 0.05015392350188795 | validation: 0.05115157118908176]
	TIME [epoch: 30.4 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050627448200078994		[learning rate: 6.6539e-06]
	Learning Rate: 6.65391e-06
	LOSS [training: 0.050627448200078994 | validation: 0.053282700973926934]
	TIME [epoch: 30.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050652332994852015		[learning rate: 6.6069e-06]
	Learning Rate: 6.60694e-06
	LOSS [training: 0.050652332994852015 | validation: 0.051445134043646173]
	TIME [epoch: 30.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05068148719036987		[learning rate: 6.5603e-06]
	Learning Rate: 6.56029e-06
	LOSS [training: 0.05068148719036987 | validation: 0.05324523442730831]
	TIME [epoch: 30.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04904373814981354		[learning rate: 6.514e-06]
	Learning Rate: 6.51398e-06
	LOSS [training: 0.04904373814981354 | validation: 0.05141058823662869]
	TIME [epoch: 30.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04911515300945712		[learning rate: 6.468e-06]
	Learning Rate: 6.46799e-06
	LOSS [training: 0.04911515300945712 | validation: 0.052343068808036936]
	TIME [epoch: 30.4 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04827791513705834		[learning rate: 6.4223e-06]
	Learning Rate: 6.42233e-06
	LOSS [training: 0.04827791513705834 | validation: 0.05254781815214798]
	TIME [epoch: 30.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05102727834943927		[learning rate: 6.377e-06]
	Learning Rate: 6.37698e-06
	LOSS [training: 0.05102727834943927 | validation: 0.05361267339492583]
	TIME [epoch: 30.4 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05002254438173223		[learning rate: 6.332e-06]
	Learning Rate: 6.33197e-06
	LOSS [training: 0.05002254438173223 | validation: 0.051764926973160034]
	TIME [epoch: 30.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050220354465265266		[learning rate: 6.2873e-06]
	Learning Rate: 6.28726e-06
	LOSS [training: 0.050220354465265266 | validation: 0.05046654620199645]
	TIME [epoch: 30.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04971570789216613		[learning rate: 6.2429e-06]
	Learning Rate: 6.24288e-06
	LOSS [training: 0.04971570789216613 | validation: 0.05229893176181876]
	TIME [epoch: 30.3 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050917227622396616		[learning rate: 6.1988e-06]
	Learning Rate: 6.1988e-06
	LOSS [training: 0.050917227622396616 | validation: 0.05267976488224999]
	TIME [epoch: 30.3 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0485048277138993		[learning rate: 6.155e-06]
	Learning Rate: 6.15504e-06
	LOSS [training: 0.0485048277138993 | validation: 0.05117415472898255]
	TIME [epoch: 30.4 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049311091065768045		[learning rate: 6.1116e-06]
	Learning Rate: 6.11159e-06
	LOSS [training: 0.049311091065768045 | validation: 0.05254017644806056]
	TIME [epoch: 30.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04904137790069884		[learning rate: 6.0684e-06]
	Learning Rate: 6.06844e-06
	LOSS [training: 0.04904137790069884 | validation: 0.05163874903075616]
	TIME [epoch: 30.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05064625463203869		[learning rate: 6.0256e-06]
	Learning Rate: 6.0256e-06
	LOSS [training: 0.05064625463203869 | validation: 0.053379873231167635]
	TIME [epoch: 30.4 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0501978112105453		[learning rate: 5.9831e-06]
	Learning Rate: 5.98306e-06
	LOSS [training: 0.0501978112105453 | validation: 0.056706097783143575]
	TIME [epoch: 30.4 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048799826545402464		[learning rate: 5.9408e-06]
	Learning Rate: 5.94082e-06
	LOSS [training: 0.048799826545402464 | validation: 0.04872986970147478]
	TIME [epoch: 30.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04986892578102113		[learning rate: 5.8989e-06]
	Learning Rate: 5.89888e-06
	LOSS [training: 0.04986892578102113 | validation: 0.04863493860860739]
	TIME [epoch: 30.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049147959103116104		[learning rate: 5.8572e-06]
	Learning Rate: 5.85723e-06
	LOSS [training: 0.049147959103116104 | validation: 0.05047897139251766]
	TIME [epoch: 30.4 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05054926704891259		[learning rate: 5.8159e-06]
	Learning Rate: 5.81588e-06
	LOSS [training: 0.05054926704891259 | validation: 0.05238627485821339]
	TIME [epoch: 30.3 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05117614411500327		[learning rate: 5.7748e-06]
	Learning Rate: 5.77482e-06
	LOSS [training: 0.05117614411500327 | validation: 0.05009370483310267]
	TIME [epoch: 30.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051783497599549345		[learning rate: 5.7341e-06]
	Learning Rate: 5.73405e-06
	LOSS [training: 0.051783497599549345 | validation: 0.05161593450398856]
	TIME [epoch: 30.4 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048898771147455874		[learning rate: 5.6936e-06]
	Learning Rate: 5.69357e-06
	LOSS [training: 0.048898771147455874 | validation: 0.05153714238962268]
	TIME [epoch: 30.4 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04983763725022683		[learning rate: 5.6534e-06]
	Learning Rate: 5.65337e-06
	LOSS [training: 0.04983763725022683 | validation: 0.05138462027194557]
	TIME [epoch: 30.4 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05089924942087732		[learning rate: 5.6135e-06]
	Learning Rate: 5.61346e-06
	LOSS [training: 0.05089924942087732 | validation: 0.052718499072791206]
	TIME [epoch: 30.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05222541480863349		[learning rate: 5.5738e-06]
	Learning Rate: 5.57383e-06
	LOSS [training: 0.05222541480863349 | validation: 0.053405195344753575]
	TIME [epoch: 30.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04847918115496346		[learning rate: 5.5345e-06]
	Learning Rate: 5.53448e-06
	LOSS [training: 0.04847918115496346 | validation: 0.05149675680271897]
	TIME [epoch: 30.4 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04987559288434197		[learning rate: 5.4954e-06]
	Learning Rate: 5.49541e-06
	LOSS [training: 0.04987559288434197 | validation: 0.05012912827984992]
	TIME [epoch: 30.3 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04991133592872893		[learning rate: 5.4566e-06]
	Learning Rate: 5.45661e-06
	LOSS [training: 0.04991133592872893 | validation: 0.054264060946700755]
	TIME [epoch: 30.3 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05076409625370612		[learning rate: 5.4181e-06]
	Learning Rate: 5.41809e-06
	LOSS [training: 0.05076409625370612 | validation: 0.05328681684723072]
	TIME [epoch: 30.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04951918579230015		[learning rate: 5.3798e-06]
	Learning Rate: 5.37984e-06
	LOSS [training: 0.04951918579230015 | validation: 0.049236460473471966]
	TIME [epoch: 30.3 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0486246309150902		[learning rate: 5.3419e-06]
	Learning Rate: 5.34186e-06
	LOSS [training: 0.0486246309150902 | validation: 0.05267932925455853]
	TIME [epoch: 30.3 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04982916736923767		[learning rate: 5.3041e-06]
	Learning Rate: 5.30415e-06
	LOSS [training: 0.04982916736923767 | validation: 0.05044024100556202]
	TIME [epoch: 30.3 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04887310596893002		[learning rate: 5.2667e-06]
	Learning Rate: 5.2667e-06
	LOSS [training: 0.04887310596893002 | validation: 0.05106275767133459]
	TIME [epoch: 30.4 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04945873461501957		[learning rate: 5.2295e-06]
	Learning Rate: 5.22952e-06
	LOSS [training: 0.04945873461501957 | validation: 0.05185365588039908]
	TIME [epoch: 30.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048937373071169174		[learning rate: 5.1926e-06]
	Learning Rate: 5.1926e-06
	LOSS [training: 0.048937373071169174 | validation: 0.049855060937162674]
	TIME [epoch: 30.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04845400692114171		[learning rate: 5.1559e-06]
	Learning Rate: 5.15594e-06
	LOSS [training: 0.04845400692114171 | validation: 0.05208870481710081]
	TIME [epoch: 30.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047308690686853574		[learning rate: 5.1195e-06]
	Learning Rate: 5.11954e-06
	LOSS [training: 0.047308690686853574 | validation: 0.049584557536199905]
	TIME [epoch: 30.3 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04935943701352492		[learning rate: 5.0834e-06]
	Learning Rate: 5.0834e-06
	LOSS [training: 0.04935943701352492 | validation: 0.05113805475514806]
	TIME [epoch: 30.2 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04817844531579106		[learning rate: 5.0475e-06]
	Learning Rate: 5.04751e-06
	LOSS [training: 0.04817844531579106 | validation: 0.050950729181099046]
	TIME [epoch: 30.2 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04784564450358558		[learning rate: 5.0119e-06]
	Learning Rate: 5.01188e-06
	LOSS [training: 0.04784564450358558 | validation: 0.052820751241723474]
	TIME [epoch: 30.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048730140547413245		[learning rate: 4.9765e-06]
	Learning Rate: 4.97649e-06
	LOSS [training: 0.048730140547413245 | validation: 0.05581402035605865]
	TIME [epoch: 30.4 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05013726099864493		[learning rate: 4.9414e-06]
	Learning Rate: 4.94136e-06
	LOSS [training: 0.05013726099864493 | validation: 0.05197937529571337]
	TIME [epoch: 30.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04910186183908766		[learning rate: 4.9065e-06]
	Learning Rate: 4.90647e-06
	LOSS [training: 0.04910186183908766 | validation: 0.05405367911408576]
	TIME [epoch: 30.2 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05010618683962781		[learning rate: 4.8718e-06]
	Learning Rate: 4.87183e-06
	LOSS [training: 0.05010618683962781 | validation: 0.051230446858852655]
	TIME [epoch: 30.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051527743267346264		[learning rate: 4.8374e-06]
	Learning Rate: 4.83744e-06
	LOSS [training: 0.051527743267346264 | validation: 0.05233943297163042]
	TIME [epoch: 30.4 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050612322374376045		[learning rate: 4.8033e-06]
	Learning Rate: 4.80329e-06
	LOSS [training: 0.050612322374376045 | validation: 0.05091999958092193]
	TIME [epoch: 30.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04968506599105144		[learning rate: 4.7694e-06]
	Learning Rate: 4.76938e-06
	LOSS [training: 0.04968506599105144 | validation: 0.051200849956339424]
	TIME [epoch: 30.3 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04855334350430331		[learning rate: 4.7357e-06]
	Learning Rate: 4.73571e-06
	LOSS [training: 0.04855334350430331 | validation: 0.05387423029746479]
	TIME [epoch: 30.3 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05264633600385579		[learning rate: 4.7023e-06]
	Learning Rate: 4.70227e-06
	LOSS [training: 0.05264633600385579 | validation: 0.04906315801493913]
	TIME [epoch: 30.4 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04657682450569518		[learning rate: 4.6691e-06]
	Learning Rate: 4.66908e-06
	LOSS [training: 0.04657682450569518 | validation: 0.052152775603838554]
	TIME [epoch: 30.3 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052186267220754076		[learning rate: 4.6361e-06]
	Learning Rate: 4.63611e-06
	LOSS [training: 0.052186267220754076 | validation: 0.05058569525803245]
	TIME [epoch: 30.3 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_193441/states/model_phi1_2b_v_mmd1_1109.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 13470.373 seconds.
